{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMdvYrtPfZqF"
   },
   "source": [
    "# **Scientific journal recommender for submitting a publication**\n",
    "\n",
    "Parser:  \n",
    "https://it.wikipedia.org/wiki/BibTeX  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T02:22:23.085650900Z",
     "start_time": "2024-01-07T02:22:23.063925800Z"
    },
    "id": "NAniabkkfix9"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "folder = \"datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "For each class (journal) there is a file in BibTeX format containing the articles published in that journal. Each file was cleaned and formatted with the following online tool [BibTeX Tidy](https://flamingtempura.github.io/bibtex-tidy/index.html).\n",
    "\n",
    "Each article is represented by a record with the following fields:\n",
    "* **abstract**: Abstract of the article.\n",
    "* **author**: Author of the article.\n",
    "* **ENTRYTYPE**: Type of entry (article, book, inproceedings, etc.).\n",
    "* **doi**: Digital Object Identifier of the article.\n",
    "* **ID**: Unique identifier of the article.\n",
    "* **issn**: International Standard Serial Number of the journal in which the article was published.\n",
    "* **journal**: Journal in which the article was published.\n",
    "* **keywords**: Keywords of the article.\n",
    "* **note**: Additional information about the article.\n",
    "* **pages**: Pages of the article.\n",
    "* **title**: Title of the article.\n",
    "* **url**: URL of the article.\n",
    "* **volume**: Volume of the journal in which the article was published.\n",
    "* **year**: Year of publication of the article.\n",
    "\n",
    "The goal is to create a model that is able to predict the **journal** in which it will be published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T02:22:34.485651200Z",
     "start_time": "2024-01-07T02:22:34.351082700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "4-CVi1n56Ygf",
    "outputId": "cbf9ad07-1618-4f09-8aeb-9d6988a54697"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bibtexparser\n",
    "import pandas as pd\n",
    "\n",
    "# 1, 3, 4, 5\n",
    "def read_bib_to_dataframe(file_path):\n",
    "    #with open(file_path, 'r', encoding='utf-8') as bibtex_file:\n",
    "    with open(file_path, 'r', encoding='latin-1') as bibtex_file:\n",
    "        return bibtexparser.load(bibtex_file)\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".bib\"):\n",
    "        filename_path = os.path.join(folder, filename)\n",
    "        bib_data = read_bib_to_dataframe(filename_path)\n",
    "        if bib_data.entries:\n",
    "            df = pd.DataFrame(bib_data.entries)\n",
    "            df.to_csv(os.path.splitext(filename_path)[0] + '.csv', index=False)\n",
    "        else:\n",
    "            print(\"Error: \", filename, \" is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T19:02:46.048240400Z",
     "start_time": "2024-01-06T19:01:17.886723400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6768\\3584873582.py:8: DtypeWarning: Columns (2,3,4,5,7,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs.append(pd.read_csv(os.path.join(folder, filename)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>doi</th>\n",
       "      <th>issn</th>\n",
       "      <th>year</th>\n",
       "      <th>pages</th>\n",
       "      <th>volume</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>ID</th>\n",
       "      <th>note</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>journal_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This study proposed to investigate the thermal...</td>\n",
       "      <td>Virtual reality headsets, Thermal comfort, Mic...</td>\n",
       "      <td>Zihao Wang and Renke He and Ke Chen</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>https://doi.org/10.1016/j.apergo.2020.103066</td>\n",
       "      <td>0003-6870</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>103066</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "      <td>Thermal comfort and virtual reality headsets</td>\n",
       "      <td>article</td>\n",
       "      <td>WANG2020103066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A one-pedal system for operating an electric v...</td>\n",
       "      <td>One-pedal operation, Electric vehicle, Emotion...</td>\n",
       "      <td>Fumie Sugimoto and Motohiro Kimura and Yuji Ta...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>https://doi.org/10.1016/j.apergo.2020.103179</td>\n",
       "      <td>0003-6870</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>103179</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "      <td>Effects of one-pedal automobile operation on t...</td>\n",
       "      <td>article</td>\n",
       "      <td>SUGIMOTO2020103179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surgery has changed significantly in recent ye...</td>\n",
       "      <td>Human reliability analysis (HRA), HEART, Dynam...</td>\n",
       "      <td>Rossella Onofrio and Paolo Trucco</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>https://doi.org/10.1016/j.apergo.2020.103150</td>\n",
       "      <td>0003-6870</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>103150</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "      <td>A methodology for Dynamic Human Reliability An...</td>\n",
       "      <td>article</td>\n",
       "      <td>ONOFRIO2020103150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truck platoon driving is a current branch of a...</td>\n",
       "      <td>Truck platoon driving, Technology acceptance, ...</td>\n",
       "      <td>Sarah-Maria Castritius and Heiko Hecht and Joh...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>https://doi.org/10.1016/j.apergo.2019.103042</td>\n",
       "      <td>0003-6870</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>103042</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "      <td>Acceptance of truck platooning by professional...</td>\n",
       "      <td>article</td>\n",
       "      <td>CASTRITIUS2020103042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This paper presents a new handle for instrumen...</td>\n",
       "      <td>Laparoscopic surgery, Handle design, Biomechanics</td>\n",
       "      <td>Ramon Sancibrian and Carlos Redondo-Figuero an...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>https://doi.org/10.1016/j.apergo.2020.103210</td>\n",
       "      <td>0003-6870</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>103210</td>\n",
       "      <td>89.0</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "      <td>Ergonomic evaluation and performance of a new ...</td>\n",
       "      <td>article</td>\n",
       "      <td>SANCIBRIAN2020103210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  This study proposed to investigate the thermal...   \n",
       "1  A one-pedal system for operating an electric v...   \n",
       "2  Surgery has changed significantly in recent ye...   \n",
       "3  Truck platoon driving is a current branch of a...   \n",
       "4  This paper presents a new handle for instrumen...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  Virtual reality headsets, Thermal comfort, Mic...   \n",
       "1  One-pedal operation, Electric vehicle, Emotion...   \n",
       "2  Human reliability analysis (HRA), HEART, Dynam...   \n",
       "3  Truck platoon driving, Technology acceptance, ...   \n",
       "4  Laparoscopic surgery, Handle design, Biomechanics   \n",
       "\n",
       "                                              author  \\\n",
       "0                Zihao Wang and Renke He and Ke Chen   \n",
       "1  Fumie Sugimoto and Motohiro Kimura and Yuji Ta...   \n",
       "2                  Rossella Onofrio and Paolo Trucco   \n",
       "3  Sarah-Maria Castritius and Heiko Hecht and Joh...   \n",
       "4  Ramon Sancibrian and Carlos Redondo-Figuero an...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.sciencedirect.com/science/article/...   \n",
       "1  https://www.sciencedirect.com/science/article/...   \n",
       "2  https://www.sciencedirect.com/science/article/...   \n",
       "3  https://www.sciencedirect.com/science/article/...   \n",
       "4  https://www.sciencedirect.com/science/article/...   \n",
       "\n",
       "                                            doi       issn    year   pages  \\\n",
       "0  https://doi.org/10.1016/j.apergo.2020.103066  0003-6870  2020.0  103066   \n",
       "1  https://doi.org/10.1016/j.apergo.2020.103179  0003-6870  2020.0  103179   \n",
       "2  https://doi.org/10.1016/j.apergo.2020.103150  0003-6870  2020.0  103150   \n",
       "3  https://doi.org/10.1016/j.apergo.2019.103042  0003-6870  2020.0  103042   \n",
       "4  https://doi.org/10.1016/j.apergo.2020.103210  0003-6870  2020.0  103210   \n",
       "\n",
       "   volume             journal  \\\n",
       "0    85.0  Applied Ergonomics   \n",
       "1    88.0  Applied Ergonomics   \n",
       "2    88.0  Applied Ergonomics   \n",
       "3    85.0  Applied Ergonomics   \n",
       "4    89.0  Applied Ergonomics   \n",
       "\n",
       "                                               title ENTRYTYPE  \\\n",
       "0       Thermal comfort and virtual reality headsets   article   \n",
       "1  Effects of one-pedal automobile operation on t...   article   \n",
       "2  A methodology for Dynamic Human Reliability An...   article   \n",
       "3  Acceptance of truck platooning by professional...   article   \n",
       "4  Ergonomic evaluation and performance of a new ...   article   \n",
       "\n",
       "                     ID note combined_text  journal_num  \n",
       "0        WANG2020103066  NaN           NaN          NaN  \n",
       "1    SUGIMOTO2020103179  NaN           NaN          NaN  \n",
       "2     ONOFRIO2020103150  NaN           NaN          NaN  \n",
       "3  CASTRITIUS2020103042  NaN           NaN          NaN  \n",
       "4  SANCIBRIAN2020103210  NaN           NaN          NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "dfs = []\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        dfs.append(pd.read_csv(os.path.join(folder, filename)))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.to_csv(folder + 'all.csv', index=False)\n",
    "\n",
    "for tmp_df in dfs:\n",
    "    tmp_df = None\n",
    "dfs = None\n",
    "\n",
    "gc.collect()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "The following features are selected:\n",
    "* **abstract**: Abstract of the article.\n",
    "* **keywords**: Keywords of the article.\n",
    "* **title**: Title of the article.\n",
    "\n",
    "The target feature is:\n",
    "* **journal**: Journal in which the article was published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T02:33:33.368392800Z",
     "start_time": "2024-01-07T02:33:10.868929400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "id": "I7xjj5RY6V_p",
    "outputId": "fb5f6523-278a-43aa-d340-602cc38d6adf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6768\\2025081068.py:3: DtypeWarning: Columns (0,1,2,3,4,5,7,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(folder + 'all.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(661201, 16)\n",
      "\n",
      " (661201, 4) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 661201 entries, 0 to 661200\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   abstract  444022 non-null  object\n",
      " 1   keywords  443826 non-null  object\n",
      " 2   title     444311 non-null  object\n",
      " 3   journal   444311 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 20.2+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 443820 entries, 0 to 444310\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   abstract  443820 non-null  object\n",
      " 1   keywords  443820 non-null  object\n",
      " 2   title     443820 non-null  object\n",
      " 3   journal   443820 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 16.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This study proposed to investigate the thermal...</td>\n",
       "      <td>Virtual reality headsets, Thermal comfort, Mic...</td>\n",
       "      <td>Thermal comfort and virtual reality headsets</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A one-pedal system for operating an electric v...</td>\n",
       "      <td>One-pedal operation, Electric vehicle, Emotion...</td>\n",
       "      <td>Effects of one-pedal automobile operation on t...</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surgery has changed significantly in recent ye...</td>\n",
       "      <td>Human reliability analysis (HRA), HEART, Dynam...</td>\n",
       "      <td>A methodology for Dynamic Human Reliability An...</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truck platoon driving is a current branch of a...</td>\n",
       "      <td>Truck platoon driving, Technology acceptance, ...</td>\n",
       "      <td>Acceptance of truck platooning by professional...</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This paper presents a new handle for instrumen...</td>\n",
       "      <td>Laparoscopic surgery, Handle design, Biomechanics</td>\n",
       "      <td>Ergonomic evaluation and performance of a new ...</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  This study proposed to investigate the thermal...   \n",
       "1  A one-pedal system for operating an electric v...   \n",
       "2  Surgery has changed significantly in recent ye...   \n",
       "3  Truck platoon driving is a current branch of a...   \n",
       "4  This paper presents a new handle for instrumen...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  Virtual reality headsets, Thermal comfort, Mic...   \n",
       "1  One-pedal operation, Electric vehicle, Emotion...   \n",
       "2  Human reliability analysis (HRA), HEART, Dynam...   \n",
       "3  Truck platoon driving, Technology acceptance, ...   \n",
       "4  Laparoscopic surgery, Handle design, Biomechanics   \n",
       "\n",
       "                                               title             journal  \n",
       "0       Thermal comfort and virtual reality headsets  Applied Ergonomics  \n",
       "1  Effects of one-pedal automobile operation on t...  Applied Ergonomics  \n",
       "2  A methodology for Dynamic Human Reliability An...  Applied Ergonomics  \n",
       "3  Acceptance of truck platooning by professional...  Applied Ergonomics  \n",
       "4  Ergonomic evaluation and performance of a new ...  Applied Ergonomics  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing unnecessary columns\n",
    "import pandas as pd\n",
    "df = pd.read_csv(folder + 'all.csv')\n",
    "\n",
    "feature_names = ['abstract', 'keywords', 'title']\n",
    "target_name = 'journal'\n",
    "\n",
    "print(df.shape)\n",
    "df = df[feature_names + [target_name]]\n",
    "print('\\n',df.shape,'\\n')\n",
    "df.info()\n",
    "\n",
    "df = df.dropna()\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T03:28:15.755367400Z",
     "start_time": "2024-01-07T02:33:33.337112800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "aG1BuPv2oREW",
    "outputId": "9de22848-a359-42d9-836c-9c32d7fbcc97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning data\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "language = 'english'\n",
    "# Convert to lowercase\n",
    "df[feature_names] = df[feature_names].applymap(lambda x: str(x).lower())\n",
    "# Remove stopwords\n",
    "stopwords_list = stopwords.words(language)\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(lambda words: ' '.join([w for w in words.split() if w not in stopwords_list])))\n",
    "# Remove punctuation\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.str.translate(str.maketrans('', '', string.punctuation)))\n",
    "# Stemming\n",
    "stemmer = nltk.stem.SnowballStemmer(language=language)\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(lambda words: ' '.join([stemmer.stem(w) for w in words.split()])))\n",
    "# Tokenize\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(nltk.word_tokenize))\n",
    "\n",
    "#df[feature_names] = df[feature_names].apply(lambda x: x.apply(lambda words: ' '.join(words)))\n",
    "feature_name = 'combined_text'\n",
    "df[feature_name] = df[feature_names[0]]\n",
    "for i in range(1, len(feature_names)):\n",
    "    df[feature_name] = df[feature_name] + df[feature_names[i]]\n",
    "\n",
    "#add journal_num column with numeric integer to process\n",
    "labels = df[target_name].unique()\n",
    "labels_value = list(labels)\n",
    "# Ordina la lista in ordine alfabetico e assegna i rank\n",
    "ranks_ordinati = [rank for rank, _ in sorted(enumerate(labels_value, start=0), key=lambda x: x[1])] # Estrai solo i rank ordinati\n",
    "target_name_num = target_name + '_num'\n",
    "df[target_name_num] = df[target_name].map(dict(zip(labels_value, ranks_ordinati)))\n",
    "\n",
    "df.head()\n",
    "df.to_csv(folder + 'all_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:39:11.826732600Z",
     "start_time": "2024-01-07T07:38:16.756126500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0MYiOGlOFYA",
    "outputId": "91079fdd-6a05-44f3-aeff-5e5876be6e08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "folder = \"datasets/\"\n",
    "feature_name = 'combined_text'\n",
    "target_name = 'journal_num'\n",
    "\n",
    "df = pd.read_csv(folder + 'all_cleaned.csv')\n",
    "#create new df with df[feature_name] and df[target_name_num]\n",
    "#Selecting only feature_name and target_name_num\n",
    "df = df[[feature_name, target_name_num]]\n",
    "\n",
    "df.to_csv(folder + 'all_cleaned_' + feature_name + '.csv', index=False)\n",
    "df_train, df_test = train_test_split(df, train_size=0.8)\n",
    "df = None\n",
    "\n",
    "#Clean memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction**\n",
    "\n",
    "* **Objective** = Convert unstructured text into a numerical structure.\n",
    "* **Result** = Feature vector valid for use by modeling algorithms (classification, clustering, etc.).\n",
    "* The most commonly used approach is the concept of **\"Bag of Words\"** or **BoW**.\n",
    "  – The sequence in which words appear and their positions in the document are not taken into account.\n",
    "\n",
    "Create a **Bag of Words** using the **CountVectorizer** class from **scikit-learn**.\n",
    "\n",
    "**Training** and **Transformation**: Apply `fit_transform()` to the collection of documents to train the vectorizer and transform the text into a term-document matrix. The result `X` is a sparse matrix in CSR (Compressed Sparse Row) format.\n",
    "\n",
    "**Term-Document Matrix**: The term-document matrix can be visualized using `X.toarray()`, which converts it into a two-dimensional NumPy array based on word **frequency**.\n",
    "\n",
    "**Vocabulary**: Obtain the vocabulary (the sorted list of words) using `vectorizer.get_feature_names_out()`.\n",
    "\n",
    "[CountVectorizer Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:42:52.393063200Z",
     "start_time": "2024-01-07T07:39:11.826732600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXJsRwKzTRSl",
    "outputId": "db4cdc2a-b806-4370-85a3-5f465c0f18da"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.65 GiB for an array with shape (355056, 1000) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m bow_Count \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      4\u001b[0m bow_Tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m X_train_Count \u001b[38;5;241m=\u001b[39m bow_Count\u001b[38;5;241m.\u001b[39mfit_transform(df_train[feature_name])\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      7\u001b[0m X_train_Tfidf \u001b[38;5;241m=\u001b[39m bow_Count\u001b[38;5;241m.\u001b[39mfit_transform(df_train[feature_name])\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      8\u001b[0m y_train \u001b[38;5;241m=\u001b[39m df_train[target_name_num]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.65 GiB for an array with shape (355056, 1000) and data type int64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "bow_Count = CountVectorizer(max_features=1000)\n",
    "bow_Tfidf = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "X_train_Count = bow_Count.fit_transform(df_train[feature_name]).toarray()\n",
    "X_train_Tfidf = bow_Count.fit_transform(df_train[feature_name]).toarray()\n",
    "y_train = df_train[target_name_num]\n",
    "\n",
    "X_test_Count = bow_Count.transform(df_test[feature_name]).toarray()\n",
    "X_test_Tfidf = bow_Count.transform(df_test[feature_name]).toarray()\n",
    "y_test = df_test[target_name_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjbf5z8aiEUt"
   },
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T08:30:07.018998800Z",
     "start_time": "2024-01-07T07:42:52.408697700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "URkwEQaGQJCO",
    "outputId": "8b14e75e-c974-4b74-86a2-f019e61c324c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def plot_class_distribution(y):\n",
    "    plt.hist(y)\n",
    "    plt.title(\"Number of instances per class\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Number of instances\")\n",
    "    plt.xticks(ranks_ordinati, labels_value)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def performance(y_test, y_pred):\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='.5%', cmap='Blues', yticklabels=labels, xticklabels=labels, cbar=False)\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(y_train)\n",
    "\n",
    "# BoW ConuntVectorizer\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_Count, y_train)\n",
    "y_pred = cls.predict(X_test_Count)\n",
    "\n",
    "performance(y_test, y_pred)\n",
    "\n",
    "# BoW TfidfVectorizer\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_Tfidf, y_train)\n",
    "y_pred = cls.predict(X_test_Tfidf)\n",
    "\n",
    "performance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T09:11:25.429026500Z",
     "start_time": "2024-01-07T08:30:07.034635400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "id": "7Ccq6Hb2oTEl",
    "outputId": "8b5e1d41-3f88-4502-a862-bbad3961cfc7"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import gc\n",
    "\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# BoW ConuntVectorizer\n",
    "X_train_under_sampled, y_train_under_sampled = sampler.fit_resample(X_train_Count, y_train)\n",
    "plot_class_distribution(y_train_under_sampled)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_Count, y_train)\n",
    "y_pred = cls.predict(X_test_Count)\n",
    "\n",
    "performance(y_test, y_pred)\n",
    "\n",
    "# BoW TfidfVectorizer\n",
    "X_train_under_sampled, y_train_under_sampled = sampler.fit_resample(X_train_Tfidf, y_train)\n",
    "plot_class_distribution(y_train_under_sampled)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_Tfidf, y_train)\n",
    "y_pred = cls.predict(X_test_Tfidf)\n",
    "\n",
    "performance(y_test, y_pred)\n",
    "\n",
    "#Clean memory\n",
    "X_train_under_sampled = None\n",
    "y_train_under_sampled = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T14:07:44.679622200Z",
     "start_time": "2024-01-07T14:07:39.693028600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "jj9_bUbx2fAx",
    "outputId": "e152f131-6ca8-43d3-dcb5-2031a46e516f"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import gc\n",
    "\n",
    "sampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# BoW ConuntVectorizer\n",
    "X_train_over_sampled, y_train_over_sampled = sampler.fit_resample(X_train_Count, y_train)\n",
    "plot_class_distribution(y_train_over_sampled)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_over_sampled, y_train_over_sampled)\n",
    "y_pred = cls.predict(X_test_Count)\n",
    "\n",
    "performance(y_test, y_pred)\n",
    "\n",
    "# BoW TfidfVectorizer\n",
    "X_train_over_sampled, y_train_over_sampled = sampler.fit_resample(X_train_Tfidf, y_train)\n",
    "plot_class_distribution(y_train_over_sampled)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_over_sampled, y_train_over_sampled)\n",
    "y_pred = cls.predict(X_test_Tfidf)\n",
    "\n",
    "performance(y_test, y_pred)\n",
    "\n",
    "X_train_over_sampled = None\n",
    "y_train_over_sampled = None\n",
    "sampler = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xk2zCYd5cmK4"
   },
   "source": [
    "\n",
    "## Connectionist techniques\n",
    "\n",
    "In this case, after pre-processing, a neural network based on an LSTM unit is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T09:11:28.847104600Z",
     "start_time": "2024-01-07T09:11:28.784559400Z"
    },
    "id": "bjKZ7Y-g9tdH"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "device = tf.config.list_physical_devices('GPU')\n",
    "print('Num. tarjetas: ', len(device))\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-07T09:11:28.800193200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hA8DkOATcsTG",
    "outputId": "af35cf0b-0c14-4c6c-dbd9-5bcec5cd4578"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(folder + 'all_cleaned_' + feature_name + '.csv')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[feature_name], df[target_name_num], test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-07T09:11:28.800193200Z"
    },
    "id": "A5p_CIfi9MVx"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_words = 1000\n",
    "sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words = num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_pad = sequence.pad_sequences(X_train_seq, maxlen=sequence_length)\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_pad= sequence.pad_sequences(X_test_seq, maxlen=sequence_length)\n",
    "\n",
    "\n",
    "num_classes = len(labels)\n",
    "# Convert your integer labels to one-hot encoded format\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with input_dim=num_words, output_dim=embedding_dim, input_length=sequence_length\n",
    "embedding_dim = 50  # You can adjust this value based on your specific task\n",
    "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=sequence_length))\n",
    "\n",
    "# Add an LSTM layer with a certain number of units (you can experiment with different values)\n",
    "lstm_units = 100\n",
    "model.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model.fit(X_train_pad, y_train_one_hot, batch_size=32, epochs=5, validation_data=(X_test_pad, y_test_one_hot), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-07T09:11:28.815818600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-06T20:16:46.919983200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-06T20:16:46.919983200Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
