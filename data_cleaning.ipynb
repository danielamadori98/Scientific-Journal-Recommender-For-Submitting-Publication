{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMdvYrtPfZqF"
   },
   "source": [
    "# **Scientific journal recommender for submitting a publication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NAniabkkfix9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fac001d6-2994-46e2-dbaa-96f9be1f6338",
    "ExecuteTime": {
     "end_time": "2024-02-15T16:16:39.988055700Z",
     "start_time": "2024-02-15T16:16:39.964487900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\ncolab_folder = 'drive/MyDrive/Colab Notebooks/ScientificJournalRecommenderForSubmittingPublication/'\\nfolder = colab_folder + folder\\nfolder_raw = colab_folder + folder_raw\\n\""
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "folder = \"dataset/\"\n",
    "folder_raw = \"dataset_raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9Ud8vp4Ozz8"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "For each class (journal) there is a file in BibTeX format containing the articles published in that journal. Each file was cleaned and formatted with the following online tool [BibTeX Tidy](https://flamingtempura.github.io/bibtex-tidy/index.html).\n",
    "\n",
    "Each article is represented by a record with the following fields:\n",
    "* **abstract**: Abstract of the article.\n",
    "* **author**: Author of the article.\n",
    "* **ENTRYTYPE**: Type of entry (article, book, inproceedings, etc.).\n",
    "* **doi**: Digital Object Identifier of the article.\n",
    "* **ID**: Unique identifier of the article.\n",
    "* **issn**: International Standard Serial Number of the journal in which the article was published.\n",
    "* **journal**: Journal in which the article was published.\n",
    "* **keywords**: Keywords of the article.\n",
    "* **note**: Additional information about the article.\n",
    "* **pages**: Pages of the article.\n",
    "* **title**: Title of the article.\n",
    "* **url**: URL of the article.\n",
    "* **volume**: Volume of the journal in which the article was published.\n",
    "* **year**: Year of publication of the article.\n",
    "\n",
    "The goal is to create a model that is able to predict the **journal** in which it will be published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4-CVi1n56Ygf",
    "ExecuteTime": {
     "end_time": "2024-02-15T16:21:38.111000800Z",
     "start_time": "2024-02-15T16:16:39.985866Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bibtexparser\n",
    "import pandas as pd\n",
    "\n",
    "def read_bib_to_dataframe(file_path):\n",
    "    #with open(file_path, 'r', encoding='utf-8') as bibtex_file:\n",
    "    with open(file_path, 'r', encoding='latin-1') as bibtex_file:\n",
    "        return bibtexparser.load(bibtex_file)\n",
    "\n",
    "for filename in os.listdir(folder_raw):\n",
    "    if filename.endswith(\".bib\"):\n",
    "        filename_path = os.path.join(folder_raw, filename)\n",
    "        bib_data = read_bib_to_dataframe(filename_path)\n",
    "        if bib_data.entries:\n",
    "            df = pd.DataFrame(bib_data.entries)\n",
    "            df.to_csv(os.path.splitext(filename_path)[0] + '.csv', index=False)\n",
    "        else:\n",
    "            print(\"Error: \", filename, \" is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-T9-8l-Oz0H",
    "outputId": "e76da136-7339-4cc5-9e11-3d285c941406",
    "ExecuteTime": {
     "end_time": "2024-02-15T16:21:41.862917900Z",
     "start_time": "2024-02-15T16:21:38.134566400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found using ID: 73\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11194 entries, 0 to 11266\n",
      "Data columns (total 14 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   abstract   11094 non-null  object\n",
      " 1   keywords   11021 non-null  object\n",
      " 2   author     11117 non-null  object\n",
      " 3   url        11194 non-null  object\n",
      " 4   doi        11194 non-null  object\n",
      " 5   issn       11194 non-null  object\n",
      " 6   year       11194 non-null  int64 \n",
      " 7   pages      11194 non-null  object\n",
      " 8   volume     11194 non-null  object\n",
      " 9   journal    11194 non-null  object\n",
      " 10  title      11194 non-null  object\n",
      " 11  ENTRYTYPE  11194 non-null  object\n",
      " 12  ID         11194 non-null  object\n",
      " 13  note       47 non-null     object\n",
      "dtypes: int64(1), object(13)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dfs = []\n",
    "for filename in os.listdir(folder_raw):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        dfs.append(pd.read_csv(os.path.join(folder_raw, filename)))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Use the following id features to remove duplicates\n",
    "for feature in ['doi', 'ID']:\n",
    "    # Remove duplicates based on the subset of non-null, non-na, and non-empty values in the feature\n",
    "    tmp_df = df[df[feature].notnull() & df[feature].notna() & (df[feature] != '')]\n",
    "    duplicates_count = tmp_df.duplicated(subset=[feature]).sum()\n",
    "\n",
    "    if duplicates_count > 0:\n",
    "        print(f'Duplicates found using {feature}: {duplicates_count}\\n\\n')\n",
    "\n",
    "        df = df[~df[feature].duplicated(keep='first') | df[feature].isnull() | df[feature].isna() | (df[feature] == '')]\n",
    "\n",
    "print(df.info())\n",
    "df.head()\n",
    "df.to_csv(folder_raw + 'all.csv', index=False)\n",
    "\n",
    "for tmp_df in dfs:\n",
    "    tmp_df = None\n",
    "dfs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBAefxsfOz0P"
   },
   "source": [
    "# Feature Selection\n",
    "\n",
    "The following features are selected:\n",
    "* **abstract**: Abstract of the article.\n",
    "* **keywords**: Keywords of the article.\n",
    "* **title**: Title of the article.\n",
    "\n",
    "The target feature is:\n",
    "* **journal**: Journal in which the article was published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7xjj5RY6V_p",
    "outputId": "175ba69b-39e9-4f28-c2d2-f0133034fb44",
    "ExecuteTime": {
     "end_time": "2024-02-15T16:21:44.930718300Z",
     "start_time": "2024-02-15T16:21:41.879313700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11194 entries, 0 to 11193\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   abstract  11094 non-null  object\n",
      " 1   keywords  11021 non-null  object\n",
      " 2   title     11194 non-null  object\n",
      " 3   journal   11194 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 349.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Removing unnecessary columns\n",
    "import pandas as pd\n",
    "df = pd.read_csv(folder_raw + 'all.csv')\n",
    "\n",
    "feature_names = ['abstract', 'keywords', 'title']\n",
    "target_name = 'journal'\n",
    "\n",
    "# Remove all the row that countains null values in the target_name column\n",
    "df = df.dropna(subset=[target_name])\n",
    "\n",
    "df = df[feature_names + [target_name]]\n",
    "print(df.info())\n",
    "df.head()\n",
    "\n",
    "df.to_csv(folder_raw + 'selected.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aG1BuPv2oREW",
    "outputId": "fa96ee05-b050-4ed8-c667-61a58256e9fd",
    "ExecuteTime": {
     "end_time": "2024-02-15T16:24:21.253403600Z",
     "start_time": "2024-02-15T16:21:44.958353500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "language = 'english'\n",
    "# Convert to lowercase\n",
    "df[feature_names] = df[feature_names].applymap(lambda x: str(x).lower())\n",
    "# Remove stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words(language)\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(lambda words: ' '.join([w for w in words.split() if w not in stopwords_list])))\n",
    "# Remove punctuation\n",
    "nltk.download('punkt')\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.str.translate(str.maketrans('', '', string.punctuation)))\n",
    "# Stemming\n",
    "stemmer = nltk.stem.SnowballStemmer(language=language)\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(lambda words: ' '.join([stemmer.stem(w) for w in words.split()])))\n",
    "# Tokenize\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(nltk.word_tokenize))\n",
    "\n",
    "df.head()\n",
    "df.to_csv(folder + 'selected_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
