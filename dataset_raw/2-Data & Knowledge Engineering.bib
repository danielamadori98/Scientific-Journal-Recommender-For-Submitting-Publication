@article{HASHEM2020101833,
	title        = {PRESS: A personalised approach for mining top-k groups of objects with subspace similarity},
	journal      = {Data & Knowledge Engineering},
	volume       = {128},
	pages        = {101833},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101833},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19301430},
	author       = {Tahrima Hashem and Lida Rashidi and Lars Kulik and James Bailey},
	keywords     = {Subspace mining, Similarity search, Association rules, Mining methods and algorithms, Personalisation},
	abstract     = {Personalised analytics is a powerful technology that can be used to improve the career, lifestyle, and health of individuals by providing them with an in-depth analysis of their characteristics as compared to other people. Existing research has often focused on mining general patterns or clusters, but without the facility for customisation to an individual?s needs. It is challenging to adapt such approaches to the personalised case, due to the high computational overhead they require for discovering patterns that are good across an entire dataset, rather than with respect to an individual. In this paper, we tackle the challenge of personalised pattern mining and propose a query-driven approach to mine objects with subspace similarity. Given a query object in a categorical dataset, our proposed algorithm, PRESS (Personalised Subspace Similarity), determines the top-k groups of objects, where each group has high similarity to the query for some particular subspace. We evaluate the efficiency and effectiveness of our approach on both synthetic and real datasets.}
}
@article{LV2021101912,
	title        = {Deep learning in the COVID-19 epidemic: A deep model for urban traffic revitalization index},
	journal      = {Data & Knowledge Engineering},
	volume       = {135},
	pages        = {101912},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101912},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000392},
	author       = {Zhiqiang Lv and Jianbo Li and Chuanhao Dong and Haoran Li and Zhihao Xu},
	keywords     = {COVID-19, Traffic revitalization index, Data mining, Data models, Mining methods and algorithms},
	abstract     = {The research of traffic revitalization index can provide support for the formulation and adjustment of policies related to urban management, epidemic prevention and resumption of work and production. This paper proposes a deep model for the prediction of urban Traffic Revitalization Index (DeepTRI). The DeepTRI builds model for the data of COVID-19 epidemic and traffic revitalization index for major cities in China. The location information of 29 cities forms the topological structure of graph. The Spatial Convolution Layer proposed in this paper captures the spatial correlation features of the graph structure. The special Graph Data Fusion module distributes and fuses the two kinds of data according to different proportions to increase the trend of spatial correlation of the data. In order to reduce the complexity of the computational process, the Temporal Convolution Layer replaces the gated recursive mechanism of the traditional recurrent neural network with a multi-level residual structure. It uses the dilated convolution whose dilation factor changes according to convex function to control the dynamic change of the receptive field and uses causal convolution to fully mine the historical information of the data to optimize the ability of long-term prediction. The comparative experiments among DeepTRI and three baselines (traditional recurrent neural network, ordinary spatial?temporal model and graph spatial?temporal model) show the advantages of DeepTRI in the evaluation index and resolving two under-fitting problems (under-fitting of edge values and under-fitting of local peaks).}
}
@article{GHERBAOUI2021101876,
	title        = {Generation of Gaussian sets for clustering methods assessment},
	journal      = {Data & Knowledge Engineering},
	volume       = {131-132},
	pages        = {101876},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101876},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000033},
	author       = {Radhwane Gherbaoui and Mohammed Ouali and Nacéra Benamrane},
	keywords     = {Mixture distributions, Clustering, Clusters overlap control, High-dimensional data, Gaussian models},
	abstract     = {Clustering methods are generally used to study the homogeneity in a set of observations. The results obtained from the clustering process differ from one method to another, to the extent that the same method or validity index gives different outcomes depending on the initial parameters. Analytical evaluation appears to be insufficient for studying the behavior of clustering methods due to its ad hoc nature. Even if the real data set is used in evaluating clustering methods, artificial data is fundamental for assessing the performance since it allows creating different scenarios of test with known structures. The main drawback of existing methods of artificial data is that they do not take into consideration the problem of sensitivity to the size of clusters. In this paper, we propose an automatic method: the high-dimensional artificial Gaussian mixture generator. By formally quantifying the overlap, the generator preserves the notion of the overlap rate between the mixture components. The advantages of this generator are its use of the notion of overlap rate, the unlimited number of mixture components, high-dimensionality of the observations, and the non-utilization of visual inspection as a criterion to quantify the overlap. In addition, we evaluate the k-means, fuzzy c-means (FCM), FCM-based splitting algorithm (FBSA), and expectation maximization (EM) in different dimensions. The results obtained confirm previous work and reveal new findings that are not pointed out when using 1D and 2D artificial data.11The source code of the implementation is publicly available at the following URL: http://193.194.88.10/bitstream/123456789/392/1/cgeg.zip.}
}
@article{HIDOURI2021101927,
	title        = {Mining Closed High Utility Itemsets based on Propositional Satisfiability},
	journal      = {Data & Knowledge Engineering},
	volume       = {136},
	pages        = {101927},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101927},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000549},
	author       = {Amel Hidouri and Said Jabbour and Badran Raddaoui and Boutheina Ben Yaghlane},
	keywords     = {Data Mining, High Utility, Symbolic Artificial Intelligence, Propositional Satisfiability},
	abstract     = {A high utility itemset mining problem is the question of recognizing a set of items that have utility values greater than a given user utility threshold. This generalization of the classical problem of frequent itemset mining is a useful and well-known task in data analysis and data mining, since it is used in a wide range of real applications. In this paper, we first propose to use symbolic Artificial Intelligence for computing the set of all closed high utility itemsets from transaction databases. Our approach is based on reduction to enumeration problems of propositional satisfiability. Then, we enhance the efficiency of our SAT-based approach using the weighted clique cover problem. After that, in order to improve scalability, a decomposition technique is applied to derive smaller and independent sub-problems in order to capture all the closed high utility itemsets. Clearly, our SAT-based encoding can be constantly enhanced by integrating the last improvements in powerful SAT solvers and models enumeration algorithms. Finally, through empirical evaluations on different real-world datasets, we demonstrate that the proposed approach is very competitive with state-of-the-art specialized algorithms for high utility itemsets mining, while being sufficiently flexible to take into account additional constraints to finding closed high utility itemsets.}
}
@article{AHMED2021101874,
	title        = {Multilingual Verbalization and Summarization for Explainable Link Discovery},
	journal      = {Data & Knowledge Engineering},
	volume       = {133},
	pages        = {101874},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101874},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X2100001X},
	author       = {Abdullah Fathi Ahmed and Mohamed Ahmed Sherif and Diego Moussallem and Axel-Cyrille {Ngonga Ngomo}},
	keywords     = {Link discovery, Verbalization, Link specification, NLP, NLG, Text summarization},
	abstract     = {The number and size of datasets abiding by the Linked Data paradigm increase every day. Discovering links between these datasets is thus central to achieving the vision behind the Data Web. Declarative Link Discovery (LD) frameworks rely on complex Link Specification (LS) to express the conditions under which two resources should be linked. Understanding such LS is not a trivial task for non-expert users. Particularly when such users are interested in generating LS to match their needs. Even if the user applies a machine learning algorithm for the automatic generation of the required LS, the challenge of explaining the resultant LS persists. Hence, providing explainable LS is the key challenge to enable users who are unfamiliar with underlying LS technologies to use them effectively and efficiently. In this paper, we extend our previous work (Ahmed et al., 2019) by proposing a generic multilingual approach that allows verbalization of LS in many languages, i.e., converts LS into understandable natural language text. In this work, we ported our LS verbalization framework into German and Spanish, in addition to English language. Our adequacy and fluency evaluations show that our approach can generate complete and easily understandable natural language descriptions even by lay users. Moreover, we devised an experimental neural approach for improving the quality of our generated texts. Our neural approach achieves promising results in terms of BLEU, METEOR and chrF++.}
}
@article{NASIRTAFRESHI2022102009,
	title        = {Forecasting cryptocurrency prices using Recurrent Neural Network and Long Short-term Memory},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {102009},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102009},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000234},
	author       = {I. Nasirtafreshi},
	keywords     = {Cryptocurrency, Recurrent Neural Network, Long Short-term Memory, Deep learning, Forecasting prices, Time series data},
	abstract     = {The rapid development of cryptocurrencies over the past decade is one of the most controversial and ambiguous innovations in the modern global economy. Numerous and unpredictable fluctuations in cryptocurrencies rates, as well as the lack of intelligent and proper management of transactions of this type of currency in most developing countries and users of this type of currency, has led to increased risk and distrust of these roses in investors. Capitalists and investors prefer to invest in programs which have the least risk, the most profit and the least time to achieve the main profit. Therefore, the issue of developing appropriate methods and models for predicting the price of cryptographic products is essential both for the scientific community and for financial analysts, investors and traders. In this research, a new deep learning model is used to predict the price of cryptocurrencies. The proposed model uses a Recurrent Neural Networks (RNN) algorithm based on Long Short-Term Memory (LSTM) method to predict the price. In the presented results of the simulation of the proposed method, factors such as the Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), R-Squared (R2) were compared with other similar methods. Finally, the superiority of the proposed method over other methods was proven.}
}
@article{AYALA2022101943,
	title        = {LEAPME: Learning-based Property Matching with Embeddings},
	journal      = {Data & Knowledge Engineering},
	volume       = {137},
	pages        = {101943},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101943},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000707},
	author       = {Daniel Ayala and Inma Hernández and David Ruiz and Erhard Rahm},
	keywords     = {Data integration, Machine learning, Knowledge graphs},
	abstract     = {Data integration tasks such as the creation and extension of knowledge graphs involve the fusion of heterogeneous entities from many sources. Matching and fusion of such entities require to also match and combine their properties (attributes). However, previous schema matching approaches mostly focus on two sources only and often rely on simple similarity measurements. They thus face problems in challenging use cases such as the integration of heterogeneous product entities from many sources. We therefore present a new machine learning-based property matching approach called LEAPME (LEArning-based Property Matching with Embeddings) that utilizes numerous features of both property names and instance values. The approach heavily makes use of word embeddings to better utilize the domain-specific semantics of both property names and instance values. The use of supervised machine learning helps exploit the predictive power of word embeddings. Our comparative evaluation against five baselines for several multi-source datasets with real-world data shows the high effectiveness of LEAPME. We also show that our approach is even effective when training data from another domain (transfer learning) is used.}
}
@article{KUMAR2022102050,
	title        = {A fuzzy clustering technique for enhancing the convergence performance by using improved Fuzzy c-means and Particle Swarm Optimization algorithms},
	journal      = {Data & Knowledge Engineering},
	volume       = {140},
	pages        = {102050},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102050},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000490},
	author       = {Niteesh Kumar and Harendra Kumar},
	keywords     = {Clustering algorithm, Clusters, Fuzzy c-means, Objective function values, Particle swarm optimization},
	abstract     = {Fuzzy clustering is a well-established technique among the well-known clustering techniques in several real-world applications due to easy implementation and produces satisfactory clustering result. However, it has some deficiency such as sensitive to outliers, result dependency on choosing initial centroid, etc. To eradicate the shortcoming of FCM algorithm, this article introduces a robust clustering technique, particle swarm optimization improved fuzzy c-means is developed by the hybridization of particle swarm optimization and improved fuzzy c-means techniques, to deal with noisy data and initialization problem. In this article, a fuzzy clustering technique is developed to increase the convergence performance of clustering techniques. Fuzzy c-means is improved by developing a new metric to tolerate the noisy environment. Particle swarm optimization has an inbuilt guidance strategy which leads the solution in particle swarm optimization to obtain useful information from the better solution and thereby helping them improve their own solution. To handle the initialization problem of fuzzy c-means, particle swarm optimization technique is used. PSO effectively enhance the performance of improved FCM to increase the effectiveness of clustering. The effectiveness of the proposed clustering technique over existing techniques in literature has been illustrated by adopting eight real worlds and three artificial data sets. The results show that the proposed algorithm generates encouraging results as compared to the established clustering technique in literature.}
}
@article{SAURABH2020101852,
	title        = {An analytical model for information gathering and propagation in social networks using random graphs},
	journal      = {Data & Knowledge Engineering},
	volume       = {129},
	pages        = {101852},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101852},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19303441},
	author       = {Samant Saurabh and Sanjay Madria and Anirban Mondal and Ashok Singh Sairam and Saurabh Mishra},
	keywords     = {Social networks, Data models, Information gathering, Data sharing, Business intelligence, Node discovery in graphs},
	abstract     = {In this paper, we propose an analytical model for information gathering and propagation in social networks using random sampling. We represent the social network using the Erdos?Renyi model of the random graph. When a given node is selected in the social network, information about itself and all of its neighbors are obtained and these nodes are considered to be discovered. We provide an analytical solution for the expected number of nodes that are discovered as a function of the number of nodes randomly sampled in the graph. We use the concepts of combinatorics, probability, and inclusion?exclusion principle for computing the number of discovered nodes. This is a computationally-intensive problem with combinatorial complexity. This model is useful when crawling and mining of the social network graph is prohibited. Our work finds application in several important real-world decision support scenarios such as survey sample selection, construction of public directory, and crowdsourced databases using social networks, targeted advertising, and recommendation systems. It can also be used for finding a randomized dominating set of a graph that finds applications in computer networks, document summarization, and biological networks. We have evaluated the performance both analytically as well as by means of simulation, and the results are comparable. The results have an accuracy of around 96% for random graphs and above 87% for the power-law graphs.}
}
@article{BRENON2022102098,
	title        = {Classifying encyclopedia articles: Comparing machine and deep learning methods and exploring their predictions},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102098},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102098},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000891},
	author       = {Alice Brenon and Ludovic Moncla and Katherine McDonough},
	keywords     = {Classification, Supervised machine learning, Deep learning, Encyclopedias, Computational humanities, Networks},
	abstract     = {This article presents a comparative study of supervised classification approaches applied to the automatic classification of encyclopedia articles written in French. Our dataset includes all  70k text articles from Diderot and d?Alembert?s Encyclopédie (1751-72). In a two-task experiment we test combinations of (1) text vectorization methods (bags-of-words and word embeddings) and (2) traditional Machine Learning and newer Deep Learning classification methods (including transformer architectures). In addition to evaluating each approach, we review the results quantitatively and qualitatively. The best model obtains an average F-score of 86% for 38 classes. Using network analysis, we highlight the difficulty of labeling semantically close classes. We also discuss misclassifications in order to understand the relationship between content and different ways of ordering knowledge. We openly release all code and results, and data is available on request.11https://gitlab.liris.cnrs.fr/geode/EDdA-Classification.}
}
@article{SAKKA2021101875,
	title        = {A profile-aware methodological framework for collaborative multidimensional modeling},
	journal      = {Data & Knowledge Engineering},
	volume       = {131-132},
	pages        = {101875},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101875},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000021},
	author       = {Amir Sakka and Sandro Bimonte and Stefano Rizzi and Lucile Sautot and François Pinet and Michela Bertolotto and Aurélien Besnard and Noura Rouillier},
	keywords     = {Data warehouse design, Collaborative systems, Quality dimensions},
	abstract     = {Multidimensional modeling, i.e., the design of cube schemata, has a key role in data warehouse (DW) projects, in self-service business intelligence, and in general to let users analyze data via the OLAP paradigm. Though an effective involvement of users in multidimensional modeling is crucial in these projects, not much has been said about how to establish a fruitful collaboration in projects involving numerous users with different skills, reputations, and degrees of authority. This issue is especially relevant in citizen science projects, where several volunteers can contribute their requirements despite not being formally-trained experts in the application domain. To fill this gap, we propose a framework for collaborative multidimensional modeling that can adapt itself to the profiles and skills of the actors involved. We first classify users depending on their authoritativeness, skills, and engagement in the project. Then, following this classification, we identify four possible methodological scenarios and propose a profile-aware methodology supported by two sets of quality attributes. Finally, we describe a Group Decision Support System that implements our methodological framework and present some experiments carried out on a real case study.}
}
@article{CHEN2022102089,
	title        = {A workload-driven method for designing aggregate-oriented NoSQL databases},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102089},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102089},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000805},
	author       = {Liu Chen and Ali Davoudian and Mengchi Liu},
	keywords     = {NoSQL database schemas, Database design, Workload-driven, Design trade-offs},
	abstract     = {Due to the scalability and availability problems with traditional relational database systems, a variety of NoSQL stores have emerged over the last decade to deal with big data. How data are structured in a NoSQL store has a large impact on the query and update performance and the storage usage. Thus, different from the traditional database design, not only the data structure but also the data access patterns need to be considered in the design of NoSQL database schemas. In this paper, we present a general workload-driven method for designing key-value, wide-column, and document NoSQL database schemas. We first present a generic logical model Query Path Graph (QPG) that can represent the data structures of the UML class diagram. We also define mappings from the SQL-based query patterns to QPG and from QPG to aggregate-oriented NoSQL schemas. We use a cost model to measure the query and update performance and optimize the QPG schemas. We evaluate the proposed method with several typical case studies by simulating workloads on databases with different schema designs. The results demonstrate that our method preserves the generality and the quality of the design.}
}
@article{TIMAKUM2021101926,
	title        = {Exploring the research landscape of data warehousing and mining based on DaWaK Conference full-text articles},
	journal      = {Data & Knowledge Engineering},
	volume       = {135},
	pages        = {101926},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101926},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000537},
	author       = {Tatsawan Timakum and Soobin Lee and Min Song},
	keywords     = {Content analysis, Data warehousing knowledge mapping, Data warehousing research trends, Co-authorship network analysis},
	abstract     = {The international conference on Data Warehousing and Knowledge Discovery (DaWaK) has become a pivotal place to exchange experiences and knowledge among researchers and practitioners in big data analytics. The conference has been essential to data warehousing and data analytics for the last 21 years (1999?2019). This study explored the knowledge structure embedded in the DaWaK Conference papers and examined the research trends over time. It also analyzed the performance of published papers, authors, and their affiliations and countries and visualized a collaboration network in DaWaK. We applied several text mining techniques, including co-word analysis, topic modeling, co-author network analysis, and network visualization. The study?s findings indicate that the core topics are data mining techniques, algorithm performance, and information systems. The popular topic trends are associated with database encryption, whereas the topics related to online analytical processing (OLAP) technology are in decline. The research metrics results demonstrate that the DaWaK papers were cited 6,262 times, with an h-index of 34 for the 722 DaWaK papers. The article titled ?Outlier Detection Using Replicator Neural Networks? reached the most citations (177), and the most productive author was Bellatreche, Ladjel (15 papers). Nanyang Technological University is the most frequently mentioned as the author?s affiliation, the United States is the country with the largest number of authors, and the National Science Foundation was the largest funding agency that supported the DaWaK researchers. Moreover, the authorship network of Bellatreche, Ladjel is the largest collaboration network in the DaWaK scholar community. The outcomes of this study would be beneficial for comprehending the knowledge in data warehousing and the relevant cross-disciplinary areas of research and collaboration networks in this field.}
}
@article{GEILER2022102100,
	title        = {An effective strategy for churn prediction and customer profiling},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102100},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102100},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X2200091X},
	author       = {Louis Geiler and Séverine Affeldt and Mohamed Nadif},
	keywords     = {Churn prediction, Customer profiling, Machine learning, Ensemble approach, Deep autoencoders},
	abstract     = {Customer churn prediction and profiling are two major economic concerns for many companies. Different learning approaches have been proposed, however the a priori choice of the most suitable model to perform both tasks remains non-trivial as it is highly dependent on the intrinsic characteristics of the churn data. Our study compares eight supervised machine learning methods combined with seven sampling approaches on thirteen public churn data sets. Our evaluations, reported in terms of area under the curve (AUC), explore the influence of rebalancing strategies and data properties on the performance of learning methods. We rely on the Nemenyi test and Correspondence Analysis as means of visualizing the association between models, rebalancing and data. This work identifies the most appropriate methods in an attrition context and proposes an effective pipeline based on an ensemble approach and deep autoencoders segmentation. Our strategy can enlighten marketing or human resources services on the behavioral patterns of customers and their attrition probability. The described experiments are fully reproducible and our proposal can be successfully applied to a wide range of churn-like datasets.}
}
@article{DEKINDEREN2022102052,
	title        = {Model-based valuation of smart grid initiatives: Foundations, open issues, requirements, and a research outlook},
	journal      = {Data & Knowledge Engineering},
	volume       = {141},
	pages        = {102052},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102052},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000520},
	author       = {Sybren {de Kinderen} and Monika Kaczmarek-Heß and Qin Ma and Iván S. Razo-Zapata},
	keywords     = {Valuation, Smart grid, Conceptual modeling},
	abstract     = {To support the value assessment of technically feasible smart grid initiatives there exist several valuation methods. To determine whether those methods address all concerns relevant for smart grid valuation, we carry out a literature analysis aiming at (1) identifying existing valuation methods and the steps they propose, (2) identifying important valuation considerations, and (3) confronting these considerations with artifacts proposed by the existing valuation methods to identify open issues, requirements, and remaining challenges. Based on the conducted analysis we identify, among others, the following main deficiencies: (1) only a limited scope of concerns relevant to valuation is covered, particularly a systematic consideration of stakeholders goals, value exchange scenarios, and the IT infrastructure is lacking; and (2) a lack of instruments dedicated to fostering accessibility of valuation, in terms of establishing a shared understanding, communicating results, or actively involving different stakeholders in the process. Based on the findings, we suggest the application of conceptual modeling as an instrument to address the identified deficiencies. Therefore, we reflect on the role that current modeling approaches can play in smart grid valuation. This paper is a part of a larger project whose ultimate goal is to develop a model-based method for multi-perspective valuation of smart grid initiatives. The purpose of this paper is to establish a foundation for the realization of the envisioned method. The design of the model-based valuation method itself, its application and evaluation, are subjects of future work.}
}
@article{ALHARBI2022101973,
	title        = {Integrating Character-level and Word-level Representation for Affect in Arabic Tweets},
	journal      = {Data & Knowledge Engineering},
	volume       = {138},
	pages        = {101973},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101973},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000938},
	author       = {Abdullah I. Alharbi and Phillip Smith and Mark Lee},
	keywords     = {Word-level embeddings, Character-level embeddings, Arabic tweets, Affect tasks},
	abstract     = {Affect tasks, which range from sentiment polarity classification to finer grained sentiment strength and emotional intensity detection, have become of increasing interest due to the vast amount of user-generated content and advanced learning models. Word representation models have been leveraged effectively within a variety of natural language processing tasks. However, these models are not always effective in the context of social media. When dealing with social media posts in Arabic, the use of Arabic dialects needs to be considered. Although using informal text to train word-level models can lead to the identification of words that convey the same meaning, these models are unable to capture the full extent of the words that are used in the real world due to out-of-vocabulary (OOV) words. The inability to identify such words is one of the main limitations of word-level models. One approach of overcoming OOV is through the use of character-level embeddings as they can effectively learn the vectors of word parts or character n-grams. This study uses a combination of character-level and word-level models to identify the most effective methods by which affective Arabic words in tweets can be represented semantically and morphologically. We evaluate our generated models and the proposed method by integrating them in a supervised learning framework that was used for a range of affect tasks and other related tasks. Our findings reveal that the developed models surpassed the performance of state-of-the-art Arabic pre-trained word embeddings over eight datasets. In addition, our models enhance previous state-of-the-art outcomes on tasks involving Arabic emotion intensity, outperforming the top-systems that used advanced ensemble learning models and several additional features.}
}
@article{LANGONE2020101850,
	title        = {Interpretable Anomaly Prediction: Predicting anomalous behavior in industry 4.0 settings via regularized logistic regression tools},
	journal      = {Data & Knowledge Engineering},
	volume       = {130},
	pages        = {101850},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101850},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1830644X},
	author       = {Rocco Langone and Alfredo Cuzzocrea and Nikolaos Skantzos},
	abstract     = {Prediction of anomalous behavior in industrial assets based on sensor reading represents a key focus in modern business practice. As a matter of fact, forecast of forthcoming faults is crucial to implement predictive maintenance, i.e. maintenance decision making based on real time information from components and systems, which allows, among other benefits, to reduce maintenance cost, minimize downtime, increase safety, enhance product quality and productivity. However, building a model able to predict the future occurrence of a failure is challenging for various reasons. First, data are usually highly imbalanced, meaning that patterns describing a faulty regime are much less numerous than normal behavior instances, which makes model design difficult. Second, model predictions should be not only accurate (to avoid false alarms and missed detections) but also explainable to operators responsible for scheduling maintenance or control actions. In this paper we introduce a method called Interpretable Anomaly Prediction (IAP) allowing to handle these issues by using regularized logistic regression as core prediction model. In particular, in contrast to anomaly detection algorithms which permit to identify if the current data are anomalous or not, the proposed technique is able to predict the probability that future data will be abnormal. Furthermore, feature extraction and selection mechanisms give insights on the possible root causes leading to failures. The proposed strategy is validated with a large imbalanced multivariate time-series dataset consisting of measurements of several process variables surrounding an high pressure plunger pump situated in a complex chemical plant.}
}
@article{BATISTA2022102012,
	title        = {Ontologically correct taxonomies by construction},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {102012},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102012},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000246},
	author       = {Jeferson O. Batista and João Paulo A. Almeida and Eduardo Zambon and Giancarlo Guizzardi},
	keywords     = {Taxonomies, Conceptual modeling, Ontologies, Graph grammars, Correctness by construction},
	abstract     = {Taxonomies play a central role in conceptual domain modeling, having a direct impact in areas such as knowledge representation, ontology engineering, and software engineering, as well as knowledge organization in information sciences. Despite this, there is little guidance on how to build high-quality taxonomies, with notable exceptions being the OntoClean methodology, and the ontology-driven conceptual modeling language OntoUML. These techniques take into account the ontological meta-properties of types to establish well-founded rules on the formation of taxonomic structures. In this paper, we show how to leverage the formal rules underlying these techniques in order to build taxonomies which are correct by construction. We define a set of correctness-preserving operations to systematically introduce types and subtyping relations into taxonomic structures. In addition to considering the ontological micro-theory of endurant types underlying OntoClean and OntoUML, we also employ the MLT (Multi-Level Theory) micro-theory of high-order types, which allows us to address multi-level taxonomies based on the powertype pattern. To validate our proposal, we formalize the model building operations as a graph grammar that incorporates both micro-theories. We apply automatic verification techniques over the grammar language to show that the graph grammar is sound, i.e., that all taxonomies produced by the grammar rules are correct, at least up to a certain size. We also show that the rules can generate all correct taxonomies up to a certain size (a completeness result).}
}
@article{BONIFAZI2022102048,
	title        = {An approach to detect backbones of information diffusers among different communities of a social platform},
	journal      = {Data & Knowledge Engineering},
	volume       = {140},
	pages        = {102048},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102048},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000489},
	author       = {Gianluca Bonifazi and Francesco Cauteruccio and Enrico Corradini and Michele Marchetti and Alberto Pierini and Giorgio Terracina and Domenico Ursino and Luca Virgili},
	keywords     = {Information diffusion, Online social networks, Information diffuser backbones, Disseminator centrality, Reddit},
	abstract     = {Information diffusion in social networks is a classic and, at the same time, very current problem. In fact, information diffusers are always looking for new techniques to disseminate information of their interest by creating backbones among them. In this paper, we focus on a specific, but very current and relevant, scenario regarding this way of proceeding. In fact, we propose an approach for the detection of possible backbones of information diffusers among different communities of a social network. Our approach is based on a new centrality measure that we call disseminator centrality. It is specifically designed to detect the so-called disseminator bridges, i.e., users belonging to multiple communities of a single social network, who want to disseminate information of their interest from one community to another by supporting each other. This paper describes the proposed approach, presents the disseminator centrality, illustrates the differences with respect to the related literature and presents the results of the experiments carried out to evaluate its performance.}
}
@article{GACEM2020101854,
	title        = {Scalable distributed reachability query processing in multi-labeled networks},
	journal      = {Data & Knowledge Engineering},
	volume       = {130},
	pages        = {101854},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101854},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19303234},
	author       = {Amina Gacem and Apostolos N. Papadopoulos and Kamel Boukhalfa},
	keywords     = {Graph mining, Reachability queries, Distributed algorithms},
	abstract     = {Testing reachability in a graph gains substantial interest as an important operation in network analysis and graph mining. In its simplest form, a reachability query is defined by a pair of nodes (u, v) and a graph G, and detects if there is a path from u to v. This paper addresses a specific case of reachability on multi-labeled distributed graphs, where the query is parameterized by a set of source nodes S, a set of target nodes T and a set of constraints C on the edge labels. We conduct a performance evaluation on both synthetic and real-world datasets, using multiple instances of Neo4j servers (as workers) running simultaneously. The results show that the number of workers, the network density and the number of cross-edges have a significant impact on the overall performance. Moreover, we observe that the proposed approach is scalable and can be used to solve label-constrained distributed set reachability queries in multi-labeled networks.}
}
@article{WU2021101890,
	title        = {Discovering closed and maximal embedded patterns from large tree data},
	journal      = {Data & Knowledge Engineering},
	volume       = {133},
	pages        = {101890},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101890},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000173},
	author       = {Xiaoying Wu and Dimitri Theodoratos and Nikos Mamoulis},
	keywords     = {Hierarchical graph data, Frequent pattern mining, Embedded tree pattern, Pattern summarization, Maximal and closed frequent pattern},
	abstract     = {Many current applications and systems produce large tree datasets and export, exchange, and represent data in tree-structured form. Extracting informative patterns from large data trees is an important research direction with multiple applications in practice. Pattern mining research initially focused on mining induced patterns and gradually evolved into mining embedded patterns. A well-known problem of frequent pattern mining is the huge number of patterns it produces. This affects not only the efficiency but also the effectiveness of mining. A typical solution to this problem is to summarize frequent patterns through closed and maximal patterns. No previous work addresses the problem of mining closed and/or maximal embedded tree patterns, not even in the framework of mining multiple small trees. We address the problem of summarizing embedded tree patterns extracted from large data trees, by defining and mining closed and maximal embedded unordered tree patterns. We design an embedded frequent pattern mining algorithm extended with a local closedness checking technique. This algorithm is called closedEmbTM-eager as it eagerly eliminates non-closed patterns. To mitigate the generation of intermediate patterns, we devise pattern search space pruning rules to proactively detect and prune branches in the pattern search space which do not correspond to closed patterns. The pruning rules are accommodated into the extended embedded pattern miner to produce a new algorithm, called closedEmbTM-prune, for mining all the closed and maximal embedded frequent patterns. Our extensive experiments on synthetic and real large-tree datasets demonstrate that, on dense datasets, closedEmbTM-prune not only generates a complete closed and maximal pattern set which is substantially smaller than that generated by the embedded pattern miner, but also runs much faster with negligible overhead on pattern pruning.}
}
@article{ALAMIN2021101930,
	title        = {Efficient machine learning on data science languages with parallel data summarization},
	journal      = {Data & Knowledge Engineering},
	volume       = {136},
	pages        = {101930},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101930},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000574},
	author       = {Sikder Tahsin Al-Amin and Carlos Ordonez},
	keywords     = {Machine learning, Parallel computation, Statistics, Data summarization},
	abstract     = {Nowadays, data science analysts prefer ?easy? high-level languages for machine learning computation like R and Python, but they present memory and speed limitations. Also, scalability is another issue when the data set size grows. On the other hand, acceleration of machine learning algorithms can be achieved with data summarization which has been a fundamental technique in data mining. With these motivations in mind, we present an efficient way to compute the statistical and machine learning models with parallel data summarization that can work with popular data science languages. Our summarization produces one or multiple summaries, accelerates a broader class of statistical and machine learning models, and requires a small amount of RAM. We present an algorithm that works in three phases and is capable to handle data sets bigger than the main memory. Our solution evaluates a vector?vector outer product with C++ code to escape the bottleneck of the high-level programming languages. We present an experimental evaluation section with a prototype in the R language where the summarization is programmed in C++. Our experiments prove that our solution can work on both data subsets and full data set without any performance penalty. Also, we compare our solution (R combined with C++) with other parallel big data systems: Spark (Spark-MLlib library), and a parallel DBMS (similar approach implemented with UDFs and SQL queries). We show our solution is simpler and mostly faster than Spark based on the storage of the data set, and it is much faster than a parallel DBMS regardless of the storage of the data set.}
}
@article{WEIGAND2021101878,
	title        = {An artifact ontology for design science research},
	journal      = {Data & Knowledge Engineering},
	volume       = {133},
	pages        = {101878},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101878},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000057},
	author       = {Hans Weigand and Paul Johannesson and Birger Andersson},
	keywords     = {Design science, IT artifact, Ontology, UFO},
	abstract     = {From a design science perspective, information systems and their components are viewed as artifacts. However, not much has been written yet on the ontological status of artifacts or their structure. After March & Smith?s (1995) initial classification of artifacts in terms of models, constructs, methods and instantiations, there have been only a few attempts to come up with a more systematic approach. This conceptual paper provides an ontological analysis of the notion of artifact grounded in the foundational ontology UFO. Its core is an ontological characterization of artifacts, and technical objects in general from a Design Science Research perspective, developed in conversation with other approaches. This general artifact ontology is applied in a systematic classification of IS artifacts. We include practical implications for Design Science Research.}
}
@article{YEPMO2022101946,
	title        = {Anomaly explanation: A review},
	journal      = {Data & Knowledge Engineering},
	volume       = {137},
	pages        = {101946},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101946},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000720},
	author       = {Véronne Yepmo and Grégory Smits and Olivier Pivert},
	keywords     = {Anomaly explanation, Anomaly detection, Outlier interpretation, Interpretability, Explainable Artificial Intelligence (XAI)},
	abstract     = {Anomaly detection has been studied intensively by the data mining community for several years. As a result, many methods to detect anomalies have emerged, and others are still under development. But during the recent years, anomaly detection, just like a lot of machine learning tasks, is facing a wall. This wall, erected by the lack of trust of the final users, has slowed down the usage of these algorithms in the real-world situations for which they are designed. Having the best empirical accuracy is not enough anymore; there is a need for algorithms to explain their outputs to the users in order to increase their trust. Consequently, a new expression has emerged recently: eXplainable Artificial Intelligence (XAI). This expression, which gathers all the methods that provide explanations to the output of algorithms has gained popularity, especially with the outbreak of deep learning. A lot of work has been devoted to anomaly detection in the literature, but not as much to anomaly explanation. There is so much work on anomaly detection that several reviews can be found on the topic. In contrast, we were not able to find a survey on anomaly explanation in particular, while there are a lot of surveys on XAI in general or on XAI for neural networks for example. With this paper, we want to provide a comprehensive review of the anomaly explanation field. After a brief recall of some important anomaly detection algorithms, the anomaly explanation methods that we discovered in the literature will be classified according to a taxonomy that we define. This taxonomy stems from an analysis of what is really important when trying to explain anomalies.}
}
@article{GOMEZ2021101893,
	title        = {Analysis and evaluation of document-oriented structures},
	journal      = {Data & Knowledge Engineering},
	volume       = {134},
	pages        = {101893},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101893},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000203},
	author       = {Paola Gómez and Claudia Roncancio and Rubby Casallas},
	keywords     = {NoSQL, Structural metrics, Document-oriented systems, MongoDB},
	abstract     = {Document-oriented bases allow high flexibility in data representation which facilitates a rapid development of applications and enables many possibilities for data structuring. Unfortunately, in many cases, due to this flexibility and the absence of data modelling, the choice of a data representation is neglected by developers leading to many issues on several aspects of the document base and application quality; e.g., memory print, data redundancy, readability and maintainability. We aim at facilitating the study of data structuring alternatives and providing objective metrics to better reveal the advantages and disadvantages of a structure with respect to user needs. The main contributions of our approach are twofold. First of all, the semi-automatic generation of many suitable alternatives for data structuring given an initial UML model. Second, the automatic computation of structural metrics, allowing a comparison of the alternatives for JSON-compatible schema abstraction. These metrics reflect the complexity of the structure and are intended to be used in decision criteria for schema analysis and design process. This work capitalises on experiences with MongoDB, XML and software complexity metrics. The paper presents the schema generation and the metrics together with a validation scenario where we discuss how to use the results in a schema recommendation perspective.}
}
@article{GHARIB2021101888,
	title        = {COPri v.2 ? A core ontology for privacy requirements},
	journal      = {Data & Knowledge Engineering},
	volume       = {133},
	pages        = {101888},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101888},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X2100015X},
	author       = {Mohamad Gharib and Paolo Giorgini and John Mylopoulos},
	keywords     = {Privacy ontology, Privacy requirements, Privacy by Design, PbD, Requirements engineering, Conceptual modeling},
	abstract     = {Nowadays, most enterprises collect, store, and manage personal information of customers to deliver their services. In such a setting, privacy has emerged as a key concern since companies often neglect or even misuse personal data. In response to multiple massive breaches of personal data, governments around the world have enacted laws and regulations for privacy protection. These laws dictate privacy requirements for any system that acquires and manages personal data. Unfortunately, these requirements are often incomplete and/or inaccurate as many RE practitioners are insufficiently versed with privacy requirements and how are they different from other requirements, such as security. To tackle this problem, we developed a comprehensive ontology for privacy requirements. In particular, the contributions of this work include the derivation of an ontology from a previously conducted systematic literature review, an implementation using an ontology definition tool (Protégé), a demonstration of its coverage through an extensive example on Ambient Assisted Living, and a validation through competency questions. Also, we evaluate the ontology against the common pitfalls for ontologies with the help of some software tools, lexical semantics experts, and privacy and security researchers. The ontology presented herein (COPri v.2) has been enhanced with extensions motivated by the feedback received from privacy and security experts.}
}
@article{COSKUN2022102090,
	title        = {ERP failure: A systematic mapping of the literature},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102090},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102090},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000817},
	author       = {Evren Co?kun and Bahar Gezici and Murat Aydos and Ayça Koluk?sa Tarhan and Vahid Garousi},
	keywords     = {ERP, Enterprise resource planning, Failure, Failure factor, Systematic literature mapping},
	abstract     = {With the development of different technologies, the use and importance of Enterprise Resource Planning (ERP) systems continue to increase daily. In parallel with this increasing use, a lot of research is being done to successfully complete ERP implementation projects. However, despite these researches, reported case studies show that the success rates of ERP projects are meager. Based on the examples of experienced failure, researchers determine very different failure factors with varying perspectives for companies in different industry sectors, cultures, and sizes. It is becoming increasingly difficult for many practitioners and researchers to understand these failure factors correctly. Our objective is to investigate the state-of-the-art ERP Failure Factors that could benefit practitioners to utilize that information potentially. We review the body of knowledge related to ERP failure factors in the form of a systematic literature mapping (SLM). We pose four sets of research questions and systematically develop and refine a classification schema. The initial pool consisted of 353 articles. Systematic voting was conducted among the authors regarding the inclusion/exclusion criteria. As a result, there were 72 technical articles in our final pool. This SLM provides an overview of ERP critical failure factors (CFF) with different focused headings. These headings cover qualitative coding about CFF names, CFF rankings, the relation between CFF and ERP processes and failure modes, etc. The results of this study would benefit three groups of stakeholders: (i) Researchers who work on ERP Failure Factors, (ii) Solution implementers who provide consultancy services to companies that carry out ERP Implementation projects, and (iii) ERP project implementation managers. These stakeholders could utilize the results of this SLM to catch the trend of ERP implementation challenges.}
}
@article{ELAYAM2022102054,
	title        = {A hierarchical graph-based model for mobility data representation and analysis},
	journal      = {Data & Knowledge Engineering},
	volume       = {141},
	pages        = {102054},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102054},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000532},
	author       = {Maryam Maslek Elayam and Cyril Ray and Christophe Claramunt},
	keywords     = {Graph-based model, Hierarchical representation, Knowledge extraction, Maritime mobility, Network analysis},
	abstract     = {Hierarchical representations of transportation networks should provide a better understanding of mobility patterns and the underlying structures at various abstraction levels. This paper introduces a hierarchical graph-based model for representing moving objects and trajectories according to multiple spatial, temporal and semantic scales. This formal model is implemented in a graph database and experimented with historical maritime data. Several experimental analyses explore and extract knowledge patterns from the hierarchical graph database. A series of queries applied to an European maritime network derive mobility patterns and highlight network structures.}
}
@article{CAUTERUCCIO2022101979,
	title        = {Extraction and analysis of text patterns from NSFW adult content in Reddit},
	journal      = {Data & Knowledge Engineering},
	volume       = {138},
	pages        = {101979},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.101979},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000015},
	author       = {Francesco Cauteruccio and Enrico Corradini and Giorgio Terracina and Domenico Ursino and Luca Virgili},
	keywords     = {Reddit, NSFW posts and comments, Text patterns, Pattern utility measures, Social Network Analysis, Triads and cliques},
	abstract     = {Reddit is one of the few social networks that handles Not Safe For Work (NSFW) content in an explicit and well-structured way. Despite this, in the past literature on Reddit, there are very few researches concerning this topic. In particular, a study on the text of NSFW comments and posts published in this social medium is missing. In this paper, we aim at contributing to fill this gap by proposing an approach for extracting and analyzing text patterns from NSFW adult content in Reddit. Some peculiarities of our approach are the following: (i) text patterns are extracted based not only on frequency but also, and mostly, on several utility measures; (ii) extracted patterns contribute to the definition of social networks whose analysis allows us to extract several useful information about the users publishing and/or accessing NSFW content and the language adopted by them; (iii) our approach is not only descriptive but also predictive, because, in addition to identifying already existing user communities, it is able to propose new ones; these are made up of users who do not yet know each other but share the same interests and the same language.}
}
@article{OTHMAN2022101962,
	title        = {Learning English and Arabic question similarity with Siamese Neural Networks in community question answering services},
	journal      = {Data & Knowledge Engineering},
	volume       = {138},
	pages        = {101962},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101962},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000847},
	author       = {Nouha Othman and Rim Faiz and Kamel Smaïli},
	keywords     = {Community question answering, Question retrieval, Siamese, LSTM, CNN},
	abstract     = {In this paper, we tackle the task of similar question retrieval (QR) which is essential for Community Question Answering (cQA) and aims to retrieve historical questions that are semantically equivalent to the new queries. Over time, with the sharp increase of community archives and the accumulation of duplicated questions, the QR problem has become increasingly challenging due to the shortness of the community questions as well as the word mismatch problem as users can formulate the same query using different wording. Although many efforts have been devoted to address this problem, existing methods mostly relied on supervised models which significantly depend on massive training data sets and manual feature engineering. Such methods are chiefly constrained by their specificities that ignore the word order and do not capture enough syntactic and semantic information in questions. In this paper, we rely on Neural Networks (NNs) which use a deep analysis of words and questions to take into consideration the semantics as well as the structure of questions to predict the semantic text similarity. We propose a deep learning approach based on a Siamese architecture with Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism to let the model give different words different attention while modeling questions. We also explore the use of Convolutional Neural Networks (CNN) nested within the Siamese architecture to retrieve relevant questions. Different similarity measures were tested to predict the semantic similarity between the pairs of questions. To evaluate the proposed approach, we conducted experiments on large-scale datasets in English and Arabic.}
}
@article{AWITI2020101837,
	title        = {Design and implementation of ETL processes using BPMN and relational algebra},
	journal      = {Data & Knowledge Engineering},
	volume       = {129},
	pages        = {101837},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101837},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19306111},
	author       = {Judith Awiti and Alejandro A. Vaisman and Esteban Zimányi},
	keywords     = {Data Warehousing, OLAP, ETL, BPMN},
	abstract     = {Extraction, transformation, and loading (ETL) processes are used to extract data from internal and external sources of an organization, transform these data, and load them into a data warehouse. The Business Process Modeling and Notation (BPMN) has been proposed for expressing ETL processes at a conceptual level. A different approach is studied in this paper, where relational algebra (RA), extended with update operations, is used for specifying ETL processes. In this approach, data tasks in an ETL workflow can be automatically translated into SQL queries to be executed over a DBMS. To illustrate this study, the paper addresses the problem of updating Slowly Changing Dimensions (SCDs) with dependencies, that is, the case when updating a SCD table impacts on associated SCD tables. Tackling this problem requires extending the classic RA with update operations. The paper also shows the implementation of a portion of the TPC-DI benchmark that results from both approaches. Thus, the paper presents three implementations: (a) An SQL implementation based on the extended RA-based specification of an ETL process expressed in BPMN4ETL; and (b) Two implementations of workflows that follow from BPMN4ETL, one that uses the Pentaho DI tool, and another one that uses Talend Open Studio for DI. Experiments over these implementations of the TPC-DI benchmark for different scale factors were carried out, and are described and discussed in the paper, showing that the extended RA approach results in more efficient processes than the ones produced by implementing the BPMN4ETL specification over the mentioned ETL tools. The reasons for this result are also discussed.}
}
@article{PLOTNIKOVA2022102013,
	title        = {Applying the CRISP-DM data mining process in the financial services industry: Elicitation of adaptation requirements},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {102013},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102013},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000258},
	author       = {Veronika Plotnikova and Marlon Dumas and Fredrik P. Milani},
	keywords     = {Data mining, CRISP-DM, Case study, Requirements},
	abstract     = {Data mining techniques have gained widespread adoption over the past decades, particularly in the financial services domain. To achieve sustained benefits from these techniques, organizations have adopted standardized processes for managing data mining projects, most notably CRISP-DM. Research has shown that these standardized processes are often not used as prescribed, but instead, they are extended and adapted to address a variety of requirements. To improve the understanding of how standardized data mining processes are extended and adapted in practice, this paper reports on a case study in a financial services organization, aimed at identifying perceived gaps in the CRISP-DM process and characterizing how CRISP-DM is adapted to address these gaps. The case study was conducted based on documentation from a portfolio of data mining projects, complemented by semi-structured interviews with project participants. The results reveal 18 perceived gaps in CRISP-DM alongside their perceived impact and mechanisms employed to address these gaps. The identified gaps are grouped into six categories. Next, they were triangulated and augmented with the gaps discovered in the other studies. Then, the requirements for adapting CRISP-DM to address the gaps were derived, and the directions for the potential adaptations were outlined. The study presents a two-fold contribution. It provides practitioners with a structured set of gaps to be considered when applying CRISP-DM, or similar processes, in the financial services sector. Additionally, the study elicits the requirements and sketches the potential solutions to address these gaps. Also, the number of the identified gaps is generic and applicable to other sectors with similar concerns (e.g. privacy), such as telecom or e-commerce.}
}
@article{VEYRINFORRER2022102097,
	title        = {In pursuit of the hidden features of GNN?s internal representations},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102097},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102097},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X2200088X},
	author       = {Luca Veyrin-Forrer and Ataollah Kamal and Stefan Duffner and Marc Plantevit and Céline Robardet},
	keywords     = {Graph Neural Networks, Explainable artificial intelligence, Monte Carlo Tree Search},
	abstract     = {We consider the problem of explaining Graph Neural Networks (GNNs). While most attempts aim at explaining the final decision of the model, we focus on the hidden layers to examine what the GNN actually captures and shed light on the hidden features built by the GNN. To that end, we first extract activation rules that identify sets of exceptionally co-activated neurons when classifying graphs in the same category. These rules define internal representations having a strong impact in the classification process. Then ? this is the goal of the current paper ? we interpret these rules by identifying a graph that is fully embedded in the related subspace identified by the rule. The graph search is based on a Monte Carlo Tree Search directed by a proximity measure between the graph embedding and the internal representation of the rule, as well as a realism factor that constrains the distribution of the labels of the graph to be similar to that observed on the dataset. Experiments including 6 real-world datasets and 3 baselines demonstrate that our method DISCERN generates realistic graphs of high quality which allows providing new insights into the respective GNN models.}
}
@article{BENKRID2022102102,
	title        = {PROADAPT: Proactive framework for adaptive partitioning for big data warehouses},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102102},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102102},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000933},
	author       = {Soumia Benkrid and Ladjel Bellatreche and Yacine Mestoui and Carlos Ordonez},
	keywords     = {Multidimensional partitioning, Utility maximization, Self-adaptive partitioning, Workload clustering, Dimension?s hierarchies},
	abstract     = {Parallel DBMSs have become more and more mature and getting several success stories in the industry. This situation has been reached by powerful data partitioning and data allocation techniques and algorithms. By analyzing these findings closely, we figure out that they are quickly stressed by the workload changes, which represents the usual case of business analytics applications. To deal with this challenge in the context of big data warehouses, several studies proposed to move to another processing paradigm outside the DBMS realm such as Spark by proposing adaptive partitioning solutions to tackle the workload changes. The majority of approaches are offline and those that are online cause significant random disk I/O costs. This is because the correlation that may exist between jobs and data blocks read from the disk is not captured to refine the adaptive partitioning algorithms. This represents one of the major causes of providing high performance of dynamic workloads. To solve such limitations, we propose in this paper a proactive framework for query-aware adaptive partitioning (called PROADAPT) that uses an AI-inspired methodology that can be connected to any query optimizer managing partitioned data. This methodology uses a genetic algorithm to solve our formalized problem that considers the interaction that may exist among workload queries. PROADAPT intensively rewrites queries by exploiting dimension hierarchies to skip irrelevant data and then improves I/O performance. Different technical modules of our framework are discussed. Finally, we conduct intensive experiments on Postgres-XL and a Spark SQL parallel cluster to show the effectiveness and efficiency of our approach.}
}
@article{ANDRIAMAMPIANINA2022102017,
	title        = {Graph data temporal evolutions: From conceptual modelling to implementation},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {102017},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102017},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000271},
	author       = {Landy Andriamampianina and Franck Ravat and Jiefu Song and Nathalie Vallès-Parlangeau},
	keywords     = {Data models, Temporal graphs, Query, Neo4j},
	abstract     = {Graph data management systems are designed for managing highly interconnected data. However, most of the existing work on the topic does not take into account the temporal dimension of such data, even though they may change over time: new interconnections, new internal characteristics of data (etc.). For decision makers, these data changes provide additional insights to explain the underlying behaviour of a business domain. The objective of this paper is to propose a complete solution to manage temporal interconnected data. To do so, we propose a new conceptual model of temporal graphs. It has the advantage of being generic as it captures the different kinds of changes that may occur in interconnected data. We define a set of translation rules to convert our conceptual model into the logical property graph. Based on the translation rules, we implement several temporal graphs according to benchmark and real-world datasets in the Neo4j data store. These implementations allow us to carry out a comprehensive study of the feasibility and usability (through business analyses), the efficiency (saving up to 99% query execution times comparing to classic approaches) and the scalability of our solution.}
}
@article{BELLOMARINI2022102073,
	title        = {Reasoning on company takeovers: From tactic to strategy},
	journal      = {Data & Knowledge Engineering},
	volume       = {141},
	pages        = {102073},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102073},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000672},
	author       = {Luigi Bellomarini and Lorenzo Bencivelli and Claudia Biancotti and Livia Blasi and Francesco Paolo Conteduca and Andrea Gentili and Rosario Laurendi and Davide Magnanimi and Michele Savini Zangrandi and Flavia Tonelli and Stefano Ceri and Davide Benedetto and Markus Nissl and Emanuel Sallinger},
	keywords     = {Logic-based reasoning, Datalog+/-, Knowledge graphs, Corporate economics},
	abstract     = {Corporate takeovers ? the purchases of a company by another ? are significant economic events. They affect the parties involved in the transaction, inducing relevant dynamics in the enterprises? lifecycle, and are also relevant at an aggregate level for the whole economy: takeovers can produce efficiency gains and improved capital allocation, while, on the other hand, they can also increase the market power of specific companies and hamper competition. In this paper, we propose a logic-probabilistic reasoning framework to study the determinants of company takeovers and predict future ones. In particular, we model the domain of interest as a logic-based knowledge graph, where the extensional knowledge contains facts concerning company ownership structures and the characteristics of the shareholders, and the intensional knowledge encodes a set of takeover suitability criteria in the form of reasoning rules, whose conditional dependencies are modeled with a Bayesian network. Our rules are expressed in Vadalog, a language of the Datalog+/- family. Our framework revolves around a data engineering process that allows eliciting the takeover determinants from a corpus of anecdotal cases, refining and encoding them into logic rules, and finally combining their outcomes. We implement and operate the framework in the Vadalog System, a state-of-the-art reasoner, and apply it to the knowledge graph of the Italian companies of the Central Bank of Italy. We provide an extensive experimental evaluation.}
}
@article{ROLDANMOLINA2021101889,
	title        = {An ontology knowledge inspection methodology for quality assessment and continuous improvement},
	journal      = {Data & Knowledge Engineering},
	volume       = {133},
	pages        = {101889},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101889},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000161},
	author       = {Gabriela R. Roldán-Molina and David Ruano-Ordás and Vitor Basto-Fernandes and José R. Méndez},
	keywords     = {Ontology, Ontology fixing, Ontology quality measures, Ontology improvement methodology, Deming cycle},
	abstract     = {Ontology-learning methods were introduced in the knowledge engineering area to automatically build ontologies from natural language texts related to a domain. Despite the initial appeal of these methods, automatically generated ontologies may have errors, inconsistencies, and a poor design quality, all of which must be manually fixed, in order to maintain the validity and usefulness of automated output. In this work, we propose a methodology to assess ontologies quality (quantitatively and graphically) and to fix ontology inconsistencies minimizing design defects. The proposed methodology is based on the Deming cycle and is grounded on quality standards that proved effective in the software engineering domain and present high potential to be extended to knowledge engineering quality management. This paper demonstrates that software engineering quality assessment approaches and techniques can be successfully extended and applied to the ontology-fixing and quality improvement problem. The proposed methodology was validated in a testing ontology, by ontology design quality comparison between a manually created and automatically generated ontology.}
}
@article{ALI2021101929,
	title        = {Self-adaptation in smartphone applications: Current state-of-the-art techniques, challenges, and future directions},
	journal      = {Data & Knowledge Engineering},
	volume       = {136},
	pages        = {101929},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101929},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000562},
	author       = {Mughees Ali and Saif Ur Rehman Khan and Shahid Hussain},
	keywords     = {Self-adaptation, Mobile applications, Smartphone applications, Self-adaptive mobile apps},
	abstract     = {Context: In the last few years, smartphones have become an essential part of our lives. Several mobile applications have been uploaded to their respective app stores on daily basis to facilitate the end-users. To ensure the availability of the application and its services, the application needs to be able to detect and deal with various types of changes. Mobile application based self-adaptation is regarded as a solution to handle the above-mentioned chnages. Objective: The objectives of this study are: (i) identification of current state-of-the-art techniques, frameworks, models, and algorithms for self-adaptation in mobile applications context, (ii) exploring the core challenges faced in the development of self-adaptive mobile applications, and (iii) conducting a SWOT (Strength Weakness Opportunity Threat) analysis to figure out the research gaps in the targeted research field. Method: To accomplish the mentioned objective, we conducted a systematic literature review to enlighten the need for adaptation in this domain, the targeted areas, their beneficial aspects, state-of-the-art proposed solutions, and their challenges. We performed the keyword-based search on 11 well-known databases to determine the potential studies published between years 2015 to 2020. In total, 31 studies (from 933 potential studies) are selected grounded on the defined inclusion and exclusion criteria. Results: The main findings of this work are: (i) MAPE-K (Monitor Analyze Plan Execute-Knowledge) model is frequently used in several studies due to its resilience, (ii) current work focuses on the android operating system due to its popularity and flexibility; however, it lacks in considering other mobile?s operating systems, and (iii) reported work also lacks in mentioning any benchmark (self-adaptive framework). Conclusion: The current study is helpful for the researchers intended to work in the smartphones domain. From developers viewpoint, this study reports the faced challenges during the development of smartphones applications. Moreover, the performed study is beneficial in filling the identified research gaps by providing a foundation useful to plan future research regarding self-adaptation in smartphone applications context.}
}
@article{BUTT2021101877,
	title        = {A provenance model for control-flow driven scientific workflows},
	journal      = {Data & Knowledge Engineering},
	volume       = {131-132},
	pages        = {101877},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101877},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000045},
	author       = {Anila Sahar Butt and Peter Fitch},
	keywords     = {Workflow provenance, Provenance model, Control-flow patterns},
	abstract     = {Provenance in the context of workflows, both for their specification and for the data they derive, is essential for result reproducibility, sharing, and knowledge reuse in the scientific community. The information models to capture the provenance of scientific workflows, known as provenance models, have been of wide interest and studied in many fields, including the semantic Web. However, most existing provenance models rely on commonly perceived data-driven execution paradigm of scientific workflows and overlook control constructs that may appear in scientific workflows. The provenance models proposed by the semantic Web community for data-driven scientific workflows underspecify the structure of the workflows (i.e., workflow provenance). Such an underspecified workflow structure can result in the misinterpretation of a scientific experiment and precludes its conformance checking, thus restricting provenance gains. This paper shows that the design of a provenance model for control-flow constructs and the means to integrate it with the existing provenance models can maximise the provenance gains for the users of scientific workflows. In this regard, first, we specify the need for a control-flow driven scientific workflow provenance model and detail the minimal characteristics which such a model should address. Secondly, we present a formal model to specify the control-flows that may appear in scientific workflows and describe how the existing provenance models can be complemented by using the proposed model. Finally, we show that the proposed model can capture accurate provenance information for the scientific workflows, which makes it possible to understand, reproduce and validate workflows and their output.}
}
@article{NAHAR2022102038,
	title        = {Integrated identity and access management metamodel and pattern system for secure enterprise architecture},
	journal      = {Data & Knowledge Engineering},
	volume       = {140},
	pages        = {102038},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102038},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000428},
	author       = {Kamrun Nahar and Asif Qumer Gill},
	keywords     = {Identity management, Access control management, Metamodel, Ontology, Enterprise architecture, Design science research},
	abstract     = {Identity and access management (IAM) is one of the key components of the secure enterprise architecture for protecting the digital assets of the information systems. The challenge is: How to model an integrated IAM for a secure enterprise architecture to protect digital assets? This research aims to address this question and develops an ontology based integrated IAM metamodel for the secure digital enterprise architecture (EA). Business domain and technology agnostic characteristics of the developed IAM metamodel will allow it to develop IAM models for different types of information systems. Well-known design science research (DSR) methodology was adopted to conduct this research. The developed IAM metamodel is evaluated by using the demonstration method. Furthermore, as a part of the evaluation, a pattern system has been developed, consisting of eight IAM patterns. Each pattern offers a solution to a specific IAM related problem. The outcome of this research indicates that enterprise, IAM and information systems architects and academic researchers can use the proposed IAM metamodel and the pattern system to design and implement situation-specific IAM models within the overall context of a secure EA for information systems.}
}
@article{COSTA2022101977,
	title        = {A core ontology on the Human?Computer Interaction phenomenon},
	journal      = {Data & Knowledge Engineering},
	volume       = {138},
	pages        = {101977},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101977},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000951},
	author       = {Simone Dornelas Costa and Monalessa Perini Barcellos and Ricardo de Almeida Falbo and Tayana Conte and Káthia M. {de Oliveira}},
	keywords     = {Human?Computer Interaction, User interface, Interactive computer system, Ontology, Ontology network},
	abstract     = {Human?Computer Interaction (HCI) is a complex communication phenomenon involving human beings and computer systems that gained large attention from industry and academia with the advent of new types of interactive systems (mobile applications, smart cities, smart homes, ubiquitous systems and so on). Despite of its importance, there is still a lack of formal and explicit representations of what the HCI phenomenon is. In this paper, we intend to clarify the main notions involved in the HCI phenomenon, by establishing an explicit conceptualization of it. To do so, we need to understand what interactive computer systems are, which types of actions users perform when interacting with an interactive computer system, and finally what human?computer interaction itself is. The conceptualization is presented as a core reference ontology, called HCIO (HCI Ontology), which is grounded in the Unified Foundational Ontology (UFO). HCIO was evaluated using ontology verification and validation techniques and has been used as core ontology of an HCI ontology network.}
}
@article{KORTMANN2022102091,
	title        = {Watch out, pothole! featuring road damage detection in an end-to-end system for autonomous driving},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102091},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102091},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000829},
	author       = {Felix Kortmann and Pascal Fassmeyer and Burkhardt Funk and Paul Drews},
	keywords     = {Road damage detection, Road damage severity, Autonomous driving architecture, Computer vision},
	abstract     = {While autonomous driving technology made significant progress in the last decade, road damage detection as a relevant challenge for ensuring safety and comfort is still under development. This paper addresses the lack of algorithms for detecting road damages that meet autonomous driving systems? requirements. We investigate the environmental perception systems? architecture and current algorithm designs for road damage detection. Based on the autonomous driving architecture, we develop an end-to-end concept that leverages data from low-cost pre-installed sensors for real-time road damage and damage severity detection as well as cloud- and crowd-based HD Feature Maps to share information across vehicles. In a design science research approach, we develop three artifacts in three iterations of expert workshops and design cycles: the end-to-end concept featuring road damages in the system architecture and two lightweight deep neural networks, one for detecting road damages and another for detecting their severity as the central components of the system. The research design draws on new self-labeled automotive-grade images from front-facing cameras in the vehicle and interdisciplinary literature regarding autonomous driving architecture and the design of deep neural networks. The road damage detection algorithm delivers cutting-edge performance while being lightweight compared to the winners of the IEEE Global Road Damage Detection Challenge 2020, which makes it applicable in autonomous vehicles. The road damage severity algorithm is a promising approach, delivering superior results compared to a baseline model. The end-to-end concept is developed and evaluated with experts of the autonomous driving application domain.}
}
@article{GIACHANOU2022101960,
	title        = {The impact of psycholinguistic patterns in discriminating between fake news spreaders and fact checkers},
	journal      = {Data & Knowledge Engineering},
	volume       = {138},
	pages        = {101960},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101960},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000835},
	author       = {Anastasia Giachanou and Bilal Ghanem and Esteban A. Ríssola and Paolo Rosso and Fabio Crestani and Daniel Oberski},
	keywords     = {Fake news, Linguistic analysis, Misinformation},
	abstract     = {Fake news is a threat to society. A huge amount of fake news is posted every day on social networks which is read, believed and sometimes shared by a number of users. On the other hand, with the aim to raise awareness, some users share posts that debunk fake news by using information from fact-checking websites. In this paper, we are interested in exploring the role of various psycholinguistic characteristics in differentiating between users that tend to share fake news and users that tend to debunk them. Psycholinguistic characteristics represent the different linguistic information that can be used to profile users and can be extracted or inferred from users? posts. We present the CheckerOrSpreader model that uses a Convolution Neural Network (CNN) to differentiate between spreaders and checkers of fake news. The experimental results showed that CheckerOrSpreader is effective in classifying a user as a potential spreader or checker. Our analysis showed that checkers tend to use more positive language and a higher number of terms that show causality compared to spreaders who tend to use a higher amount of informal language, including slang and swear words.}
}
@article{DOST2022101975,
	title        = {Aligning and linking entity mentions in image, text, and knowledge base},
	journal      = {Data & Knowledge Engineering},
	volume       = {138},
	pages        = {101975},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101975},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X2100094X},
	author       = {Shahi Dost and Luciano Serafini and Marco Rospocher and Lamberto Ballan and Alessandro Sperduti},
	keywords     = {AI, NLP, Computer Vision, Machine Learning, Knowledge Representation, Semantic Web, Entity recognition and linking},
	abstract     = {A picture is worth a thousand words, the adage reads. However, pictures cannot replace words in terms of their ability to efficiently convey clear (mostly) unambiguous and concise knowledge. Images and text, indeed reveal different and complementary information that, if combined will result in more information than the sum of that contained in a single media. The combination of visual and textual information can be obtained by linking the entities mentioned in the text with those shown in the pictures. To further integrate this with the agent?s background knowledge, an additional step is necessary. That is, either finding the entities in the agent knowledge base that correspond to those mentioned in the text or shown in the picture or, extending the knowledge base with the newly discovered entities. We call this complex task Visual-Textual-Knowledge Entity Linking (VTKEL). In this article, after providing a precise definition of the VTKEL task, we present two datasets called VTKEL1k* and VTKEL30k. These datasets consisting of images and corresponding captions, in which the image and textual mentions are both annotated with the corresponding entities typed according to the YAGO ontology. The datasets can be used for training and evaluating algorithms of the VTKEL task. Successively, we introduce a baseline algorithm called VT-LinKEr (Visual-Textual-Knowledge Entity Linker) for the solution of the VTKEL task. We evaluate the performances of VT-LinKEr on both datasets. We then contribute a supervised algorithm called ViTKan (Visual-Textual-Knowledge Alignment Network). We trained the ViTKan algorithm using features data of the VTKEL1k* dataset. The experimental results on VTKEL1k* and VTKEL30k datasets show that ViTKan substantially outperforms the baseline algorithm.}
}
@article{TURET2022102056,
	title        = {Hybrid methodology for analysis of structured and unstructured data to support decision-making in public security},
	journal      = {Data & Knowledge Engineering},
	volume       = {141},
	pages        = {102056},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102056},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000544},
	author       = {Jean Gomes Turet and Ana Paula Cabral Seixas Costa},
	keywords     = {Machine learning, Public security, Decision-making process},
	abstract     = {This work proposes a hybrid methodology that enables the integration of structured and unstructured data to support the decision-making process in public security contexts. The proposed methodology facilitates classification and prediction of crime in a given region, making it possible to identify actions to improve public security based on the results. The integration of the data takes place in two main steps: (1) loading and analyzing structured data made available by government agencies; and (2) absorbing, classifying, and analyzing unstructured data from digital platforms such as Twitter, Where I Was Robbed, and CityCop. In this way, it becomes possible to transform these unstructured data into structured data to be incorporated into a historical database on which algorithms can act to classify, measure, and predict crime. To illustrate the applicability of this methodology, we conducted a study in the city of Recife, Brazil. Structured and unstructured data were gathered in order to conduct a neighborhood classification analysis of crime hot spots. Based on that analysis, we conducted a series of actions intended to bring improvements to the region by the local police. We obtained an increase in the algorithms? accuracy rate of 80%, indicating that public security organizations can base their actions on the results of the proposed methodology.}
}
@article{ZHONG2022101981,
	title        = {Clustering sequence graphs},
	journal      = {Data & Knowledge Engineering},
	volume       = {138},
	pages        = {101981},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.101981},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000027},
	author       = {Haodi Zhong and Grigorios Loukides and Solon P. Pissis},
	keywords     = {Sequence clustering, Graph clustering, Sequential data},
	abstract     = {In application domains ranging from social networks to e-commerce, it is important to cluster users with respect to both their relationships (e.g., friendship or trust) and their actions (e.g., visited locations or rated products). Motivated by these applications, we introduce here the task of clustering the nodes of a sequence graph, i.e., a graph whose nodes are labeled with strings (e.g., sequences of users? visited locations or rated products). Both string clustering algorithms and graph clustering algorithms are inappropriate to deal with this task, as they do not consider the structure of strings and graph simultaneously. Moreover, attributed graph clustering algorithms generally construct poor solutions because they need to represent a string as a vector of attributes, which inevitably loses information and may harm clustering quality. We thus introduce the problem of clustering a sequence graph. We first propose two pairwise distance measures for sequence graphs, one based on edit distance and shortest path distance and another one based on SimRank. We then formalize the problem under each measure, showing also that it is NP-hard. In addition, we design a polynomial-time 2-approximation algorithm, as well as a heuristic for the problem. Experiments using real datasets and a case study demonstrate the effectiveness and efficiency of our methods.}
}
@article{YAGANOGLU2022102087,
	title        = {Hepatitis C virus data analysis and prediction using machine learning},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102087},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102087},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000787},
	author       = {Mete Ya?ano?lu},
	keywords     = {Hepatitis C, Machine learning, Data science, Visualization},
	abstract     = {Medical decision support systems have been on the rise with technological advances and they have been the subject of many studies. Developing an effective medical decision support system requires a high amount of accuracy, precision, and sensitivity as well as time efficiency that is inversely proportional to the complexity of the model. Hepatitis C virus (HCV) infection is one of the most important causes of chronic liver disease worldwide. In this study, data discovery has been made by applying data science processes, and the HCV has been estimated with machine learning methods. By analyzing and visualizing the values in the data set, features that may be important for HCV was determined, and HCV estimation was made using various machine learning methods, pre-processing and feature extraction. According to the features obtained from this study, the estimation of HCV can be made automatically and can be a decision support system that helps the researchers and clinicians. In this study, HCV was obtained with 99.31% accuracy by adding new features and eliminating imbalances between classes. The model in this study can be used as an alternative method in the prediction of Hepatitis C-related diseases.}
}
@article{LEGUILLY2022101944,
	title        = {SQL query extensions for imprecise questions},
	journal      = {Data & Knowledge Engineering},
	volume       = {137},
	pages        = {101944},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101944},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000719},
	author       = {Marie {Le Guilly} and Jean-Marc Petit and Vasile-Marian Scuturici},
	keywords     = {Query extensions, Imprecise questions, SQL},
	abstract     = {Within the big data tsunami, relational databases and SQL remain inescapable in most cases for accessing data. If SQL is easy-to-use and has proved its robustness over the years, it is not always easy to formulate SQL queries as it is more and more frequent to have databases with hundreds of tables and/or attributes. Identifying the pertinent conditions to select the desired data, or even the relevant attributes, is not trivial, especially when the user only has an imprecise question in mind, and is not sure of how to translate its conditions directly into SQL. To make it easier to write SQL queries when the initial question is imprecise, we propose SQL query extensions: given a query, it suggests several possible additional selection clauses, to complete the Where clause of the query, as a form of SQL query semantic autocompletion. This is helpful for both understanding the initial query?s results, and refining the query to reach the desired tuples. The process is iterative, as a query constructed using an extension can also be completed. It is also adaptable, as the number of extensions to compute is flexible. A prototype has been implemented in a SQL editor on top of a database management system, and two types of evaluation are proposed. A first one looks at the scaling of the system with a large number of tuples. Then a user study examines two questions: does the extension tool speed up the writing of SQL queries? And is it easily adopted by users? A thorough experiment was conducted on a group of 70 computer science students divided in two groups (one with the extension tool and the other one without) to answer those questions. In the end, the results showed a faster answering time for students that could use the extensions: 32 min on average to complete the test for the group with extensions, against 48 min for the others.}
}
@article{HAO2022102079,
	title        = {On efficient top-k transaction path query processing in blockchain database},
	journal      = {Data & Knowledge Engineering},
	volume       = {141},
	pages        = {102079},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102079},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000702},
	author       = {Kun Hao and Junchang Xin and Zhiqiong Wang and Zhongming Yao and Guoren Wang},
	keywords     = {Blockchain, Query processing, Top- query, Transaction path, Collaborative query model},
	abstract     = {With the rapid development of blockchain, user?s daily financial bills and transfer records can be safely and completely stored in a transaction form. These transactions express commercial preferences for various scenarios and can be used to improve the quality of application services such as data analysis, data security, and recommendation systems. Therefore, there is a growing demand for processing diverse queries over blockchain transactions. In this paper, we formulate a new problem of blockchain top-k transaction path query (BCTkPQ) which returns first k transactions in a given query path according to the user-specified conditions. For processing BCTkPQ, we design a novel Collaborative Query Model (CQM) consisting of a set of collaborative peers and each of which contains three key components, i.e., parser, indexer and executor. First, in the parser, we propose a graph-based structure to denote the transaction paths. Then, in the indexer, we propose a two-level index to increase query and verification efficiency. Finally, in the executor, we present an optimized query algorithm for answering BCTkPQ securely and efficiently and further give a verification algorithm to guarantee the correctness of the query results. We conduct extensive experiments to illustrate the efficiency and effectiveness of our solution.}
}
@article{XU2022101985,
	title        = {FaNDS: Fake News Detection System using energy flow},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {101985},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.101985},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000040},
	author       = {Jiawei Xu and Vladimir Zadorozhny and Danchen Zhang and John Grant},
	keywords     = {Fake news, Inconsistency graph, Energy flow},
	abstract     = {Recently, the term ?fake news? has been broadly and extensively utilized for disinformation, misinformation, hoaxes, propaganda, satire, rumors, click-bait, and junk news. It has become a serious problem around the world. We present a new system, FaNDS, that detects fake news efficiently. The system is based on several concepts used in some previous works but in a different context. There are two main concepts: an Inconsistency Graph and Energy Flow. The Inconsistency Graph contains news items as nodes and inconsistent opinions between them for edges. Energy Flow assigns each node an initial energy and then some energy is propagated along the edges until the energy distribution on all nodes converges. To illustrate FaNDS we use the original data from the Fake News Challenge (FNC-1). First, the data has to be reconstructed in order to generate the Inconsistency Graph. The graph contains various subgraphs with well-defined shapes that represent different types of connections between the news items. Then the Energy Flow method is applied. The nodes with high energy are the candidates for being fake news. In our experiments, all these were indeed fake news as we checked each using several reliable web sites. We compared FaNDS to several other fake news detection methods and found it to be more sensitive in discovering fake news items.}
}
@article{PIRNAY2022102019,
	title        = {How to build data-driven Strategy Maps? A methodological framework proposition},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {102019},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102019},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000283},
	author       = {Lhorie Pirnay and Corentin Burnay},
	keywords     = {Strategy Map, Causalities, Performance measurement models, Strategic decision-making, Data mining, Methodologies and tools},
	abstract     = {The Strategy Map is a strategic tool that enables companies to formulate, control and communicate their strategy and positively influence their performance. Introduced in 2000, the methodology for developing Strategy Maps has evolved over the past two decades, but still relies exclusively on human input. In practice, Strategy Map causalities ? the core elements of this tool ? are determined by managers? opinions and judgments, which can lead to a lack of accuracy, completeness and longitudinal perspective. Although authors in the literature have pointed out these problems in the past, there are few recommendations on how to address them. In this paper, we propose a methodological framework which uses operational data and data mining techniques to systematize the detection of causalities in Strategy Maps. We apply time series techniques and Granger causality tests to increase the efficiency of such strategic tool. We demonstrate the feasibility and relevance of this methodology using data from skeyes, the Belgian air traffic control company. 11This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.}
}
@article{KANAGARATHINAM2022102042,
	title        = {Machine learning-based risk prediction model for cardiovascular disease using a hybrid dataset},
	journal      = {Data & Knowledge Engineering},
	volume       = {140},
	pages        = {102042},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102042},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000441},
	author       = {Karthick Kanagarathinam and Durairaj Sankaran and R. Manikandan},
	keywords     = {Heart disease, Machine learning classifier, Feature selection, Hybrid heart disease dataset, CatBoost},
	abstract     = {CVD (cardiovascular disease) is one of the most common causes of death in the world today. CVD prediction allows health professionals to make an informed decision about their patients? health. Data mining is the process of transforming large amounts of medical data in its raw form into actionable insights that can be used to make intelligent forecasts and decisions. Machine learning (ML) based prediction models provide a better solution to help patients? health diagnoses in the health care industry. The objective of this research is to create a hybrid dataset to aid in the development of a best CVD risk prediction model. The Hungarian, the Switzerland, the Cleveland, and the Long Beach datasets are the most commonly used datasets in heart disease (HD) prediction. These datasets have a maximum of 303 instances with missing values in their features, and the presence of missing values reduces the accuracy of the prediction model. So, in this article, we created the ?Sathvi? dataset by combining these datasets, and it has 531 instances with 12 attributes with no missing data. The Pearson?s correlation method was used to eliminate redundant features during the feature selection process. The Naive Bayes (NB), XGBoost, k-nearest neighbour (k-NN), multilayer perceptron (MLP), support vector machine (SVM), and CatBoost ML classifiers have been applied for prediction. The CatBoost ML classifier was validated with 10-fold cross validation, and the best accuracy ranged from 88.67% to 98.11%, with a mean of 94.34%.}
}
@article{GHORBEL2020101864,
	title        = {Handling data imperfection?False data inputs in applications for Alzheimer?s patients},
	journal      = {Data & Knowledge Engineering},
	volume       = {130},
	pages        = {101864},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101864},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X2030094X},
	author       = {Fatma Ghorbel and Fayçal Hamdi and Nassira Achich and Elisabeth Metais},
	keywords     = {Applications for Alzheimer?s patients, Imperfection types, False data inputs, Believability},
	abstract     = {Handling data imperfection is a crucial issue in many application domains. This is particularly true when handling imperfect data inputs in applications for Alzheimer?s patients. In this paper we first propose a typology of imperfection for data entered by Alzheimer?s patients or their caregivers in the context of these applications (mainly due to the memory discordance caused by the disease). This topology includes nine direct and three indirect imperfection types. The direct ones are deduced from the data inputs e.g. uncertainty and uselessness. The indirect imperfection types are deduced from the direct ones, e.g. the redundancy. We then propose an approach, called DBE_ALZ, that handles false data entry by estimating the believability of each data input. Based on the proposed typology, the falsity of these data is related to five imperfection types: uncertainty, confusion, typing error, wrong knowledge and inconsistency. DBE_ALZ includes a believability model that defines a set of dimensions and sub-dimensions allowing a qualitative estimation of the believability of a given data input. It is estimated based on its reasonableness and the reliability of its author. Compared to related work, the data input reasonableness is measured not only based on common-sense standard, but also based on a set of personalized assertions. The reliability of the patient is estimated based on the progression of the disease and the state of his memory at the moment of entry. However, the reliability of the caregiver is estimated based on his age and his knowledge about the data input?s field. Based on the believability model, we estimate quantitatively the believability of the data input by defining a set of metrics associated to the proposed dimensions and sub-dimensions. The measurement methods rely on probability and fuzzy set theories to reason about uncertain and imprecise knowledge (Bayesian networks and Mamdani fuzzy inference systems). Three languages are supported: English, French and Arabic. Based on the generated believability degrees, a set of decisive actions are proposed to guarantee the quality of the data inputs e.g., inferring or not based on a given data. We illustrate the usefulness of our approach in the context of the Captain Memo memory prosthesis. Finally, we discuss the encouraging results derived from the evaluation step.}
}
@article{DUARTE2021101892,
	title        = {An ontological analysis of software system anomalies and their associated risks},
	journal      = {Data & Knowledge Engineering},
	volume       = {134},
	pages        = {101892},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101892},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000197},
	author       = {Bruno Borlini Duarte and Ricardo {de Almeida Falbo} and Giancarlo Guizzardi and Renata Guizzardi and Vítor E. Silva Souza},
	keywords     = {Software defects, Errors and failures, Ontological foundations of software systems, Conceptual modeling, Methods and methodologies, Software system risk, Unified Foundational Ontology (UFO)},
	abstract     = {Software systems have an increasing value in our lives, as our society relies on them for the numerous services they provide. However, as our need for larger and more complex software systems grows, the risks involved in their operation also grows, with possible consequences in terms of significant material and social losses. The rational management of software defects and possible failures is a fundamental requirement for a mature software industry. Standards, professional guides and capability models directly emphasize how important it is for an organization to know and to have a well-established history of failures, errors and defects as they occur in software activities. The problem is that each of these reference models employs its own vocabulary to deal with these phenomena, which can lead to a deficiency in the understanding of these notions by software engineers, causing potential interoperability problems between supporting tools, and, consequently, a poorer adoption of these standards and tools in practice. In this paper, we address this problem of the lack of a consensual conceptualization in this area by proposing two reference conceptual models: an Ontology of Software Defects, Errors and Failures (OSDEF), which takes into account an ecosystem of software artifacts, and a Reference Ontology of Software Systems (ROSS), which characterizes software systems and related artifacts at different levels of abstraction. Moreover, we use OSDEF and ROSS to perform an ontological analysis of the impact of defects, errors and failures of software systems from a risk analysis perspective. To do that, we employee an existing core ontology, namely, the Common Ontology of Value and Risk (COVR). The ontologies presented here are grounded on the Unified Foundational Ontology (UFO) and based on well-known and widely-accepted standards, professional and scientific guides and capability models. We demonstrate how this approach can suitably promote conceptual clarification and terminological harmonization in this area.}
}
@article{MUPPAVARAPU2021101923,
	title        = {Knowledge extraction using semantic similarity of concepts from Web of Things knowledge bases},
	journal      = {Data & Knowledge Engineering},
	volume       = {135},
	pages        = {101923},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101923},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000501},
	author       = {Vamsee Muppavarapu and Gowtham Ramesh and Amelie Gyrard and Mahda Noura},
	keywords     = {Interoperability, Internet of Things, Semantic Web of Things, Popular concepts, Smart building, Smart home},
	abstract     = {The Internet of Things (IoT) is one of the rapidly growing technologies with the aim of establishing communication among objects, people, and processes. This rapidly growing technology faces a lot of challenges that hinder its wider adoption, specifically in developing applications that involve heterogeneous domains. Currently, developing such interoperable applications require substantial efforts by the developers to hard code the requirements to ensure the correctness of transferring knowledge. The efforts can be significantly reduced by developing an interoperable platform that ensures seamless communication between heterogeneous IoT devices. W3C Web of Things (WoT) is a significant step towards enabling interoperability between IoT devices by integrating the existing Web ecosystem with ?Things?. WoT provides a unified interface over a suitable network protocol facilitating interactions between different IoT protocols. WoT Thing Descriptions (TD) enrich interoperability providing both human and machine readable metadata about a Thing. However, the WoT still falls short in providing semantic interoperability due to insufficient standard vocabularies which can describe different IoT application domains. In this paper, we propose a semantic similarity-based approach to automatically identify and extract the most common concepts from sixteen popular ontologies belonging to smart home and smart building domains. The proposed method helps the developers and researchers to develop a domain ontology with reduced effort. The extracted concepts are evaluated by the domain experts and are found to be sufficient in describing the smart home and smart building domains.}
}
@article{EICHLER2021101931,
	title        = {Modeling metadata in data lakes?A generic model},
	journal      = {Data & Knowledge Engineering},
	volume       = {136},
	pages        = {101931},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101931},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000586},
	author       = {Rebecca Eichler and Corinna Giebler and Christoph Gröger and Holger Schwarz and Bernhard Mitschang},
	keywords     = {Metadata management, Metadata model, Data lake, Data management, Data lake zones, Metadata classification},
	abstract     = {Data contains important knowledge and has the potential to provide new insights. Due to new technological developments such as the Internet of Things, data is generated in increasing volumes. In order to deal with these data volumes and extract the data?s value new concepts such as the data lake were created. The data lake is a data management platform designed to handle data at scale for analytical purposes. To prevent a data lake from becoming inoperable and turning into a data swamp, metadata management is needed. To store and handle metadata, a generic metadata model is required that can reflect metadata of any potential metadata management use case, e.g., data versioning or data lineage. However, an evaluation of existent metadata models yields that none so far are sufficiently generic as their design basis is not suited. In this work, we use a different design approach to build HANDLE, a generic metadata model for data lakes. The new metadata model supports the acquisition of metadata on varying granular levels, any metadata categorization, including the acquisition of both metadata that belongs to a specific data element as well as metadata that applies to a broader range of data. HANDLE supports the flexible integration of metadata and can reflect the same metadata in various ways according to the intended utilization. Furthermore, it is created for data lakes and therefore also supports data lake characteristics like data lake zones. With these capabilities HANDLE enables comprehensive metadata management in data lakes. HANDLE?s feasibility is shown through the application to an exemplary access-use-case and a prototypical implementation. By comparing HANDLE with existing models we demonstrate that it can provide the same information as the other models as well as adding further capabilities needed for metadata management in data lakes.}
}
@article{ACCUOSTO2020101840,
	title        = {Mining arguments in scientific abstracts with discourse-level embeddings},
	journal      = {Data & Knowledge Engineering},
	volume       = {129},
	pages        = {101840},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101840},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X20300446},
	author       = {Pablo Accuosto and Horacio Saggion},
	abstract     = {Argument mining consists in the automatic identification of argumentative structures in texts. In this work we leverage existing discourse-level annotations to facilitate the identification of argumentative components and relations in scientific texts, which has been recognized as a particularly challenging task. We propose a new annotation schema and use it to augment a corpus of computational linguistics abstracts that had previously been annotated with discourse units and relations. Our initial experiments with the enriched corpus confirm the potential value of incorporating discourse information in argument mining tasks. In order to tackle the limitations posed by the lack of corpora containing both discourse and argumentative annotations we explore two transfer learning approaches in which discourse parsing is used as an auxiliary task when training argument mining models. In this case, as no discourse information is used as input, the resulting models could be used to predict the argumentative structure of unannotated texts.}
}
@article{ROYHUBARA2022101950,
	title        = {Selecting databases for Polyglot Persistence applications},
	journal      = {Data & Knowledge Engineering},
	volume       = {137},
	pages        = {101950},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101950},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000744},
	author       = {Noa Roy-Hubara and Peretz Shoval and Arnon Sturm},
	keywords     = {Database selection, Database models, NoSQL},
	abstract     = {In the last decade, new types of databases emerged, most notably NoSQL databases. Within this family of databases, there are specific models, such as document-based, graph-based, and more, each of which, along with the relational model, may be best suited to particular applications. Therefore, the issue of which database model to select for a given application is essential. Nowadays, the selection of a database model is not based on systematic methods that consider the specific requirements and characteristics of the application. This paper proposes a structured method for database model selection that considers various factors, including data-related, functional, and non-functional requirements. Based on these factors, the method recommends the most appropriate database models for the application. We discuss the sensitivity of the method and evaluate it via several case studies.}
}
@article{COMBI2021101895,
	title        = {Seamless conceptual modeling of processes with transactional and analytical data},
	journal      = {Data & Knowledge Engineering},
	volume       = {134},
	pages        = {101895},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101895},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000227},
	author       = {Carlo Combi and Barbara Oliboni and Mathias Weske and Francesca Zerbato},
	keywords     = {Conceptual modeling, Business process modeling, BPMN, Data modeling, Data warehouse, Decision support},
	abstract     = {In the field of Business Process Management (BPM), modeling business processes and related data is a critical issue since process activities need to manage data stored in databases. The connection between processes and data is usually handled at the implementation level, even if modeling both processes and data at the conceptual level should help designers in improving business process models and identifying requirements for implementation. Especially in data- and decision-intensive contexts, business process activities need to access data stored both in databases and data warehouses. In this paper, we complete our approach for defining a novel conceptual view that bridges process activities and data. The proposed approach allows the designer to model the connection between business processes and database models and define the operations to perform, providing interesting insights on the overall connected perspective and hints for identifying activities that are crucial for decision support.}
}
@article{AMEZAGAHECHAVARRIA2022102034,
	title        = {A modified attention mechanism powered by Bayesian Network for user activity analysis and prediction},
	journal      = {Data & Knowledge Engineering},
	volume       = {140},
	pages        = {102034},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102034},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000398},
	author       = {Alexis {Amezaga Hechavarria} and M. Omair Shafiq},
	keywords     = {Deep learning, Bayesian Networks, Click-stream data, Hybrid methods},
	abstract     = {Analyzing and predicting user activity is important in the current digital era with a lot of use-cases and applications. In this paper, we present an approach that facilitates a modification of the attention mechanism in a Transformer model. This work enables to improve the predictive capacity of a forecasting model which is progressively fed by somewhat erratic and small data generated by the early stages of online activity. The key element of the work is to use a Bayesian Network (BN) as a tool for feature engineering that helps to modify the attention mechanism in the Transformer model in that scenario. The model predicts the next activity on a sequence of online activities that the user will engage in while interacting with a Learning Management System (LMS). Click-stream data refers to a detailed log of how participants navigate through an online platform during a working session. The main application of our work is to improve the Predictor module of a smart hybrid-classifier for an LMS. Several configurations and architectures for the RNN-powered predictor, are tested and assessed. The results of the improved predictive capacity of this work can be useful to users in an online learning environment where early assistance in quasi-real time is required. This research answers the questions of how click-stream data can assist in refining the tasks of the Attention mechanism to improve the quality of the prediction. Performance is measured by the accuracy, right-content and first-state accuracy scores for the incoming sequence and compared across alternative models. The method also provides systematic customization of the attention mechanism in Transformers that can be applied to a range of problems involving click-stream data.}
}
@article{ZREIK2022102015,
	title        = {Matching and analysing conservation?restoration trajectories},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {102015},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102015},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X2200026X},
	author       = {Alaa Zreik and Zoubida Kedad},
	keywords     = {Trajectory matching, Ontology, Semantic trajectory, Semantic similarity, Trajectory analysis},
	abstract     = {The context of this work is an on-going project at the French National Library (BnF), which aims at providing predictions of the documents physical state based on their conservation?restoration histories. A document can be either in a good state and available to the readers, or damaged and unavailable to them. As libraries may contain millions of documents, the manual monitoring and analysis of their physical state is not realistic in practice. We therefore propose to analyse their conservation histories in order to derive reliable predictions of their physical state. To achieve this goal, we introduce in this paper the following contributions. First, we propose a representation of a document conservation history as a conservation?restoration trajectory, and we define its different types of events. We also propose a trajectory matching process that computes a similarity score between two conservation?restoration trajectories considering the terminological heterogeneity of the events, using an ontological model that represents the domain experts knowledge. Second, we provide a trajectory analysis process which identifies the most representative sequences of events of the deteriorated documents. Finally, we propose a prediction model for the physical state of the documents based on the trajectory analysis process. We present some experiments showing the effectiveness of the matching process as well as the prediction model.}
}
@article{ANANTHAJOTHI2022102092,
	title        = {Explicit and implicit oriented Aspect-Based Sentiment Analysis with optimal feature selection and deep learning for demonetization in India},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102092},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102092},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000830},
	author       = {K. Ananthajothi and K. Karthikayani and R. Prabha},
	keywords     = {Demonetization, Aspect-Based Sentiment Analysis, Polarity, Holoentropy, Neural Network, Self Adaptive Beetle Swarm Optimization, Recurrent Neural Network},
	abstract     = {Aspect-Based Sentiment Analysis (ABSA) is a popular scheme that looks for the prediction of the sentiment of positive characteristics in text. The sentiment of text sequences is analyzed by deep neural networks and attained noteworthy results. Conversely, these models also have some problems with the limitation of past-training word embeddings and lack of communication between the context and the particular characteristic of the attention scheme. The main part of this task is to develop the novel ABSA concerning both explicit and implicit aspects using demonetization dataset reviews from India. Initially, the pre-processing of online tweets is performed by stop word removal, tokenization, lower case conversion, and stemming. Further, the explicit aspects are extracted, as it is simple to extract from the sentence and the polarity score is computed. A machine learning algorithm termed as Neural Network (NN) is utilized that helps for training the data regarding the implicit aspects, and further, helps to differentiate properly for the testing data with exact polarity score. Optimal feature selection is performed using the Self Adaptive Beetle Swarm Optimization (SA-BSO). These optimal features are given to a deep structured architecture called Recurrent Neural Network (RNN) with hidden neuron optimization by SA-BSO, which categorizes the demonetization reviews into positive, negative, or neutral. While taking the findings, the accuracy of the offered SA-BSO-RNN is secured at 4.67%, 6.56%, 3.54%, and 7.12% progressed than PSO-RNN, FF-RNN, CSA-RNN, and BSO-RNN, at 3-fold analysis for dataset 1. Results show that the designed ABSA concerning both explicit and implicit aspects using the demonetization method that provides enriched performance with diverse performance metrics.}
}
@article{ZORGATI2022102025,
	title        = {Finding Internet of Things resources: A state-of-the-art study},
	journal      = {Data & Knowledge Engineering},
	volume       = {140},
	pages        = {102025},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102025},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000313},
	author       = {Hela Zorgati and Raoudha Ben Djemaa and Ikram Amous Ben Amor},
	keywords     = {Internet of Things, IoT resources discovery, State-of-the-art study},
	abstract     = {Since its emergence, the Internet of Things (IoT) has aimed to join the physical world to the virtual world with a basic vision, which is to create intelligent spaces in which users interact seamlessly with IoT objects. Therefore, the user will be surrounded by a large number of services offered by these connected objects which he/she will inevitably interact with them. However, the IoT environments are characterized by a large number of heterogeneous connected objects. These characteristics make it difficult to find their adjacent resources. Importantly, because the traditional discovery solutions are inefficient, therefore, a mechanism that makes it possible to dynamically inventory and control the resources present in an IoT environment is crucial. In fact, a resource discovery mechanism basically aims to dynamically and regularly update the base of available resources in the IoT environment. To meet these needs, many research works were conducted where many architectures have been proposed to provide solutions for resource discovery in the IoT environment. In this work, we provide a state-of-the-art study of resource discovery techniques in IoT environment. First, the resource discovery techniques are classified according to their search approaches. Then, a review of a selection of recent works that propose solutions for resource discovery in the IoT environment is provided. Finally, the study gives some implications for further study and a number of guidelines that helps to meet a large part of the requirements of a resource discovery solution in the IoT environment.}
}
@article{SANTRA2022102058,
	title        = {From base data to knowledge discovery ? A life cycle approach ? Using multilayer networks},
	journal      = {Data & Knowledge Engineering},
	volume       = {141},
	pages        = {102058},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102058},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000556},
	author       = {Abhishek Santra and Kanthi Komar and Sanjukta Bhowmick and Sharma Chakravarthy},
	keywords     = {Knowledge discovery life cycle, Multilayer networks, Conceptual modeling, Objectives to computable expressions, Decoupling-based analysis, Drill-down & visualization, Data and knowledge visualization},
	abstract     = {Analysis of complex data sets to infer/discover meaningful information/knowledge involves (after data collection and cleaning): (i) Modeling the data ? an approach for deriving a suitable representation of data for analysis, (ii) translating analysis objectives into computations on the generated model instance; these computations can be as simple as a query or a complex computation (e.g., community detection over multiple layers), (iii) computation of expressions generated ? considering efficiency and scalability, and (iv) drill-down of results to understand them clearly. Beyond this, it is also useful to visualize results for easier understanding. Covid-19 visualization dashboard presented in this paper is an example of this. This paper covers the above steps of data analysis life cycle using a representation (or model) that is gaining importance. With complex data sets containing multiple entity types and relationships, an appropriate model to represent the data is important. For these data sets, we first establish the advantages of Multilayer Networks (or MLNs) as a data model. Then we use an entity-relationship based approach to convert the data set into MLNs for a precise representation of the data set. After that, we outline how expected analysis objectives can be translated using keyword-mapping to aggregate analysis expressions. Finally, we demonstrate, through a set of example data sets and objectives, how the expressions corresponding to objectives are evaluated using an efficient decoupling-based approach. Results are further drilled down to obtain actionable knowledge from the data set. Using the widely popular Enhanced Entity Relationship (EER) approach for requirements representation, we demonstrate how to generate EER diagrams for data sets and further generate, algorithmically, MLNs as well as Relational schema for analysis and drill down, respectively. Using communities and centrality for aggregate analysis, we demonstrate the flexibility of the chosen model to support diverse set of objectives. We also show that compared to current analysis approaches, a ?decoupling-based? approach using MLNs is more appropriate as it preserves structure as well as semantics of the results and is very efficient. For this computation, we need to derive expressions for each analysis objective using the MLN model. We provide guidelines to translate English queries into analysis expressions based on keywords. Finally, we use several data sets to establish the effectiveness of modeling using MLNs and their analysis using the decoupling approach that has been proposed recently. For coverage, we use different types of MLNs for modeling, and community and centrality computations for analysis. The data sets used are from US commercial airlines, IMDb (a large international movie data set), the familiar DBLP (or bibliography database), and the Covid-19 data set. Our experimental analyses using the identified steps validate modeling, breadth of objectives that can be computed, and overall versatility of the life cycle approach. Correctness of results is verified, where possible, using independently available ground truth. Furthermore, we demonstrate drill-down that is afforded by this approach (due to structure and semantics preservation) for a better understanding and visualization of results.}
}
@article{ZHANG2022101983,
	title        = {Chinese named-entity recognition via self-attention mechanism and position-aware influence propagation embedding},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {101983},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.101983},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000039},
	author       = {Bo Zhang and Kehao Liu and Haowen Wang and Maozhen Li and Jianguo Pan},
	keywords     = {Chinese named-entity recognition, Self-attention, Position-aware influence propagation, Gaussian kernel},
	abstract     = {Chinese Named Entity Recognition (NER) has received extensive research attention in recent years. However, Chinese texts lack delimiters to divide the boundaries of words, and some existing approaches cannot capture the long-distance interdependent features. In this paper, we propose a novel end-to-end model for Chinese NER. A new global word boundary detection approach is designed to capture the semantic dependency via a self-attention mechanism to represent character embedding by assigning compatible weights for each character in a sentence. To improve the representation ability of Chinese named-entity boundaries, we introduce position-aware influence propagation with the Gaussian kernel for each character, which combines convergence propagation and radiation propagation. Convergence propagation mainly measures the influence of surrounding characters on the target character. The purpose of radiation propagation is to measure the range of influence of the target character on surrounding characters. The proposed method has been evaluated and shown to offer strong performance in two Chinese NER datasets: MSRA and PFR.}
}
@article{AUDEMARD2022102088,
	title        = {On the explanatory power of Boolean decision trees},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102088},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102088},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000799},
	author       = {Gilles Audemard and Steve Bellart and Louenas Bounia and Frédéric Koriche and Jean-Marie Lagniez and Pierre Marquis},
	keywords     = {eXplainable AI, Decision trees},
	abstract     = {Decision trees have long been recognized as models of choice in sensitive applications where interpretability is of paramount importance. In this paper, we examine the computational ability of Boolean decision trees for the explanation purpose. We focus on both abductive explanations (suited to explaining why a given instance has been classified as such by the decision tree at hand) and on contrastive explanations (suited to explaining why a given instance has not been classified by the decision tree as it was expected). More precisely, we are interested in deriving, minimizing, and counting abductive explanations and contrastive explanations. We prove that the set of all irredundant abductive explanations (also known as PI-explanations or sufficient reasons) for an instance given a decision tree can be exponentially larger than the size of the input (the instance and the decision tree). Therefore, generating the full set of sufficient reasons for an instance can be out of reach. In addition, deriving a single sufficient reason, though computationally easy when dealing with decision trees, does not prove enough in general; indeed, two sufficient reasons for the same instance may differ on many features. To deal with this issue and generate synthetic views of the set of all sufficient reasons, we define notions of relevant features and of necessary features that characterize the (possibly negated) features appearing in at least one or in every sufficient reason for an instance, and we show that they can be computed in polynomial time. We also introduce the notion of explanatory importance, that indicates how frequent each (possibly negated) feature is in the set of all sufficient reasons. We show how the explanatory importance of a (possibly negated) feature and the number of sufficient reasons for an instance can be obtained via a model counting operation, which turns out to be practical in many cases. We also explain how to enumerate minimum-size sufficient reasons. We finally show that, unlike sufficient reasons, the set of all contrastive explanations for an instance given a decision tree can be derived, minimized and counted in polynomial time.}
}
@article{GALLINUCCI2020101836,
	title        = {Mo.Re.Farming: A hybrid architecture for tactical and strategic precision agriculture},
	journal      = {Data & Knowledge Engineering},
	volume       = {129},
	pages        = {101836},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101836},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19306238},
	author       = {Enrico Gallinucci and Matteo Golfarelli and Stefano Rizzi},
	keywords     = {BI 2.0, Precision agriculture, Data integration},
	abstract     = {In this paper we propose an innovative architecture, called Mo.Re.Farming, for handling agricultural data in an integrated fashion and supporting decision making in the precision agriculture domain. This architecture is oriented to data analysis and is inspired by Business Intelligence 2.0 approaches. It is hybrid in that it couples traditional and big data technologies to integrate heterogeneous data, at different levels of detail, from several owned and open data sources; its goal is to demonstrate that such integration is feasible and beneficial in supporting situ-specific and large-scale analyses. The proposed architecture has been developed in the context of the Mo.Re.Farming project, aimed at providing a Decision Support System for agricultural technicians in the Emilia-Romagna region and to enable analyses related to the use of water and chemical resources. The architecture is fully deployed and serves as a hub for agricultural data in Emilia-Romagna; the integrated data are made available in open access mode and can be accessed through web interfaces and through a set of web services. The paper describes the architecture from the technological and functional points of view and discusses the Mo.Re.Farming project outcomes and lessons learnt.}
}
@article{SHEN2022101987,
	title        = {Entity alignment with adaptive margin learning knowledge graph embedding},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {101987},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.101987},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000052},
	author       = {Linshan Shen and Rongbo He and Shaobin Huang},
	keywords     = {Entity matching, Entity alignment, Knowledge representation, Knowledge embedding},
	abstract     = {A large number of knowledge graphs have been constructed at present. However, there is diversity and heterogeneity among different knowledge graphs. The relation and attribute of the knowledge graph contain rich semantic information, which helps construct the potential semantic representation of the knowledge graph. At present, the method based on knowledge representation is an important method of entity alignment, which can align entities by transforming them into spatial vectors. And it helps to reduce the heterogeneity among different knowledge domains. However, existing methods use the same optimization goal for triples under different relations, ignoring the difference between relationships. In this article, we put forward a kind of entity alignment method based on the TransE model and use adaptive margin strategies in training. At the same time, this paper studies the LSTM encoder model and the BERT pre-training model in the application of entity alignment. To enhance the model?s robustness, we put forward the triple selection strategy based on attribute similarity. Experimental results on real datasets show that this method is significantly improved compared with the baseline model.}
}
@article{XU2022102024,
	title        = {A random walk sampling on knowledge graphs for semantic-oriented statistical tasks},
	journal      = {Data & Knowledge Engineering},
	volume       = {140},
	pages        = {102024},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102024},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000295},
	author       = {Xiaoliang Xu and Qifan Hong and Yuxiang Wang and Jiahui Jin and Xinle Xuan and Tao Fu},
	keywords     = {Knowledge graph, Random walk sampling, Approximate estimation},
	abstract     = {A knowledge graph (KG) manages large-scale and real-world facts as a big graph in a schema-flexible manner, which has recently attracted considerable attention . It is very common that users deploy some statistical tasks on a KG to achieve the latent information of interest. There are two types of the statistical tasks, that are, topology-oriented and semantic-oriented statistical tasks. Many efforts have been made for the former one (e.g., finding the average degree of a KG). The basic idea is concluded as: estimating an approximate statistical result based on a random sample collected through a topology-aware KG sampling approach. Unfortunately, this method cannot be directly deployed to support semantic-oriented statistical tasks (e.g., achieving the average fuel economy of cars produced in Germany), because the topology-aware sampling does not consider the semantics of a KG (or we say the sample is collected only based on the topological information of a KG, while excluding the semantics of a KG), hence leading to a low-quality random sample and would significantly affect the accuracy. In this paper, we propose a semantic-aware random walk sampling on KGs to quickly and accurately collect samples that match the semantic constraint of the semantic-oriented statistical task, and obtain an approximate statistical result by well-designed unbiased estimators. Moreover, we propose an optimization on our semantic-aware sampling to improve the sampling efficiency. Finally, extensive experiments were conducted on our method, which confirmed the effectiveness and efficiency of our approach.}
}
@article{ABUL2022102006,
	title        = {Location-privacy preserving partial nearby friends querying in urban areas},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {102006},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102006},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000210},
	author       = {Osman Abul},
	keywords     = {Location based services, Location privacy, Proximity services, k-anonymity},
	abstract     = {This work studies the location-privacy preserving proximity querying in the context of proximity-based services (PBSs), a special kind of location-based services (LBSs). The users register with the trusted PBS provider and specify their own personalized location privacy profile to be enforced against the curious friends (other registered users). Due to the urban area constraint, the user mobility is only on the city road network which is modeled as a weighted directed graph. The users share their own precise locations with the PBS provider and also query the nearby friends, the metric of which is defined on the shortest path on the graph. The proposed location privacy model ensures the location anonymity of the friends on the graph. To this end, two anonymity models, called weak location k-anonymity and strong location k-anonymity, are introduced to protect against the identified consecutive attack scenarios. The attack scenarios model the belief of the attacker (the query issuer) on the whereabouts of the friends. The PBS provider simulates the belief of each attacker on every users? whereabouts and suppresses some friends from the query result to ensure the location anonymity of each and every user at all times. Effective and efficient algorithms, needing no cryptographic protocols, have been developed to provide weak/strong location k-anonymity. An extensive experimental evaluation, mainly addressing the issues of privacy/utility tradeoff and runtime efficiency, on two real graphs with a simulated mobility is presented.}
}
@article{GUIZZARDI2021101891,
	title        = {Types and taxonomic structures in conceptual modeling: A novel ontological theory and engineering support},
	journal      = {Data & Knowledge Engineering},
	volume       = {134},
	pages        = {101891},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101891},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000185},
	author       = {Giancarlo Guizzardi and Claudenir M. Fonseca and João Paulo A. Almeida and Tiago Prince Sales and Alessander Botti Benevides and Daniele Porello},
	keywords     = {Taxonomic structures, Ontology-driven conceptual modeling, OntoUML, Unified foundational ontology (UFO), Conceptual modeling, Ontologies},
	abstract     = {Types are fundamental for conceptual modeling and knowledge representation, being an essential construct in all major modeling languages in these fields. Despite that, from an ontological and cognitive point of view, there has been a lack of theoretical support for precisely defining a consensual view on types. As a consequence, there has been a lack of precise methodological support for users when choosing the best way to model general terms representing types that appear in a domain, and for building sound taxonomic structures involving them. For over a decade now, a community of researchers has contributed to the development of the Unified Foundational Ontology (UFO) - aimed at providing foundations for all major conceptual modeling constructs. At the core of this enterprise, there has been a theory of types specially designed to address these issues. This theory is ontologically well-founded, psychologically informed, and formally characterized. These results have led to the development of a Conceptual Modelling language dubbed OntoUML, reflecting the ontological micro-theories comprising UFO. Over the years, UFO and OntoUML have been successfully employed on conceptual model design in a variety of domains including academic, industrial, and governmental settings. These experiences exposed improvement opportunities for both the OntoUML language and its underlying theory, UFO. In this paper, we revise the theory of types in UFO in response to empirical evidence. The new version of this theory shows that many of OntoUML?s meta-types (e.g. kind, role, phase, mixin) should be considered not as restricted to substantial types but instead should be applied to model endurant types in general, including relator types, quality types, and mode types. We also contribute with a formal characterization of this fragment of the theory, which is then used to advance a new metamodel for OntoUML (termed OntoUML 2). To demonstrate that the benefits of this approach are extended beyond OntoUML, the proposed formal theory is then employed to support the definition of UFO-based lightweight Semantic Web ontologies with ontological constraint checking in OWL. Additionally, we report on empirical evidence from the literature, mainly from cognitive psychology but also from linguistics, supporting some of the key claims made by this theory. Finally, we propose a computational support for this updated metamodel.}
}
@article{PELDSZUS2021101907,
	title        = {Ontology-driven evolution of software security},
	journal      = {Data & Knowledge Engineering},
	volume       = {134},
	pages        = {101907},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101907},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000343},
	author       = {Sven Peldszus and Jens Bürger and Timo Kehrer and Jan Jürjens},
	keywords     = {Software engineering, Model-based security, Security context knowledge, Ontology evolution, Semantic editing patterns, Security compliance},
	abstract     = {Ontologies as a means to formally specify the knowledge of a domain of interest have made their way into information and communication technology. Most often, such knowledge is subject to continuous change, which demands for consistent evolution of ontologies and dependent artifacts. In this article, we study ontology evolution in the context of software security, where ontologies may be used to formalize the security context knowledge which is needed to properly implement security requirements. In this application scenario, techniques for detecting ontology changes and determining their semantic impact are required to maintain the security of a software-intensive system in response to changing security context knowledge. Our solution is capable of detecting semantic editing patterns, which may be customly defined using graph transformation rules, but it does not depend on information about editing processes such as persistently managed changelogs. We leverage semantic editing patterns for (i) generating system co-evolution proposals, (ii) adapting the configuration of standard security checks, and (iii) performing incremental security compliance analyses between co-evolved system models and the implementation. We demonstrate the feasibility of the approach using a realistic medical information system known as iTrust.}
}
@article{SOUIBGUI2022102003,
	title        = {An embedding driven approach to automatically detect identifiers and references in document stores},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {102003},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000209},
	author       = {Manel Souibgui and Faten Atigui and Sadok {Ben Yahia} and Samira {Si-Said Cherfi}},
	keywords     = {Business intelligence and analytics, ETL, Document stores, Join, Identifier discovery, Reference discovery},
	abstract     = {NoSQL stores have become ubiquitous since they offer a new cost-effective and schema-free system. Although NoSQL systems are widely accepted today, Business Intelligence & Analytics (BI&A) wields relational data sources. Exploiting schema-free data for analytical purposes is a challenge since it requires reviewing all the BI&A phases, particularly the Extract-Transform-Load (ETL) process, to fit big data sources as document stores. In the ETL process, the join of several collections, with a lack of explicitly known join fields is a significant dare. Detecting these fields manually is time and effort-consuming and infeasible in large-scale datasets. In this paper, we study the problem of discovering join fields automatically. We introduce an algorithm that aims to automatically detect both identifiers and references on several document stores. The modus operandi of our approach underscores three core stages: (i) global schema extraction; (ii) discovery of candidate identifiers; and (iii) identifying candidate pairs of identifier and reference fields. We use scoring features and pruning rules to discover true candidate identifiers from many initial ones efficiently. To find candidate pairs between several document stores, we put into practice node2vec as a graph embedding technique, which yields significant advantages while using syntactic and semantic similarity measures for pruning pointless candidates. Finally, we report our experimental findings that show encouraging results.}
}
@article{KOVACIC2022101948,
	title        = {OLAP Patterns: A pattern-based approach to multidimensional data analysis},
	journal      = {Data & Knowledge Engineering},
	volume       = {138},
	pages        = {101948},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101948},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000732},
	author       = {Ilko Kovacic and Christoph G. Schuetz and Bernd Neumayr and Michael Schrefl},
	keywords     = {Business intelligence, Conceptual modeling, Solution pattern, Online analytical processing, Data warehouse},
	abstract     = {Users of a business intelligence (BI) system employ an approach referred to as online analytical processing (OLAP) to view multidimensional data from different perspectives. Query languages, e.g., SQL or MDX, allow for flexible querying of multidimensional data but query formulation is often time-consuming and cognitively challenging for many users. Alternatives to using a query language, e.g., graphical OLAP clients, parameterized reports, or dashboards, are often not a full-blown alternative to using a query language. Experience in cooperative research projects with industry led to the following observations regarding the use of OLAP queries in practice. First, within the same organization, similar OLAP queries are repeatedly composed from scratch in order to satisfy similar information needs. Second, across different organizations and even domains, OLAP queries with similar structures are repeatedly composed from scratch. Finally, vague requirements regarding frequently composed OLAP queries in the early stages of a project potentially lead to rushed development in later stages, which can be alleviated by following best practices for OLAP query composition. In engineering, knowledge about best-practice solutions to frequently arising challenges is often documented and represented using patterns. In that spirit, an OLAP pattern describes a generic solution for composing a query that allows a BI user to satisfy a certain type of information need given fragments of a conceptual model. This paper introduces a formal definition of OLAP patterns as well as an expressive, flexible, and generally applicable definition language.}
}
@article{FONSECA2021101894,
	title        = {Multi-level conceptual modeling: Theory, language and application},
	journal      = {Data & Knowledge Engineering},
	volume       = {134},
	pages        = {101894},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101894},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000215},
	author       = {Claudenir M. Fonseca and João Paulo A. Almeida and Giancarlo Guizzardi and Victorio A. Carvalho},
	keywords     = {Multi-level modeling, Modeling language, Conceptual modeling, Methodologies and tools},
	abstract     = {In many important subject domains, there are central real-world phenomena that span across multiple classification levels. In these subject domains, besides having the traditional type-level domain regularities (classes) that classify multiple concrete instances, we also have higher-order type-level regularities (metaclasses) that classify multiple instances that are themselves types. Multi-Level Modeling aims to address this technical challenge. Despite the advances in this area in the last decade, a number of requirements arising from representation needs in subject domains have not yet been addressed in current modeling approaches. In this paper, we address this issue by proposing an expressive multi-level conceptual modeling language (dubbed ML2). We follow a principled language engineering approach in the design of ML2, constructing its abstract syntax as to reflect a fully axiomatized theory for multi-level modeling (termed MLT*). We show that ML2 enables the expression of a number of multi-level modeling scenarios that cannot be currently expressed in the existing multi-level modeling languages. A textual syntax for ML2 is provided with an implementation in Xtext. We discuss how the formal theory influences the language in two aspects: (i) by providing rigorous justification for the language?s syntactic rules, which follow MLT* theorems and (ii) by forming the basis for model simulation and verification. We show that the language can reveal problems in multi-level taxonomic structures, using Wikidata fragments to demonstrate the language?s practical relevance.}
}
@article{DONMEZ2022102075,
	title        = {Enhancing classification capacity of CNN models with deep feature selection and fusion: A case study on maize seed classification},
	journal      = {Data & Knowledge Engineering},
	volume       = {141},
	pages        = {102075},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102075},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000684},
	author       = {Emrah Dönmez},
	keywords     = {Haploid Maize Seed Identification, Deep features, Convolutional neural networks, Artificial learning, Image processing},
	abstract     = {Developing computer-assisted agricultural analysis systems using machine learning methods is a focused area in recent years. As in every field, it is aimed to improve the production process and product quality in agricultural applications with computer-assisted systems. The maize plant is a very important species in terms of providing sufficient food to the world?s population. The separation process of the haploid and diploid maize seeds is a critical issue in terms of maize breeding time and production efficiency. In this study, a haploid?diploid maize seed classification method is proposed using deep features obtained from convolutional neural networks (CNN). In the first stage, deep features were obtained from fully connected layers of different CNN models. Then, the best 100 features were selected by using the MRMR (Max-Relevance and Min-Redundancy) feature selection method for 1000 features obtained in each CNN model. These selected features have been fused according to different combinations of the CNN models. These fused features have been used in the training and testing stages of a conventional classifier method in the last stage. Eventually, according to the experimental results, it was determined that the general accuracy performance has been around 96.74%. It has been observed that the proposed approach exhibits high performance in the classification process of maize seeds.}
}
@article{SANTANGELO2022102093,
	title        = {Wikipedia searches and the epidemiology of infectious diseases: A systematic review},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102093},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102093},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000842},
	author       = {Omar Enzo Santangelo and Vincenza Gianfredi and Sandro Provenzano},
	keywords     = {Wikipedia, Epidemiology, Internet, Medical informatics computing, Vaccine-preventable diseases, Medical informatics},
	abstract     = {This review aims to collect, analyse and synthesize the available evidence that can be provided by Wikipedia for epidemiologic surveillance purposes. PRISMA guidelines were followed. PubMed/Medline and Scopus were consulted. Out of 238 retrieved articles, 16 articles were included in the systematic review. The most frequently assessed infectious disease was Influenza, followed by arboviruses and measles. Influenza studies show that Wikipedia could be considered a scientifically valid surveillance system that fills the main gaps in existing traditional surveillance systems. As regards arboviruses, searches on the Web have positively mediated the relationship between epidemiological data and the number of Wikipedia page visualization. Regarding measles, studies showed a strong/moderate temporal correlation between infectious disease notification bulletins and Wikipedia search trends. Despite the type of infectious agents, three main aims can be detected: (i) understand the public?s interest, (ii) explore the use of Wikipedia by organizations, and (iii) assess the accuracy of Wikipedia content. These new strategies for surveillance of infectious diseases should be implemented, to date they could be useful in supporting traditional surveillance.}
}
@article{ZHOU2022102060,
	title        = {A feature selection method based on term frequency difference and positive weighting factor},
	journal      = {Data & Knowledge Engineering},
	volume       = {141},
	pages        = {102060},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102060},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000568},
	author       = {Hongfang Zhou and Xiang Li and Chenguang Wang and Yiming Ma},
	keywords     = {Text classification, Feature selection, Term frequency, Document frequency},
	abstract     = {Firstly, a new concept of term frequency difference factor is proposed to balance the influences of term frequency and document frequency on feature selection. Secondly, the idea of positive weighting factor is advanced to balance the roles of the document frequency in the positive and negampared with six popular algorithms on six datasets using two classifiers of Naive Bayes and Support tive categories. And finally, a new feature selection algorithm based on term frequency difference and positive weighting factor, PWTF-TCM, is presented based on the two above concepts. In the experiments, PWTF-TCM is coVector Machines. The experimental results show that PWTF-TCM outperforms by 75% for Macro-F1 and 58.33% for Micro-F1. In addition, PWTF-TCM improves the classification accuracy by 4.58% compared with Trigonometric comparison measure.}
}
@article{FUMAGALLI2022102040,
	title        = {Conceptual model visual simulation and the inductive learning of missing domain constraints},
	journal      = {Data & Knowledge Engineering},
	volume       = {140},
	pages        = {102040},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102040},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X2200043X},
	author       = {Mattia Fumagalli and Tiago Prince Sales and Fernanda Araujo Baião and Giancarlo Guizzardi},
	keywords     = {Conceptual modeling, Constraints learning, Model validation, Inductive learning, Inductive Logic Programming, Model simulation},
	abstract     = {Conceptual modeling plays a fundamental role in information systems engineering, and in data and systems interoperability. To play their role as instruments for domain modeling, conceptual models must contain the exact set of constraints that represent the worldview of the relevant domain stakeholders. However, as empirical results show, conceptual modelers are subject to cognitive limitations and biases and, hence, in practice, they systematically produce models that fall short in that respect. Moreover, automating the process of formally assessing conceptual models in this sense (i.e., model validation) is notoriously hard, mainly because the intended worldview at hand lies in the mind of these stakeholders. In this paper, we provide a novel approach to model validation and automated constraint learning that combines, on one hand, Model Finding via the visual simulation of that model?s valid instances and, on the other hand, Inductive Logic Programming techniques. In our approach, we properly channel the results produced by the application of a visual model finding technique as input to a learning process. We then show how the approach is able to support the modeler in identifying missing constraints from the original model. The approach is validated against a catalog of empirically-elicited conceptual modeling anti-patterns. As we show here, the approach is able to support the automated learning of constraints that are needed to rectify a number of relevant anti-patterns in this catalog.}
}
@article{BIMONTE2020101832,
	title        = {A linear programming-based framework for handling missing data in multi-granular data warehouses},
	journal      = {Data & Knowledge Engineering},
	volume       = {128},
	pages        = {101832},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101832},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19301016},
	author       = {Sandro Bimonte and Libo Ren and Nestor Koueya},
	abstract     = {Data Warehouse (DW) and OLAP systems are first citizens of Business Intelligence tools. They are widely used in the academic and industrial communities for numerous different fields of application. Despite the maturity of DW and OLAP systems, with the advent of Big Data, more and more sources of data are available, and warehousing this data can lead to important quality issues. In this work, we focus on missing numerical and categorical in presence of aggregated facts. Motivated by the lack of a formal approach for the imputation of this kind of data taking into account all type of aggregation functions (distributive, algebraic and holistic), we propose an new methodology based on linear programming. Our methodology allows dealing with the relaxed constraints over classical SQL aggregation functions. The proposed approach is tested on two well-known datasets. Experiments show the effectiveness of the proposed approach.}
}
@article{SFAXI2020101862,
	title        = {DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects},
	journal      = {Data & Knowledge Engineering},
	volume       = {130},
	pages        = {101862},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101862},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19303830},
	author       = {Lilia Sfaxi and Mohamed Mehdi Ben Aissa},
	keywords     = {Big Data, Methodology, Decisional systems, Agile, Data governance, Data quality},
	abstract     = {Decision making is the lifeblood of the enterprise ? from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being ?data driven? is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.}
}
@article{LIMA2020101866,
	title        = {An analysis of the collaboration network of the International Conference on Conceptual Modeling at the Age of 40},
	journal      = {Data & Knowledge Engineering},
	volume       = {130},
	pages        = {101866},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101866},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X20303529},
	author       = {Lucas Henrique C. Lima and Alberto H.F. Laender and Mirella M. Moro and José Palazzo M. {de Oliveira}},
	keywords     = {Collaboration networks, Coauthorship networks, Databases, Conceptual modeling, ER},
	abstract     = {The International Conference on Conceptual Modeling celebrated 40 years of existence at its 38th edition held in Salvador, Brazil, on 4?7 November 2019. As one of the most traditional and well-known conferences in the database area, it has its origins on the Entity-Relationship Model proposed by Peter P. Chen in 1975. To celebrate such an accomplishment, this article goes over the ER history from distinct perspectives. Overall, we investigate the complete ER collaboration network built on bibliographic data collected from DBLP, comprising its 38 editions held from 1979 to 2019. We analyze several aspects regarding the evolution of its network metrics, such as degree, clustering coefficient and average shortest path, over the four decades. In particular, we analyze the role of the most engaged ER authors, the number of distinct authors, institutions and published papers, and the evolution of some of the most frequent terms presented in the titles of its papers, as well as the influence and impact of the prominent ER authors.}
}
@article{LATIFIPAKDEHI2021101922,
	title        = {DBHC: A DBSCAN-based hierarchical clustering algorithm},
	journal      = {Data & Knowledge Engineering},
	volume       = {135},
	pages        = {101922},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101922},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000495},
	author       = {Alireza Latifi-Pakdehi and Negin Daneshpour},
	keywords     = {Clustering, Density based clustering, DBSCAN, Hierarchical clustering},
	abstract     = {Clustering is the process of partitioning objects of a dataset into some groups according to similarities and dissimilarities between its objects. DBSCAN is one of the most important clustering algorithms in the density based approach of clustering. In spite of the numerous advantages of the DBSCAN algorithm, it has two important input parameters, MinPts and Eps, which determining their values is still a great challenge. This problem arises because values of these parameters are heavily dependent on data distribution. To overcome this challenge, firstly features of these parameters are investigated and the data distribution are analyzed. Then a DBSCAN-based hierarchical clustering (DBHC) method is proposed in this paper in order to fix this challenge. For this purpose, DBHC first determines values of these parameters using the notion of k nearest neighbor and k-dist plot. Because most of the real world data is not distributed uniformly, it is needed to be produced several values for the Eps parameter. Then, DBHC executes the DBSCAN algorithm several times based on the number of Eps produced earlier. Finally, DBHC method merges obtained clusters if the number of produced clusters is larger than the number which has estimated by the user. To evaluate the performance of the DBHC method, several experiments were performed on some of benchmark datasets of UCI database. Obtained results were compared with other previous works. The obtained results consistently showed that the DBHC method led to better results in comparison to the other works.}
}
@article{MAASS2021101909,
	title        = {Pairing conceptual modeling with machine learning},
	journal      = {Data & Knowledge Engineering},
	volume       = {134},
	pages        = {101909},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101909},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000367},
	author       = {Wolfgang Maass and Veda C. Storey},
	keywords     = {Conceptual modeling, Machine learning, Methodologies and tools, Models, Database management, Framework for incorporating conceptual modeling into data science projects, Artificial intelligence},
	abstract     = {Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.}
}
@article{LECLAIR2022102044,
	title        = {Architecture for ontology-supported multi-context reasoning systems},
	journal      = {Data & Knowledge Engineering},
	volume       = {140},
	pages        = {102044},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102044},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000453},
	author       = {Andrew LeClair and Jason Jaskolka and Wendy MacCaull and Ridha Khedri},
	keywords     = {Software architecture, Knowledge-based system, Ontology-based system, Intelligent system},
	abstract     = {Modern smart systems such as those needed for Industry 4.0 integrate data from various sources and increasingly require that data be contextualized with domain knowledge. The integration and contextualization of data allows for the advanced reasoning needed to generate knowledge grounded in the data under consideration. In this paper, we propose an architecture for an ontology-supported multi-context reasoning system which inherently supports a number of desired system qualities including data transparency, system interactivity, and graceful aging. The architecture is inspired by the Presentation?Abstraction?Control architecture style, which is an interaction-based architecture. Our architecture uses a two level hierarchy with three agents and can incorporate and utilize multiple contexts. It is flexible, supporting an interface between data and users, highly interactive, and easily maintained. The evolution of data is isolated to a single component of the system and therefore does not cascade to several others. A domain of application can be easily determined by the use of archetypes and domain-specification components. Our architecture is demonstrated using a case study involving data from the city of San Francisco.}
}
@article{PLAZAS2022101971,
	title        = {Sense, Transform & Send for the Internet of Things (STS4IoT): UML profile for data-centric IoT applications},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {101971},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101971},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000926},
	author       = {Julian Eduardo Plazas and Sandro Bimonte and Michel Schneider and Christophe {de Vaulx} and Pietro Battistoni and Monica Sebillo and Juan Carlos Corrales},
	keywords     = {Data-centric conceptual modelling, Model-driven architecture, Automatic code generation, Internet of Things},
	abstract     = {The Internet of Things is currently one of the most representative sources of Big Data. It can acquire real-time data from multiple spatially distributed points, allowing for the extraction of valuable insights. However, an appropriate integration, processing, and analysis of these data depends on several factors starting from the correct definition of the information systems. This paper introduces STS4IoT, a UML profile and automatic code-generation tool for model-driven IoT, to address this issue. STS4IoT allows designing and implementing an IoT application from the required data only, bridging the gaps between the IoT and database design worlds. The IoT data design includes both different in-network transformations and the join of streams from multiple sources. Besides, it follows the Model-Driven Architecture (MDA) guidelines to provide abstraction levels oriented to the different roles participating in the application design. The STS4IoT validation shows it has an excellent structure and is highly understandable. Its instance models are well-formed, highly abstract and readable. And the automatic implementation tool can generate complete code for complex real-world applications involving multiple IoT devices. Then, STS4IoT simplifies the definition and development of IoT applications and their integration into other information systems, such as stream data warehouses.}
}
@article{RAMACHANDRAN2021101913,
	title        = {A novel domain and event adaptive tweet augmentation approach for enhancing the classification of crisis related tweets},
	journal      = {Data & Knowledge Engineering},
	volume       = {135},
	pages        = {101913},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101913},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000409},
	author       = {Dharini Ramachandran and Parvathi R.},
	keywords     = {Twitter analytics, Tweets augmentation, Deep learning, Crisis analytics, Text Augmentation},
	abstract     = {One of the purposes of detecting the crisis related tweets is the ability to single out the tweets that provide information about the helps needed and offered. Classification of such tweets is difficult because of the unavailability of sufficient annotated tweets in those categories. To facilitate such classifications, a domain and event adaptive augmentation approach is proposed. The main objective of the research is to enhance the classification of crisis related tweets that have less training samples. The proposed algorithms are designed to integrate the innate domain and event information during the selection of words for augmentation. Components such as CrisisLex lexicon, Word2Vec embeddings and WordNet are utilized for the proposed augmentation. Experimentation is carried out to substantiate the benefits of augmentation. Results indicate increased performance of the classifier when provided with the expanded dataset including the augmented and original tweets. To combat the problem of overfitting and class imbalance arising due to the lesser training samples, a novel tweets augmentation algorithm can be utilized. The advantage in the proposed algorithms is the ability to retain the structure and inherent nature of the tweets during the augmentation.}
}
@article{BUI2022102077,
	title        = {Combining Specialized Word Embeddings and Subword Semantic Features for Lexical Entailment Recognition},
	journal      = {Data & Knowledge Engineering},
	volume       = {141},
	pages        = {102077},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102077},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000696},
	author       = {Van-Tan Bui and Phuong-Thai Nguyen and Van-Lam Pham},
	keywords     = {Lexical entailment, Hypernymy detection, Taxonomic relation, Lexical entailment recognition},
	abstract     = {The challenge of Lexical Entailment Recognition (LER) aims to identify the is-a relation between words. This problem has recently received attention from researchers in the field of natural language processing because of its application to varied downstream tasks. However, almost all prior studies have only focused on datasets that include single words; thus, how to handle compound words effectively is still a challenge. In this study, we propose a novel method called LERC (Lexical Entailment Recognition Combination) to solve this problem by combining embedding representations and subword semantic features. For this aim, firstly a specialized word embedding model for the LER tasks is trained. Secondly, subword semantic information of word pairs is exploited to compute another feature vector. This feature vector is combined with embedding vectors for supervised classification. We considered three LER tasks, including Lexical Entailment Detection, Lexical Entailment Directionality, and Lexical Entailment Determination. Experimental results conducted on several benchmark datasets in English and Vietnamese languages demonstrated that the subword semantic feature is useful for these tasks. Moreover, LERC outperformed several methods published recently.}
}
@article{RAMOSTUBINO2022102094,
	title        = {Towards a better identification of Bitcoin actors by supervised learning},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102094},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102094},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000854},
	author       = {Rafael {Ramos Tubino} and Céline Robardet and Rémy Cazabet},
	keywords     = {Machine learning application, Blockchain actor identification},
	abstract     = {Bitcoin is the most widely used crypto-currency, and one of the most studied. Thanks to the open nature of the Blockchain, transaction records are freely accessible and can be analyzed by anyone. The first step in most analytics work is to group anonymous addresses into a set of addresses, called aggregates, that are meant to correspond to unique actors. In this paper, we propose new methods to discover more accurate address aggregates using supervised learning. We introduce a way to create a labeled training set based on reliable heuristics and external information, and propose two methods. The first method automatically finds address aggregates from a set of transactions. The second one improves an address aggregate of a target actor by specializing the training for a single actor. We empirically validate our results on large-scale datasets. A striking result of our analysis is that training a model to recognize the change addresses of a particular actor is more efficient than using a larger dataset that does not target that particular actor. In doing so, we clearly show the feasibility and interest of supervised machine learning to identify Bitcoin actors.}
}
@article{ANDREASEN2020101848,
	title        = {Natural logic knowledge bases and their graph form},
	journal      = {Data & Knowledge Engineering},
	volume       = {129},
	pages        = {101848},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101848},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18306165},
	author       = {Troels Andreasen and Henrik Bulskov and Per Anker Jensen and Jørgen Fischer Nilsson},
	keywords     = {Natural Logic, Knowledge management applications, Ontologies, Query, Metalogic, Bioinformatics databases},
	abstract     = {This paper describes how knowledge bases can be represented in and reasoned with in natural logic. Natural logic is a regimented fragment of natural language possessing a well-defined logical semantics. As such, natural logic may be considered an attractive alternative among the various knowledge representation logics such as description logics. Our version of natural logic expands formal ontologies with affirmative propositions expressing a variety of relationships between concepts. It comprises (nested) restrictive relative clauses and prepositional phrases and, as a new construct, adverbial prepositional phrases. The natural logic knowledge base is to be used for deductive query answering applying inference rules. This is facilitated by introduction of Datalog as an embedding meta-logic. The inference rules are stated in Datalog and act directly on the natural logic formulations. The knowledge base propositions are decomposed into a graph form enabling path finding between concepts. The examples in the paper are derived from text source life-science descriptions.}
}
@article{ESTRADATORRES2021101897,
	title        = {Discovering business process simulation models in the presence of multitasking and availability constraints},
	journal      = {Data & Knowledge Engineering},
	volume       = {134},
	pages        = {101897},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101897},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000240},
	author       = {Bedilia Estrada-Torres and Manuel Camargo and Marlon Dumas and Luciano García-Bañuelos and Ibrahim Mahdy and Maksym Yerokhin},
	keywords     = {Multitasking, Process mining, Process simulation, Resource availability, Timetables},
	abstract     = {Business process simulation is a versatile technique for quantitative analysis of business processes. A well-known limitation of process simulation is that the accuracy of the simulation results is limited by the faithfulness of the process model and simulation parameters given as input to the simulator. To tackle this limitation, various authors have proposed to discover simulation models from process execution logs, so that the resulting simulation models more closely match reality. However, existing techniques in this field make certain assumptions about resource behavior that do not typically hold in practice, including: (i) that each resource performs one task at a time; and (ii) that resources are continuously available (24/7). In reality, resources may engage in multitasking behavior and they work only during certain periods of the day or the week. This article proposes an approach to discover process simulation models from execution logs in the presence of multitasking and availability constraints. To account for multitasking, we adjust the processing times of tasks in such a way that executing the multitasked tasks sequentially with the adjusted times is equivalent to executing them concurrently with the original times. Meanwhile, to account for availability constraints, we use an algorithm for discovering calendar expressions from collections of time-points to infer resource timetables from an execution log. We then adjust the parameters of this algorithm to maximize the similarity between the simulated log and the original one. We evaluate the approach using real-life and synthetic datasets. The results show that the approach improves the accuracy of simulation models discovered from execution logs both in the presence of multitasking and availability constraints.}
}
@article{ALEKSEEV2021101921,
	title        = {TopicBank: Collection of coherent topics using multiple model training with their further use for topic model validation},
	journal      = {Data & Knowledge Engineering},
	volume       = {135},
	pages        = {101921},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101921},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000483},
	author       = {Vasiliy Alekseev and Evgeny Egorov and Konstantin Vorontsov and Alexey Goncharov and Kaidar Nurumov and Timur Buldybayev},
	keywords     = {Topic modeling, Multiple model training, Topic coherence, Stability, Regularization},
	abstract     = {Probabilistic topic modeling of a text collection is a tool for unsupervised learning of the inherent thematic structure of the collection. Given only the text of documents as input, the topic model aims to reveal latent topics as probability distributions over words. The shortcomings of topic models are that they are unstable in the sense that topics may depend on the random initialization, and incomplete in the sense that each new run of the model on the same collection may discover some new topics. This means that data exploration using topic modeling usually requires too many experiments for looking over many topic models and tuning their parameters in search of a model that describes the data best. To deal with the instability and incompleteness of topic models, we propose to gradually accumulate interpretable topics in a ?topic bank? using multiple model training. To add topics into the bank, we learn a child level in a hierarchical topic model, then we analyze the coherence of child subtopics and their relationships with parent bank topics in order to exclude irrelevant and duplicate subtopics instead of adding them to the bank. Then we introduce a new way to topic model evaluation by comparing the topics found by the model with the ones that were collected beforehand in a bank. Our experiments with several datasets and topic models show that the proposed method does help in finding a model with more interpretable topics.}
}
@article{RAFIEI2021101908,
	title        = {Group-based privacy preservation techniques for process mining},
	journal      = {Data & Knowledge Engineering},
	volume       = {134},
	pages        = {101908},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101908},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000355},
	author       = {Majid Rafiei and Wil M.P. {van der Aalst}},
	keywords     = {Responsible process mining, Privacy preservation, Result utility, Data utility, Event data},
	abstract     = {Process mining techniques help to improve processes using event data. Such data are widely available in information systems. However, they often contain highly sensitive information. For example, healthcare information systems record event data that can be utilized by process mining techniques to improve the treatment process, reduce patient?s waiting times, improve resource productivity, etc. However, the recorded event data include highly sensitive information related to treatment activities. Responsible process mining should provide insights about the underlying processes, yet, at the same time, it should not reveal sensitive information. In this paper, we discuss the challenges regarding directly applying existing well-known group-based privacy preservation techniques, e.g., k-anonymity, l-diversity, etc, to event data. We provide formal definitions of attack models and introduce an effective group-based privacy preservation technique for process mining. Our technique covers the main perspectives of process mining including control-flow, time, case, and organizational perspectives. The proposed technique provides interpretable and adjustable parameters to handle different privacy aspects. We employ real-life event data and evaluate both data utility and result utility to show the effectiveness of the privacy preservation technique. We also compare this approach with other group-based approaches for privacy-preserving event data publishing.}
}
@article{HOLUBOVA2021101932,
	title        = {Evolution management in multi-model databases},
	journal      = {Data & Knowledge Engineering},
	volume       = {136},
	pages        = {101932},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101932},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000598},
	author       = {Irena Holubová and Michal Vavrek and Stefanie Scherzinger},
	keywords     = {Multi-model databases, Evolution management, Change propagation},
	abstract     = {Following the Gartner predictions, most of the DBMSs, both traditional relational and NoSQL, have become multi-model. However, this functionality brought on plenty of related issues. One of the most complex ones is evolution management and respective propagation of changes to all affected parts of the system. In this paper we introduce our prototype implementation called MM-evolver which enables to carry out user-triggered schema modification operations over a multi-model database, and propagates them across all models. As a novelty, MM-evolver supports both inter- and intra-model schema modification operators. To the best of our knowledge, ours is the first tool addressing evolution management in the world of multi-model databases.}
}
@article{MIRBAGHERI2021101924,
	title        = {Mining high utility patterns in interval-based event sequences},
	journal      = {Data & Knowledge Engineering},
	volume       = {135},
	pages        = {101924},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101924},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000513},
	author       = {S. Mohammad Mirbagheri and Howard J. Hamilton},
	keywords     = {High utility interval-based, Utility mining, Sequential pattern mining, Temporal pattern, Event sequence},
	abstract     = {Sequential pattern mining is an interesting research area with broad range of applications. Most prior research on sequential pattern mining has considered point-based data where events occur instantaneously. However, in many application domains, events persist over intervals of time of varying lengths. Furthermore, traditional frameworks for sequential pattern mining assume all events have the same weight or utility. This simplifying assumption neglects the opportunity to find informative patterns in terms of utilities, such as profits. To address these issues, we incorporate the concept of utility into interval-based sequences and define a framework to mine high utility patterns in interval-based sequences i.e., patterns whose utility meets or exceeds a minimum threshold. In the proposed framework, the utility of events is considered while assuming multiple events can occur coincidentally and persist over varying periods of time. An algorithm named High Utility Interval-based Pattern Miner (HUIPMiner) is proposed and applied to real datasets. To achieve an efficient solution, HUIPMiner is augmented with two effective pruning strategies. Experimental results show that HUIPMiner is an effective solution to the problem of mining high utility interval-based sequences. Moreover, it is shown that the execution time of the algorithm is reduced when the proposed pruning strategies are applied.}
}
@article{GENEST2022102099,
	title        = {French translation of a dialogue dataset and text-based emotion detection},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102099},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102099},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000908},
	author       = {Pierre-Yves Genest and Laurent-Walter Goix and Yasser Khalafaoui and El?d Egyed-Zsigmond and Nistor Grozavu},
	keywords     = {Affective computing, Classification, Emotion detection, Natural language processing, Methodologies and tools},
	abstract     = {Chatbots allow computer programs to interact naturally with a user. However, they remain limited due to their lack of sensitivity to the user?s state of mind and emotions. This sensitivity will allow the chatbots to provide more accurate answers. Text-based emotion detection has already been explored for the english language (Chatterjee et al., 2019), yet no satisfying french dataset is available. We propose to translate the emotion corpus of multi-party conversation EmotionLines, which is based on the Friends TV show, by exploiting its french broadcasting. Our translation-based dataset generation method is adaptable to any dataset deriving from foreign movies, or TV shows broadcasted in french. Using this translated dataset, we propose a classifier based on BERT, able to detect the user?s emotion from text. It takes into account the context of the discussion to improve its inferences.}
}
@article{HEWASINGHAGE2021101896,
	title        = {Managing polyglot systems metadata with hypergraphs},
	journal      = {Data & Knowledge Engineering},
	volume       = {134},
	pages        = {101896},
	year         = {2021},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2021.101896},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X21000239},
	author       = {Moditha Hewasinghage and Alberto Abelló and Jovan Varga and Esteban Zimányi},
	keywords     = {Metadata management, NoSQL, Polystore},
	abstract     = {A single type of data store can hardly fulfill every end-user requirements in the NoSQL world. Therefore, polyglot systems use different types of NoSQL datastores in combination. However, the heterogeneity of the data storage models makes managing the metadata a complex task in such systems, with only a handful of research carried out to address this. In this paper, we propose a hypergraph-based approach for representing the catalog of metadata in a polyglot system. Taking an existing common programming interface to NoSQL systems, we extend and formalize it as hypergraphs. Then, we define design constraints and query transformation rules for three representative data store types. Next, we propose a simple query rewriting algorithm from the metadata of the catalog to underlying data store specific ones and provide a prototype implementation. Furthermore, we introduce a storage statistics estimator on the underlying data stores. Finally, we show the feasibility of our approach on a use case of an existing polyglot system, and its usefulness in metadata and physical query path calculations.}
}
@article{RAM2022102007,
	title        = {OFES: Optimal feature evaluation and selection for multi-class classification},
	journal      = {Data & Knowledge Engineering},
	volume       = {139},
	pages        = {102007},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102007},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000222},
	author       = {Vallam Sudhakar Sai Ram and Namrata Kayastha and Kewei Sha},
	keywords     = {Feature evaluation, Feature selection, Classification},
	abstract     = {The complexity and accuracy of classification algorithms largely depend on the size and the quality of the feature set used to build classifiers. Feature evaluation and selection are critical steps to decide a small set of high-quality features to build accurate and efficient classifiers since low-quality features not only have negative impacts on classification results but also increase the complexity of classification algorithms. Current popular feature selection algorithms are not sufficient in selecting a set of high-quality features and discarding low-quality features, especially for streaming data. This paper proposes a novel and efficient approach, optimal feature evaluation and selection (OFES), to evaluate and select high-quality features for multi-class classification. OFES first measures the difference between any two classes based on the feature that is to be evaluated. Then, it defines two quantitative measures to evaluate quality of the feature and identify high-quality features. Applying OFES in a multi-class classification application that identifies users based on their arm movement patterns, we find when compared with other popular feature evaluation and selection approaches, such as Information Gain Feature Ranking and Random Projections with Matlab feature ranking, OFES identifies a set of high-quality features that improves the accuracy of classification regardless of different classification algorithms. It also demonstrates great scalability with the increase of number of classes and yields a higher accuracy of 95%.}
}
@article{LUKYANENKO2022102062,
	title        = {System: A core conceptual modeling construct for capturing complexity},
	journal      = {Data & Knowledge Engineering},
	volume       = {141},
	pages        = {102062},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102062},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X2200057X},
	author       = {Roman Lukyanenko and Veda C. Storey and Oscar Pastor},
	keywords     = {System, Systemism, Conceptual modeling, Complexity, CESM+, Emergent properties, Ontology, Bunge Systemist Ontology (BSO), Retrospective case study, Citizen science},
	abstract     = {The digitalization of human society continues at a relentless rate. However, to develop modern information technologies, the increasing complexity of the real-world must be modeled, suggesting the general need to reconsider how to carry out conceptual modeling. This research proposes that the often-overlooked notion of ?system? should be a separate, and core, conceptual modeling construct and argues for incorporating it and related concepts, such as emergence, into existing approaches to conceptual modeling. The work conducts a synthesis of the ontology of systems and general systems theory. These modeling foundations are then used to propose a CESM+ template for conducing systems-grounded conceptual modeling. Several new conceptual modeling notations are introduced. The systemist modeling is then applied to a case study on the development of a citizen science platform. The case demonstrates the potential contributions of the systemist approach and identifies specific implications of explicit modeling with systems for theory and practice. The paper provides recommendations for how to incorporate systems into existing projects and suggests fruitful opportunities for future conceptual modeling research.}
}
@article{FOPA2022102095,
	title        = {A parameter-free KNN for rating prediction},
	journal      = {Data & Knowledge Engineering},
	volume       = {142},
	pages        = {102095},
	year         = {2022},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2022.102095},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X22000866},
	author       = {Medjeu Fopa and Modou Gueye and Samba Ndiaye and Hubert Naacke},
	keywords     = {Mining methods and algorithms, Recommendation systems, Collaborative filtering, K nearest neighbors},
	abstract     = {Among the most popular collaborative filtering algorithms are methods based on the K nearest neighbors (KNN). In their basic operation, KNN methods consider a fixed number of neighbors to make recommendations. However, it is not easy to choose an appropriate number of neighbors. Thus, it is generally fixed by calibration to avoid inappropriate values which would negatively affect the accuracy of the recommendations. In the literature, some authors have addressed the problem of dynamically finding an appropriate number of neighbors. But they use additional parameters which limit their proposals because these parameters also require calibration. In this paper, we propose a parameter-free KNN method for rating prediction. It is able to dynamically select an appropriate number of neighbors to use. The experiments that we did on four publicly available datasets demonstrate the efficiency of our proposal. It rivals those of the state of the art in their best configurations.}
}
@article{LAI2019101738,
	title        = {Stance polarity in political debates: A diachronic perspective of network homophily and conversations on Twitter},
	journal      = {Data & Knowledge Engineering},
	volume       = {124},
	pages        = {101738},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101738},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19300187},
	author       = {Mirko Lai and Marcella Tambuscio and Viviana Patti and Giancarlo Ruffo and Paolo Rosso},
	keywords     = {Stance, Political debates, Homophily, Twitter},
	abstract     = {In the last decade, social media gained a very significant role in public debates, and despite the many intrinsic difficulties of analyzing data streaming from on-line platforms that are poisoned by bots, trolls, and low-quality information, it is undeniable that such data can still be used to test the public opinion and overall mood and to investigate how individuals communicate with each other. With the aim of analyzing the debate in Twitter on the 2016 referendum on the reform of the Italian Constitution, we created an Italian annotated corpus for stance detection for automatically estimating the stance of a relevant number of users. We take into account a diachronic perspective to shed lights on users? opinion dynamics. Furthermore, different types of social network communities, based on friendships, retweets, quotes, and replies were investigated, in order to analyze the communication among users with similar and divergent viewpoints. We observe particular aspects of users? behavior. First, our analysis suggests that users tend to be less explicit in expressing their stances after the outcome of the vote; simultaneously, users who exhibit a high number of cross-stance relations tend to become less polarized or to adopt a more neutral style in the following phase of the debate. Second, despite social media networks are generally aggregated in homogeneous communities, we highlight that the structure of the network can strongly change when different types of social relations are considered. In particular, networks defined by means of reply-to messages exhibit inverse homophily by stance, and users use more often replies for expressing diverging opinions, instead of other forms of communication. Interestingly, we also observe that the political polarization increases forthcoming the election and decreases after the election day.}
}
@article{TERHORST2020101764,
	title        = {Learning soft domain constraints in a factor graph model for template-based information extraction},
	journal      = {Data & Knowledge Engineering},
	volume       = {125},
	pages        = {101764},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101764},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19300345},
	author       = {Hendrik {ter Horst} and Matthias Hartung and Philipp Cimiano and Nicole Brazda and Hans Werner Müller and Roman Klinger},
	keywords     = {Template-based information extraction, Slot-filling, Probabilistic graphical models, Learning domain constraints, Database population},
	abstract     = {The ability to accurately extract key information from textual documents is necessary in several downstream applications e.g., automatic knowledge base population from text, semantic information retrieval, question answering, or text summarization. However, information extraction (IE) systems are far from being errorless and in some cases commit errors that seem obvious to a human expert as they violate common sense or domain knowledge. Towards improving the performance of IE systems, we focus on the question of how domain knowledge can be incorporated into IE models to reduce the number of spurious extractions. Starting from the assumption that such domain knowledge cannot be incorporated explicitly and manually by domain experts due to the amount of effort and technical complexities involved, we propose a machine learning approach in which domain constraints are acquired as a byproduct of learning a model that learns to extract key information in a supervised setting. We frame the task as a template-based information extraction problem in which several dependent slots need to be automatically filled and propose a factor graph based approach to model the joint distribution of slot assignments given a text. Beyond using standard textual features in factors that score the compatibility of slot fillers in relation to the text, we use additional features that are text-independent and capture soft domain constraints. During the training process, these constraints receive a weight as part of the parameter learning process indicating how strongly a constraint should be enforced. These domain constraints are thus ?soft? in the sense that they can be violated, but the system learns to penalize solutions that violate them. The soft constraints we introduce come in two flavors: on the one hand we incorporate information about the mean of numerical attributes and use features that indicate how far a certain value is from the mean. We call these features single slot soft constraints. On the other hand, we model the pairwise compatibility between slot filler assignments independent of the textual context, thus modeling the (domain) compatibility of the slot assignments. We call the latter ones pairwise slot soft constraints. As main result of our work, we show that learning pairwise slot soft constraints improves the performance of our extraction model compared to single slot soft constraints by up to 6 points in F1, leading to an F1 score of 0.91 for individual template types. Further, the human readable output format of our model enables the extraction and interpretation of the learned soft constraints. Based on this, we show in an evaluation by domain experts that more than 68% of the learned soft constraints are regarded as plausible.}
}
@article{TEKLI2018133,
	title        = {Full-fledged semantic indexing and querying model designed for seamless integration in legacy RDBMS},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {133--173},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.007},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16301835},
	author       = {Joe Tekli and Richard Chbeir and Agma J.M. Traina and Caetano Traina and Kokou Yetongnon and Carlos Raymundo Ibanez and Marc {Al Assad} and Christian Kallas},
	keywords     = {Semantic queries, Inverted index, NoSQL indexing, Semantic network, Semantic-aware data processing, Textual databases},
	abstract     = {In the past decade, there has been an increasing need for semantic-aware data search and indexing in textual (structured and NoSQL) databases, as full-text search systems became available to non-experts where users have no knowledge about the data being searched and often formulate query keywords which are different from those used by the authors in indexing relevant documents, thus producing noisy and sometimes irrelevant results. In this paper, we address the problem of semantic-aware querying and provide a general framework for modeling and processing semantic-based keyword queries in textual databases, i.e., considering the lexical and semantic similarities/disparities when matching user query and data index terms. To do so, we design and construct a semantic-aware inverted index structure called SemIndex, extending the standard inverted index by constructing a tightly coupled inverted index graph that combines two main resources: a semantic network and a standard inverted index on a collection of textual data. We then provide a general keyword query model with specially tailored query processing algorithms built on top of SemIndex, in order to produce semantic-aware results, allowing the user to choose the results' semantic coverage and expressiveness based on her needs. To investigate the practicality and effectiveness of SemIndex, we discuss its physical design within a standard commercial RDBMS allowing to create, store, and query its graph structure, thus enabling the system to easily scale up and handle large volumes of data. We have conducted a battery of experiments to test the performance of SemIndex, evaluating its construction time, storage size, query processing time, and result quality, in comparison with legacy inverted index. Results highlight both the effectiveness and scalability of our approach.}
}
@article{DANENAS2020101822,
	title        = {Natural language processing-enhanced extraction of SBVR business vocabularies and business rules from UML use case diagrams},
	journal      = {Data & Knowledge Engineering},
	volume       = {128},
	pages        = {101822},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101822},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1930299X},
	author       = {Paulius Danenas and Tomas Skersys and Rimantas Butleris},
	keywords     = {SBVR business vocabulary and rules, UML use case diagram, Model-to-model transformation, Controlled natural language, Natural language processing, Information extraction},
	abstract     = {Discovery, specification and proper representation of various aspects of business knowledge plays crucial part in model-driven information systems engineering, especially when it comes to the early stages of systems development. Being among the most applicable and advanced features of model-driven development, model transformation could help improving one of the most time- and resource-consuming efforts in this process, namely, discovery and specification of business vocabularies and business rules within the problem domain. One of our latest developments in this area was the solution for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams, which was arguably one of the most comprehensive developments of this kind currently available in public. In this paper, we present an enhancement to our previous development by introducing a novel natural language processing component to it. This enhancement provides more advanced extraction capabilities (such as recognition of entities, entire noun and verb phrases, multinary associations) and better quality of the extraction results compared to our previous solution. The main contributions presented in this paper are pre- and post-processing algorithms, and two extraction algorithms using custom-trained POS tagger. Based on the related work findings, it is safe to state that the presented solution is novel and original in its approach of combining together M2M transformation of UML and SBVR models with natural language processing techniques in the field of model-driven information systems engineering.}
}
@article{HASHEM2018216,
	title        = {Comparative study of different binarization methods through their effects in characters localization in scene images},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {216--224},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.011},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18300806},
	author       = {Abdel-Rahiem A. Hashem and Mohd Yamani Idna Idris and Abd El-Baset A. Ahmad},
	keywords     = {Binarization methods, Characters localization, Naïve Bayes classifier, Connected-components analysis},
	abstract     = {In this paper, we focus on the binarization methods as a core step in most image processing algorithms especially localization of the characters in scene images. We have developed in this paper our previous scheme which based on shape properties and geometric features to define text region and adopt our binarization scheme which based on Naïve Bayes classifier to convert grayscale image to binary image. Then we compare this binarization scheme with four famous different methods and explore their effects on detection characters in scene images. We found that our method outperforms the other four prior methods in detection characters with respect to Recall metric and the Otsu method follow our methods.}
}
@article{KIM20191,
	title        = {Trigonometric comparison measure: A feature selection method for text categorization},
	journal      = {Data & Knowledge Engineering},
	volume       = {119},
	pages        = {1--21},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.10.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18300922},
	author       = {Kyoungok Kim and See Young Zzang},
	keywords     = {Feature selection, Text categorization, Text classification, Dimension reduction},
	abstract     = {Text data represented using vector space model is high dimensional data since the number of words can easily grow to tens of thousands for a moderate sized dataset. It may contain lots of redundant or irrelevant features that degrade the performance of a classifier for text categorization. To address this problem, feature selection can be applied for dimensionality reduction and it aims to find a set of highly distinguishing features. Most of filter feature selection methods for text categorization are based on document frequencies in positive and negative classes. Considering only document frequencies favors terms frequently used in a larger class and ignores relative document frequencies in the classes. In this paper, we present a new filter feature selection method, named Trigonometric Comparison Measure (TCM) considering relative document frequencies. The proposed method utilizes true positive rate and false positive rate to determine a better subset of features for text categorization and prefers terms that appear only in documents of one class with high probability. In order to assign a higher rank to terms that are frequently used in one class and rarely appears in another class, TCM calculates off-axis angles of a vector represented as (tpr,fpr) and gives a larger score to terms with a small angle using sin andcos functions. The proposed method is compared with eight well-known filter feature selection methods including balanced accuracy measure (ACC2), information gain (IG), chi-squared (CHI), odds ratio (OR), Gini index (Gini), Deviation from a Poisson distribution (DP), distinguishing feature selector (DFS) and normalized difference measure (NDM) on ten datasets using the multinomial naïve Bayes and support vector machines. The experimental results show that TCM achieves significantly better performance for text categorization.}
}
@article{GUL201880,
	title        = {A multiple criteria credit rating approach utilizing social media data},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {80--99},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.05.005},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303701},
	author       = {Sait Gül and Özgür Kabak and Ilker Topcu},
	keywords     = {Credit rating, Cumulative belief degrees, Sentiment analysis, Social media, Web mining, Text mining},
	abstract     = {Credit rating is a process for building a classification system for credit lenders to characterize current or potential credit borrowers. By such a process, financial institutions classify borrowers for lending decision by evaluating their financial and/or nonfinancial performances. Recently, use of social media data has emerged an important source of information. Accordingly, social media data can be very useful in evaluating companies' credibility when financial or non-financial assessments are missing or unreliable as well as when credit analyzers' subjective perceptions manipulate the decision. In this study, a multiple criteria credit rating approach is proposed to determine companies' credibility level utilizing social media data as well as financial measures. Additionally, to strengthen the lender's interpretation and inference competency, ratings are represented with a risk distribution based on cumulative belief degrees. Sentiment analysis, a web mining and text classification method, is used to collect social media data on Twitter. Importance of criteria is revealed through pairwise comparisons. Companies' performance scores and ratings are obtained by a cumulative belief degree approach. The proposed approach is applied to 64 companies. Results indicate that social media provides valuable information to determine companies' creditability. However credit ratings tend to decrease when social media data is considered.}
}
@article{LYU201821,
	title        = {Privacy-preserving collaborative fuzzy clustering},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {21--41},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.05.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16303214},
	author       = {Lingjuan Lyu and James C. Bezdek and Yee Wei Law and Xuanli He and Marimuthu Palaniswami},
	keywords     = {Participatory sensing, Collaborative learning, Clustering, Privacy-preserving, Randomisation},
	abstract     = {The proliferation of Internet of Things devices has contributed to the emergence of participatory sensing (PS), where multiple individuals collect and report their data to a third-party data mining cloud service for analysis. The need for the participants to collaborate with each other for this analysis gives rise to the concept of collaborative learning. However, the possibility of the cloud service being semi-honest poses a key challenge: preserving the participants' privacy. In this paper, we address this challenge with a two-stage scheme called RG+RP: in the first stage, each participant perturbs his/her data by passing the data through a nonlinear function called repeated Gompertz (RG); in the second stage, he/she then projects his/her perturbed data to a lower dimension in an (almost) distance-preserving manner, using a specific random projection (RP) matrix. The nonlinear RG function is designed to mitigate maximum a posteriori (MAP) estimation attacks, while random projection resists independent component analysis (ICA) attacks and ensures clustering accuracy. The proposed two-stage randomisation scheme is assessed in terms of its recovery resistance to MAP estimation attacks. Preliminary theoretical analysis as well as experimental results on synthetic and real-world datasets indicate that RG+RP has better recovery resistance to MAP estimation attacks than most state-of-the-art techniques. For clustering, fuzzy c-means (FCM) is used. Results using seven cluster validity indices, root mean squared error (RMSE) and accuracy ratio show that clustering results based on two-stage-perturbed data are comparable to the clustering results based on raw data ? this confirms the utility of our privacy-preserving scheme when used with either FCM or HCM.}
}
@article{PAPANIKOLAOU201842,
	title        = {Hierarchical partitioning of the output space in multi-label data},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {42--60},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.05.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17304512},
	author       = {Yannis Papanikolaou and Grigorios Tsoumakas and Ioannis Katakis},
	keywords     = {Knowledge discovery, Machine learning, Supervised learning, Text mining},
	abstract     = {Hierarchy Of Multi-label classifiERs (HOMER) is a multi-label learning algorithm that breaks the initial learning task to several, easier sub-tasks by first constructing a hierarchy of labels from a given label set and secondly employing a given base multi-label classifier (MLC) to the resulting sub-problems. The primary goal is to effectively address class imbalance and scalability issues that often arise in real-world multi-label classification problems. In this work, we present the general setup for a HOMER model and a simple extension of the algorithm that is suited for MLCs that output rankings. Furthermore, we provide a detailed analysis of the properties of the algorithm, both from an aspect of effectiveness and computational complexity. A secondary contribution involves the presentation of a balanced variant of the k means algorithm, which serves in the first step of the label hierarchy construction. We conduct extensive experiments on six real-world data sets, studying empirically HOMER's parameters and providing examples of instantiations of the algorithm with different clustering approaches and MLCs, The empirical results demonstrate a significant improvement over the given base MLC.}
}
@article{BORODIN2020101811,
	title        = {Search-by-example over SQL repositories using structural and intent-driven similarity},
	journal      = {Data & Knowledge Engineering},
	volume       = {128},
	pages        = {101811},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101811},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18306505},
	author       = {Gregory Borodin and Yaron Kanza},
	keywords     = {SQL, Structural search, Intention-driven search, Semantic search, Syntactic search, Query similarity, Tree edit distance, SQL vector model},
	abstract     = {Searching the query log of a database system has a variety of applications. In a complex database, relevant queries in the log can serve as an initial example for query formulation, or may elucidate how to query the data in an optimized manner. Searching for queries that may cause a security or a privacy breach could be used to detect leaks of sensitive data. In general, queries in the query log can provide valuable information about how data have been accessed and used. Finding relevant queries requires conducting search over a repository of SQL queries. However, expressing the information need, to specify which queries should be retrieved, is not easy. In this paper we study the approach of search-by-example, where, given an SQL query Q, the goal is to retrieve queries that are similar to Q. We distinguish between two types of search?structural search and intent-driven search. In structural search, queries are considered similar if their textual formulations are similar, i.e., a small number of edit operations transform one query into the other. In intent-driven search, two queries are deemed similar if they were written for the same task. We illustrate these two types of similarity and the differences between them. We present four heuristics for testing query similarity. Two of the methods are exhaustive and two are less accurate and efficient. We explain how to utilize the efficient methods to boost a search using the exhaustive methods. An experimental evaluation and a user study illustrate the effectiveness of the methods.}
}
@article{BAIZAL2020101813,
	title        = {Computational model for generating interactions in conversational recommender system based on product functional requirements},
	journal      = {Data & Knowledge Engineering},
	volume       = {128},
	pages        = {101813},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101813},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1830524X},
	author       = {Z.K.A. Baizal and Dwi H. Widyantoro and Nur Ulfa Maulidevi},
	keywords     = {Recommender systems, Conversational recommender system, Knowledge-based recommendation, User modeling, Ontology-based knowledge, User interaction},
	abstract     = {Conversational recommender system is a tool to help customer in deciding products they are going to buy, by conversational mechanism. By this mechanism, the system is able to imitate natural conversation between customer and professional sales support, for eliciting customer preference. However, many customers are not familiar with the technical features of multi-function and multi-feature products. A more natural way to explore customer preferences is by asking what they want to use with the product they are looking for (product functional requirements). Therefore, this paper proposes a computational model incorporating product functional requirements for interaction. The proposed model covers ontology and its structure as well as algorithms for generating interaction that comprises asking question, recommending products and presenting explanation of why a product is recommended. Based on our user studies, both expert users (familiar with product technical features) and novice users (not familiar with product technical feature) prefer our proposed interaction model than that of the flat interaction model (interaction model based on technical features). Meanwhile, functional requirements-based explanation is able to improve user trust in recommended products by 30% for novice users and 17% for expert users.}
}
@article{TONG2019123,
	title        = {An efficient integer coding index algorithm for multi-scale time information management},
	journal      = {Data & Knowledge Engineering},
	volume       = {119},
	pages        = {123--138},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.01.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17302616},
	author       = {Xiaochong Tong and Chengqi Cheng and Rong Wang and Lu Ding and Yong Zhang and Guangling Lai and Lin Wang and Bo Chen},
	keywords     = {Multi-scale time, Segmentation, Coding index, Time scales, Temporal relationship, Spatial?temporal big data},
	abstract     = {The era of Big Data has given rise to a massive amount of data containing spatio-temporal information, such as video data, traffic data, and social media data. Current research on spatio-temporal data mainly focuses on the spatial information, while temporal information is usually processed in an auxiliary manner using simple approaches. In order to solve the existing problems in the current time coding schemes, this paper proposed a new type of integer coding method for the time segments called Multi-scale Time Segment Integer Coding (MTSIC). Based on the tree structures and the sorting of the integers, this method is able to describe various temporal relationships among different scales of time segments, such as the start and end, containment/inclusion, which makes it a uniform integer coding scheme for multiple scales of time information. Furthermore, this paper has studied on the MTSIC-based calculations and conversions, including the hierarchical calculations, the hierarchical regression calculations, the conversion between the conventional time coding schemes, the calculations on temporal relationships, and the conversion of arbitrary time spans. These efforts support highly efficient calculations and queries based on the time segments. Preliminary investigations have also been conducted on the potential applications and prospects of the MTSIC method. The results of experiments demonstrated that the implementation of MTSIC is simple and reliable, and MTSICs are easily converted into conventional expressions of time. Additionally, the MTSIC method also exhibits a very high level of efficiency in terms of the time needed to query and perform computations on time values.}
}
@article{BENAVIDES2018107,
	title        = {An ontology-based approach to knowledge representation for Computer-Aided Control System Design},
	journal      = {Data & Knowledge Engineering},
	volume       = {118},
	pages        = {107--125},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.10.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305189},
	author       = {Carmen Benavides and Isaías García and Héctor Alaiz and Luis Quesada},
	keywords     = {Conceptual modeling, Data and knowledge visualization, Ontologies, Computer-Aided Control System Design},
	abstract     = {Different approaches have been used in order to represent and build control engineering concepts for the computer. Software applications for these fields are becoming more and more demanding each day, and new representation schemas are continuously being developed. This paper describes a study of the use of knowledge models represented in ontologies for building Computer Aided Control Systems Design (CACSD) tools. The use of this approach allows the construction of formal conceptual structures that can be stated independently of any software application and be used in many different ones. In order to show the advantages of this approach, an ontology and an application have been built for the domain of design of lead/lag controllers with the root locus method, presenting the results and benefits found.}
}
@article{BAYOUDHI2018138,
	title        = {How to Repair Inconsistency in OWL 2 DL Ontology Versions?},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {138--158},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.05.010},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16303172},
	author       = {Leila Bayoudhi and Najla Sassi and Wassim Jaziri},
	keywords     = {OWL 2 DL ontology, Evolution, Inconsistency, A priori approach},
	abstract     = {Semantic modeling knowledge formalisms, such as ontologies, have to follow the continuous evolution and changes of knowledge. However, ontology changes should never affect its consistency. Ontology needs to remain in a consistent state along its whole engineering process. In the literature, most of approaches check/repair ontology inconsistencies in an a posteriori way. In this paper, an a priori inconsistency approach was proposed to generate consistent OWL 2 DL ontology versions. It relies on the OWL 2 DL change kits, which anticipate inconsistencies upon each change request on an ontology version. The proposed approach predicts potential inconsistencies, provides an a priori repair action and applies the required changes. Consistency rules were defined and used to check logical inconsistencies, but also syntactical invalidities and style issues. A protégé plugin was implemented to validate our approach.}
}
@article{LINHARESPONTES2020101763,
	title        = {Compressive approaches for cross-language multi-document summarization},
	journal      = {Data & Knowledge Engineering},
	volume       = {125},
	pages        = {101763},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101763},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19300217},
	author       = {Elvys {Linhares Pontes} and Stéphane Huet and Juan-Manuel Torres-Moreno and Andréa Carneiro Linhares},
	keywords     = {Cross-language text summarization, Sentence compression, Multi-sentence compression, Optimization},
	abstract     = {The popularization of social networks and digital documents has quickly increased the multilingual information available on the Internet. However, this huge amount of data cannot be analyzed manually. This paper deals with Cross-Language Text Summarization (CLTS) that produces a summary in a different language from the source documents. We describe three compressive CLTS approaches that analyze the text in the source and target languages to compute the relevance of sentences. Our systems compress sentences at two levels: clusters of similar sentences are compressed using a multi-sentence compression (MSC) method and single sentences are compressed using a Neural Network model. The version of our approach using multi-sentence compression generated more informative French-to-English cross-lingual summaries than extractive state-of-the-art systems. Moreover, these cross-lingual summaries have a grammatical quality similar to extractive approaches.}
}
@article{OMAR2020101796,
	title        = {Semi-automated development of conceptual models from natural language text},
	journal      = {Data & Knowledge Engineering},
	volume       = {127},
	pages        = {101796},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101796},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19301429},
	author       = {Mussa Omar and George Baryannis},
	keywords     = {Conceptual modelling, Information extraction, Natural language processing, Ontologies, Semi-structured data},
	abstract     = {The process of converting natural language specifications into conceptual models requires detailed analysis of natural language text, and designers frequently make mistakes when undertaking this transformation manually. Although many approaches have been used to partly automate this process, one of the main limitations is the lack of a domain-independent ontology that can be used as a repository for entities and relationships, thus guiding the transformation process. In this paper, a semi-automated system for mapping natural language text into conceptual models is proposed. The system, called SACMES, combines a linguistic approach with an ontological approach and human intervention to achieve the task. SACMES learns from the natural language specifications that it processes and stores the information that is learnt in a conceptual model ontology and a user history knowledge database. It then uses the stored information to improve performance and reduce the need for human intervention. The evaluation conducted on SACMES demonstrates that: (1) by using the system, precision and recall for users identifying entities of conceptual models is increased by 6% and 13%, respectively, while for relationships, increases are even higher, 14% for precision and 23% for recall; (2) the performance of the system is improved by processing more natural language requirements, and thus, the need for human intervention is decreased.}
}
@article{BURATTIN20191,
	title        = {Learning process modeling phases from modeling interactions and eye tracking data},
	journal      = {Data & Knowledge Engineering},
	volume       = {121},
	pages        = {1--17},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.04.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303282},
	author       = {Andrea Burattin and Michael Kaiser and Manuel Neurauter and Barbara Weber},
	keywords     = {Process of process modeling, Eye tracking, Interaction tracking, Automatic phase detection, Classification, Sequence labeling},
	abstract     = {The creation of a process model is a process consisting of five distinct phases, i.e., problem understanding, method finding, modeling, reconciliation, and validation. To enable a fine-grained analysis of process model creation based on phases or the development of phase-specific modeling support, an automatic approach to detect phases is needed. While approaches exist to automatically detect modeling and reconciliation phases based on user interactions, the detection of phases without user interactions (i.e., problem understanding, method finding, and validation) is still a problem. Exploiting a combination of user interactions and eye tracking data, this paper presents a two-step approach that is able to automatically detect the sequence of phases a modeler is engaged in during model creation. The evaluation of our approach shows promising results both in terms of quality as well as computation time demonstrating its feasibility.}
}
@article{ZHOU2018183,
	title        = {An overlapping community detection algorithm in complex networks based on information theory},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {183--194},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.009},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301015},
	author       = {HongFang Zhou and Yao Zhang and Jin Li},
	keywords     = {Overlapping community detection, Complex networks, Clustering, Data mining},
	abstract     = {In this paper, a new algorithm for overlapping community detection is proposed. First, we propose a node importance evaluation matrix to calculate the important degree for each node; second, we put forward the difference function to detect overlapping points in complex networks; finally, we use triangle principle to detect communities in complex networks. We adopt two measures of Normalized Mutual Information and Modularity to evaluate the algorithm. The experimental results show that our algorithm has a good performance on detecting overlapping community.}
}
@article{WAN201871,
	title        = {ICGT: A novel incremental clustering approach based on GMM tree},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {71--86},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16303962},
	author       = {Yuchai Wan and Xiabi Liu and Yi Wu and Lunhao Guo and Qiming Chen and Murong Wang},
	keywords     = {Incremental data clustering, Streaming data, Gaussian mixture model (GMM), Tree structure},
	abstract     = {Streaming data presents new challenges to data mining algorithms. To conduct data clustering on the streaming data, this paper proposes a novel incremental clustering approach utilizing Gaussian Mixture Model (GMM), termed as ICGT (Incremental Construction of GMM Tree). The ICGT creates and dynamically adjusts a GMM tree consistent to the sequentially presented data. Each leaf node in the tree corresponds to a dense Gaussian distribution and each non-leaf node to a GMM. To update the GMM tree for insertion of the newly arrived data points, we introduce the definitions of node connectivity and connected subsets, and present the tree update algorithm. We further develop a clustering evaluation criterion and search strategy to determine the final partition of the data set based on the constructed GMM tree. We evaluated the proposed approach on synthetic and real-world data sets and compared ICGT with other incremental and static clustering methods. The experimental results confirm that our approach is effective and promising.}
}
@article{BANCHHOR2020101788,
	title        = {Integrating Cuckoo search-Grey wolf optimization and Correlative Naive Bayes classifier with Map Reduce model for big data classification},
	journal      = {Data & Knowledge Engineering},
	volume       = {127},
	pages        = {101788},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101788},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18305925},
	author       = {Chitrakant Banchhor and N. Srinivasu},
	keywords     = {Big data classification, CNB classifier, GWO, CS algorithm, MapReduce model},
	abstract     = {Big data is progressively being used in various areas, such as industry, financial dealing, medicine, and so on, as it can handle the challenges in processing large amounts of data. One of the data mining techniques used widely and effectively to classify big data is the MapReduce model. In this paper, an approach for the classification of big data is developed using Cuckoo?Grey wolf based Correlative Naive Bayes classifier and MapReduce Model (CGCNB-MRM). Accordingly, a novel classifier, named Cuckoo?Grey wolf based Correlative Naive Bayes classifier (CG-CNB), is designed by modifying CNB classifier with a newly developed optimization algorithm, Cuckoo?Grey Wolf based Optimization (CGWO). CGWO algorithm is designed by the effective integration of Cuckoo Search (CS) Algorithm into Grey Wolf Optimizer (GWO), to optimize the CNB model by the optimal selection of the model parameters. Finally, the proposed CGCNB-MRM approach performs the classification for each data samples based on the probability index table and the posterior probability of the data. Three metrics, such as accuracy, sensitivity, and specificity, are utilized for the performance evaluation of the proposed CGCNB-MRM approach, where it could achieve 80.7% accuracy with 84.5% sensitivity and 76.9% specificity and thus, prove its effectiveness in big data classification.}
}
@article{DAS2020101736,
	title        = {Query processing on large graphs: Approaches to scalability and response time trade offs},
	journal      = {Data & Knowledge Engineering},
	volume       = {126},
	pages        = {101736},
	year         = {2020},
	note         = {Special Issue DAWAK 2018 (Data Warehousing and Knowledge Discovery)},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101736},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19304380},
	author       = {Soumyava Das and Abhishek Santra and Jay Bodra and Sharma Chakravarthy},
	keywords     = {Graph query processing, Plan generation, Query evaluation on partitioned graphs, Scalability, Map/Reduce},
	abstract     = {Graphs, being an expressive data structure, have become increasingly important for modeling real-world applications, such as collaboration, different kinds of transactions, social networks, to name a few. With the advent of social networks and the web, the graph sizes have grown too large to fit in main memory precipitating the need for alternative approaches for an efficient, scalable evaluation of queries on graphs of any size. In this paper, we use the time-tested ?divide and conquer? approach by partitioning a graph into desired number of partitions (and possibly with appropriate characteristics) and process queries over those partitions to obtain all or specified number of answers. This entails correctly computing answers that span multiple partitions or even need the same partition more than once. Given a set of partitions, there are a number of approaches using which a query can be evaluated: (i) One Partition At a Time (OPAT) approach, (ii) Traditional use of Multiple Processors (TraditionalMP), and (iii) using the Map/Reduce Multi-Processor approach (MapReduceMP) approach. The first approach, detailed in this paper, has established scalability through independent processing of partitions. The other two approaches address response time in addition to scalability. For the OPAT query evaluation approach, necessary minimal book keeping has been identified and its correctness established in this paper. Query answering on partitioned graphs also requires analyzing partitioning schemes for their impact on query processing and determining the number as well as the sequence in which partitions need to be loaded to reduce the response time for processing queries. We correlate query properties and partition characteristics to reduce query processing time in terms of the resources available. We also identify a set of quantitative metrics and use them for formulating heuristics to determine the order of loading partitions for efficient query processing. For OPAT approach, extensive experiments on large graphs (synthetic and real-world) using different partitioning schemes analyze the proposed heuristics on a variety of query types. The other two approaches are fleshed out, analyzed, and contrasted with the OPAT approach. An existing graph querying system has been extended to evaluate queries on partitioned graphs. Finally all three approaches are compared for their strengths and weaknesses.}
}
@article{SENE201818,
	title        = {Data mining for decision support with uncertainty on the airplane},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {18--36},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.06.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1730071X},
	author       = {A. Sene and B. Kamsu-Foguem and P. Rumeau},
	keywords     = {Dempster-Shafer theory, Frequent pattern mining, Semantic reasoning, Decision support system, In-flight medical incidents},
	abstract     = {This study describes the formalization of the medical decision-making process under uncertainty underpinned by conditional preferences, the theory of evidence and the exploitation of high-utility patterns in data mining. To assist a decision maker, the medical process (clinical pathway) was implemented using a Conditional Preferences Base (CPB). Then for knowledge engineering, a Dempster-Shafer ontology integrating uncertainty underpinned by evidence theory was built. Beliefs from different sources are established with the use of data mining. The result is recorded in an In-flight Electronic Health Records (IEHR). The IEHR contains evidential items corresponding to the variables determining the management of medical incidents. Finally, to manage tolerance to uncertainty, a belief fusion algorithm was developed. There is an inherent risk in the practice of medicine that can affect the conditions of medical activities (diagnostic or therapeutic purposes). The management of uncertainty is also an integral part of decision-making processes in the medical field. Different models of medical decisions under uncertainty have been proposed. Much of the current literature on these models pays particular attention to health economics inspired by how to manage uncertainty in economic decisions. However, these models fail to consider the purely medical aspect of the decision that always remains poorly characterized. Besides, the models achieving interesting decision outcomes are those considering the patient's health variable and other variables such as the costs associated with the care services. These models are aimed at defining health policy (health economics) without a deep consideration for the uncertainty surrounding the medical practices and associated technologies. Our approach is to integrate the management of uncertainty into clinical reasoning models such as Clinical Pathway and to exploit the relationships between the determinants of incident management using data mining tools. To this end, how healthcare professionals see and conceive uncertainty has been investigated. This allowed for the identification of the characteristics determining people under uncertainty and to understand the different forms and representations of uncertainty. Furthermore, what an in-flight medical incident is and how its management is a decision under uncertainty issues was defined. This is the first phase of common data mining that will provide an evidential transaction basis. Subsequently an evidential and ontological reasoning to manage this uncertainty has been established in order to support decision making processes on the airplane.}
}
@article{ZHANG2020101790,
	title        = {Hierarchy construction and classification of heterogeneous information networks based on RSDAEf},
	journal      = {Data & Knowledge Engineering},
	volume       = {127},
	pages        = {101790},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101790},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18300995},
	author       = {Jinli Zhang and Zongli Jiang and Yongping Du and Tong Li and Yida Wang and Xiaohua Hu},
	keywords     = {Heterogeneous information networks, Relax strategy, Stacked denoising auto encoder, Hierarchy construction},
	abstract     = {Heterogeneous information networks (HINs) composed of multiple types of nodes and links, play increasingly important roles in real life applications. Classification of the related data is an essential work in network analysis. Existing methods can effectively solve these classification tasks when they are applied to homogeneous information networks and simple data, but not for the noisy and sparse data. To address the problem, we propose Stacked Denoising Auto Encoder (SDAE) with sparse factors to learn features of nodes in heterogeneous networks. In particular, sparse factors are added in each hidden layer of the proposed stacked denoising auto-encoder to efficiently extract features from noisy and sparse data. Moreover, a relax strategy is employed to construct class hierarchy with high-quality based. Finally, nodes of the heterogeneous information network can be classified. Our proposed framework Relax strategy on Stacked Denoising Auto Encoder with sparse factors (RSDAEf) comparison with several existing methods clearly indicates RSDAEf outperforms the existing methods and achieves a classification precision of 88.3% on DBLP dataset.}
}
@article{GAO2019101729,
	title        = {An efficient and scalable multi-dimensional indexing scheme for modular data centers},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101729},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101729},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18301289},
	author       = {Yuanning Gao and Xiaofeng Gao and Yichen Zhu and Guihai Chen},
	keywords     = {Multi-dimensional data, Distributed two-layer index, Modular data centers},
	abstract     = {An efficient distributed indexing scheme plays an important role in improving the performance of cloud storage systems. To achieve concurrent query service and high manageability, the indexing scheme should meet the requirements of high scalability and low latency. In this paper, we propose RB-Index, an efficient and scalable multi-dimensional indexing scheme for modular data centers with the BCube topology. RB-Index is a two-layer indexing scheme integrating the BCube based routing protocol and the R-tree based indexing structure. In RB-Index, we build several distinct indexing spaces with dimensions selected according to query history. Each server takes responsibility for a portion of the indexing space according to a mapping scheme. A data pretreatment method and a publishing scheme are presented to uniformly distribute the global index across all the servers in the network. Index maintenance strategies are designed to keep the system cost at a low level. Efficient and complete query strategies are also introduced to support highly concurrent queries. We conduct experiments on Amazon EC2 platform to evaluate the performance of RB-Index and compare its performance with RT-CAN and FT-Index. Experiment results manifest the efficiency and scalability of our indexing scheme.}
}
@article{FEVGAS201918,
	title        = {LB-Grid: An SSD efficient Grid File},
	journal      = {Data & Knowledge Engineering},
	volume       = {121},
	pages        = {18--41},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.04.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1830418X},
	author       = {Athanasios Fevgas and Panayiotis Bozanis},
	keywords     = {Spatial databases, Indexing methods, Modern hardware, Solid state drives},
	abstract     = {Recent advances in non-volatile memory technology have led to the introduction of solid state drives (SSD). NVMe SSDs are the latest development in flash based solid state drives and they were designed as a means of low latency and high bandwidth. Many research studies seek for taking advantage of this new technology to accelerate data management. Multidimensional indexes are fundamental for the efficiency of spatial query processing. In this work, we study the implication of high performance NVMe drives on spatial indexing. More specifically, we present an in-depth performance analysis of the Grid File in flash storage and we introduce LB-Grid, a write efficient variant of Grid File for flash based solid state drives. We present new query algorithms for both LB-Grid and Grid File that exploit the high internal parallelism and I/O bandwidth of NVMe SSDs. Experimental results unveil the efficiency of the proposed algorithms. Utilizing a test set of 500M points, LB-Grid appears to be up to 2.26 times faster than Grid File, up to 5.5 times faster than the R?-tree, and up to 3.3 times faster than the FAST-Rtree in update intensive workloads. On the other hand, the Grid File presents better performance in read intensive workloads; exploiting a batch read operation, it achieves a speedup up to 10.2x in range queries, up to 1.56x in kNN and 4.6x in group point queries.}
}
@article{WHANG2020101756,
	title        = {Building social networking services systems using the relational shared-nothing parallel DBMS},
	journal      = {Data & Knowledge Engineering},
	volume       = {125},
	pages        = {101756},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101756},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18306013},
	author       = {Kyu-Young Whang and Inju Na and Tae-Seob Yun and Jin-Ah Park and Kyu-Hyun Cho and Se-Jin Kim and Ilyeop Yi and Byung Suk Lee},
	abstract     = {We propose methods to enable the relational model to meet scalability and functionality needs of a large-scale social networking services (SNS) system. NewSQL has emerged recently indicating that shared-nothing parallel relational DBMSs can be used to guarantee the ACID properties of transactions while keeping the high scalability of NoSQL. Leading commercial SNS systems, however, rely on a graph ? not relational ? data model with key?value storage and, for certain operations, suffer overhead of unnecessarily accessing multiple system nodes. Exploiting higher semantics with the relational data model could be the remedy. The solution we offer aims to perform a transaction as a set of independent local transactions whenever possible based on the conceptual semantics of the SNS database schema. First, it hierarchically clusters entities that are sitting on a path of frequently navigated one-to-many relationships, thereby avoiding inter-node joins. Second, when a multi-node delete transaction is performed over many-to-many relationships, it defers deletion of related references until they are accessed later, thereby amortizing the cost of multi-node updates. These solutions have been implemented in Odysseus/SNS ? an SNS system using a shared nothing parallel DBMS. Performance evaluation using synthetic workload that reflects the real SNS workload demonstrates significant improvement in processing time. We also note that our work is the first to present the entity-relationship schema and its relational representation of the SNS database.}
}
@article{OHARE2019181,
	title        = {An unsupervised blocking technique for more efficient record linkage},
	journal      = {Data & Knowledge Engineering},
	volume       = {122},
	pages        = {181--195},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.06.005},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18306098},
	author       = {Kevin O?Hare and Anna Jurek-Loughrey and Cassio {de Campos}},
	keywords     = {Unsupervised blocking, Record linkage, Entity resolution},
	abstract     = {Record linkage, referred to also as entity resolution, is the process of identifying pairs of records representing the same real-world entity (for example, a person) within a dataset or across multiple datasets. This allows for the integration of multi-source data which allows for better knowledge discovery. In order to reduce the number of record comparisons, record linkage frameworks initially perform a process commonly referred to as blocking, which involves separating records into blocks using a partition (or blocking) scheme. This restricts comparisons among records that belong to the same block during the linkage process. Existing blocking techniques often require some form of manual fine-tuning of parameter values for optimal performance. Optimal parameter values may be selected manually by a domain expert, or automatically learned using labelled data. However, in many real world situations no such labelled dataset may be available. In this paper we propose a novel unsupervised blocking technique for structured datasets that does not require labelled data or manual fine-tuning of parameters. Experimental evaluations, across a large number of datasets, demonstrate that this novel approach often achieves superior levels of proficiency to both supervised and unsupervised baseline techniques, often in less time.}
}
@article{JEONG2020101785,
	title        = {Utilizing adjacency of colleagues and type correlations for enhanced link prediction},
	journal      = {Data & Knowledge Engineering},
	volume       = {125},
	pages        = {101785},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101785},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303890},
	author       = {Hyun Ji Jeong and Myoung Ho Kim},
	keywords     = {Heterogeneous information networks, Link prediction, Graph mining},
	abstract     = {Discoveries of new relationships in the network of objects have been required in various applications such as social networks, DBLP bibliographic networks and biological networks. Specifically, link prediction in heterogeneous information networks (HINs) that consist of multiple types of nodes and links has received much attention recently because many information networks of the real world are HINs. We observe various factors that affect the existence of a link in HINs. Firstly, certain structural characteristics of nodes whose types are the same as that of a source (or target) node give important information for link prediction. Secondly, in the HINs, there can be meaningful correlation between links of a particular link type and paths of a particular path type (also called a meta-path). In other words, paths of different path types affect the existence of links differently. Finally, we use the number of paths between source and target nodes to measure proximity of two nodes. Based on these observations, we newly propose several features and a prediction model. We show through various experiments that our proposed method works effectively and performs better than the other existing methods.}
}
@article{PADMAJA2018174,
	title        = {Evaluating the influence of parameter values on the performance of random subset feature selection algorithm on scientific data},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {174--182},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.008},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16301653},
	author       = {D. Lakshmi Padmaja and B. Vishnuvardhan},
	keywords     = {RSFS, Optimization, Regression modeling, Scientific data},
	abstract     = {Random Subset Feature Selection (RSFS) is Feature Subset Selection (FSS) algorithm based on the random forest technique. This algorithm is useful for selecting relevant features from large datasets resulting from scientific experiments. The random selection process eliminates bias and offers superior performance compared to other feature selection algorithms. The performance of the RSFS algorithm, which is primarily used in data mining, depends on proper parameter selection. The RSFS algorithm parameters dummy features, stopping criteria (Delta), maximum number of iterations, and K nearest neighbor distance are used for selecting the feature subset. The resulting subset, which is a reduced dataset is subjected to further processing such as classification and, detection. This study, is based on the design of experiments approach and model the effects of parameter variation on the RSFS algorithm performance. In this study, the influence of algorithm parameters on classification accuracy is evaluated.}
}
@article{LIU201861,
	title        = {Mechanisms to improve clustering uncertain data with UKmeans},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {61--79},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.05.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16303287},
	author       = {Chuan-Ming Liu and Zhendong Niu and Kuan-Teng Liao},
	keywords     = {Uncertain data, Clustering, Centroid boundary},
	abstract     = {Uncertain data in Kmeans clustering, namely UKmeans, have been discussed in decade years. UKmeans clustering, however, has some difficulties of time performance and effectiveness because of the uncertainty of objects. In this study, we propose some modified UKmeans clustering mechanisms to improve the time performance and effectiveness, and to enable the clustering to be more complete. The main issues include (1) reducing the consideration of time performance in clustering, (2) increasing the effectiveness of clustering, and (3) considering the determination of the number of clusters. In time performance, we use simplified object expressions to reduce the time spent in comparing similarities. Regarding the effectiveness of clustering, we propose compounded factors including the distance, the overlapping of clusters and objects, and the cluster density as the clustering standard to determine similarity. In addition, to increase the effectiveness of clustering, we also propose the concept of a cluster boundary, which affects the belongingness of an object by the overlapping factor. Finally, we use the evaluating approach of the number of uncertain clusters to determine the appropriate the number of clusters. In the experiment, clustering results generated using strategies commonly used in processing uncertain data clustering in UKmeans clusters are compared. Our proposed model shows more favorable performance, higher effectiveness of clustering, and a more appropriate number of clusters compared to other models.}
}
@article{HUNG201852,
	title        = {INSiGHT: A system to detect violent extremist radicalization trajectories in dynamic graphs},
	journal      = {Data & Knowledge Engineering},
	volume       = {118},
	pages        = {52--70},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.09.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303920},
	author       = {Benjamin W.K. Hung and Anura P. Jayasumana and Vidarshana W. Bandara},
	keywords     = {Graph pattern matching, Pattern matching trajectories, Investigative graph search, Radicalization, Violent extremists},
	abstract     = {The number and lethality of violent extremist plots motivated by the Salafi-jihadist ideology have been growing for nearly the last decade in many parts of the world including both the U.S and Western Europe. While detecting the radicalization of violent extremists is a key component in preventing future terrorist attacks, it remains a significant challenge to law enforcement due to the issues of both scale and dynamics. We propose the development of a radicalization trend detection system as a risk assessment assistance technology that relies on data mined from public data and government databases for individuals who exhibit risk indicators for extremist violence, and enables law enforcement to monitor those individuals at the scope and scale that is lawful, and accounts for the dynamic indicative behaviors of the individuals and their associates rigorously and automatically. We frame our approach to monitoring the radicalization pattern of behaviors as a unique dynamic graph pattern matching problem, and develop a technology called INSiGHT (Investigative Search for Graph-Trajectories) to help identify individuals or small groups with conforming subgraphs to a radicalization query pattern, and follow the match trajectories over time. This paper presents the overall INSiGHT architecture and is aimed at assisting law enforcement and intelligence agencies in monitoring and screening for those individuals whose behaviors indicate a significant risk for violence, and allow for the better prioritization of limited investigative resources. We demonstrated the performance of INSiGHT on a variety of datasets, to include small synthetic radicalization-specific datasets and a real behavioral dataset of time-stamped radicalization indicators of recent U.S. violent extremists.}
}
@article{THANISCH2019116,
	title        = {Detecting measurement issues in SQL arithmetic expressions and aggregations},
	journal      = {Data & Knowledge Engineering},
	volume       = {122},
	pages        = {116--129},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.06.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1730589X},
	author       = {Peter Thanisch and Tapio Niemi and Jyrki Nummenmaa and Marko Niinimäki},
	keywords     = {SQL, Measurement scale and unit, Aggregation, Summarizability},
	abstract     = {Research on user errors in retrieving information from SQL databases has focused on erroneous syntax in the query language and erroneous semantics concerning the data model. In the present paper, we investigate a third source of error, namely erroneous aggregations that break the limitations imposed by the numerical properties of the data. An erroneous aggregation might arise because of the SQL programmer?s misunderstanding concerning those numerical properties, or because of a simple mistake. We show that for database queries in the SQL language, significant classes of erroneous aggregations can be detected by non-intrusive, off-line checking, using only a simple set of metadata rules that can be supplied by the data provider. We have implemented software that performs static checks on users? SQL queries, looking for evidence of misunderstandings concerning the measurement properties of the numerical data.}
}
@article{WEILAND2018114,
	title        = {Knowledge-rich image gist understanding beyond literal meaning},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {114--132},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.006},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17302793},
	author       = {Lydia Weiland and Ioana Hulpu? and Simone Paolo Ponzetto and Wolfgang Effelsberg and Laura Dietz},
	keywords     = {Image understanding, Language and vision, Entity ranking},
	abstract     = {We investigate the problem of understanding the message (gist) conveyed by images and their captions as found, for instance, on websites or news articles. To this end, we propose a methodology to capture the meaning of image-caption pairs on the basis of large amounts of machine-readable knowledge that have previously been shown to be highly effective for text understanding. Our method identifies the connotation of objects beyond their denotation: where most approaches to image understanding focus on the denotation of objects, i.e., their literal meaning, our work addresses the identification of connotations, i.e., iconic meanings of objects, to understand the message of images. We view image understanding as the task of representing an image-caption pair on the basis of a wide-coverage vocabulary of concepts such as the one provided by Wikipedia, and cast gist detection as a concept-ranking problem with image-caption pairs as queries. Our proposed algorithm brings together aspects of entity linking and clustering, subgraph selection, semantic relatedness, and learning-to-rank in a novel way. In addition to this novel task and a complete evaluation of our approach, we introduce a novel dataset to foster further research on this problem. To enable a throughout investigation of the problem of gist understanding, we produce a gold standard of over 300 image-caption pairs and over 8000 gist annotations covering a wide variety of topics at different levels of abstraction. We use this dataset to experimentally benchmark the contribution of different kinds of signals from heterogeneous sources, namely image and text. The best result with a Mean Average Precision (MAP) of 0.69 indicate that by combining both dimensions we are able to better understand the meaning of our image-caption pairs than when using language or vision information alone. Our supervised approach relies on the availability of human-annotated gold standard datasets. Annotating images with, possibly complex, topic labels is arguably a very time-consuming task that must rely on expert human annotators. We accordingly investigate whether parts of this process could be automatized using automatic image annotation and caption generation techniques. Our results indicate the general feasibility of an end-to-end approach to gist detection when replacing one of the two dimensions with automatically generated input, i.e., using automatically generated image tags or generated captions. However, we also show experimentally that state-of-the-art image and text understanding is better at understanding literal meanings of image-caption pairs, with non-literal pairs being instead generally more difficult to detect, thus paving the way for future work on understanding the message of images beyond their literal content.}
}
@article{SEVENS2018264,
	title        = {Less is more: A rule-based syntactic simplification module for improved text-to-pictograph translation},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {264--289},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.05.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17304974},
	author       = {Leen Sevens and Vincent Vandeghinste and Ineke Schuurman and Frank {Van Eynde}},
	keywords     = {Syntactic simplification, Text-to-pictograph translation, Augmented and alternative communication, Social media},
	abstract     = {In order to enable or facilitate online communication for people with an intellectual disability, the Text-to-Pictograph translation system automatically translates Dutch written text into a series of Sclera or Beta pictographs. The baseline system presents the reader with a more or less verbatim pictograph-per-word translation. As a result, long and complex input sentences lead to long and complex pictograph translations, leaving the end users confused and distracted. To overcome these problems, we developed a rule-based simplification system for Dutch Text-to-Pictograph translation. By using recursion and applying the simplification operations in a logical way, only one syntactic parse is needed per message. Promising results are obtained.}
}
@article{LUAN201841,
	title        = {Experimental identification of hard data sets for classification and feature selection methods with insights on method selection},
	journal      = {Data & Knowledge Engineering},
	volume       = {118},
	pages        = {41--51},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.09.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301465},
	author       = {Cuiju Luan and Guozhu Dong},
	keywords     = {Classification methods, Feature selection methods, Hard data sets, Method ranking, Performance comparison, Classification, Mining methods and algorithms},
	abstract     = {The paper reports an experimentally identified list of benchmark data sets that are hard for representative classification and feature selection methods. This was done after systematically evaluating a total of 48 combinations of methods, involving eight state-of-the-art classification algorithms and six commonly used feature selection methods, on 129 data sets from the UCI repository (some data sets with known high classification accuracy were excluded). In this paper, a data set for classification is called hard if none of the 48 combinations can achieve an AUC over 0.8 and none of them can achieve an F-Measure value over 0.8; it is called easy otherwise. A total of 15 out of the 129 data sets were found to be hard in that sense. This paper also compares the performance of different methods, and it produces rankings of classification methods, separately on the hard data sets and on the easy data sets. This paper is the first to rank methods separately for hard data sets and for easy data sets. It turns out that the classifier rankings resulting from our experiments are somehow different from those in the literature and hence they offer new insights on method selection. It should be noted that the Random Forest method remains to be the best in all groups of experiments.}
}
@article{CHEN201981,
	title        = {Time-aware spatial keyword cover query},
	journal      = {Data & Knowledge Engineering},
	volume       = {122},
	pages        = {81--100},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.05.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18303896},
	author       = {Zijun Chen and Tingting Zhao and Wenyuan Liu},
	keywords     = {Spatial keyword cover query, Positional relevance, TR-tree, Valid time of objects},
	abstract     = {The existing spatial keyword cover query only considers the relevance of the text and the position, and ignores the temporal information of the geospatial objects. In this paper, we define a new query, time-aware spatial keyword cover query (TSKCQ), which takes into account the textual relevance, positional relevance, and temporal information of the objects. A new cost function is proposed in TSKCQ, which is used to evaluate the user?s satisfaction in time and space under the premise of satisfying textual constraints proposed by the user. With this function, an object set with the best cost function value would be returned by TSKCQ. We propose a TR-tree for indexing the temporal and spatial information of objects. Based on this, we propose an exact algorithm to tackle TSKCQ, in which effective pruning strategies are used. Finally, experiments demonstrate the efficiency of the proposed algorithm.}
}
@article{ALLAM201898,
	title        = {Improved suffix blocking for record linkage and entity resolution},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {98--113},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.005},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16300714},
	author       = {Amin Allam and Spiros Skiadopoulos and Panos Kalnis},
	keywords     = {Database integration, Data warehouse and repository},
	abstract     = {Record linkage is the problem that identifies the different records that represent the same real-world object. Entity resolution is the problem that ensures that a real-world object is represented by a single record. The incremental versions of record linkage and entity resolution address the respective problems after the insertion of a new record in the dataset. Record linkage, entity resolution and their incremental versions are of paramount importance and arise in several contexts such as data warehouses, heterogeneous databases and data analysis. Blocking techniques are usually utilized to address these problems in order to avoid comparing all record pairs. Suffix blocking is one of the most efficient and accurate blocking techniques. In this paper, we consider the non-incremental variation of record linkage and present a method that is more than five times faster and achieves similar accuracy to the current state-of-the-art suffix-based blocking method. Then, we consider the incremental variation of record linkage and propose a novel incremental suffix-based blocking mechanism that outperforms existing incremental blocking methods in terms of blocking accuracy and efficiency. Finally, we consider incremental entity resolution and present two novel techniques based on suffix blocking that are able to handle the tested dataset in a few seconds (while a current state-of-the-art technique requires more than eight hours). Our second technique proposes a novel method that keeps a history of the deleted records and the merging process. Thus, we are able to discover alternative matches for the inserted record that are not possible for existing methods and improve the accuracy of the algorithm. We have implemented and extensively experimentally evaluated all our methods. We offer two implementations of our proposals. The first one is memory-based and offers the best efficiency while the second one is disk-based and scales seamlessly to very large datasets.}
}
@article{XU2020101800,
	title        = {A multi-view similarity measure framework for trouble ticket mining},
	journal      = {Data & Knowledge Engineering},
	volume       = {127},
	pages        = {101800},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101800},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18304580},
	author       = {Jian Xu and Jiapeng Mu and Gaorong Chen},
	keywords     = {Trouble ticket, Similarity measure, Semantic similarity, Machine learning},
	abstract     = {Text similarity measures play a very important role in several text mining applications. Although there is an extensive literature on measuring the similarity between long texts, there is less work related to the measurement of similarity between short texts. And most of these works on short text similarity are based on adaptations of long-text similarity methods. Unfortunately, the description of a trouble ticket is just a kind of short texts. Thus, ticket mining applications such as ticket classification, ticket clustering, and ticket resolution recommendation often suffer from poor performance because of tickets? particular characteristics of unstructured, short free-text with large vocabulary size, large volume, non-English dictionary words, and so on. Therefore, the ability to accurately measure the similarity between two tickets is critical to the performance of ticket mining. To address this performance issue, this paper proposes a multi-view similarity measure framework that easily integrates several kinds of existing similarity measures including surface matching based measures, semantic similarity measures and syntax based measures. Further, in order to make full use of the strengths of different similarity measures, our framework adopts four different policies to combine them. In particular, we consider a machine learning based policy that can be applied to integrate various similarity measures in a more general way, which makes our framework flexible and extensible. To demonstrate the effectiveness of measures generated from our framework, we empirically validate them on a publicly available short text data set and apply them to a real-world ticket data set from a large enterprise IT infrastructure. Some important findings obtained via the result analysis will be helpful to further improve performance.}
}
@article{KACFAHEMANI2019130,
	title        = {NALDO: From natural language definitions to OWL expressions},
	journal      = {Data & Knowledge Engineering},
	volume       = {122},
	pages        = {130--141},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.06.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18306086},
	author       = {Cheikh {Kacfah Emani} and Catarina {Ferreira Da Silva} and Bruno Fiès and Parisa Ghodous},
	keywords     = {Ontologies, Natural language definitions, Ontology enrichment, OWL DL, Semantic web},
	abstract     = {Domain ontologies are pivotal for Semantic Web applications. The richness of an ontology goes in hand with its usefulness and efficiency. Unfortunately, manually enriching an ontology is very time-consuming. In this paper, we propose to enrich an ontology automatically by obtaining logical expressions of concepts. We present NALDO, a novel approach that provides an OWL DL (Web Ontology Language Description Logics) expression of a concept from two inputs: (1) the natural language definition of the concept and (2) an ontology describing the domain of this concept. NALDO uses as much as possible entities provided by the domain ontology, however it can suggest, when needed, new entities. The expressiveness of expressions provided by NALDO covers value and cardinality restrictions, subsumption and equivalence. We evaluate our approach against the definitions and the corresponding ontologies of the BEAUFORD benchmark. Our results show that NALDO is able to perform the correct identification of formal entities with an F1-measure up to 0.79.}
}
@article{MASUD2019101715,
	title        = {Generate pairwise constraints from unlabeled data for semi-supervised clustering},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101715},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101715},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17302690},
	author       = {Md Abdul Masud and Joshua Zhexue Huang and Ming Zhong and Xianghua Fu},
	keywords     = {Constrained clustering, I-nice approach, Pairwise constraints selection, Semi-supervised clustering},
	abstract     = {Pairwise constraint selection methods often rely on the label information of data to generate pairwise constraints. This paper proposes a new method of selecting pairwise constraints from unlabeled data for semi-supervised clustering to improve clustering accuracy. Given a dataset without any label information, it is first clustered by using the I-nice method into a set of initial clusters. From each initial cluster, a dense group of objects is obtained by removing the faraway objects. Then, the most informative object and the informative objects are identified with the local density estimation method in each dense group of objects. The identified objects are used to form a set of pairwise constraints, which are incorporated in the semi-supervised clustering algorithm to guide the clustering process toward a better solution. The advantage of this method is that no label information of data is required for selection pairwise constraints. Experimental results demonstrate that the new method improved the clustering accuracy and outperformed four state-of-the-art pairwise constraint selection methods, namely, random, FFQS, min?max, and NPU, on both synthetic and real-world datasets.}
}
@article{XU2018205,
	title        = {Expert recommendation for trouble ticket routing},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {205--218},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.06.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305165},
	author       = {Jian Xu and Rouying He},
	keywords     = {Trouble ticket, Resolution recommendation, Sequence mining, Signature},
	abstract     = {A trouble ticket is an important information carrier in system maintenance, which records problem symptoms, the resolving process, and resolutions. A critical challenge for the ticket management system is how to quickly assign a proper expert to deal with trouble tickets and fix problems. Thousands of tickets bouncing among multiple experts before being fixed will consume limited system maintenance resources and may also violate the service level agreement (SLA). Thus, for an incoming ticket, an expert should be recommended as quickly as possible in order to reduce the processing delay. In this paper, to address the challenge in the expert assignment, we exploit an expert collaboration network model by combining expertise profiles and social profiles learned from problem descriptions and resolution sequences of the historical resolved tickets, and develop several two-stage expert recommendation algorithms to determine a resolver to solve the problem. To evaluate the effectiveness of expert recommendation algorithms, we conduct extensive experiments on a real ticket data set. The experimental results show that the proposed algorithms can effectively shorten the mean number of steps to resolve (MSTR) with a high recommendation precision, especially for the long routing sequences generated from manual assignments. The proposed model and algorithms have the potential of being used in a ticket routing recommendation engine to greatly reduce human intervention in the routing process.}
}
@article{MOUTAFIS201942,
	title        = {Efficient processing of all-k-nearest-neighbor queries in the MapReduce programming framework},
	journal      = {Data & Knowledge Engineering},
	volume       = {121},
	pages        = {42--70},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.04.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18303926},
	author       = {Panagiotis Moutafis and George Mavrommatis and Michael Vassilakopoulos and Spyros Sioutas},
	keywords     = {Spatial query processing, Nearest neighbor query, Plane sweep, Quadtrees, MapReduce, Apache hadoop},
	abstract     = {Numerous modern applications, from social networking to astronomy, need efficient answering of queries on spatial data. One such query is the All k Nearest-Neighbor Query, or k Nearest-Neighbor Join, that takes as input two datasets and, for each object of the first one, returns the k nearest-neighbors from the second one. It is a combination of the k nearest-neighbor and join queries and is computationally demanding. Especially, when the datasets involved fall in the category of Big Data, a single machine cannot efficiently process it. Only in the last few years, papers proposing solutions for distributed computing environments have appeared in the literature. In this paper, we focus on parallel and distributed algorithms using the Apache Hadoop framework. More specifically, we focus on an algorithm that was recently presented in the literature and propose improvements to tackle three major challenges that distributed processing faces: improvement of load balancing (we implement an adaptive partitioning scheme based on Quadtrees), acceleration of local processing (we prune points during calculations by utilizing plane-sweep processing), and reduction of network traffic (we restructure and reduce the output size of the most demanding phase of computation). Moreover, by using real 2D and 3D datasets, we experimentally study the effect of each improvement and their combinations on performance of this literature algorithm. Experiments show that by carefully addressing the three aforementioned issues, one can achieve significantly better performance. Thereby, we conclude to a new scalable algorithm that adapts to the data distribution and significantly outperforms its predecessor. Moreover, we present an experimental comparison of our algorithm against other well-known MapReduce algorithms for the same query and show that these algorithms are also significantly outperformed.}
}
@article{NASERIPARSA2019101758,
	title        = {XSnippets: Exploring semi-structured data via snippets},
	journal      = {Data & Knowledge Engineering},
	volume       = {124},
	pages        = {101758},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101758},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18301903},
	author       = {Mehdi Naseriparsa and Md. Saiful Islam and Chengfei Liu and Lu Chen},
	keywords     = {Data exploration, Recommendations, XML},
	abstract     = {Users are usually not familiar with the content and structure of the data when they explore the data source. However, to improve the exploration usability, they need some primary hints about the data source. These hints should represent the overall picture of the data source and include the trending issues that can be extracted from the query log. In this paper, we propose a two-phase interactive exploratory search framework for the clueless users that exploits the snippets for conducting the search on the XML data. In the first phase, we present the primary snippets that are generated from the keywords of the query log to start the exploration. To retrieve the primary snippets, we develop an A* search-based technique on the keyword space of the query log. To improve the performance of computations, we store the primary snippet computations in an index data structure to reuse it for the next steps. In the second phase, we exploit the co-occurring content of the snippets to generate more specific snippets with the user interaction. To expedite the performance, we design two pruning techniques called inter-snippet and intra-snippet pruning to stop unnecessary computations. Finally, we discuss a termination condition that checks the cardinality of the snippets to stop the interactive phase and present the final Top-l snippets to the user. Our experiments on real datasets verify the effectiveness and efficiency of the proposed framework.}
}
@article{KAZEMI2020101794,
	title        = {Content-based Node2Vec for representation of papers in the scientific literature},
	journal      = {Data & Knowledge Engineering},
	volume       = {127},
	pages        = {101794},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101794},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1830185X},
	author       = {B. Kazemi and A. Abhari},
	keywords     = {Distributed representation, Artificial neural networks, Node2Vec, Link prediction},
	abstract     = {Lower-dimensional representation of scientific text has attracted much attention among researchers due to its impact on many data mining and recommendation tasks. This paper studies two main research streams in scientific literature representation. First, both local and distributed representation viewpoints are reviewed and their advantages and disadvantages in lower dimensional representation are discussed. The paper then proposes a novel hybrid distributed technique for text representation. Using scientific articles as the major source of textual information, both the article?s content and citation network are used to build a distributed and universal lower dimensional representation. The superiority of the new technique to the traditional methods is then justified in predicting the existence of links in large citation graphs.}
}
@article{MARTINEZGIL2018195,
	title        = {Accurate and efficient profile matching in knowledge bases},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {195--215},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.010},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305554},
	author       = {Jorge Martinez-Gil and Alejandra Lorena Paoletti and Gábor Rácz and Attila Sali and Klaus-Dieter Schewe},
	keywords     = {Profile, Lattice, Filter, Matching measure, Plausibility constraint, Top- query, Gap query},
	abstract     = {A profile describes a set of properties, e.g. a set of skills a person may have, a set of skills required for a particular job, or a set of abilities a football player may have with respect to a particular team strategy. Profile matching aims to determine how well a given profile fits to a requested profile and vice versa. The approach taken in this article is grounded in a matching theory that uses filters in lattices to represent profiles, and matching values in the interval [0,1]: the higher the matching value the better is the fit. Such lattices can be derived from knowledge bases to represent the knowledge about profiles. An interesting question is, how human expertise concerning the matching can be exploited to obtain most accurate matchings. It will be shown that if a set of filters together with matching values by some human expert is given, then under some mild plausibility assumptions a matching measure can be determined such that the computed matching values preserve the relevant rankings given by the expert. A second question concerns the efficient querying of databases of profile instances. For matching queries that result in a ranked list of profile instances matching a given one it will be shown how corresponding top-k queries can be evaluated on grounds of pre-computed matching values. In addition, it will be shown how the matching queries can be exploited for gap queries that determine how profile instances need to be extended in order to improve in the rankings.}
}
@article{LUO201837,
	title        = {A novel representation in three-dimensions for high dimensional data sets},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {37--52},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18302830},
	author       = {Jianfeng Luo and Haifeng Yan and Yubo Yuan},
	keywords     = {High dimensional data, Data mining, Representation, Information loss, Clustering, p-norm},
	abstract     = {Data representation is an important topic in the field of data engineering. In this paper, we focus on the representation of high dimensional data sets. We present the construction method of the set-valued mapping in 3-C representation and propose a novel representation algorithm based on K-means clustering method. The main contribution is to obtain the cluster centers of these high dimensional data sets, and get the correspondence coordinates in 3-C space with the projection along the center's direction. To verify the effectiveness of the proposed method, three sections of experiments had been completed. The first one is ten data sets from UCI. The second one is web images from Corel5k. The last one is the syllabus, a data set consists of text documents from the MIT OpenCourseWare project. All of the results can make sure that the corresponding similarity of data points or attributes are displayed clearly and show that the proposed algorithm's feasibility and scalability. Especially, the results on web images and syllabus are very excellent. As a result, the proposed representation algorithm in three dimension space will make significant influence on data classification and dimensionality reduction.}
}
@article{CHEN201814,
	title        = {Leveraging social media news to predict stock index movement using RNN-boost},
	journal      = {Data & Knowledge Engineering},
	volume       = {118},
	pages        = {14--24},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.08.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305839},
	author       = {Weiling Chen and Chai Kiat Yeo and Chiew Tong Lau and Bu Sung Lee},
	keywords     = {Recurrent neural networks, Adaboost, Time series prediction, Online social networks},
	abstract     = {News from traditional media has been used to facilitate the prediction of stock movement for a long time. However, in recent times, online social networks (OSN) have played an increasing significant role as a platform for information sharing. News content posted on these OSN provides very useful insight about public moods. In this paper, we carefully select official accounts from China?s largest online social networks ? Sina Weibo and analyze the news content crawled from these accounts by extracting sentiment features and Latent Dirichlet allocation (LDA) features. We then input these features together with technical indicators into a novel hybrid model called RNN-boost to predict the stock volatility in the Chinese stock market. The Shanghai-Shenzhen 300 Stock Index (HS300) is the use case for this research. Experimental results show that our model outperforms other prevalent methods and can achieve a good prediction performance.}
}
@article{GHORBEL2019101719,
	title        = {Ontology-based representation and reasoning about precise and imprecise temporal data: A fuzzy-based view},
	journal      = {Data & Knowledge Engineering},
	volume       = {124},
	pages        = {101719},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101719},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19300382},
	author       = {Fatma Ghorbel and Fayçal Hamdi and Elisabeth Métais and Nebrasse Ellouze and Faïez Gargouri},
	keywords     = {Precise Temporal Data, Imprecise Temporal Data, Temporal Representation, Temporal Reasoning, Fuzzy Ontology},
	abstract     = {Temporal representation and reasoning are important facets in the design of many Semantic Web applications. Several approaches exist to represent and reason about precise temporal data in ontology. However, most of them handle only time intervals and associated qualitative relations. Besides, to the best of our knowledge, there is no approach devoted to handle imprecise temporal data (e.g., ?late 1970s?). In this paper, we propose an ontology-based approach for representing and reasoning about precise and imprecise temporal data. Quantitative temporal data (i.e., time intervals and points) and qualitative ones (i.e., relations between time intervals, relations between a time interval and a time point and relations between time points) are taken into consideration. Our approach is three folds: (i) extending the 4D-fluents approach with new crisp and fuzzy components, to represent precise and imprecise temporal data, (ii) extending the Allen?s interval algebra to enable reasoning about precise and imprecise temporal data, and (iii) creating a Fuzzy-OWL 2 ontology TimeOnto that, based on the extended Allen?s interval algebra, instantiates our 4D-fluents-based representation. The extension that we propose for the Allen?s interval algebra handles precise and imprecise time intervals. Indeed, it enables expressing precise (e.g., ?before?) and imprecise (e.g., ?just before?) temporal relations. Compared to related work, our imprecise relations are personalized, in the sense that they are not limited to a defined set of interval relations and their meanings are determined by the domain expert. For instance, the classic Allen?s relation ?Before? may be generalized in 5 imprecise relations, where ?Before(1)? means ?just before? and gradually the time gap between the two intervals increases until ?Before(5)? which means ?very long before?. To enable this representation, we propose an extension of the Vilain and Kautz?s point algebra and redefined the Allen?s relations by means of this extended algebra. We show in this paper that, unlike most related work, the resulting relations preserve many of the desirable properties of the Allen?s interval algebra. The definitions of the resulting interval relations are adapted to allow relating a time interval and a time point, and two time points, where time intervals and points maybe both precise or both imprecise. These relations can be used for temporal reasoning by means of four transitivity tables. Finally, we describe a prototype based on ?TimeOnto? that infers new relations using a set of SWRL and fuzzy IF-THEN rules. This prototype was integrated in an ontology-based memory prosthesis for Alzheimer?s patients.}
}
@article{FOURNIERVIGER2020101733,
	title        = {Discovering rare correlated periodic patterns in multiple sequences},
	journal      = {Data & Knowledge Engineering},
	volume       = {126},
	pages        = {101733},
	year         = {2020},
	note         = {Special Issue DAWAK 2018 (Data Warehousing and Knowledge Discovery)},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101733},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19304343},
	author       = {Philippe Fournier-Viger and Peng Yang and Zhitian Li and Jerry Chun-Wei Lin and Rage Uday Kiran},
	keywords     = {Periodic pattern, Rare pattern, Correlated pattern, Sequences},
	abstract     = {Periodic-Frequent Pattern Mining (PFPM) is an emerging problem, which consists of identifying frequent patterns that periodically occur over time in a sequence of events. Though PFPM is useful in many domains, traditional algorithms have two important limitations. First, they are not designed to find rare patterns. But discovering rare patterns is useful in many domains (e.g. to study rare diseases). Second, traditional PFPM algorithms are generally designed to find patterns in a single sequence, but identifying periodic patterns that are common to a set of sequences is often desirable (e.g. to find patterns common to several hospital patients or customers). To address these limitations, this paper proposes to discover a novel type of patterns in multiple sequences called Rare Correlated Periodic Patterns. Properties of the problem are studied, and an efficient algorithm named MRCPPS (Mining Rare Correlated Periodic Patterns common to multiple Sequences) is presented to efficiently find these patterns. It relies on a novel RCPPS-list structure to avoid repeatedly scanning the database. Experiments have been done on several real datasets, and it was observed that the proposed MRCPPS algorithm can efficiently discover all rare correlated periodic patterns common to multiple sequences, and filter many non rare and correlated patterns.}
}
@article{ALAMI2020101792,
	title        = {A framework for multidimensional skyline queries over streaming data},
	journal      = {Data & Knowledge Engineering},
	volume       = {127},
	pages        = {101792},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101792},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19301077},
	author       = {Karim Alami and Sofian Maabout},
	keywords     = {Multidimensional skylines, Streaming data, Index structure, Query optimization},
	abstract     = {Skyline query has attracted a great deal of interest during last years because of its ability to help decision makers when multi-criteria objectives are to be handled. Several authors have pointed the interest of multidimensional skylines, i.e., the set of criteria become a parameter of the query. In order to efficiently evaluate these queries, index structures have been proposed. In this paper, we address the problem of efficiently handling multidimensional skyline queries in the context of streaming data. The appended records have a validity time interval after which they become outdated and hence, can be discarded. To that end, we propose a framework that handles an index structure periodically updated. Then the queries consider just the indexed data. This is the price we pay to deal with the streaming nature of the data we consider. Through extensive experiments, we demonstrate our framework?s ability to handle multidimensional skyline queries with challenging streaming data. The main criteria we consider to assess the performance of our solution are query execution time and both index structure maintenance time and its memory consumption.}
}
@article{PRATESI2020101786,
	title        = {PRIMULE: Privacy risk mitigation for user profiles},
	journal      = {Data & Knowledge Engineering},
	volume       = {125},
	pages        = {101786},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101786},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18305342},
	author       = {Francesca Pratesi and Lorenzo Gabrielli and Paolo Cintia and Anna Monreale and Fosca Giannotti},
	keywords     = {Mobile phone data, Call detail record, Privacy, Anonymization},
	abstract     = {The availability of mobile phone data has encouraged the development of different data-driven tools, supporting social science studies and providing new data sources to the standard official statistics. However, this particular kind of data are subject to privacy concerns because they can enable the inference of personal and private information. In this paper, we address the privacy issues related to the sharing of user profiles, derived from mobile phone data, by proposing PRIMULE, a privacy risk mitigation strategy. Such a method relies on PRUDEnce (Pratesi et al., 2018), a privacy risk assessment framework that provides a methodology for systematically identifying risky-users in a set of data. An extensive experimentation on real-world data shows the effectiveness of PRIMULE strategy in terms of both quality of mobile user profiles and utility of these profiles for analytical services such as the Sociometer (Furletti et al., 2013), a data mining tool for city users classification.}
}
@article{VATSALAN2020101809,
	title        = {Incremental clustering techniques for multi-party Privacy-Preserving Record Linkage},
	journal      = {Data & Knowledge Engineering},
	volume       = {128},
	pages        = {101809},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101809},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19303015},
	author       = {Dinusha Vatsalan and Peter Christen and Erhard Rahm},
	keywords     = {Data linkage, Privacy, Scalability, Graph matching, Multiple databases, Subset matching},
	abstract     = {Privacy-Preserving Record Linkage (PPRL) supports the integration of sensitive information from multiple datasets, in particular the privacy-preserving matching of records referring to the same entity. PPRL has gained much attention in many application areas, with the most prominent ones in the healthcare domain. PPRL techniques tackle this problem by conducting linkage on masked (encoded) values. Employing PPRL on records from multiple (more than two) parties/sources (multi-party PPRL, MP-PPRL) is an increasingly important but challenging problem that so far has not been sufficiently solved. Existing MP-PPRL approaches are limited to finding only those entities that are present in all parties thereby missing entities that match only in a subset of parties. Furthermore, previous MP-PPRL approaches face substantial scalability limitations due to the need of a large number of comparisons between masked records. We thus propose and evaluate new MP-PPRL approaches that find matches in any subset of parties and still scale to many parties. Our approaches maintain all matches within clusters, where these clusters are incrementally extended or refined by considering records from one party after the other. An empirical evaluation using multiple real datasets ranging from 3 to 26 parties each containing up to 5 million records validates that our protocols are efficient, and significantly outperform existing MP-PPRL approaches in terms of linkage quality and scalability.}
}
@article{ADAM2020101781,
	title        = {Dynamic monitoring of software use with recurrent neural networks},
	journal      = {Data & Knowledge Engineering},
	volume       = {125},
	pages        = {101781},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101781},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19300941},
	author       = {Chloé Adam and Antoine Aliotti and Fragkiskos D. Malliaros and Paul-Henry Cournède},
	keywords     = {Recurrent neural networks, LSTM, Action embeddings, Action representation, Next action prediction, Crash monitoring, Medical imaging software},
	abstract     = {User interaction with a software may be formalized as a sequence of actions. In this paper we propose two methods ? based on different representations of input actions ? to address two distinct industrial issues: next action prediction and software crash risk detection. Both methods take advantage of the recurrent structure of Long Short Term Memory neural networks to capture dependencies among our sequential data as well as their capacity to potentially handle different types of input representations for the same data. Given the history of user actions in the interface, our first method aims at predicting the next action. The proposed recurrent neural network outperforms state-of-the-art proactive user interface algorithms with standard one-hot vectors as inputs. Besides, we propose to feed the LSTM with actions embeddings. This continuous representation performs better than one-hot encoded vector LSTM and its lower dimension reduces at the same time the computational cost. Using the same data set, the second method aims at crash risk detection. To address this task, we propose to use feature vectors composed of actions with above average crash probabilities as inputs of the LSTM ? with the idea to take advantage of its ability to learn relevant past information to detect crash patterns. The method outperforms state-of-the-art sequence classification methods. Our approaches are demonstrated on medical imaging software logs from ten different hospitals worldwide, though they might be applied to various user interfaces in a wide range of applications.}
}
@article{LI201871,
	title        = {Approximate top-K answering under uncertain schema mappings},
	journal      = {Data & Knowledge Engineering},
	volume       = {118},
	pages        = {71--91},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.09.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305773},
	author       = {Longzhuang Li and Feng Tian and Yonghuai Liu and Shanxian Mao},
	keywords     = {Database integration, Uncertain schema mappings, Top- query, Histogram-based approximation},
	abstract     = {Data integration techniques provide a communication bridge between isolated sources and offer a platform for information exchange. When the schemas of heterogeneous data sources map to the centralized schema in a mediated data integration system or a source schema maps to a target schema in a peer-to-peer system, multiple schema mappings may exist due to the ambiguities in the attribute matching. The obscure schema mappings lead to the uncertainty in query answering, and frequently people are only interested in retrieving the best k answers (top-k) with the biggest probabilities. Retrieving the top-k answers efficiently has become a research issue. For uncertain queries, two semantics, by-table and by-tuple, have been developed to capture top-k answers based on the schema mapping probabilities. However, although the existing algorithms support certain features to capture the accurate top-k answers and avoid accessing all data from sources, they cannot effectively reduce the number of processed tuples in most cases. In this paper, new algorithms based on the histogram approximation and heuristic are proposed to efficiently identify the top-k answers for the data integration systems under uncertain schema mappings. In the experiments, the Histogram algorithm in the by-table semantics and the expected approach in the by-tuple semantics are shown to significantly reduce the number of processed tuples while maintaining high accuracy with the estimated probabilistic confidence.}
}
@article{STROUTHOPOULOS201945,
	title        = {Core discovery in hidden networks},
	journal      = {Data & Knowledge Engineering},
	volume       = {120},
	pages        = {45--59},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.12.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1830079X},
	author       = {Panagiotis Strouthopoulos and Apostolos N. Papadopoulos},
	keywords     = {Graph mining, Hidden graphs, Core decomposition},
	abstract     = {Network exploration is an important research direction with many applications. In such a setting, the network is, usually, modeled as a graph G, whereas any structural information of interest is extracted by inspecting the way nodes are connected together. In the case where the adjacency matrix or the adjacency list of G is available, one can directly apply graph mining algorithms to extract useful knowledge. However, there are cases where this is not possible because the graph is hidden or implicit, meaning that the edges are not recorded explicitly in the form of an adjacency representation. In such a case, the only alternative is to apply a sequence of edge probing queries asking for the existence or not of a particular graph edge. However, checking all possible node pairs is costly (quadratic on the number of nodes). Thus, our objective is to execute as few edge probing queries as possible, since each such query is expected to be costly. In this work, we center our focus on the core decomposition of a hidden graph. In particular, we provide an efficient algorithm to detect the maximal subgraph Sk of G where the induced degree of every node u?Sk is at least k. Performance evaluation results demonstrate that significant performance improvements are achieved in comparison to baseline approaches.}
}
@article{PARK2020101798,
	title        = {Top-k user-specified preferred answers in massive graph databases},
	journal      = {Data & Knowledge Engineering},
	volume       = {127},
	pages        = {101798},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2020.101798},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17304950},
	author       = {Noseong Park and Andrea Pugliese and Edoardo Serra and V.S. Subrahmanian},
	keywords     = {Graph databases, Top-k querying, Preferred answers},
	abstract     = {There are numerous applications where users wish to identify subsets of vertices in a social network or graph database that are of interest to them. They may specify sets of patterns and vertex properties, and each of these confers a score to a subgraph. The users want to find the subgraphs with top-k highest scores. Examples in the real world where such subgraphs involve custom scoring methods include: techniques to identify sets of coordinated influence bots on Twitter, methods to identify suspicious subgraphs of nodes involved in nuclear proliferation networks, and sets of sockpuppet accounts seeking to illicitly influence star ratings on e-commerce platforms. All of these types of applications have numerous custom scoring methods. This motivates the concept of Scoring Queries presented in this paper ? unlike past work, an important aspect of scoring queries is that the users get to choose the scoring mechanism, not the system. We present the Advanced top-k (ATK) algorithm and show that it intelligently leverages graph indexes from the past but also presents novel pruning opportunities. We present an implementation of ATK showing that it beats out a baseline algorithm that builds on advanced subgraph matching methods with multiple graph database backends including Jena and GraphDB. We show that ATK scales well on real world graph databases from YouTube, Flickr, IMDb, and CiteSeerX.}
}
@article{CHEN2019100,
	title        = {Monitoring best region in spatial data streams in road networks},
	journal      = {Data & Knowledge Engineering},
	volume       = {120},
	pages        = {100--118},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.03.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18303951},
	author       = {Zijun Chen and Qin Yuan and Wenyuan Liu},
	keywords     = {Spatio-temporal databases, Data streams, Best region search, Road network},
	abstract     = {Given a set of spatial objects O, the search radius r and the submodular function f are specified by the user. The best region search (BRS) is to find an optimal area with fixed range size, in which the object set has the maximum submodular function value. The BRS problem has long been studied because of its wide application in spatial data mining, facility locating and so on. However, most of the existing work focus on either Euclidean space or motionless objects, which is not applicable in many real-life cases. In this paper, we propose the best region monitoring problem in the spatial data streams in road networks (MBRS). Many real life applications can obtain benefit from MBRS problem, such as monitoring traffic and tracking in ecology. We first propose an efficient algorithm to the static BRS problem in road networks, and extend the solution to a naive method to solve the MBRS problem. Then, we put forward effective pruning strategies and branch-and-bound algorithm GER on the basis of the preprocessing to monitor best region at different times. Finally, a large number of experiments verify the efficiency of the proposed method.}
}
@article{SANCHEZFERRERES201825,
	title        = {Aligning textual and model-based process descriptions},
	journal      = {Data & Knowledge Engineering},
	volume       = {118},
	pages        = {25--40},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.09.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18301496},
	author       = {Josep Sànchez-Ferreres and Han {van der Aa} and Josep Carmona and Lluís Padró},
	keywords     = {Business process management, Process models, Natural language processing, Alignments},
	abstract     = {Process model descriptions are an ubiquitous source of information that exists in any organization. To reach different types of stakeholders, distinct descriptions are often kept, so that process understandability is boosted with respect to individual capabilities. While the use of distinct representations allows more stakeholders to interpret process information, it also poses a considerable challenge: to keep different process descriptions aligned. In this paper, a novel technique to align process models and textual descriptions is proposed. The technique is grounded on projecting knowledge extracted from these two representations into a uniform representation that is amenable for comparison. It applies a tailored linguistic analysis of each description, so that the important information is considered when aligning description? elements. Compared to existing approaches that address this use case, our technique provides more comprehensive alignments, which encompass process model activities, events and gateways. Furthermore, the technique, which has been implemented into the platform nlp4bpm.cs.upc.edu, shows promising results based on experiments with real-world data.}
}
@article{KAMISALIC201936,
	title        = {Multi-level medical knowledge formalization to support medical practice for chronic diseases},
	journal      = {Data & Knowledge Engineering},
	volume       = {119},
	pages        = {36--57},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.12.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16303937},
	author       = {Aida Kami?ali? and David Riaño and Suzana Kert and Tatjana Welzer and Lili {Nemec Zlatolas}},
	keywords     = {Knowledge representation, Decision-making, Medical processes, Procedural knowledge modeling, Knowledge-based framework, Medical decision support},
	abstract     = {Medical processes combine medical actions which are performed by health care professionals while they observe signs and symptoms and decide about interventions, prescriptions, or tests, in order to deal with the health problem that affects a particular patient. Our research was centered in the representation of knowledge for the purpose of decision making in medical processes for chronic diseases. In order to achieve this objective, we followed three steps: (1) We performed an analysis and comparison of formal languages for procedural knowledge representation from a decision-making perspective. (2) We proposed an intuitive, easy, and effective mechanism of medical knowledge formalization. And, (3) we defined a methodology to model medical processes. Our new formalism to represent knowledge is called the extended Timed Transition Diagram (eTTD) and can be used to describe three basic levels of decision making in a long-term treatment: therapy strategy, dosage, and intolerances. The methodology can be applied manually to build eTTDs from clinical practice guidelines (CPGs) or automatically to construct them using the information available in clinical records about individual, multi-level medical processes. We validated eTTDs with clinical practice guidelines for arterial hypertension. The obtained models can be used as a baseline framework for medical, procedural decision support systems development.}
}
@article{PEREZ2018239,
	title        = {A case study on the use of machine learning techniques for supporting technology watch},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {239--251},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.08.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18302106},
	author       = {Alain Perez and Rosa Basagoiti and Ronny Adalberto Cortez and Felix Larrinaga and Ekaitz Barrasa and Ainara Urrutia},
	keywords     = {Text mining, Knowledge management applications, Multi-classification, Technology watch automation, Semantic annotations},
	abstract     = {Technology Watch human agents have to read many documents in order to manually categorize and dispatch them to the correct expert, that will later add valued information to each document. In this two step process, the first one, the categorization of documents, is time consuming and relies on the knowledge of a human categorizer agent. It does not add direct valued information to the process that will be provided in the second step, when the document is revised by the correct expert. This paper proposes Machine Learning tools and techniques to learn from the manually pre-categorized data to automatically classify new content. For this work a real industrial context was considered. Text from original documents, text from added value information and Semantic Annotations of those texts were used to generate different models, considering manually pre-established categories. Moreover, three algorithms from different approaches were used to generate the models. Finally, the results obtained were compared to select the best model in terms of accuracy and also on the reduction of the amount of document readings (human workload).}
}
@article{SILVAMUNOZ2019101724,
	title        = {A time-indexed mereology for SUMO},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101724},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101724},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303749},
	author       = {Lydia {Silva Muñoz} and Michael Grüninger},
	keywords     = {Mereology, Time-indexed mereology, Temporary mereology, Ontology, Upper-level ontology, Foundational ontology, SUMO, DOLCE, Ontology mapping, Change, Mereological change},
	abstract     = {While the period of time during which a subprocess occurs is precisely the time during which the part-whole relation with its main process takes place, part-whole relations between objects do not obey such a rule. The parts of an object can exist before the object is conformed as such, and can survive its dismantlement. In fact, there are no means for knowing when, during the existence of the part and the whole, their parthood relation holds unless an explicit account of time is represented. A time-indexed mereology characterizes how objects gain and lose parts over time by associating a time index to their part-whole relations. Keeping an account of when objects lose or gain parts is necessary for the correct representation of their spatial location and their participation in processes. Upper-level ontologies characterize the properties of the most basic, domain-independent entities, such as time, space, objects and processes. Two upper-level ontologies broadly used are The Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) and The Suggested Upper Merged Ontology (SUMO). However, while DOLCE provides a first-order time-indexed mereology for structuring its entities over time, SUMO provides a weaker axiomatization that does not represent the rules that determine how the mereological structure of objects evolve through time in the real world. This work proposes a first-order logic time-indexed mereology for SUMO based on its current representation of objects, time, and temporal location, thereby characterizing how objects gain and lose parts over time. The proposed theory sets the stage for the development of a time-indexed theory of spatial location, and for the representation of temporal restrictions on the participation of objects in processes. The time-indexed mereology of DOLCE and the proposed theory are formally compared, and their relative strength established by using ontology mapping. In order to achieve such a comparison, the representations of time, and temporal location of both upper-level ontologies are also formally compared.}
}
@article{DELIMANETO2018225,
	title        = {A semiotic-inspired machine for personalized multi-criteria intelligent decision support},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {225--238},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.012},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17300757},
	author       = {Fernando Buarque {de Lima Neto} and Denis Mayr {Lima Martins} and Gottfried Vossen},
	keywords     = {Multi-criteria decision support, Computational intelligence, Computational semiotics, Intelligent semiotic machine},
	abstract     = {The need for appropriate decisions to tackle complex problems increases every day. Selecting destinations for vacation, comparing and optimizing resources to create valuable products, or purchasing a suitable car are just a few examples of puzzling situations in which there is no standard form to find an appropriate solution. Such scenarios become arduous when the number of possibilities, restrictions, and factors affecting the decision rise, thereby turning decision makers into almost mere spectators. In such circumstances, decision support systems (DSS) can play an important role in guiding people and organizations towards more accurate decision making. However, conventional DSS lack the necessary adaptability to account for dynamic changes and are frequently inadequate to tackle the subjectivity inherent in decision-maker's preferences and intention. We argue that these shortcomings can be addressed by a suitable combination of Semiotic Theory and Computational Intelligence algorithms, which together can make up a new generation of DSS. In this article, a formal description of an Intelligent Semiotic Machine is provided and tried out in practical decision contexts. The results obtained show that our approach can provide well-suited decisions based on user preferences, achieving appropriateness while fanning out subjective options without losing decision context, objectivity, or accuracy.}
}
@article{WIESE2020101732,
	title        = {CloudDBGuard: A framework for encrypted data storage in NoSQL wide column stores},
	journal      = {Data & Knowledge Engineering},
	volume       = {126},
	pages        = {101732},
	year         = {2020},
	note         = {Special Issue DAWAK 2018 (Data Warehousing and Knowledge Discovery)},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101732},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19304331},
	author       = {Lena Wiese and Tim Waage and Michael Brenner},
	keywords     = {Property-preserving encryption, NoSQL databases, Wide Column Stores},
	abstract     = {Nowadays, cloud storage providers are widely used for outsourcing data. These remote cloud servers are not trustworthy when storing sensitive data. In this article we focus on the use case of storing data in a cloud database using a particular sub-category of NoSQL databases ? so-called wide column stores. Unfortunately security was not a primary concern of the NoSQL systems designers. Using encryption before outsourcing the data can provide security. Conventional encryption however limits the options for interaction because the encrypted data lacks properties of the plaintext data that the database systems rely on. Various schemes have been proposed for property-preserving encryption in order to overcome these issues, allowing a database to process queries over encrypted data. In this article we comprehensively present details of our framework CloudDBGuard that allows using property-preserving encryption in unmodified wide column stores. It hides the complexity of the encryption and decryption process and allows various adjustments on specific use cases in order to achieve a maximum of security, functionality and performance.}
}
@article{VANAHALLI2019101721,
	title        = {An efficient dynamic switching algorithm for mining colossal closed itemsets from high dimensional datasets},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101721},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101721},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18301009},
	author       = {Manjunath K. Vanahalli and Nagamma Patil},
	keywords     = {Bioinformatics, High dimensional dataset, Preprocessing, Frequent Colossal Closed Itemsets, Rowset, Closeness, Pruning strategy},
	abstract     = {The abundant data across a variety of domains including bioinformatics has led to the formation of dataset with high dimensionality. The conventional algorithms expend most of their time in mining a large number of small and mid-sized itemsets which does not enclose complete and valuable information for decision making. The recent research is focused on Frequent Colossal Closed Itemsets (FCCI), which plays a significant role in decision making for many applications, especially in the field of bioinformatics. The state-of-the-art algorithms in mining FCCI from datasets consisting of a large number of rows and a large number of features are computationally expensive, as they are either pure row or feature enumeration based algorithms. Moreover, the existing preprocessing techniques fail to prune the complete set of irrelevant features and irrelevant rows. The proposed work emphasizes an Effective Improvised Preprocessing (EIP) technique to prune the complete set of irrelevant features and irrelevant rows, and a novel efficient Dynamic Switching Frequent Colossal Closed Itemset Mining (DSFCCIM) algorithm. The proposed DSFCCIM algorithm efficiently switches between row and feature enumeration methods based on data characteristics during the mining process. Further, the DSFCCIM algorithm is integrated with a novel Rowset Cardinality Table, Itemset Support Table, two efficient methods to check the closeness of rowset and itemset, and two efficient pruning strategies to cut down the search space. The proposed DSFCCIM algorithm is the first dynamic switching algorithm to mine FCCI from datasets consisting of a large number of rows and a large number of features. The performance study shows the improved effectiveness of the proposed EIP technique over the existing preprocessing techniques and the improved efficiency of the proposed DSFCCIM algorithm over the existing algorithms.}
}
@article{SHAO201892,
	title        = {Mining range associations for classification and characterization},
	journal      = {Data & Knowledge Engineering},
	volume       = {118},
	pages        = {92--106},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.10.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17304329},
	author       = {Jianhua Shao and Achilleas Tziatzios},
	keywords     = {Classification, Characterization, Numerical ranges, Classification association rule mining},
	abstract     = {In this paper, we propose a method that is able to derive rules involving range associations from numerical attributes, and to use such rules to build comprehensible classification and characterization (data summary) models. Our approach follows the classification association rule mining paradigm, where rules are generated in a way similar to association rule mining, but search is guided by rule consequents. This allows many credible rules, not just some dominant rules, to be mined from the data to build models. In so doing, we propose several sub-range analysis and rule formation heuristics to deal with numerical attributes. Our experiments show that our method is able to derive range-based rules that offer both accurate classification and comprehensible characterization for numerical data.}
}
@article{HAMIDI2019101754,
	title        = {Consensus clustering algorithm based on the automatic partitioning similarity graph},
	journal      = {Data & Knowledge Engineering},
	volume       = {124},
	pages        = {101754},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101754},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18304919},
	author       = {Seyed Saeed Hamidi and Ebrahim Akbari and Homayun Motameni},
	keywords     = {Data clustering, Clustering ensemble, Consensus clustering, Automatic partitioning similarity graph},
	abstract     = {Consensus clustering has been recently applied as a solution to the clustering problem. This combines multiple clusterings of a set of objects into a single integrated clustering. Consensus clustering algorithms attempt to find stable and robust results by composing calculated results from the base clustering algorithms. However, finding a consensus clustering algorithm capable of obtaining the final clusters automatically has remained a challenge. Furthermore, most of them are affected by an outlier. In the present paper, a cluster-based consensus clustering algorithm is proposed based on partitioning similarity graph in which each vertex is a cluster composed of a set of points. Within the proposed algorithm, the Cosine, Jaccard, and Dice similarity measures are used to measure the similarity between two vertices. The proposed algorithm operates in three steps. First, outlier clusters are automatically obtained by pruning similarity graph. Then, meta-clusters are obtained by splitting graph and merging sub-graphs. Finally, based on the meta-clusters, the consensus solution and outlier points are obtained using majority voting. The proposed algorithm has linear time complexity in terms of the number of points. The number of clusters also is obtained automatically in a consensus solution. To evaluate the performance of the algorithm, real and artificial datasets were used. The obtained results showed a dramatic improvement in the accuracy of the final clusters.}
}
@article{KALYVAS201955,
	title        = {Skyline and reverse skyline query processing in SpatialHadoop},
	journal      = {Data & Knowledge Engineering},
	volume       = {122},
	pages        = {55--80},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.04.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18300715},
	author       = {Christos Kalyvas and Manolis Maragoudakis},
	keywords     = {Skyline queries, MapReduce, Computational geometry, SpatialHadoop, Big data, Database management, Methodologies and tools},
	abstract     = {In this paper, we study the problem of skyline and reverse skyline computation using SpatialHadoop, an extension of Hadoop that enhances its capabilities with spatial awareness. The exploitation of spatial indexing structures and the spatial properties of data can exploit MapReduce-based methods by reducing the reading, writing, computational and communicational overhead. Through our study, we propose two methods for skyline and reverse skyline computation, which operates in the spatial aware environment that SpatialHadoop provides. This environment allows for performing filtering on the initial dataset to retrieve an answer efficiently by using existing state-of-the-art indexing approaches. The proposed algorithms make use of the full capabilities of the indexing mechanisms provided by the SpatialHadoop and have been tested against large-scale datasets including a real-life, large-scale OpenStreetMap dataset. To the best of our knowledge, this is the first work that studies reverse skyline over SpatialHadoop.}
}
@article{PARK2019101748,
	title        = {DZI: An air index for spatial queries in one-dimensional channels},
	journal      = {Data & Knowledge Engineering},
	volume       = {124},
	pages        = {101748},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101748},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18302854},
	author       = {Kwangjin Park and Alexis Joly and Patrick Valduriez},
	keywords     = {Moving objects, Mobile computing, Selective tuning, Spatial index},
	abstract     = {The wireless data broadcast environment characteristics cause the data to be delivered sequentially via one-dimensional channels. A space-filling curve has been proposed for recent wireless data broadcast environments. However, air indexing introduces various problems, including the increase in the size of the index, conversion costs, and an increase in the search space because of an inefficient structure. In this paper, we propose a distribution-based Z-order air index and query processing algorithms suitable for a wireless data broadcast environment. The proposed index organizes the object identification (hereafter called ID) hierarchically only in terms of objects that are present. We compare the proposed technique with the well-known spatial indexing technique DSI by creating equations that represent the access time and tuning time, followed by conducting a simulation-based performance evaluation. The results from experimental show that our proposed index and algorithms support efficient query processing in both range queries and K-nearest neighbor queries.}
}
@article{ALFRJANI201988,
	title        = {A Hybrid Semantic Knowledgebase-Machine Learning Approach for Opinion Mining},
	journal      = {Data & Knowledge Engineering},
	volume       = {121},
	pages        = {88--108},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.05.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18300399},
	author       = {Rowida Alfrjani and Taha Osman and Georgina Cosma},
	keywords     = {Feature extraction, Classification, Semantic web, Knowledgebase},
	abstract     = {Opinion mining tools enable users to efficiently process a large number of online reviews in order to determine the underlying opinions. This paper presents a Hybrid Semantic Knowledgebase-Machine Learning approach for mining opinions at the domain feature level and classifying the overall opinion on a multi-point scale. The proposed approach benefits from the advantages of deploying a novel Semantic Knowledgebase approach to analyse a collection of reviews at the domain feature level and produce a set of structured information that associates the expressed opinions with specific domain features. The information in the knowledgebase is further supplemented with domain-relevant facts sourced from public Semantic datasets, and the enriched semantically-tagged information is then used to infer valuable semantic information about the domain as well as the expressed opinions on the domain features by summarising the overall opinions about the domain across multiple reviews, and by averaging the overall opinions about other cinematic features. The retrieved semantic information represents a valuable resource for modelling a machine learning classifier to predict the numerical rating of each review. Experimental evaluation revealed that the proposed Hybrid Semantic Knowledgebase-Machine Learning approach improved the precision and recall of the extracted domain features, and hence proved suitable for producing an enriched dataset of semantic features that resulted in higher classification accuracy.}
}
@article{JUCKETT201922,
	title        = {Concept detection using text exemplars aligned with a specialized ontology},
	journal      = {Data & Knowledge Engineering},
	volume       = {119},
	pages        = {22--35},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.11.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18301642},
	author       = {David A. Juckett and Eric P. Kasten and Fred N. Davis and Mark Gostine},
	keywords     = {Concept extraction, Exemplar matching, NLP, Ontology, Taxonomy, Progress notes, Pain medicine},
	abstract     = {Knowledge extraction from text documents requires identifying and classifying semantic content. Utilizing an appropriate domain ontology can facilitate this process if words and phrases can be linked to the classes and relationships within the ontology. This paper presents an exemplar-based algorithm to link text to semantically similar classes within an ontology constructed for the chronic pain medicine domain. Human annotators linked classes to text segments within a random document set for construction of an exemplar dictionary, which we examined for completeness using Zipf plot analysis. An algorithm was created to use this dictionary on previously unseen text to form a map between sentence text and probable class assignments. We performed a 5×5 cross-validation between human and algorithm annotations and examined both ROC and precision versus recall curves to show that the algorithm can identify the many medical and biopsychosocial components from the texts. We briefly describe a use case for detecting pain relief from various interventions utilizing the word-by-class maps. We conclude that an exemplar-based method can be a valuable tool in knowledge extraction from texts that share similar construction, such as medical progress notes.}
}
@article{MAJDARA2019101718,
	title        = {Online density estimation over high-dimensional stationary and non-stationary data streams},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101718},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101718},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18305962},
	author       = {Aref Majdara and Saeid Nooshabadi},
	keywords     = {Multivariate density estimation, Non-parametric, High-dimensional datasets, Blockized Bayesian sequential partitioning, Streaming data mining, Non-stationary data streams, Kernel density estimation, KL divergence},
	abstract     = {Efficient density estimation over an open-ended stream of high-dimensional data is of primary importance to machine learning. In general, parametric methods for density estimation are not suitable for high dimensions, and the widely used non-parametric methods like kernel density estimation (KDE) method fail for high-dimensional datasets. In this paper we present a framework for density estimation over stationary and non-stationary high-dimensional data streams. It is based on a blockized implementation of the Bayesian sequential partitioning (BSP) algorithm. The proposed framework satisfies the general design criteria for systems with the mission of online machine learning and data mining over data streams.}
}
@article{JUSZCZUK2020101782,
	title        = {Using similarity measures in prediction of changes in financial market stream data?Experimental approach},
	journal      = {Data & Knowledge Engineering},
	volume       = {125},
	pages        = {101782},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101782},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18306451},
	author       = {Przemys?aw Juszczuk and Jan Kozak and Krzysztof Kania},
	keywords     = {Similarity measures, Stream data, Data prediction, Financial data},
	abstract     = {In this study, we experimentally investigated the possibilities of using selected similarity measures for predicting future price directions in the market. The basic premise for this approach was the common assumption relating to the technical analysis, namely that ?history repeats itself,? and the ?instrument price reflects all factors that have an impact on its value.? This approach has been studied extensively in many publications. We purport that the subjective interpretation of the chart by the decision-maker should be taken into account. As every decision in the market in the case of manual trading or decision support systems is eventually made by a human, it is necessary to emphasize that the same situation in the market may be interpreted in a different manner by two different decision-makers. Our goal is to use the proposed similarity measure to identify past situations that occurred in the market, and invest accordingly. Under these assumptions, we tested the usefulness of selected measures proposed in the literature, as well as the measure proposed by us, on 21 financial instrument datasets divided into three groups (stock companies, currency pairs, and stock indexes). Moreover, we statistically verified the prediction efficiency for different financial instruments, including stocks, currency pairs, and stock indexes. The statistical verification demonstrated that the proposed approach exhibited higher predictive strength than the classical measures proposed in the literature.}
}
@article{BILALLI2019101727,
	title        = {PRESISTANT: Learning based assistant for data pre-processing},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101727},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101727},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18305123},
	author       = {Besim Bilalli and Alberto Abelló and Tomàs Aluja-Banet and Robert Wrembel},
	keywords     = {Data pre-processing, Data mining, Meta-learning},
	abstract     = {Data pre-processing is one of the most time consuming and relevant steps in a data analysis process (e.g., classification task). A given data pre-processing operator can have positive, negative, or zero impact on the final result of the analysis. Expert users have the required knowledge to find the right pre-processing operators. However, when it comes to non-experts, they are overwhelmed by the amount of pre-processing operators and it is challenging for them to find operators that would positively impact their analysis (e.g., increase the predictive accuracy of a classifier). Existing solutions either assume that users have expert knowledge, or they recommend pre-processing operators that are only ?syntactically? applicable to a dataset, without taking into account their impact on the final analysis. In this work, we aim at providing assistance to non-expert users by recommending data pre-processing operators that are ranked according to their impact on the final analysis. We developed a tool, PRESISTANT, that uses Random Forests to learn the impact of pre-processing operators on the performance (e.g., predictive accuracy) of 5 different classification algorithms, such as Decision Tree (J48), Naive Bayes, PART, Logistic Regression, and Nearest Neighbor (IBk). Extensive evaluations on the recommendations provided by our tool, show that PRESISTANT can effectively help non-experts in order to achieve improved results in their analytic tasks.}
}
@article{PEREIRA2020101760,
	title        = {A knowledge representation of the beginning of the innovation process: The Front End of Innovation Integrative Ontology (FEI2O)},
	journal      = {Data & Knowledge Engineering},
	volume       = {125},
	pages        = {101760},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101760},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18301162},
	author       = {Ariane Rodrigues Pereira and João José Pinto Ferreira and Alexandra Lopes},
	keywords     = {Front end of innovation, Ontology, Entrepreneurship, Concept development, Design science},
	abstract     = {The initial phase of the innovation process is widely accepted as an important driver of positive results for new products and for the success of businesses. The Front End of Innovation (FEI) is a multidisciplinary area that includes a variety of activities, such as ideation, opportunity identification and analysis, feasibility analysis, global trends analysis, concept definition, customer and competitor analysis, and even business model development. Due to the number and variety of FEI responsibilities, this phase entails a considerable level of complexity and decision making. This fact is reflected in the literature, where one finds a variety of FEI approaches and proposals, seldom overlapping and offering no clear consensual guidance. This work aimed at overcoming this gap by proposing an Ontology for the Front End of Innovation as a comprehensive knowledge representation of the FEI, the so-called Front End of Innovation Integrative Ontology (FEI2O). The ontology balanced the differences and addressed the shortcomings of the main FEI Reference Models and included contributions from the field. This research builds on a combination of qualitative and quantitative methodologies. It combines the qualitative methods of interviewing and focus group discussion to collect the views of domain experts, used to refine the artefact and later to evaluate the final ontology. Quantitative analysis of data was carried out using the Attribute Agreement approach. The FEI2O explicitly provides a description of a domain regarding concepts, properties and relations of concepts. The main benefit of the FEI2O is to provide a comprehensive formal reference model and a common vocabulary.}
}
@article{XIE201887,
	title        = {Uncertain data classification with additive kernel support vector machine},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {87--97},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16302701},
	author       = {Zongxia Xie and Yong Xu and Qinghua Hu},
	keywords     = {Uncertain data, Additive kernel, Support vector machines, Classification},
	abstract     = {In this work, a classification learning algorithm is designed within the framework of support vector machines through modeling uncertain data with additive kernels, which are introduced to calculate the similarity between uncertain samples characterized by probability density functions (PDFs). The PDFs are used as features of the uncertain samples, where the value of a feature is not a single value, but a set of values that represent the probability distribution of the noise. This is different with the existing methods which represent an uncertain sample by a set of new samples around it, but use the farthest or nearest value in the distribution to construct the optimal hyperplane. With the properties of kernel functions, we can easily extend additive kernels to compute the similarity between samples described with multiple uncertain features. Furthermore, we introduce an efficient algorithm to compute the kernel functions, and solve the additive kernel SVMs. The experimental results show the efficiency of additive-kernel SVMs in uncertain data classification.}
}
@article{GUPTA2020101777,
	title        = {An overlapping community detection algorithm based on rough clustering of links},
	journal      = {Data & Knowledge Engineering},
	volume       = {125},
	pages        = {101777},
	year         = {2020},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101777},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17304780},
	author       = {Samrat Gupta and Pradeep Kumar},
	keywords     = {Complex networks, Community structure, Overlapping communities, Rough sets, Clustering},
	abstract     = {The growth of networks is prevalent in almost every field due to the digital transformation of consumers, business and society at large. The unfolding of community structure in such real-world complex networks is crucial since it aids in gaining strategic insights leading to informed decisions. Moreover, the co-occurrence of disjoint, overlapping and nested community patterns in such networks demands methodologically rigorous community detection algorithms so as to foster cumulative tradition in data and knowledge engineering. In this paper, we introduce an algorithm for overlapping community detection based on granular information of links and concepts of rough set theory. First, neighborhood links around each pair of nodes are utilized to form initial link subsets. Subsequently, constrained linkage upper approximation of the link subsets is computed iteratively until convergence. The upper approximation subsets obtained during each iteration are constrained and merged using the notion of mutual link reciprocity. The experimental results on ten real-world networks and comparative evaluation with state-of-the-art community detection algorithms demonstrate the effectiveness of the proposed algorithm.}
}
@article{SAINTDIZIER2018290,
	title        = {Mining incoherent requirements in technical specifications: Analysis and implementation},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {290--306},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.05.006},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305232},
	author       = {Patrick Saint-Dizier},
	keywords     = {Requirement engineering, Linguistics of requirements, Incoherence analysis, Natural language processing},
	abstract     = {Requirements are designed to specify the features of systems. Even for a simple system, several thousands of requirements produced by different authors are needed. Overlap and incoherence problems are frequently observed. In this article, we propose a method to construct a corpus of various types of incoherences and a categorization that leads to the definition of patterns to mine incoherent requirements. We focus in this contribution on incoherences (1) which can be detected solely from linguistic factors and (2) which concern pairs of requirements. Together, these represent about 60% of the different types of incoherences; the other types require extensive domain knowledge and reasoning. The second part of this article develops several language-based patterns to detect incoherent requirements in texts. An indicative evaluation of the results concludes this contribution. More generally, this contribution opens new perspectives on incoherence analysis in texts.}
}
@article{WATTANAKITRUNGROJ201853,
	title        = {BEstream: Batch Capturing with Elliptic Function for One-Pass Data Stream Clustering},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {53--70},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.07.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301027},
	author       = {Niwan Wattanakitrungroj and Saranya Maneeroj and Chidchanok Lursinsap},
	keywords     = {Data stream clustering, One-pass learning, Elliptic-micro-cluster},
	abstract     = {Tremendous data have been generated in forms of streaming data and various distributions in most applications in different areas such as business, science, engineering, and medicine. This creates a new problem of space and time complexities where the incoming data can overflow the memory of an analysing machine and the flow of data may contain some scattered portions of data from different clusters. This situation leads to the incorrect clustering results. The challenge of the clustering on streaming data is clustering the data which continuously growing, unstable, and non-existent from time to time. This paper proposed the concept of discard-after-cluster based on the structure of adaptive hyper-elliptic micro-cluster components. Instead of gradually including each datum into its true cluster, a newly proposed set of algorithms capture the data in forms streaming batch and identify the cluster afterwards. The number of micro-clusters can be increased or decreased according to the dynamical distribution of incoming data as well as the overlap conditions of micro-clusters. A set of new recursive functions for updating parameters, checking overlap conditions, removing micro-clusters, and merging micro-clusters after discarding previously clustered data were introduced. The proposed algorithm was tested on synthetic and real data sets. The elliptic-micro-cluster structure is more suitable for capturing data than the other structures in the compared previous methods. In addition, our method named BEstream showed the more efficient results than the previous data stream clustering algorithms based on the rand index and normalized mutual information measures.}
}
@article{ALMARS2019139,
	title        = {Modelling user attitudes using hierarchical sentiment-topic model},
	journal      = {Data & Knowledge Engineering},
	volume       = {119},
	pages        = {139--149},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.01.005},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18304828},
	author       = {Abdulqader Almars and Xue Li and Xin Zhao},
	keywords     = {Hierarchical learning, Sentiment analysis, Hierarchical user-sentiment topic model},
	abstract     = {Uncovering the latent structure of various hotly discussed topics and the corresponding sentiments from different social media user groups (e.g., Twitter) is critical for helping organizations and governments understand how users feel about their services and facilities, along with the events happening around them. Although numerous research texts have explored sentiment analysis on the different aspects of a product, fewer works have focused on why users like or dislike those products. In this paper, a novel probabilistic model is proposed, namely, the Hierarchical User Sentiment Topic Model (HUSTM), to discover the hidden structure of topics and users while performing sentiment analysis in a unified way. The goal of the HUSTM is to hierarchically model the users? attitudes (opinions ) using different topic and sentiment information, including the positive, negative, and neutral. The experiment results on real-world data sets show the high quality of the hierarchy obtained by the HUSTM in comparison to those discovered using other state-of-the-art techniques.}
}
@article{GUO20181,
	title        = {Research on case retrieval of Bayesian network under big data},
	journal      = {Data & Knowledge Engineering},
	volume       = {118},
	pages        = {1--13},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.08.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18300624},
	author       = {Yuan Guo and Yuan Guo and K. Wu},
	keywords     = {Case retrieval, Big data, BN model, Hadoop platform},
	abstract     = {Although case retrieval of Bayesian network has greatly promoted the application of CBR technique in engineering fields, it is facing huge challenges with the arrival of the era of big data. First, huge computation task of BN learning caused by big data seriously hampers the efficiency of case retrieval; Second, with the increasing data size, the accuracy of case retrieval becomes poorer and poorer because existing methods of improving probability learning become unfit for new situation. Aiming at the first problem, this paper proposes Within-Cross algorithm to assign computation task to improve the result of parallel data processing and gain better efficiency of case retrieval. For the second problem, this paper proposes a new method called Weighted Index Coefficient of Dirichlet Distribution (WICDD) algorithm, which first measures the influence of different factors on probability learning and then gives a weight to each super parameter of Dirichlet Distribution to adjust the result of probability learning. Thus with WICDD algorithm, the effect of probability learning is greatly improved, which then further enhances the accuracy of case retrieval. Finally, lots of experiments are executed to validate the effectiveness of the proposed method.}
}
@article{ZHAO2018307,
	title        = {A comprehensive study: Sentence compression with linguistic knowledge-enhanced gated neural network},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {307--318},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.05.007},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305505},
	author       = {Yang Zhao and Xiaoyu Shen and Hajime Senuma and Akiko Aizawa},
	abstract     = {Sentence compression aims to shorten a sentence into a compression while remaining grammatical and preserving the underlying meaning of the original sentence. Previous works have recognized that linguistic features such as parts-of-speech tags and dependency labels are helpful to compression generation. In this work, we introduce a gating mechanism and propose a gated neural network that selectively exploits linguistic knowledge for deletion-based sentence compression. An extensive experiment was conducted on four downstream datasets, showing that the proposed gated neural network method leads to better compression upon both automatic metrics and human evaluation, compared to previous competitive compression methods. We also observed that the generated compression by the proposed gated neural network share more grammatical relations in common with the ground-truth compression than the baseline method, indicating that important grammatical relations, such as subject or object of a sentence, are more likely to be kept in the compression by the proposed method. Furthermore, visualization analysis is conducted to explore the selective use of linguistic features, suggesting that the gate mechanism could condition the predicted compression on different linguistic features.}
}
@article{WEAVER201971,
	title        = {Investigation design: The structural elements of knowledge-seeking efforts},
	journal      = {Data & Knowledge Engineering},
	volume       = {119},
	pages        = {71--88},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.12.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17304536},
	author       = {Bryan Weaver and Dieter Pfoser},
	keywords     = {Investigation design, Provenance, Collaborative information systems, Workflow, Knowledge management, Investigation meta model},
	abstract     = {Knowledge about human systems usually comes from deliberate, organized efforts. These efforts are increasingly collaborative with partnering among diverse teams of experts. This has spawned research in the means for data integration and data lineage, or provenance, to better enable sharing of data and workflows. Such research has focused on specific problems in domain representation, such as ontology domain modeling, provenance standards and methodologies, and automated workflow management. While all these contributions are highly relevant for knowledge seeking efforts, what is missing is a meta model that accounts for all elements of investigation. This work introduces the concept of investigation as a means to formalize knowledge seeking efforts involving collaborative human action. We model the investigation concept as a set of seven elements common to all knowledge-seeking efforts. Incorporated into the proposed investigation design are the concepts from emerging sensor-observation standards and W3C provenance standards. This design differs from other approaches for knowledge acquisition modeling in its focus on reifying the management of effort ? here referred to as an initiative, and the abstraction and reification of analytic results ? here referred to as judgments. Further, we provide and initial attempt to identify specific workflow and data model design patterns within each of the seven investigation elements. An example illustrates the various aspects of our approach.}
}
@article{MARTINRODILLA2018177,
	title        = {Assessing data analysis performance in research contexts: An experiment on accuracy, efficiency, productivity and researchers? satisfaction},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {177--204},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.06.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16303512},
	author       = {Patricia Martin-Rodilla and Jose Ignacio Panach and Cesar Gonzalez-Perez and Oscar Pastor},
	keywords     = {Data-analysis, Software-assistance, Data-analysis measurement, Data-analysis performance, Cognitive processes},
	abstract     = {Any knowledge generation process involves raw data comprehension, evaluation and inferential reasoning. These practices, common to different disciplines, are known as data analysis, and represent the most important set of activities in research contexts. Researchers use data analysis software methods and tools for generating new knowledge in their daily data analysis. In recent years, data analysis software has been incorporating explicit references in modelling of cognitive processes, in order to improve the assistance offered in data analysis tasks. However, data analysis software commercial suites are still resisting this inclusion, and there is little empirical work done in knowing more about how cognitive aspects inclusion in software helps researchers in analyzing data. In this paper, we evaluate the impact produced by the explicit inclusion of cognitive processes in the assistance logic of software tools design and development. We conducted an empirical experiment comparing data analysis performance using traditional software versus data analysis performance using software-assistance tools which incorporate cognitive processes in their design. The experiment is designed in terms of accuracy, efficiency, productivity and user satisfaction during the data analysis made by researchers. It allowed us to find some clear benefits of the cognitive inclusion in the software designed for research contexts, with statistically significant differences in terms of accuracy, productivity and researcher's satisfaction in support of this explicit inclusion, although some efficiency weaknesses are detected. We also discuss the implications of these results for the priority of cognitive inclusion in the software tools design for research contexts data analysis.}
}
@article{COBA2019142,
	title        = {Personalised novel and explainable matrix factorisation},
	journal      = {Data & Knowledge Engineering},
	volume       = {122},
	pages        = {142--158},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.06.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1830332X},
	author       = {Ludovik Coba and Panagiotis Symeonidis and Markus Zanker},
	abstract     = {Recommendation systems personalise suggestions to individuals to help them in their decision making and exploration tasks. In the ideal case, these recommendations, besides of being accurate, should also be novel and explainable. However, up to now most platforms fail to provide both, novel recommendations that advance users? exploration along with explanations to make their reasoning more transparent to them. For instance, a well-known recommendation algorithm, such as matrix factorisation (MF), optimises only the accuracy criterion, while disregarding other quality criteria such as the explainability or the novelty, of recommended items. In this paper, to the best of our knowledge, we propose a new model, denoted as NEMF, that allows to trade-off the MF performance with respect to the criteria of novelty and explainability, while only minimally compromising on accuracy. In addition, we recommend a new explainability metric based on nDCG, which distinguishes a more explainable item from a less explainable item. An initial user study indicates how users perceive the different attributes of these ?user? style explanations and our extensive experimental results demonstrate that we attain high accuracy by recommending also novel and explainable items.}
}
@article{CHEN2018124,
	title        = {A cost-effective solution for blog search},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {124--137},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.05.009},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301386},
	author       = {Lin-Chih Chen},
	keywords     = {Blog search, Aspect model, EM algorithm, NGD, Machine learning},
	abstract     = {In recent years, blogging research has grown rapidly in social networks and the number of posts has continued to grow. An effective search method for these growing posts is to use a blog search engine to help bloggers quickly and accurately find the information they need. The blog search engine faces an important problem of the short query entered by the user like the general search engine. This problem makes it difficult for search engines to correctly define the nature of user queries. Relevant literature shows that many researchers have tried to solve this problem by using different semantic analysis models. However, these models are not suitable for big data environments such as search engines because they need significant computing time. In this paper, we propose a semantic analysis model with a dynamic judgment mechanism. According to the experimental results, our model can achieve a cost-effective solution in computing time and execution performance. That is, we can use a relatively small amount of computing time to achieve a near-optimal solution performance.}
}
@article{BISKUP20191,
	title        = {Publishing inference?proof relational data: An implementation and experiments},
	journal      = {Data & Knowledge Engineering},
	volume       = {120},
	pages        = {1--44},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.11.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18301551},
	author       = {Joachim Biskup and Christine Dahn and Katharina Diekmann and Ralf Menzel and Dirk Schalge and Lena Wiese},
	keywords     = {A priori knowledge, Branch-and-bound, Combinatorial optimization, Confidentiality, Constant invention, Controlled interaction execution, Constraint, Data publishing, Data semantics, Data sharing, Distortion minimality, First-order logic, Inference proofness, Information sharing, Local lower bounds, Parallelization, Priority searching, Pure-predicate heuristic, Security, integrity, and protection, View},
	abstract     = {An agent might want to share information maintained by a relational database by means of data publishing, i.e., by generating a view customized for the further unrestricted usage by the anticipated clients. Often, however, the usability of the view has to be confined to ensure the confidentiality of particular pieces of information in need of being excluded from sharing. Previous work on Controlled Interaction Execution provides a sound and complete generation procedure for an inference?proof (i.e., consistent and confidentiality-preserving) view that has minimal distortion distance to the original database instance. Confidentiality is achieved regarding a policy declared in terms of first-order logic sentences to be kept hidden. Consistency ensures the compliance with postulated a priori knowledge of the clients, expressed as first-order logic sentences, too. The generation procedure has been designed to perform a depth-first search for satisfying the constraints and follows a branch-and-bound strategy for minimizing distortions. In this work we present an actual implementation of the generation procedure together with several optimizations. We exploit sophisticated local lower bounds on the number of additional distortions in subtrees to be explored to prune them early, and we employ coordinated parallelization for searching in many subtrees concurrently. Moreover, we describe an experimental evaluation in terms of runtime behavior. Finally, we also explore replacing depth-first searching by priority searching, exhibit special cases that can be handled more efficiently, present heuristics for only approximating distortion minimality, and discuss options of refined mechanisms to employ and invent constants to resolve current violations of constraints.}
}
@article{WANG2019105,
	title        = {The optimal feasible knowledge transfer path in a knowledge creation driven team},
	journal      = {Data & Knowledge Engineering},
	volume       = {119},
	pages        = {105--122},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.01.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303609},
	author       = {Xiangyu Wang and Jun Wang and Ruilin Zhang},
	keywords     = {Knowledge transfer, Knowledge creation, Ba, Knowledge transfer path},
	abstract     = {Knowledge transfer plays a significant role in knowledge-driven teams. In this study, we built a knowledge transfer measurement model based on the graph theory, in which we also built an individuals? correlated matrix. We devised a knowledge transfer solution algorithm and the optimal knowledge transfer path?s evaluation indexes and principles. And then, an experiment was conducted by a knowledge-driven team, from which the proposed model and algorithm have been confirmed. The results presented here may offer insight into reallocating the team?s resources and knowledge management effectively and efficiently.}
}
@article{LEGUEY2019101,
	title        = {Circular Bayesian classifiers using wrapped Cauchy distributions},
	journal      = {Data & Knowledge Engineering},
	volume       = {122},
	pages        = {101--115},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.05.005},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17304937},
	author       = {Ignacio Leguey and Concha Bielza and Pedro Larrañaga},
	keywords     = {Data mining, Classification, Circular statistics, Wrapped Cauchy distribution, Bayesian networks, Cortical layer},
	abstract     = {Capturing the dependences among circular variables within supervised classification models is a challenging task. In this paper, we propose four different supervised Bayesian classification algorithms where the predictor variables follow all circular wrapped Cauchy distributions. For this purpose, we introduce four wrapped Cauchy classifiers. The bivariate wrapped Cauchy distribution is the only bivariate circular distribution whose marginals and conditionals are also wrapped Cauchy distributions, a property that makes it possible to define these models easily. Furthermore, the wrapped Cauchy tree-augmented naive Bayes classifier requires the definition of a conditional circular mutual information measure between variables that follow wrapped Cauchy distributions. Synthetic data is used to illustrate, compare and evaluate the classification algorithms (including a comparison with the Gaussian TAN classifier, decision tree, random forest, multinomial logistic regression, support vector machine and simple neural network), leading to satisfactory predictive results. We also use a real neuromorphological dataset obtained from juvenile rat somatosensory cortex cells, where we measure the bifurcation angles of the dendritic basal arbors.}
}
@article{BARZEGAR2018319,
	title        = {Classification of composite semantic relations by a distributional-relational model},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {319--335},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.06.005},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1730561X},
	author       = {Siamak Barzegar and Brian Davis and Siegfried Handschuh and Andre Freitas},
	keywords     = {Semantic relation, Distributional semantic, Deep learning, Classification},
	abstract     = {Different semantic interpretation tasks such as text entailment and question answering require the classification of semantic relations between terms or entities within text. However, in most cases, it is not possible to assign a direct semantic relation between entities/terms. This paper proposes an approach for composite semantic relation classification using one or more relations between entities/term mentions, extending the traditional semantic relation classification task. The proposed model is different from existing approaches which typically use machine learning models built over lexical and distributional word vector features in that is uses a combination of a large commonsense knowledge base of binary relations, a distributional navigational algorithm and sequence classification to provide a solution for the composite semantic relation classification problem. The proposed approach outperformed existing baselines with regard to F1-score, Accuracy, Precision and Recall.}
}
@article{ALMUTAIRI2020101734,
	title        = {A Cryptographic Ensemble for secure third party data analysis: Collaborative data clustering without data owner participation},
	journal      = {Data & Knowledge Engineering},
	volume       = {126},
	pages        = {101734},
	year         = {2020},
	note         = {Special Issue DAWAK 2018 (Data Warehousing and Knowledge Discovery)},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.101734},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X19304355},
	author       = {Nawal Almutairi and Frans Coenen and Keith Dures},
	keywords     = {Data mining as a service, Privacy preserving data mining, Security, Data outsourcing},
	abstract     = {This paper introduces the twin concepts Cryptographic Ensembles and Global Encrypted Distance Matrices (GEDMs), designed to provide a solution to outsourced secure collaborative data clustering. The cryptographic ensemble comprises: Homomorphic Encryption (HE) to preserve raw data privacy, while supporting data analytics; and Multi-User Order Preserving Encryption (MUOPE) to preserve the privacy of the GEDM. Clustering can therefore be conducted over encrypted datasets without requiring decryption or the involvement of data owners once encryption has taken place, all with no loss of accuracy. The GEDM concept is applicable to large scale collaborative data mining applications that feature horizontal data partitioning. In the paper DBSCAN clustering is adopted for illustrative and evaluation purposes. The results demonstrate that the proposed solution is both efficient and accurate while maintaining data privacy.}
}
@article{CI2019159,
	title        = {General representational automata using deep neural networks},
	journal      = {Data & Knowledge Engineering},
	volume       = {122},
	pages        = {159--180},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.06.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1830627X},
	author       = {Johnpaul C.I. and Munaga V.N.K. Prasad and S. Nickolas and G.R. Gangadharan},
	keywords     = {Powerset, Representational learning, Automata, Unsupervised, Unlabeled, Categorical, Transition, Renewable energy, Bankruptcy},
	abstract     = {Unlabeled data representation constitutes a major challenge in data mining. Different unsupervised learning methods such as clustering and dimensionality reduction form the basis of data representations. The impact of attribute combinations and their interactions on data is less addressed by such models. A representation model supported with machine learning concepts can reveal more information about the nature of underlying data. We herein present a novel unsupervised minimum attribute instance selection (UMAIS) labeling algorithm that selects a categorical attribute as a class label, and a novel attribute-based powerset generation (APSG) algorithm for describing the formation of relevant attribute sets using correlation and powerset. Using these algorithms, we present a diagrammatic representation known as Representational Automata that depict the importance of interactions among correlated and non-correlated attributes present in an unlabeled dataset. We performed experiments using two large-scale datasets from the energy and financial domains and compared our approach with other standard classifiers. Our approach obtains a significantly better classification accuracy of 92.187% and 87.32% for the energy and financial datasets, respectively, compared to 74% and 82% of the linear classifier, respectively.}
}
@article{ZHANG201971,
	title        = {A most influential node group discovery method for influence maximization in social networks: A trust-based perspective},
	journal      = {Data & Knowledge Engineering},
	volume       = {121},
	pages        = {71--87},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.05.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305761},
	author       = {Bo Zhang and Lele Zhang and Cui Mu and Qin Zhao and Qianqian Song and Xuan Hong},
	keywords     = {Social network, Most influential node group, Influence maximization, Influence evaluation, Trust},
	abstract     = {Developing a computational method for discovering the most influential nodes in social networks is a significant challenge that reveals an approach for maximizing the influence diffusion. To improve the influence degree evaluation mechanism, we propose a trust-based most influential node discovery (TMID) method for discovering influential nodes in a social network. Four phases are performed to establish influence degrees for influential node discovery: (1) an influence propagation process, which reveals the influence diffusion records among nodes for obtaining the categories of nodes in the social network; (2) a trust evaluation method, which provides methods for calculating two types of trust relationships among users, namely, direct trust and indirect trust; (3) an influence evaluation phase, which calculates the explicit binary influence among users (named active influence), the potential binary influence among users (named inactive influence), and the unary influence of nodes (named node influence); and (4) a set of algorithms for discovering the most influential nodes, which comprise two phases: a heuristic phase and a greedy phase. We also list the results of a series of simulation tests for evaluating the performance of our mechanism.}
}
@article{AUGUSTO2018373,
	title        = {Automated discovery of structured process models from event logs: The discover-and-structure approach},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {373--392},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.04.007},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18301708},
	author       = {Adriano Augusto and Raffaele Conforti and Marlon Dumas and Marcello {La Rosa} and Giorgio Bruno},
	keywords     = {Automated process discovery, Process mining, Structured process model},
	abstract     = {This article tackles the problem of discovering a process model from an event log recording the execution of tasks in a business process. Previous approaches to this reverse-engineering problem strike different tradeoffs between the accuracy of the discovered models and their structural complexity. With respect to the latter property, empirical studies have demonstrated that block-structured process models are generally more understandable and less error-prone than unstructured ones. Accordingly, several methods for automated process model discovery generate block-structured models only. These methods however intertwine the objective of producing accurate models with that of ensuring their structuredness, and often sacrifice the former in favour of the latter. In this paper we propose an alternative approach that separates these concerns. Instead of directly discovering a structured process model, we first apply a well-known heuristic that discovers accurate but oftentimes unstructured (and even unsound) process models, and then we transform the resulting process model into a structured (and sound) one. An experimental evaluation on synthetic and real-life event logs shows that this discover-and-structure approach consistently outperforms previous approaches with respect to a range of accuracy and complexity measures.}
}
@article{KIM201958,
	title        = {Moving view field nearest neighbor queries},
	journal      = {Data & Knowledge Engineering},
	volume       = {119},
	pages        = {58--70},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.12.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305517},
	author       = {Wooil Kim and Changbeom Shim and Wan Heo and Sungmin Yi and Yon Dohn Chung},
	keywords     = {Moving view field nearest neighbor query, Spatial databases, Continuous query, Augmented reality, Location-based service},
	abstract     = {In this paper, we introduce a novel query type, the moving view field nearest neighbor (MVFNN) query ?a continuous version of the view field nearest neighbor (VFNN) query. This query continuously retrieves the nearest object in the query?s view field taking into account the changes of the query location and view field. In order to improve the performance of the query processing, we propose the notion of geographical and angular safe boundaries. We can skip redundant computation if the moved query satisfies the geographical and angular safe boundaries. Our method is easily applicable to existing services since we do not transform the general index structures. We prove the efficiency of our method by a series of experiments varying the parameters such as query?s moving speed, view field angle, and the distribution of data objects.}
}
@article{HARIKUMAR2019109,
	title        = {SubspaceDB : In-database subspace clustering for analytical query processing},
	journal      = {Data & Knowledge Engineering},
	volume       = {121},
	pages        = {109--129},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.05.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305633},
	author       = {Sandhya Harikumar and M.R. Kaimal},
	keywords     = {Machine learning, Subspace clustering, RDBMS, SQL operators, PostgreSQL},
	abstract     = {High dimensional data analysis within relational database management systems (RDBMS) is challenging because of inadequate support from SQL. Currently, subspace clustering of high dimensional data is implemented either outside DBMS using wrapper code or inside DBMS using SQL User Defined Functions/Aggregates(UDFs/UDAs). However, both these approaches have potential disadvantages from performance, resource usage, and security perspective for voluminous and frequently updated data. Hence, we propose an efficient querying system, named SubspaceDB, that implements subspace clustering directly within an RDBMS. SubspaceDB provides a novel set of query operators, each with an optimization objective, to facilitate interactive analysis for subspace clustering. The query operators focus on retrieving optimal answers to four key query types : (a) Medoid queries, (b) Neighbourhood queries, (c) Partial similarity queries, and (d) Prominence queries, that aid the formation of subspace clusters. Experimental studies on real and synthetic databases of size 15M tuples and 104 attributes show that our proposed approach SubspaceDB can be over 10 times faster as compared to a conventional wrapper-based or SQL UDF approach. The proposed approach is also efficient in retrieving at least 50% data with performance improvement of at least 25%.}
}
@article{XIAO201983,
	title        = {A state based energy optimization framework for dynamic virtual machine placement},
	journal      = {Data & Knowledge Engineering},
	volume       = {120},
	pages        = {83--99},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.03.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305116},
	author       = {Zhijiao Xiao and Zhong Ming},
	keywords     = {Virtual machine placement, Optimization, Multiplayer game theory, Evolutionary algorithm},
	abstract     = {The dynamic optimization of virtual machine (VM) placement is to dynamically adjust the placement of VMs on physical machines (PMs) to accomplish some objectives with certain constraints. On one hand, the number of possible combinations of PMs and VMs can be extremely large, which make the optimal solution very hard to get. On the other hand, the optimized solution needs be reachable from the old solution. To solve the problem from both sides, a partitioned optimization framework is proposed. First, four different states of PM, i.e. off, sleeping, ready and running, are introduced with different energy consumption. Running pool, sleeping pool and off pool are set up which partition PMs based on their different states. The classification helps us build the energy consumption model which is needed to evaluate mapping solutions. To decide if a new solution is better than the old one, only three parts of energy need be considered, i.e. energy changes for PMs in different states, energy consumed for changing the states of PMs, and extra energy consumption for migrating VMs. An energy model composed of these three parts is built as the optimization objective. A method is presented to decide the most suitable range to conduct the energy optimization through excluding some PMs in the sleep or off pool if the best solution achieved with those PMs included cannot be better than old solution. A memetic algorithm combining the partheno-genetic algorithm with the multiplayer random evolutionary game theory is proposed to achieve the global optimal solution and generate the executable live migration sequence from old mapping to the target one at the same time. According to our experimental results, our method can decrease the optimization range remarkably. Within the optimized scales, the proposed algorithm performed very well to approach the global optimal solution and guarantee the solution?s feasibility from old solution at the same time.}
}
@article{MEZNI2018100,
	title        = {A cloud services recommendation system based on Fuzzy Formal Concept Analysis},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {100--123},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.05.008},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17304664},
	author       = {Haithem Mezni and Taher Abdeljaoued},
	keywords     = {Cloud computing, Recommender system, Cloud service recommendation, Collaborative filtering, Fuzzy Formal Concept Analysis},
	abstract     = {Cloud computing is an attractive paradigm which offers variant services on demand. Many available cloud services offer the same or similar functionalities, which made it challenging for cloud users to choose a suitable service that meets with their preferences. Existing service selection approaches were not enough to solve this challenge. That's why researchers went for recommendation approaches trying to find a solution. Cloud service recommendation has become an important technique for cloud services. It helps users decide whether a service satisfies their requirements or not. However, two main recommendation problems remain unsolved yet, data sparsity and cold start. In addition, existing solutions mostly tried to adapt techniques inherited from Web service and e-commerce domains. This approach is not always adequate due to many reasons such as the cloud architecture, the various service models, etc. To address the problems stated above, we propose a Collaborative Filtering based recommendation system for cloud services using Fuzzy Formal Concept Analysis (Fuzzy FCA). Fuzzy FCA has a solid mathematical foundation and it's based on the lattice theory. The lattice representation will give an explicit description of our cloud environment (users, services, ratings, etc.) and, then, extract the pertinent information from it (similar users to an active user, ratings of each similar user, top services, etc.) which will make the recommendations more suitable. Experimental results confirmed our expectations and proved the efficiency of such an approach.}
}
@article{OUARET201960,
	title        = {AuMixDw: Towards an automated hybrid approach for building XML data warehouses},
	journal      = {Data & Knowledge Engineering},
	volume       = {120},
	pages        = {60--82},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.01.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18302477},
	author       = {Zoubir Ouaret and Doulkifli Boukraa and Omar Boussaid and Rachid Chalal},
	keywords     = {XML, SBVR, BIR, Data warehouse, User requirement, Dimensional modelling},
	abstract     = {In this paper, we present a mixed approach for building XML data warehouses from both XML data sources and user requirements. Our proposed approach aims at obtaining a unique multidimensional schema of theXML data warehouse. The approach follows three steps. During the first step, an intermediate SBVR model extended with template rules is used to accommodate a data warehousing system and to facilitate the automatic identification of facts and dimensions from the user requirements. After modelling XML data sources in UML, the second step corresponds to identifying candidate DW schemata from such data sources. The third step compares these candidate schemata with the reference model obtained from the user requirements. In this step, we propose to adapt similarity metric-extended Boolean models (BIR) and to use them in order to measure, rank and select the most appropriate data warehouse schema. Such a schema should best describe the data sources and exhaustively cover all the needed user requirements. To demonstrate our approach, we present a case study of the bibliographic database dblp.}
}
@article{GOLFARELLI201989,
	title        = {An active learning approach to build adaptive cost models for web services},
	journal      = {Data & Knowledge Engineering},
	volume       = {119},
	pages        = {89--104},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2019.01.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301957},
	author       = {Matteo Golfarelli and Simone Graziani and Stefano Rizzi},
	keywords     = {Cost models, Web services, Active learning, Regression trees},
	abstract     = {Delivering accurate estimates of query costs in web services is important in different contexts, e.g., to measure their Quality of Service. However, building a reliable cost model is difficult as (i) a web service is a black box often hiding a complex computation, (ii) a call to the same service can yield completely different costs by simply changing a parameter value, and (iii) execution costs can drift with time. In this paper we propose Tiresias, an approach that, given a web service exposing an interface with a fixed number of parameters, initializes and actively adapts a model to accurately predict query costs. The cost model is represented by a regression tree trained through two interleaved querying cycles: a passive one, where the costs measured for user-generated queries are used to update the tree, and an active one, where the service is probed through system-generated queries to cope with drifts in the cost function. Tiresias is finally evaluated in terms of effectiveness and efficiency through a set of experimental tests performed on both real and synthetic datasets.}
}
@article{PEREZ2018159,
	title        = {Automatic query reformulations for feature location in a model-based family of software products},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {159--176},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.06.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301714},
	author       = {Francisca Pérez and Jaime Font and Lorena Arcega and Carlos Cetina},
	keywords     = {Conceptual modeling, Information retrieval, Feature location, Query reformulation, Software maintenance and evolution, Families of software products},
	abstract     = {No maintenance activity can be completed without Feature Location (FL), which is finding the set of software artifacts that realize a particular functionally. Despite the importance of FL, the vast majority of work has been focused on retrieving code, whereas other software artifacts such as the models have been neglected. Furthermore, locating a piece of information from a query in a large repository is a challenging task as it requires knowledge of the vocabulary used in the software artifacts. This can be alleviated by automatically reformulating the query (adding or removing terms). In this paper, we test four existing query reformulation techniques, which perform the best for FL in code but have never been used for FL in models. Specifically, we test these techniques in two industrial domains: a model-based family of firmwares for induction hobs, and a model-based family of PLC software to control trains. We compare the results provided by our FL approach using the query and the reformulated queries by means of statistical analysis. Our results show that reformulated queries do not improve the performance in models, which could lead towards a new direction in the creation or reconsideration of these techniques to be applied in models.}
}
@article{FURTADO201816,
	title        = {A branch and bound strategy for Fast Trajectory Similarity Measuring},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {16--31},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.01.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17302021},
	author       = {Andre Salvaro Furtado and Laercio Lima Pilla and Vania Bogorny},
	keywords     = {Movement data, GPS trajectory similarity, Fast Trajectory Similarity},
	abstract     = {The increasing use of GPS-enabled devices allowed the collection of huge volumes of movement data in the form of trajectories. An important research problem in trajectory data analysis is the similarity measurement. For most applications, a trajectory-to-trajectory comparison is needed, and therefore, scalability of trajectory similarity measures directly impact the viability to use these techniques. Most similarity measures adopt a dynamic programming implementation, which has a quadratic time complexity in all cases, computing the pair-wise distance for all trajectory points, thus limiting the scalability of these measures. In this article we present a new strategy which takes into account the distance properties in Euclidean spaces to reduce the number of pair-wise point comparison required to determine all the matching points of two trajectories. An extensive experimental evaluation over real GPS trajectory datasets demonstrates the pruning power over 85% in the number of distance computations required to determine the matchings, and a significant execution time speed-up of up to one order of magnitude over the dynamic programming approach.}
}
@article{DO201867,
	title        = {A time-dependent model with speed windows for share-a-ride problems: A case study for Tokyo transportation},
	journal      = {Data & Knowledge Engineering},
	volume       = {114},
	pages        = {67--85},
	year         = {2018},
	note         = {Special Issue on Knowledge and Systems Engineering (KSE 2016)},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.06.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17300460},
	author       = {Phan-Thuan Do and Nguyen-Viet-Dung Nghiem and Ngoc-Quang Nguyen and Quang-Dung Pham},
	keywords     = {Passenger and parcels sharing, Share-a-ride, Tokyo Taxi, Dynamic graphs, Heuristic algorithms},
	abstract     = {This paper introduces a new fully time-dependent model of a public transportation system in the urban context that allows sharing a taxi between one passenger and parcels with speed widows consideration. The model contains many real-life case features and is presented by a mathematical formulation. We study both static and dynamic scenarios in comparison to traditional strategies, i.e., the direct delivery model. Moreover, we classify speed windows by different zones and congestion levels during a day in the urban context. Different speed windows induce the dynamic graph model for road networks and make the problem much more difficult to solve. Because of the complex model, the preprocessing steps on data as well as on dynamic graphs are very important. We use a greedy algorithm to initiate the solution and then use some local search techniques to improve the solution quality. The experimental data set is recorded by Tokyo-Musen Taxi company. The data set includes more than 20000 requests per day, more than 4500 used taxis per day and more than 130000 crossing points on the Tokyo map. Experimental results are analyzed on various factors such as the total benefit, the accumulating traveling time during the day, the number of used taxis and the number of shared requests.}
}
@article{LIU20181,
	title        = {Log sequence clustering for workflow mining in multi-workflow systems},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {1--17},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.04.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16301355},
	author       = {Xumin Liu and Moayad Alshangiti and Chen Ding and Qi Yu},
	keywords     = {Workflow mining, Sequence clustering, User behavior pattern, Probabilistic suffix tree, Non-negative matrix factorization},
	abstract     = {Current workflow mining efforts aim to discover process knowledge from user-system interaction logs and represent it as high-level workflow models. They assume there is one single workflow model in a system, or rely on the information that can explicitly link each log sequence to the underlying workflow model. Such assumptions may not be applicable to multi-workflow systems where the instances of different workflow models are mixed together without being differentiated. To address this issue, this paper proposes to apply sequence clustering methods to group similar log sequences together. Each sequence cluster corresponds to a workflow model and the log sequences in the cluster are the corresponding instances. This paper investigates different similarity measures, including structure-based and user-based, as well as different clustering algorithms, including one-side clustering and co-clustering. In order to incorporate user factors into sequence clustering, which is novel to the current sequence clustering methods, this paper proposes to model User Behavior Patterns (UBPs) as probabilistic distributions over sequences and learn it from the event log. We represent a UBP as a Probabilistic Suffix Tree and use it to measure sequence similarity. The co-clustering method leverages the dyad relationship between UBPs and log sequences to improve the clustering accuracy. An experimental study has been conducted and the result indicates that user-based methods outperform structure-based methods in terms of accuracy and they are more effective on dealing with noises in the log and the increase of log size. The UBP-sequence co-clustering method achieves the best performance which indicates the effectiveness of incorporating user factors and applying co-clustering.}
}
@article{DONG20181,
	title        = {Secure partial encryption with adversarial functional dependency constraints in the database-as-a-service model},
	journal      = {Data & Knowledge Engineering},
	volume       = {116},
	pages        = {1--20},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.01.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16300520},
	author       = {Boxiang Dong and Hui (Wendy) Wang},
	keywords     = {Database-as-a-service, Data outsourcing, Security, integrity, and protection, Database management, Management of integrity constraints},
	abstract     = {Cloud computing enables end-users to outsource their dataset and data management needs to a third-party service provider. One of the major security concerns of the outsourcing paradigm is how to protect sensitive information in the outsourced dataset. In some applications, only partial values are considered sensitive. In general, the sensitive information can be protected by encryption. However, data dependency constraints (together with the unencrypted data) in the outsourced data may serve as adversary knowledge and bring security vulnerabilities to the encrypted data. In this paper, we focus on functional dependency (FD), an important type of data dependency constraints, and study the security threats by the adversarial FDs. We design a practical scheme that can defend against the FD attack by encrypting a small amount of non-sensitive data (encryption overhead). We prove that finding the scheme that leads to the optimal encryption overhead is NP-complete, and design efficient heuristic algorithms, under the presence of one or multiple FDs. We design a secure query rewriting scheme that enables the service provider to answer various types of queries on the encrypted data with provable security guarantee. We extend our study to enforce security when there are conditional functional dependencies (CFDs) and data updates. We conduct an extensive set of experiments on two real-world datasets. The experiment results show that our heuristic approach brings small amounts of encryption overhead (at most 1% more than the optimal overhead), and enjoys a 10-time speedup compared with the optimal solution. Besides, our approach can reduce up to 90% of the encryption overhead of state-of-the-art solution.}
}
@article{BEDO201818,
	title        = {The Merkurion approach for similarity searching optimization in Database Management Systems},
	journal      = {Data & Knowledge Engineering},
	volume       = {113},
	pages        = {18--42},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.09.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16302245},
	author       = {Marcos V.N. Bedo and Daniel S. Kaster and Agma J.M. Traina and Caetano {Traina Jr.}},
	keywords     = {Similarity searching, Query optimization, Selectivity estimation, Design and implementation techniques},
	abstract     = {Modern Database Management Systems (DBMSs) retrieve songs that resemble those in a music dataset, identify plagiarism in a set of documents, or provide past cases to physicians by taking into account the characteristics of a query exam. All such tasks require the comparison of data by similarity, which can be expressed in terms of distance-based queries in metric spaces. Traditional query processing relies mostly on histograms for describing the data distribution space and choosing a data retrieval path that quickly leads to the answer, discarding comparisons of most unwanted data. However, DBMSs still lack adequate support for selectivity estimation of query operators for data types embedded in metric spaces. This article addresses a novel strategy that extends the query optimizer of a DBMS, so that it can also perform both logical and physical query plan optimizations in searches that include similarity predicates. The proposal, named Merkurion, updates the concept of Data Distribution Space and captures data distributions according to the distances between the elements within a dataset. Moreover, it employs concise representations of such distributions, called synopses, for the definition of rules that enable similarity searching optimization. An extensive evaluation of Merkurion in real-world datasets has proven its effectiveness and broad applicability to many data domains.}
}
@article{LEE2018116,
	title        = {An information-theoretic filter approach for value weighted classification learning in naive Bayes},
	journal      = {Data & Knowledge Engineering},
	volume       = {113},
	pages        = {116--128},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.11.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16301276},
	author       = {Chang-Hwan Lee},
	keywords     = {Feature weighting, Feature selection, Naive Bayes, Kullback-Leibler},
	abstract     = {Assigning weights in features has been an important topic in some classification learning algorithms. In this paper, we propose a new paradigm of assigning weights in classification learning, called value weighting method. While the current weighting methods assign a weight to each feature, we assign a different weight to the values of each feature. The performance of naive Bayes learning with value weighting method is compared with that of some other traditional methods for a number of datasets. The experimental results show that the value weighting method could improve the performance of naive Bayes significantly.}
}
@article{FAKAS20181,
	title        = {Thematic ranking of object summaries for keyword search},
	journal      = {Data & Knowledge Engineering},
	volume       = {113},
	pages        = {1--17},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.08.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16301793},
	author       = {Georgios J. Fakas and Yilun Cai and Zhi Cai and Nikos Mamoulis},
	keywords     = {Keyword search, Object summaries, Top- Queries, Relational databases},
	abstract     = {An Object Summary (OS) is a tree structure of tuples that summarizes the context of a particular Data Subject (DS) tuple. The OS has been used as a model of keyword search in relational databases; where given a set of keywords, the objective is to identify the DSs tuples relevant to the keywords and their corresponding OSs. However, a query result may return a large amount of OSs, which brings in the issue of effectively and efficiently ranking them in order to present only the most important ones to the user. In this paper, we propose a model that ranks OSs containing a set of identifying keywords (e.g., Chen) according to their relevance to a set of thematic keywords (e.g. Mining). We argue that the effective thematic ranking of OSs should combine gracefully IR-style properties, authoritative ranking and affinity. Our ranking problem is modeled and solved as a top-k group-by join; we propose an algorithm that computes the join efficiently, taking advantage of appropriate count statistics and compare it with baseline approaches. An experimental evaluation on the DBLP and TPC-H databases verifies the effectiveness and efficiency of our proposal.}
}
@article{KAPLAN201843,
	title        = {Location disclosure risks of releasing trajectory distances},
	journal      = {Data & Knowledge Engineering},
	volume       = {113},
	pages        = {43--63},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.10.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16302452},
	author       = {Emre Kaplan and Mehmet Emre Gürsoy and Mehmet Ercan Nergiz and Yücel Saygin},
	keywords     = {Privacy, Spatio-temporal data, Trajectory data, Data mining},
	abstract     = {Location tracking devices enable trajectories to be collected for new services and applications such as vehicle tracking and fleet management. While trajectory data is a lucrative source for data analytics, it also contains sensitive and commercially critical information. This has led to the development of systems that enable privacy-preserving computation over trajectory databases, but many of such systems in fact (directly or indirectly) allow an adversary to compute the distance (or similarity) between two trajectories. We show that the use of such systems raises privacy concerns when the adversary has a set of known trajectories. Specifically, given a set of known trajectories and their distances to a private, unknown trajectory, we devise an attack that yields the locations which the private trajectory has visited, with high confidence. The attack can be used to disclose both positive results (i.e., the victim has visited a certain location) and negative results (i.e., the victim has not visited a certain location). Experiments on real and synthetic datasets demonstrate the accuracy of our attack.}
}
@article{ZENI2018407,
	title        = {NómosT: Building large models of law with a tool-supported process},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {407--418},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.04.009},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18301733},
	author       = {N. Zeni and E.A. Seid and P. Engiel and J. Mylopoulos},
	abstract     = {Laws and regulations impact the design of software systems, as they introduce new requirements and constrain existing ones. The analysis of a software system and the degree to which it complies with applicable laws can be greatly facilitated by models of applicable laws. However, laws are inherently voluminous, often consisting of hundreds of pages of text, and so are their models, consisting of thousands of concepts and relationships. This paper studies the possibility of building models of law semi-automatically by using the NómosT tool. Specifically, we present the NómosT architecture and the process by which a user constructs a model of law semi-automatically, by first annotating the text of a law and then generating from it a model. We then evaluate the performance of the tool relative to building a model of a fragment of law manually. In addition, we offer statistics on the quality of the final output that suggest that tool supported generation of models of law reduces substantially human effort without affecting the quality of the output.}
}
@article{KLIEGR2018174,
	title        = {Antonyms are similar: Towards paradigmatic association approach to rating similarity in SimLex-999 and WordSim-353},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {174--193},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.03.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301325},
	author       = {Tomá? Kliegr and Ond?ej Zamazal},
	keywords     = {Word similarity, Word relatedness, WordSim353, SimLex-999},
	abstract     = {SimLex-999 is a widely used lexical resource for tracking progress in word similarity computation. It anchors similarity in synonymy, while other researchers such as Agirre et al. (2009) adopt broader similarity definition, involving also hyponymy and antonymy relations. Paradigmatic association covers synonymy, antonymy and co-hyponymy relations (Lapesa et al., 2014) largely overlapping with this broader similarity definition. Two words are paradigmatically associated if they can replace one another without affecting the grammaticality or acceptability of the sentence. Paradigmatic association can be elicited by asking for word interchangeability, which we hypothesize might be more natural than instructing raters with a list of relations to consider. To validate the proposed approach, we reannotated WordSim353 and SimLex-999 using two new guidelines: one explicitly qualifying antonymy as a similarity relation, the second one eliciting word interchangeability. As additional datasets we present a crowdsourced version of WordSim353 and a Czech version of SimLex-999. The paper also includes detailed analysis of lexical content of SimLex-999 and benchmark of thesaurus-based and distributional algorithms on multiple word similarity and relatedness datasets.}
}
@article{COMBI201894,
	title        = {A hybrid logic for XML reference constraints},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {94--115},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.02.005},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301842},
	author       = {Carlo Combi and Andrea Masini and Barbara Oliboni and Margherita Zorzi},
	keywords     = {XML, DTD, Constraints, Hybrid logics},
	abstract     = {XML emerged as the (meta) mark-up language for representing, exchanging, and storing semistructured data. The structure of an XML document may be specified either through DTD (Document Type Definition) language or through the specific language XML Schema. While the expressiveness of XML Schema allows one to specify both the structure and constraints for XML documents, DTD does not allow the specification of integrity constraints for XML documents. On the other side, DTD has a very compact notation opposed to the complex notation and syntax of XML Schema. Thus, it becomes important to consider the issue of how to express further constraints on DTD-based XML documents, still retaining the simplicity and succinctness of DTDs. According to this scenario, in this paper we focus on a (as much as possible) simple logic, named XHyb, expressive enough to allow the specification of the most common integrity and reference constraints in XML documents. In particular, we focus on constraints on ID and IDREF(S) attributes, which are the common way of logically connecting parts of XML documents, besides the usual parent-child relationship of XML elements. Differently from other previously proposed hybrid logics, in XHyb IDREF(S) attributes are explicitly expressible by means of suitable syntactical constructors. Moreover, we propose a refinement of the usual graph representation of XML documents in order to represent XML documents in a formal and intuitive way without flatten accessibility through IDREF(S) to the usual parent-child relationship. Model checking algorithms are then proposed, to verify that a given XML document satisfies the considered constraints.}
}
@article{KOPKE201925,
	title        = {Annotation paths for matching XML-Schemas},
	journal      = {Data & Knowledge Engineering},
	volume       = {122},
	pages        = {25--54},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.12.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17300654},
	author       = {Julius Köpke},
	keywords     = {Semantic annotation, Schema matching, Schema mapping, Document transformations, SAWSDL, XML, Interoperability},
	abstract     = {Annotation paths are a technique for the semantic annotation of XML-Schemas. The design rationale was to develop an embedded annotation method on top of SAWSDL which is fully declarative, easily applicable and still provides the proper expressiveness for high-quality logic-based schema matching. Annotation paths capture significantly more semantics than plain model references, the declarative annotation method of the W3C standard SAWSDL. While the concept of annotation paths was introduced in earlier works, we provide a new formalization of their structure and based thereon define their semantics and introduce matching methods to derive simple and complex value correspondences. Such correspondences can be used for the generation of executable schema mappings using state of the art mapping tools. We provide a comprehensive evaluation of our annotation method and the proposed matching algorithms using real-world schemas and reference ontologies and demonstrate the feasibility of generating executable mappings using a state of the art mapping system. Our evaluations show that our annotation-based matcher achieves outstanding matching quality (avg. f-measure between 0.98 and 1.0).}
}
@article{SONG2019101603,
	title        = {An effective High Recall Retrieval method},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101603},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.07.006},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303488},
	author       = {Justin JongSu Song and Wookey Lee and Jafar Afshar},
	keywords     = {High Recall Retrieval problem, Patent retrieval, Dynamic retrieval, Independent Dominating Set problem},
	abstract     = {The High Recall Retrieval (HRR) problem is one of the fundamental tasks for many applications such as patent retrieval, legal search, medical search, marketing research, charging and collecting tax, and literature review, etc. Given the data set obtained by the user?s query, the HRR problem is defined as finding the full set of relevant documents while less review effort will be required. It is very expensive to review a lot of documents since most of the reviewers are experts in the specific fields such as patent attorneys, lawyers, marketing, and medical professionals. However, the existing HRR methods have been far from satisfactory to make them enumerate all relevant documents. This is due to the fact that not only the sheer volume of documents inevitably including noises (non-relevant documents) but also the threshold measurements have been inadequately adopted. To deal with these problems, we propose a novel solution to efficiently find all the relevant documents among a large set of results. It consists of two steps: (a) to effectively classify the entire documents and (b) to select the representative documents in each class. We formalized the problem and theoretically verified the upper-bound of our method. In the experiments, our method is more efficient than the state-of-the-art query expansion methods.}
}
@article{KICHERER2018252,
	title        = {What you use, not what you do: Automatic classification and similarity detection of recipes},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {252--263},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.04.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17305402},
	author       = {Hanna Kicherer and Marcel Dittrich and Lukas Grebe and Christian Scheible and Roman Klinger},
	keywords     = {Recipe, Cooking food, Lassification, Multi-label, Text mining, Similarity search},
	abstract     = {Social media data is notoriously noisy and unclean. Recipe collections and their manual categorization built by users are no exception. However, a consistent and transparent categorization is vital to users who search for a specific entry. Similarly, curators are faced with the same challenge given a large collection of existing recipes: They first need to understand the data to be able to build a clean system of categories. This paper presents an empirical study using machine learning classifiers (logistic regression and decision trees) for the automatic classification of recipes on the German cooking website Chefkoch.de. The central question we aim at answering is: Which information is necessary to perform well at this task? In particular, we compare features extracted from the free text instructions of the recipe to those taken from the list of ingredients. On a sample of 5000 recipes with 87 classes, our feature analysis shows that a combination of nouns from the textual description of the recipe with ingredient features performs best in the logistic regression model (48% F1). Nouns alone achieve 45% F1 and ingredients alone 46% F1. However, other word classes do not complement the information from nouns. Decision trees constantly underperform the logistic regression, however, lead to an interpretable model. On a bigger training set of 50,000 instances, the best configuration shows an improvement to 57% highlighting the importance of a sizeable data set. In addition, we report on the use of these feature vectors for similarity search and ranking of recipes and evaluate on the task of (near) duplicate detection. We show that our method can reduce the manual curation with precision@3?=?0.52.}
}
@article{CHU2019101601,
	title        = {Enhancing portfolio return based on sentiment-of-topic},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101601},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.07.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303464},
	author       = {Victor W. Chu and Raymond K. Wong and Fang Chen and Ivan Ho and Joe Lee},
	keywords     = {Sentiment-of-topic, Topic modeling, Text mining, Business intelligence},
	abstract     = {While time-series analysis is commonly used in financial forecasting, a key source of market-sentiments is often omitted. Financial news is known to be making persuasive impact on the markets. Without considering this additional source of signals, only sub-optimal predictions can be made. This paper proposes a notion of sentiment-of-topic (SoT) to address the problem. It is achieved by considering sentiment-linked topics, which are retrieved from time-series with heterogeneous dimensions (i.e., numbers and texts). Using this approach, we successfully improve the prediction accuracy of a proprietary trade recommendation platform. Different from traditional sentiment analysis and unsupervised topic modeling methods, topics associated with different sentiment levels are used to quantify market conditions. In particular, sentiment levels are learned from historical market performances and commentaries instead of using subjective interpretations of human expressions. By capturing the domain knowledge of respective industries and markets, an impressive double-digit improvement in portfolio return is obtained as shown in our experiments.}
}
@article{PHAM201826,
	title        = {Learning multiple layers of knowledge representation for aspect based sentiment analysis},
	journal      = {Data & Knowledge Engineering},
	volume       = {114},
	pages        = {26--39},
	year         = {2018},
	note         = {Special Issue on Knowledge and Systems Engineering (KSE 2016)},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.06.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17300496},
	author       = {Duc-Hong Pham and Anh-Cuong Le},
	keywords     = {Sentiment analysis, Aspect based sentiment analysis, Representation learning, Multiple layer representation, Compositional vector models, Word embeddings},
	abstract     = {Sentiment Analysis is the task of automatically discovering the exact sentimental ideas about a product (or service, social event, etc.) from customer textual comments (i.e. reviews) crawled from various social media resources. Recently, we can see the rising demand of aspect-based sentiment analysis, in which we need to determine sentiment ratings and importance degrees of product aspects. In this paper we propose a novel multi-layer architecture for representing customer reviews. We observe that the overall sentiment for a product is composed from sentiments of its aspects, and in turn each aspect has its sentiments expressed in related sentences which are also the compositions from their words. This observation motivates us to design a multiple layer architecture of knowledge representation for representing the different sentiment levels for an input text. This representation is then integrated into a neural network to form a model for prediction of product overall ratings. We will use the representation learning techniques including word embeddings and compositional vector models, and apply a back-propagation algorithm based on gradient descent to learn the model. This model consequently generates the aspect ratings as well as aspect weights (i.e. aspect importance degrees). Our experiment is conducted on a data set of reviews from hotel domain, and the obtained results show that our model outperforms the well-known methods in previous studies.}
}
@article{GOLPIRA2018116,
	title        = {A novel Multiple Attribute Decision Making approach based on interval data using U2P-Miner algorithm},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {116--128},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.03.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17302872},
	author       = {Hêri? Golpîra},
	keywords     = {Supply chain, Supplier selection, MADM, Knowledge discovery in databases, Uncertainty, Pattern mining, Linear assignment method},
	abstract     = {This paper aims to introduce a technique for order of preference using pattern mining based on Decision Makers (DMs) level of risk aversion. However, the model is essentially defined on the problem of supplier selection, it can be used to deal with almost any similar decision making problem. This novel Multiple Attribute Decision Making (MADM) model takes the advantages of the U2P-Miner algorithm, the interval data weighting method, and the Linear Assignment Method (LAM). The key idea behind the method is to consider the attribute with more frequent patterns as the common attribute and to assign a smaller weight to it. Since, the model handles interval data as input, it can be guaranteed that the model uses the detailed information and, therefore, the resulting weight factors are more realistic. The DMs risk aversion level is also addressed in the model, which is necessary in real-life situations. Accordingly, the proposed decision making process depends directly on DMs attitude toward risk. It gives DM the opportunity to make a decision in two ways: 1) based on the specified risk aversion level, 2) based on an integrated approach using LAM. The linearity of the LAM, by itself, enhances the scalability of the model. Moreover, the necessity of providing pairwise comparison judgments is completely eliminated in the model and, therefore, the reliability of the decision making is enhanced. The effectiveness of the model is finally demonstrated through a numerical example while the broad comparative and sensitivity analysis further proves its validity and superiority.}
}
@article{BASSILIADES201881,
	title        = {PaaSport semantic model: An ontology for a platform-as-a-service semantically interoperable marketplace},
	journal      = {Data & Knowledge Engineering},
	volume       = {113},
	pages        = {81--115},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.11.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17300551},
	author       = {Nick Bassiliades and Moisis Symeonidis and Panagiotis Gouvas and Efstratios Kontopoulos and Georgios Meditskos and Ioannis Vlahavas},
	keywords     = {Cloud computing, Platform-as-a-Service, Cloud Marketplace, Semantic interoperability, Ontologies, Quality and metrics},
	abstract     = {PaaS is a Cloud computing service that provides a computing platform to develop, run, and manage applications without the complexity of infrastructure maintenance. SMEs are reluctant to enter the growing PaaS market due to the possibility of being locked in to a certain platform, mostly provided by the market's giants. The PaaSport Marketplace aims to avoid the provider lock-in problem by allowing Platform provider SMEs to roll out semantically interoperable PaaS offerings and Software SMEs to deploy or migrate their applications on the best-matching offering, through a thin, non-intrusive Cloud broker. In this paper, we present the PaaSport semantic model, namely an OWL ontology, extension of the DUL ontology. The ontology is used for semantically representing (a) PaaS offering capabilities and (b) requirements of applications to be deployed. The ontology has been designed to optimally support a semantic matchmaking and ranking algorithm that recommends the best-matching PaaS offering to the application developer. The DUL ontology offers seamless extensibility, since both PaaS Characteristics and parameters are defined as classes; therefore, extending the ontology with new characteristics and parameters requires the addition of new specialized subclasses of the already existing classes, which is less complicated than adding ontology properties. The PaaSport ontology is evaluated through verification tools, competency questions, human experts, application tasks and query performance tests.}
}
@article{PARK2019101604,
	title        = {Constructing a paraphrase database for agglutinative languages},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101604},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.07.007},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1730349X},
	author       = {Hancheol Park and Kyo-Joong Oh and Ho-Jin Choi and Gahgene Gweon},
	keywords     = {Affix modification-based bilingual pivoting method, Paraphrase database, Text mining, Paraphrase generation, Question-answering systems},
	abstract     = {Paraphrase databases (PPDBs) are valuable resources for applications that use natural language processing (NLP) technology. In order to construct a high-quality PPDB for agglutinative languages, we propose a phrasal paraphrase extraction method; namely, affix modification-based bilingual pivoting method (AMBPM). AMBPM is suitable for agglutinative languages because it addresses the problems of lexical data sparsity and of not considering morphological word structure. In addition, we propose ?improved AMBPM,? which is an improvement on AMBPM by addressing the problem of extracting incorrect stem paraphrase pairs caused by low semantic content stems (LSCSs) by using a rule-based filtering approach. In our experiments on AMBPM, we evaluate AMBPM and compare two state-of-the-art paraphrase extraction methods: the syntactic constraints-based bilingual pivoting method (SCBPM) and word embedding method. In the experiments on improved AMPBM, we evaluate our method and compare the resulting PPDB with four types of databases; PPDB constructed by using the original AMBPM, two PPDBs constructed by using two types of word-embedding-based methods (stem embedding and phrase embedding), and an existing thesaurus. The comparison is performed by using two NLP applications: sentential paraphrase generation and a question answering (QA) system. The experimental results demonstrate that, AMBPM outperforms the state-of-the-art paraphrase extraction methods. In addition, the improved AMBPM, which uses a rule-based filtering method, significantly improves AMBPM. Moreover, although a small amount of training data was used with no aid from linguistic resources, the PPDB constructed with the improved AMBPM is more useful than the four databases for the agglutinative language used in our study. We also publicized the Korean PPDB that was constructed using the improved AMBPM.}
}
@article{DUGGIMPUDI20191,
	title        = {Spatio-temporal outlier detection algorithms based on computing behavioral outlierness factor},
	journal      = {Data & Knowledge Engineering},
	volume       = {122},
	pages        = {1--24},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.12.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301829},
	author       = {Maria Bala Duggimpudi and Shaaban Abbady and Jian Chen and Vijay V. Raghavan},
	keywords     = {Spatio-temporal outliers, Algorithms, Behavioral outlierness factor, Cluster, Hurricane, Efficiency},
	abstract     = {A major task in spatio-temporal outlier detection is to identify objects that exhibit abnormal behavior either spatially, and/or temporally. There have only been a few algorithms proposed for detecting spatial and/or temporal outliers. One example is the Local Density-Based Spatial Clustering of Applications with Noise (LDBSCAN). Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is mainly for clustering; it just tells us whether an object belongs to a cluster or it is an outlier. A measure known as Local Outlier Factor (LOF) gives a quantitative measure of outlierness to each object, where a high LOF score means it is potentially an outlier. LDBSCAN algorithm, which combines the above notions, considers only the spatial context. Furthermore, the notion of a cluster is defeated (i.e. LDBSCAN may report clusters having less than the minimum required points in a cluster), and some of the outliers may not be detected because of the limitation of the existing conditions in the LDBSCAN algorithm. In this paper, we propose two algorithms, namely Spatio-Temporal Behavioral Density-based Clustering of Applications with Noise (ST-BDBCAN) and Approx-ST-BDBCAN. ST-BDBCAN algorithm adopts the proposed, new concept, called Spatio-Temporal Behavioral Outlier Factor (ST-BOF), which is a spatio-temporal extension to LOF. It also uses both spatial and temporal attributes simultaneously to define the context. By doing so, the relative importance of spatial continuity or temporal continuity appropriate to the application at hand can be established. The Approx-ST-BDBCAN algorithm achieves improved scalability, with minimal loss of detection accuracy by partitioning data points for parallel processing. Experimental results on synthetic, and buoy datasets suggest that our proposed algorithms are accurate and computationally efficient. Additionally, new Outlier Association with Hurricane Intensity Index (OAHII) measures are introduced for quantitative evaluation of the results from buoy dataset.}
}
@article{ROBLOT2018339,
	title        = {Cardinality constraints and functional dependencies over possibilistic data},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {339--358},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.04.005},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1830168X},
	author       = {Tania Roblot and Sebastian Link},
	keywords     = {Data and knowledge visualization, Data models, Database semantics, Management of integrity constraints, Requirements engineering},
	abstract     = {Modern applications require advanced techniques and tools to process large volumes of uncertain data. For that purpose we study cardinality constraints and functional dependencies as a declarative mechanism to control the occurrences and interrelationships of uncertain data. Uncertainty is modeled qualitatively by assigning to each object a degree of possibility by which the object occurs in an uncertain instance. Cardinality constraints and functional dependencies are assigned a degree of certainty that stipulates on which objects they hold. Our framework empowers users to model uncertainty in an intuitive way, without the requirement to put a precise value on it. Our class of cardinality constraints and functional dependencies enjoys a natural possible world semantics, which is exploited to establish several tools to reason about them. We characterize the associated implication problem axiomatically and algorithmically in linear input time. Furthermore, we show how to visualize any given set of our cardinality constraints and functional dependencies in the form of an Armstrong sketch. Even though the problem of finding an Armstrong sketch is precisely exponential, our algorithm computes a sketch with conservative use of time and space. Data engineers may therefore compute Armstrong sketches that they can jointly inspect with domain experts in order to consolidate the set of cardinality constraints and functional dependencies meaningful for a given application domain.}
}
@article{ALSHARUEE2018194,
	title        = {Sentiment analysis: An automatic contextual analysis and ensemble clustering approach and comparison},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {194--213},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.04.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301866},
	author       = {Murtadha Talib AL-Sharuee and Fei Liu and Mahardhika Pratama},
	keywords     = {Text mining, Sentiment analysis, Unsupervised learning, Contextual analysis, Ensemble learning, k-means algorithm},
	abstract     = {Product reviews are one of the most important resources to determine public sentiment. The existing literature on review sentiment analysis mostly utilizes supervised models, which usually suffer from domain-dependency and require expensive manual labelling effort to provide training data. This article addresses these issues by describing a completely automatic and unsupervised approach to sentiment analysis. The method consists of two phases, which are contextual analysis and unsupervised ensemble learning. In the implementation of both phases, a sentiment lexicon, SentiWordNet, is deployed. Using effective contextual procedures and modifying the base learning component (the k-means algorithm) results in developing a successful approach to sentiment analysis which can overcome the domain-dependency and the labelling cost problems. The results show that the proposed nonrandom initialization of k-means yields a significant improvement compared to other algorithms. In terms of accuracy and performance, the proposed method is effective compared to supervised and unsupervised approaches. We also introduce new sentiment analysis problems relating to Australian airlines and home builders which could be potential benchmark problems in the sentiment analysis field. Our experiments on datasets from different domains show that contextual analysis and the ensemble phases improve the clustering performance in term of accuracy, stability and generalizability.}
}
@article{LIU201880,
	title        = {Using big data and network analysis to understand Wikipedia article quality},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {80--93},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.02.004},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18300685},
	author       = {Jun Liu and Sudha Ram},
	abstract     = {The research reported in this paper focuses on the question of why Wikipedia articles are different in quality. Since these articles are developed in an open and social environment, our work investigates if the social capital of contributors plays a role in determining the quality of the articles. We focus on three major types of social capital with respect to teams of contributors working on Wikipedia articles: internal bonding, external bridging and functional diversity. Through a social network analysis of these articles based on a dataset extracted from its edit history, our research finds that all three types of social capital have a significant impact on their quality. In addition, we found that internal bonding interacts positively with external bridging resulting in a multiplier effect on article quality. The findings of our research have implications for developing automated techniques for quality assessment of Wikipedia and also provide insights into improving quality of these articles.}
}
@article{MARAN2018152,
	title        = {Domain content querying using ontology-based context-awareness in information systems},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {152--173},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.03.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301428},
	author       = {Vinícius Maran and Alencar Machado and Guilherme Medeiros Machado and Iara Augustin and José Palazzo M. {de Oliveira}},
	keywords     = {Ontology, Context-awareness, Information systems, Ubiquitous computing},
	abstract     = {Ubiquitous computing technologies have been applied in several areas. However, it still presents a number of challenges, both for the full implementation of technologies and for the integration with existing information systems. One of the main mismatches evidenced by recent works is how context-awareness, a widely used capability in ubiquitous computing and actual information systems with relational databases may be integrated to allow ubiquitous and traditional systems to query relational data sources without the necessity to modify the schema of the database. This paper presents an integration model relating context and domain information allowing relational data to be retrieved in context without the necessity to change the originally used relational queries. A set of linking rules and algorithms are formalized in a model and this model is implemented in a prototype. The evaluation of the model is performed by applying it in a case study in a Massive Open Online Course (MOOC) platform. The evaluation of the model by the application of it in a case study in a MOOC platform demonstrated the possibility to use an ontology frequently used in ubiquitous middleware as an extra filtering layer for information systems without the necessity to recreate queries or make a re-engineering in the relational database schema. The results of the queries after the application of the model showed an average decrease of 21% in returned tuples, which was evaluated as a significant reduce in tuple results.}
}
@article{YAGO201848,
	title        = {ON-SMMILE: Ontology Network-based Student Model for MultIple Learning Environments},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {48--67},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.02.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301945},
	author       = {Hector Yago and Julia Clemente and Daniel Rodriguez and Pedro Fernandez-de-Cordoba},
	keywords     = {Ontological engineering, Student modeling, Ontology network, Learning supervision, Semantic web},
	abstract     = {Currently, many educational researchers focus on the extraction of information about the learning progress to properly assist students. We present ON-SMMILE, a student-centered and flexible student model which is represented as an ontology network combining information related to (i) students and their knowledge state, (ii) assessments that rely on rubrics and different types of objectives, (iii) units of learning and (iv) information resources previously employed as support for the student model in intelligent virtual environment for training/instruction and here extended. The aim of this work is to design and build methodologically, throughout ontological engineering, the ON-SMMILE model to be used as support of future works closely linked to supervision of student's learning as competence-based recommender system. For this purpose, our model is designed as a set of ontological resources that have been extended, standardized, interrelated and adapted to be used in multiple learning environments. In this paper, we also analyze the available approaches based on instructional design which can be added to ontology network to build the proposed model. As a case study, a chemical experiment in a virtual environment and its instantiation are described in terms of ON-SMMILE.}
}
@article{LI2019101605,
	title        = {Social emotion classification based on noise-aware training},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101605},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.07.008},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303506},
	author       = {Xin Li and Yanghui Rao and Haoran Xie and Xuebo Liu and Tak-Lam Wong and Fu Lee Wang},
	keywords     = {Social emotion classification, Emotional concentration, Convolutional neural network, Topic modeling},
	abstract     = {Social emotion classification draws many natural language processing researchers? attention in recent years, since analyzing user-generated emotional documents on the Web is quite useful in recommending products, gathering public opinions, and predicting election results. However, the documents that evoke prominent social emotions are usually mixed with noisy instances, and it is also challenging to capture the textual meaning of short messages. In this work, we focus on reducing the impact of noisy instances and learning a better representation of sentences. For the former, we introduce an ?emotional concentration? indicator, which is derived from emotional ratings to weight documents. For the latter, we propose a new architecture named PCNN, which utilizes two cascading convolutional layers to model the word-phrase relation and the phrase?sentence relation. This model regards continuous tokens as phrases based on an assumption that neighboring words are very likely to have internal relations, and semantic feature vectors are generated based on the phrase representation. We also present a Bayesian-based model named WMCM to learn document-level semantic features. Both PCNN and WMCM classify social emotions by capturing semantic regularities in language. Experiments on two real-world datasets indicate that the quality of learned semantic vectors and the performance of social emotion classification can be improved by our models.}
}
@article{BUI201840,
	title        = {A novel evolutionary multi-objective ensemble learning approach for forecasting currency exchange rates},
	journal      = {Data & Knowledge Engineering},
	volume       = {114},
	pages        = {40--66},
	year         = {2018},
	note         = {Special Issue on Knowledge and Systems Engineering (KSE 2016)},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.07.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1730054X},
	author       = {Lam Thu Bui and Van {Truong Vu} and Thi Thu {Huong Dinh}},
	keywords     = {Currency exchange rates forecasting, Ensemble learning, Multi-objective evolutionary, Non-dominated differential evolution},
	abstract     = {Due to the potential impact of the (currency) exchange rate risk in the financial market, forecasting exchange rate (FET) has become a hot topic in both academic and practical worlds. For many years, the various methods have been proposed and used for FET problems including the method of the artificial neural network (ANN). However, in many cases of FET, there is the limitation of using separate methods since they are not able to fully capture financial characteristics. Recently, more researchers have been beginning to pay attention to FET based on an ensemble of forecasting models (in other words, the combination of individual methods). Previous studies of ensemble methods have shown that the performance of an ensemble depends on two key elements (1) The individual performance and (2) diversity degree of base learners. The main idea behind this paper comes from these key elements, the authors use ANNs as the base method (or weak learners), and weights of these ANNs will be optimized by using multi-objective evolutionary algorithms (MOEAs) including the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) and the Non-Dominated Sorting Differential Evolution (NSDE) using directional information. To assist MOEAs, a number of diversity-preservation mechanisms are used to generate diverse sets of base classifiers and finally we propose to use modified Adaboost algorithms to combine the results of weak learners for overall forecasts. The results show that the proposed novel ensemble learning approach can achieve higher forecasting performance than those of individual ones.}
}
@article{NALCHIGAR2018359,
	title        = {Business-driven data analytics: A conceptual modeling framework},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {359--372},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.04.006},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18301691},
	author       = {Soroosh Nalchigar and Eric Yu},
	keywords     = {Conceptual modeling, Data analytics, Machine learning, Business analytics, Goal-oriented requirements engineering, Enterprise modeling},
	abstract     = {The effective development of advanced data analytics solutions requires tackling challenges such as eliciting analytical requirements, designing the machine learning solution, and ensuring the alignment between analytics initiatives and business strategies, among others. The use of conceptual modeling methods and techniques is seen to be of considerable value in overcoming such challenges. This paper proposes a modeling framework (including a set of metamodels and a set of design catalogues) for requirements analysis and design of data analytics systems. It consists of three complementary modeling views: business view, analytics design view, and data preparation view. These views are linked together to connect enterprise strategies to analytics algorithms and to data preparation activities. The framework includes a set of design catalogues that codify and represent an organized body of business analytics design knowledge. As the first attempt to validate the framework, three real-world data analytics case studies are used to illustrate the expressiveness and usability of the framework. Findings suggest that the framework provides an adequate set of concepts to support the design and implementation of analytics solutions.}
}
@article{MCDANIEL201832,
	title        = {Assessing the quality of domain ontologies: Metrics and an automated ranking system},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {32--47},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.02.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16304062},
	author       = {Melinda McDaniel and Veda C. Storey and Vijayan Sugumaran},
	keywords     = {Domain ontology, Interoperability, Metrics, Ontology assessment, Ontology evaluation, Ranking, Ontology, Semiotics, Semiotic layers, Domain Ontology Ranking System},
	abstract     = {The ability of a user to select an appropriate, high-quality domain ontology from a set of available options would be most useful in knowledge engineering and other intelligent applications. This capability, however, requires good quality assessment metrics as well as automated support when there is a large number of ontologies from which to make a selection. This research analyzes existing metrics for domain ontology evaluation and extends them to derive a Layered Ontology Metrics Suite based on semiotic theory. The metrics are implemented in a Domain Ontology Ranking System (DoORS) prototype, the purpose of which is to search an ontology library for specific terms to retrieve candidate domain ontologies and then assess their quality and suitability based upon the suite of metrics. The prototype system is compared to existing approaches to automated ontology quality ranking to illustrate the usefulness of the research.}
}
@article{CABRERADIEGO2018184,
	title        = {SummTriver: A new trivergent model to evaluate summaries automatically without human references},
	journal      = {Data & Knowledge Engineering},
	volume       = {113},
	pages        = {184--197},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.09.001},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301581},
	author       = {Luis Adrián Cabrera-Diego and Juan-Manuel Torres-Moreno},
	keywords     = {Automatic text summarization, Summarization evaluation, Jensen-Shanon divergence, Kullback-Leibler divergence, Trivergence of probabilities},
	abstract     = {The automatic evaluation of summaries is a hard task that continues to be open. The assessment aims to measure simultaneously the informativeness and readability of summaries. The scientific community has tackled this problem with partial solutions, in terms of informativeness, using ROUGE. However, to use this method, it is necessary to have multiple summaries made by humans (the references). Methods without human references have been implemented, but there are still far from being highly correlated to manual evaluations. In this paper we present SummTriver, an automatic evaluation method that tries to be more correlated to manual evaluation by using multiple divergences. The results are promising, especially for summarization campaigns. Besides this, we also present an interesting analysis, at micro-level, of how correlated the manual and automatic summaries evaluation methods are, when we make use of a large quantity of observations.}
}
@article{NGUYEN2018129,
	title        = {A heuristics approach to mine behavioural data logs in mobile malware detection system},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {129--151},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.03.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303063},
	author       = {Giang Nguyen and Binh Minh Nguyen and Dang Tran and Ladislav Hluchy},
	keywords     = {Mobile security, Situational awareness, Anomaly detection, Incremental machine learning, Natural language processing, Scalable solution design},
	abstract     = {Nowadays, in the era of Internet of Things when everything is connected via the Internet, the number of mobile devices has risen exponentially up to billions around the world. In line with this increase, the volume of data generated is enormous and has attracted malefactors who do ill deeds to others. For hackers, one of the popular threads to mobile devices is to spread malware. These actions are very difficult to prevent because the application installation and configuration rights are set by owners, who usually have very low knowledge or do not care about the security. In this study, our aim is to improve security in the environment of mobile devices by proposing a novel system to detect malware intrusions automatically. Our solution is based on modelling user behaviours and applying the heuristic analysis approach to mobile logs generated during the device operation process. Although behaviours of individual users have a significant impact on the social cyber-security, to achieve the user awareness has still remained one of the major challenges today. For this task, there is proposed a light-weight semantic formalization in the form of physical and logical taxonomy for classifying the collected raw log data. Then a set of techniques is used, like sliding windows, lemmatization, feature selection, term weighting, and so on, to process data. Meanwhile, malware detection tasks are performed based on incremental machine learning mechanisms, because of the potential complexity of this tasks. The solution is developed in the manner to allow the scalability with several blocks that cover pre-processing raw collected logs from mobile devices, automatically creating datasets for machine learning methods, using the best selected model for detecting suspicious activity surrounding malware intrusions, and supporting decision making using a predictive risk factor. We experimented cautiously with the proposal and achieved test results confirm the effectiveness and feasibility of the proposed system in applying to the large-scale mobile environment.}
}
@article{PHAN201812,
	title        = {Automatically classifying source code using tree-based approaches},
	journal      = {Data & Knowledge Engineering},
	volume       = {114},
	pages        = {12--25},
	year         = {2018},
	note         = {Special Issue on Knowledge and Systems Engineering (KSE 2016)},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.07.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17300344},
	author       = {Anh Viet Phan and Phuong Ngoc Chau and Minh Le Nguyen and Lam Thu Bui},
	keywords     = {Abtract Syntax Tree (AST), Tree-based convolutional neural networks(TBCNN), Support Vector Machines (SVMs), K-Nearest Neighbors (kNN)},
	abstract     = {Analyzing source code to solve software engineering problems such as fault prediction, cost, and effort estimation always receives attention of researchers as well as companies. The traditional approaches are based on machine learning, and software metrics obtained by computing standard measures of software projects. However, these methods have faced many challenges due to limitations of using software metrics which were not enough to capture the complexity of programs. To overcome the limitations, this paper aims to solve software engineering problems by exploring information of programs' abstract syntax trees (ASTs) instead of software metrics. We propose two combination models between a tree-based convolutional neural network (TBCNN) and k-Nearest Neighbors (kNN), support vector machines (SVMs) to exploit both structural and semantic ASTs' information. In addition, to deal with high-dimensional data of ASTs, we present several pruning tree techniques which not only reduce the complexity of data but also enhance the performance of classifiers in terms of computational time and accuracy. We survey many machine learning algorithms on different types of program representations including software metrics, sequences, and tree structures. The approaches are evaluated based on classifying 52000 programs written in C language into 104 target labels. The experiments show that the tree-based classifiers dramatically achieve high performance in comparison with those of metrics-based or sequences-based; and two proposed models TBCNN + SVM and TBCNN + kNN rank as the top and the second classifiers. Pruning redundant AST branches leads to not only a substantial reduction in execution time but also an increase in accuracy.}
}
@article{BOICEA20181,
	title        = {Sampling strategies for extracting information from large data sets},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {1--15},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.01.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16302385},
	author       = {Alexandru Boicea and Ciprian-Octavian Truic? and Florin R?dulescu and Elena-Cristina Bu?e},
	keywords     = {Sampling algorithms, Space complexity, Time complexity, Set operations, Data set cardinality, Time optimization},
	abstract     = {Getting information from large volumes of data is very expensive in terms of resources like CPU and memory, as well as computation time. The analysis of a small data set extracted from the original set is preferred. From this small set, called sample, approximate results can be obtained. The errors are acceptable given the reduced cost necessary for processing the data. Using sampling algorithms with small errors saves execution time and resources. This paper presents comparisons between sampling algorithms in order to determine which one performs better when taking into account set operations such as intersect, union and difference. The comparison focuses on the errors introduced by each algorithm for different sample sizes and on execution times.}
}
@article{PALOMARES201864,
	title        = {Multi-view fuzzy information fusion in collaborative filtering recommender systems: Application to the urban resilience domain},
	journal      = {Data & Knowledge Engineering},
	volume       = {113},
	pages        = {64--80},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.10.002},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301532},
	author       = {Iván Palomares and Fiona Browne and Peadar Davis},
	keywords     = {Collaborative filtering recommender systems, Multi-view similarity information fusion, Fuzzy aggregation, Ordered weighted averaging, Uninorm, Urban resilience},
	abstract     = {Recommender systems play an increasingly important role in on-line web services for the personalization and recommendation of content to individual users. The quantity and quality of user-based information has progressed presenting the opportunity to further tailor recommendations to users based on feature view integration. In this work, we propose a hybrid framework which combines a collaborative filtering recommendation system with fuzzy decision-making approaches (based on the use of aggregation functions) to improve the accuracy of domain-specific recommendations. We extend upon the classical, neighborhood-based collaborative filtering process by conflating preference information with user-profile data in the recommendation process. This is performed using intelligent information fusion techniques whereby Ordered Weighted Averaging (OWA) operators and uninorm aggregation functions are implemented in the fusion of multiple views of pairwise similarity degrees between users. To address the shortcoming of generating sensible recommendations to cold users, we incorporate a novel weighting scheme based on fuzzy set modeling within the uninorm-based aggregation of similarity views. We finally outline the application of the proposed approach through an empirical study based in the Urban Resilience domain, along with an example to movie recommendation.}
}
@article{DUONG20181,
	title        = {Exploring alignment-classification methods in the context of professional writing assistance},
	journal      = {Data & Knowledge Engineering},
	volume       = {114},
	pages        = {1--11},
	year         = {2018},
	note         = {Special Issue on Knowledge and Systems Engineering (KSE 2016)},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.08.005},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1730037X},
	author       = {Mai Duong and Minh-Quoc Nghiem and Ngan Luu-Thuy Nguyen},
	keywords     = {Writing assistance, Alignment classification, Word alignment, Second language learning},
	abstract     = {Proofreading, the act of checking first-draft writings performed by native experts, is essential for professional writing by non-native speakers. Usually, proofreading experts return the corrected texts to the writer without reasons of correction, which makes it difficult for the writer to learn from their errors. The combination of word alignment and classification techniques can help us to analyze the original and corrected texts and use them for language learning. In this study, we explore different alignment-classification methods for this task. Our experimental results show that the best method achieved 71.8% in accuracy. We also propose a new error taxonomy for tagging learner corpora, and present our alignment-classification results on the corpus tagged with this new tagset.}
}
@article{CHARALAMPIDIS2018214,
	title        = {Semantic Web user interfaces ? A model and a review},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {214--227},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.04.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16302087},
	author       = {Charalampos C. Charalampidis and Euclid A. Keramopoulos},
	keywords     = {SPARQL, Data structure, Data relation, Visualization, Exploration, Information retrieval, Agents, Zotero, EasyRDF},
	abstract     = {In the introduction of the Semantic Web vision, the software agents seek information, perform transactions and interact with physical devices. However, the Semantic Web is not yet fully implemented nor the software agents are yet capable for this critical mission. The access of the Semantic Web is still a task mainly intended for the humans. This access is through the user interfaces and is practiced mostly for information seeking tasks. The goal of this work is to create a review for the issues related to the user interfaces, with respect to their application in the access of the Semantic Web. Therefore we build a model and a web application, to abstract the interaction between the humans and the Semantic Web and investigate the features of the user interfaces as far as the information seeking in the Semantic Web is concerned. At first a study of related literature is performed, and in it are identified and analyzed those distinctive characteristics that a user interface needs to support. Then, it is conducted a field research in the World Wide Web, in order to discover and record Semantic Web's user interfaces. Based on the analysis of the reviewed literature, the model is devised, and the model's formalism is applied to the findings of the field research. After that, it is conducted an evaluation study and with the help of a dedicated application, comparative tables are outlined for reviewing user interfaces.}
}
@article{SATTARI2018155,
	title        = {A spreading activation-based label propagation algorithm for overlapping community detection in dynamic social networks},
	journal      = {Data & Knowledge Engineering},
	volume       = {113},
	pages        = {155--170},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.12.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X16303500},
	author       = {Mohammad Sattari and Kamran Zamanifar},
	keywords     = {Spreading activation, Label propagation algorithm, Overlapping community detection, Dynamic social networks},
	abstract     = {Community detection in temporal social networks is an increasingly challenging subject in network analysis. The Label Propagation Algorithm (LPA) is a simple and fast approach for community detection in dynamic networks. However, it tends to generate monster communities which decrease the accuracy of community detection, especially in dynamic social networks. In this paper, we propose a modified LPA, called Spreading Activation Label Propagation Algorithm in order to solve the problem. This method assigns a property, called activation value, to each label, where pairs (label name, activation value) are propagated by spreading activation process and the LPA. Furthermore, this algorithm uses two weighting algorithms, where each of them corresponds to one variation of the proposed method. Here, the variations of the proposed method and other available methods on real and synthetic networks are implemented. Experimental results on both real and synthetic networks show that all variations of the proposed method detect communities more accurately compared to the benchmark methods while they are slower than these methods.}
}
@article{KUSS2018393,
	title        = {A probabilistic evaluation procedure for process model matching techniques},
	journal      = {Data & Knowledge Engineering},
	volume       = {117},
	pages        = {393--406},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.04.008},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1830171X},
	author       = {Elena Kuss and Henrik Leopold and Han {van der Aa} and Heiner Stuckenschmidt and Hajo A. Reijers},
	keywords     = {Probabilistic evaluation, Process model matching, Evaluation techniques},
	abstract     = {Process model matching refers to the automatic identification of corresponding activities between two process models. It represents the basis for many advanced process model analysis techniques such as the identification of similar process parts or process model search. A central problem is how to evaluate the performance of process model matching techniques. Current evaluation methods require a binary gold standard that clearly defines which correspondences are correct. The problem is that often not even humans can agree on a set of correct correspondences. Hence, evaluating the performance of matching techniques based on a binary gold standard does not take the true complexity of the matching problem into account and does not fairly assess the capabilities of a matching technique. In this paper, we propose a novel evaluation procedure for process model matching techniques. In particular, we build on the assessments of multiple annotators to define the notion of a non-binary gold standard. In this way, we avoid the problem of agreeing on a single set of correct correspondences. Based on this non-binary gold standard, we introduce probabilistic versions of precision, recall, and F-measure as well as a distance-based performance measure. We use a dataset from the Process Model Matching Contest 2015 and a total of 16 matching systems to assess and compare the insights that can be obtained by using our evaluation procedure. We find that our probabilistic evaluation procedure allows us to gain more detailed insights into the performance of matching systems than a traditional evaluation based on a binary gold standard.}
}
@article{KIM2019101602,
	title        = {A secure kNN query processing algorithm using homomorphic encryption on outsourced database},
	journal      = {Data & Knowledge Engineering},
	volume       = {123},
	pages        = {101602},
	year         = {2019},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.07.005},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17303476},
	author       = {Hyeong-Il Kim and Hyeong-Jin Kim and Jae-Woo Chang},
	keywords     = {Database outsourcing, Database encryption, Encrypted index structure, Data privacy, kNN query processing},
	abstract     = {With the adoption of cloud computing, database outsourcing has emerged as a new platform. Due to the serious privacy concerns associated with cloud computing, databases must be encrypted before being outsourced to the cloud. Therefore, various k-nearest neighbor (kNN) query processing techniques have been proposed for encrypted databases. However, existing schemes are either insecure or inefficient. In this paper, we propose a new secure kNN query processing algorithm. Our algorithm guarantees the confidentiality of both encrypted data and users? query records. To achieve a high level of query processing efficiency, we also devise an encrypted index search scheme that performs data filtering without revealing data access patterns. A performance analysis shows that the proposed scheme outperforms the existing scheme in terms of query processing costs while preserving data privacy.}
}
@article{CORRADINI2018129,
	title        = {A Guidelines framework for understandable BPMN models},
	journal      = {Data & Knowledge Engineering},
	volume       = {113},
	pages        = {129--154},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.11.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X1630341X},
	author       = {Flavio Corradini and Alessio Ferrari and Fabrizio Fornari and Stefania Gnesi and Andrea Polini and Barbara Re and Giorgio O. Spagnolo},
	keywords     = {Models understandability, Business process modeling, BPMN, Modeling guidelines, Model quality, Tool},
	abstract     = {Business process modeling allows abstracting and reasoning on how work is structured within complex organizations. Business process models represent blueprints that can serve different purposes for a variety of stakeholders. For example, business analysts can use these models to better understand how the organization works; employees playing a role in the process can use them to learn the tasks that they are supposed to perform; software analysts/developers can refer to the models to understand the system-as-is before designing the system-to-be. Given the variety of stakeholders that need to interpret these models, and considering the pivotal function that models play within organizations, understandability becomes a fundamental quality that need to be taken into particular account by modelers. In this paper we provide a set of fifty guidelines that can help modelers to improve the understandability of their models. The work focuses on the Business Process Modelling Notation 2.0 standard published by the Object Management Group, which has acquired a clear predominance among the modeling notations for business processes. Guidelines were derived by means of a thoughtful literature review ? which allowed identifying around one hundred guidelines ? and through successive activities of synthesis and homogenization. In addition, we implemented a freely available open source tool, named BEBoP (understandaBility vErifier for Business Process models), to check the adherence of a model to the guidelines. Finally, guidelines violation has been checked with BEBoP on a dataset of 11,294 models available in a publicly accessible repository. Our tests show that, although the majority of the guidelines are respected by the models, some guidelines, which are recognized as fundamental by the literature, are frequently violated.}
}
@article{OUKSILI2018171,
	title        = {Pattern oriented RDF graphs exploration},
	journal      = {Data & Knowledge Engineering},
	volume       = {113},
	pages        = {171--183},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2017.06.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X17301118},
	author       = {Hanane Ouksili and Zoubida Kedad and Stéphane Lopes and Sylvaine Nugier},
	keywords     = {RDF graph exploration, Theme discovery, Keyword search, Pattern},
	abstract     = {An increasing number of RDF datasets are available on the Web. In order to query these datasets, users must have some information about their content as well as some knowledge of a query language such as SPARQL. Our goal is to facilitate the exploration of these datasets. In this paper, we introduce two complementary approaches designed to explore RDF(S)/OWL data: theme-based exploration and keyword search. These two approaches rely on the definition of patterns to formalize users' requirements during the exploration process. We present PatEx, a system designed to explore RDF(S)/OWL datasets using the two exploration strategies, allowing the user to interactively switch between them. We also present some experiments on real datasets to illustrate the effectiveness of our approach.}
}
@article{SONG201868,
	title        = {The landscape of smart aging: Topics, applications, and agenda},
	journal      = {Data & Knowledge Engineering},
	volume       = {115},
	pages        = {68--79},
	year         = {2018},
	issn         = {0169-023X},
	doi          = {https://doi.org/10.1016/j.datak.2018.02.003},
	url          = {https://www.sciencedirect.com/science/article/pii/S0169023X18300673},
	author       = {Il-Yeol Song and Min Song and Tatsawan Timakum and Su-Ryeon Ryu and Hanju Lee},
	keywords     = {smart aging, Aged, Well-being, Self-care, Information communication technology},
	abstract     = {Smart aging is an emerging research topic that has a profound impact on society and well-being of aging population. To the best of our knowledge, there has been no systematic analysis of grasping what research has been conducted on smart aging. Thus, there is no discussion of major issues and future directions of smart aging. In this paper, we provide an overview of smart aging in three ways: 1) to synthesize the components of smart aging based on the comprehensive literature review, 2) to examine the range of topics extracted from 3760 web pages and 3) to analyze the research activities on smart aging by conducting a content analysis of 4500 web pages of the NIH funded organizations' websites related to smart aging. The results of the comprehensive literature review indicate that the discussions on smart aging in the scientific publications are by and large classified into the following three directions: Technologies, Aging Medical Care, and Behavior and Social. In addition, the major topics from search engine datasets, which echoes more general discussions from various different parties, are related to entertainment program and social media, along with medical science and innovation technologies, whereas the research activities of NIH funded organizations focused on cross-disciplinary research in Behavioral and Social science, and Medical Care.}
}
