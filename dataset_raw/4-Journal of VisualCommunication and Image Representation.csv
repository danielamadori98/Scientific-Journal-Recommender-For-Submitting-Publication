abstract,keywords,author,url,doi,issn,year,pages,volume,journal,title,ENTRYTYPE,ID
"Partial domain adaptation (PDA) is a special domain adaptation task where the label space of the target domain is a subset of the source domain. In this work, we present a novel adversarial PDA method named Confidence Based Class Weight and Embedding Discrepancy Constraint Network (CEN). Specifically, we design a robust weighting scheme that takes sample confidence and class information into account. It can automatically distinguish outlier samples in the source domain and reduce their importance. Besides, we consider the relationship between feature norm and domain shift. We limit the expectation of the feature norms of both domains to an adaptive value. By this means, we can align the feature distributions and help the deep model learn domain-invariant representations. Comprehensive experiments on three domain adaptation datasets Office-31, Office-home, and Visda2017 show that our approach surpasses state-of-the-art methods on various PDA tasks.","Partial domain adaptation, Deep transfer learning, Adversarial alignment, Classification learning",Mingkai Luo and Zhao Yang and Weiwei Ai and Jiehao Liu,https://www.sciencedirect.com/science/article/pii/S104732032200150X,https://doi.org/10.1016/j.jvcir.2022.103630,1047-3203,2022,103630,88,Journal of Visual Communication and Image Representation,Confidence based class weight and embedding discrepancy constraint network for partial domain adaptation,article,LUO2022103630
"Siamese trackers have attracted considerable attention in the field of object tracking because of their high precision and speed. However, one of the main disadvantages of Siamese trackers is that their feature extraction network is relatively single. They often use AlexNet or ResNet50 as the backbone network. AlexNet is shallow and thus cannot easily extract abundant semantic information, whereas ResNet50 has many convolutional layers, reducing the real-time performance of Siamese trackers. We propose a multi-branch feature aggregation network with different designs in the shallow and deep convolutional layers. We use the residual module to build the shallow convolutional layers to extract textural and edge features. The deep convolution layers, designed with two independent branches, are built with residual and parallel modules to extract different semantic features. The proposed network has a depth of only nine modules, and thus it is a simple and effective network. We then apply the network to a Siamese tracker to form SiamMBFAN. We design multi-layer classification and regression subnetworks in the Siamese tracker by aggregating the last three modules of the two branches, improving the localization ability of the tracker. Our tracker achieves a better balance between performance and speed. Finally, SiamMBFAN is tested on four challenging benchmarks, including OTB100, VOT2016, VOT2018, and UAV123. Compared with other trackers, our tracker improves by 7% (OTB100).","Visual tracking, Siamese network, Multi-branch network, Feature aggregation",Hao Zhang and Yan Piao and Bailiang Huang and Baolin Tan,https://www.sciencedirect.com/science/article/pii/S1047320322001912,https://doi.org/10.1016/j.jvcir.2022.103671,1047-3203,2022,103671,89,Journal of Visual Communication and Image Representation,SiamMBFAN: Siamese tracker with multi-branch feature aggregation network,article,ZHANG2022103671
"Human visual theory is closely related to stereo image quality assessment (SIQA), which determines whether the evaluation results of SIQA method can keep good consistency with subjective perception. Many SIQA methods are not fully based on human visual theory, so there is still room for improvement. The research on the visual system tends to the dorsal and ventral pathways, which ignores the information differences in the early visual pathways. It is worth noting that the ON and OFF receptive fields in retinal ganglion cells (RGCs) respond asymmetrically to the statistical features of images. Inspired by this, in this paper, we propose an SIQA method based on monocular and binocular visual features, which takes into account the difference of ON and OFF response features in early visual pathways. Moreover, the different information interaction mechanisms of visual cortex are used to fuse the response maps information of left and right images. Final, monocular and binocular features are extracted and sent to support vector regression (SVR) for quality prediction. Experimental results show that the proposed method is superior to several mainstream SIQA metrics on four publicly available stereo image databases.","Stereo image quality assessment, Retinal ganglion cells, ON and OFF receptive fields, Monocular and binocular features",Yongli Chang and Sumei Li and Jie Jin and Anqi Liu and Wei Xiang,https://www.sciencedirect.com/science/article/pii/S1047320322001638,https://doi.org/10.1016/j.jvcir.2022.103643,1047-3203,2022,103643,89,Journal of Visual Communication and Image Representation,Stereo image quality assessment considering the difference of statistical feature in early visual pathway,article,CHANG2022103643
"Versatile video coding (VVC) is the newest video compression standard. It adopts quadtree with nested multi-type tree (QT-MTT) to encode square or rectangular coding units (CUs). The QT-MTT coding structure is more flexible for encoding video texture, but it is also accompanied by many time-consuming algorithms. So, this work proposes fast algorithms to determine horizontal or vertical split for binary or ternary partition of a 32Â ÃÂ 32 CU in the VVC intra coding to replace the rate-distortion optimization (RDO) process, which is time-consuming. The proposed fast algorithms are actually a two-step algorithm, including feature analysis method and deep learning method. The feature analysis method is based on variances of pixels, and the deep learning method applies the convolution neural networks (CNNs) for classification. Experimental results show that the proposed method can reduce encoding time by 28.94% on average but increase Bjontegaard delta bit rate (BDBR) by about 0.83%.","Versatile video coding, Quadtree with nested multi-type tree, Coding unit, Intra coding, Convolutional neural network",Jiunn-Tsair Fang and Bang-Hao Liu and Pao-Chi Chang,https://www.sciencedirect.com/science/article/pii/S1047320322000827,https://doi.org/10.1016/j.jvcir.2022.103542,1047-3203,2022,103542,87,Journal of Visual Communication and Image Representation,Fast coding unit partitioning algorithms for versatile video coding intra coding,article,FANG2022103542
"Recently, the task of lane detection has been greatly improved with the rapid development of deep learning and autonomous driving. However, there exist limitations like the challenging complex scenarios and real-time efficiency. In this paper, we present a novel Attention Based Dual Path Network (ADPNet) to handle the task of lane detection. The ADPNet treat the process of lane detection as a task of binary semantic segmentation, where the Detail Path is designed to capture detailed low-level information and the Semantic Path with dual attention module is designed to capture contextual high-level information. We use the Feature Aggregation Module to fuse the information of the two paths, followed by the process of lane fitting to get a parametric description of lanes. The proposed ADPNet achieves good trade-off between the accuracy and real-time efficiency on TuSimple and CULane, which are two popular lane detection benchmark datasets. The results demonstrate that our architecture outperforms the current state-of-the-art methods.","Lane detection, Semantic segmentation, Attention mechanism, Lane fitting",Fenglei Ren and Haibo Zhou and Lu Yang and Fulong Liu and Xin He,https://www.sciencedirect.com/science/article/pii/S1047320322001080,https://doi.org/10.1016/j.jvcir.2022.103574,1047-3203,2022,103574,87,Journal of Visual Communication and Image Representation,ADPNet: Attention based dual path network for lane detection,article,REN2022103574
"Attention modules embedded in deep networks mediate the selection of informative regions for object recognition. In addition, the combination of features learned from different branches of a network can enhance the discriminative power of these features. However, fusing features with inconsistent scales is a less-studied problem. In this paper, we first propose a multi-scale channel attention network with an adaptive feature fusion strategy (MSCAN-AFF) for face recognition (FR), which fuses the relevant feature channels and improves the networkâs representational power. In FR, face alignment is performed independently prior to recognition, which requires the efficient localization of facial landmarks, which might be unavailable in uncontrolled scenarios such as low-resolution and occlusion. Therefore, we propose utilizing our MSCAN-AFF to guide the Spatial Transformer Network (MSCAN-STN) to align feature maps learned from an unaligned training set in an end-to-end manner. Experiments on benchmark datasets demonstrate the effectiveness of our proposed MSCAN-AFF and MSCAN-STN.","Attention network, Feature alignment, Multi-scale features, Adaptive feature fusion",M. Saad Shakeel and Yuxuan Zhang and Xin Wang and Wenxiong Kang and Arif Mahmood,https://www.sciencedirect.com/science/article/pii/S1047320322001481,https://doi.org/10.1016/j.jvcir.2022.103628,1047-3203,2022,103628,88,Journal of Visual Communication and Image Representation,Multi-scale attention guided network for end-to-end face alignment and recognition,article,SHAKEEL2022103628
"With the popularity of cloud servers, an increasing number of people are willing to store their images in the cloud due to many conveniences such as online browsing and managing images. On the other hand, this inevitably causes usersâ concerns about image privacy leakage. Many image encryption schemes are proposed to prevent privacy leakage, while most of them focus only on privacy protection and ignore the usability of encrypted images. For this purpose, Marohn etÂ al. (2017) designed two approximate thumbnail-preserving encryption (TPE) schemes to balance image privacy and usability. However, the decrypted image in these two schemes are only perceptually close to the original one and the original image cannot be perfectly recovered. To this end, we design a perfectly recoverable approximate TPE scheme in this paper, which combines reversibledata hiding (RDH) with encryption schemes. The thumbnails of the original and processed images are similar to balance image privacy and usability well. Meanwhile, the reversibility of RDH and encryption schemes is utilized to ensure the perfect recoverability in the proposed scheme. Experiments show that the proposed approximate TPE scheme is no longer limited to balancing usability and privacy but attains perfect recovery.","Difference expansion, Privacy protection, Perfect recoverability, Approximate thumbnail-preserving encryption",Xi Ye and Yushu Zhang and Ruoyu Zhao and Rushi Lan and Yong Xiang,https://www.sciencedirect.com/science/article/pii/S1047320322001122,https://doi.org/10.1016/j.jvcir.2022.103589,1047-3203,2022,103589,87,Journal of Visual Communication and Image Representation,PRA-TPE: Perfectly Recoverable Approximate Thumbnail-Preserving Image Encryption,article,YE2022103589
"Most of the time, when people observe, interact or speak to each other, they focus the attention on the ocular parts of the face. This daily life experience has a strong impact on the analysis of periocular facial regions. These facial regions may be exploited in order to identify individuals for several applications, including access control and services such as telebanking and electronic transactions. In this paper we suggest studying the efficiency of the periocular regions on gender and race prediction. Most researchers propose a local texture description based on LBP (Local Binary Pattern) and HoG (Histogram of Oriented Gradients) for the purpose of predicting gender. On the other hand, Deep learning techniques were proposed to predict the gender. However, this requires a huge labeled periocular data for gender which is not available. Also, the expressivity of gender and race can be decreased on the final representation of the Deep architectures comparing to the earlier stages. To overcome these points and for the aim of predicting gender and race, considering also the high impact of DCNNs (Deep Convolutional Neural Networks) techniques to solve several aspects in biometrics, we suggest a Deep architecture based on visual attention on the periocular part. The visual saliency extraction is based on primary layersâ activation by analyzing the feature-maps. We study how the visual attention-based features coupled to Deep Neural Networks can be used to discriminate between gender and race, hence extract a significant feature from periocular regions. Different pretrained architectures such as Alexnet and ResNet-50 were considered to extract visual saliency points or interest points. Several experiments were performed on periocular regions and a comparative study was conducted. The present results not only demonstrate the feasibility but also the robustness of the extracted interest points.","Visual-attention, Periocular regions, Gender prediction, Deep learning, Local description",Souad Khellat-Kihel and Jawad Muhammad and Zhenan Sun and Massimo Tistarelli,https://www.sciencedirect.com/science/article/pii/S104732032200147X,https://doi.org/10.1016/j.jvcir.2022.103627,1047-3203,2022,103627,88,Journal of Visual Communication and Image Representation,Gender and ethnicity recognition based on visual attention-driven deep architectures,article,KHELLATKIHEL2022103627
"Cross-modal retrieval attracts much research attention due to its wide applications in numerous search systems. Sketch based 3D shape retrieval is a typical challenging cross-modal retrieval task for the huge divergence between sketch modality and 3D shape view modality. Existing approaches project the sketches and shapes into a common space for feature update and data alignment. However, these methods contain several disadvantages: Firstly, the majority approaches ignore the modality-shared information for divergence compensation in descriptor generation process. Secondly, traditional fusion method of multi-view features introduces much redundancy, which decreases the discrimination of shape descriptors. Finally, most approaches only focus on the cross-modal alignment, which omits the modality-specific data relevance. To address these limitations, we propose a Joint Feature Learning Network (JFLN). Firstly, we design a novel modality-shared feature extraction network to exploit both modality-specific characteristics and modality-shared information for descriptor generation. Subsequently, we introduce a hierarchical view attention module to gradually focus on the effective information for multiview feature updating and aggregation. Finally, we propose a novel cross-modal feature learning network, which can simultaneously contribute to modality-specific data distribution and cross-modal data alignment. We conduct exhaustive experiments on three public databases. The experimental results validate the superiority of the proposed method. Full Codes are available at https://github.com/dlmuyy/JFLN.","Sketch-based 3D shape retrieval, Attention mechanism, Cross-modal retrieval",Yue Zhao and Qi Liang and Ruixin Ma and Weizhi Nie and Yuting Su,https://www.sciencedirect.com/science/article/pii/S1047320322001882,https://doi.org/10.1016/j.jvcir.2022.103668,1047-3203,2022,103668,89,Journal of Visual Communication and Image Representation,JFLN: Joint Feature Learning Network for 2D sketch based 3D shape retrieval,article,ZHAO2022103668
"Raw point cloud processing using capsule networks is widely adopted in classification, reconstruction, and segmentation due to its ability to preserve spatial agreement of the input data. However, most of the existing capsule based network approaches are computationally heavy and fail at representing the entire point cloud as a single capsule. We address these limitations in existing capsule network based approaches by proposing PointCaps, a novel convolutional capsule architecture with parameter sharing. Along with PointCaps, we propose a novel Euclidean distance routing algorithm and a class-independent latent representation. The latent representation captures physically interpretable geometric parameters of the point cloud, with dynamic Euclidean routing, PointCaps well-represents the spatial (point-to-part) relationships of points. PointCaps has a significantly lower number of parameters and requires a significantly lower number of FLOPs while achieving better reconstruction with comparable classification and segmentation accuracy for raw point clouds compared to state-of-the-art capsule networks.","Point cloud reconstruction, Classification, Capsule networks, Error routing",Dishanika Denipitiyage and Vinoj Jayasundara and Ranga Rodrigo and Chamira U.S. Edussooriya,https://www.sciencedirect.com/science/article/pii/S1047320322001365,https://doi.org/10.1016/j.jvcir.2022.103612,1047-3203,2022,103612,88,Journal of Visual Communication and Image Representation,PointCaps: Raw point cloud processing using capsule networks with Euclidean distance routing,article,DENIPITIYAGE2022103612
"The key to fine-grained image classification is to find discriminative regions. Most existing methods only use simple baseline networks or low-recognition attention modules to discover object differences, which will limit the model to finding discriminative regions hidden in images. This article proposes an effective method to solve this problem. The first is a novel layered training method, which uses a new training method to enhance the feature extraction ability of the baseline model. The second step focuses on key regions of the image based on improved long short-term memory (LSTM) and multi-head attention. In the third step, based on the feature map obtained by the dual attention network, spatial mapping is performed by a multi-layer perceptron (MLP). Then the element-by-element mutual multiplication calculation of the channel is performed to obtain a feature map with finer granularity. Finally, the CUB-200-2011, FGVC Aircraft, Stanford Cars, and MedMNIST v2 datasets achieved good performance.","Data augmentation, Hierarchical training, Denoising autoencoder, Dual attention mechanism, Interactive attention",Qiangxi Zhu and Wenlan Kuang and Zhixin Li,https://www.sciencedirect.com/science/article/pii/S1047320322001523,https://doi.org/10.1016/j.jvcir.2022.103632,1047-3203,2022,103632,88,Journal of Visual Communication and Image Representation,Dual attention interactive fine-grained classification network based on data augmentation,article,ZHU2022103632
"Most deep learning (DL)-based image restoration methods have exploited excellent performance by learning a non-linear mapping function from low quality images to high quality images. However, two major problems restrict the development of the image restoration methods. First, most existing methods based on fixed degradation suffer from significant performance drop when facing the unknown degradation, because of the huge gap between the fixed degradation and the unknown degradation. Second, the unknown-degradation estimation may lead to restoration task failure due to uncertain estimation errors. To handle the unknown degradation in the real application, we introduce a degradation representation network for single image blind restoration (DRN). Different from the methods of estimating pixel space, we use an encoder network to learn abstract representations for estimating different degradation kernels in the representation space. Furthermore, a degradation perception module with flexible adaptability to different degradation kernels is used to restore more structural details. In our experiments, we compare our DRN with several state-of-the-art methods for two image restoration tasks, including image super-resolution (SR) and image denoising. Quantitative results show that our degradation representation network is accurate and efficient for single image restoration.","Deep learning, Contrastive learning, Image restoration, Convolution neural network",Yan Jin and Zhiwei Jiang and Zhizhong Xue and Yibiao Hu,https://www.sciencedirect.com/science/article/pii/S1047320322001006,https://doi.org/10.1016/j.jvcir.2022.103564,1047-3203,2022,103564,87,Journal of Visual Communication and Image Representation,Image blind restoration based on degradation representation network,article,JIN2022103564
"Underwater images are usually degraded due to light scattering and absorption. To recover the scene radiance of degraded underwater images, a new haze removal method is presented by incorporating a learning-based approach to blurriness estimation with the image formation model. Firstly, the image blurriness is estimated with a linear model trained on a set of selected grayscale images, the average Gaussian images and blurriness images. With the estimated image blurriness, three intermediate background lights (BLs) are computed to obtain the synthesized BL. Then the scene depth is calculated by using the estimated image blurriness and BL to construct a transmission map and restore the scene radiance. Compared with other haze removal methods, haze in degraded underwater images can be removed more accurately with our proposed method. Moreover, visual inspection, quantitative evaluation and application test demonstrate that our method is superior to the compared methods and beneficial to high-level vision tasks.","Underwater image, Image dehazing, Image restoration, Image enhancement",Jian Chen and Hao-Tian Wu and Lu Lu and Xiangyang Luo and Jiankun Hu,https://www.sciencedirect.com/science/article/pii/S1047320322001766,https://doi.org/10.1016/j.jvcir.2022.103656,1047-3203,2022,103656,89,Journal of Visual Communication and Image Representation,Single underwater image haze removal with a learning-based approach to blurriness estimation,article,CHEN2022103656
"Mixed reality can overlay and display 3D digital content in the real world, convey abstract concepts to users, and promote the understanding of complex tasks. However, the abstract graphics overlaid on the physical space may cause a certain cognitive load for local users and reduce the efficiency of collaboration. To improve the efficiency of remote collaboration, we conducted an elicitation study on assembly tasks, explored the user needs for collaboration, and defined the design goals of our remote collaboration method. Inspired by the mirror-neuron mechanism, we present an imitative collaboration method that allows local users to imitate the interaction behavior of remote users to complete tasks. We also propose a series of interaction methods for remote users to select, copy, and interact with the local point clouds to facilitate the expression of collaboration intentions. Finally, the results of a user study evaluating our imitative collaboration method on assembly tasks are reported, confirming that our method improves collaboration efficiency while reducing the cognitive load of local users.","Mixed reality, Remote collaboration, Mirror-neuron, Gestural interaction, Point cloud interaction",Zhenning Zhang and Zhigeng Pan and Weiqing Li and Zhiyong Su,https://www.sciencedirect.com/science/article/pii/S1047320322001249,https://doi.org/10.1016/j.jvcir.2022.103600,1047-3203,2022,103600,88,Journal of Visual Communication and Image Representation,Imitative Collaboration: A mirror-neuron inspired mixed reality collaboration method with remote hands and local replicas,article,ZHANG2022103600
"To increase the richness of the extracted text modality feature information and deeply explore the semantic similarity between the modalities. In this paper, we propose a novel method, named adaptive weight multi-channel center similar deep hashing (AMCDH). The algorithm first utilizes three channels with different configurations to extract feature information from the text modality; and then adds them according to the learned weight ratio to increase the richness of the information. We also introduce the Jaccard coefficient to measure the semantic similarity level between modalities from 0 to 1, and utilize it as the penalty coefficient of the cross-entropy loss function to increase its role in backpropagation. Besides, we propose a method of constructing center similarity, which makes the hash codes of similar data pairs close to the same center point, and dissimilar data pairs are scattered at different center points to generate high-quality hash codes. Extensive experimental evaluations on four benchmark datasets show that the performance of our proposed model AMCDH is significantly better than other competing baselines. The code can be obtained from https://github.com/DaveLiu6/AMCDH.git.","Multi-channel, Center similar, Multimodal retrieval, Deep cross-modal hashing",Xinghua Liu and Guitao Cao and Qiubin Lin and Wenming Cao,https://www.sciencedirect.com/science/article/pii/S1047320322001626,https://doi.org/10.1016/j.jvcir.2022.103642,1047-3203,2022,103642,89,Journal of Visual Communication and Image Representation,Adaptive weight multi-channel center similar deep hashing,article,LIU2022103642
"Many previous occluded person re-identification(re-ID) methods try to use additional clues (pose estimation or semantic parsing models) to focus on non-occluded regions. However, these methods extremely rely on the performance of additional clues and often capture pedestrian features by designing complex modules. In this work, we propose a simple Fine-Grained Multi-Feature Fusion Network (FGMFN) to extract discriminative features, which is a dual-branch structure consisting of global feature branch and partial feature branch. Firstly, we utilize a chunking strategy to extract multi-granularity features to make the pedestrian information contained in it more comprehensive. Secondly, a spatial transformer network is introduced to localize the pedestrianâs upper body, and then introduce a relation-aware attention module to explore the fine-grained information. Finally, we fuse the features obtained from the two branches to obtain a more robust pedestrian representation. Extensive experiments verify the effectiveness of our method under the occlusion scenario.","Occluded person re-identification, Multi-granularity feature, Feature fusion",Guoqing Zhang and Chao Chen and Yuhao Chen and Hongwei Zhang and Yuhui Zheng,https://www.sciencedirect.com/science/article/pii/S1047320322001109,https://doi.org/10.1016/j.jvcir.2022.103581,1047-3203,2022,103581,87,Journal of Visual Communication and Image Representation,Fine-grained-based multi-feature fusion for occluded person re-identification,article,ZHANG2022103581
"Significant challenges still remain despite the impressive recent advances in machine learning techniques, particularly in multimedia data understanding. One of the main challenges in real-world scenarios is the nature and relation between training and test datasets. Very often, only small sets of coarse-grained labeled data are available to train models, which are expected to be applied on large datasets and fine-grained tasks. Weakly supervised learning approaches handle such constraints by maximizing useful training information in labeled and unlabeled data. In this research direction, we propose a weakly supervised approach that analyzes the dataset manifold to expand the available labeled set. A hypergraph manifold ranking algorithm is exploited to represent the contextual similarity information encoded in the unlabeled data and identify strong similarity relations, which are taken as a path to label expansion. The expanded labeled set is subsequently exploited for a more comprehensive and accurate training process. The proposed model was evaluated jointly with supervised and semi-supervised classifiers, including Graph Convolutional Networks. The experimental results on image and video datasets demonstrate significant gains and accurate results for different classifiers in diverse scenarios.","Weakly supervised learning, Manifold learning, Ranking, Hypergraph",JoÃ£o Gabriel Camacho Presotto and Samuel Felipe {dos Santos} and Lucas Pascotti Valem and Fabio Augusto Faria and JoÃ£o Paulo Papa and Jurandy Almeida and Daniel Carlos GuimarÃ£es Pedronette,https://www.sciencedirect.com/science/article/pii/S1047320322001869,https://doi.org/10.1016/j.jvcir.2022.103666,1047-3203,2022,103666,89,Journal of Visual Communication and Image Representation,Weakly supervised learning based on hypergraph manifold ranking,article,PRESOTTO2022103666
"Crypto-space reversible image steganography has attracted increasing attention, given its ability to embed authentication information without revealing the image content. This paper presents an efficient reversible data hiding scheme for crypto-images: a block predictor is applied to compute prediction errors, then an adaptive block mapping algorithm is utilized to compress them whose amplitudes are within a small threshold, finally, this strategy can be applied in a multi-level manner to achieve a higher embedding capacity. Due to the correlations among adjacent pixels in the block, images can be sufficiently compressed to reserve abundant space for additional data embedding. Different from the prior arts, the compression code of the image is fully encrypted. Experimental results verify that the embedded data and original image can be perfectly recovered, the security is higher compared with the state-of-the-arts, and a significant improvement in the average embedding rate is achieved on two large-scale image datasets.","Reversible data hiding, Encrypted images, Multi-Level, Block mapping, Very high embedding capacity",Xu Wang and Ching-Chun Chang and Chia-Chen Lin and Chin-Chen Chang,https://www.sciencedirect.com/science/article/pii/S104732032200092X,https://doi.org/10.1016/j.jvcir.2022.103556,1047-3203,2022,103556,87,Journal of Visual Communication and Image Representation,On the multi-level embedding of crypto-image reversible data hiding,article,WANG2022103556
"With the development of multimedia technology, fine-grained image retrieval has gradually become a new hot topic in computer vision, while its accuracy and speed are limited due to the low discriminative high-dimensional real-valued embedding. To solve this problem, we propose an end-to-end framework named DFMH (Discriminative Feature Mining Hashing), which consists of the DFEM (Discriminative Feature Extracting Module) and SHCM (Semantic Hash Coding Module). Specifically, DFEM explores more discriminative local regions by attention drop and obtains finer local feature expression by attention re-sample. SHCM generates high-quality hash codes by combining the quantization loss and bit balance loss. Validated by extensive experiments and ablation studies, our method consistently outperforms both the state-of-the-art generic retrieval methods as well as fine-grained retrieval methods on three datasets, including CUB Birds, Stanford Dogs and Stanford Cars.","Fine-grained image retrieval, Attention drop, Attention re-sample, Deep hashing",Wenxi Lang and Han Sun and Can Xu and Ningzhong Liu and Huiyu Zhou,https://www.sciencedirect.com/science/article/pii/S1047320322001195,https://doi.org/10.1016/j.jvcir.2022.103592,1047-3203,2022,103592,87,Journal of Visual Communication and Image Representation,Discriminative feature mining hashing for fine-grained image retrieval,article,LANG2022103592
"Images with visual pleasing bokeh effect are often unattainable for mobile cameras with compact optics and tiny sensors. To balance the aesthetic requirements on photo quality and expensive high-end SLR cameras, synthetic bokeh effect rendering has emerged as an attractive machine learning topic for engineering applications on imaging systems. However, most of bokeh rendering models either heavily relied on prior knowledge such as scene depth or were topic-irrelevant data-driven networks without task-specific knowledge, which restricted modelsâ training efficiency and testing accuracy. Since bokeh is closely related to a phenomenon called âcircle of confusionâ, therefore, in this paper, following the principle of bokeh generation, a novel self-supervised multi-scale pyramid fusion network has been proposed for bokeh rendering. During the pyramid fusion process, structure consistencies are employed to emphasize the importance of respective bokeh components. Task-specific knowledge which mimics the âcircle of confusionâ phenomenon through disk blur convolutions is utilized as self-supervised information for network training. The proposed network has been evaluated and compared with several state-of-the-art methods on a public large-scale bokeh dataset- the âEBB!â Dataset. The experiment performance demonstrates that the proposed network has much better processing efficiency and can achieve better realistic bokeh effect with much less parameters size and running time. Related source codes and pre-trained models of the proposed model will be available soon on https://github.com/zfw-cv/MPFNet.","Bokeh rendering, Circle of confusion, Self-supervised, Multi-scale fusion, Structure consistency",Zhifeng Wang and Aiwen Jiang and Chunjie Zhang and Hanxi Li and Bo Liu,https://www.sciencedirect.com/science/article/pii/S1047320322001110,https://doi.org/10.1016/j.jvcir.2022.103580,1047-3203,2022,103580,87,Journal of Visual Communication and Image Representation,Self-supervised multi-scale pyramid fusion networks for realistic bokeh effect rendering,article,WANG2022103580
"LiDAR-based 3D object detection is important for autonomous driving scene perception, but point clouds produced by LiDAR are irregular and unstructured in nature, and cannot be adopted by the conventional Convolutional Neural Networks (CNN). Recently, Graph Convolutional Networks (GCN) has been proved as an ideal way to handle non-Euclidean structure data, as well as for point cloud processing. However, GCN involves massive computation for searching adjacent nodes, and the heavy computational cost limits its applications in processing large-scale LiDAR point cloud in autonomous driving. In this work, we adopt a frustum-based point cloud-image fusion scheme to reduce the amount of LiDAR point clouds, thus making the GCN-based large-scale LiDAR point clouds feature learning feasible. On this basis, we propose an efficient graph attentional network to accomplish the goal of 3D object detection in autonomous driving, which can learn features from raw LiDAR point cloud directly without any conversions. We evaluate the model on the public KITTI benchmark dataset, the 3D detection mAP is 63.72% on KITTI Cars, Pedestrian and Cyclists, and the inference speed achieves 7.9 fps on a single GPU, which is faster than other methods of the same type.","3D object detection, Multi-sensors fusion, Graph convolutional networks, Attention mechanism, Autonomous driving",Zhenming Liang and Yingping Huang and Zhenwei Liu,https://www.sciencedirect.com/science/article/pii/S1047320322001870,https://doi.org/10.1016/j.jvcir.2022.103667,1047-3203,2022,103667,89,Journal of Visual Communication and Image Representation,Efficient graph attentional network for 3D object detection from Frustum-based LiDAR point clouds,article,LIANG2022103667
"The expected patch log-likelihood (EPLL) model is a patch prior-based image restoration method which received extensive attention in image processing in recent years for its outstanding ability to preserve the detail and structure. However, due to using the Gaussian mixture model (GMM) with the noise sensitivity as the local prior, the EPLL model suffers from undesired artifact and poor robustness frequently. In this paper, to restrain the generation of artifact of EPLL model, we replace the GMM with a bounded asymmetrical Studentâs-t mixture model (BASMM), which is sufficiently flexible to fit different shapes of image data, such as non-Gaussian, non-symmetric, and bounded support data. Then, the anisotropic nonlocal self-similarity (ANSS) based regularization parameters are designed to improve the robustness of the proposed model. Experimental results demonstrate the competitiveness of our proposed model compared with that of state-of-the-art methods in performance both visually and quantitatively.","EPLL image restoration, Finite mixture model, Bounded asymmetrical Studentâs-t mixture model, Anisotropic nonlocal self-similarity, Regularization parameters",Qiqiong Yu and Guo Cao and Hao Shi and Youqiang Zhang and Peng Fu,https://www.sciencedirect.com/science/article/pii/S1047320322001353,https://doi.org/10.1016/j.jvcir.2022.103611,1047-3203,2022,103611,88,Journal of Visual Communication and Image Representation,EPLL image restoration with a bounded asymmetrical Studentâs-t mixture model,article,YU2022103611
"Block-based progressive visual cryptography scheme (BPVCS) divides a secret image into non-overlapping blocks and encodes each block as sub-shadows. The final shadows for BPVCS are created by combining the associated sub-shadows. When enough shadows are superimposed, some of the secret blocks will be exposed. More information will be revealed as more shadows are used. This is referred to as progressive recovery. Hou et al. introduced a (2,n)-BPVCS. Yang et al. further extended the (2,n) scheme to a general (k,n) scheme. However, Yang et al. (k,n)-BPVCS suffers from the non-uniform progressive recovery and inconsistent background of recovered secret blocks. In this paper, we introduce a (k,n)-BPVCS to address the mentioned two defects. Theoretical analysis and experimental results are provided to illustrate the benefits of the proposed approach.","Secret sharing, Visual cryptography, Progressive recovery, Consistent background, Image block",Xiaotian Wu and Zhonglin Luo,https://www.sciencedirect.com/science/article/pii/S1047320322001511,https://doi.org/10.1016/j.jvcir.2022.103631,1047-3203,2022,103631,88,Journal of Visual Communication and Image Representation,Block-based progressive visual cryptography scheme with uniform progressive recovery and consistent background,article,WU2022103631
"3D point cloud has tremendous potential in many application tasks. However, the huge amount of data limits this potential. To simplify point clouds and improve their downstream application efficiency, this paper proposes AS-Net, an attention-aware downsampling network oriented to classification tasks. AS-Net realizes downsampling through an Attention-aware Sampling Module, which including an Input Embedding Module and an Attention Module. The former is designed to extract the global and local features of the point cloud, the latter is to generate the Sampling-Map to simulate the differentiable downsampling. Thanks to the attention mechanism, AS-Net may select the critical points of the original point cloud for classification tasks. In addition, AS-Net designs a Constraint Matching Module to match the sampled points to be a subset of the original point cloud at the inference phase. For end-to-end training, AS-Net construct a joint loss function that includes a task loss, a sampling loss, and a constraint loss. Extensive experiments on the ModelNet10/40 and ShapeNet datasets demonstrate that AS-Net achieves a good performance on the point cloud classification task. Especially when the downsampling size is small, the result is better than the referenced methods.","Point cloud, Downsampling, Attention, Constraint match, Classification task",Yakun Yang and Anhong Wang and Donghan Bu and Zewen Feng and Jie Liang,https://www.sciencedirect.com/science/article/pii/S1047320322001596,https://doi.org/10.1016/j.jvcir.2022.103639,1047-3203,2022,103639,89,Journal of Visual Communication and Image Representation,AS-Net: An attention-aware downsampling network for point clouds oriented to classification tasks,article,YANG2022103639
"Deep image compression efficiency has been improved in the past years. However, to fully exploit context information for compressing image objects of different scales and shapes, more adaptive geometric structure of inputs should be considered. In this paper, we novelly introduce deformable convolution and its spatial attention extension into deep image compression task to fully exploit the context information. Specifically, a novel deep image compression network with Multi-Scale Deformable Convolution and Spatial Attention, named MS-DCSA, is proposed to better extract compact and efficient latent representation as well as reconstruct higher-quality images. First, multi-scale deformable convolution is presented to provide multi-scale receptive fields for learning spatial sampling offsets in deformable operations. Subsequently, multi-scale deformable spatial attention module is developed to generate attention masks to re-weight extracted features according to their importance. In addition, the multi-scale deformable convolution is applied to design delicate up/down sampling modules. Extensive experiments demonstrate that the proposed MS-DCSA network achieves improved performance on both PSNR and MS-SSIM quality metrics, compared to conventional as well as competing deep image compression methods.","Deep image compression, Multi-scale deformable convolution, Spatial attention",Daowen Li and Yingming Li and Heming Sun and Lu Yu,https://www.sciencedirect.com/science/article/pii/S1047320322001079,https://doi.org/10.1016/j.jvcir.2022.103573,1047-3203,2022,103573,87,Journal of Visual Communication and Image Representation,Deep image compression based on multi-scale deformable convolution,article,LI2022103573
"Infrared dim and small target detection is a key technology for space-based infrared search and tracking systems. Traditional detection methods have a high false alarm rate and fail to handle complex background and high-noise scenarios. Also, the methods cannot effectively detect targets on a small scale. In this paper, a U-Transformer method is proposed, and a transformer is introduced into the infrared dim and small target detection. First, a U-shaped network is constructed. In the encoder part, the self-attention mechanism is used for infrared dim and small target feature extraction, which helps to solve the problems of losing dim and small target features of deep networks. Meanwhile, by using the encoding and decoding structure, infrared dim and small target features are filtered from the complex background while the shallow features and semantic information of the target are retained. Experiments show that anchor-free and transformer have great potential for infrared dim and small target detection. On the datasets with a complex background, our method outperforms the state-of-the-art detectors and meets the real-time requirement. The code is publicly available at https://github.com/Linaom1214/U-Transformer.","Infrared small and dim target detection, Swin transformer, Anchor free, Object detection, Heatmap",Jian Lin and Kai Zhang and Xi Yang and Xiangzheng Cheng and Chenhui Li,https://www.sciencedirect.com/science/article/pii/S1047320322002048,https://doi.org/10.1016/j.jvcir.2022.103684,1047-3203,2022,103684,89,Journal of Visual Communication and Image Representation,Infrared dim and small target detection based on U-Transformer,article,LIN2022103684
"Workspace awareness is critical for remote assistance with physical tasks, yet it remains difficult to facilitate. For example, if the remote helper is limited to the single viewpoint provided by the workerâs hand-held or head-mounted camera, she lacks the ability to gain an overview of the workspace. This may be addressed by granting the helper view-independence, e.g., through a multi-camera system. However, it can be cumbersome to set up and calibrate multiple cameras, and it can be challenging for the local worker to identify the current viewpoint of the remote helper. We present CueCam, a multi-camera remote assistance system that supports mutual workspace awareness through a flexible ad-hoc camera calibration and various Augmented Reality cues that communicate the helperâs viewpoint and focus. In particular, we propose visual cues presented through a head-mounted Augmented Reality display (Virtual Hand, Color Cue), and sound cues emitted from the camerasâ physical locations (Spatial Sound). Findings from a lab study indicate that all proposed cues effectively support the workerâs awareness of helperâs location and focus, while the Color Cue demonstrated superiority in task performance and preference ratings during a search task.","Remote assistance, Augmented reality, Workspace awareness, Awareness cues, Calibration",Troels Rasmussen and Tiare Feuchtner and Weidong Huang and Kaj GrÃ¸nbÃ¦k,https://www.sciencedirect.com/science/article/pii/S1047320322001754,https://doi.org/10.1016/j.jvcir.2022.103655,1047-3203,2022,103655,89,Journal of Visual Communication and Image Representation,Supporting workspace awareness in remote assistance through a flexible multi-camera system and Augmented Reality awareness cues,article,RASMUSSEN2022103655
"This paper presents a novel attention-based adversarial autoencoder network (A3N) that consists of a two-stream decoder to detect abnormal events in video sequences. The first stream of the decoder is a reconstructive model responsible for recreating the input frame sequence. However, the second stream is a future predictive model used to predict the future frame sequence through adversarial learning. A global attention mechanism is employed at the decoder side that helps to decode the encoded sequences effectively. The training of A3N is carried out on normal video data. The attention-based reconstructive model is used during the inference stage to compute the anomaly score. A3N delivers a considerable average speed of 0.0227Â s (â¼44 fps ) for detecting anomalies in the testing phase on used datasets. Several experiments and ablation analyses have been performed on UCSD Pedestrian, CUHK Avenue and ShanghaiTech datasets to validate the efficiency of the proposed model.","Anomaly detection, Attention mechanism, Adversarial autoencoder, Generative adversarial network",Nazia Aslam and Prateek Kumar Rai and Maheshkumar H. Kolekar,https://www.sciencedirect.com/science/article/pii/S1047320322001237,https://doi.org/10.1016/j.jvcir.2022.103598,1047-3203,2022,103598,87,Journal of Visual Communication and Image Representation,A3N: Attention-based adversarial autoencoder network for detecting anomalies in video sequence,article,ASLAM2022103598
"Most previous methods for emotion recognition focus on facial emotion and ignore the rich context information that implies important emotion states. To make full use of the contextual information to make up for the facial information, we propose the Context-Dependent Net (CD-Net) for robust context-aware human emotion recognition. Inspired by the long-range dependency of the transformer, we introduce the tubal transformer which forms the shared feature representation space to facilitate the interactions among the face, body, and context features. Besides, we introduce the hierarchical feature fusion to recombine the enhanced multi-scale face, body, and context features for emotion classification. Experimentally, we verify the effectiveness of the proposed CD-Net on the two large emotion datasets, CAER-S and EMOTIC. On the one hand, the quantitative evaluation results demonstrate the superiority of the proposed CD-Net over other state-of-the-art methods. On the other hand, the visualization results show CD-Net can capture the dependencies among the face, body, and context components and focus on the important features related to the emotion.","Context-based Emotion Recognition, Tubal Transformer, Hierarchical Fusion, Affective Computing",Zili Wang and Lingjie Lao and Xiaoya Zhang and Yong Li and Tong Zhang and Zhen Cui,https://www.sciencedirect.com/science/article/pii/S1047320322001997,https://doi.org/10.1016/j.jvcir.2022.103679,1047-3203,2022,103679,89,Journal of Visual Communication and Image Representation,Context-dependent emotion recognition,article,WANG2022103679
"The tone mapping operator (TMO) enables high dynamic range (HDR) images to be presented on low dynamic range (LDR) consumer electronic devices. However, the results obtained by this method are not always ideal due to the reduced number of bits. In comparison, the multi-exposure image fusion (MEF) bypasses the intermediate HDR image composition and directly produces an image presented on standard devices. Inspired by this, this paper proposes a quality assessment method for tone-mapped image (TMI) based on generating multi-exposure sequences. Specifically, the method uses a generative adversarial network (GAN) to generate a set of sequences with different exposure levels based on the TMIs. Then a two-branch convolutional neural network (CNN) is used to extract features from the tone-mapped images and the multi-exposure reference sequences, respectively. Finally, the transformer is used to mine the intrinsic connections between TMIs and multi-exposure sequences and learn the mapping relationships from feature space to quality space. We conducted extensive experiments on the ESPL-LIVE HDR database. The applicability and effectiveness of the proposed method are verified by comparing and analyzing relevant features and model configurations with existing mainstream evaluation algorithms.","High dynamic range, Tone-mapped image, Image quality assessment (IQA)",Jiachen Yang and Yanshuang Zhou and Yang Zhao and Jiabao Wen,https://www.sciencedirect.com/science/article/pii/S104732032200089X,https://doi.org/10.1016/j.jvcir.2022.103553,1047-3203,2022,103553,87,Journal of Visual Communication and Image Representation,Blind quality assessment of tone-mapped images using multi-exposure sequences,article,YANG2022103553
"Localization is an essential part of object detection, which is usually accomplished by bounding box regression guided by ân-norm-based or IoU-based loss functions, where IoU is known for its scale-invariant characteristics. However, introducing the scale-invariance into regression loss in traditional IoU-based methods may result in a bias in favor of smaller boxes and cause redundancy and unstable oscillations. To make up for these shortages of IoU-based losses, we propose a Scale-Balanced Factor (SF) that stabilizes the regression process via a simple adaptive factor. Furthermore, to compensate for the imbalance of different types of losses caused by SF and other IoU-based loss functions, regression losses are always multiplied by a hyperparameter, which is purely empirical and is hard to find an optimum. To address this issue, a Multi-Task Reinforced Equilibrium (MRE) is proposed to dynamically tweak the learning rate of each task based on reinforcement learning. The MRE can guarantee more balanced parameters and maximize the benefit of SF or other improvement methods for IoU. By incorporating the proposed SF and MRE into the classic detectors (RetinaNet, YOLO, and Faster R-CNN, etc.), we have achieved significant performance gains on MS COCO (0.8 APâ¼1.9 AP) and PASCAL VOC (0.6 APâ¼2.2 AP).","Object detection, Reinforcement learning, Bounding box regression",Chenzhong Wang and Xun Gong,https://www.sciencedirect.com/science/article/pii/S1047320322001857,https://doi.org/10.1016/j.jvcir.2022.103665,1047-3203,2022,103665,89,Journal of Visual Communication and Image Representation,Bounding box regression with balance for harmonious object detection,article,WANG2022103665
"Crowd counting with density estimation has been an active research community due to its significant applications in the fields of public security, video surveillance, traffic monitoring. However, Crowd counting for congested scenes often suffers from some obstacles including severe occlusions, large scale variations, noise interference, etc. In this paper, using the first ten layers of a modified VGG16 and dilated convolution layers as the framework, we have proposed a CNN based crowd counting and density estimation model improved by the attention aware modules with residual connections. To tackle the problem of noise interference, convolutional block attention modules have been introduced into the deep network to segment the foreground and background to focus on interest information, refining deeper features of the input image. To improve information transmission and reuse, residual connections are utilized to link 3 attention blocks. Meanwhile, dilated convolution layers keep larger reception fields and obtain high-resolution density maps. The proposed method has been evaluated on three public benchmarks, i.e. Shanghai Tech A & B, UCF-QNRF and MALL, achieving the mean absolute errors of 64.6 & 8.3, 113.8 and 1.68, respectively. The results outperform some existing excellent approaches. This indicates that the proposed model has high accuracy and better robustness, which is suitable for crowd counting and density estimation in various congested scenes.","Crowd counting, Density estimation, Attentive maps",Li Zhaoxin and Lu Shuhua and Lan Lingqiang and Liu Qiyuan,https://www.sciencedirect.com/science/article/pii/S1047320322001183,https://doi.org/10.1016/j.jvcir.2022.103591,1047-3203,2022,103591,87,Journal of Visual Communication and Image Representation,Crowd counting in complex scenes based on an attention aware CNN network,article,ZHAOXIN2022103591
"Superpixel and saliency-based evaluation methods play important roles in full reference image quality assessment (FR IQA). However, we find that these methods have one complementary principle and three limitations: (1) the weighted maps of superpixel-based methods conflict with the perception of the human visual system; (2) saliency-based methods are inefficient in terms of the block distortion; (3) the general two-direction gradient extraction factor must be extended to be multidirectional. To address these limitations, we propose an enhanced image quality assessment by synergizing superpixels and visual saliency. Specifically, the calculation of a newly proposed framework involves three similarities and two strategies: the saliency, superpixel and multidirectional gradient similarities of the neighborhoods, and the saliency pooling strategy, the fusion strategy of these similarities. Theoretical analysis and experimental results show that the proposed method can effectively address the limitations noted above and outperform the existing methods.","Full reference, Image quality assessment, Visual saliency, Superpixel segmentation, Limitations, Complementary",Jiehang Deng and Haomin Chen and Zhongming Yuan and Guosheng Gu and Shihe Xu and Shaowei Weng and Hao Wang,https://www.sciencedirect.com/science/article/pii/S1047320322001341,https://doi.org/10.1016/j.jvcir.2022.103610,1047-3203,2022,103610,88,Journal of Visual Communication and Image Representation,An enhanced image quality assessment by synergizing superpixels and visual saliency,article,DENG2022103610
"Recently, numerous sand dust removal algorithms have been proposed. To our best knowledge, however, most methods evaluated their performance in a no-reference way using few selected real-world images from the internet. It is unclear how to quantitatively analyze the performance of the algorithms in a supervised way. Moreover, due to the absence of large-scale datasets, there are no well-known sand dust reconstruction report algorithms up till now. To bridge the gaps, we presented a comprehensive perceptual study and analysis of real-world sandstorm images, then constructed a Sand-dust Image Reconstruction Benchmark(SIRB) for training Convolutional Neural Networks(CNNs) and evaluating the algorithmâs performance. We adopted the existing image transformation neural network trained on SIRB as the baseline to illustrate the generalization of SIRB for training CNNs. Finally, we conducted a comprehensive evaluation to demonstrate the performance and limitations of the sandstorm enhancement algorithms, which shed light on future research in sandstorm image reconstruction.","Sand dust image, Benchmark dataset, Image reconstruction, Comprehensive evaluation, Convolutional neural networks",Yazhong Si and Fan Yang and Ya Guo and Wei Zhang and Yipu Yang,https://www.sciencedirect.com/science/article/pii/S1047320322001584,https://doi.org/10.1016/j.jvcir.2022.103638,1047-3203,2022,103638,89,Journal of Visual Communication and Image Representation,A comprehensive benchmark analysis for sand dust image reconstruction,article,SI2022103638
"At present, pose transfer and attribute control tasks are still the challenges for image synthesis network. At the same time, there are often artifacts in the images generated by the image synthesis network when the above two tasks are completed. The existence of artifacts causes the loss of the generated image details or introduces some wrong image information, which leads to the decline of the overall performance of the existing work. In this paper, a generative adversarial network (GAN) named ACGAN is proposed to accomplish the above two tasks and effectively eliminate artifacts in generated images. The proposed network was compared quantitatively and qualitatively with previous works on the DeepFashion dataset and better results are obtained. Moreover, the overall network has advantages over the previous works in speed and number of parameters.","GAN, Person image synthesis, Artifact",ShaoYue Lin and YanJun Zhang,https://www.sciencedirect.com/science/article/pii/S1047320322001067,https://doi.org/10.1016/j.jvcir.2022.103572,1047-3203,2022,103572,87,Journal of Visual Communication and Image Representation,ACGAN: Attribute controllable person image synthesis GAN for pose transfer,article,LIN2022103572
"Designing efficient deep neural networks has achieved great interest in image super-resolution (SR). However, exploring diverse network structures is computationally expensive. More importantly, each layer in a network has a distinct role that leads to the design of a specialized structure. In this work, we present a novel neural architecture search (NAS) algorithm that efficiently explores layer-wise structures. Specifically, we construct a supernet allowing flexibility in choosing the number of channels and per-channel activation functions according to the role of each layer. The search process runs efficiently via channel pruning since gradient descent jointly optimizes the Mult-Adds and the accuracy of the searched models. We facilitate estimating the model Mult-Adds in a differentiable manner using relaxations in the backward pass. The searched model, named FGNAS, outperforms the state-of-the-art NAS-based SR methods by a large margin.","Image super-resolution, Neural architecture search, Convolutional neural network",Heewon Kim and Seokil Hong and Bohyung Han and Heesoo Myeong and Kyoung Mu Lee,https://www.sciencedirect.com/science/article/pii/S1047320322001742,https://doi.org/10.1016/j.jvcir.2022.103654,1047-3203,2022,103654,89,Journal of Visual Communication and Image Representation,Fine-grained neural architecture search for image super-resolution,article,KIM2022103654
"Shadow detection is significant for scene understanding. As a common scenario, soft shadows have more ambiguous boundaries than hard shadows. However, they are rarely present in the available benchmarks since annotating for them is time-consuming and needs expert help. This paper discusses how to transfer the shadow detection capability from available shadow data to soft shadow data and proposes a novel shadow detection framework (MUSD) based on multi-scale feature fusion and unsupervised domain adaptation. Firstly, we set the existing labeled shadow dataset (i.e., SBU) as the source domain and collect an unlabeled soft shadow dataset (SSD) as the target domain to formulate an unsupervised domain adaptation problem. Next, we design an efficient shadow detection network based on the double attention module and multi-scale feature fusion. Then, we use the globalâlocal feature alignment strategy to align the task-related feature distributions between the source and target domains. This allows us to obtain a robust model and achieve domain adaptation effectively. Extensive experimental results show that our method can detect soft shadows more accurately than existing state-of-the-art methods.","Image processing, Shadow detection, Unsupervised domain adaptation, Multi-scale feature fusion",Kai Zhou and Wen Wu and Yan-Li Shao and Jing-Long Fang and Xing-Qi Wang and Dan Wei,https://www.sciencedirect.com/science/article/pii/S1047320322001225,https://doi.org/10.1016/j.jvcir.2022.103596,1047-3203,2022,103596,88,Journal of Visual Communication and Image Representation,Shadow detection via multi-scale feature fusion and unsupervised domain adaptation,article,ZHOU2022103596
"Deep neural network models with strong feature extraction capacity are prone to overfitting and fail to adapt quickly to new tasks with few samples. Gradient-based meta-learning approaches can minimize overfitting and adapt to new tasks fast, but they frequently use shallow neural networks with limited feature extraction capacity. We present a simple and effective approach called Meta-Transfer-Adjustment learning (MTA) in this paper, which enables deep neural networks with powerful feature extraction capabilities to be applied to few-shot scenarios while avoiding overfitting and gaining the capacity for quickly adapting to new tasks via training on numerous tasks. Our presented approach is classified into two major parts, the Feature Adjustment (FA) module, and the Task Adjustment (TA) module. The feature adjustment module (FA) helps the model to make better use of the deep network to improve feature extraction, while the task adjustment module (TA) is utilized for further improve the modelâs fast response and generalization capabilities. The proposed model delivers good classification results on the benchmark small sample datasets MiniImageNet and Fewshot-CIFAR100, as proved experimentally.","Few-shot learning, Deep neural networks, Feature adjustment, Task adjustment",Yadang Chen and Hui Yan and Zhi-Xin Yang and Enhua Wu,https://www.sciencedirect.com/science/article/pii/S1047320322001985,https://doi.org/10.1016/j.jvcir.2022.103678,1047-3203,2022,103678,89,Journal of Visual Communication and Image Representation,Meta-transfer-adjustment learning for few-shot learning,article,CHEN2022103678
"While many efforts have been devoted to addressing image denoising and achieve continuously improving results during the past few decades, it is fair to say that no a stand-alone method is consistently better than others. Nonetheless, many existing denoising methods, each having a different denoising capability, can yield various but complementary denoised images with respect to specific local areas. To effectively exploit the complementarity and diversity among the denoised images obtained with different denoisers, in this work we fuse them to produce an overall better result, which is fundamental to achieve robust and competitive denoising performance especially for complex scenes. A framework called deep fusion network (DFNet) is proposed to generate a consistent estimation about the final denoised image, taking advantage of the complementarity of denoisers and suppressing the bias. Specifically, given a noisy image, we first exploit a set of representative image denoisers to denoise it respectively, and obtain the corresponding initial denoised images. Then these initial denoised images are concatenated and fed into the proposed DFNet, and the proposed DFNet seeks to adjust its network parameters to produce the fused image (as the final denoised image) with an unsupervised training strategy through minimizing the carefully designed loss function. The experimental results show that our approach outperforms the stand-alone methods as well as the ones using combination strategy by large margin both in objective and subjective evaluations. Compared to the those methods that are relatively close to our strategy, the proposed DFNet is extensible and parameter free, which means it can cope with a variable number of different denoisers and avoid the manual intervention during the fusion process. The proposed DFNet has greater flexibility and better practicality.","Image denoising, Boosting denoising performance, Deep fusion network, Unsupervised training strategy",Shaoping Xu and Xiaojun Chen and Jie Luo and Xiaohui Cheng and Nan Xiao,https://www.sciencedirect.com/science/article/pii/S1047320322001468,https://doi.org/10.1016/j.jvcir.2022.103626,1047-3203,2022,103626,88,Journal of Visual Communication and Image Representation,An unsupervised fusion network for boosting denoising performance,article,XU2022103626
"Recently, inferring usersâ personality traits on social media has attracted extensive attention. Existing studies have shown that usersâ personality traits can be inferred from their preferences for images. However, since usersâ preferences on images are often affected by multiple factors, some liked images cannot effectively reflect their personality traits. To handle this issue, this paper proposes a personality modeling approach based on image aesthetic attribute-aware graph representation learning, which can leverage aesthetic attributes to refine the liked images that are consistent with usersâ personality traits. Specifically, we first utilize a Convolutional Neural Network (CNN) to train an aesthetic attribute prediction module. Then, attribute-aware graph representation learning is introduced to refine the images with similar aesthetic attributes from usersâ liked images. Finally, the aesthetic attributes of all refined images are combined to predict personality traits through a Multi-Layer Perceptron (MLP). Experimental results and visual analysis have shown that the proposed method is superior to state-of-the-art personality modeling methods.","Social media, Personality modeling, Image aesthetics, Attribute-aware graph, Convolutional neural network",Hancheng Zhu and Yong Zhou and Qiaoyue Li and Zhiwen Shao,https://www.sciencedirect.com/science/article/pii/S104732032200195X,https://doi.org/10.1016/j.jvcir.2022.103675,1047-3203,2022,103675,89,Journal of Visual Communication and Image Representation,Personality modeling from image aesthetic attribute-aware graph representation learning,article,ZHU2022103675
"Video super-resolution aims at restoring the spatial resolution of the reference frame based on consecutive input low-resolution (LR) frames. Existing implicit alignment-based video super-resolution methods commonly utilize convolutional LSTM (ConvLSTM) to handle sequential input frames. However, vanilla ConvLSTM processes input features and hidden states independently in operations and has limited ability to handle the inter-frame temporal redundancy in low-resolution fields. In this paper, we propose a multi-stage spatio-temporal adaptive network (MS-STAN). A spatio-temporal adaptive ConvLSTM (STAC) module is proposed to handle input features in low-resolution fields. The proposed STAC module utilizes the correlation between input features and hidden states in the ConvLSTM unit and modulates the hidden states adaptively conditioned on fused spatio-temporal features. A residual stacked bidirectional (RSB) architecture is further proposed to fully exploit the processing ability of the STAC unit. The proposed STAC and RSB architecture promote the vanilla ConvLSTMâs ability to exploit the inter-frame correlations, thus improving the reconstruction quality. Furthermore, different from existing methods that only aggregate features from the temporal branch once at a specified stage of the network, the proposed network is organized in a multi-stage manner. The corresponding temporal correlation in features at different stages can be fully exploited. Experimental results on Vimeo-90K-T and UMD10 datasets show that the proposed method has comparable performance with current video super-resolution methods. The code is available at https://github.com/yhjoker/MS-STAN.","Video super-resolution, ConvLSTM, Inter-frame correlation, Residual stacked bidirectional architecture",Yuhang Zhang and Zhenzhong Chen and Shan Liu,https://www.sciencedirect.com/science/article/pii/S1047320322000918,https://doi.org/10.1016/j.jvcir.2022.103555,1047-3203,2022,103555,87,Journal of Visual Communication and Image Representation,A multi-stage spatio-temporal adaptive network for video super-resolution,article,ZHANG2022103555
"Weakly supervised temporal action localization (WSTAL) is crucial for real world applications, as it relieves the huge burden of frame-level annotations for fully supervised action detection. Most existing WSTAL methods focused on classifying video snippets, or detecting action boundaries. However, the predictions from these well-designed models have not been fully utilized. Accordingly, we propose a weakly-supervised framework called the progressive enhancement network (PEN), which takes full advantages of the predictions generated by the preceding models to enhance the subsequent models. Specifically, snippet-level pseudo labels are generated from the preceding predictions by considering the similarity and temporal distance between action snippets. Then subsequent models are progressively enhanced by using pseudo labels as a supervision, and utilizing their underlying semantics to make the feature representation more qualified for the temporal localization task. Extensive experiments which are carried out on two popular benchmarks, THUMOSâ14 and ActivityNet v1.2, demonstrate the effectiveness of our method.","Temporal action localization, Weak supervision, Pseudo label, Video understanding",Qingyun Wang and Yan Song and Rong Zou and Xiangbo Shu,https://www.sciencedirect.com/science/article/pii/S1047320322001171,https://doi.org/10.1016/j.jvcir.2022.103590,1047-3203,2022,103590,87,Journal of Visual Communication and Image Representation,Progressive enhancement network with pseudo labels for weakly supervised temporal action localization,article,WANG2022103590
"Unlike existing reversible data hiding with contrast enhancement (RDHCE) methods, which excessively improve the image contrast for achieving the required capacity, the proposed method improves the image contrast appropriately while providing satisfactory embedding capacity. To this end, an adaptive multi-histogram RDHCE method is proposed in thisÂ study to improve the local and global contrastÂ by considering the local properties ofÂ the histograms. On the one hand, fuzzy C-means clustering combining multiple features that are deliberately designed for contrast enhancement is employed to generate seven sharply-distributed prediction error histograms (PEHs). Subsequently, the genetic algorithm is utilized to adaptively select the optimal pairs achieving the best embedding performance for each PEH according to the local characteristics of PEH distribution, resulting in improving the local contrast adaptively and embedding significant amount of data. Additionally, two-sided histogram shifting (HS) is utilized to improve the global contrast appropriately while embedding reasonable amount of data. The experimental results demonstrate that the proposed method achieves better local and global contrast while providing a high embedding capacity compared with other existing RDHCE methods.","Contrast enhancement, FCM clustering, Multiple histograms, Reversible data hiding, F Genetic algorithm",Tiancong Zhang and Caijie Yang and Shaowei Weng and Tanshuai Hou,https://www.sciencedirect.com/science/article/pii/S1047320322001572,https://doi.org/10.1016/j.jvcir.2022.103637,1047-3203,2022,103637,89,Journal of Visual Communication and Image Representation,Adaptive multi-histogram reversible data hiding with contrast enhancement,article,ZHANG2022103637
"Compression of captured video frames is crucial for saving the power in wireless capsule endoscopy (WCE). A low complexity encoder is desired to limit the power consumption required for compressing the WCE video. Distributed video coding (DVC) technique is best suitable for designing a low complexity encoder. In this technique, frames captured in RGB colour space are converted into YCbCr colour space. Both Y and CbCr representing luma and chroma components of the WynerâZiv (WZ) frames are processed and encoded in existing DVC techniques proposed for WCE video compression. In the WCE video, consecutive frames exhibit more similarity in texture and colour properties. The proposed work uses these properties to present a method for processing and encoding only the luma component of a WZ frame. The chroma components of the WZ frame are predicted by an encoderâdecoder based deep chroma prediction model at the decoder by matching luma and texture information of the keyframe and WZ frame. The proposed method reduces the computations required for encoding and transmitting of WZ chroma component. The results show that the proposed DVC with a deep chroma prediction model performs better when compared to motion JPEG and existing DVC systems for WCE at the reduced encoder complexity.","Distributed video coding, Chroma prediction, Convolutional neural networks, Video compression, Wireless capsule endoscopy",Sushma B. and Aparna P.,https://www.sciencedirect.com/science/article/pii/S1047320322001055,https://doi.org/10.1016/j.jvcir.2022.103578,1047-3203,2022,103578,87,Journal of Visual Communication and Image Representation,Deep chroma prediction of WynerâZiv frames in distributed video coding of wireless capsule endoscopy video,article,B2022103578
"In this paper, we strive to propose a self-interpretable framework, termed PrimitiveTree, that incorporates deep visual primitives condensed from deep features with a conventional decision tree, bridging the gap between deep features extracted from deep neural networksÂ (DNNs) and treesâ transparent decision-making processes. Specifically, we utilize a codebook, which embeds the continuous deep features into a finite discrete spaceÂ (deep visual primitives) to distill the most common semantic information. The decision tree adopts the spatial location information and the mapped primitives to present the decision-making process of the deep features in a tree hierarchy. Moreover, the trained interpretable PrimitiveTree can inversely explain the constituents of the deep features, highlighting the most critical and semantic-rich image patches attributing to the final predictions of the given DNN. Extensive experiments and visualization results validate the effectiveness and interpretability of our method.","Interpretability, Deep neural network, Discrete representation learning",Mengqi Xue and Haofei Zhang and Qihan Huang and Jie Song and Mingli Song,https://www.sciencedirect.com/science/article/pii/S1047320322002024,https://doi.org/10.1016/j.jvcir.2022.103682,1047-3203,2022,103682,89,Journal of Visual Communication and Image Representation,Learn decision trees with deep visual primitives,article,XUE2022103682
"The brute-force behavior of High Efficiency Video Coding (HEVC) is the biggest hurdle in the communication of multimedia content. Therefore, two novel methods will be presented here to expedite the intra mode decision process of HEVC. In the first algorithm, the feasibility of Histogram of Oriented Gradients (HOG) for early intra mode decision is presented by using statistical evidence. Then, HOG of the current block and 35 intra predictions are obtained. The intra-prediction that gives the least sum of absolute difference (SAD) with the HOG of the current block is selected as the termination point. In the second algorithm, the difference between the Hardmard-cost of intra modes is modeled to achieve fast intra mode decision. The proposed algorithms accelerated the encoding process of the HEVC by 5% and 35.57%, while their Bjontegaard Delta Bit Rate (BD-BR) is 1.09% and 1.61%, respectively.","Histogram of Oriented Gradients, HEVC, Intra mode, Fast Encoding",Junaid Tariq and Amir Ijaz and Ammar Armghan and Hameedur Rahman and Hashim Ali and Fayadh Alenezi,https://www.sciencedirect.com/science/article/pii/S1047320322001213,https://doi.org/10.1016/j.jvcir.2022.103594,1047-3203,2022,103594,88,Journal of Visual Communication and Image Representation,HEVCâs intra mode process expedited using Histogram of Oriented Gradients,article,TARIQ2022103594
"Modern deep convolutional neural networks(CNNs) are often designed to be scalable, leading to the model family concept. A model family is a large (possibly infinite) collection of related neural network architectures. The isomorphism of a model family refers to the fact that the models within it share the same high-level structure. Meanwhile, the models within the model family are called isomorphic models for each other. Existing weight initialization methods for CNNs use random initialization or data-driven initialization. Even though these methods can perform satisfactory initialization, the isomorphism of model families is rarely explored. This work proposes an isomorphic model-based initialization method (IM Init) for CNNs. It can initialize any network with another well-trained isomorphic model in the same model family. We first formulate the widely used general network structure of CNNs. Then a structural weight transformation is presented to transform the weight between two isomorphic models. Finally, we apply our IM Init to the model down-sampling and up-sampling scenarios and confirm its effectiveness in improving accuracy and convergence speed through experiments on various image classification datasets. In the model down-sampling scenario, IM Init initializes the smaller target model with a larger well-trained source model. It improves the accuracy of RegNet200MF by 1.59% on the CIFAR-100 dataset and 1.9% on the CUB200 dataset. Inversely, IM Init initializes the larger target model with a smaller well-trained source model in the model up-sampling scenario. It significantly speeds up the convergence of RegNet600MF and improves the accuracy by 30.10% under short training schedules. Code will be available.","Convolutional neural networks, Weight initialization, Isomorphic model, Structural weight transformation",Hong Zhang and Yang Li and Hanqing Yang and Bin He and Yu Zhang,https://www.sciencedirect.com/science/article/pii/S1047320322001973,https://doi.org/10.1016/j.jvcir.2022.103677,1047-3203,2022,103677,89,Journal of Visual Communication and Image Representation,Isomorphic model-based initialization for convolutional neural networks,article,ZHANG2022103677
"Most of the existing Action Quality Assessment (AQA) methods for scoring sports videos have deeply researched how to evaluate the single action or several sequential-defined actions that performed in short-term sport videos, such as diving, vault, etc. They attempted to extract features directly from RGB videos through 3D ConvNets, which makes the features mixed with ambiguous scene information. To investigate the effectiveness of deep pose feature learning on automatically evaluating the complicated activities in long-duration sports videos, such as figure skating and artistic gymnastic, we propose a skeleton-based deep pose feature learning method to address this problem. For pose feature extraction, a spatialâtemporal pose extraction module (STPE) is built to capture the subtle changes of human body movements and obtain the detail representations for skeletal data in space and time dimensions. For temporal information representation, an inter-action temporal relation extraction module (ATRE) is implemented by recurrent neural network to model the dynamic temporal structure of skeletal subsequences. We evaluate the proposed method on figure skating activity of MIT-skate and FIS-V datasets. The experimental results show that the proposed method is more effective than RGB video-based deep feature learning methods, including SENet and C3D. Significant performance progress has been achieved for the Spearman Rank Correlation (SRC) on MIT-Skate dataset. On FIS-V dataset, for the Total Element Score (TES) and the Program Component Score (PCS), better SRC and MSE have been achieved between the predicted scores against the judgeâs ones when compared with SENet and C3D feature methods.","Action quality assessment, Figure skating sport videos, Spatialâtemporal pose feature extraction, Action relation learning",Huiying Li and Qing Lei and Hongbo Zhang and Jixiang Du and Shangce Gao,https://www.sciencedirect.com/science/article/pii/S1047320322001456,https://doi.org/10.1016/j.jvcir.2022.103625,1047-3203,2022,103625,89,Journal of Visual Communication and Image Representation,Skeleton-based deep pose feature learning for action quality assessment on figure skating videos,article,LI2022103625
"Nowadays, stereoscopic image quality assessment (SIQA) based on convolutional neural network (CNN) has become the mainstream model of image quality assessment (IQA). Compared with the two-dimensional quality evaluation model, stereoscopic image quality evaluation is more challenging due to the effects of depth and parallax information. In this paper, we propose a two-stream interactive network model to perform quality evaluation, which can well simulate the process of human stereo visual perception. Meanwhile, we enhance the extraction of local and global features of images by asymmetric convolution kernel and interactive sub-networks of inter-layers, respectively, which can further optimize our network model. Our proposed algorithm was evaluated on four public databases. The final experimental results show that our proposed algorithm exhibits good performance not only on the whole database but also on each single distortion type.","Stereoscopic image quality evaluation, Binocular fusion, Asymmetric convolution kernel, CNN, Summation and difference channels",Yun Liu and Baoqing Huang and Guanghui Yue and Jingkai Wu and Xiaoxu Wang and Zhi Zheng,https://www.sciencedirect.com/science/article/pii/S104732032200116X,https://doi.org/10.1016/j.jvcir.2022.103586,1047-3203,2022,103586,87,Journal of Visual Communication and Image Representation,Two-stream interactive network based on local and global information for No-Reference Stereoscopic Image Quality Assessment,article,LIU2022103586
"In this paper, we propose a strong two-stream point cloud sequence network VirtualActionNet for 3D human action recognition. In the data preprocessing stage, we transform the depth sequence into a point cloud sequence as the input of our VirtualActionNet. In order to encode intra-frame appearance structures, static point cloud technologies are first employed as a virtual action generation sequence module to abstract the point cloud sequence into a virtual action sequence. Then, a two-stream network framework is presented to model the virtual action sequence. Specifically, we design an appearance stream module for aggregating all the appearance information preserved in each virtual action frame. Moreover, a motion stream module is introduced to capture dynamic changes along the time dimension. Finally, a joint loss strategy is adopted during data training to improve the action prediction accuracy of the two-stream network. Extensive experiments on three publicly available datasets demonstrate the effectiveness of the proposed VirtualActionNet.","Two-stream network, 3D action recognition, Point cloud sequence",Xing Li and Qian Huang and Zhijian Wang and Tianjin Yang,https://www.sciencedirect.com/science/article/pii/S1047320322001614,https://doi.org/10.1016/j.jvcir.2022.103641,1047-3203,2022,103641,89,Journal of Visual Communication and Image Representation,VirtualActionNet: A strong two-stream point cloud sequence network for human action recognition,article,LI2022103641
"The Hashing process is an effective tool for handling large-scale data (for example, images, videos, or multi-model data) retrieval problems. To get better retrieval accuracy, hashing models usually are imposed with three rigorous constraints, i.e., discrete binary constraint, uncorrelated condition, and the balanced constraint, which will lead to being âNP-hardâ. In this study, we divide the whole constraints set into the uncorrelated (orthogonality) constraint and the binary discrete balance constraint and propose a fast and accurate penalty function semi-continuous thresholding (PFSCT) hash coding algorithm based on forwardâbackward algorithms. In addition, we theoretically analyze the equivalence between the relaxed model and the original problems. Extensive numerical experiments on diverse large-scale benchmark datasets demonstrate comparable performance and effectiveness of the proposed method.","Hash coding, Large-scale data retrieval, Orthogonality constraint, Quantization error reduction",Qian Chen and Zhengwei Shen and Zhe Chen,https://www.sciencedirect.com/science/article/pii/S1047320322000888,https://doi.org/10.1016/j.jvcir.2022.103552,1047-3203,2022,103552,87,Journal of Visual Communication and Image Representation,A penalty function semi-continuous thresholding methods for constraints of hashing problems,article,CHEN2022103552
"Dynamic hand gesture recognition is still an interesting topic for the computer vision community. A set of feature vectors can represent any hand gesture. A Recurrent Neural Network (RNN) can recognize these feature vectors as a hand gesture that analyzes the temporal and contextual information of the gesture sequence. Thus, we proposed a hybrid deep learning framework to recognize dynamic hand gestures. In the Hybrid model GoogleNet is pipelined with a Bidirectional GRU unit to recognize the dynamic hand gesture. Dynamic hand gestures consist of many frames, and features of each frame need to be extracted to get the temporal and dynamic information of the performed gesture. As RNN takes input as a sequence of feature vectors, we extract features from videos using pretrained GoogleNet. As Gated Recurrent Unit is one of the variants of RNN to classify the sequential data, we created a feature vector that corresponds to each video and passed it to the bidirectional GRU (BGRU) network to classify the gestures. We evaluate our model on four publicly available hand gesture datasets. The proposed method performs well and is comparable with the existing methods. For instance, we achieved 98.6% accuracy on Northwestern University Hand Gesture(NWUHG), 99.6% on SKIG, 99.4% on Cambridge Hand Gesture (CHG) datasets respectively. We performed our experiments on DHG14/28 dataset and achieved an accuracy of 97.8% with 14-gesture classes and 92.1% on 28-gesture classes. DHG14/28 dataset contains skeleton and depth data, and our proposed model used depth data and achieved comparable accuracy.","Dynamic hand gesture, Gated recurrent unit, Convolutional neural network, Recurrent neural network",Bindu Verma,https://www.sciencedirect.com/science/article/pii/S1047320322000906,https://doi.org/10.1016/j.jvcir.2022.103554,1047-3203,2022,103554,87,Journal of Visual Communication and Image Representation,A two stream convolutional neural network with bi-directional GRU model to classify dynamic hand gesture,article,VERMA2022103554
"Compared with traditional visibleâvisible person re-identification, the modality discrepancy between visible and infrared images makes person re-identification more challenging. Existing methods rely on learning efficient transformation mechanisms in paired images to reduce the modality gap, which inevitably introduces noise. To get rid of these limitations, we propose a Hierarchical Cross-modal shared Feature Network (HCFN) to mine modality-shared and modality-specific information. Since infrared images lack color and other information, we construct an Intra-modal Feature Extraction Module (IFEM) to learn the content information and reduce the difference between visible and infrared images. In order to reduce the heterogeneous division, we apply a Cross-modal Graph Interaction Module (CGIM) to align and narrow the set-level distance of the inter-modal images. By jointly learning two modules, our method can achieve 66.44% Rank-1 on SYSU-MM01 dataset and 74.81% Rank-1 on RegDB datasets, respectively, which is superior compared with the state-of-the-art methods. In addition, ablation experiments demonstrate that HCFN is at least 4.9% better than the baseline network.","Visible-infrared person re-identification, Feature extraction, Deep learning, Hierarchical attention mechanism",Yueying Li and Huaxiang Zhang and Li Liu,https://www.sciencedirect.com/science/article/pii/S1047320322002097,https://doi.org/10.1016/j.jvcir.2022.103689,1047-3203,2022,103689,89,Journal of Visual Communication and Image Representation,HCFN: Hierarchical cross-modal shared feature network for visible-infrared person re-identification,article,LI2022103689
"Image inpainting aims to fill in the missing regions of damaged images with plausible content. Existing inpainting methods tend to produce ambiguous artifacts and implausible structures. To address the above issues, our method aims to fully utilize the information of known regions to provide style and structural guidance for missing regions. Specifically, the Adaptive Style Fusion (ASF) module reduces artifacts by transferring visual style features from known regions to missing regions. The Gradient Attention Guidance (GAG) module generates accurate structures by aggregating semantic information along gradient boundary regions. In addition, the Multi-scale Attentional Feature Extraction (MAFE) module extracts global contextual information and enhances the representation of image features. The sufficient experimental results on the three datasets demonstrate that our proposed method has superior performance in terms of visual plausibility and structural consistency compared to state-of-the-art inpainting methods.","Image inpainting, Style transfer, Gradient attention, Multi-scale gradient loss",Ye Zhu and Chao Wang and Shuze Geng and Yang Yu and Xiaoke Hao,https://www.sciencedirect.com/science/article/pii/S1047320322002012,https://doi.org/10.1016/j.jvcir.2022.103681,1047-3203,2022,103681,89,Journal of Visual Communication and Image Representation,Multi-scale gradient attention guidance and adaptive style fusion for image inpainting,article,ZHU2022103681
"Image dehazing methods aim to solve the problem of poor visibility in images due to haze. Techniques proposed for image dehazing in literature focus on image priors, haze lines or data driven statistical models. Variations of the classical methods relying on prior model or haze line model use no-reference image quality metrics to prove their dehazing performance. Recently developed deep learning models rely on huge amounts of hazy, haze-free pairs for training, and uses PSNR and SSIM like image reconstruction metrics to show their performance. These methods perform poorly on no-reference image quality assessments and also dehazes poorly at the depths of the image. These methods though can be optimized for memory usage and are faster. This work presents a deep learning model (Feature Fusion Attention Network) trained on a domain randomized synthetic dataset generated in simulation. The proposed model achieves the highest scores on blind image assessments through the gradient rationing technique for a deep learning-based approach by a significant margin. The images were evaluated on full-reference metrics as well and obtained favorable results. This approach also yields one of the highest edge sharpness obtained after dehazing. The training procedure adopted to obtain significant gains on real-world dehazing, without using any real-world data is also detailed in this paper.","Dehazing, Deep learning, Domain randomization, Dataset",Abdul Fathaah Shamsuddin and Krupasankari Ragunathan and Abhijith P. and Deepak {Raja Sekar P.M.} and Praveen Sankaran,https://www.sciencedirect.com/science/article/pii/S1047320322001560,https://doi.org/10.1016/j.jvcir.2022.103636,1047-3203,2022,103636,89,Journal of Visual Communication and Image Representation,From synthetic to natural â single natural image dehazing deep networks using synthetic dataset domain randomization,article,SHAMSUDDIN2022103636
"Fine-grained Visual Categorization (FGVC) in computer vision aims to recognize images belonging to multiple subordinate categories of a super-category. The difficulty of FGVC lies in the close resemblance among inter-classes and large variations among intra-classes. Most existing networks only focus on a few discriminative regions, while ignoring many subtle complementary features. So we propose a Progressive Erasing Network (PEN). In PEN, a Multi-Grid Erasure mechanism augments data samples and assists in capturing the local discriminative features, where the overall structure of the image is destroyed indirectly through pixel-wise erasure. Cross-layer feature aggregation by extracting salient class features is of great significance in FGVC. However, the capability of cross-layer feature representation based on a simple aggregation strategy is still inefficient. To this end, the proposed Consistency loss explores the cross-layer semantic affinity, which guides the Cross-Layer Incentive (CLI) block to mine more efficient feature representations of different granularity. We also integrate Cross Entropy and Complementary Entropy to take the distribution of negative classes into account for better classification performance. Our method uses end-to-end training with only classification labels. Experimental results show that our model outperforms the state-of-the-art on three fine-grained benchmarks.","FGVC, PEN, Multi-grid erasure mechanism, Cross-layer incentive block, Consistency loss",Jin Peng and Yongxiong Wang and Zeping Zhou,https://www.sciencedirect.com/science/article/pii/S1047320322001043,https://doi.org/10.1016/j.jvcir.2022.103570,1047-3203,2022,103570,87,Journal of Visual Communication and Image Representation,Progressive Erasing Network with consistency loss for fine-grained visual classification,article,PENG2022103570
"This paper presents a novel No-Reference Video Quality Assessment (NR-VQA) model that utilizes proposed 3D steerable wavelet transform-based Natural Video Statistics (NVS) features as well as human perceptual features. Additionally, we proposed a novel two-stage regression scheme that significantly improves the overall performance of quality estimation. In the first stage, transform-based NVS and human perceptual features are separately passed through the proposed hybrid regression scheme: Support Vector Regression (SVR) followed by Polynomial curve fitting. The two visual quality scores predicted from the first stage are then used as features for the similar second stage. This predicts the final quality scores of distorted videos by achieving score level fusion. Extensive experiments were conducted using five authentic and four synthetic distortion databases. Experimental results demonstrate that the proposed method outperforms other published state-of-the-art benchmark methods on synthetic distortion databases and is among the top performers on authentic distortion databases. The source code is available at https://github.com/anishVNIT/two-stage-vqa.","Human visual system, No-reference video quality assessment, 3D steerable DWT, Perceptual features, User generated content, Support vector regression",Anish Kumar Vishwakarma and Kishor M. Bhurchandi,https://www.sciencedirect.com/science/article/pii/S1047320322001961,https://doi.org/10.1016/j.jvcir.2022.103676,1047-3203,2022,103676,89,Journal of Visual Communication and Image Representation,No-Reference Video Quality Assessment using novel hybrid features and two-stage hybrid regression for score level fusion,article,VISHWAKARMA2022103676
"With the cutting-edge improvement of web, online abuses have been increasing rapidly. Phishing is the most widely recognized abuses performed by digital crooks nowadays. It is an activity to steal private data (for example, client names, passwords and Visa data) in an electronic correspondence. It is a sort of fraud with the end goal of monetary benefit and other fake exercises. It utilizes phony websites that resemble genuine ones. Phishing messages might contain links to sites that are contaminated with malware. In this paper, âan anti-phishing approach using multi secret sharing schemeâ is implemented as an answer to this problem. Here, Dynamic Image CAPTCHA based verification using multi secret sharing is performed. Image CAPTCHA is divided into two pieces called shares. Multiple secret pictures are revealed by overlapping the same set of shares at different angles. In the proposed approach, shares are of different modes i.e., userâs share is imprinted on a physical transparency while serverâs share is in digital mode. By using the proposed approach, websites and end clients can cross confirm their identity.","Phishing, Dynamic image CPTCHA, Visual cryptography, Multi secret sharing",Akanksha Arora and Hitendra Garg and Shivendra Shivani,https://www.sciencedirect.com/science/article/pii/S1047320322001444,https://doi.org/10.1016/j.jvcir.2022.103624,1047-3203,2022,103624,88,Journal of Visual Communication and Image Representation,Anti-phishing technique based on dynamic image captcha using multi secret sharing scheme,article,ARORA2022103624
"Current state-of-the-art two-stage models on instance segmentation task suffer from several types of imbalances. In this paper, we address the Intersection over the Union (IoU) distribution imbalance of positive input Regions of Interest (RoIs) during the training of the second stage. Our Self-Balanced R-CNN (SBR-CNN), an evolved version of the Hybrid Task Cascade (HTC) model, brings brand new loop mechanisms of bounding box and mask refinements. With an improved Generic RoI Extraction (GRoIE), we also address the feature-level imbalance at the Feature Pyramid Network (FPN) level, originated by a non-uniform integration between low- and high-level features from the backbone layers. In addition, the redesign of the architecture heads toward a fully convolutional approach with FCC further reduces the number of parameters and obtains more clues to the connection between the task to solve and the layers used. Moreover, our SBR-CNN model shows the same or even better improvements if adopted in conjunction with other state-of-the-art models. In fact, with a lightweight ResNet-50 as backbone, evaluated on COCO minival 2017 dataset, our model reaches 45.3% and 41.5% AP for object detection and instance segmentation, with 12 epochs and without extra tricks. The code is available at https://github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn.","Object detection, Instance segmentation, Imbalance in R-CNN networks, Two-stage deep learning architectures",Leonardo Rossi and Akbar Karimi and Andrea Prati,https://www.sciencedirect.com/science/article/pii/S1047320322001201,https://doi.org/10.1016/j.jvcir.2022.103595,1047-3203,2022,103595,87,Journal of Visual Communication and Image Representation,Self-Balanced R-CNN for instance segmentation,article,ROSSI2022103595
"This paper focuses on the task of human-object interaction (HOI) recognition, which aims to classify the interaction between human and objects. It is a challenging task partially due to the extremely imbalanced data among classes. To solve this problem, we propose a language-guided graph parsing attention network (LG-GPAN) that makes use of the word distribution in language to guide the classification in vision. We first associate each HOI class name with a word embedding vector in language and then all the vectors can construct a language space specified for HOI recognition. Simultaneously, the visual feature is extracted from the inputs via the proposed graph parsing attention network (GPAN) for better visual representation. The visual feature is then transformed into the linguistic one in language space. Finally, the output score is obtained via measuring the distance between the linguistic feature and the word embedding of classes in language space. Experimental results on the popular CAD-120 and V-COCO datasets validate our design choice and demonstrate its superior performance in comparison to the state-of-the-art.","Human-object interaction, Language-guided, Graph parsing attention network, Word embedding",Qiyue Li and Xuemei Xie and Jin Zhang and Guangming Shi,https://www.sciencedirect.com/science/article/pii/S1047320322001602,https://doi.org/10.1016/j.jvcir.2022.103640,1047-3203,2022,103640,89,Journal of Visual Communication and Image Representation,Language-guided graph parsing attention network for human-object interaction recognition,article,LI2022103640
"In contrast to Convolutional Neural Networks (CNNs), Vision Transformers (ViT) cannot capture sequence ordering of input tokens and require position embeddings. As a learnable fixed-dimension vector, the position embedding improves accuracy while limiting the migration of the model between different input sizes. Hence, this paper conducts an empirical study on position embeddings of pre-trained models, which mainly focuses on two questions: (1) What do the position embeddings learn from training? (2) How do the position embeddings affect the self-attention modules? This paper analyzes the pattern of position embedding in pre-trained models and finds that the linear combination of Gabor filters and edge markers can fit the learned position embeddings well. The Gabor filters and edge markers can occupy some channels to append the position information, and the edge markers have flowed to values in self-attention modules. The experimental results can guide future work to choose suitable position embeddings.","Vision transformer, Position embeddings, Gabor filters",Kai Jiang and Peng Peng and Youzao Lian and Weisheng Xu,https://www.sciencedirect.com/science/article/pii/S1047320322001845,https://doi.org/10.1016/j.jvcir.2022.103664,1047-3203,2022,103664,89,Journal of Visual Communication and Image Representation,The encoding method of position embeddings in vision transformer,article,JIANG2022103664
"High Efficiency Video Coding (HEVC) is well-known as an internationally popular video coding standard, and HEVC-based steganography has received increasing attention. In this paper, a new adaptive HEVC video information hiding method based on Prediction Unit (PU) partition mode and double-layer embedding strategy is proposed. Double-layer embedding is a method to complete the first-layer embedding using the mapping rules of PU partition mode, and to perform the second-layer embedding after the first-layer embedding. The cost assignment function designed in this paper can accurately evaluate the second-layer data embedding distortion. The frame position, motion properties and block size of PU are taken into consideration for the second-layer data embedding, and the syndrome-trellis codes (STCs) are used to minimize the embedding distortion. Experimental results show that the proposed adaptive double-layer embedding algorithm has better embedding efficiency and less embedding distortion in most cases.","Adaptive video information hiding, HEVC, PU partition mode, STC, Double-layer embedding",Songhan He and Dawen Xu and Lin Yang and Yong Liu,https://www.sciencedirect.com/science/article/pii/S1047320322000876,https://doi.org/10.1016/j.jvcir.2022.103549,1047-3203,2022,103549,87,Journal of Visual Communication and Image Representation,HEVC video information hiding scheme based on adaptive double-layer embedding strategy,article,HE2022103549
"The challenges of cross-domain person re-identification mainly derive from two aspects: (1) The missing of target data labels. (2) The bias between source domain and target domain. Most of existing works focus on only one problem in the above two or deal with them separately. In this paper, we propose a new approach referred as to multi-level mutual supervision to achieve full utilization of labeled source data and unlabeled target data. Along this approach, we construct a dual-branch framework of which the upper branch is trained with original source data and target data while the lower branch is trained with augmented source data and target data. By applying common-pseudo-label and Maximum Mean Discrepancy (MMD) loss in our framework, the mutual supervision in multi levels is achieved. The results show that our model achieves SOTA performance on multiple popular benchmark datasets.","Unsupervised domain adaptation, Mutual supervision, Person Re-identification, Cross domain",Chunren Tang and Dingyu Xue and Dongyue Chen,https://www.sciencedirect.com/science/article/pii/S1047320322001948,https://doi.org/10.1016/j.jvcir.2022.103674,1047-3203,2022,103674,89,Journal of Visual Communication and Image Representation,Multi-level mutual supervision for cross-domain Person Re-identification,article,TANG2022103674
"The most prevalent type of digital image falsification occurs when a portion of a image is copied and pasted onto another section of the same image. Falsification of the image made in this way is called copy-move forgery (CMF). This study presents a new and effective approach for copy-move forgery detection (CMFD) using the Local Intensity Order Pattern (LIOP) to overcome the restrictions of existing CMFD techniques. The input image is first converted to a YCbCr color space and then split into Y, Cb, and Cr color channels. The LIOP features are then extracted from each color channel and all the features are combined. The feature vectors are ordered lexicographically and related features are detected by comparing the LIOP features. Although the LIOP feature has rarely been used in CMFD prior to this study, the success rate of the proposed method is high. In addition, since the channels are not correlated to each other in the YCbCr color space, each color channel is considered as a gray image, and the success rate is increased by combining the features extracted from each of the color channels. The proposed approach was assessed using the CoMoFoD and GRIP datasets. Experimental findings demonstrated that the suggested method was successful and displayed robustness in post-processing attacks.","Copy-move forgery, LIOP, YCbCr, Keypoint, Image processing",YÄ±ldÄ±z AydÄ±n,https://www.sciencedirect.com/science/article/pii/S104732032200181X,https://doi.org/10.1016/j.jvcir.2022.103661,1047-3203,2022,103661,89,Journal of Visual Communication and Image Representation,A new Copy-Move forgery detection method using LIOP,article,AYDIN2022103661
"A deep learning method called PTR-CNN (Predicted frame with Transform unit partition and prediction Residual aided CNN) is proposed for in-loop filtering in video compression. To reduce the computational complexity of an end-to-end CNN in-loop filter, a non-learning method of reference frame selection is designed to select the highest quality frame based on the frameâs blurriness and smoothiness scores. The transform unit (TU) partition and the prediction residual (PR) of the current frame are used as extra inputs to the neural network as the filtering guidance. The selected similar and high quality reference frame (RF) and the current unfiltered frame (CUF) are input to a CNN based motion compensation module to generate a predicted frame (PF). Finally input the PF, the CUF, the CUFâs TU partition and the CUFâs PR into the main CNN to reconstruct the filtered frame. The model is implemented in Tensorflow and tested in HEVC and AV1. Experimental results show that the complexity of proposed PTR-CNN is less than SOTA CNN-based reference aided in-loop filtering methods and slightly outperforms their RD performance. The scheme introduces a complexity overhead of 7% on the encoder. In particular, for random access, the proposed model achieves 11.78% coding gain over HEVC with DBF/SAO off, while has a gain of 4.76% over HEVC with DBF/SAO on. Ablation study demonstrates that the RF contributes about 10% of the total gain, and the TU and PR contribute over 4% of the total one, proving the effectiveness of each module. Moreover, it is observed that the proposed method can restore detailed structures and textures and hence improve the subjective quality.","Video coding, Blocking artifact, In-loop filter, Convolutional neural network, Image quality assessment",Tong Shao and Tianqi Liu and Dapeng Wu and Chia-Yang Tsai and Zhijun Lei and Ioannis Katsavounidis,https://www.sciencedirect.com/science/article/pii/S1047320322001390,https://doi.org/10.1016/j.jvcir.2022.103615,1047-3203,2022,103615,88,Journal of Visual Communication and Image Representation,PTR-CNN for in-loop filtering in video coding,article,SHAO2022103615
"At present, it is difficult for the multiple images zero-watermark algorithm to protect all the images in the image set, and repeated operations will reduce the efficiency of the algorithm. To solve these issues, the proposed algorithm can design a reasonable copyright protection scheme according to the number of images in the image set to realize the protection of all images, and reduce the cost of time and storage. The gray-weighted average image fusion method is used to fuse multiple normalized standard images into one image. The LWT(Lifting the Wavelet Transform)-QR decomposition is applied to the effective area of the fusion image to obtain the robust feature image. Non-extended visual cryptography is used to enhance the security of the algorithm. A zero-watermark image is obtained by using the XOR manipulation for the feature image and the public shared image. Experimental results demonstrate that the proposed algorithm has good performance.","Zero-watermark, Multiple images, Copyright protection, Non-extended visual cryptography, Gray-weighted average image fusion",Baowei Wang and Weishen Wang and Peng Zhao,https://www.sciencedirect.com/science/article/pii/S1047320322001031,https://doi.org/10.1016/j.jvcir.2022.103569,1047-3203,2022,103569,87,Journal of Visual Communication and Image Representation,A zero-watermark algorithm for multiple images based on visual cryptography and image fusion,article,WANG2022103569
"Representing contextual features at multiple scales is important for RGB-D SOD. Recently, due to advances in backbone convolutional neural networks (CNNs) revealing stronger multi-scale representation ability, many methods achieved comprising performance. However, most of them represent multi-scale features in a layer-wise manner, which ignores the fine-grained global contextual cues in a single layer. In this paper, we propose a novel global contextual exploration network (GCENet) to explore the performance gain of multi-scale contextual features in a fine-grained manner. Concretely, a cross-modal contextual feature module (CCFM) is proposed to represent the multi-scale contextual features at a single fine-grained level, which can enlarge the range of receptive fields for each network layer. Furthermore, we design a multi-scale feature decoder (MFD) that integrates fused features from CCFM in a top-down way. Extensive experiments on five benchmark datasets demonstrate that the proposed GCENet outperforms the other state-of-the-art (SOTA) RGB-D SOD methods.","Salient object detection, Convolution neural network, Multi-scale, Global contextual",Chenxing Xia and Songsong Duan and Xiuju Gao and Yanguang Sun and Rongmei Huang and Bin Ge,https://www.sciencedirect.com/science/article/pii/S1047320322002000,https://doi.org/10.1016/j.jvcir.2022.103680,1047-3203,2022,103680,89,Journal of Visual Communication and Image Representation,GCENet: Global contextual exploration network for RGB-D salient object detection,article,XIA2022103680
"Recently, very deep convolution neural network (CNN) has shown strong ability in single image super-resolution (SISR) and has obtained remarkable performance. However, most of the existing CNN-based SISR methods rarely explicitly use the high-frequency information of the image to assist the image reconstruction, thus making the reconstructed image looks blurred. To address this problem, a novel contour enhanced Image Super-Resolution by High and Low Frequency Fusion Network (HLFN) is proposed in this paper. Specifically, a contour learning subnetwork is designed to learn the high-frequency information, which can better learn the texture of the image. In order to reduce the redundancy of the contour information learned by the contour learning subnetwork during fusion, the spatial channel attention block (SCAB) is introduced, which can select the required high-frequency information adaptively. Moreover, a contour loss is designed and it is used with the â1 loss to optimize the network jointly. Comprehensive experiments demonstrate the superiority of our HLFN over state-of-the-art SISR methods.","Contour, Attention mechanism, Deep convolution neural network",Linhua Kong and Yiming Wang and Dongxia Chang and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S1047320322001791,https://doi.org/10.1016/j.jvcir.2022.103659,1047-3203,2022,103659,89,Journal of Visual Communication and Image Representation,Contour enhanced image super-resolution,article,KONG2022103659
"High dynamic range imaging (HDRI) is an excellent high-quality image acquisition technique, which can reflect real human visual characteristics from one (or several) captured low dynamic range (LDR) image. However, the input LDR image only provides partial information of the scene. Besides, in traditional HDRI methods that require multiple captured images as input, field of view errors can be induced, which will be difficult to apply it to the emerging image acquisition systems. Here, we propose a novel HDRI method that reconstructs an HDR image from only a pair of short- and long-exposure images based on artificial remapping using multi-scale exposure fusion. Firstly, we introduce a simulated exposure model called artificial remapping to synthesize a multi-exposure image sequence from the input LDR image pairs. Then, weighting maps of the sequence for fusion can be obtained according to the evaluation factors of contrast, saturation, as well as improved exposedness. Finally, we utilize the pyramid based multiscale exposure fusion framework to integrate them into an enhanced HDR image. Comparative experiments, fully implemented on some source images, have been demonstrated that better performance can be realized compared with some competing methods in qualitative and quantitative evaluation. Note that the operation of the proposed method is simple yet effective, which is easy to popularize. The method thus can be potentially applied to the emerging image acquisition systems where two images are captured simultaneously by two image sensors or by one image sensor with a pair of short- and long-exposure setting.","High dynamic range imaging, Exposure fusion, Artificial remapping, Image processing",Junbao Hu and Lingfeng Wu and Na Li,https://www.sciencedirect.com/science/article/pii/S1047320322001158,https://doi.org/10.1016/j.jvcir.2022.103585,1047-3203,2022,103585,87,Journal of Visual Communication and Image Representation,High dynamic range imaging with short- and long-exposures based on artificial remapping using multiscale exposure fusion,article,HU2022103585
"Non-uniform motion deblurring has been a challenging problem in the field of computer vision. Currently, deep learning-based deblurring methods have made promising achievements. In this paper, we propose a new joint strong edge and multi-stream adaptive fusion network to achieve non-uniform motion deblurring. The edge map and the blurred map are jointly used as network inputs and Edge Extraction Network (EEN) guides the Deblurring Network (DN) for image recovery and to complement the important edge information. The Multi-stream Adaptive Fusion Module (MAFM) adaptively fuses the edge information and features from the encoder and decoder to reduce feature redundancy to avoid image artifacts. Furthermore, the Dense Attention Feature Extraction Module (DAFEM) is designed to focus on the severely blurred regions of blurry images to obtain important recovery information. In addition, an edge loss function is added to measure the difference of edge features between the generated and clear images to further recover the edges of the deblurred images. Experiments show that our method outperforms currently public methods in terms of PSNR, SSIM and VIF, and generates images with less blur and sharper edges.","Non-uniform motion deblurring, Attention mechanisms, Edge extraction algorithm, Generative adversarial network",Zihan Li and Guangmang Cui and Jufeng Zhao and Qinlei Xiang and Bintao He,https://www.sciencedirect.com/science/article/pii/S1047320322001833,https://doi.org/10.1016/j.jvcir.2022.103663,1047-3203,2022,103663,89,Journal of Visual Communication and Image Representation,Joint strong edge and multi-stream adaptive fusion network for non-uniform image deblurring,article,LI2022103663
"Understanding the underlying semantics of performing arts like dance is a challenging task. Analysis of dance is useful to preserve cultural heritage, make video recommendation systems, and build tutoring systems. To create such a dance analysis application, three aspects of dance analysis must be addressed: (1) segment the dance video to find representative action elements, (2) recognize the detected action elements, and (3) recognize sequences formed by combining action elements according to specific rules. This paper attempts to address the three fundamental problems of dance analysis raised above, with a focus on Indian Classical Dance, em Bharatanatyam. Since dance is driven by music, we use both musical and motion information to extract action elements. The action elements are then recognized using machine learning and deep learning techniques. Finally, the Hidden Markov Model (HMM) and Long Short-Term Memory (LSTM) are used to recognize the dance sequence.","Posture recognition, Sequence recognition, Dance segmentation, Multi-modal dance modeling, Machine learning, Bharatanatyam Dance analysis",Tanwi Mallick and Partha Pratim Das and Arun Kumar Majumdar,https://www.sciencedirect.com/science/article/pii/S1047320322000864,https://doi.org/10.1016/j.jvcir.2022.103548,1047-3203,2022,103548,87,Journal of Visual Communication and Image Representation,Posture and sequence recognition for Bharatanatyam dance performances using machine learning approaches,article,MALLICK2022103548
"Multiple JPEG compressions leave artifacts in digital images: residual traces that could be exploited in forensics investigations to recover information about the device employed for acquisition or image editing software. In this paper, a novel First Quantization Estimation (FQE) algorithm based on convolutional neural networks (CNNs) is proposed. In particular, a solution based on an ensemble of CNNs was developed in conjunction with specific regularization strategies exploiting assumptions about neighboring element values of the quantization matrix to be inferred. Mostly designed to work in the aligned case, the solution was tested in challenging scenarios involving different input patch sizes, quantization matrices (both standard and custom) and datasets (i.e., RAISE and UCID collections). Comparisons with state-of-the-art solutions confirmed the effectiveness of the presented solution demonstrating for the first time to cover the widest combinations of parameters of double JPEG compressions.","First quantization estimation, Multimedia forensics, JPEG, Image tampering",Sebastiano Battiato and Oliver Giudice and Francesco Guarnera and Giovanni Puglisi,https://www.sciencedirect.com/science/article/pii/S1047320322001559,https://doi.org/10.1016/j.jvcir.2022.103635,1047-3203,2022,103635,89,Journal of Visual Communication and Image Representation,CNN-based first quantization estimation of double compressed JPEG images,article,BATTIATO2022103635
"Owing to the large storage and fast machine recognition, QR codes have been widely utilized in many fields such as mobile payment, website navigation and user identity authentication. However, any QR code reader can access to the message contained in the QR code, the security becomes a major challenge to QR codes for privacy usage scenarios. Moreover, the management of QR codes for users are also inconvenient, since the human vision is hard to distinguish a QR code from the others. To solve the security and management problems, we propose the three-level QR codes for a group of participants. The first-level management information and the second-level public information are recognizable for the human vision and QR code reader device, respectively. The third-level privacy information is protected using visual cryptography scheme, and can be decoded using simple and non-cryptography computations. Furthermore, the shares can be stored or transferred in not only e-format but also print-format and photo-format, leading to the broad applicability. Experimental results and analysis demonstrate that the proposed scheme can encode three-level information into several distributed QR codes, and has more advantages compared with the previous schemes.","Three-level QR code, Visual cryptography scheme, Multiple formats",Zhengxin Fu and Liguo Fang and Hangying Huang and Bin Yu,https://www.sciencedirect.com/science/article/pii/S104732032200102X,https://doi.org/10.1016/j.jvcir.2022.103567,1047-3203,2022,103567,87,Journal of Visual Communication and Image Representation,Distributed three-level QR codes based on visual cryptography scheme,article,FU2022103567
"Copy-move tampering is one of the most popular tampering techniques at present. The tampered region of the image has good fusion with the original image, which increases the difficulty of detection. After years of research, the current detection method based on key points still has the following problems: 1) Failure to achieve forgery detection of small areas/self-similar areas/smooth areas, 2) Lack of reasonable feature point extraction methods, 3) The various stages of copy-move forgery detection (CMFD) work are relatively independent and lack close connections, 4) A fixed threshold is used as the region of interest similarity metric in the matching and localization stages. The failure to consider tampered images and the diversity of tampered regions leads to the limited detection capability of the algorithm. Considering the actual situation of tampering with the picture, to solve the above problems, we propose a copy-move forgery detection method based on the dynamic threshold. First, we determine the point extraction strategy in each super pixel block according to the size of the simple line interface calculation (SLIC) super pixel block and the Weber local descriptor (WLD) descriptor to ensure the reasonable allocation of feature points and reduce unnecessary points. These key points are then characterized by the scaling, flip and rotation invariants of the fractional general Jacobi-Fourier moments (FJFMs). Then, the matching and mismatch filtering thresholds of each feature point are determined through the WLD and SLIC features, and the SLIC feature is used to replace the distance threshold to improve the detection accuracy of small manufacturing areas. Finally, based on the matching results and SLIC features, an effective positioning method is proposed to improve the speed and accuracy of positioning. Experimental results show that the proposed algorithm is superior to the classic methods in recent years in terms of time and accuracy.","Copy-move tampering, FJFMs, SLIC, Dynamic threshold, WLD",Xiangyang Wang and Wencong Chen and Panpan Niu and Hongying Yang,https://www.sciencedirect.com/science/article/pii/S104732032200178X,https://doi.org/10.1016/j.jvcir.2022.103658,1047-3203,2022,103658,89,Journal of Visual Communication and Image Representation,Image copy-move forgery detection based on dynamic threshold with dense points,article,WANG2022103658
"High efficiency video coding (HEVC) video codec applies different techniques in order to achieve high compression ratios and video quality that supports real-time applications. One of the critical techniques in HEVC is the Context adaptive Binary Arithmetic Coding (CABAC) which is type of entropy coding. CABAC comes at the cost of increased computational complexity, especially for parallelization and pipeline of these blocks: binarization, context modeling and binary arithmetic encoding. The Binarization (BZ) and de-Binarization (DBZ) methods are considered as important techniques in HEVC CABAC encoder and decoder respectively. Indeed, an important goal is to get high throughput in hardware architectures of CABAC BZ and DBZ in order to achieve high resolution applications. This work is the only one found on recent literature which focuses on design and implementation of full BZ and full DBZ compatible with H.265 and H.264. Consequently, a hardware architectures of BZ and DBZ are designed and implemented by using VHDL language, targeted an FPGA virtex4 xc4vsx25-12ff668 board and emulated with ModelSim. As a result, the implementation of BZ and DBZ can process 2 bins/cycle for each syntax element when operated at 697.83Â MHz and 789.26Â MHz, respectively. The proposed designs exhibits an improved high-throughput of 1395.66 Mbins/s for BZ and 1578.52 Mbins/s for the DBZ. The obtained Area Efficiencies in our proposed BZ and DBZ are about 0.544 Mbins/s/slices and 0.606 Mbins/s/slices, respectively, and it is better than many recent works.","Binarization, CABAC, De-binarization, FPGA, HEVC, Matlab, VHDL",Wahiba Menasri and Manel Djabri and Sarah Chennoufi and Abdellah Skoudarli and Mounir Bouhedda and Omar Benzineb,https://www.sciencedirect.com/science/article/pii/S1047320322001936,https://doi.org/10.1016/j.jvcir.2022.103673,1047-3203,2022,103673,89,Journal of Visual Communication and Image Representation,Hardware implementation of HEVC CABAC binarization/de-binarization,article,MENASRI2022103673
"In this paper, a global convolutional network (GCN)-based fast coding unit (CU) partition method of intra-mode VVC is proposed. By using the GCN module with large kernel size convolutions, the proposed method can capture global information in CUs, leading to an accurate partition mode prediction in the quad-tree plus multi-type tree (QTMT) structure. Ranked according to predicted probabilities, the partition modes with lower probabilities are discarded, which reduces the computational complexity of VVC. Additionally, tradeoffs between performance and complexity can be achieved with different strategies. Experimental results demonstrated that the proposed method can reduce encoding time by 51.06%â¼61.15% while increasing BjÃ¸ntegaard delta bit-rate (BD-BR) by 0.84%â¼1.52% when implemented in VTM 10.0, outperforming the state-of-the-art methods, and that the proposed method can be used to accelerate VVenC 1.0 at the preset slower, achieving higher performance and lower complexity compared with the original VVenC 1.0 at the presets slow and medium.","Versatile Video Coding, Intra partition mode prediction, Complexity reduction, Global convolutional network",Saiping Zhang and Shixuan Feng and Jingwu Chen and Chunjie Zhou and Fuzheng Yang,https://www.sciencedirect.com/science/article/pii/S1047320322001419,https://doi.org/10.1016/j.jvcir.2022.103621,1047-3203,2022,103621,88,Journal of Visual Communication and Image Representation,A GCN-based fast CU partition method of intra-mode VVC,article,ZHANG2022103621
"Nowadays, various image editing tools are available that can be utilized for manipulating the original images; here copy-move forgery is most common forgery. In copy-move forgery, some part of the original image is copied and pasted into the same image at some other location. However, Artificial Intelligence (AI) based approaches can extract manipulated features easily. In this study, a deep learning-based method is proposed to classify the copy-move forged images. For classifying the forged images, a deep learning (DL) based hybrid model is presented named as VI-NET using fusion of two DL architectures, i.e., VGG16 and Inception V3. Further, output of two models is concatenated and connected with two additional convolutional layers. Cross-validation protocols, K10 (90Â % training, 10Â % testing), K5 (80Â % training, 20Â % testing), and K2 (50Â % training, 50Â % testing) are applied on the COMOFOD dataset. Moreover, the performance of VI-NET is compared with transfer learning and machine learning models using evaluation metrics such as accuracy, precision, recall, F1 score, etc. Proposed hybrid model performed better than other approaches with classification accuracy of 99Â Â±Â 0.2Â % in comparison to accuracy of 95Â Â±Â 4Â % (Inception V3), 93Â Â±Â 5Â % (MobileNet), 59Â Â±Â 8Â % (VGG16), 60Â Â±Â 1Â % (Decision tree), 87Â Â±Â 1Â % (KNN), 54Â Â±Â 1Â % (NaÃ¯ve Bayes) and 65Â Â±Â 1Â % (random forest) under K10 protocol. Similarly, results are evaluated based on K2 and K5 validation protocols. It is experimentally observed that the proposed model performance is better than existing standard and customized deep learning architectures.","Copy-move forgery, COMOFOD dataset, Convolution neural network, VGG16, Inception V3",Sanjeev Kumar and Suneet K. Gupta and Manjit Kaur and Umesh Gupta,https://www.sciencedirect.com/science/article/pii/S104732032200164X,https://doi.org/10.1016/j.jvcir.2022.103644,1047-3203,2022,103644,89,Journal of Visual Communication and Image Representation,VI-NET: A hybrid deep convolutional neural network using VGG and inception V3 model for copy-move forgery classification,article,KUMAR2022103644
"The tracker based on the Siamese network regards tracking tasks as solving a similarity problem between the target template and search area. Using shallow networks and offline training, these trackers perform well in simple scenarios. However, due to the lack of semantic information, they have difficulty meeting the accuracy requirements of the task when faced with complex backgrounds and other challenging scenarios. In response to this problem, we propose a new model, which uses the improved ResNet-22 network to extract deep features with more semantic information. Multilayer feature fusion is used to obtain a high-quality score map to reduce the influence of interference factors in the complex background on the tracker. In addition, we propose a more powerful Corner Distance IoU (intersection over union) loss function so that the algorithm can better regression to the bounding box. In the experiments, the tracker was extensively evaluated on the object tracking benchmark data sets, OTB2013 and OTB2015, and the visual object tracking data sets, VOT2016 and VOT2017, and achieved competitive performance, proving the effectiveness of this method.","Visual Tracking Siamese network, Multilayer feature fusion, Intersection over union (IoU) loss",Weisheng Li and Junye Zhu,https://www.sciencedirect.com/science/article/pii/S1047320322002073,https://doi.org/10.1016/j.jvcir.2022.103687,1047-3203,2022,103687,89,Journal of Visual Communication and Image Representation,Siamese visual tracking with multilayer feature fusion and corner distance IoU loss,article,LI2022103687
"Multiple description coding (MDC) approaches improve the error-resilient performance of video transmission by introducing redundancy. The existing multiple description video coding (MDVC) schemes are rarely designed for particular coding structure of HEVC, and the characteristics of the human visual system (HVS) are seldom considered. In this paper, a spatial-frequency multiple description video coding with adaptive perceptual redundancy allocation framework, named SF-PMDVC, is proposed for HEVC. For descriptions generation, after polyphase down-sampling in spatial domain, a transformation based on integer discrete cosine transform (DCT) is expended to adapt to the flexible coding unit partitioning process in HEVC, and the frequency coefficients are segmented and mapped to reduce the bitrate of each description. To further improve the performance of MDVC, an adaptive perceptual redundancy allocation strategy based on visual saliency is proposed, which improves the coding efficiency adapting to the visual perception. Experimental results show that the proposed scheme improves the error resiliency of HEVC by achieving superior objective and subjective reconstructed video quality as compared to the state-of-the-art MDVC methods for HEVC.","Multiple description coding, High efficiency video coding (HEVC), Human visual system, Error resilience",Feifeng Wang and Jing Chen and Huanqiang Zeng and Canhui Cai,https://www.sciencedirect.com/science/article/pii/S1047320322001389,https://doi.org/10.1016/j.jvcir.2022.103614,1047-3203,2022,103614,88,Journal of Visual Communication and Image Representation,Spatial-frequency HEVC multiple description video coding with adaptive perceptual redundancy allocation,article,WANG2022103614
"Moving object detection is frequently used as a springboard for advanced computer vision analysis in complex scenes. Nevertheless, due to unstable changes in the background, most existing background model hardly maintain superior performance. To this concern, we propose a novel pixel-level background model that has three innovations. First, we introduce K-means to directly model the spatiotemporal dependencies between pixels. These dependencies are exploited to discover static core information in the high-frequency changing spatial domain, resulting in excellent property in dynamic backgrounds. Besides, the notion of complementarity is taken as a feature selection criterion. In multi-feature model, the ability to supervise each other between features is important in the ambiguity challenges, e.g., shadow. Finally, feature models recommend each other in the update mechanism, and the diffusion rate of effective information in each feature model can be maximized by finding the best candidate feature. By virtue of this mechanism, model can be updated efficiently when large background migration occurs, e.g., PTZ. Experimental results on some standard benchmarks show that SIM-MFR can achieve promising performance compared to some state-of-the-art approaches.","Object detection, Dynamic backgrounds, K-means, Multi-feature, Complementary notion",Wei He and Jiexin Li and Qi Qi and Bing Tu and Xianfeng Ou and Longyuan Guo,https://www.sciencedirect.com/science/article/pii/S1047320322001420,https://doi.org/10.1016/j.jvcir.2022.103622,1047-3203,2022,103622,88,Journal of Visual Communication and Image Representation,SIM-MFR: Spatial interactions mechanisms based multi-feature representation for background modeling,article,HE2022103622
"Cover selection is one of the important techniques to improve the security of image steganography. However, existing methods mostly focus on cover selection of grayscale image. In this paper, we propose a novel Cover Selection Algorithm of color images based on Hybrid Local Texture Descriptor, named HLTD-CSA. A green-channel related Local Binary Pattern (LBP) is designed which utilizes local statistics of intra-channel and cross-channel correlations efficiently. Besides, Local Phase Quantization is introduced to serve as a complementary component of our improved LBP. To further enhance the performance of cover selection for color image, a hybrid local texture descriptor (HLTD) is obtained by combining these two types of local texture descriptors with a proper combination strategy. Finally, the proposed algorithm selects images which have larger values of HLTD to construct the cover image set. Extensive experiments are conducted to verify the effectiveness of our method.","Color image steganography, Cover selection, Hybrid local texture, Channel correlations",Menghua Chen and Peisong He and Jiayong Liu,https://www.sciencedirect.com/science/article/pii/S1047320322001663,https://doi.org/10.1016/j.jvcir.2022.103646,1047-3203,2022,103646,89,Journal of Visual Communication and Image Representation,HLTD-CSA: Cover selection algorithm based on hybrid local texture descriptor for color image steganography,article,CHEN2022103646
"We propose a general deep variational model (reduced version, full version as well as the extension) via a comprehensive fusion approach in this paper. It is able to realize various image tasks in a completely unsupervised way without learning from samples. Technically, it can properly incorporate the CNN based deep image prior (DIP) architecture into the classic variational image processing models. The minimization problem solving strategy is transformed from iteratively minimizing the sub-problem for each variable to automatically minimizing the loss function by learning the generator network parameters. The proposed deep variational (DV) model contributes to the high order image edition and applications such as image restoration, inpainting, decomposition and texture segmentation. Experiments conducted have demonstrated significant advantages of the proposed deep variational model in comparison with several powerful techniques including variational methods and deep learning approaches.","Unsupervised learning, Integration approach, Deep neural networks, Variational general frameworks, Diverse applications",Lu Tan and Ling Li and Wan-Quan Liu and Sen-Jian An and Kylie Munyard,https://www.sciencedirect.com/science/article/pii/S1047320322001146,https://doi.org/10.1016/j.jvcir.2022.103588,1047-3203,2022,103588,87,Journal of Visual Communication and Image Representation,Unsupervised learning of multi-task deep variational model,article,TAN2022103588
"We propose an efficient two-stage framework for stereo 3D object detection, called ETS-3D. Contrary to many recent approaches that rely on depth maps predicted using time-consuming stereo matching models, our approach utilizes the well-designed features to generate high-quality 3D proposals in stage-1, without explicitly exploiting predicted depth map. Specifically, we leverage pixel-wise correlation to produce normalized cost volumes to weight the left image features, and fuse multi-scale weighted features to obtain the weighted and fused features for 3D proposal generation. To maintain fast computation, only the filtered positive 3D proposals are fed into the stage-2 sub-network for further proposal refinement and quality prediction. Furthermore, we reconstruct the 3D proposal features in stage-2 to make use of different feature representations, achieving more accurate detection results. The experimental results on the KITTI 3D object detection benchmark demonstrate that our method achieves state-of-the-art performance, and can run at more than 10 fps.","3D Object Detection, Deep Learning, Stereo Matching, Autonomous Driving",Chaofeng Ji and Guizhong Liu and Dan Zhao,https://www.sciencedirect.com/science/article/pii/S1047320322001547,https://doi.org/10.1016/j.jvcir.2022.103634,1047-3203,2022,103634,88,Journal of Visual Communication and Image Representation,ETS-3D: An Efficient Two-Stage Framework for Stereo 3D Object Detection,article,JI2022103634
"We propose a multi-task learning framework for video anomaly detection based on a novel pipeline. Our model contains two crossing streams, one stream employs the backbone of Attention-R2U-net for future frame prediction, while the other is designed based on an encoderâdecoder network to reconstruct the input optical flow maps. In addition, the latent layers of the two streams are merged together and assigned with a Deep SVDD-based loss at each location individually. Through the combination of these three tasks, the two-stream-crossing pipeline can be trained end-to-end to provide a comprehensive evaluation for the anomaly targets. Experimental results on several popular benchmark datasets show that our model outperforms the state-of-the-art competing models, which can be applied to different types of anomalous targets and meanwhile achieves remarkable precision.","Anomaly detection, Multi-task learning, Deep SVDD, Future frame prediction, Local probability estimation",Xingya Chang and Yuxin Zhang and Dingyu Xue and Dongyue Chen,https://www.sciencedirect.com/science/article/pii/S1047320322000852,https://doi.org/10.1016/j.jvcir.2022.103547,1047-3203,2022,103547,87,Journal of Visual Communication and Image Representation,Multi-task learning for video anomaly detection,article,CHANG2022103547
"Sandstorm is a meteorological phenomenon common in arid and semi-arid regions. A sandstorm can carry large volumes of sand unexpectedly, which leads to severe color deviations and significantly degraded visibility when an image is taken in such a scenario. However, existing image enhancement methods cannot enhance sandstorm images well due to the challenging degradations and the scarcity of sandstorm training data. In this paper, we propose a Transformer with rotary position embedding to perform sandstorm image enhancement via building multi-scale and multi-patch dependencies. Our key insights in this work are 1) a multi-scale Transformer can globally eliminate the color deviations of sandstorm images via aggregating global information, 2) a multi-patch Transformer can recover local details well via learning the spatial variant degradations, and 3) a U-shape Transformer with rotary position embedding as the core unit of multi-scale and multi-patch Transformer can effectively build the long-range dependencies. We also contribute a real-world Sandstorm Image Enhancement (SIE) dataset including 1,400 sandstorm images with different degrees of degradations and various scenes. Experiments performed on synthetic images and real-world sandstorm images demonstrate that our proposed method not only obtains visually pleasing results but also outperforms state-of-the-art methods qualitatively and quantitatively.","Sandstorm image enhancement, Transformer, Two-stage network, Dataset",Pengwei Liang and Wenyu Ding and Lu Fan and Haoyu Wang and Zihong Li and Fan Yang and Bo Wang and Chongyi Li,https://www.sciencedirect.com/science/article/pii/S1047320322001821,https://doi.org/10.1016/j.jvcir.2022.103662,1047-3203,2022,103662,89,Journal of Visual Communication and Image Representation,Multi-scale and multi-patch transformer for sandstorm image enhancement,article,LIANG2022103662
"Fast image codecs are a current need in applications that deal with large amounts of images. Graphics Processing Units (GPUs) are suitable processors to speed up most kinds of algorithms, especially when they allow fine-grain parallelism. Bitplane Coding with Parallel Coefficient processing (BPC-PaCo) is a recently proposed algorithm for the core stage of wavelet-based image codecs tailored for the highly parallel architectures of GPUs. This algorithm provides complexity scalability to allow faster execution at the expense of coding efficiency. Its main drawback is that the speedup and loss in image quality is controlled only roughly, resulting in visible distortion at low and medium rates. This paper addresses this issue by integrating techniques of visually lossless coding into BPC-PaCo. The resulting method minimizes the visual distortion introduced in the compressed file, obtaining higher-quality images to a human observer. Experimental results also indicate 12% speedups with respect to BPC-PaCo.","High-throughput image coding, Visually lossless coding, JPEG2000",Francesc AulÃ­-LlinÃ s and Carlos {de Cea-Dominguez} and Miguel HernÃ¡ndez-Cabronero,https://www.sciencedirect.com/science/article/pii/S1047320322001924,https://doi.org/10.1016/j.jvcir.2022.103672,1047-3203,2022,103672,89,Journal of Visual Communication and Image Representation,Accelerating BPC-PaCo through Visually Lossless Techniques,article,AULILLINAS2022103672
"Real-time moving object detection is challenging for moving cameras due to the moving background. Many studies use homography matrix to compensate for global motion by warping the background model to the current frame. Then, the pixel difference between the current frame and the background model is used for background subtraction. Moving pixels are extracted by applying adaptive threshold and some post-processing techniques. On the other hand, deep learning-based dense optical flow can be efficient enough to extract the moving pixels, but it increases computational cost. This study proposes a method to enhance a classical background modeling method with deep learning-based dense optical flow. The main contribution of this paper is to propose a fusing algorithm for dense optical flow and background modeling approach. The background modeling methods are error-prone, especially with continuous camera movement, while the optical flow method alone may not always be efficient. Our hybrid method fuses both techniques to improve the detection accuracy. We propose a software architecture to run background modeling and dense optical flow methods in parallel processes. The proposed implementation approach significantly increases the methodâs working speed, while the proposed fusion and combining strategy improve detection results. The experimental results show that the proposed method can run at high speed and has satisfying performance against the methods in the literature.","Motion detection, Moving object detection, Dense optical flow, Moving camera",Ibrahim Delibasoglu and Irfan Kosesoy and Muhammed Kotan and Feyza Selamet,https://www.sciencedirect.com/science/article/pii/S1047320322001407,https://doi.org/10.1016/j.jvcir.2022.103616,1047-3203,2022,103616,88,Journal of Visual Communication and Image Representation,Motion detection in moving camera videos using background modeling and FlowNet,article,DELIBASOGLU2022103616
"Object detection on 360Â°images is a vital component of 3D environment perception. The existing methods either treat panoramic images (usually represented in equirectangular projectionâERP) as normal FoV images and endure the distortions or project them into the less-distortion format and narrow the FoV, leading to unsatisfactory performance in practical applications. To solve this problem, we propose a dual-projection 360Â°object detection network named Bip R-CNN, consisting of three modules: a bi-projection feature extractor, a cross-projection region-of-interest (RoI) searcher, and a classification and regression predictor. Specifically, we extract the equirectangular and corresponding dual-cubemap features simultaneously from the input images. Besides, Projection-Inter Feature Fusion and Projection-Intra Feature Fusion are designed to allow the mutual interaction between the bi-projective features and promote the integration of features at different scales, respectively. In the proposed cross-projection RoI Searcher, we search for the bounding box (BBox) locations on cubemap from the corresponding ERP spherical proposals, bridging the RoIs of two different projection formats at feature level. Finally, the cube proposals are used to detect objects in the last predictor module. Considering the scarceness of the existing panoramic dataset (only indoor scenes), we propose an efficient approach to convert conventional datasets into annotated panoramic datasets without manual intervention, increasing the diversity of panoramic datasets. Extensive experiments are conducted on the synthetic and real-world datasets with spherical criteria, demonstrating our superiority to other state-of-the-art solutions.","360Â°image, Object detection, Equirectangular, Cubemap",Zishuo Zheng and Chunyu Lin and Lang Nie and Kang Liao and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S1047320322001808,https://doi.org/10.1016/j.jvcir.2022.103660,1047-3203,2022,103660,89,Journal of Visual Communication and Image Representation,Bi-projection for 360Â°image object detection bridged by RoI Searcher,article,ZHENG2022103660
"Compared with the widely used supervised blind image quality assessment (BIQA) models, unsupervised BIQA models require little prior knowledge for calculating the objective quality scores of distorted images. In this paper, we propose an unsupervised BIQA method that aims to achieve both good performance and generalization capability with low computational complexity. Carefully selected and extensive structure and natural scene statistics (NSS) features can better represent image quality. First, we employ phase congruency (PC) and finely selected gradient magnitude map and Laplacian of Gaussian response (GM-LOG) features to represent image structure information. Second, we calculate the local mean-subtracted and contrast-normalized (MSCN) coefficients and the KarhunenâLoÃ©ve transform (KLT) coefficients to represent the naturalness of the distorted images. Last, multivariate Gaussian (MVG) model with joint features extracted from both the pristine images and the distorted images is adopted to calculate the objective image quality. Extensive experiments conducted on nine IQA databases demonstrate that the proposed method achieves better performance than the state-of-the-art BIQA methods.","Blind image quality assessment, Structure information, Natural scene statistics, KarhunenâLoÃ©ve transform",Qinglin He and Chao Yang and Fanxi Yang and Ping An,https://www.sciencedirect.com/science/article/pii/S1047320322001092,https://doi.org/10.1016/j.jvcir.2022.103579,1047-3203,2022,103579,87,Journal of Visual Communication and Image Representation,Unsupervised blind image quality assessment based on joint structure and natural scene statistics features,article,HE2022103579
"In this paper, we present the first video decomposition framework, named SyCoMo, that factorizes a video into style, content, and motion. Such a fine-grained decomposition enables flexible video editing, and for the first time allows for tripartite video synthesis. SyCoMo is a unified and domain-agnostic learning framework which can process videos of various object categories without domain-specific design or supervision. Different from other motion decomposition work, SyCoMo derives motion from style-free content by isolating style from content in the first place. Content is organized into subchannels, each of which corresponds to an atomic motion. This design naturally forms an information bottleneck which facilitates a clean decomposition. Experiments show that SyCoMo decomposes videos of various categories into interpretable content subchannels and meaningful motion patterns. Ablation studies also show that deriving motion from style-free content makes the keypoints or landmarks of the object more accurate. We demonstrate the photorealistic quality of the novel tripartite video synthesis in addition to three bipartite synthesis tasks named as style, content, and motion transfer.","Video decomposition, Video synthesis, Self-supervised learning",Yaosi Hu and Dacheng Yin and Yuwang Wang and Zhenzhong Chen and Chong Luo,https://www.sciencedirect.com/science/article/pii/S1047320322002061,https://doi.org/10.1016/j.jvcir.2022.103686,1047-3203,2022,103686,89,Journal of Visual Communication and Image Representation,"Decomposing style, content, and motion for videos",article,HU2022103686
"Zero-shot learning has received growing attention, which aims to improve generalization to unseen concepts. The key challenge in zero-shot tasks is to precisely model the relationship between seen and unseen classes. Most existing zero-shot learning methods capture inter-class relationships via a shared embedding space, leading to inadequate use of relationships and poor performance. Recently, knowledge graph-based methods have emerged as a new trend of zero-shot learning. These methods use a knowledge graph to accurately model the inter-class relationships. However, the currently dominant method for zero-shot learning directly extracts the fixed connection from off-the-shelf WordNet, which will inherit the inherent noise in WordNet. In this paper, we propose a novel method that adopts class-level semantic information as a guidance to construct a new semantic guided knowledge graph (SG-KG), which can correct the errors in the existing knowledge graph and accurately model the inter-class relationships. Specifically, our method includes two main steps: noise suppression and semantic enhancement. Noise suppression is used to eliminate noise edges in the knowledge graph, and semantic enhancement is used to connect two classes with strong relations. To promote high efficient information propagation among classes, we develop a novel multi-granularity fusion network (MGFN) that integrates discriminative information from multiple GCN branches. Extensive experiments on the large-scale ImageNet-21K dataset and AWA2 dataset demonstrate that our method consistently surpasses existing methods and achieves a new state-of-the-art result.","Large-scale zero-shot learning, Semantic guided knowledge graph, Multi-granularity fusion network",Jiwei Wei and Haotian Sun and Yang Yang and Xing Xu and Jingjing Li and Heng Tao Shen,https://www.sciencedirect.com/science/article/pii/S1047320322001493,https://doi.org/10.1016/j.jvcir.2022.103629,1047-3203,2022,103629,88,Journal of Visual Communication and Image Representation,Semantic guided knowledge graph for large-scale zero-shot learning,article,WEI2022103629
"Fractional-order calculus is an extension of integer order calculus.In signal processing, fractional-order calculus can non-linearly enhance the low-frequency signal and suppress the high-frequency signal. In this paper, a new fractional-order local minimum pixel prior (FOLMP) is proposed by combining fractional-order calculus with the local minimum pixel prior. The FOLMP of the sharp images includes fewer non-zero pixels than the blur images. A new blur kernel estimation algorithm is proposed by combining L0 regularized FOLMP with the maximum posterior probability. Furthermore, thekernel similarity is employed to adjust the iteration times to accelerate the computational efficiency. Comparative experiments show that the proposed algorithm can perform better on different types of datasets than the most advanced algorithms. In addition, non-overlapping image patches are adopted to compute the FOLMP, and the kernel similarity is used to suppress excessive iterations.Therefore, the proposed algorithm is several times or even tens of times more efficient than the classical prior-based methods.","Motion deblurring, Deconvolution, Kernel estimation, Fractional-Order calculus Theory, Local Minimal Pixel Prior",Jing Liu and Jieqing Tan and Xianyu Ge and Dandan Hu and Lei He,https://www.sciencedirect.com/science/article/pii/S1047320322001651,https://doi.org/10.1016/j.jvcir.2022.103645,1047-3203,2022,103645,89,Journal of Visual Communication and Image Representation,Blind deblurring with fractional-order calculus and local minimal pixel prior,article,LIU2022103645
"The underwater image enhancement techniques are essential for ocean research and engineering applications. In this paper, we propose a progressive multi-branch embedding fusion network (PMEFN) to improve image quality. Specifically, a multi-branch embedding fusion module (MEFM) is designed. The distorted images and its sharpened versions are used as the input, which are fused to learn the contextualized features based on a two-branch hybrid encoderâdecoder module (HEDM2) combined with the triple attention module to focus on the noise region. Afterwards, we use the multi-stage refining framework to decompose the image enhancement or marine snow removal tasks into multiple stages and progressively learn the nonlinear functions from the distorted inputs. Additionally, the outputs generated at each stage are further refined and enhanced based on a three-branch hybrid encoderâdecoder module (HEDM3). We perform experiments using real underwater datasets, including EUVP, UFO-120, UIEB, and synthetic dataset MSRB. The experimental results show that the proposed method has a superior performance as compared to other methods in terms of quantitative performance and visual quality. In addition, the effectiveness of each component is further validated by performing ablation experiments.","Underwater image enhancement, Marine snow removal, Multi-branch embedding fusion, Multi-stage framework",Kaichuan Sun and Fei Meng and Yubo Tian,https://www.sciencedirect.com/science/article/pii/S1047320322001134,https://doi.org/10.1016/j.jvcir.2022.103587,1047-3203,2022,103587,87,Journal of Visual Communication and Image Representation,Progressive multi-branch embedding fusion network for underwater image enhancement,article,SUN2022103587
"Speckle noise removal is a well-established problem in synthetic aperture radar (SAR) image processing. Among different methods focused on the reconstruction of SAR images, variational models have achieved state-of-the-art performance. In this paper, a Rayleigh based speckle reduction algorithm is developed using the variational framework. The forward model is combined with recently proposed regularization by denoising (RED) prior. However, RED has been proposed in literature for the additive noise model. Multiplicative noise in SAR images prevents the direct application of RED to variational models. Hence, logarithm transformation is applied to change the multiplicative noise model to additive model, and the forward model from Rayleigh to FisherâTippett distribution. The resulting optimization problem is solved using the alternating direction method of multipliers. Further, the proof of the convergence analysis is carried out for the above framework. Simulations convey that the proposed method has better despeckling performance compared to that of state-of-the-art methods.","Denoiser, Despeckling, PnP priors, Rayleigh noise, RED, Synthetic aperture radar",Satyakam Baraha and Ajit Kumar Sahoo,https://www.sciencedirect.com/science/article/pii/S1047320322000840,https://doi.org/10.1016/j.jvcir.2022.103546,1047-3203,2022,103546,86,Journal of Visual Communication and Image Representation,Restoration of speckle noise corrupted SAR images using regularization by denoising,article,BARAHA2022103546
"In the present era of machines and edge-cutting technologies, still document frauds persist. They are done intuitively by using almost identical inks, that it becomes challenging to detect themâthis demands an approach that efficiently investigates the document and leaves it intact. Hyperspectral imaging is one such a type of approach that captures the images from hundreds to thousands of spectral bands and analyzes the images through their spectral and spatial features, which is not possible by conventional imaging. Deep learning is an edge-cutting technology known for solving critical problems in various domains. Utilizing supervised learning imposes constraints on its usage in real scenarios, as the inks used in forgery are not known prior. Therefore, it is beneficial to use unsupervised learning. An unsupervised feature extraction through a Convolutional Autoencoder (CAE) followed by Logistic Regression (LR) for classification is proposed (CAE-LR). Feature extraction is evolved around spectral bands, spatial patches, and spectral-spatial patches. We inspected the impact of spectral, spatial, and spectral-spatial features by mixing inks in equal and unequal proportion using CAE-LR on the UWA writing ink hyperspectral images dataset for blue and black inks. Hyperspectral images are captured at multiple correlated spectral bands, resulting in information redundancy handled by restoring certain principal components. The proposed approach is compared with eight state-of-art approaches used by the researchers. The results depicted that by using the combination of spectral and spatial patches, the classification accuracy enhanced by 4.85% for black inks and 0.13% for blue inks compared to state-of-art results. In the present scenario, the primary area concern is to identify and detect the almost similar inks used in document forgery, are efficiently managed by the proposed approach.","Document forgery, Spectral, Spatial, Spectral-spatial, Autoencoders, Unsupervised Deep Learning",Garima Jaiswal and Arun Sharma and Sumit {Kumar Yadav},https://www.sciencedirect.com/science/article/pii/S1047320322002103,https://doi.org/10.1016/j.jvcir.2022.103690,1047-3203,2022,103690,89,Journal of Visual Communication and Image Representation,DFD-SS: Document Forgery Detection using Spectral â Spatial Features for Hyperspectral Images,article,JAISWAL2022103690
"In this work, we study the method exploiting natural language network to improve tracking performance. We propose a novel architecture which can combine class and visual information presented in tracking. To this end, we introduce a multimodal feature association network, allowing us to correlate the target class with its appearance during training and aid the localization of the target during inference. Specifically, we first utilize an appearance model to extract the target visual features, from which we obtain appearance cues, for instance shape and color. In order to employ target class information, we design a learned lightweight embedding network to embed the target class into a feature representation. The association network of our architecture contains a multimodal fusion module and a predictor module. The fusion module is used to combine features from class and appearance, yielding multimodal features with more expressive representations for the subsequent module. The predictor module is used to determine the target location in the current frame, from which we associate the class to the appearance. The class embedding module thus can learn appearance cues by exploiting the back-propagation functionality. To verify the abilities of our method, we select the official training and test splits of the LaSOT with annotated images and classes to perform experiments. In particular, we analyze the imbalance in the samples and employ a class validator discriminator to alleviate this problem. Extensive experimental results on LaSOT, UAV20L and UAV123@10fps demonstrate our method achieves competitive results while maintaining a considerable real-time speed.","Object tracking, Multimodal, Object class",Zhongjie Mao and Xi Chen and Jia Yan and Tao Qu,https://www.sciencedirect.com/science/article/pii/S1047320322001894,https://doi.org/10.1016/j.jvcir.2022.103669,1047-3203,2022,103669,89,Journal of Visual Communication and Image Representation,Multimodal object tracking by exploiting appearance and class information,article,MAO2022103669
"The drastic growth of research in image compression, especially deep learning-based image compression techniques, poses new challenges to objective image quality assessment (IQA). Typical artifacts encountered in the emerging image codecs are significantly different from that produced by traditional block-based codecs, leading to inapplicability of the existing objective IQA algorithms. Towards advancing the development of objective IQA algorithms for recent compression artifacts, we built a learning-based compressed image quality assessment (LCIQA) database involving traditional block-based image codecs, hybrid neural network based image codecs, convolutional neural network based and generative adversarial network (GAN) based end-to-end optimized image coding approaches. Our study confirms the statistical difference and human perception difference between reconstructions of learned compression and traditional block-based compression. We propose a two-step deep learning model for learning-based compressed image quality assessment. Extensive experiments on LCIQA database demonstrate that our proposed model performs better than other counterparts on learning-based compressed images, especially on GAN compressed images, and achieves competitive performance to the state-of-the-art IQA metrics on traditional compressed images.","Image quality assessment, Learning-based image compression, Generated image compression",Jiaqi Zhang and Zhigao Fang and Lu Yu,https://www.sciencedirect.com/science/article/pii/S1047320322001377,https://doi.org/10.1016/j.jvcir.2022.103617,1047-3203,2022,103617,88,Journal of Visual Communication and Image Representation,A no-reference perceptual image quality assessment database for learned image codecs,article,ZHANG2022103617
"Generalized zero-shot learning (GZSL) aims at training a model on seen data to recognize objects from both seen and unseen classes. Existing generated-based methods show encouraging performance by directly generating unseen samples. However, due to insufficient exploration of unseen label space and limited class-wise semantic descriptions, existing methods still face the bias problem. In this paper, we divide the bias problem into seen-biased and neighbor-biased problems and propose a GZSL method named Unbiased Feature Generating. For the seen-biased problem, we train a classifier in complete label space by introducing the discriminative information contained in fake unseen samples. For the neighbor-biased problem, we generate untypical samples and refine the classification boundaries among neighbor classes. The classifier in complete label space and generator are trained in an iterative process to complement each other. The experimental results on four widely used datasets verify our method achieves encouraging performance compared with the state-of-the-art methods.","Generalized zero-shot learning, Generative adversarial network, Image classification",Chang Niu and Junyuan Shang and Junchu Huang and Junmei Yang and Yuting Song and Zhiheng Zhou and Guoxu Zhou,https://www.sciencedirect.com/science/article/pii/S1047320322001778,https://doi.org/10.1016/j.jvcir.2022.103657,1047-3203,2022,103657,89,Journal of Visual Communication and Image Representation,Unbiased feature generating for generalized zero-shot learning,article,NIU2022103657
"Image steganalysis based on convolutional neural networks(CNN) has attracted great attention. However, existing networks lack attention to regional features with complex texture, which makes the ability of discrimination learning miss in network. In this paper, we described a new CNN designed to focus on useful features and improve detection accuracy for spatial-domain steganalysis. The proposed model consists of three modules: noise extraction module, noise analysis module and classification module. A channel attention mechanism is used in the noise extraction module and analysis module, which is realized by embedding the SE(Squeeze-and-Excitation) module into the residual block. Then, we use convolutional pooling instead of average pooling to aggregate features. The experimental results show that detection accuracy of the proposed model is significantly better than those of the existing models such as SRNet, Zhu-Net and GBRAS-Net. Compared with these models, our model has better generalization ability, which is critical for practical application.","Steganalysis, Convolutional neural network, Channel attention, Convolutional pooling",Tong Fu and Liquan Chen and Zhangjie Fu and Kunliang Yu and Yu Wang,https://www.sciencedirect.com/science/article/pii/S1047320322001535,https://doi.org/10.1016/j.jvcir.2022.103633,1047-3203,2022,103633,88,Journal of Visual Communication and Image Representation,CCNet: CNN model with channel attention and convolutional pooling mechanism for spatial image steganalysis,article,FU2022103633
"Recent generative adversarial networks (GANs) have yielded remarkable performance in face image synthesis. GAN inversion embeds an image into the latent space of a pretrained generator, enabling it to be used for real face manipulation. However, current inversion approaches for real faces suffer the dilemma of initialization collapse and identity loss. In this paper, we propose a hierarchical GAN inversion for real faces with identity preservation based on mutual information maximization. We first use a facial domain guaranteed initialization to avoid the initialization collapse. Furthermore, we prove that maximizing the mutual information between inverted faces and their identities is equivalent to minimizing the distance between identity features from inverted and original faces. Optimization for real face inversion with identity preservation is implemented on this mutual information-maximizing constraint. Extensive experimental results show that our approach outperforms state-of-the-art solutions for inverting and editing real faces, particularly in terms of face identity preservation.","Generative adversarial network, GAN inversion, Mutual information maximizing, Face identity preservation, Face editing",Chengde Lin and Shengwu Xiong and Yaxiong Chen,https://www.sciencedirect.com/science/article/pii/S1047320322001018,https://doi.org/10.1016/j.jvcir.2022.103566,1047-3203,2022,103566,87,Journal of Visual Communication and Image Representation,Mutual information maximizing GAN inversion for real face with identity preservation,article,LIN2022103566
"Knowledge distillation has become a key technique for making smart and light-weight networks through model compression and transfer learning. Unlike previous methods that applied knowledge distillation to the classification task, we propose to exploit the decomposition-and-replacement based distillation scheme for depth estimation from a single RGB color image. To do this, Laplacian pyramid-based knowledge distillation is firstly presented in this paper. The key idea of the proposed method is to transfer the rich knowledge of the scene depth, which is well encoded through the teacher network, to the student network in a structured way by decomposing it into the global context and local details. This is fairly desirable for the student network to restore the depth layout more accurately with limited resources. Moreover, we also propose a new guidance concept for knowledge distillation, so-called ReplaceBlock, which replaces blocks randomly selected in the decoded feature of the student network with those of the teacher network. Our ReplaceBlock gives a smoothing effect in learning the feature distribution of the teacher network by considering the spatial contiguity in the feature space. This process is also helpful to clearly restore the depth layout without the significant computational cost. Based on various experimental results on benchmark datasets, the effectiveness of our distillation scheme for monocular depth estimation is demonstrated in details. The code and model are publicly available at : https://github.com/tjqansthd/Lap_Rep_KD_Depth.","Monocular depth estimation, Knowledge distillation, Laplacian pyramid, ReplaceBlock",Minsoo Song and Wonjun Kim,https://www.sciencedirect.com/science/article/pii/S1047320322000669,https://doi.org/10.1016/j.jvcir.2022.103523,1047-3203,2022,103523,85,Journal of Visual Communication and Image Representation,Decomposition and replacement: Spatial knowledge distillation for monocular depth estimation,article,SONG2022103523
"Vehicle re-identificationÂ (V-ReID) aims at discovering an image of a specific vehicle from a set of images typically captured by different cameras. Vehicles are one of the most important objects in cross-camera target recognition systems, and recognizing them is one of the most difficult tasks due to the subtle differences in the visible characteristics of vehicle rigid objects. Compared to various methods that can improve re-identification accuracy, data augmentation is a more straightforward and effective technique. In this paper, we propose a novel data synthesis method for V-ReID based on local-region perspective transformation, transformation state adversarial learning and a candidate pool. Specifically, we first propose a parameter generator network, which is a lightweight convolutional neural network, to generate the transformation states. Secondly, an adversarial module is designed in our work, it ensures that noise information is added as much as possible while keeping the labeling and structure of the dataset intact. With this adversarial module, we are able to promote the performance of the network and generate more proper and harder training samples. Furthermore, we use a candidate pool to store harder samples for further selection to improve the performance of the model. Our system pays more balanced attention to the features of vehicles. Extensive experiments show that our method significantly boosts the performance of V-ReID on the VeRi-776, VehicleID and VERI-Wild datasets.","Vehicle re-identification, Data synthesis, Local-region perspective transformation, Transformation state adversarial, Candidate pool, Parameter generator network",Yanbing Chen and Wei Ke and Hong Lin and Chan-Tong Lam and Kai Lv and Hao Sheng and Zhang Xiong,https://www.sciencedirect.com/science/article/pii/S1047320321002911,https://doi.org/10.1016/j.jvcir.2021.103432,1047-3203,2022,103432,83,Journal of Visual Communication and Image Representation,Local perspective based synthesis for vehicle re-identification: A transformation state adversarial method,article,CHEN2022103432
"The face is the window to the soul. This is what the 19th-century French doctor Duchenne de Boulogne thought. Using electric shocks to stimulate muscular contractions and induce bizarre-looking expressions, he wanted to understand how muscles produce facial expressions and reveal the most hidden human emotions. Two centuries later, this research field remains very active. We see automatic systems for recognizing emotion and facial expression being applied in medicine, security and surveillance systems, advertising and marketing, among others. However, there are still fundamental questions that scientists are trying to answer when analyzing a personâs emotional state from their facial expressions. Is it possible to reliably infer someoneâs internal state based only on their facial musclesâ movements? Is there a universal facial setting to express basic emotions such as anger, disgust, fear, happiness, sadness, and surprise? In this research, we seek to address some of these questions through convolutional neural networks. Unlike most studies in the prior art, we are particularly interested in examining whether characteristics learned from one group of people can be generalized to predict anotherâs emotions successfully. In this sense, we adopt a cross-dataset evaluation protocol to assess the performance of the proposed methods. Our baseline is a custom-tailored model initially used in face recognition to categorize emotion. By applying data visualization techniques, we improve our baseline model, deriving two other methods. The first method aims to direct the networkâs attention to regions of the face considered important in the literature but ignored by the baseline model, using patches to hide random parts of the facial image so that the network can learn discriminative characteristics in different regions. The second method explores a loss function that generates data representations in high-dimensional spaces so that examples of the same emotion class are close and examples of different classes are distant. Finally, we investigate the complementarity between these two methods, proposing a late-fusion technique that combines their outputs through the multiplication of probabilities. We compare our results to an extensive list of works evaluated in the same adopted datasets. In all of them, when compared to works that followed an intra-dataset protocol, our methods present competitive numbers. Under a cross-dataset protocol, we achieve state-of-the-art results, outperforming even commercial off-the-shelf solutions from well-known tech companies.","Emotion recognition, Facial analysis, Cross-dataset evaluation, Deep learning",William Dias and Fernanda AndalÃ³ and Rafael Padilha and Gabriel Bertocco and Waldir Almeida and Paula Costa and Anderson Rocha,https://www.sciencedirect.com/science/article/pii/S1047320321002637,https://doi.org/10.1016/j.jvcir.2021.103395,1047-3203,2022,103395,82,Journal of Visual Communication and Image Representation,Cross-dataset emotion recognition from facial expressions through convolutional neural networks,article,DIAS2022103395
"Face completion is a domain-specific image inpainting problem. Most existing face completion methods fail to synthesize fine-grained facial structures due to the undifferentiated treatment of face images and other scene images. To handle this problem, we propose an end-to-end deep generative model based approach which makes full use of the facial prior knowledge, including 2D facial geometry priors from facial parsing maps and landmarks, as well as the 3D depth prior. We adopt a coarse-to-fine inpainting framework where the 2D facial geometry priors based on coarse faces are extracted to guide the refinement network for better planar facial textures and structures. Moreover, a novel 3D regularized reconstruction loss is proposed for the enhancement of the stereo perception of generated faces. Experimental results on two large-scale benchmarks CelebA and CelebA-HQ show that our method significantly outperforms the state-of-the-arts in generating more visually realistic and pleasing faces. Code is available at .","Face completion, 2D geometry priors, 3D depth prior, Generative model",Jing Liu and Weikang Wang and Jiexiao Yu and Chunping Zhang and Yuting Su,https://www.sciencedirect.com/science/article/pii/S1047320321002510,https://doi.org/10.1016/j.jvcir.2021.103380,1047-3203,2022,103380,82,Journal of Visual Communication and Image Representation,3DFP-FCGAN: Face completion generative adversarial network with 3D facial prior,article,LIU2022103380
"Point cloud registration is mainly to estimate a rigid transformation between point clouds. The traditional optimization-based registration method requires a good initial position, and it is easy to fall into a local optimal solution. Some learning-based methods are introduced to reduce the dependence on the initial transformation, but they cannot handle partial-to-partial registration tasks. This paper proposes a learning-based registration method for partial-to-partial scenario. The local geometry is encoded into the feature representation of each point. A transformer network is used to enhance attention features. A designed sampling network down-sample key matching points and their corresponding features. The rigid transformation is calculated according to virtual correspondence by a singular value decomposition layer. The ModelNet40 dataset and Stanford 3D Scanning models are used to test the registration performance. Experimental results show that the proposed method achieves better registration accuracy than traditional methods, and it is robust to any initial transformation and noise.","Point cloud registration, Partial correspondence, Sampling network, Deep learning",Yanan Song and Weiming Shen and Peng Lu,https://www.sciencedirect.com/science/article/pii/S1047320321002753,https://doi.org/10.1016/j.jvcir.2021.103411,1047-3203,2022,103411,82,Journal of Visual Communication and Image Representation,A novel partial-to-partial registration method based on sampling network,article,SONG2022103411
"This paper describes an ultra high definition (UHD) video dataset named DVL2021 for the perceptual study of video quality assessment (VQA). To our knowledge, DVL2021 is the first authentically distorted 4K (3840Â ÃÂ 2160) UHD video quality dataset. The dataset contains 206 versatile 4K UHD video sequences, which are all collected in in-the-wild scenarios. Each sequence is captured at 50 frames per second (fps), stored in raw 10-bit 4:2:0 YUV format, and has a duration of 10Â s. Following the subjective evaluation method of TV image quality granted by ITU-R BT.500-13, 32 unique participants take part in the manual annotation process, whose ages are from teenage to sixties (32.7 years old on average). DVL2021 has the following merits: (1) enormous variety of video contents, (2) captured by different types of cameras, (3) complex types and multiple levels of authentic distortion, (4) broadly distributed temporal/spatial information, and (5) a wide spectrum of mean opinion scores (MOS) distribution. Furthermore, we conduct a benchmark experiment by evaluating several mainstream VQA methods on DVL2021. The baseline results are higher than 0.75 in Spearmanâs rank order correlation coefficient (SROCC) metric. Our study provides a basis for the UHD VQA problem. DVL2021 is publicly available at https://github.com/GZHU-DVL/DVL2021.","UHD video dataset, Video quality assessment, Authentic distortion, Synthetic distortion",Fengchuang Xing and Yuan-Gen Wang and Hanpin Wang and Jiefeng He and Jinchun Yuan,https://www.sciencedirect.com/science/article/pii/S1047320321002479,https://doi.org/10.1016/j.jvcir.2021.103374,1047-3203,2022,103374,82,Journal of Visual Communication and Image Representation,DVL2021: An ultra high definition video dataset for perceptual quality study,article,XING2022103374
"Preventive conservation is the constant monitoring of the state of conservation of an artwork to reduce the risk of damages and so to minimize the necessity of restorations. Many methods have been proposed during time, generally including a mix of different analytical techniques. In this work, we present a probabilistic approach based on the a-contrario framework for the detection of alterations on varnished surfaces, in particular those of historical musical instruments. Our method is a one step Number of False Alarms (NFA) clustering solution which considers simultaneously gray-level and spatial density information in a single background model. The proposed approach is robust to noise and avoids parameter tuning as well as any assumption about the shape and size of the worn-out areas. Tests have been conducted on UV induced fluorescence (UVIFL) image sequences included in the âViolins UVIFL imageryâ dataset. UVIFL photography is a well known diagnostic technique used to see details of a surface not perceivable with visible light. The obtained results prove the capability of the algorithm to properly detect the altered regions. Comparisons with other the state-of-the-art clustering methods show improvement in both precision and recall.","A-contrario framework, Defect detection, Preventive conservation, Historical violins",Alireza Rezaei and Sylvie {Le HÃ©garat-Mascle} and Emanuel Aldea and Piercarlo Dondi and Marco Malagodi,https://www.sciencedirect.com/science/article/pii/S1047320321002352,https://doi.org/10.1016/j.jvcir.2021.103357,1047-3203,2022,103357,83,Journal of Visual Communication and Image Representation,A-contrario framework for detection of alterations in varnished surfaces,article,REZAEI2022103357
"The images captured by the cameras contain distortions, misclassified pixels, uncertainties and poor contrast. Therefore, the multi-focus image fusion (MFIF) integrates various input image features to produce a single fused image using all its objects in focus. However, it is computationally complex, which leads to inconsistency. Hence, the MFIF method is employed to generate the fused image by integrating the fuzzy sets (FS) and convolutional neural network (CNN) to detect focused and unfocused parts in both source images. It is also compared with other competing six MFIF methods like Neutrosophic set based stationary wavelet transform (NSWT), guided filters, CNN, ensemble CNN, image fusion-based CNN and deep regression pair learning (DRPL). Benchmark datasets validate the superiority of the proposed FCNN method in terms of four non-reference assessment measures having mutual information (1.1678), edge information (0.7281), structural similarity (0.9850) and human perception (0.8020) and two reference metrics such as Peak signal-to-noise ratio (57.23) and root mean square error (1.814).","Deep learning, Fusion, Fuzzy sets, Multi-focus images",Kanika Bhalla and Deepika Koundal and Bhisham Sharma and Yu-Chen Hu and Atef Zaguia,https://www.sciencedirect.com/science/article/pii/S1047320322000396,https://doi.org/10.1016/j.jvcir.2022.103485,1047-3203,2022,103485,84,Journal of Visual Communication and Image Representation,A fuzzy convolutional neural network for enhancing multi-focus image fusion,article,BHALLA2022103485
"In recent years, artificial intelligence has been widely used in such fields as agricultural informatization, precision agriculture and precision animal husbandry. Due to limited research on deep learning in real-time agricultural and pastoral situations, deep learning and computer vision have become very important topics in the agricultural field. Recent studies have shown that the fusion of features under different attention mechanisms will help advance the utilization of such features, and will thus influence the accuracy and generalization ability of the models used. In this paper, we propose a lightweight network structure based on feature fusion under a dual attention mechanism with the same activation and joint loss functions. More specifically, we propose an innovative method to improve the network structure of two different attention mechanisms, and achieve feature fusion by combining the two. At the same time, we keep the activation functions consistent with those of the original network structure, and we develop a joint loss function to expand the use of various features. We also take the novel approach of applying the trajectory behavior analysis method to walking and standing. Experiments using both a publicly available data set and a data set obtained from a farm show that our algorithm achieves state-of-the-art performance in terms of accuracy and generalization ability, as compared to other methods.","Dual attention mechanism, Feature fusion, Behavior recognition, Smart animal husbandry",Cheng Shang and Feng Wu and MeiLi Wang and Qiang Gao,https://www.sciencedirect.com/science/article/pii/S1047320322000670,https://doi.org/10.1016/j.jvcir.2022.103524,1047-3203,2022,103524,85,Journal of Visual Communication and Image Representation,Cattle behavior recognition based on feature fusion under a dual attention mechanism,article,SHANG2022103524
"Dynamic gestures have attracted much attention in recent years due to their user-friendly interactive characteristics. However, accurate and efficient dynamic gesture understanding remains a challenge due to complex scenarios and motion information. Conventional handcrafted features are computationally cheap but can only extract low-level image features. This leads to performance degradation when dealing with complex scenes. In contrast, deep learning-based methods have a stronger feature expression ability and hence can capture more abstract and high-level image features. However, they critically rely on a large amount of training data. To address the above issues, a novel dynamic gesture understanding algorithm based on feature fusion is proposed for accurate dynamic gesture prediction. It leverages the advantages of handcrafted features and transfer learning. Aimed at small-scale dynamic gesture data, transfer learning is introduced for capturing effective feature expression. To precisely model the critical temporal information associated with dynamic gestures, a novel feature descriptor, namely, AlexNet2, is proposed for effective feature expression of dynamic gestures from the spatial and temporal domain. On this basis, a decision-level feature fusion framework based on support vector machine (SVM) and DempsterâShafer (DS) evidence theory is constructed to utilize handcrafted features and AlexNet2 to realize high-precision dynamic gesture understanding. To verify the effectiveness and robustness of the proposed recognition algorithm, analysis and comparison experiments are performed on the public Cambridge gesture dataset and Northwestern University hand gesture dataset. The proposed gesture recognition algorithm achieves prediction accuracies of 99.50% and 96.97% on these two datasets. Experimental results show that the proposed recognition framework exhibits a better recognition performance in comparison with related prediction algorithms.","Dynamic gesture understanding, Transfer learning, Feature fusion, DempsterâShafer evidence theory, Support vector machine",Yanhong Liu and Shouan Song and Lei Yang and Guibin Bian and Hongnian Yu,https://www.sciencedirect.com/science/article/pii/S1047320322000153,https://doi.org/10.1016/j.jvcir.2022.103454,1047-3203,2022,103454,83,Journal of Visual Communication and Image Representation,A novel dynamic gesture understanding algorithm fusing convolutional neural networks with hand-crafted features,article,LIU2022103454
"Object framework detection has been extensively studied in computer vision for applications such as document digitization and whiteboard scanning. Similarly, it is essential for display-camera communication systems, particularly when imperceptible data modulation is employed to enable simultaneous video playback and data transmission. Reliable and accurate localization of the encoded display area is critical for data demodulation and decoding. However, existing systems typically adapt established methods developed for other applications that do not meet the system requirements for high-rate data transmission. In this article, we propose a novel method for display area detection in the camera images by embedding a new localization marker into the display corners. While the localization marker is less obtrusive than conventional fiducial markers, our detection algorithm demonstrated excellent reliability regardless of the display content and background, according to simulation and experimental results. In addition, the detector achieved subpixel accuracy and real-time performance with modern smartphones.","Display area detection, Display camera communication, Rectangle detection, Pattern recognition, Visible light communication",Jianshuang Xu and Johannes Klein and JÃ¶rn Jochims and Niklas Weissner and RÃ¼diger Kays,https://www.sciencedirect.com/science/article/pii/S1047320322000554,https://doi.org/10.1016/j.jvcir.2022.103510,1047-3203,2022,103510,85,Journal of Visual Communication and Image Representation,A reliable and unobtrusive approach to display area detection for imperceptible display camera communication,article,XU2022103510
"Moving object detection is one of the essential tasks for surveillance video analysis. The dynamic background often composed by waving trees, rippling water or fountains, etc. in nature scene greatly interferes with the detection of moving objects in the form of noise. In this paper, a method simulating heat conduction is proposed to extract moving objects from dynamic background video sequences. Based on the visual background extractor (ViBe) with an adaptable distance threshold, we design a temperature field relying on the generated mask image to distinguish between the moving objects and the noise caused by dynamic background. In temperature field, a brighter pixel is associated with more energy. It will transfer a certain amount of energy to its neighboring darker pixels. Through multiple steps of energy transfer the noise regions loss more energy so that they become darker than the detected moving objects. After heat conduction, K-Means algorithm with the customized initial clustering centers is utilized to separate the moving objects from background. We test our method on many videos with dynamic background from public datasets. The results show that the proposed method is feasible and effective for moving object detection from dynamic background sequences.","Moving object detection, Dynamic background, Heat conduction, Entropy, K-means clustering",Yuan Dai and Long Yang,https://www.sciencedirect.com/science/article/pii/S1047320322000037,https://doi.org/10.1016/j.jvcir.2022.103439,1047-3203,2022,103439,83,Journal of Visual Communication and Image Representation,Detecting moving object from dynamic background video sequences via simulating heat conduction,article,DAI2022103439
"With the widespread use of WeChat Mini Programs, their security has attracted more and more attention. The WeChat Mini Program can be accessed by scanning a WeChat Mini Program code. We should protect the code thus to protect the Mini Program. In this paper, based on studying the function in each pattern of WeChat Mini Program codes, a visual secret sharing (VSS) scheme for WeChat Mini Program codes (MPCVSS) with (n,n)(nâ¥4) threshold is proposed to control and identify the users of Mini Program. MPCVSS combines the error-correcting characteristic of WeChat Mini Program codes with the theory of VSS. A secret WeChat Mini Program code is shared into n shared codes slightly modified from n cover codes. Each shared code is a valid code that can be scanned and decoded correctly. The secret code can be recovered by XORing n shares. The recovered code can be decoded as the same secret Mini Program. Theoretical analysis and experiments show that the proposed VSS scheme is practical and feasible.","Visual secret sharing, WeChat Mini Program codes, Error correction, Access control",Jia Chen and Yongjie Wang and Xuehu Yan and Jiayu Wang and Longlong Li,https://www.sciencedirect.com/science/article/pii/S104732032100273X,https://doi.org/10.1016/j.jvcir.2021.103409,1047-3203,2022,103409,82,Journal of Visual Communication and Image Representation,"Visual secret sharing scheme with (n,n) threshold based on WeChat Mini Program codes",article,CHEN2022103409
"HumanâObject Interaction (HOI) detection is a crucial problem for comprehensive visual understanding, which aims to detect <human,action,object> triplets within an image. Many existing methods often exploit to integrate the human and object visual features , the spatial layout of humanâobject pairs, human poses, contextual information, and even object semantic information into a framework to infer the interactions, proving that all these components can contribute to improve the HOI detection. However, most methods simply concatenate these components that are not explicitly embedded in the feature learning for HOI detection. In this paper, we are trying to fuse these components explicitly using a multi-stream feature refinement network. The network extracts the visual features of humans, contexts, and objects, which receives the attentions from human poses, spatial configurations, and semantic prior knowledge of objects to refine these visual features, respectively. In addition, an additional graph neural network is employed here to learn the structural features of humanâobject pairs. We verify our method on V-COCO and HICO-DET datasets with extensive experiments. The experimental results demonstrate that our method is a simple yet effective for HOI detection, achieving superior performance to those state-of-the-art methods.","Human object interaction, Action recognition, Feature refinement and learning, Multi-stream neural network, Human pose, Object semantic information",Zhanpeng Shao and Zhongyan Hu and Jianyu Yang and Youfu Li,https://www.sciencedirect.com/science/article/pii/S1047320322000712,https://doi.org/10.1016/j.jvcir.2022.103529,1047-3203,2022,103529,86,Journal of Visual Communication and Image Representation,Multi-stream feature refinement network for human object interaction detection,article,SHAO2022103529
"Recently neural style transfer has achieved great development, but there is still a big gap compared with manual creation. Most of the existing methods ignore the comprehensive consideration of preserving various semantic information of original content images, resulting in distortion or loss of original content features of the generated works, which are dull and difficult to convey the original themes and emotions. In this paper, we analyze the ability of the existing methods to maintain single semantic information and propose a fast style transfer framework with multi-semantic preservation. The experiments indicate that our method can effectively retain the original semantic information including salience and depth features, so that the final artwork has better visual effect by highlighting its regional focus and depth information. Compared with existing methods, our method has better ability in semantic preservation and can generate more artworks with distinct regions, controllable semantics, diverse contents and rich emotions.","Neural style transfer, Multiple semantic preservation, Comprehensive framework, Stylized artistic quality",Wujian Ye and Xueke Zhu and Zuoteng Xu and Yijun Liu and Chin-Chen Chang,https://www.sciencedirect.com/science/article/pii/S1047320321002509,https://doi.org/10.1016/j.jvcir.2021.103378,1047-3203,2022,103378,82,Journal of Visual Communication and Image Representation,A comprehensive framework of multiple semantics preservation in neural style transfer,article,YE2022103378
"Scientific efforts have expanded in age-invariant face recognition (AIFR). Matching faces of large age difference is, therefore, a problem, mostly because of a substantial disparity in the appearance of both young and old age. Owing to age, both the appearance and shape of the face are impaired, making recognition of the face the most challenging task. In recent years, AIFR has become a very common and demanding task. The set of feature extraction and classification algorithm is of prime importance in this field. As the numbers of features obtained from the datasets are large, there is a need to introduce a dimensionality reduction method to map high dimensionality feature space to low variance filter to form the final integrated face age model to be used in the classification process. In this paper, we introduced a novel concept of an improved Active Shape Model (ASM) in conjunction with a specially designed 7-layered Convolutional Neural Network (CNN) in order to accomplish a combination of feature extraction and classification in a single unit. The study approach involves conducting extensive experiments to evaluate the proposed system's performance using three standard datasets: FG-NET, LAG, and CACD. The results reveal that the proposed method outperforms state-of-the-art approaches and achieves excellent accuracy in face recognition across age. The maximum accuracies achieved by demonstrated ASM-CNN methodology for FG-NET, LAG, and CACD databases are 95.02%, 91.76Â % and 99.4Â % respectively.","Age invariance face recognition, CNN architecture, Improved Active shape model, Handcrafted and deep features, Principal component analysis, The features reduction",Ashutosh Dhamija and R.B. Dubey,https://www.sciencedirect.com/science/article/pii/S1047320321002625,https://doi.org/10.1016/j.jvcir.2021.103393,1047-3203,2022,103393,82,Journal of Visual Communication and Image Representation,A novel active shape model-based DeepNeural network for age invariance face recognition,article,DHAMIJA2022103393
"Recently, deep learning-based methods have reached an excellent performance on License Plate (LP) detection and recognition tasks. However, it is still challenging to build a robust model for Chinese LPs since there are not enough large and representative datasets. In this work, we propose a new dataset named Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP images as a supplement to the existing public benchmarks. The images are mainly captured with electronic monitoring systems with detailed annotations. To our knowledge, CRPD is the largest public multi-objective Chinese LP dataset with annotations of vertices. With CRPD, a unified detection and recognition network with high efficiency is presented as the baseline. The network is end-to-end trainable with totally real-time inference efficiency (30 fps with 640 p). The experiments on several public benchmarks demonstrate that our method has reached competitive performance. The code and dataset will be publicly available at https://github.com/yxgong0/CRPD.","Chinese license plate dataset, License plate detection and recognition, End-to-end, Real-time",Yanxiang Gong and Linjie Deng and Shuai Tao and Xinchen Lu and Peicheng Wu and Zhiwei Xie and Zheng Ma and Mei Xie,https://www.sciencedirect.com/science/article/pii/S1047320322000815,https://doi.org/10.1016/j.jvcir.2022.103541,1047-3203,2022,103541,86,Journal of Visual Communication and Image Representation,Unified Chinese License Plate detection and recognition with high efficiency,article,GONG2022103541
"To encrypt sensitive information existing in a color DICOM images, a medical privacy protection scheme (called as MPPS) based on chaos and DNA coding was proposed by using two coupled chaotic systems to produce cryptographic primitives. Relying on some empirical analyzes and experimental results, the designers of MPPS claimed that it can withstand a chosen-plaintext attack and some other classic attacking models. However, this statement is groundless. In this paper, we investigate the essential properties of MPPS and DNA coding, and we then propose an efficient chosen-plaintext attack to disclose its equivalent secret-key. The attack only needs âlog256(3â
Mâ
N)â+4 pair of chosen plain-images and the corresponding cipher-images, where MÃN and â3â are the size of the RGB color image and the number of color channels, respectively. In addition, the other claimed superiorities are questioned from the perspective of modern cryptography. Both theoretical and experimental results are presented to support the efficiency of the proposed attack and the other reported security faults. The proposed cryptanalysis results will promote the proper application of DNA encoding to protect multimedia privacy data, especially that in a DICOM image.","Cryptanalysis, Chaotic cryptography, Chosen-plaintext attack, DNA coding, DICOM image security, Privacy protection",Lei Chen and Chengqing Li and Chao Li,https://www.sciencedirect.com/science/article/pii/S1047320321002868,https://doi.org/10.1016/j.jvcir.2021.103424,1047-3203,2022,103424,83,Journal of Visual Communication and Image Representation,Security measurement of a medical communication scheme based on chaos and DNA coding,article,CHEN2022103424
"Motivated by the powerful capability of deep neural networks in feature learning, a new graph-based neural network is proposed to learn local and global relational information on skeleton sequences represented as spatio-temporal graphs (STGs). The pipeline of our network architecture consists of three main stages. As the first stage, spatialâtemporal sub-graphs (sub-STGs) are projected into a latent space in which every point is represented as a linear subspace. The second stage is based on message passing to acquire the localized correlated features of the nodes in the latent space. The third stage relies on graph convolutional networks (GCNs) to reason the long-range spatio-temporal dependencies through a graph representation of the latent space. Finally, the average pooling layer and the softmax classifier are then employed to predict the action categories based on the extracted local and global correlations. We validate our model in terms of action recognition using three challenging datasets: the NTU RGB+D, Kinetics Motion, and SBU Kinect Interaction datasets. The experimental results demonstrate the effectiveness of our approach and show that our proposed model outperforms the state-of-the-art methods.","Deep learning, Graph neural networks, Graph convolutional network, Message passing, Spatio-temporal graph, Grassmannian geometry",Wenwen Ding and GuangHui Zhou and ChongYang Ding and Guang Li and Kai Liu,https://www.sciencedirect.com/science/article/pii/S1047320321002741,https://doi.org/10.1016/j.jvcir.2021.103410,1047-3203,2022,103410,83,Journal of Visual Communication and Image Representation,Graph-based relational reasoning in a latent space for skeleton-based action recognition,article,DING2022103410
"Lane detection is an important task of road environment perception for autonomous driving. Deep learning methods based on semantic segmentation have been successfully applied to lane detection, but they require considerable computational cost for high complexity. The lane detection is treated as a particular semantic segmentation task due to the prior structural information of lane markings which have long continuous shape. Most traditional CNN are designed for the representation learning of semantic information, while this prior structural information is not fully exploited. In this paper, we propose a recurrent slice convolution module (called RSCM) to exploit the prior structural information of lane markings. The proposed RSCM is a special recurrent network structure with several slice convolution units (called SCU). The RSCM could obtain stronger semantic representation through the propagation of the prior structural information in SCU. Furthermore, we design a distance loss in consideration of the prior structure of lane markings. The lane detection network can be trained more steadily via the overall loss function formed by combining segmentation loss with the distance loss. The experimental results show the effectiveness of our method. We achieve excellent computation efficiency while keeping decent detection quality on lane detection benchmarks and the computational cost of our method is much lower than the state-of-the-art methods.","Lane marking detection, Deep neural network, Structure-prior",Degui Xiao and Lin Zhuo and Jianfang Li and Jiazhi Li,https://www.sciencedirect.com/science/article/pii/S1047320321002467,https://doi.org/10.1016/j.jvcir.2021.103373,1047-3203,2021,103373,81,Journal of Visual Communication and Image Representation,Structure-prior deep neural network for lane detection,article,XIAO2021103373
"Existing face aging (FA) approaches usually concentrate on a universal aging pattern, and produce restricted aging faces from one-to-one mapping. However, the diversity of living environments impact individuals differently in their oldness. To simulate various aging effects, we propose a multimodal FA framework based on face disentanglement technique of age-specific and age-irrelevant information. A Variational Autoencoder (VAE)-based encoder is designed to represent the distribution of the age-specific attributes. To capture the age-irrelevant features, a cycle-consistency loss of unpaired faces is utilized among various age spans. The extensive experimental results demonstrate that the sampled age-specific codes along with an age-irrelevant feature make the multimodal FA diverse and realistic.","Face aging, Disentangled representation, Variational auto-encoder, KL divergence, Generative adversarial network",Lu Liu and Shenghui Wang and Lili Wan and Haibo Yu,https://www.sciencedirect.com/science/article/pii/S1047320322000141,https://doi.org/10.1016/j.jvcir.2022.103452,1047-3203,2022,103452,83,Journal of Visual Communication and Image Representation,Multimodal face aging framework via learning disentangled representation,article,LIU2022103452
"Automatic target detection and tracking systems are used extensively in complex scenes. In long-term tracking, some visual attributes of objects are changing, such as illumination, size, profile, and so on. To address the issue, it is particularly important to describe the essential properties of the objects in tracking. An enhanced kernelized correlation filter tracking strategy fused multiple features with location prediction is proposed. To make the object appearance models more accuracy and robustness, based on the original histogram of oriented gradient features, we integrate the hue, saturation, value, and grayscale information to construct a new descriptor to represent the target appearance. Moreover, location prediction and bi-linear interpolation are employed to obtain the more accurate target position. Experiments show that the proposed strategy can obtain superior or competitive performance in challenging benchmark data sets. In practice, the algorithm is applied to track shuttle bus targets in the airport apron.","Visual properties, Target tracking, Multiple features, Kernelized correlation filter",Dequan Guo and Gexiang Zhang and Ferrante Neri and Sheng Peng and Qiang Yang and Paul Liu,https://www.sciencedirect.com/science/article/pii/S1047320322000384,https://doi.org/10.1016/j.jvcir.2022.103484,1047-3203,2022,103484,84,Journal of Visual Communication and Image Representation,An adaptive kernelized correlation filters with multiple features in the tracking application,article,GUO2022103484
"Kinship verification using facial images is mainly performed with a single face sample per person. To perform with a single sample, it is very difficult to specify an age group where kin pairs may have higher similarities. To address the above problem, we propose a novel weighted multi sample fusion (WMSF) method. The proposed WMSF method combines kin signals present in multiple samples per person (MSPP) to form a FuseKin image. To select the most discriminant features from the extracted feature vector, we propose a patch based discriminative analysis (PDA) method. Weights are calculated using the PDA method so as to reduce the discrimination between positive FuseKin pairs. Experiments were conducted on two different datasets which contain multiple face image samples per person, namely Family101 and Family in the Wild (FIW) to validate the performance of the proposed methods. Our method achieves competitive results as compared to other state-of-the-art methods.","Kinship verification, Image fusion, Feature selection, Face frontalization",Moumita Mukherjee and Toshanlal Meenpal and Aarti Goyal,https://www.sciencedirect.com/science/article/pii/S1047320322000268,https://doi.org/10.1016/j.jvcir.2022.103470,1047-3203,2022,103470,84,Journal of Visual Communication and Image Representation,FuseKin: Weighted image fusion based kinship verification under unconstrained age group,article,MUKHERJEE2022103470
"Anomaly detection is a challenging task in the field of intelligent video surveillance. It aims to identify anomalous events by monitoring the video captured by visual sensors. The main difficulty of this task is that the definition of anomalies is ambiguous. In recent years, most anomaly detection methods use a two-stage learning strategy, i.e., feature extraction and model building. In this paper, with the idea of refactoring, we propose an end-to-end anomaly detection framework using cyclic consistent adversarial networks (CycleGAN). Dynamic skeleton features are used as network constraints to alleviate the inaccuracy of feature extraction algorithms of a single generative adversarial network. In the training phase, only normal video frames and the corresponding skeleton features are used to train the generator and discriminator. In the testing phase, anomalous behaviors with high reconstruction errors can be filtered out by manually set thresholds. To the best of our knowledge, this is the first time CycleGAN has been used for video anomaly detection. Experimental results on challenging datasets show that our method can accurately detect anomalous behaviors in videos collected by video surveillance systems and is comparable to the current state-of-the-art methods.","Anomaly detection, Cycle-consistent adversarial networks, Pose estimation, Dynamic skeleton feature, Reconstruction error",Zheyi Fan and Shuhan Yi and Di Wu and Yu Song and Mengjie Cui and Zhiwen Liu,https://www.sciencedirect.com/science/article/pii/S1047320322000542,https://doi.org/10.1016/j.jvcir.2022.103508,1047-3203,2022,103508,85,Journal of Visual Communication and Image Representation,Video anomaly detection using CycleGan based on skeleton features,article,FAN2022103508
"In recent years, with the development of cloud storage, more and more people upload images to the cloud for storage. However, confidentiality and integrity issues may arise during transmission and storage to the cloud. Aiming at these security problems, a fragile watermarking scheme based on the encrypted domain is proposed. A watermark is divided into two types, one is for detection, the other is for recovery. After embedding the two types of watermarks into the host image, the watermarked image will be transferred to the cloud for storage. A three-level tamper detection mechanism is used in the detection process, and the first-level tamper detection can be processed in the cloud. While in recovery process, a mechanism of âblock-level detection, pixel-level recoveryâ is proposed to recover the tampered area. The experimental results show that the watermarked image has greatly changed the original image and guarantees the confidentiality. The three-level tamper detection mechanism can accurately detect the tampered area, the image can be effectively restored in different situations, when the tampering rate is as high as 80%, the average PSNR reaches 34.62Â dB, and the average SSIM is higher than 0.93.","Fragile watermarking, Tamper detection and localization, Image recover, Cloud storage",Li Huang and Da Kuang and Cheng-long Li and Yu-jian Zhuang and Shao-hua Duan and Xiao-yi Zhou,https://www.sciencedirect.com/science/article/pii/S1047320322000025,https://doi.org/10.1016/j.jvcir.2022.103437,1047-3203,2022,103437,83,Journal of Visual Communication and Image Representation,A self-embedding secure fragile watermarking scheme with high quality recovery,article,HUANG2022103437
"Imperceptibility, robustness and data payload are three main requirements of any image watermarking systems to guarantee desired functionalities, but there is a tradeoff among them from the information-theoretic perspective. How to achieve this balance is a major challenge. In this paper, we propose a new statistical image watermarking scheme, which is based on the high-order difference coefficients in nonsubsampled Shearlet transform (NSST) domain and the bounded generalized Gaussian mixture model-based hidden Markov tree (BGGMM-HMT). In the watermark embedding process, we use a nonlinear embedding approach to hide the digital watermark into the robust high-order difference coefficients, which can achieve better imperceptibility. In the watermark detection process, high-order difference coefficients are accurately modeled by using BGGMM-HMT, where the distribution characteristics of high-order difference coefficients can be captured through BGGMM, and the scale dependencies of high-order difference coefficients can be captured through HMT. Statistical model parameters are then estimated by combining the approach of minimizing the higher bound on data negative log-likelihood function and upwardâdownward algorithm. Finally, an image watermark detector based on BGGMM-HMT is developed using the locally optimum (LO) decision rule. For the proposed detector, the receiver operating characteristic (ROC) expression is derived in detail. We evaluate the proposed scheme from different aspects and compare it with the state-of-the-art schemes. After a large number of experimental tests, the encouraging results obtained prove the effectiveness of our watermarking scheme.","Image watermarking, Statistical watermark detector, High-order difference coefficients, Bounded generalized Gaussian mixture, Hidden Markov tree, Nonsubsampled Shearlet transform",Xiang-yang Wang and Xin Shen and Pan-pan Niu and Hong-ying Yang,https://www.sciencedirect.com/science/article/pii/S104732032200013X,https://doi.org/10.1016/j.jvcir.2022.103450,1047-3203,2022,103450,83,Journal of Visual Communication and Image Representation,BGGMM-HMT based locally optimum image watermark detector in high-order NSST difference domain,article,WANG2022103450
"In visual cryptography (VC), cheating is an important security concern where dishonest participants will fool honest ones and make them accept a fake secret by providing fake shares. Share and blind authentications are two categories of cheating prevention, and the last one relies on the inherent robust of shares against cheating attacks. In the previous studies, cheating in VC only focuses on operating a âpixel blockâ instead of a region of adjacent pixels. However, the well-known advantage of VC is to decode the secret image by using the human vision system (HVS), so it leads to a natural issue to reconsider cheating a region. In this paper, we formally address the binocular cheating attack (BCA) for a region to augment effectiveness of original cheating for a block. Finally, we demonstrate how to realize BCA by presenting non-trivial techniques against some blind authentication schemes, and further obtain implausible results. The BCA can also be applied to halftone secret.","Visual cryptography, Human vision system, Binocular cheating, Cheating prevention, Security",Yu-Chi Chen and Gwoboa Horng,https://www.sciencedirect.com/science/article/pii/S1047320322000426,https://doi.org/10.1016/j.jvcir.2022.103489,1047-3203,2022,103489,85,Journal of Visual Communication and Image Representation,Cheating in (halftone-secret) visual cryptography: Analysis of blind authentication schemes,article,CHEN2022103489
"Counting objects is a fundamental but challenging problem. In this paper, we propose diffusion-based, geometry-free, and learning-free methodologies to count the number of objects in images. The main idea is to represent each object by a unique index value regardless of its intensity or size, and to simply count the number of index values. First, we place different vectors, refer to as seed vectors, uniformly throughout the mask image. The mask image has boundary information of the objects to be counted. Secondly, the seeds are diffused using an edge-weighted harmonic variational optimization model within each object. We propose an efficient algorithm based on an operator splitting approach and alternating direction minimization method, and theoretical analysis of this algorithm is given. An optimal solution of the model is obtained when the distributed seeds are completely diffused such that there is a unique intensity within each object, which we refer to as an index. For computational efficiency, we stop the diffusion process before a full convergence, and propose to cluster these diffused index values. We refer to this approach as Counting Objects by Diffused Index (CODI). We explore scalar and multi-dimensional seed vectors. For Scalar seeds, we use Gaussian fitting in histogram to count, while for vector seeds, we exploit a high-dimensional clustering method for the final step of counting via clustering. The proposed method is flexible even if the boundary of the object is not clear nor fully enclosed. We present counting results in various applications such as biological cells, agriculture, concert crowd, and transportation. Some comparisons with existing methods are presented.","Object counting, Variational analysis, Alternating minimization, Fast methods, Clustering, Convergence analysis",Mengyi Tang and Maryam Yashtini and Sung Ha Kang,https://www.sciencedirect.com/science/article/pii/S1047320322000700,https://doi.org/10.1016/j.jvcir.2022.103527,1047-3203,2022,103527,86,Journal of Visual Communication and Image Representation,Counting Objects by Diffused Index: Geometry-free and training-free approach,article,TANG2022103527
"Cross-age face recognition (CAFR) is a challenging task, due to significant intra-personal variations. Furthermore, the training and testing data may contain random noise components. To address these issues, this paper proposes a deep low-rank feature learning and encoding method. Firstly, our method employs manifold learning in the low-rank optimization, which preserves the global and local structure of the data samples, while learning the clean low-rank features. Secondly, we encode the low-rank features using our locality-constrained feature encoding method, which learns an age-insensitive codebook from training data, and enables the intra-class samples to share the same local bases in a codebook. In the testing stage, the gallery and probe features are encoded by the learned codebook, which represents the images of the same identity by similar codewords for recognition. Furthermore, the periocular region of human faces is investigated for CAFR. Extensive experiments on five datasets demonstrate the effectiveness of our method.","Cross-age face recognition, Feature encoding, Kernel canonical correlation analysis, Low-rank features, Manifold learning",M. {Saad Shakeel} and Kin-Man Lam,https://www.sciencedirect.com/science/article/pii/S1047320321002856,https://doi.org/10.1016/j.jvcir.2021.103423,1047-3203,2022,103423,82,Journal of Visual Communication and Image Representation,Deep low-rank feature learning and encoding for cross-age face recognition,article,SAADSHAKEEL2022103423
"The key benefits of cloud services such as low cost, access flexibility, and mobility have attracted worldwide users to utilize deep learning algorithms for computer vision. These cloud servers are maintained by third parties, where users are always concerned about sharing their confidential data with them. In this paper, we addressed these concerns for by developing SecureDL, a privacy-preserving image recognition model for encrypted data over cloud. The proposed block-based image encryption scheme is well designed to protect imageâs visual information. The scheme constitutes an order-preserving permutation ordered binary number system and pseudo-random matrices. The proposed method is proved to be secure in a probabilistic viewpoint, and using various cryptographic attacks. Experiments are conducted over several image recognition datasets, and the trade-off analytics between the achieved recognition accuracy and data encryption is well described. SecureDL overcomes the storage and computational overheads that occur with fully-homomorphic and multi-party computation based secure recognition schemes.","Cloud computing, Image classification, Permutation ordered binary number system, Encrypted domain",Vishesh Kumar Tanwar and Balasubramanian Raman and Amitesh Singh Rajput and Rama Bhargava,https://www.sciencedirect.com/science/article/pii/S1047320322000529,https://doi.org/10.1016/j.jvcir.2022.103503,1047-3203,2022,103503,86,Journal of Visual Communication and Image Representation,SecureDL: A privacy preserving deep learning model for image recognition over cloud,article,TANWAR2022103503
"Analysis of human behavior through visual information has been one of the active research areas in computer vision community during the last decade. Vision-based human action recognition (HAR) is a crucial part of human behavior analysis, which is also of great demand in a wide range of applications. HAR was initially performed via images from a conventional camera; however, depth sensors have recently embedded as an additional informative resource to cameras. In this paper, we have proposed a novel approach to largely improve the performance of human action recognition using Complex Network-based feature extraction from RGB-D information. Accordingly, the constructed complex network is employed for single-person action recognition from skeletal data consisting of 3D positions of body joints. The indirect features help the model cope with the majority of challenges in action recognition. In this paper, the meta-path concept in the complex network has been presented to lessen the unusual actions structure challenges. Further, it boosts recognition performance. The extensive experimental results on two widely adopted benchmark datasets, the MSR-Action Pairs, and MSR Daily Activity3D indicate the efficiency and validity of the method.","Human action recognition, Complex network, Meta-path, 3D skeleton joints",Alaa Barkoky and Nasrollah Moghaddam Charkari,https://www.sciencedirect.com/science/article/pii/S1047320321002455,https://doi.org/10.1016/j.jvcir.2021.103371,1047-3203,2022,103371,82,Journal of Visual Communication and Image Representation,Complex Network-based features extraction in RGB-D human action recognition,article,BARKOKY2022103371
"Real-time and reliable head pose tracking is the basis of humanâcomputer interaction and face analysis applications. Aiming at the problems of accuracy and real time performance in current tracking method, a new head pose tracking method based on stereo visual SLAM is proposed in this paper. The sparse head map is constructed based on ORB feature points extraction and stereo matching, then the 3D-2D matching relations between 3D mappoints and 2D feature points are obtained by projection matching. Finally, the camera pose solved by the Bundle Adjustment is converted to head pose, which realizes the tracking of head pose. The experimental results show that this method can obtain high precise head pose. The mean errors of three Euler angles are all less than 1Â°. Therefore, the proposed head pose tracking method can track and estimate precise head pose in real time under smooth background.","Head pose tracking, Stereo visual SLAM, Bundle adjustment",Suibin Huang and Kun Yang and Hua Xiao and Peng Han and Jian Qiu and Li Peng and Dongmei Liu and Kaiqing Luo,https://www.sciencedirect.com/science/article/pii/S1047320321002698,https://doi.org/10.1016/j.jvcir.2021.103402,1047-3203,2022,103402,82,Journal of Visual Communication and Image Representation,A new head pose tracking method based on stereo visual SLAM,article,HUANG2022103402
"This paper proposes a new high-capacity reversible data hiding scheme in encrypted images. The content owner first divides the cover image into blocks. Then, the block permutation and the bitwise stream cipher processes are applied to encrypt the image. Upon receiving the encrypted image, the data hider analyzes the image blocks and adaptively decides an optimal block-type labeling strategy. Based on the adaptive block encoding, the image is compressed to vacate the spare room, and the secret data are encrypted and embedded into the spare space. According to the granted authority, the receiver can restore the cover image, extract the secret data, or do both. Experimental results show that the embedding capacity of the proposed scheme outperforms state-of-the-art schemes. In addition, security level and robustness of the proposed scheme are also investigated.","Encrypted image, Data hiding, Adaptive block encoding, Embedding capacity",Kai Gao and Ji-Hwei Horng and Chin-Chen Chang,https://www.sciencedirect.com/science/article/pii/S1047320322000372,https://doi.org/10.1016/j.jvcir.2022.103481,1047-3203,2022,103481,84,Journal of Visual Communication and Image Representation,High-capacity reversible data hiding in encrypted images based on adaptive block encoding,article,GAO2022103481
"Video retrieval methods have been developed for a single query. Multi-query video retrieval problem has not been investigated yet. In this study, an efficient and fast multi-query video retrieval framework is developed. Query videos are assumed to be related to more than one semantic. The framework supports an arbitrary number of video queries. The method is built upon using binary video hash codes. As a result, it is fast and requires a lower storage space. Database and query hash codes are generated by a deep hashing method that not only generates hash codes but also predicts query labels when they are chosen outside the database. The retrieval is based on the Pareto front multi-objective optimization method. Re-ranking performed on the retrieved videos by using non-binary deep features increases the retrieval accuracy considerably. Simulations carried out on two multi-label video databases show that the proposed method is efficient and fast in terms of retrieval accuracy and time.","Video hashing, Pareto optimization, Multi-query video retrieval",Enver Akbacak and Cabir Vural,https://www.sciencedirect.com/science/article/pii/S1047320322000530,https://doi.org/10.1016/j.jvcir.2022.103501,1047-3203,2022,103501,85,Journal of Visual Communication and Image Representation,Deep multi-query video retrieval,article,AKBACAK2022103501
"With the emergence of diverse multimedia editing software, a great number of edited or tampered video resources appear on the Internet, some of which can mix with the genuine ones. Digital video authenticity is an important step to make the best use of these video resources. As a common video forgery operation, frame tampering can change the video content and confuse viewers by removing or inserting some specific frames. In this paper, we explore the traces created by compression process and propose a new method to detect frame tampering based on the high-frequency features of reconstructed DCT coefficients in the tampered sequences. Experimental results demonstrate that our proposed method can effectively detect frame tampering operation, and accurately locate the breakpoint of frame tampering in the streams.","Digital video forensics, Frame tampering detection, Nonlinear quantization, S transform",Xiao Jin and Yuting Su and Peiguang Jing,https://www.sciencedirect.com/science/article/pii/S1047320322000013,https://doi.org/10.1016/j.jvcir.2022.103436,1047-3203,2022,103436,83,Journal of Visual Communication and Image Representation,Video frame deletion detection based on timeâfrequency analysis,article,JIN2022103436
"Image deraining is a significant problem that ensures the visual quality of images to prompt computer vision systems. However, due to the insufficiency of captured rain streaks features and global information, current image deraining methods often face the issues of rain streaks remaining and image blurring. In this paper, we propose a Multi-receptive Field Aggregation Network (MRFAN) to restore a cleaner rain-free image. Specifically, we construct a Multi-receptive Field Feature Extraction Block (MFEB) to capture rain features with different receptive fields. In MFEB, we design a Self-supervised Block (SSB) and an Aggregation Block (AGB). SSB can make the network adaptively focus on the critical rain features and rain-covered areas. AGB effectively aggregates and redistributes the multi-scale features to help the network simulate rain streaks better. Experiments show that our method achieves better results on both synthetic datasets and real-world rainy images.","Image deraining, Dilated convolution, Attention mechanism",Songliang Liang and Xiaozhe Meng and Zhuo Su and Fan Zhou,https://www.sciencedirect.com/science/article/pii/S1047320322000256,https://doi.org/10.1016/j.jvcir.2022.103469,1047-3203,2022,103469,84,Journal of Visual Communication and Image Representation,Multi-receptive Field Aggregation Network for single image deraining,article,LIANG2022103469
"According to the World Health Organization, falling is a significant health problem that causes thousands of deaths every year. Fall detection and fall prediction tasks enable accurate medical assistance to vulnerable populations whenever required, allowing local authorities to predict daily health care resources and to reduce fall damages accordingly. We present in this paper, a fall detection approach that explores human body geometry available at different frames of the video sequence. Especially, pose estimation, the angle and the distance between the vector formed by the head-centroid of the identified facial image and the center hip of the body, and the vector aligned with the horizontal axis of the center hip, are employed to construct new distinctive image features. A two-class Support Vector Machine (SVM) classifier and a Temporal Convolution Network (TCN) are trained on the newly constructed feature images. At the same time, a Long-Short-Term Memory (LSTM) network is trained on the calculated angle and distance sequences to classify fall and non-fall activities. We perform experiments on the Le2i FD dataset and the UR FD dataset, where we also propose a cross-dataset evaluation. The results demonstrate the effectiveness and efficiency of the developed approach.","Body geometry, Elderly assistance, Fall detection, Pose estimation, Video sequence",Djamila Romaissa Beddiar and Mourad Oussalah and Brahim Nini,https://www.sciencedirect.com/science/article/pii/S1047320321002728,https://doi.org/10.1016/j.jvcir.2021.103407,1047-3203,2022,103407,82,Journal of Visual Communication and Image Representation,Fall detection using body geometry and human pose estimation in video sequences,article,BEDDIAR2022103407
"We propose a deep learning framework to improve segmentation accuracy of the lung region in Chest X-Ray (CXR) images. The proposed methodology implements a âdivide and conquerâ strategy where the original CXRs are subdivided into smaller image patches, segmented them individually, and then reassembled to achieve the complete segmentation. This approach ensembles two models, the first of which is a traditional Convolutional Neural Network (CNN) used to classify the image patches and subsequently merge them to obtain a pre-segmentation. The second model is a modified U-Net architecture to segment the patches and subsequently combines them to obtain another pre-segmented image. These two pre-segmented images are combined using a binary disjunction operation to get the initial segmentation, which is later post-processed to obtain the final segmentation. The post-processing steps consist of traditional image processing techniques such as erosion, dilation, connected component labeling, and region-filling algorithms. The robustness of the proposed methodology is demonstrated using two public (MC, JPCL) and one proprietary (The University of Texas Medical Branch - UTMB) datasets of CXR images. The proposed framework outperformed many state-of-the-arts competitions presented in the literature.","Lung segmentation, Chest X-ray, Deep learning, UNet, CNNs, Ensemble",Md Fashiar Rahman and Yan Zhuang and Tzu-Liang (Bill) Tseng and Michael Pokojovy and Peter McCaffrey and Eric Walser and Scott Moen and Alex Vo,https://www.sciencedirect.com/science/article/pii/S1047320322000657,https://doi.org/10.1016/j.jvcir.2022.103521,1047-3203,2022,103521,85,Journal of Visual Communication and Image Representation,Improving lung region segmentation accuracy in chest X-ray images using a two-model deep learning ensemble approach,article,RAHMAN2022103521
"Joint photographic experts group (JPEG) can provide good quality with small file size but also eliminate extensively the redundancies of images. Therefore, hiding data into JPEG images in terms of maintaining high visual quality at small file sizes has been a great challenge for researchers. In this paper, an adaptive reversible data hiding method for JPEG images containing multiple two-dimensional (2D) histograms is proposed. Adaptability is mainly reflected in three aspects. The first one is to preferentially select sharper histograms for data embedding after K histograms are established by constructing the kth (kâ{1,2,â¦,K}) histogram using the kth non-zero alternating current (AC) coefficient of all the quantized discrete cosine transform blocks. On the other hand, to fully exploit the strong correlation between coefficients of one histogram, the smoothness of each coefficient is estimated by a block smoothness estimator so that a sharply-distributed 2D-histogram is constructed by combining two coefficients with similar smoothness into a pair. The pair corresponding to low complexity is selected priorly for data embedding, leading to high embedding performance while maintaining low file size. Besides, we design multiple embedding strategies to adaptively select the embedding strategy for each 2D histogram. Experimental results demonstrate that the proposed method can achieve higher rateâdistortion performance which maintaining lower file storage space, compared with previous studies.","Reversible data hiding, Multiple histograms, Two-dimensional histogram, Rateâdistortion model",Shaowei Weng and Ye Zhou and Tiancong Zhang,https://www.sciencedirect.com/science/article/pii/S1047320322000414,https://doi.org/10.1016/j.jvcir.2022.103487,1047-3203,2022,103487,85,Journal of Visual Communication and Image Representation,Adaptive reversible data hiding for JPEG images with multiple two-dimensional histograms,article,WENG2022103487
"Recent years have witnessed several modified discriminative correlation filter (DCF) models exhibiting excellent performance in visual tracking. A fundamental drawback to these methods is that rotation of the target is not well addressed which leads to model deterioration. In this paper, we propose a novel rotation-aware correlation filter to address the issue. Specifically, samples used for training of the modified DCF model are rectified when rotation occurs, rotation angle is effectively calculated using phase correlation after transforming the search patch from Cartesian coordinates to the Log-polar coordinates, and an adaptive selection mechanism is further adopted to choose between a rectified target patch and a rectangular patch. Moreover, we extend the proposed approach for robust tracking by introducing a simple yet effective Kalman filter prediction strategy. Extensive experiments on five standard benchmarks show that the proposed method achieves superior performance against state-of-the-art methods while running in real-time on single CPU.","Correlation filter, Visual tracking, Rotation, Phase correlation, Kalman filter",Jiawen Liao and Chun Qi and Jianzhong Cao and Xiaofang Wang and Long Ren and Chaoning Zhang,https://www.sciencedirect.com/science/article/pii/S1047320321002844,https://doi.org/10.1016/j.jvcir.2021.103422,1047-3203,2022,103422,83,Journal of Visual Communication and Image Representation,Rotation-aware correlation filters for robust visual tracking,article,LIAO2022103422
"This paper presents a fragile watermarking scheme for tamper localization using Singular Value Decomposition (SVD) and logistic map. The proposed scheme divides the image into blocks of size 2Â ÃÂ 2 pixels and generates an 8-bit watermark from each block. The watermark is computed by permuting the six Most Significant Bits (MSBs) of each pixel in the block using the logistic map, followed by SVD. To secure, the watermark thus generated is further encrypted using the logistic map. This encrypted watermark is embedded into 2 Least Significant Bits (LSBs) of each pixel to enable tamper detection and localization. The experimental results demonstrate that the proposed scheme can precisely locate tampered regions under copy-paste, content removal, text addition, noise addition, vector quantization, collage, content only, and constant feature attacks. Tamper localization accuracy is better or comparable to the state-of-the-art tamper localization algorithms.","Image authentication, Fragile watermarking, Tamper localization, Logistic map, Singular value decomposition",Neena Raj N.R. and Shreelekshmi R.,https://www.sciencedirect.com/science/article/pii/S1047320322000517,https://doi.org/10.1016/j.jvcir.2022.103500,1047-3203,2022,103500,85,Journal of Visual Communication and Image Representation,Fragile watermarking scheme for tamper localization in images using logistic map and singular value decomposition,article,NR2022103500
"In this work we have explored the hybrid deep learning architecture for recognizing the tampering from the videos. This hybrid architecture explores the features from the authentic videos to categorize the tampered portions from the forged videos. Initially, the process begins by compressing the input video using the Discrete cosine transform (DCT) based double compression approach. Then, the filtering process is carried out to improve the quality of compressed frame using the bilateral filtering. Then, the modified segmentation approach is applied to segment the frames into different regions. The features from these segmented portions are extracted and fed into hybrid DNN-AGSO (deep neural network- Adaptivf RELATED WORKSe Galactic Swarm Optimization) using Gabor wavelet transform (GWT) technique. Three different datasets are used to evaluate the overall performance they are, VTD, MFC-18, and VIRAT by MATLAB platform. The recognition rate achieved by VTD, MFC-18, and VIRAT datasets are 96%, 95.2%, and 93.47% respectively.","Video forgery, Double compression, Modified Brain storm optimization (MBSO), Adaptive galactic Swarm optimization (AGSO), Deep Neural Network (DNN)",Malle Raveendra and K. Nagireddy,https://www.sciencedirect.com/science/article/pii/S1047320321002686,https://doi.org/10.1016/j.jvcir.2021.103401,1047-3203,2022,103401,82,Journal of Visual Communication and Image Representation,Tamper video detection and localization using an adaptive segmentation and deep network technique,article,RAVEENDRA2022103401
"This paper proposes a blind image deconvolution method which consists of two sequential phases, i.e., blur kernel estimation and image restoration. In the first phase, we adopt the L0-norm of image gradients and total variation (TV) to regularize the latent image and blur kernel, respectively. Then we design an alternating optimization algorithm which jointly incorporates the estimation of intermediately restored image, blur kernel and regularization parameters into account. In the second phase, we propose to take the mixture of L0-norm of image gradients and TV to regularize the latent image, and design an efficient non-blind deconvolution algorithm to achieve the restored image. Experimental results on both a benchmark image dataset and real-world blurred images show that the proposed method can effectively restore image details while suppress noise and ringing artifacts, the result is of high quality which is competitive with some state of the art methods.","Blind image deconvolution, -norm gradient regularization, TV regularization",Shuyin Tao and Wende Dong and Jian Xu and Jianfeng Lu and Guili Xu and Yueting Chen,https://www.sciencedirect.com/science/article/pii/S1047320321002443,https://doi.org/10.1016/j.jvcir.2021.103370,1047-3203,2021,103370,81,Journal of Visual Communication and Image Representation,An adaptive two phase blind image deconvolution algorithm for an iterative regularization model,article,TAO2021103370
"In this paper, the feature representation of an image by CNN is used to hide the secret image into the cover image. The style of the cover image hides the content of the secret image and produce a stego image using Neural Style Transfer (NST) algorithm, which resembles the cover image and also contains the semantic content of secret image. The main technical contributions are to hide the content of the secret image in the in-between hidden layered style features of the cover image, which is the first of its kind in the present state-of-art-technique. Also, to recover the secret image from the stego image, destylization is done with the help of conditional generative adversarial networks (GANs) using Residual in Residual Dense Blocks (RRDBs). Further, stego images from different layer combinations of content and style features are obtained and evaluated. Evaluation is based on the visual similarity and quality loss between the cover-stego pair and the secret-reconstructed secret pair of images. From the experiments, it has been observed that the proposed algorithm has 43.95Â dB Peak Signal-to-Noise Ratio (PSNR)), .995 Structural Similarity Index (SSIM), and .993 Visual Information Fidelity (VIF) for the ImageNet dataset. The proposed algorithm is found to be more robust against StegExpose than the traditional methods.","Image security, Convolutional neural network, Information hiding, StegoExpose, Steganography",Mallika and Jagpal Singh Ubhi and Ashwani Kumar Aggarwal,https://www.sciencedirect.com/science/article/pii/S1047320322000360,https://doi.org/10.1016/j.jvcir.2022.103483,1047-3203,2022,103483,85,Journal of Visual Communication and Image Representation,Neural Style Transfer for image within images and conditional GANs for destylization,article,MALLIKA2022103483
"Facial aging is widely used in criminal tracking and the search for lost children. If the aging face is made up, it will greatly affect the discrimination of the tracking system. Therefore, the research on the makeup of different aging faces is extremely important. Existing studies have achieved a good transition from the non-makeup domain to the makeup domain in facial makeup transfer. But few studies involve the transfer of facial makeup at different ages. In addition, existing datasets rarely contain both age and makeup attributes, which make the transfer of facial makeup for different ages full of challenges. To solve the above problems, we propose a learning framework, called AM-Net, which can realize facial makeup transfer for different ages while protecting identity information. AM-Net is composed of two sub-network modules: Aging-Net and Makeup-Net. AM-Net first learns the aging mechanism of faces through Aging-Net, and then, it feeds the learned aging mode to Makeup-Net. After that, AM-Net trains Makeup-Net to realize the mapping relationship between the non-makeup domain to the makeup domain and transfer the makeup style to the face of the non-makeup. Throughout the network, multiple losses are applied to ensure AM-Net preserve information about the identity, background, etc. Extensive experiments are conducted on different datasets with different state-of-the-art methods, which prove the effectiveness of AM-Net.","Domain, Facial aging, GAN, Makeup",Sen Fang and Mingxing Duan and Kenli Li and Keqin Li,https://www.sciencedirect.com/science/article/pii/S1047320322000244,https://doi.org/10.1016/j.jvcir.2022.103464,1047-3203,2022,103464,85,Journal of Visual Communication and Image Representation,Facial makeup transfer with GAN for different aging faces,article,FANG2022103464
"Visual cryptography scheme (VCS) is a branch of secret sharing, in which a secret image is encrypted into n noise-like shares. In the recovering phase, the secret image can be reconstructed by stacking sufficient shares. Itâs obvious that VCS with multiple decryptions can further widen the range of application. However, the investigations on the existing scheme with multiple decryptions are not sufficient. In this paper, we develop a novel contrast improved OR and XOR based (k,n)-VCS without pixel expansion. Significantly, we give a general simplified calculation formula to compute the theoretical contrast of the proposed scheme. In addition, if there are no computing devices, then we can reconstruct the secret image by stacking the shares directly. Meanwhile, we recover the secret image perfectly by performing XOR operation when computing devices are available. Since the proposed scheme is based on the parity basis matrices, our scheme has no pixel expansion. Finally, Theoretical analysis and experimental results are conducted to evaluate the efficiency and security of the proposed scheme.","Visual cryptography, Multiple decryptions, Contrast, Perfect recovery, Threshold",Yongkang Zhao and Fang-Wei Fu,https://www.sciencedirect.com/science/article/pii/S1047320321002716,https://doi.org/10.1016/j.jvcir.2021.103408,1047-3203,2022,103408,82,Journal of Visual Communication and Image Representation,"A contrast improved OR and XOR based (k,n) visual cryptography scheme without pixel expansion",article,ZHAO2022103408
"Secret image sharing (SIS) can divide a secret image into several shadow images for protection. Information hiding in the sharing domain (IHSD) fuses SIS and information hiding (IH) to simultaneously share any secret image and hide any information, and this technique can be applied in cloud computing, law enforcement and medical diagnoses. IHSD not only marks shadow images with information to prevent malicious tampering and for convenient management, search and identification but also enhances the robustness of IH. In this paper, we first introduce a formal definition of IHSD. Then, we describe a general IHSD model and algorithms with a concrete example in detail. In IHSD, we design the random element utilization model to control the random pixels generated from SIS. Then, we obtain shadow images with hidden information to realize SIS and IH simultaneously. The inputs of SIS with secret images, steganography and extra information in algorithms are without any limitations. Theoretical analyses, experiments and comparisons are presented to prove the effectiveness and feasibility of IHSD.","Secret image sharing, Random element utilization model, Information hiding",Fengyue Xing and Xuehu Yan and Long Yu and Yuyuan Sun,https://www.sciencedirect.com/science/article/pii/S1047320322000645,https://doi.org/10.1016/j.jvcir.2022.103520,1047-3203,2022,103520,86,Journal of Visual Communication and Image Representation,Information hiding in the sharing domain,article,XING2022103520
"Colored point cloud (PC) will inevitably encounter distortion during its acquisition, processing, coding and transmission, which may affect the visual quality of the colored PC. Therefore, it is necessary to design an effective tool for colored PC quality assessment (PCQA). In this paper, considering the mapping relationship of perception between the colored PC and its corresponding projection images, we propose a novel PCQA method based on texture and geometry projection (denoted as TGP-PCQA). The main idea of the proposed TGP-PCQA method is to obtain texture and geometry projection maps from different perspectives for evaluating the colored PC. Specifically, 4D tensor decomposition is used to obtain the combination and difference information between the reference and distorted texture projection maps for mainly characterizing texture distortion of colored PC. Meanwhile, the edge features of the geometry projection map are calculated to measure the global or local geometry distortion. All of the extracted features are combined to predict an overall quality of colored PC. In addition, this paper establishes a multi-distorted colored PC database named CPCD2.0 with compression distortions and Gaussian noise, which orients to the influence of both geometry and texture components in distortion. Experimental results on two open subjective evaluation databases (IRPC and SJTU-PCQA) and the self-built CPCD2.0 database show that the proposed TGP-PCQA method outperforms the state-of-the-art PCQA methods. We are also providing the self-built CPCD2.0 database free of charge at https://github.com/cherry0415/CPCD2.0.","Colored point cloud, Visual quality assessment, Texture and geometry projection, Objective quality assessment",Zhouyan He and Gangyi Jiang and Mei Yu and Zhidi Jiang and Zongju Peng and Fen Chen,https://www.sciencedirect.com/science/article/pii/S1047320322000128,https://doi.org/10.1016/j.jvcir.2022.103449,1047-3203,2022,103449,83,Journal of Visual Communication and Image Representation,TGP-PCQA: Texture and geometry projection based quality assessment for colored point clouds,article,HE2022103449
"3D skeleton sequences contain more effective and discriminative information than RGB video and are more suitable for human action recognition. Accurate extraction of human skeleton information is the key to the high accuracy of action recognition. Considering the correlation between joint points, in this work, we first propose a skeleton feature extraction method based on complex network. The relationship between human skeleton points in each frame is coded as a network. The changes of action over time are described by a time series network composed of skeleton points. Network topology attributes are used as feature vectors, complex network coding and LSTM are combined to recognize human actions. The method was verified on the NTU RGBÂ +Â D60, MSR Action3D and UTKinect-Action3D dataset, and have achieved good performance, respectively. It shows that the method of extracting skeleton features based on complex network can properlyidentify different actions. This method that considers the temporal information and the relationship between skeletons at the same time plays an important role in the accurate recognition of human actions.","Skeleton-based action recognition, Complex network coding, LSTM, Feature extraction",Xiangpei Shen and Yanrui Ding,https://www.sciencedirect.com/science/article/pii/S1047320321002558,https://doi.org/10.1016/j.jvcir.2021.103386,1047-3203,2022,103386,82,Journal of Visual Communication and Image Representation,Human skeleton representation for 3D action recognition based on complex network coding and LSTM,article,SHEN2022103386
"The underlining task for fine-grained image recognition captures both the inter-class and intra-class discriminate features. Existing methods generally use auxiliary data to guide the network or a complex network comprising multiple sub-networks. They have two significant drawbacks: (1) Using auxiliary data like bounding boxes requires expert knowledge and expensive data annotation. (2) Using multiple sub-networks make network architecture complex and requires complicated training or multiple training steps. We propose an end-to-end Spatial Self-Attention Network (SSANet) comprising a spatial self-attention module (SSA) and a self-attention distillation (Self-AD) technique. The SSA encodes contextual information into local features, improving intra-class representation. Then, the Self-AD distills knowledge from the SSA to a primary feature map, obtaining inter-class representation. By accumulating classification losses from these two modules enables the network to learn both inter-class and intra-class features in one training step. The experiment findings demonstrate that SSANet is effective and achieves competitive performance.","Fine-grained recognition, Spatial self-attention, Knowledge distillation, Convolutional neural network",Adu Asare Baffour and Zhen Qin and Yong Wang and Zhiguang Qin and Kim-Kwang Raymond Choo,https://www.sciencedirect.com/science/article/pii/S104732032100242X,https://doi.org/10.1016/j.jvcir.2021.103368,1047-3203,2021,103368,81,Journal of Visual Communication and Image Representation,Spatial self-attention network with self-attention distillation for fine-grained image recognition,article,BAFFOUR2021103368
"Tilt and pan camera movements are common in computer games or social media videos. These types of videos contain numerous perspective transforms while todayâs video codecs rely on translational and affine motion models for motion compensation. The general perspective motion model with 8 parameters (8PMM) has unreasonably high processing time. In this paper, the eight-parameter perspective transform is simplified into a six-parameter transform to keep the time complexity within an acceptable range while modeling the most relevant transforms. Also, two motion prediction modes, Advanced Perspective Motion Vector Prediction (APMVP) and Perspective Model Merge (PMM), are proposed. The implementation results show an average of 7.0% BD-rate reduction over H.266/VVC Test Model with a maximum of 20% encoding time overhead. The results also show a 71% processing time reduction in comparison to 8PMM while experiencing an average of 5.6% increase in BD-Rate. Much better visual quality is measured through VMAF quality meter.","Multimedia streaming, Video coding, Motion estimation, Spatial transform, BD-rate reduction",Iman {Soltani Mohammadi} and Mohammad Ghanbari and Mahmoud Reza Hashemi,https://www.sciencedirect.com/science/article/pii/S104732032200061X,https://doi.org/10.1016/j.jvcir.2022.103514,1047-3203,2022,103514,85,Journal of Visual Communication and Image Representation,An efficient six-parameter perspective motion model for VVC,article,SOLTANIMOHAMMADI2022103514
"Due to privacy and security concerns, the researches of reversible data hiding in encrypted images (RDHEI) have become increasingly important. Conventional schemes vacate the spare room after image encryption (VRAE) suffer from the low embedding rate, high error rate of data extraction, and imperfect image recovery. To address these issues, we propose a separable reversible data hiding scheme for encrypted images that utilizes a novel pixel rotation technique to embed data into fully encrypted images. The block complexities of four decrypted rotation states are considered when recovering image. To realize perfect image recovery, we further devise a lossless version (LPR-RDHEI). Experimental results demonstrate that the proposed PR-RDHEI scheme achieves an embedding rate of 0.4994 bpp on average and ensures lossless data extraction. Meanwhile, the proposed LPR-RDHEI scheme still has a 0.4494 bpp embedding rate on average. The embedding rates of our two schemes are significantly improved compared with state-of-the-arts.","Reversible data hiding, Encrypted images, Pixel rotation, Lossless scheme",Xu Wang and Ching-Chun Chang and Chia-Chen Lin and Chin-Chen Chang,https://www.sciencedirect.com/science/article/pii/S1047320321002832,https://doi.org/10.1016/j.jvcir.2021.103421,1047-3203,2022,103421,82,Journal of Visual Communication and Image Representation,Reversal of pixel rotation: A reversible data hiding system towards cybersecurity in encrypted images,article,WANG2022103421
"Attention mechanism has been found effective for human gaze estimation, and the attention and diversity of learned features are two important aspects of attention mechanism. However, the traditional attention mechanism used in existing gaze model is more prone to utilize first-order information that is attentive but not diverse. Though the existing bilinear pooling-based attention could overcome the shortcoming of traditional attention, it is limited to extract high-order contextual information. Thus we introduce a novel bilinear pooling-based attention mechanism, which could extract the second-order contextual information by the interaction between local deep learned features. To make the gaze-related features robust for spatial misalignment, we further propose an attention-in-attention method, which consists of a global average pooling and an inner attention on the second-order features. For the purpose of gaze estimation, a new bilinear pooling-based attention networks with attention-in-attention is further proposed. Extensive evaluation shows that our method surpasses the state-of-the-art by a big margin.","Gaze tracking, Deep learning, Bilinear pooling, Attention",Dakai Ren and Jiazhong Chen and Jian Zhong and Zhaoming Lu and Tao Jia and Zongyi Li,https://www.sciencedirect.com/science/article/pii/S1047320321002431,https://doi.org/10.1016/j.jvcir.2021.103369,1047-3203,2021,103369,81,Journal of Visual Communication and Image Representation,Gaze estimation via bilinear pooling-based attention networks,article,REN2021103369
"Quality assessment of natural images is influenced by perceptual mechanisms, e.g., attention and contrast sensitivity, and quality perception can be generated in a hierarchical process. This paper proposes an architecture of Attention Integrated Hierarchical Image Quality networks (AIHIQnet) for no-reference quality assessment. AIHIQnet consists of three components: general backbone network, perceptually guided neck network, and head network. Multi-scale features extracted from the backbone network are fused to simulate image quality perception in a hierarchical manner. The attention and contrast sensitivity mechanisms modelled by an attention module capture essential information for quality perception. Considering that image rescaling potentially affects perceived quality, appropriate pooling methods in the non-convolution layers in AIHIQnet are employed to accept images with arbitrary resolutions. Comprehensive experiments on publicly available databases demonstrate outstanding performance of AIHIQnet compared to state-of-the-art models. Ablation experiments were performed to investigate the variants of the proposed architecture and reveal importance of individual components.","Attention, Hierarchical networks, Image quality assessment (IQA), Perceptual mechanisms, Quality perception",Junyong You and Jari Korhonen,https://www.sciencedirect.com/science/article/pii/S1047320321002674,https://doi.org/10.1016/j.jvcir.2021.103399,1047-3203,2022,103399,82,Journal of Visual Communication and Image Representation,Attention integrated hierarchical networks for no-reference image quality assessment,article,YOU2022103399
"Depth estimation from a single RGB image is a challenging task. It is ill-posed since a single 2D image may correspond to various 3D scenes at different scales. On the other hand, estimating the relative depth relationship between two objects in a scene is easier and may yield more reliable results. Thus, in this paper, we propose a novel algorithm for monocular depth estimation using relative depths. First, using a convolutional neural network, we estimate two types of depths at multiple spatial resolutions: ordinary depth maps and relative depth tensors. Second, we restore a relative depth map from each relative depth tensor. A relative depth map is equivalent to an ordinary depth map with global scale information removed. For the restoration, sparse pairwise comparison matrices are constructed from available relative depths, and missing entries are filled in using the alternative least square (ALS) algorithm. Third, we decompose the ordinary and relative depth maps into components and recombine them to yield a final depth map. To reduce the computational complexity, relative depths at fine spatial resolutions are directly used to refine the final depth map. Extensive experimental results on the NYUv2 dataset demonstrate that the proposed algorithm provides state-of-the-art performance.","Monocular depth estimation, Relative depth, 3D analysis",Jae-Han Lee and Chang-Su Kim,https://www.sciencedirect.com/science/article/pii/S1047320322000190,https://doi.org/10.1016/j.jvcir.2022.103459,1047-3203,2022,103459,84,Journal of Visual Communication and Image Representation,Single-image depth estimation using relative depths,article,LEE2022103459
"Seamless streaming of high quality video under unstable network condition is a big challenge. HTTP adaptive streaming (HAS) provides a solution that adapts the video quality according to the network conditions. Traditionally, HAS algorithm runs at the client side while the clients are unaware of bottlenecks in the radio channel and competing clients. The traditional adaptation strategies do not explicitly coordinate between the clients, servers, and cellular networks. The lack of coordination has been shown to lead to suboptimal user experience. As a response, multi-access edge computing (MEC)-assisted adaptation techniques emerged to take advantage of computing and content storage capabilities in mobile networks. In this study, we investigate the performance of both MEC-assisted and client-side adaptation methods in a multi-client cellular environment. Evaluation and comparison are performed in terms of not only the video rate and dynamics of the playback buffer but also the fairness and bandwidth utilization. We conduct extensive experiments to evaluate the algorithms under varying client, server, dataset, and network settings. Results demonstrate that the MEC-assisted algorithms improve fairness and bandwidth utilization compared to the client-based algorithms for most settings. They also reveal that the buffer-based algorithms achieve significant quality of experience; however, these algorithms perform poorly compared with throughput-based algorithms in protecting the playback buffer under rapidly varying bandwidth fluctuations. In addition, we observe that the preparation of the representation sets affects the performance of the algorithms, as does the playback buffer size and segment duration. Finally, we provide suggestions based on the behaviors of the algorithms in a multi-client environment.","Quality of experience, DASH, Fairness, HTTP adaptive streaming, Video",Waqas {ur Rahman} and Muhammad Bilal Amin and Md Delowar Hossain and Choong {Seon Hong} and Eui-Nam Huh,https://www.sciencedirect.com/science/article/pii/S1047320321002790,https://doi.org/10.1016/j.jvcir.2021.103415,1047-3203,2022,103415,82,Journal of Visual Communication and Image Representation,QoE optimization for HTTP adaptive streaming: Performance evaluation of MEC-assisted and client-based methods,article,URRAHMAN2022103415
"Because the imaging spectra of infrared images and visible light images are different, there is a huge modal difference between visible light images and infrared ones. Existing methods use image conversion to solve the problem of modal difference between two images, but these methods usually fail to focus on the complete information of images, which lead to the results of cross modal person re-identification are unstable. To solve this problem, we propose a new visibleâinfrared person re-identification method, called dual-path image pair joint discriminant model (DPJD), which simultaneously optimizes the distance within and between classes, and supervises the network learning to identify feature representations. We generate images with different modalities for the samples, and separately compose the same modality image pair and different modality image pair so as to overcome the inconsistent alignment issues. In addition, we also propose a discriminant module based on dual-path (DMDP) to improve the generation quality and discrimination accuracy of image pairs. Experiments on two benchmark datasets SYSU-MM01 and RegDB demonstrate its effectiveness.","Person re-identification, visibleâinfrared, Image pair generation, Dual-path discrimination",Zhongjie Wang and Li Liu and Huaxiang Zhang,https://www.sciencedirect.com/science/article/pii/S1047320322000591,https://doi.org/10.1016/j.jvcir.2022.103512,1047-3203,2022,103512,85,Journal of Visual Communication and Image Representation,Dual-path image pair joint discrimination for visibleâinfrared person re-identification,article,WANG2022103512
"In conventional multi-image secret sharing schemes (MISSS) images are shared by the trusted dealer and the shares are sent to the set of participants through a secure channel. During reconstruction, the participants submit shares to a trusted combiner. But the method will collapse if any of the actors perform cheating. This brings verifiable image secret sharing in the research arena. Verifying the trustworthiness of the dealer by the shareholders before accepting shares (dealer verification), examining the genuineness of the shares submission request received from a combiner (combiner verification), checking the authenticity of the shares received from participants by the combiner (cheating detection/ cheater identification) are techniques related to verifiable secret sharing. Medical images, images used at the military or diplomatic level; contain sensible information. Thus the authenticity of the reconstructed images should be checked beforehand. In the case of multi-image secret sharing, the researchers use bit padding if the plaintext images are of different sizes. This adds an extra level of burden during sharing and retrieval. A verifiable varying size (m,n,n) multi-image secret sharing is addressed in this article. Here m (wheremâ¤n) varying sized images are shared among n participants and during reconstruction all the shares are required. The major contribution of the addressed technique is that it has the capability of dealer authentication, combiner verification, and cheater identification. Another advancement is that most of the communication can be made through a public channel. The test results generate noise like images and statistical analysis, security analysis say in favor of the claims. Comparison with some state-of-the-art techniques gives it a stable platform in verifiable multi-image secret sharing.","Multi-image secret sharing, Varying sized, Verifiable multi image secret sharing, Dealer authentication, Cheater identification, Combiner verification",Aswini Vinay Soreng and Shyamalendu Kandar,https://www.sciencedirect.com/science/article/pii/S1047320322000232,https://doi.org/10.1016/j.jvcir.2022.103466,1047-3203,2022,103466,84,Journal of Visual Communication and Image Representation,"Verifiable varying sized (m,n,n) multi-image secret sharing with combiner verification and cheater identification",article,SORENG2022103466
"Optimizing a ranking-based metric as the loss function, such as Average Precision (AP), has been found very effective in image retrieval tasks, but it has received less attention in Person Re-Identification (Re-ID). In this paper, Low Rank High Weight (LRHW) AP is proposed to apply the AP-optimizing method on the Re-ID task. LRHW-AP employs high weight on the low rank positive instances, which provides more information for model optimization than high rank positive instances and distribute in high gradient area. We propose a new pooling method called Power Activation Weighted Mean (PAWM) pooling which can unify a set of pooling methods because of a changeable activation function and a trainable parameter. Thus one can adjust and train PAWM to adapt to the target task to improve the model performance. Besides, we incorporate Warmup and Exponentially Decay Scheduler with a delay period, called Warmup Delay Exponentially Decay Scheduler, which brings further improvement. Through an extensive set of ablation studies, we verify that all methods mentioned above contribute to the performance boosts on Re-ID and the model achieves 95.3% rank-1 and 88.4% mAP on Market1501 with ResNet50.","Person Re-Identification, Image Retrieval, Average Precision, Pooling",Yifei Liu and Yaling Liang and Ziheng Chen,https://www.sciencedirect.com/science/article/pii/S1047320322000633,https://doi.org/10.1016/j.jvcir.2022.103517,1047-3203,2022,103517,85,Journal of Visual Communication and Image Representation,LRHW-AP: Using ranking-based metric as loss for Person Re-Identification,article,LIU2022103517
"Multi-label recognition is a fundamental, and yet is a challenging task in computer vision. Recently, deep learning models have achieved great progress towards learning discriminative features from input images. However, conventional approaches are unable to model the inter-class discrepancies among features in multi-label images, since they are designed to work for image-level feature discrimination. In this paper, we propose a unified deep network to learn discriminative features for the multi-label task. Given a multi-label image, the proposed method first disentangles features corresponding to different classes. Then, it discriminates between these classes via increasing the inter-class distance while decreasing the intra-class differences in the output space. By regularizing the whole network with the proposed loss, the performance of applying the well-known ResNet-101 is improved significantly. Extensive experiments have been performed on COCO-2014, VOC2007 and VOC2012 datasets, which demonstrate that the proposed method outperforms state-of-the-art approaches by a significant margin of 3.5% on large-scale COCO dataset. Moreover, analysis of the discriminative feature learning approach shows that it can be plugged into various types of multi-label methods as a general module.","Multi-label recognition, Multi-label-contrastive learning, Contrastive representation, Deep learning",Mohammed Hassanin and Ibrahim Radwan and Salman Khan and Murat Tahtali,https://www.sciencedirect.com/science/article/pii/S1047320322000116,https://doi.org/10.1016/j.jvcir.2022.103448,1047-3203,2022,103448,83,Journal of Visual Communication and Image Representation,Learning discriminative representations for multi-label image recognition,article,HASSANIN2022103448
"Feature extraction for visibleâinfrared person re-identification (VI-ReID) is challenging because of the cross-modality discrepancy in the images taken by different spectral cameras. Most of the existing VI-ReID methods often ignore the potential relationship between features. In this paper, we intend to transform low-order person features into high-order graph features, and make full use of the hidden information between person features. Therefore, we propose a multi-hop attention graph convolution network (MAGC) to extract robust person joint feature information using residual attention mechanism while reducing the impact of environmental noise. The transfer of higher order graph features within MAGC enables the network to learn the hidden relationship between features. We also introduce the self-attention semantic perception layer (SSPL) which can adaptively select more discriminant features to further promote the transmission of useful information. The experiments on VI-ReID datasets demonstrate its effectiveness.","Visibleâinfrared person re-identification, Feature extraction, Multi-hop, Self-attention",Wenbo Gao and Li Liu and Lei Zhu and Huaxiang Zhang,https://www.sciencedirect.com/science/article/pii/S104732032200058X,https://doi.org/10.1016/j.jvcir.2022.103511,1047-3203,2022,103511,85,Journal of Visual Communication and Image Representation,Visibleâinfrared person re-identification based on key-point feature extraction and optimization,article,GAO2022103511
"A distortionless secret image sharing scheme using finite field GF(PG) (PG is the largest prime less than a given number) is investigated for sharing the absolute moment block truncation coding (AMBTC) images. Two adjusting operations are devised to modify the AMBTC trios (i.e., quantization pixels and bit maps) suitable for GF(PG) sharing. Polynomials under GF(PG) are constructed for encrypting the quantization pixels and bit maps. When sharing the trios under GF(PG), the AMBTC image is perfectly recovered. Moreover, the bit map sharing under GF(PG) can be extended to GF(PS) (PS is the smallest prime larger than a given number). Another scheme using GF(PS) is constituted by combining quantization pixel sharing under GF(PG) and bit map sharing under GF(PS). But the scheme using GF(PS) is not always lossless. Experimental results show that the proposed schemes are effective. Since GF(PG) and GF(PS) are simple modular arithmetic, improved computational efficiency is obtained.","Secret image sharing, Secret sharing, Simple modular arithmetic, AMBTC, Distortion-free",Ching-Nung Yang and Xiaotian Wu and Min-Jung Chung and Xuliang Zhang,https://www.sciencedirect.com/science/article/pii/S1047320322000359,https://doi.org/10.1016/j.jvcir.2022.103482,1047-3203,2022,103482,84,Journal of Visual Communication and Image Representation,AMBTC-based secret image sharing by simple modular arithmetic,article,YANG2022103482
"Hand pose estimation is a challenging task owing to the high flexibility and serious self-occlusion of the hand. Therefore, an optimized convolutional pose machine (OCPM) was proposed in this study to estimate the hand pose accurately. Traditional CPMs have two components, a feature extraction module and an information processing module. First, the backbone network of the feature extraction module was replaced by Resnet-18 to reduce the number of network parameters. Furthermore, an attention module called the convolutional block attention module (CBAM) is embedded into the feature extraction module to enhance the information extraction. Then, the structure of the information processing module was adjusted through a residual connection in each stage that consist of a series of continuous convolutional operations, and requires a dense fusion between the output from all previous stages and the feature extraction module. The experimental results on two public datasets showed that the OCPM network achieved excellent performance.","Convolutional block attention module (CBAM), Convolutional pose machine (CPM), 2D hand pose estimation, Resnet-18, Feature fusion",Tianhong Pan and Zheng Wang and Yuan Fan,https://www.sciencedirect.com/science/article/pii/S1047320322000219,https://doi.org/10.1016/j.jvcir.2022.103461,1047-3203,2022,103461,83,Journal of Visual Communication and Image Representation,Optimized convolutional pose machine for 2D hand pose estimation,article,PAN2022103461
"Fine-grained visual classification (FGVC) is a critical task in the field of computer vision. However, FGVC is full of challenges due to the large intra-class variation and small inter-class variation of the classes to be classified on an image. The key in dealing with the problem is to capture subtle visual differences from the image and effectively represent the discriminative features. Existing methods are often limited by insufficient localization accuracy and insufficient feature representation capabilities. In this paper, we propose a cross-layer progressive attention bilinear fusion (CPABF in short) method, which can efficiently express the characteristics of discriminative regions. The CPABF method involves three components: 1) Cross-Layer Attention (CLA) locates and reinforces the discriminative region with low computational costs; 2) The Cross-Layer Bilinear Fusion Module (CBFM) effectively integrates the semantic information from the low-level to the high-level 3) Progressive Training optimizes the parameters in the network to the best state in a delicate way. The CPABF shows excellent performance on the four FGVC datasets and outperforms some state-of-the-art methods.","Fine-grained visual classification, Feature fusion, Attention, Progressive",Chaoqing Wang and Yurong Qian and Weijun Gong and Junjong Cheng and Yongqiang Wang and Yuefei Wang,https://www.sciencedirect.com/science/article/pii/S1047320321002789,https://doi.org/10.1016/j.jvcir.2021.103414,1047-3203,2022,103414,82,Journal of Visual Communication and Image Representation,Cross-layer progressive attention bilinear fusion method for fine-grained visual classification,article,WANG2022103414
"Visual cryptography scheme can divide the secret image into several shares. It can be used to enhance the secure transmission of the secret image on the Internet. Most schemes cannot fully restore the secret color image and generate meaningful shares. This paper proposes two new schemes by using color XOR to solve these problems. The first proposed scheme generates meaningless shares. The second proposed scheme can generate nâ1 meaningful shares and a meaningless share. Based on the first proposed scheme, nâ1 shares are modified to color QR codes in the second proposed scheme. These color QR codes can be decoded by the general decoder instead of the standard decoder. All shares are performed the color XOR to restore the secret color image completely. This paper uses some experiments to test two proposed schemes. Experimental results show that the two proposed schemes are feasible.","Visual cryptography scheme, Color QR code, Color XOR",Jeng-Shyang Pan and Tao Liu and Hong-Mei Yang and Bin Yan and Shu-Chuan Chu and Tongtong Zhu,https://www.sciencedirect.com/science/article/pii/S1047320321002704,https://doi.org/10.1016/j.jvcir.2021.103405,1047-3203,2022,103405,82,Journal of Visual Communication and Image Representation,Visual cryptography scheme for secret color images with color QR codes,article,PAN2022103405
"With the development of deep networks in dealing with various visual tasks, the deep network based on binocular vision is expected to tackle the issue of stereoscopic image quality assessment. Here, we present a stereoscopic image quality assessment method using the deep network with four channels together, which takes the left view, right view, binocular summing view, and binocular differencing view as the inputs of the network. The visual features are enhanced through the concatenation in a weighted way, so that the binocular vision can be adequately included in the binocular addition and subtraction information. Compared with the state-of-the-art metrics, the proposed method exhibits relatively high performances on four benchmark databases.","Stereoscopic image quality assessment, Deep regression network, Binocular summing, Binocular differencing",Jinbin Hu and Xuejin Wang and Xiongli Chai and Feng Shao and Qiuping Jiang,https://www.sciencedirect.com/science/article/pii/S1047320321002820,https://doi.org/10.1016/j.jvcir.2021.103420,1047-3203,2022,103420,82,Journal of Visual Communication and Image Representation,Deep network based stereoscopic image quality assessment via binocular summing and differencing,article,HU2022103420
"Multispectral satellites that measure the reflected energy from the different regions on the Earth generate the multispectral (MS) images continuously. The following MS image for the same region can be acquired with respect to the satellite revisit period. The images captured at different times over the same region are called multitemporal images. Traditional compression methods generally benefit from spectral and spatial correlation within the MS image. However, there is also a temporal correlation between multitemporal images. To this end, we propose a novel generative adversarial network (GAN) based prediction method called MultiTempGAN for compression of multitemporal MS images. The proposed method defines a lightweight GAN-based model that learns to transform the reference image to the target image. Here, the generator parameters of MultiTempGAN are saved for the reconstruction purpose in the receiver system. Due to MultiTempGAN has a low number of parameters, it provides efficiency in multitemporal MS image compression. Experiments were carried out on three Sentinel-2 MS image pairs belonging to different geographical regions. We compared the proposed method with JPEG2000-based conventional compression methods and three deep learning methods in terms of signal-to-noise ratio, mean spectral angle, mean spectral correlation, and laplacian mean square error metrics. Additionally, we have also evaluated the change detection performances and visual maps of the methods. Experimental results demonstrate that MultiTempGAN not only achieves the best metric values among the other methods at high compression ratios but also presents convincing performances in change detection applications.","Multispectral image compression, Generative adversarial networks, Big data, Remote sensing, Multitemporal images",Ali Can Karaca and Ozan Kara and Mehmet Kemal GÃ¼llÃ¼,https://www.sciencedirect.com/science/article/pii/S1047320321002546,https://doi.org/10.1016/j.jvcir.2021.103385,1047-3203,2021,103385,81,Journal of Visual Communication and Image Representation,MultiTempGAN: Multitemporal multispectral image compression framework using generative adversarial networks,article,KARACA2021103385
"Automatic retrieval of faces from videos based on query images effectively helps during the investigation. When the suspectâs image is unavailable, a face sketch, drawn based on eyewitnessâs memory recollection, is used to search against photos. Present research works primarily focus on Heterogeneous Face Matching (HFM) sketches to mugshot images in databases. This paper proposes a sketch-face matching in a video that includes profile faces, different illumination, and poses, using a new Generative Adversarial Network called SpyGAN. Faces in the video detected using YOLOv3 are converted into realistic sketches by the proposed SpyGAN focusing on key facial regions. The generated sketches are represented using PCA-SIFT descriptors and are matched based on the cosine distance metric. Experimental results show that the proposed methodology has achieved an accuracy of 88.9% on the Chokepoint dataset and 78% on the OWN Short face-video linked dataset and has demonstrated effectiveness over the state-of-the-art methods.","SpyGAN, Heterogeneous face matching, Generated sketch, Illumination, OWN short face-video linked dataset",Yogameena B. and Geeta Jakkamsetti and Aishwarya S.,https://www.sciencedirect.com/science/article/pii/S1047320321002662,https://doi.org/10.1016/j.jvcir.2021.103400,1047-3203,2022,103400,82,Journal of Visual Communication and Image Representation,SpyGAN sketch: Heterogeneous Face Matching in video for crime investigation,article,B2022103400
"Recently, techniques that can automatically figure out the incisive information from gigantic visual databases are urging popularity. The existing multi-feature hashing method has achieved good results by fusing multiple features, but in processing these multi-features, fusing multi-features into one feature will cause the feature dimension to be very high, increasing the amount of calculation. On the one hand, it is not easy to discover the internal ties between different features. This paper proposes a novel unsupervised multiple feature hashing for image retrieval and indexing (MFHIRI) method to learn multiple views in a composite manner. The proposed scheme learns the binary codes of various information sources in a composite manner, and our scheme relies on weighted multiple information sources and improved KNN concept. In particular, here we adopt an adaptive weighing scheme to preserve the similarity and consistency among binary codes. Precisely, we follow the graph modeling theory to construct improved KNN concept, which further helps preserve different statistical properties of individual sources. The important aspect of improved KNN scheme is that we can find the neighbors of a data point by searching its neighborsâ neighbors. During optimization, the sub-problems are solved in parallel which efficiently lowers down the computation cost. The proposed approach shows consistent performance over state-of-the-art (three single-view and eight multi-view approaches) on three broadly followed datasets viz. CIFAR-10, NUS-WIDE and Caltech-256.","Computer vision, Image indexing, Multi-view hashing, Approximate nearest neighbor search, Feature fusion, Graph theory",Saurabh Sharma and Vishal Gupta and Mamta Juneja,https://www.sciencedirect.com/science/article/pii/S1047320322000220,https://doi.org/10.1016/j.jvcir.2022.103467,1047-3203,2022,103467,84,Journal of Visual Communication and Image Representation,A novel unsupervised multiple feature hashing for image retrieval and indexing (MFHIRI),article,SHARMA2022103467
"Facial expression recognition (FER) is an active research area that has attracted much attention from both academics and practitioners of different fields. In this paper, we investigate an interesting and challenging issue in FER, where the training and testing samples are from a cross-domain dictionary. In this context, the data and feature distribution are inconsistent, and thus most of the existing recognition methods may not perform well. Given this, we propose an effective dynamic constraint representation approach based on cross-domain dictionary learning for expression recognition. The proposed approach aims to dynamically represent testing samples from source and target domains, thereby fully considering the feature elasticity in a cross-domain dictionary. We are therefore able to use the proposed approach to predict class information of unlabeled testing samples. Comprehensive experiments carried out using several public datasets confirm that the proposed approach is superior compared to some state-of-the-art methods.","Facial expression recognition, Cross-domain dictionary learning, Dynamic constraint representation",Zhe Sun and Raymond Chiong and Zheng-ping Hu and Sandeep Dhakal,https://www.sciencedirect.com/science/article/pii/S1047320322000189,https://doi.org/10.1016/j.jvcir.2022.103458,1047-3203,2022,103458,85,Journal of Visual Communication and Image Representation,A dynamic constraint representation approach based on cross-domain dictionary learning for expression recognition,article,SUN2022103458
"Considering the high requirements for omnidirectional video compression, we propose an objective quality evaluation method to assess quality loss in encoding omnidirectional videos. According to characteristics of 360Â° videos, we consider multi-space signal characterization (MSSC) to fully characterize the distortions of video signals from spatial/image domains to frequency domains and from image content to motion information, and further consider multi-channel information aggregation (MCIA) to fuse scores from multiple projection planes and temporal divided groups. The main innovation of our method is to establish a universal framework in bridging the connection between typical quality assessment and 360Â° quality assessment to measure 360Â° video quality effectively and efficiently. Experimental results show that our method outperforms state-of-the-art 2D quality metrics and quality metrics for omnidirectional images.","Omnidirectional video quality assessment, Image quality assessment, Compression distortion, Signal characterization, Information aggregation",Xiongli Chai and Feng Shao,https://www.sciencedirect.com/science/article/pii/S1047320321002819,https://doi.org/10.1016/j.jvcir.2021.103419,1047-3203,2022,103419,82,Journal of Visual Communication and Image Representation,M2OVQA: Multi-space signal characterization and multi-channel information aggregation for quality assessment of compressed omnidirectional videos,article,CHAI2022103419
"In this paper a general framework to adopt different predictors for reversible data hiding in the encrypted image is presented. Employing linear regression, we propose innovative predictors that contribute more significantly to accomplish more payload than conventional ones. Reserving room before encryption (RRBE) is designated in the proposed scheme making possible to attain high embedding capacity. In RRBE procedure, pre-processing is allowed before image encryption. In our scheme, pre-processing comprises of three main steps: computing prediction-errors, blocking and labeling of the errors. By blocking, we obviate the need for lossless compression to when a content owner is not enthusiastic. Lossless compression is employed in recent state of the art schemes to improve payload. We surpass the prior arts exploiting proper predictors, more efficient labeling procedure and blocking of the prediction-errors.","Encrypted image, Local difference, Prediction-errors, Reversible data hiding",Ammar Mohammadi,https://www.sciencedirect.com/science/article/pii/S1047320322000347,https://doi.org/10.1016/j.jvcir.2022.103478,1047-3203,2022,103478,85,Journal of Visual Communication and Image Representation,A general framework for reversible data hiding in encrypted images by reserving room before encryption,article,MOHAMMADI2022103478
"Aiming at the performance degradation of the existing presentation attack detection methods due to the illumination variation, a two-stream vision transformers framework (TSViT) based on transfer learning in two complementary spaces is proposed in this paper. The face images of RGB color space and multi-scale retinex with color restoration (MSRCR) space are fed to TSViT to learn the distinguishing features of presentation attack detection. To effectively fuse features from two sources (RGB color space images and MSRCR images), a feature fusion method based on self-attention is built, which can effectively capture the complementarity of two features. Experiments and analysis on Oulu-NPU, CASIA-MFSD, and Replay-Attack databases show that it outperforms most existing methods in intra-database testing and achieves good generalization performance in cross-database testing.","Presentation attack detection, Multi-scale retinex with color restoration, Vision transformer, Deep learning, Feature fusion",Fei Peng and Shao-hua Meng and Min Long,https://www.sciencedirect.com/science/article/pii/S1047320322000621,https://doi.org/10.1016/j.jvcir.2022.103518,1047-3203,2022,103518,85,Journal of Visual Communication and Image Representation,Presentation attack detection based on two-stream vision transformers with self-attention fusion,article,PENG2022103518
"In object tracking applications, it is common for trackers to experience drift problems when the object of interest becomes deformed, which compromises the ability of the tracker to track the object. It is therefore desirable to develop a learning tracker classifier that is robust to deformations. The performance of existing trackers that employ deep classification networks degrades when the amount of training data is limited and does not cover all possible scenarios. While these limitations can be mitigated in part by using larger training datasets, these datasets may still not cover all situations and the positive samples are still monotonous. To overcome this problem, we propose a novel deformation samples generator that generates samples that would normally be difficult for the tracker to classify. In the proposed framework, both the classifier and deformation samples generator learn in a joint manner. Our experiments show that the proposed approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations for the visual object tracking task.","Object tracking, Convolutional neural network, Deformation samples generator, Alternating strategy",Xuesong Gao and Yuan Zhou and Shuwei Huo and Zizi Li and Keqiu Li,https://www.sciencedirect.com/science/article/pii/S1047320322000104,https://doi.org/10.1016/j.jvcir.2022.103446,1047-3203,2022,103446,83,Journal of Visual Communication and Image Representation,Robust object tracking via deformation samples generator,article,GAO2022103446
"Human activity recognition is one of the most studied topics in the field of computer vision. In recent years, with the availability of RGB-D sensors and powerful deep learning techniques, research on human activity recognition has gained momentum. From simple human atomic actions, the research has advanced towards recognizing more complex human activities using RGB-D data. This paper presents a comprehensive survey of the advanced deep learning based recognition methods and categorizes them in human atomic action, humanâhuman interaction, humanâobject interaction. The reviewed methods are further classified based on the individual modality used for recognition i.e. RGB based, depth based, skeleton based, and hybrid. We also review and categorize recent challenging RGB-D datasets for the same. In addition, the paper also briefly reviews RGB-D datasets and methods for online activity recognition. The paper concludes with a discussion on limitations, challenges, and recent trends for promising future directions.","Human action recognition, CNN, LSTM, Humanâhuman interaction, Humanâobject interaction, Deep learning, RGB-D sensors, Multi-modality, Fusion, Skeleton, GCN",Pushpajit Khaire and Praveen Kumar,https://www.sciencedirect.com/science/article/pii/S1047320322000724,https://doi.org/10.1016/j.jvcir.2022.103531,1047-3203,2022,103531,86,Journal of Visual Communication and Image Representation,"Deep learning and RGB-D based human action, humanâhuman and humanâobject interaction recognition: A survey",article,KHAIRE2022103531
"Object trackers based on Siamese network usually transform the tracking task into a matching problem between the candidate samples and the target template. However, with the increasing depth and width of backbone networks, researches on Siamese trackers using backbone networks are not very advanced. Therefore, it is necessary for us to further investigate the characteristics of backbone network. As a fact, the ability of backbone network to extract features can directly determine the performance of object tracker. Given this, in this paper, we first propose an asymmetric convolutional network to improve the representational capability of backbone network. And then, the strip convolution is employed to enhance the operational capability of square kernel convolution in the backbone network. Besides, we also construct a novel module named Feature Dropblock (i.e., FD) to simulate the occlusion of hidden space, which goal is to improve the performance of backbone network in the target tracking under occlusion. To demonstrate the effectiveness of the proposed tracker, extensive ablation studies are conducted. Better results are obtained on the tracking benchmarks OTB100 and VOT2018, compared to other state-of-the-art trackers.","Visual tracking, Siamese network, Asymmetric convolution, FD, Occlusion target",Zhen Yang and Chaohe Wen and Lingkun Luo and Hongping Gan and Tao Zhang,https://www.sciencedirect.com/science/article/pii/S1047320322000207,https://doi.org/10.1016/j.jvcir.2022.103465,1047-3203,2022,103465,84,Journal of Visual Communication and Image Representation,ACSiam: Asymmetric convolution structures for visual tracking with Siamese network,article,YANG2022103465
"This paper proposes AMEA-GAN, an attention mechanism enhancement algorithm. It is cycle consistency-based generative adversarial networks for single image dehazing, which follows the mechanism of the human retina and to a great extent guarantees the color authenticity of enhanced images. To address the color distortion and fog artifacts in real-world images caused by most image dehazing methods, we refer to the human visual neurons and use the attention mechanism of similar Horizontal cell and Amazon cell in the retina to improve the structure of the generator adversarial networks. By introducing our proposed attention mechanism, the effect of haze removal becomes more natural without leaving any artifacts, especially in the dense fog area. We also use an improved symmetrical structure of FUNIE-GAN to improve the visual color perception or the color authenticity of the enhanced image and to produce a better visual effect. Experimental results show that our proposed model generates satisfactory results, that is, the output image of AMEA-GAN bears a strong sense of reality. Compared with state-of-the-art methods, AMEA-GAN not only dehazes images taken in daytime scenes but also can enhance images taken in nighttime scenes and even optical remote sensing imagery.","Image dehazing, Attention mechanism, Deep learning, Generative adversarial networks",Yan Liu and Hassan Al-Shehari and Hongying Zhang,https://www.sciencedirect.com/science/article/pii/S1047320321002935,https://doi.org/10.1016/j.jvcir.2021.103434,1047-3203,2022,103434,83,Journal of Visual Communication and Image Representation,Attention mechanism enhancement algorithm based on cycle consistent generative adversarial networks for single image dehazing,article,LIU2022103434
"Gaze prediction is a significant approach for processing a large amount of incoming visual information of videos. Recent gaze prediction algorithms often employ sparse models with the assumption that every superpixel in the video frames can be represented as linear combinations of a few salient superpixels. However, they are not actuated enough because of the insufficient knowledge that video signals contain a non-negative request. Hence, we develop a novel gaze prediction based on an inverse sparse coding framework with a determinant sparse measure. By introducing this sparse measure, the solutions are non-negative and sparser than conventional sparse constraints. However, the proposed optimization problem becomes nonconvex, which is difficult to solve. To efficiently address the corresponding nonconvex optimization problem, we propose a novel algorithm based on the difference in convex function programming, which can yield the global solutions. Experimental results indicate the improved accuracy of the proposed approach compared with state-of-the-art algorithms.","Gaze prediction, First-person video, Sparse coding, Determinant measure, Inverse",Yujie Li and Benying Tan and Shotaro Akaho and Hideki Asoh and Shuxue Ding,https://www.sciencedirect.com/science/article/pii/S1047320321002418,https://doi.org/10.1016/j.jvcir.2021.103367,1047-3203,2021,103367,81,Journal of Visual Communication and Image Representation,Gaze prediction for first-person videos based on inverse non-negative sparse coding with determinant sparse measure,article,LI2021103367
"Visual cryptography scheme with essential shadows (EVCS) is of great significance since it provides different levels of the importance to shadows. In this paper, we propose a general construction method for (t, s, k, n)-VCS with essential shadows based on XOR operation ((t, s, k, n)-EXVCS), which originates from the partition of access structure. The secret image is encrypted into s essential shadows and n-s non-essential shadows. Any k shadows including at least t essentials can cooperate to decode the secret image and the decoding process is implemented by XOR operation on the involved shadows. Our scheme achieves perfectly reconstruction of secret image in the revealed image and the less size of shadows and revealed images. The experiments are conducted to testify the feasibility and practicability of the proposed scheme.","XOR operation, Essential shadows, Visual cryptography, Secret image sharing, Shadow image, Access structure partition",Peng Li and Liping Yin and Jianfeng Ma and Hongtao Wang,https://www.sciencedirect.com/science/article/pii/S1047320322000608,https://doi.org/10.1016/j.jvcir.2022.103513,1047-3203,2022,103513,85,Journal of Visual Communication and Image Representation,XOR-based visual cryptography scheme with essential shadows,article,LI2022103513
"Most existing image restoration methods based on deep neural networks are developed for images which only degraded by a single degradation mode and imaging under an ideal condition. They cannot be directly used to restore the images degraded by multi-factor coupling. A complex task decomposition regularization optimization strategy (TDROS) is proposed to solve the problem. The restoration of images degraded by multi-factor coupling is a complex task that can be solved by separating these multiple factors, that is, breaking the complex task into numbers of simpler tasks to make the entire complex problem be overcome more easily. Motivated by this idea, the TDROS decomposes the complex task of image restoration into two sub-task: the potential task constrained by regularization and the main task for reconstructing high-definition images. In TDROS, the front of the neural network is focused on the restoration of images degraded by additive noise, while the other part of the network is focused mainly on the restoration of images degraded by blur. We applied the TDROS to an 11-layer convolutional neural network (CNN) and compared it with initial CNNs from the aspects of restoration accuracy and generalization ability. Based on these results, we used TDROS to design a novel network model for the restoration of atmospheric turbulence-degraded images. The experimental results demonstrate that the proposed TDROS can improve the generalization ability of the existing network more effectively than current popular methods, offering a better solution for the problem of severely degraded image restoration. Moreover, the TDROS concept provides a flexible framework for low-level visual complex tasks and can be easily incorporated into existing CNNs.","Complex task, Decomposition regularization, Convolutional neural network, Atmospheric turbulence, Blind restoration",Gongping Chen and Zhisheng Gao and Bin Zhou and Chenglin Zuo,https://www.sciencedirect.com/science/article/pii/S1047320321002534,https://doi.org/10.1016/j.jvcir.2021.103384,1047-3203,2022,103384,82,Journal of Visual Communication and Image Representation,Optimization and regularization of complex task decomposition for blind removal of multi-factor degradation,article,CHEN2022103384
"This paper proposes a novel two-stream encoderâdecoder network that utilizes both the high-level and the low-level image features for precisely localizing forged regions in a manipulated image. This is motivated by the fact that the forgery creation process generally introduces both the high-level artefacts (e.g., unnatural contrast) and the low-level artefacts (e.g., noise inconsistency) to the forged images. In the proposed two-stream network, one stream learns the low-level manipulation-related features in the encoder side by extracting noise residuals through a set of high-pass filters in the first layer. In the second stream, the encoder learns the high-level image manipulation features from the input image RGB values. The coarse feature maps each encoder are upsampled by the corresponding decoder network to produce the dense feature maps. The dense feature maps of the two streams are concatenated and fed to a final convolutional layer with sigmoidal activation to produce the pixel-wise prediction. We have carried out experimental analyses on multiple standard forensics datasets to evaluate the performance of the proposed method. The experimental results show the efficacy of the proposed method with respect to the state-of-the-art.","Image forensics, Forgery localization, CNN, Encoderâdecoder network",Aniruddha Mazumdar and Prabin Kumar Bora,https://www.sciencedirect.com/science/article/pii/S1047320321002777,https://doi.org/10.1016/j.jvcir.2021.103417,1047-3203,2022,103417,82,Journal of Visual Communication and Image Representation,Two-stream encoderâdecoder network for localizing image forgeries,article,MAZUMDAR2022103417
"Learning-based shadow detection methods have achieved an impressive performance, while these works still struggle on complex scenes, especially ambiguous soft shadows. To tackle this issue, this work proposes an efficient shadow detection network (ESDNet) and then applies uncertainty analysis and graph convolutional networks for detection refinement. Specifically, we first aggregate global information from high-level features and harvest shadow details in low-level features for obtaining an initial prediction. Secondly, we analyze the uncertainty of our ESDNet for an input shadow image and then take its intensity, expectation, and entropy into account to formulate a semi-supervised graph learning problem. Finally, we solve this problem by training a graph convolution network to obtain the refined detection result for every training image. To evaluate our method, we conduct extensive experiments on several benchmark datasets, i.e., SBU, UCF, ISTD, and even on soft shadow scenes. Experimental results demonstrate that our strategy can improve shadow detection performance by suppressing the uncertainties of false positive and false negative regions, achieving state-of-the-art results.","Deep learning, Image processing, Shadow detection, Uncertainty quantification, Graph convolution networks",Wen Wu and Kai Zhou and Xiao-Diao Chen,https://www.sciencedirect.com/science/article/pii/S1047320321002650,https://doi.org/10.1016/j.jvcir.2021.103397,1047-3203,2022,103397,82,Journal of Visual Communication and Image Representation,Single image shadow detection via uncertainty analysis and GCN-based refinement strategy,article,WU2022103397
"For the exemplar-based image inpainting problem, the filling order and local intensity smoothness are two crucial factors that should be considered carefully. This work gives a new exemplar-based image inpainting method, preventing geometric structures from being destroyed and reconstructing textures well to obtain elegant-looking outputs. For a better filling order, we define a new adaptive two-stage structure-tensor based priority function. To promote the local intensity smoothness, we adopt a non-local way, and at the same time, propose a weighted filter based on a Gaussian-like function to generate the ideal filling patch by combining non-local patches. We compare the proposed method with some recent state-of-the-art image inpainting approaches on different tasks, such as texture and structure synthesis, object removal, and remote sensing images inpainting. Experimental results demonstrate the superiority of the proposed method, both visually and quantitatively.","Exemplar-based image inpainting, Non-local texture matching, Structure-tensor, Texture and structure synthesis, Object removal, Remote sensing image inpainting",Ting Xu and Ting-Zhu Huang and Liang-Jian Deng and Xi-Le Zhao and Jin-Fan Hu,https://www.sciencedirect.com/science/article/pii/S1047320321002893,https://doi.org/10.1016/j.jvcir.2021.103430,1047-3203,2022,103430,83,Journal of Visual Communication and Image Representation,Exemplar-based image inpainting using adaptive two-stage structure-tensor based priority function and nonlocal filtering,article,XU2022103430
"Current imaging devices coupled with advanced hardware and software are smart enough to enhance low light images taken in clear weather. But in hazy or foggy environments, the captured images are of degraded quality. To address this issue, image processing algorithms are employed to enhance the degraded images to make useful for extracting meaningful features. In this study, we propose a haze removal algorithm to improve the color and contrast of images captured in hazy environments. The first step involves generation of images with various exposures using the theory of dynamic stochastic resonance. The images are then fused in a multi-scale fusion framework crafting weight maps viz. haze density, chromaticity, and luminance gradient. The fusion process focuses on uniformly enhancing the dark and bright regions of the image. However, it may overemphasize haze affected regions. Therefore, in the second step, the atmospheric scattering equation is referred and its modified version is applied that accomplishes the haze removal task. Quantitative and qualitative analyses demonstrate the effectiveness of the proposed method.","Dynamic stochastic resonance, Image dehazing, Restoration, Weight maps, Multi-scale fusion, Modified atmospheric scattering equation",Avishek Kumar and Rajib Kumar Jha and Naveen K. Nishchal,https://www.sciencedirect.com/science/article/pii/S1047320321002492,https://doi.org/10.1016/j.jvcir.2021.103376,1047-3203,2021,103376,81,Journal of Visual Communication and Image Representation,A multi-exposure fusion framework for contrast enhancement of hazy images employing dynamic stochastic resonance,article,KUMAR2021103376
"With the rapid popularity of multi-camera networks, one human action is usually captured by multiple cameras located at different angles simultaneously. Multi-camera videos contain the distinct perspectives of one action, therefore multiple views can overcome the impacts of illumination and occlusion. In this paper, we propose a novel multi-camera video clustering model, named Shareability-Exclusivity Representation on Product Grassmann Manifolds (PGM-SER), to address two key issues in traditional multi-view clustering methods (MVC): (1) Most MVC methods directly construct a shared similarity matrix by fusing multi-view data or their corresponding similarity matrices, which ignores the exclusive information in each view; (2) Most MVC methods are designed for multi-view vectorial data, which cannot handle the nonlinear manifold structure hidden in multi-camera videos. The proposed PGM-SER firstly adopts Product Grassmann Manifolds to represent multi-camera videos, then simultaneously learn their shared and exclusive information in global structures to achieve multi-camera video clustering. We provide an effective optimization algorithm to solve PGM-SER and present the corresponding convergence analysis. Finally, PGM-SER is tested on three multi-camera human action video datasets and obtain satisfied experimental results.","Multi-camera video clustering, Grassmann manifolds, Product Grassmann manifolds",Yongli Hu and Cuicui Luo and Junbin Gao and Boyue Wang and Yanfeng Sun and Baocai Yin,https://www.sciencedirect.com/science/article/pii/S1047320322000177,https://doi.org/10.1016/j.jvcir.2022.103457,1047-3203,2022,103457,84,Journal of Visual Communication and Image Representation,Shareability-Exclusivity Representation on Product Grassmann Manifolds for Multi-camera video clustering,article,HU2022103457
"Computer generated (CG) images have been gradually overspread on the Internet, resulting in difficult discrimination from natural images (NIs) captured by an authentic imaging device. Although some discriminators can deal with NIs in JPEG format, the classification between uncompressed NIs (that are possibly generated in any imaging procedure before compression) and CG ones still remains unknown. Thus, this paper aims to establish multiple discriminators classifying between NIs and CG images. We first describe the main imaging procedure and its intrinsic property, which characterizes the discriminative features for classification. Then, the residual noise (representing intrinsic characteristic) is extracted. Its statistical distribution indeed helps us establish multiple discriminators, consisting of the generalized likelihood ratio test (GLRT) under the framework of hypothesis testing theory. Extensive experiments empirically verify our proposed multiple discriminators outperform many prior arts. Furthermore, the robustness of discriminators is validated with considering some post-processing attacks.","Image origin forensics, Imaging procedure, Distribution model, Likelihood ratio test",Tong Qiao and Xiangyang Luo and Hongwei Yao and Ran Shi,https://www.sciencedirect.com/science/article/pii/S1047320322000578,https://doi.org/10.1016/j.jvcir.2022.103506,1047-3203,2022,103506,85,Journal of Visual Communication and Image Representation,Classifying between computer generated and natural images: An empirical study from RAW to JPEG format,article,QIAO2022103506
"Vision-based gait emerged as the preferred biometric in smart surveillance systems due to its unobtrusive nature. Recent advancements in low-cost depth sensors resulted in numerous 3D skeleton-based gait analysis techniques. For spatialâtemporal analysis, existing state-of-the-art algorithms use frame-level information as the timestamp. This paper proposes gait event-level spatialâtemporal features and LSTM-based deep learning model that treats each gait event as a timestamp to identify individuals from walking patterns observed in single and multi-view scenarios. On four publicly available datasets, the proposed system stands superior to state-of-the-art approaches utilizing a variety of conventional benchmark protocols. The proposed system achieved a recognition rate of greater than 99% in low-level ranks during the CMC test, making it suitable for practical applications. The statistical study of gait event-level features demonstrated retrieved featuresâ discriminating capacity in classification. Additionally, the ANOVA test performed on findings from K folds demonstrated the proposed systemâs significance in human identification.","Biometric, Deep learning, Gait recognition, Human identification, Long Short Term Memory (LSTM), Smart surveillance",Rashmi M. and Ram Mohana Reddy Guddeti,https://www.sciencedirect.com/science/article/pii/S1047320321002807,https://doi.org/10.1016/j.jvcir.2021.103416,1047-3203,2022,103416,82,Journal of Visual Communication and Image Representation,Human identification system using 3D skeleton-based gait features and LSTM model,article,M2022103416
"Currently, position check-in on mobile devices has become a fashionable social activity. Meanwhile, criminals probably tamper the geographical position (geo-position) information to provide an alibi. Therefore, it is of importance to identify the authenticity of geo-position. To our knowledge, many current methods for geo-position spoofing detection mainly rely on geo-position information in the database. However, these methods possibly fail in the case of missing prior information or lacking rich training samples. To address that challenge, this paper proposes an alternative manner for detecting the geo-position spoofing via camera sensor fingerprint. In particular, the camera sensor fingerprint is first extracted through the images posted by an inquiry user based on the well-designed denoising filter. Second, the authenticity of the geo-position is verified by comparing the consistency of the residual noise from newly-posted images with position check-in and the unique camera sensor fingerprint from an inquiry user. Finally, the extensive experiments are conducted on the image database, that empirically indicates the relevance of our proposed simple but effective method.","Geographical position spoofing detection, Source identification, Camera sensor fingerprint",Tong Qiao and Qianru Zhao and Ning Zheng and Ming Xu and Li Zhang,https://www.sciencedirect.com/science/article/pii/S104732032100211X,https://doi.org/10.1016/j.jvcir.2021.103320,1047-3203,2021,103320,81,Journal of Visual Communication and Image Representation,Geographical position spoofing detection based on camera sensor fingerprint,article,QIAO2021103320
"A fundamental requirement for designing compression artifact reduction techniques is to restore the artifact free image from its compressed version regardless of the compression level. Most existing algorithms require the prior knowledge of JPEG encoding parameters to operate effectively. Although there are works that attempt to train universal models to deal with different compression levels, some JPEG quality factors (QF) are still missing. To overcome these potential limitations, in this paper, we present a generalized JPEG-compression artifact reduction framework that relies on improved QF estimator and rectified networks to take into account all possible QF values. Our method, called a generalized compression artifact reducer (G-CAR), first predicts QF by analyzing luminance patches with high activity. Then, based on the estimated QF, images are adaptively restored by the cascaded residual encoderâdecoder networks learned in multiple domains. Results tested on six benchmark datasets demonstrate the effectiveness of our proposed model.","Artifact reduction, Residual network, Compression artifact, Quality factor estimation",Yi Zhang and Damon M. Chandler and Xuanqin Mou,https://www.sciencedirect.com/science/article/pii/S104732032100287X,https://doi.org/10.1016/j.jvcir.2021.103425,1047-3203,2022,103425,83,Journal of Visual Communication and Image Representation,Multi-domain residual encoderâdecoder networks for generalized compression artifact reduction,article,ZHANG2022103425
"Single image dehazing has great significance in computer vision. In this paper, we propose a novel unsupervised Dark Channel Attention optimized CycleGAN (DCA-CycleGAN) to deal with the challenging scene with uneven and dense haze concentration. Firstly, the DCA-CycleGAN adopts the dark channel as input and then generate attention through a DCA subnetwork to handle the nonhomogeneous haze. Secondly, in addition to the conventional global discriminator, we also leverage two local discriminators to enhance the dehazing performance on the local dense haze, and a new local adversarial loss calculated strategy is been proposed. Specifically, the dehazing generator consists of two subnetworks: an auto-encoder and a dark channel attention subnetwork. The auto-encoder consists of an encoder, a feature transformation module, and a decoder. The dark channel attention subnetwork has the same structure as the encoder and the feature transformation module to ensure the same receptive field, which utilizes the dark channel to generate attention map and fine-tune the auto-encoder. Experimental results against several state-of-the-art methods demonstrate that our method can generate better visual effects, and is effective.","Image dehazing, Adversarial network, Dark channel attention, Local discriminator",Yaozong Mo and Chaofeng Li and Yuhui Zheng and Xiaojun Wu,https://www.sciencedirect.com/science/article/pii/S1047320321002923,https://doi.org/10.1016/j.jvcir.2021.103431,1047-3203,2022,103431,82,Journal of Visual Communication and Image Representation,DCA-CycleGAN: Unsupervised single image dehazing using Dark Channel Attention optimized CycleGAN,article,MO2022103431
"In Content-based Image Retrieval (CBIR), the user provides the query image in which only a selective portion of the image carries the foremost vital information known as the object region of the image. However, the human visual system also focuses on a particular salient region of an image to instinctively understand its semantic meaning. Therefore, the human visual attention technique can be well imposed in the CBIR scheme. Inspired by these facts, we initially utilized the signature saliency map-based approach to decompose the image into its respective main object region (ObR) and non-object region (NObR). ObR possesses most of the vital image information, so block-level normalized singular value decomposition (SVD) has been used to extract salient features of the ObR. In most natural images, NObR plays a significant role in understanding the actual semantic meaning of the image. Accordingly, multi-directional texture features have been extracted from NObR using Gabor filter on different wavelengths. Since the importance of ObR and NObR features are not equal, a new homogeneity-based similarity matching approach has been devised to enhance retrieval accuracy. Finally, we have demonstrated retrieval performances using both the combined and distinct ObR and NObR features on seven standard coral, texture, object, and heterogeneous datasets. The experimental outcomes show that the proposed CBIR system has a promising retrieval efficiency and outperforms various existing systems substantially.","Content-based Image Retrieval (CBIR), Gabor filter, Object detection, Saliency map, Singular value decomposition (SVD)",Jitesh Pradhan and Arup Kumar Pal and Haider Banka,https://www.sciencedirect.com/science/article/pii/S1047320321002649,https://doi.org/10.1016/j.jvcir.2021.103396,1047-3203,2022,103396,83,Journal of Visual Communication and Image Representation,A CBIR system based on saliency driven local image features and multi orientation texture features,article,PRADHAN2022103396
"Underwater images usually contain severely blurred details, color distortion, and low contrast, warranting efficient methods to obtain clean images. However, most convolutional neural network-based approaches involve high computational cost, numerous model parameters, and even poor performance. Besides, the mapping from input to output is learned using a single path, ignoring the frequency domain information. To solve these challenges, we propose a novel progressive frequency-interleaved network (PFIN) for underwater imagery super-resolution and enhancement. Specifically, progressive frequency-domain module (PFDM) and convolution-guided module (CGM) constitute PFIN for effective color deviation correction and detail enhancement. PFDM that possesses global spatial attention, multi-scale residual, and frequency information modulation blocks gradually learn frequency features and explicitly compensate for detail loss. Furthermore, CGM comprising a series of convolution blocks generates discriminative characteristics to modulate in PFDM for better accommodating degraded representations. Extensive experiments demonstrate the superiority of our PFIN regarding quantitative evaluations and visual quality.","Underwater image, Super-resolution, Enhancement, Deep learning, Frequency domain",Li Wang and Lizhong Xu and Wei Tian and Yunfei Zhang and Hui Feng and Zhe Chen,https://www.sciencedirect.com/science/article/pii/S1047320322000839,https://doi.org/10.1016/j.jvcir.2022.103545,1047-3203,2022,103545,86,Journal of Visual Communication and Image Representation,Underwater image super-resolution and enhancement via progressive frequency-interleaved network,article,WANG2022103545
"Generative Adversarial Networks (GANs) have facilitated a new direction to tackle the image-to-image transformation problem. Different GANs use generator and discriminator networks with different losses in the objective function. Still there is a gap to fill in terms of both the quality of the generated images and close to the ground truth images. In this work, we introduce a new Image-to-Image Transformation network named Cyclic Discriminative Generative Adversarial Networks (CDGAN) that fills the above mentioned gaps. The proposed CDGAN generates high quality and more realistic images by incorporating the additional discriminator networks for cycled images in addition to the original architecture of the CycleGAN. The proposed CDGAN is tested over three image-to-image transformation datasets. The quantitative and qualitative results are analyzed and compared with the state-of-the-art methods. The proposed CDGAN method outperforms the state-of-the-art methods when compared over the three baseline Image-to-Image transformation datasets. The code is available at https://github.com/KishanKancharagunta/CDGAN.","Deep networks, Computer vision, Generative Adversarial Nets, Image-to-image transformation, Cyclic-Discriminative Adversarial loss",Kancharagunta {Kishan Babu} and Shiv Ram Dubey,https://www.sciencedirect.com/science/article/pii/S1047320321002522,https://doi.org/10.1016/j.jvcir.2021.103382,1047-3203,2022,103382,82,Journal of Visual Communication and Image Representation,CDGAN: Cyclic Discriminative Generative Adversarial Networks for image-to-image transformation,article,KISHANBABU2022103382
"Retrieving 3D shapes with 2D images has become a popular research area nowadays, and a great deal of work has been devoted to reducing the discrepancy between 3D shapes and 2D images to improve retrieval performance. However, most approaches ignore the semantic information and decision boundaries of the two domains, and cannot achieve both domain alignment and category alignment in one module. In this paper, a novel Collaborative Distribution Alignment (CDA) model is developed to address the above existing challenges. Specifically, we first adopt a dual-stream CNN, following a similarity guided constraint module, to generate discriminative embeddings for input 2D images and 3D shapes (described as multiple views). Subsequently, we explicitly introduce a joint domain-class alignment module to dynamically learn a class-discriminative and domain-agnostic feature space, which can narrow the distance between 2D image and 3D shape instances of the same underlying category, while pushing apart the instances from different categories. Furthermore, we apply a decision boundary refinement module to avoid generating class-ambiguity embeddings by dynamically adjusting inconsistencies between two discriminators. Extensive experiments and evaluations on two challenging benchmarks, MI3DOR and MI3DOR-2, demonstrate the superiority of the proposed CDA method for 2D image-based 3D shape retrieval task.","3D shape retrieval, Cross-domain retrieval, Multi-view learning",Nian Hu and Heyu Zhou and An-An Liu and Xiangdong Huang and Shenyuan Zhang and Guoqing Jin and Junbo Guo and Xuanya Li,https://www.sciencedirect.com/science/article/pii/S1047320321002881,https://doi.org/10.1016/j.jvcir.2021.103426,1047-3203,2022,103426,83,Journal of Visual Communication and Image Representation,Collaborative Distribution Alignment for 2D image-based 3D shape retrieval,article,HU2022103426
"Compressed sensing (CS) can recover an image from a few random measurements by exploiting the sparsity assumption on the structure of images. Some recent generative model-based CS recovery methods have removed the sparsity constraint, but their recovery process is slow and the recovered signal is constrained to be in the generator range. Here, we propose a new framework, called Proximal-Gen, for CS recovery. Specifically, we first formulate a general domain of the recovered signals, this allows the subsequent recovery algorithms to recover the signals that deviate from the generator range. Then based on the general domain, we develop a fast recovery algorithm, which mainly consists of two sub-algorithms, namely network-based projected gradient descent (NPGD) and denoiser-based proximal gradient descent (DPGD). The NPGD is used to obtain an intermediate signal lying in the generator range, while the DPGD is proposed to recover a deviation signal. Compared with multiple recent generative model-based recovery methods, our method can achieve better reconstruction performance and higher efficiency under most measurements.","Compressed sensing, Generative models, Generator range, Reconstruction efficiency",Lei Cai and Yuli Fu and Tao Zhu and Youjun Xiang and Huanqiang Zeng,https://www.sciencedirect.com/science/article/pii/S1047320321002364,https://doi.org/10.1016/j.jvcir.2021.103358,1047-3203,2022,103358,82,Journal of Visual Communication and Image Representation,Proximal-Gen for fast compressed sensing recovery,article,CAI2022103358
"Since a huge part of elderly people are living alone, assisted-living tools have become an essential in-home telemonitoring device. Hence, this paper proposes an automatic human fall detection in videos. In order to improve the system reliability, a new shape descriptor called multi-oriented run length (MORL) is proposed. This descriptor is exploited in a proposed scheme to generate static and dynamic features to represent human falls with complementary information. The generated static and dynamic features are fused through the Choquet fuzzy integral. Experimental results conducted on three well-known datasets containing almost 1300 video segments show an interesting adaptation of the proposed approaches. More precisely, the proposed MORL descriptor shows its superiority against known descriptors such as LBP and HOG. Moreover, Choquet fuzzy integral significantly improves the results versus standard combiners. In general, the obtained results highlight the reliability of the proposed system versus recent studies for human fall detection.","Multi-oriented run length, Static and dynamic features, Choquet fuzzy integral, Tree architecture, Human fall detection",Bilal Hadjadji and Matthieu Saumard and Michael Aron,https://www.sciencedirect.com/science/article/pii/S1047320321002480,https://doi.org/10.1016/j.jvcir.2021.103375,1047-3203,2022,103375,82,Journal of Visual Communication and Image Representation,Multi-oriented run length based static and dynamic features fused with Choquet fuzzy integral for human fall detection in videos,article,HADJADJI2022103375
"The more advanced multi-view extension, MV-HEVC, effectively exploits visual similarities between multi-view videos and enables high compression efficiency. Each view in the multi-view sequence depends on the captured scene, the distance between cameras and recording angles. Increasing the distance between dependent viewpoints generates an inter-view disparity. This impacts the inter-view similarities, affects the disparity estimation and further increases the computational complexity of the MV-HEVC encoder. In this paper, an efficient earlier disparity estimation is proposed for low complexity MV-HEVC. This algorithm is based on reducing the complexity of disparity estimation by eliminating the inter-view offset. Moreover, the inter-view similarities are controlled by considering the reliability of each coding unit size in the search range. This reliability is estimated by reducing the number of searching points within a new limited window. For reliable motion estimation, we further proposed an earlier decision of coding units splitting in the dependent views according to those in the reference views. Experimental results show that the proposed algorithm can achieve an average encoding time saving of 20.37%â40,61% with marginal performance degradation.","Inter-view offset, Disparity estimation, Coding unit, Inter-view correlations, Prediction unit",Amel Belbel and Amara Bekhouch and Noureddine Doghmane and Saliha Harize and Nasreddine Kouadria,https://www.sciencedirect.com/science/article/pii/S1047320322000682,https://doi.org/10.1016/j.jvcir.2022.103525,1047-3203,2022,103525,86,Journal of Visual Communication and Image Representation,Improved inter-view correlations for low complexity MV-HEVC,article,BELBEL2022103525
"Recently, Siamese trackers have received widespread attention for visual object tracking owing to their good balance between speed and performance. Those Siamese trackers heavily depend on target template while conventional practice fixes the template to initial frame. This strategy makes it unable to cope with variation of target appearance, which often leads to tracking failures and causes the gap in performance from other tracking methods. Despite the performance gain achieved by few template update methods with target templates generated by the tracked results, these tracked templates are easy to accumulate errors and cause tracking drift. In this paper, we propose two template update mechanisms to effectively adapt the target template during the tracking process which is dubbed as DTDU (Dynamic Template with Dual Update). Unlike predecessors that directly use the tracked template, we use initial template to perform similar transformation to the tracked template. Then the similar transformed template and the tracked template are combined linearly to capture the variation of target appearance. These updated templates are stored in a memory bank and retrieved to generate the final target template. In order to enhance quick update of memory bank to accommodate the target appearance, we use the retrieved template to further update the templates in memory bank for subsequent tracking. Extensive experiments on OTB-2015, VOT2016, VOT2018 and GOT-10k datasets have proved the effectiveness of these two update mechanisms and the proposed tracker achieves a real-time speed of 44 fps.","Visual object tracking, Siamese trackers, Template update mechanism, Real-time tracking",Jing Liu and Yating Wang and Xiangdong Huang and Yuting Su,https://www.sciencedirect.com/science/article/pii/S1047320322000165,https://doi.org/10.1016/j.jvcir.2022.103456,1047-3203,2022,103456,84,Journal of Visual Communication and Image Representation,Tracking by dynamic template: Dual update mechanism,article,LIU2022103456
"Most existing trackers are based on using a classifier and multi-scale estimation to estimate the target state. Consequently, and as expected, trackers have become more stable while tracking accuracy has stagnated. While trackers adopt a maximum overlap method based on an intersection-over-union (IoU) loss to mitigate this problem, there are defects in the IoU loss itself, that make it impossible to continue to optimize the objective function when a given bounding box is completely contained within/without another bounding box; this makes it very challenging to accurately estimate the target state. Accordingly, in this paper, we address the above-mentioned problem by proposing a novel tracking method based on a distance-IoU (DIoU) loss, such that the proposed tracker consists of target estimation and target classification. The target estimation part is trained to predict the DIoU score between the target ground-truth bounding-box and the estimated bounding-box. The DIoU loss can maintain the advantage provided by the IoU loss while minimizing the distance between the center points of two bounding boxes, thereby making the target estimation more accurate. Moreover, we introduce a classification part that is trained online and optimized with a Conjugate-Gradient-based strategy to guarantee real-time tracking speed. Comprehensive experimental results demonstrate that the proposed method achieves competitive tracking accuracy when compared to state-of-the-art trackers while with a real-time tracking speed.","Visual tracking, Bounding-box regression, Distance-IoU loss",Di Yuan and Xiu Shu and Nana Fan and Xiaojun Chang and Qiao Liu and Zhenyu He,https://www.sciencedirect.com/science/article/pii/S104732032100290X,https://doi.org/10.1016/j.jvcir.2021.103428,1047-3203,2022,103428,83,Journal of Visual Communication and Image Representation,Accurate bounding-box regression with distance-IoU loss for visual tracking,article,YUAN2022103428
"Visual-based target tracking is easily influenced by multiple factors, such as background clutter, targetsâ fast-moving, illumination variation, object shape change, occlusion, etc. These factors influence the tracking accuracy of a target tracking task. To address this issue, an efficient real-time target tracking method based on a low-dimension adaptive feature fusion is proposed to allow us the simultaneous implementation of the high-accuracy and real-time target tracking. First, the adaptive fusion of a histogram of oriented gradient (HOG) feature and color feature is utilized to improve the tracking accuracy. Second, a convolution dimension reduction method applies to the fusion between the HOG feature and color feature to reduce the over-fitting caused by their high-dimension fusions. Third, an average correlation energy estimation method is used to extract the relative confidence adaptive coefficients to ensure tracking accuracy. We experimentally confirm the proposed method on an OTB100 data set. Compared with nine popular target tracking algorithms, the proposed algorithm gains the highest tracking accuracy and success tracking rate. Compared with the traditional Sum of Template and Pixel-wise LEarners (STAPLE) algorithm, the proposed algorithm can obtain a higher success rate and accuracy, improving by 2.3% and 1.9%, respectively. The experimental results also demonstrate that the proposed algorithm can reach the real-time target tracking with 50+fps. The proposed method paves a more promising way for real-time target tracking tasks under a complex environment, such as appearance deformation, illumination change, motion blur, background, similarity, scale change, and occlusion.","Feature fusion, Target tracking, Real time",Yanyan Liu and Changcheng Pan and Minglin Bie and Jin Li,https://www.sciencedirect.com/science/article/pii/S1047320322000566,https://doi.org/10.1016/j.jvcir.2022.103505,1047-3203,2022,103505,85,Journal of Visual Communication and Image Representation,An efficient real-time target tracking algorithm using adaptive feature fusion,article,LIU2022103505
"Ultrasound image technology is to measure the energy and time of arrival of the reflected echo after the pulse acoustic signal is sent out by the ultrasonic wave. Usually, the distance between the ultrasonic source and the reflector is measured. Super-resolution image reconstruction aims to recover high-resolution images from one or more low-resolution images. Super-resolution image reconstruction is a software approach to solve the problem of low-resolution images by overcoming the limitations of hardware. In this paper, an image super-resolution reconstruction method based on sparse representation model and multi-feature fusion is proposed. Sparse dictionary is used to learn and reconstruct the luminance details of ultrasonic images, and edge interpolation is used to improve the edge clarity. Experimental results show that the proposed method is superior to Bicubic interpolation, SCSR and JOR in PSNR, and is 0.35â¯dB higher than RAISR. On the FSIM index, this method is also slightly better than other comparison methods, which can get better reconstruction results. Visual effects and numerical evaluation results of reconstructed images are better than several comparison methods.","Ultrasonic image, Super-resolution, Coefficient representation, Multi feature fusion",Wuli Song and Linna Li and Zihui Ren,https://www.sciencedirect.com/science/article/pii/S1047320319302548,https://doi.org/10.1016/j.jvcir.2019.102633,1047-3203,2019,102633,64,Journal of Visual Communication and Image Representation,Ultrasonic image processing based on fusion super-resolution reconstruction of familiar models,article,SONG2019102633
"Image classification plays an important role in computer vision and its applications, such as scene categorization, image retrieval. Convolutional neural network based methods have shown competitive performance in image classification, which aims to exploit deep feature of training images. In this paper, based on CNN methods and image quality assessment (IQA) algorithms, we propose a novel method for medical application, that is breast cancer classification. First, we leverage CNN architecture to calculate the number of pixels in the lesions, where maximum pooling layers are used. Then, large density of pixel regions will be assigned with large quality scores, which reflect more texture and grayscale features. Finally, we construct a multi-SVM based image kernel using obtained quality scores to achieve breast cancer classification. Experimental results show our proposed method outperforms single recognition based image classification methods such as pixel grayscale or gradient.","Image classification, CNN, Quality score",Yan Fang and Jing Zhao and Lingzhi Hu and Xiaoping Ying and Yanfang Pan and Xiaoping Wang,https://www.sciencedirect.com/science/article/pii/S1047320319302305,https://doi.org/10.1016/j.jvcir.2019.102609,1047-3203,2019,102609,64,Journal of Visual Communication and Image Representation,Image classification toward breast cancer using deeply-learned quality features,article,FANG2019102609
"Key pose recognition (KPR) is widely used in sport analysis, which provides effective tools for coaches, athletes and other professionals to conduct game analysis and auxiliary training. KPR from video stream can be divided into individual-oriented and group-oriented. The former method is based on the segmentation and tracking of each target, and the characteristics of the individual are used to study the events in the group. The latter is to process and sample the global image, obtain the overall information, and then process the collected data to classify the abnormal situation and normal situation. In this paper, we propose key pose recognition method based on deep learning. Specifically, we first train an FCN network for foreground extraction with weightlifting video frame images to remove a large amount of background interference in the weightlifting video image. Further, by fine-tuning the CNN network, a network model suitable for the weight-of-weight video classification of the region of interest is obtained. Finally, according to the classification result, the classification result selection strategy is designed to extract the key pose. In addition, our algorithm can select high image quality key pose frame, which is important for sports training. The experimental results show that the proposed method is very competitive.","Deep learning, Image processing, Motion model, Image quality",Cao Zhi-chao and Lingling Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319301853,https://doi.org/10.1016/j.jvcir.2019.06.013,1047-3203,2019,102571,63,Journal of Visual Communication and Image Representation,Key pose recognition toward sports scene using deeply-learned model,article,ZHICHAO2019102571
"Intelligent video surveillance systems have garnered substantial research attention in recent years within the transportation surveillance field. The systems can assist in identifying activities, interactions, and abnormal behaviors of individuals in traffic. We propose a novel unsupervised learning framework based on a two-layer BNBP-PFA topic model to simultaneously model multiple types of behaviors in crowded and complicated traffic videos. We provide the modelâs structure, its inference algorithm, and design a corresponding likelihood function based on an abnormality detection algorithm. Compared to similar existing algorithms, ours readily reveals both the local topic-motion pattern and the global topic-traffic pattern. Comparative experiments on two public traffic video datasets show that our model outperforms the state-of-art algorithms in regards to effective topic discovery and abnormality detection.","Nonparametric topic model, Motion pattern, Traffic pattern, Abnormality detection, Beta negative binomial process, Possion factor analysis",Houkui Zhou and Huimin Yu and Roland Hu and Guangqun Zhang and Junguo Hu and Tao He,https://www.sciencedirect.com/science/article/pii/S1047320319302706,https://doi.org/10.1016/j.jvcir.2019.102649,1047-3203,2019,102649,64,Journal of Visual Communication and Image Representation,Analyzing multiple types of behaviors from traffic videos via nonparametric topic model,article,ZHOU2019102649
"This paper introduces a novel video saliency model for salient object detection in videos. Firstly, we generate multi-level deep features via a symmetrical convolutional neural network, in which the inputs are the current frame and the optical flow image. Then, the multi-level deep features are integrated in a hierarchical manner using a fusion network, which deploys attention module to make a selection for deep features. Lastly, the integrated deep feature is combined with the boundary information originated from shallow layer of the feature extraction networks, and the saliency map is generated by the saliency prediction step. The key advantages of our model lie on the attention module, the hierarchical integration and the boundary information, in which the former one acts as weight filter and is used to select the most salient regions in deep features, the middle one gives an effective integration manner for features from different layers and the last one provides well-defined boundaries for saliency map. Extensive experiments are performed on two challenging video dataset, and the experimental results show that our model consistently outperforms the state-of-the-art saliency models in a large margin.","Video saliency, Convolutional network, Feature integration, Boundary",Hongfa Wen and Xiaofei Zhou and Yaoqi Sun and Jiyong Zhang and Chenggang Yan,https://www.sciencedirect.com/science/article/pii/S1047320319301737,https://doi.org/10.1016/j.jvcir.2019.05.018,1047-3203,2019,279--285,62,Journal of Visual Communication and Image Representation,Deep fusion based video saliency detection,article,WEN2019279
"Image quality assessment (IQA) is widely applied in image processing, such as image retrieval, image aesthetic evaluation and image classification. To expand application of IQA, we propose a novel method toward biology application using IQA for genetic research. The proposed approach breaks through limitations of traditional biology research, which integrates image processing algorithms with biology applications. Specifically, we first conduct the dataset acquisition including frozen semen images and their according biology scores that reflect genetic attributes. Then, each obtained image will be assigned a quality score according to its grayscale features and texture features. Afterward, we leverage BP neural network for deep feature extraction with fusing quality score and biology score as tags. Finally, given a test image, we can predict its genetic attribute according to deep-learned model. Comprehensive experiments conducted on goat genetic research demonstrate satisfied performance of proposed method.","Image quality assessment, Low-level features, Deep learning, BP neural network",Yanjun Zhang and Shuai Gong and Mingjiu Luo,https://www.sciencedirect.com/science/article/pii/S1047320319302275,https://doi.org/10.1016/j.jvcir.2019.102606,1047-3203,2019,102606,64,Journal of Visual Communication and Image Representation,Image quality guided biology application for genetic analysis,article,ZHANG2019102606
"To compress deep convolutional neural networks (CNNs) with large memory footprint and long inference time, this paper proposes a novel pruning criterion based on layer-wise Ln-norms of feature maps to identify unimportant convolutional kernels. We calculate the Ln-norm of the feature map outputted by each convolutional kernel to evaluate the importance of the kernel. Furthermore, we use different Ln-norms for different layers, e.g., L1-norm for the first convolutional layer, L2-norm for middle convolutional layers and Lâ-norm for the last convolutional layer. With the ability of accurately identifying unimportant convolutional kernels in each layer, the proposed method achieves a good balance between model size and inference accuracy. Experimental results on CIFAR, SVHN and ImageNet datasets and an application example in a railwayintelligent surveillance system show that the proposed method outperforms existing kernel-norm-based methods and is generally applicable to any deep neural network with convolutional operations.","Network compression, Convolutional neural network, Pruning criterion, Channel-level pruning",Wei Wang and Liqiang Zhu and Baoqing Guo,https://www.sciencedirect.com/science/article/pii/S1047320319302032,https://doi.org/10.1016/j.jvcir.2019.102582,1047-3203,2019,102582,63,Journal of Visual Communication and Image Representation,Reliable identification of redundant kernels for convolutional neural network compression,article,WANG2019102582
"Accurate prediction of power supply load is vital in power industry, which provides economic operation decision for the power operation department. For the unpredictability and periodicity of power load, nonlinear intelligent forecasting method is adopted. A modified firefly algorithm (MFA) combined with support vector machine (SVM) is proposed to predict the load of power supply data in this paper. The nonlinear mapping function is used to deal with the nonlinear regression problem in SVM, in which the parameters affect the accuracy of load prediction, so the MFA method is adopted to optimized the parameters of SVM. In order to verify the accuracy of SVM-MFA, mean absolute percentage error (eMAPE) was used as the fitness function for simulation and comparison experiment. The results show that the SVM-MFA proposed in this paper has stronger global search ability and faster convergence rate than the traditional artificial neural network, and it is verified that the method proposed in this paper has higher accuracy and higher stability of network load prediction.","Support vector machine, Improved firefly algorithm, Load forecasting, Nonlinear regression, Evaluation criteria",Wen Ma and Xinyang Zhang and Yong Xin and Shenzhang Li,https://www.sciencedirect.com/science/article/pii/S1047320319302676,https://doi.org/10.1016/j.jvcir.2019.102646,1047-3203,2019,102646,65,Journal of Visual Communication and Image Representation,Study on short-term network forecasting based on SVM-MFA algorithm,article,MA2019102646
"Image information management (IIM) is a key technique to improve the performance of large-scale image retrieval. However, IIM is still a big challenge due to the large sum of image datasets and traditional algorithms cannot cope with this problem. In order to solve these disadvantages, we propose a novel image classification algorithm based on image quality assessment (IQA) for image information management. Specifically, we first incorporate both low-level, high-level features as well as quality scores for image representation, where we leverage convolution neural network for deep feature extraction. Then, deep feature vector can be generated by column-wise stacking. Thus, each image can be represented by a feature vector. We leverage GMM to learn the distribution of obtained feature vectors. Similar image categories have similar probability distributions, we leverage the learned GMM model to calculate the posterior probability and image can be classified into corresponding category. Experimental results demonstrate the performance of our proposed method, and image information management is easier to implement.","Deep learning, Convolutional neural network, Image retrieval, Image quality assessment",Kuo-Min Ko and Po-Chang Ko and Shih-Yang Lin and Zhen Hong,https://www.sciencedirect.com/science/article/pii/S1047320319302159,https://doi.org/10.1016/j.jvcir.2019.102594,1047-3203,2019,102594,63,Journal of Visual Communication and Image Representation,Quality-guided image classification toward information management applications,article,KO2019102594
"A novel shape model of multi-scale topological features is proposed which considers those features relating to connected components and holes. This is achieved by considering the persistent homology of a pair of sublevel set functions corresponding to a pair of distance functions defined on the ambient space. The model is applicable to both single and multiple component shapes and, to the authors knowledge, is the first shape model to consider multi-scale topological features of multiple component shapes. It is demonstrated, both qualitatively and quantitatively, that the proposed model models useful multi-scale topological features and outperforms a commonly used benchmark models with respect to the task of multiple component shape retrieval.","Multiple component shapes, Topology, Multi-scale, Persistent homology",Padraig Corcoran and JoviÅ¡a Å½uniÄ and Paul L. Rosin,https://www.sciencedirect.com/science/article/pii/S104732031930238X,https://doi.org/10.1016/j.jvcir.2019.102617,1047-3203,2019,102617,64,Journal of Visual Communication and Image Representation,A multi-scale topological shape model for single and multiple component shapes,article,CORCORAN2019102617
"This paper proposes an adaptive dual-image-based reversible information hiding scheme. First, for a pixel pair in the original image, an associated square lattice is selected to determine the maximum distortion that will be induced by hiding information. Then, an embedding rule, which combines the selected square lattice with integer rounding, is constructed. In addition, a novel histogram shifting mechanism to solve the problems of overflow and underflow is designed to decrease the distortions that are caused by the shifting of a large number of pixels. In our approach, both the original image and the secret messages can be recovered losslessly. Our experimental results show that the proposed scheme achieved performance that was superior to the state-of-the-art schemes, resulting in embedding rates up to 1.40â¯bpp and average PSNRs of approximately 47.60â¯dB. The proposed scheme also can be used in real-time systems due to the simplicity of its computations.","Dual-image, Reversible information hiding, Embedding rate, Histogram shifting, Real-time",Guo-Dong Su and Yanjun Liu and Chin-Chen Chang,https://www.sciencedirect.com/science/article/pii/S1047320319302391,https://doi.org/10.1016/j.jvcir.2019.102618,1047-3203,2019,102618,64,Journal of Visual Communication and Image Representation,A square lattice oriented reversible information hiding scheme with reversibility and adaptivity for dual images,article,SU2019102618
"Deep learning has been widely applied in image processing and computer vision due to its powerful learning capability. Although some learning models have been proposed to suppress noise in images, most of them are developed for Gaussian noise and few are for impulse noise. This paper proposes an image recovery method based on deep convolutional neural networks for impulse noise removal. The proposed framework falls into two components: a classifier network which divides image pixels into noisy and noise-free, and a regression network which is trained for image reconstruction. In the regression network, the noise-free pixels identified by the classifier network together with the original noisy image are used for recovery of the noisy image. Furthermore, batch normalization is embedded to the network to improve denoising performance. Experimental results show that the proposed method can excellently remove impulse noise, providing clear performance improvements over other state-of-the-art denoising methods.","Image, Impulse noise, Convolution neural network, Denoising",Lianghai Jin and Wenhua Zhang and Guangzhi Ma and Enmin Song,https://www.sciencedirect.com/science/article/pii/S1047320319301609,https://doi.org/10.1016/j.jvcir.2019.05.005,1047-3203,2019,193--205,62,Journal of Visual Communication and Image Representation,Learning deep CNNs for impulse noise removal in images,article,JIN2019193
"Fitting facial landmarks on unconstrained videos is a challenging task with broad applications. At present, many methods of one-shot landmark fitting have been proposed with varying degrees of success. However, most of them are heavily sensitive to initializations and usually rely on offline-trained static models, which limit their performance on sequential images with extensive variations. Therefore, they usually canât align the deformed face very well. To address these limitations, we propose a method of deformed landmark fitting (DLF) for sequential faces, which is designed based on active shape model (ASM) and deformation tracking/correction. This method overcomes the loss of consecutive information between frames, and makes full use of the motion variation information of video sequences in time and space dimensions. Firstly, the optical flow values of several possible deformation points on the face are calculated by the large displacement optical flow (LDOF) model, and the tracking of these points in the current frame are performed through the optical flow motion vector. Secondly, the initial shape of face in each frame is established by the locations of these deformation points and the global shape model in ASM algorithm. Finally, on the basis of initial shape, according to the guidance of local texture model in ASM algorithm, different correction strategies are applied to different landmarks for local search, and then each landmark is reasonably suppressed to obtain the ultimate results. Our DLF observably improves the fitting accuracy for deformed faces, and takes full advantage of the continuity among video sequences. Compared with some state-of-the-art landmarkers, extensive experiments on landmark fitting for sequential faces show that our DLF performs outstandingly in terms of accuracy and robustness.","Landmark fitting, Active shape model, Large displacement optical flow model, Global shape model",Shoudong Han and Ziqing Yang and Qianqian Li and Yang Chen,https://www.sciencedirect.com/science/article/pii/S104732031930183X,https://doi.org/10.1016/j.jvcir.2019.06.010,1047-3203,2019,381--393,62,Journal of Visual Communication and Image Representation,Deformed landmark fitting for sequential faces,article,HAN2019381
"Fog computing is a technology that can expands the network computing mode of cloud computing and extends network computing from the network center to the network edge. It adds fog layer between cloud data center layer and Internet of Things (IoT) device layer, and provides data storage, processing, forwarding and other functions for devices using the network edge. In mobile fog computing (MFC) networks, fog nodes communicate with end users through wireless networks. Malicious users can choose different attack modes to attack legitimate users. There is a lack of research on the subjective choice of attack modes for malicious users in current work. To solve this problem, an intelligent attack defense scheme based on Double Q-learning (DQL) algorithm in MFC is proposed. Firstly, the security model involving malicious users in MFC is described. Based on Prospect Theory (PT), a static method of subjective zero-sum game between malicious users and legitimate users is constructed. Secondly, a dynamic subjective game scheme based on DQL algorithm is proposed to resist intelligent attacks. The simulation results show that compared with the Q-learning-based method for resisting intelligent attacks, the proposed method can enhance the security of MFC network and enhance the protection performance.","Moving fog computing, Intelligent attack, Physical layer security, Prospect theory, Reinforcement learning",Yuan Meng and Shanshan Tu and Jinliang Yu and Fengming Huang,https://www.sciencedirect.com/science/article/pii/S1047320319302779,https://doi.org/10.1016/j.jvcir.2019.102656,1047-3203,2019,102656,65,Journal of Visual Communication and Image Representation,Intelligent attack defense scheme based on DQL algorithm in mobile fog computing,article,MENG2019102656
"Image quality assessment towards cell diffraction image is significant for both the academic and medical domain. It plays an important role in medical detection and recognition, such as cell morphology and heterogeneity classification. However, cell diffraction image quality assessment is still a challenging task due to the high heterogeneity of cells and various appearance. To solve this problem, we propose a cell diffraction image quality assessment. More specifically, we first collect cell diffraction images including Jurkat and Ramos. To remove cell impurity and debris images, we leverage the K-means clustering algorithm and support vector machine (SVM) to eliminate these images. Subsequently, we calculate the Gray Level Co-occurrence Matrix (GLCM) of each image and extract deep representation by using DNN. Afterward, we fuse luminance, contrast, GLCM, and deep representation to calculate the feature similarity between the reference image and the test image. Extensive experiments conducted on Jurkat cells and Ramos cells datasets have shown the effectiveness of our proposed method.","Image quality assessment, Cell diffraction image, Deep neural network",Xikun Zhang and Jie Hou,https://www.sciencedirect.com/science/article/pii/S1047320319302536,https://doi.org/10.1016/j.jvcir.2019.102632,1047-3203,2019,102632,64,Journal of Visual Communication and Image Representation,Quality assessment towards cell diffraction image based on multi-channel feature fusion,article,ZHANG2019102632
"Spectral imaging has recently gained traction for face recognition in biometric systems. We investigate the merits of spectral imaging for face recognition and the current challenges that hamper the widespread deployment of spectral sensors for face recognition. The reliability of conventional face recognition systems operating in the visible range is compromised by illumination changes, pose variations and spoof attacks. Recent works have reaped the benefits of spectral imaging to counter these limitations in surveillance activities (defence, airport security checks, etc.). However, the implementation of this technology for biometrics, is still in its infancy due to multiple reasons. We present an overview of the existing work in the domain of spectral imaging for face recognition, different types of modalities and their assessment, availability of public databases for sake of reproducible research as well as evaluation of algorithms, and recent advancements in the field, such as, the use of deep learning-based methods for recognizing faces from spectral images.","Spectral imaging, Biometrics, Face recognition, Spoof attacks, Deep learning",Rumaisah Munir and Rizwan Ahmed Khan,https://www.sciencedirect.com/science/article/pii/S1047320319302810,https://doi.org/10.1016/j.jvcir.2019.102660,1047-3203,2019,102660,65,Journal of Visual Communication and Image Representation,An extensive review on spectral imaging in biometric systems: Challenges & advancements,article,MUNIR2019102660
"Existing template matching based visual object tracking algorithms usually require to manually update the template and have high execution cost on general embedded systems. To address these issues, an adaptive template matching-based single object tracking algorithm with parallel acceleration is proposed in this paper. In this algorithm, we propose an adaptive single object tracking algorithm framework to achieve template update online. Based on the Faster-RCNN model, we design a single object capture method to update the template. Meanwhile, we present a parallel strategy to accelerate the process of template matching. To evaluate the proposed algorithm, we use OTB benchmark to compare the performance with several state-of-the-art trackers on TX2 embedded platform. Experimental results show that the proposed method achieves a 5.9 times execution speed and 71.9% accuracy improvement over the comparison methods.","Visual object tracking, Adaptive template update, Parallel acceleration, Deep learning, Embedded platform",Baicheng Yan and Limin Xiao and Hang Zhang and Daliang Xu and Li Ruan and Zhaokai Wang and Yiyang Zhang,https://www.sciencedirect.com/science/article/pii/S104732031930224X,https://doi.org/10.1016/j.jvcir.2019.102603,1047-3203,2019,102603,64,Journal of Visual Communication and Image Representation,An adaptive template matching-based single object tracking algorithm with parallel acceleration,article,YAN2019102603
"It is challenging to develop an effective quality assessment method for the stereoscopic video (SV) in the free viewpoint video (FVV) system because of asymmetric distortion and the interactively changing combinations. This paper proposes a quality assessment method that takes the characteristics of the SV in FVV systems into consideration. Specifically, considering the distortion introduced by the rendering process, the proposed method extracts a critical distortion area that can be perceivable by the human eye. The distortion degree of each critical distortion area is quantified and the overall results are pooled to obtain the initial quality score. The video pairs are classified into different combination types and the initial quality score is refined to obtain the final quality score. The experimental results show that the proposed method outperforms conventional objective methods.","Free viewpoint video system, Human visual characteristics, Video quality assessment, View synthesis",Zongju Peng and Shipei Wang and Fen Chen and Wenhui Zou and Gangyi Jiang and Mei Yu,https://www.sciencedirect.com/science/article/pii/S1047320319301841,https://doi.org/10.1016/j.jvcir.2019.06.011,1047-3203,2019,102569,63,Journal of Visual Communication and Image Representation,Quality assessment of stereoscopic video in free viewpoint video system,article,PENG2019102569
"Video surveillance is widely applied in modern intelligent systems, such as access control, pedestrian re-identification. However, with the rapid development of urbanization, urban traffic situation becomes more and more complex. It is a big challenge for video surveillance to cope with such massive data. In addition, existing emergency systems are far from modern requirement. So in this paper, we propose an image quality based framework to improve the performance of video surveillance and design a new urban intelligent emergency system. Specifically, we first analyze emergency evacuation of social group security incident for data acquisition including surveillance video and labels. Then, incorporating image quality assessment and convolution neural network, the dataset can be classified into several parts, and each part demonstrates a particular situation. Afterward, we introduce entropy theory to study the application of urban intelligence emergency. The results show that the research method proposed in this paper effectively obtains the evacuation parameters of evacuation personnel in the evacuation of sudden social group events, and improves the timeliness of information transmission in the evacuation process. The results show that the research method of this paper significantly improves the pertinence, effectiveness and perfection of the emergency plan in the application of urban emergency system.","Entropy theory, Big data, Wisdom emergency, Quality model, Neural network",Guanghui Wei and Zhou Sheng,https://www.sciencedirect.com/science/article/pii/S1047320319301968,https://doi.org/10.1016/j.jvcir.2019.102581,1047-3203,2019,102581,63,Journal of Visual Communication and Image Representation,Image quality assessment for intelligent emergency application based on deep neural network,article,WEI2019102581
"Cross-modal retrieval develops rapidly due to the growth and widespread applications of multimodal data. How to reduce the heterogeneous gap and impose effective constraints on different modalities are two basic problems. In this paper, we propose a novel Semantic Consistency cross-modal Dictionary learning algorithm with rank Constraint (SCDC) to solve these aforementioned problems. An orthogonal space learned by spectral regression is introduced, in which different modalities can be measured directly. Specifically, images and texts are encoded by their dictionaries to obtain corresponding reconstruction coefficients. A l21-norm term is imposed on these coefficients in order to select discriminative features and avoid over-fitting simultaneously. In the meantime, a rank constraint is imposed on the transformed features so as to improve the correlation of different modalities. Experimental results on three popular datasets demonstrate that SCDC is significantly superior to several state-of-the-art methods.","Cross-modal retrieval, Dictionary learning, Rank constraint",Fei Shang and Huaxiang Zhang and Jiande Sun and Li Liu,https://www.sciencedirect.com/science/article/pii/S1047320319301725,https://doi.org/10.1016/j.jvcir.2019.05.017,1047-3203,2019,259--266,62,Journal of Visual Communication and Image Representation,Semantic consistency cross-modal dictionary learning with rank constraint,article,SHANG2019259
"Data sparsity and prediction quality have been recognized as the crucial challenges in recommender system. With the expansion of social network data, social network analysis is becoming more and more important. Traditional Recommendation System assumes that users are independent and distributed equally, which ignores the social interaction or connection among users. In order to solve the prediction quality of friend recommendation in social networks, a user recommendation algorithm for social networks based on sentiment analysis and matrix factorization is proposed in this paper. This method is based on the traditional matrix factorization model. By integrating Sentiment (S), Important (I) and Objective (O) of user topic content in the social network, this paper proposes the approach base on sentiment analysis and matrix factorization to solve the poor prediction accuracy by employing social network. SIO model solves the problem that users in social networks canâ²t score the content of topics. User-topic matrix is constructed by SIO model. Combining the SIO model with matrix factorization, algorithm called SIO-TMF algorithm is proposed. Applying this method on social network, comparing with some traditional recommendation algorithms from four aspects: accuracy, diversity, novelty and coverage, the experimental results show that the proposed method improves the prediction quality of recommender system.","Sentiment analysis, Matrix factorization, Social network, Recommendation system",Chongchao Cai and Huahu Xu,https://www.sciencedirect.com/science/article/pii/S1047320319302780,https://doi.org/10.1016/j.jvcir.2019.102657,1047-3203,2019,102657,65,Journal of Visual Communication and Image Representation,A topic sentiment based method for friend recommendation in online social networks via matrix factorization,article,CAI2019102657
"In Alzheimerâs Disease (AD) studies, high dimension and small sample size have been always an issue and it is common to apply a dimension reduction method to predict the early diagnosis of AD. In this paper, we propose a multi-view feature selection algorithm embedded with exclusive lasso learning and sparse learning. It extracts the feature subsets that best represent the symptoms of patients through feature selection, so as to reduce the dimension and achieve a better diagnosis rate. Firstly, in order to overcome the limitation of non-overlapping, the features under different view are clustered by fuzzy C-means clustering. Then, the exclusive group lasso learning is performed according to the clustering results and each view is sparsely learned through the l2,1-norm, resulting in better removal of redundant features. Finally, the results of each view are combined to obtain the final features subsets. This exclusive lasso learning combined through multiple views is novel in clinical practice and can effectively target AD. At the same time, the experimental results show that our method could achieve better results compared to its competing methods.","Alzheimerâs Disease, Multi-view, Exclusive lasso learning, Feature selection, Sparse learning",Jiaye Li and Lin Wu and Guoqiu Wen and Zhi Li,https://www.sciencedirect.com/science/article/pii/S1047320319302263,https://doi.org/10.1016/j.jvcir.2019.102605,1047-3203,2019,102605,64,Journal of Visual Communication and Image Representation,Exclusive feature selection and multi-view learning for Alzheimerâs Disease,article,LI2019102605
"Using GF-1 and Landsat8 remote sensing images as data sources, combined with the experimental data of wetland soil sampling in Anyi County and Gao'an Research Area, the ability and difference of two remote sensing images in inversion of soil organic matter content were compared. The results show that the reflectivity of the two remote sensing images in the visible and near-infrared bands is significantly correlated with the soil organic matter content, and the correlation is the most in the near-infrared band. The index model established by the near-infrared band of GF-1 is more than the near-infrared band using Landsat8. The power model estimates slightly better. The multi-regression model established by introducing the blue band (dark blue band) and red band has higher inversion precision than the single-band model, especially the improvement effect of Landsat8 remote sensing image. Compared with Landsat8, GF-1 remote sensing image has higher spatial resolution and shorter revisit period, and has similar predictive ability in detecting soil organic matter content, which can replace Landsat8 remote sensing image.","Landsat 8, Soil organic matter, Quantitative inversion, Remote sensing image",Maotong Zhai,https://www.sciencedirect.com/science/article/pii/S1047320319302664,https://doi.org/10.1016/j.jvcir.2019.102645,1047-3203,2019,102645,64,Journal of Visual Communication and Image Representation,Inversion of organic matter content in wetland soil based on Landsat 8 remote sensing image,article,ZHAI2019102645
"Image quality assessment (IQA) is a key technique in computer vision, which is widely applied in image classification, image aesthetic prediction. IQA plays an important role in advertising assessment system, which can recommend higher quality advertising for users. However, traditional algorithms cannot effectively predict advertising quality. In this paper, we propose an advertising assessment system using IQA algorithm based on neural networks. Specifically, we first incorporate both low-level features and high-level sematic features for image representation, where manifold learning algorithm is leveraged for high-level feature learning. Then, we leverage CNN based method for deep representation learning, which will be concatenated into deep feature vector. Finally, we leverage HMM model for learning image quality of advertising based on learned feature vector. Comprehensive experiments show the effectiveness of our proposed method.","Deep learning, Image quality assessment, Image classification",Cher-Min Fong and Hui-Wen Wang and Chien-Hung Kuo and Pei-Chun Hsieh,https://www.sciencedirect.com/science/article/pii/S1047320319302147,https://doi.org/10.1016/j.jvcir.2019.102593,1047-3203,2019,102593,63,Journal of Visual Communication and Image Representation,Image quality assessment for advertising applications based on neural network,article,FONG2019102593
"Dim and small target detection based on passive millimeter wave or infrared imaging is of great value in both security and military fields and has been studied extensively. The problems of weak distinction between small targets and backgrounds and of less extractable features of targets have always been a technical bottleneck for accurate detection of dim and small targets. For dim and small targets with few pixel-based features on complex and diverse backgrounds, we propose a high-precision detection algorithm based on feature mapping deep neural networks with a spindle network structure. Firstly, the features of low-dimension dim and small target blocks are mapped to a higher-dimensional space. An encoded neural network is then used to extract high-discriminant features to complete the background and target recognition. Background suppression and target enhancement is realized according to the intensity (the distinguished output of the network). Finally, a detection method based on the constant false alarm rate is used to detect dim and small targets. The experimental results show that, compared with several popular algorithms for millimeter-wave and infrared image detection in different scenarios, the proposed algorithm has a lower false alarm rate, higher detection accuracy and stronger robustness. Statistics for experiments on under various false alarm rates and signal-to-noise ratios show that the detection rate of the proposed method is about 15% higher than that of the compared algorithms. In experiments on real data, the detection rate of our algorithm is more than 25% higher than that of the suboptimal algorithm.","Dim and small target detection, Feature mapping, Deep neural network, Constant false alarm rate, Background suppression",Zhisheng Gao and Jiao Dai and Chunzhi Xie,https://www.sciencedirect.com/science/article/pii/S1047320319301695,https://doi.org/10.1016/j.jvcir.2019.05.013,1047-3203,2019,206--216,62,Journal of Visual Communication and Image Representation,Dim and small target detection based on feature mapping neural networks,article,GAO2019206
"The automated recognition of facial expressions has been actively researched due to its wide-ranging applications. The recent advances in deep learning have improved the performance facial expression recognition (FER) methods. In this paper, we propose a framework that combines discriminative features learned using convolutional neural networks and handcrafted features that include shape- and appearance-based features to further improve the robustness and accuracy of FER. In addition, texture information is extracted from facial patches to enhance the discriminative power of the extracted textures. By encoding shape, appearance, and deep dynamic information, the proposed framework provides high performance and outperforms state-of-the-art FER methods on the CK+ dataset.","Convolutional neural network, Facial expression recognition, Feature extraction",Xijian Fan and Tardi Tjahjadi,https://www.sciencedirect.com/science/article/pii/S1047320319302809,https://doi.org/10.1016/j.jvcir.2019.102659,1047-3203,2019,102659,65,Journal of Visual Communication and Image Representation,Fusing dynamic deep learned features and handcrafted features for facial expression recognition,article,FAN2019102659
"Scene categorization is an indispensable technique in intelligent systems, such as scene parsing, video surveillance or autonomous driving. Considering traffic analysis under big data, in this paper, we propose scene categorization towards urban tunnel traffic based on image quality assessment. Specifically, the dataset is obtained through analyzing urban tunnel traffic incidents from 2016 to 2018. And we classify the traffic accidents in the big data environment. Then, the vehicles in the surveillance videos are extracted using conventional detector. The spatial information of vehicles in the image reflects the traffic situation. In order to encode such important information, we leverage the information clustering algorithm based on information entropy for image classification. Afterward, we establish a quality evaluation model based on each clustered images. The trained image quality assessment model will guide tunnel traffic classification and event analysis. The experimental results show the correct rate is more than 90%, and the overall detection effect is better than the k-modes algorithm and the Ngâk-modes algorithm.","Scene categorization, Tunnel traffic analysis, Information entropy, Rough clustering, Image quality model",Huachun Zhou and Sheng Zhou,https://www.sciencedirect.com/science/article/pii/S1047320319302767,https://doi.org/10.1016/j.jvcir.2019.102655,1047-3203,2019,102655,65,Journal of Visual Communication and Image Representation,Scene categorization towards urban tunnel traffic by image quality assessment,article,ZHOU2019102655
"With the development of concrete performance research, more and more attention has been paid to the study of concrete performance, resulting in a variety of micro-structure finite element models, such as lattice model, beam-particle model, random aggregate model, with the emergence of CT technology, the realization of the non-destructive state of concrete internal micro-structure with digital The rendering method is presented. If the real or near-real finite element model of meso-structure can be established by using the information of CT plane images, it will play a certain role in the numerical simulation of concrete. Concrete includes aggregate, cement mortar and pore three parts, from the image characteristics, aggregate close to white, pore tend to black, cement mortar is between the two. Because of the relative obvious density difference between aggregate, mortar and pore, after CT scanning and converting to image, each component of concrete has better contrast, and it is easier to observe and extract aggregate contour. In this paper, CT image processing technology is used to preprocess the section image of concrete cylinder specimens in order to obtain accurate aggregate geometry and position information. On this basis, the reconstructed micro-finite element model is simulated and simulated in MATLAB, and the results are compared with those of other finite element models. The results show that the finite element concrete micro-model can make up for the shortcomings of the traditional random aggregate concrete model, and better reflect the mechanical characteristics of concrete materials, which opens up a new way for the ultimate in-depth study of the micro-damage mechanism of concrete materials.","CT image, Numerical model, Concrete, Failure process",Wenwei Yang,https://www.sciencedirect.com/science/article/pii/S1047320319302524,https://doi.org/10.1016/j.jvcir.2019.102631,1047-3203,2019,102631,64,Journal of Visual Communication and Image Representation,Finite element model of concrete material based on CT image processing technology,article,YANG2019102631
"Deep feature aggregation, which refers to aggregating a set of local convolutional features into a global image-level vector, has attracted increasing attention in object instance retrieval. In this manuscript, we propose an unsupervised framework that aggregates feature maps by an adaptive selection and two weighting strategies. Particularly, the selection process finds the foreground contour by explaining the semantic structure implicated in the feature maps, while two weighting process including an adaptive Gaussian filter that highlights semantic features and an element-value sensitive channel vector that activates feature channels corresponding to sparse yet distinctive image patterns. Experimental results on benchmark image retrieval datasets validate that the selection and two weighting schemes are complementary in improving the discriminative power of image vectors. With the same experimental settings, the proposed approach outperforms state-of-the-art aggregation approaches by a considerable margin.","Object retrieval, Deep convolutional features, Aggregation",Jihua Zhu and Jiaxing Wang and Shanmin Pang and Weili Guan and Zhongyu Li and Yaochen Li and Xueming Qian,https://www.sciencedirect.com/science/article/pii/S1047320319301798,https://doi.org/10.1016/j.jvcir.2019.06.006,1047-3203,2019,368--380,62,Journal of Visual Communication and Image Representation,Co-weighting semantic convolutional features for object retrieval,article,ZHU2019368
"Image quality assessment is of great significance for the designment and application of remote sensing systems. CNN based method is proposed for image quality assessment on remote sensing image in this paper. Specifically, we first introduce the convolutional neural network and deep learning method. Then a deep CNN architecture is constructed to automatically extract image features to evaluate image quality. Afterward, the information entropy threshold is used to remove the image blocks with less information content. Finally, a deep network model with two convolutional layers is used to achieve feature extraction and image quality scoring. The experimental results show that the quality score of this method has good subjective and objective consistency for multi-distortion remote sensing images and common multi-distortion images. Evaluation of distorted images does not depend on a specific database and has database independence. In addition, our proposed method is simple to achievement.","Image quality assessment, Remote sensing image, Deep learning, Information entropy",Guobin Chen and Maotong Zhai,https://www.sciencedirect.com/science/article/pii/S1047320319301956,https://doi.org/10.1016/j.jvcir.2019.102580,1047-3203,2019,102580,63,Journal of Visual Communication and Image Representation,Quality assessment on remote sensing image based on neural networks,article,CHEN2019102580
"In the age of information explosion, image classification is the key technology of dealing with and organizing a large number of image data. Currently, the classical image classification algorithms are mostly based on RGB images or grayscale images, and fail to make good use of the depth information about objects or scenes. The depth information in the images has a strong complementary effect, which can enhance the classification accuracy significantly. In this paper, we propose an image classification technology using principal component analysis based on multi-view depth characters. In detail, firstly, the depth image of the original image is estimated; secondly, depth characters are extracted from the RGB views and the depth view separately, and then the reducing dimension operation through the PCA is implemented. Eventually, the SVM is applied to image classification. The experimental results show that the method has good performance.","Image classification, Principal component analysis, Multi-view depth characters",Yaoqi Sun and Liang Li and Liang Zheng and Ji Hu and Wenchao Li and Yatong Jiang and Chenggang Yan,https://www.sciencedirect.com/science/article/pii/S1047320319301713,https://doi.org/10.1016/j.jvcir.2019.05.016,1047-3203,2019,253--258,62,Journal of Visual Communication and Image Representation,Image classification base on PCA of multi-view deep representation,article,SUN2019253
"In this paper, we focus on the problem of temporal activity detection, which aims to directly predict the temporal bounds of actions. Most existing temporal activity detection algorithms treat the classification of each action proposal separately and neglect vital semantic correlations between actions in one video. This will deteriorate the classification performance in the scenario of long-tail problems, where only a handful of examples are available for uncommon actions. To solve this problem, we propose to incorporate knowledge to reason over large scale action classes and maintain semantic coherency within one video. Specifically, we employ an implicit knowledge reasoning module and an explicit knowledge reasoning module to incorporate the knowledge constraints to facilitate temporal activity localization. To demonstrate the superiority of the proposed model, we test the proposed method on large-scale action detection datasets, namely ActivityNet and THUMOSâ14 datasets. The experimental results have demonstrated the superiority of the proposed model. Codes and models will be released once this paper is accepted.","Temporal activity detection, Knowledge constraints, Reasoning module",Changlin Li and Zhihui Li and Zongyuan Ge and Mingjie Li,https://www.sciencedirect.com/science/article/pii/S1047320319302494,https://doi.org/10.1016/j.jvcir.2019.102628,1047-3203,2019,102628,64,Journal of Visual Communication and Image Representation,Knowledge driven temporal activity localization,article,LI2019102628
"In this paper, we propose a novel algorithm to estimate Gaussian noise levels for captured natural images by rigorously analyzing the limiting distributions of the eigenvalue spectrum of a large covariance matrix with Gaussian samples. In order to select a relatively homogeneous region that best represents the noise, the corresponding image patches are first rearranged to construct a high-dimensional noise covariance matrix. And then, an optimal criterion for classifying homogeneous regions is derived based on the statistical relationship between the largest and the second largest eigenvalues of a sample covariance matrix. Moreover, we further explore the reasons for the bias of the maximum likelihood estimator of the noise variance both in high-dimensional settings and finite samples. According to random matrix theory, we clarify the asymptotic properties of the trace of a sample covariance matrix to measure the error bounds of estimation and then propose a new bias-corrected estimator. To this end, an effective estimation method for the noise level is devised based on the boundness and asymptotic behavior of pure noise eigenvalues of the selected patches. The estimation performance of our method has been guaranteed both theoretically and empirically. Experimental results have demonstrated that our approach can reliably infer true noise variance and is superior to the competing methods in terms of both estimation accuracy and robustness.","Noise level estimation, Eigenvalue distributions, Large sample covariance matrix, Random matrix theory",Rui Chen and Fei Zhao and Changshui Yang and Yuan Li and Tiejun Huang,https://www.sciencedirect.com/science/article/pii/S1047320319302251,https://doi.org/10.1016/j.jvcir.2019.102604,1047-3203,2019,102604,63,Journal of Visual Communication and Image Representation,Robust estimation for image noise based on eigenvalue distributions of large sample covariance matrices,article,CHEN2019102604
"This paper presents a scene flow estimation method which functions by depth map upsampling and layer assignment for the camera-LiDAR (Light Detection And Ranging) system. The 3D geometry and motion of the observed scene are estimated simultaneously based on two consecutive frames from a camera and a LiDAR. The proposed technique begins with dense depth map upsamling guided by a corresponding RGB image. The scene is then classified to various moving layers by a hybrid method. Finally, the motion of each layer is constrained by the RGB image and depth image which provide a coarse 3D rigid motion. Experimental results on both public datasets and a real-word platform demonstrate the effectiveness of this technique.","3D scene flow, Sensor fusion, Depth map upsampling",Cheng Zou and Bingwei He and Mingzhu Zhu and Liwei Zhang and Jianwei Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319302378,https://doi.org/10.1016/j.jvcir.2019.102616,1047-3203,2019,102616,64,Journal of Visual Communication and Image Representation,Scene flow estimation by depth map upsampling and layer assignment for camera-LiDAR system,article,ZOU2019102616
"Breast cancer is generally acknowledged as the second leading cause of cancer death among women. Therefore, accurately understanding breast cancer from X-ray images is an indispensable technique in medical sciences and image analysis. In the work, we propose a novel perceptual deep architecture that hierarchically learns deep features from large-scale X-ray images, wherein human visual perception is naturally encoded. More specifically, given a rich number of breast cancer images, we first employ the well-known BING objectness measure to identify all possible visually/semantically salient patches. Due to the relatively huge number of BING object patches, a weakly-supervised ranking algorithm is designed to select high quality object patches according to human visual perception. Subsequently, an aggregation scheme is utilized to derive the deep features of high quality object patches within each brain cancer image. Based on the aggregated deep feature, a multi-class SVM is trained to classify each breast cancer into multiple levels. Extensive comparative studies and visualization results have demonstrated the effectiveness and efficiency of our proposed deep architecture.","Breast cancer, Deep learning, Quality-related, Weakly-supervised, Ranking algorithm",Xusheng Wang and Xing Chen and Congjun Cao,https://www.sciencedirect.com/science/article/pii/S1047320319302652,https://doi.org/10.1016/j.jvcir.2019.102644,1047-3203,2019,102644,64,Journal of Visual Communication and Image Representation,Hierarchically engineering quality-related perceptual features for understanding breast cancer,article,WANG2019102644
"Visual cryptography (VC) is a technique that encodes the content of a secret image into two or more images, which are called shares. These shares are printed on transparencies and superimposed to reveal the original secret image. Frequently, VC techniques require a pixel expansion and a good alignment, which reduces the final spatial resolution. In this paper, we propose a physical VC scheme that only requires two shares and do not demand a pixel expansion. The first share is a colored transparency printed on a Polyvinyl Chloride (PVC) surface of 3â¯mm, while the second share is a colored image displayed on a smartphone screen. The decoded pixels of the proposed scheme have defined colors and a good resolution. We perform a physical evaluation of the color interference properties of the two shares, to find the most adequate color space, and test the proposed method with practical examples.","Visual cryptography, Visual secret sharing, Color interference",Max E. {Vizcarra Melgar} and MylÃ¨ne C.Q. Farias,https://www.sciencedirect.com/science/article/pii/S1047320319302135,https://doi.org/10.1016/j.jvcir.2019.102592,1047-3203,2019,102592,63,Journal of Visual Communication and Image Representation,"A (2,2) XOR-based visual cryptography scheme without pixel expansion",article,VIZCARRAMELGAR2019102592
"This work presents and compares different vision-based approaches to estimate the occupancy status of parking areas by counting cars and non-empty parking stalls. Our investigation considers both the scenario in which parking stalls are marked on the ground and the more challenging one in which no assumption on the presence or position of stalls is assumed. We carry out an experimental analysis on a real-world dataset of videos collected in different parking areas. Specifically, this work compares solutions based on image classification, vehicle detection and semantic segmentation. Our analysis highlights that: (1) methods based on image classification can be effectively leveraged when the position of parking stalls is known in advance, (2) methods based on image segmentation should be preferred over methods based on object detection when the geometry of the scene is not known, (3) temporal smoothing can be effectively used to improve predictions over time.","Scene understanding, Deep learning, Smart cities, Counting, Vehicle detection, Semantic segmentation",D. {Di Mauro} and A. Furnari and G. PatanÃ¨ and S. Battiato and G.M. Farinella,https://www.sciencedirect.com/science/article/pii/S1047320319301683,https://doi.org/10.1016/j.jvcir.2019.05.015,1047-3203,2019,234--244,62,Journal of Visual Communication and Image Representation,Estimating the occupancy status of parking areas by counting cars and non-empty stalls,article,DIMAURO2019234
"With the continuous development of society and the rapid advancement of science and technology, all walks of life are increasingly required to effectively manage their projects or affairs. Decision-making is the basis of project management, and the formation of decisions determines the formulation of corresponding programs, organizations and businesses. Decision-making is generated by decision makers based on the multi-faceted information they have, so decision management systems and information systems are inseparable in the management of projects and transactions. At present, research on decision management systems emphasizes management and decisions, and few researchers combine decision management and related information. In view of the current research deficiencies, and taking into account the needs of various industries for image applications, this paper introduces image quality evaluation methods, and systematically studies the decision management system and information system framework. The experiment part takes the intelligent image recognition and prevention system of citrus pests and diseases as an example, and the role of image quality in decision management system and information system from the perspective of practice. The results show that through the image quality evaluation system, in the identification and prevention of citrus pests and diseases, it can combine various aspects of information to make effective prevention and control decisions. The results show that the introduction of image quality evaluation can improve the comprehensiveness of information system. On this basis, decision-making system can help decision makers make correct decisions quickly and effectively.","Image quality, Quality model, Decision management, Information system, Decision support system",Jui-Chan Huang and Hao-Chen Huang and Su-Hui Chu,https://www.sciencedirect.com/science/article/pii/S1047320319302093,https://doi.org/10.1016/j.jvcir.2019.102588,1047-3203,2019,102588,63,Journal of Visual Communication and Image Representation,Research on image quality in decision management system and information system framework,article,HUANG2019102588
"With the development of society, the demand for power is increasing, but the inefficiency of the original power grid has gradually become an obstacle to social development, so it is urgent to reform the power grid, and the power Internet of things is an important means of power reform. Effective infrared image recognition of power Internet of Things equipment can achieve real-time monitoring of equipment status. The quality of image recognition is not only related to recognition methods, but also to the quality of the image itself. In order to improve the quality of image recognition, an image enhancement algorithm based on membership function is proposed in this paper. Firstly, piecewise linear transformation is introduced, and on the basis of it, an improved piecewise linear transformation method is proposed. Secondly, an image enhancement algorithm based on membership function is proposed by improving the traditional fuzzy enhancement method. Finally, based on the image enhancement algorithm of membership function, an improved piecewise linear transformation method is introduced to form an image blur enhancement system. Through the simulation experiment, we can find that the two image enhancement algorithms are improved in this paper, which can enhance the image enhancement effect of the original two enhancement algorithms, and the image fuzzy enhancement system composed of two improved enhancement methods can enhance the image enhancement effect again, and effectively improve the infrared image recognition quality of power Internet of things equipment.","Power internet of things, Image recognition quality, Membership function, Image enhancement, Quality model",Liu Dun Nan and Hou Rui and Li Qiang and Zhao Ning Ning and Ge Rui and Lu Yi,https://www.sciencedirect.com/science/article/pii/S1047320319301828,https://doi.org/10.1016/j.jvcir.2019.06.009,1047-3203,2019,359--367,62,Journal of Visual Communication and Image Representation,Research on fuzzy enhancement algorithms for infrared image recognition quality of power internet of things equipment based on membership function,article,NAN2019359
"In this paper, we propose a standard-compliant image compression framework based on image representation network (IRN) and post-processing neural network (PNN), which are trained by learning a virtual codec network (VCN). Firstly, we use a mixed-resolution image coding considering different types of distortions caused by image compression with different quality factors. Secondly, the VCN is introduced to learn a differentiable soft-projection from the represented image to the post-processed image to resolve the non-differentiable problem of hard quantization. Thirdly, the PNN is used to greatly enhance the quality of decoded images, since standard codecs always result in visually unpleasant blocking artifacts and ringing artifacts. Finally, our framework is trained in an end-to-end manner, whose convolutional kernels of the IRN, PNN and VCN are initialized by pre-training an auto-encoder network. Experimental results verify that our method has higher coding efficiency than the newest image representation-based compression method and many post-processing approaches.","Image representation, Image compression, Soft-projection, Virtual codec, Post-processing",Lijun Zhao and Huihui Bai and Anhong Wang and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S104732031930210X,https://doi.org/10.1016/j.jvcir.2019.102589,1047-3203,2019,102589,63,Journal of Visual Communication and Image Representation,Learning a virtual codec based on deep convolutional neural network to compress image,article,ZHAO2019102589
"In the trackers based on correlation filter, the limitation of tracking samples and the efficiency of the solution affect the performance of the trackers. Based on the Discriminative Scale Space Tracking(DSST), we proposed a new framework to improve correlation filter tracking via context fusion and subspace constraint. First, the target and background are jointly modeled to enhance the ability to distinguish between target and background. Second, to achieve higher robustness, subspace constraint is applied to DSST, and it can be solved efficiently by subspace-based alternating direction method of multipliers(SADMM). Finally, through the model update strategy, high-quality samples are selected to update. The proposed algorithm is validated on the OTB2013, OTB2015 and TC128. Experimental results show that compared with fast DSST(fDSST), success rate and accuracy of the proposed method are increased by 4.2% and 6.8% respectively. Compared with the state-of-the-art trackers, the proposed method achieves a competitive result with real-time speed.","Object tracking, Correlation filter, Context fusion, Subspace constraint, Model update",Jiahong Xu and Cheng Cai and Jifeng Ning and Yunsong Li,https://www.sciencedirect.com/science/article/pii/S1047320319301701,https://doi.org/10.1016/j.jvcir.2019.05.014,1047-3203,2019,182--192,62,Journal of Visual Communication and Image Representation,Robust correlation filter tracking via context fusion and subspace constraint,article,XU2019182
"Image plays an important role in today's society and is an important information carrier. However, due to the problems in shooting or processing, image quality is often difficult to be guaranteed, and low-quality images are often difficult to identify, which results in the waste of information. How to effectively identify low-quality images has become a hot research topic in today's society. Deep learning has a good application in image recognition. In this paper, it is applied to low-quality image recognition. An image quality recognition technology based on deep learning is studied to effectively realize low-quality image recognition. Firstly, in the stage of image preprocessing, a low-quality image enhancement method is proposed, which uses non-linear transformation to enhance image contrast image, restore image details and enhance image quality. Secondly, the convolutional neural network is used to extract image features, and the L2 regularization method is introduced to optimize the over-fitting problem. Finally, SVM is used to recognize the output of convolutional neural network to realize low quality image recognition. Through simulation analysis, it is found that the image enhancement method proposed in the pre-processing stage can effectively enhance the image quality, and deep learning can effectively realize the recognition of the enhanced image and improve the recognition accuracy.","Low quality image, Deep learning, Image recognition, Support vector machines(SVM)",Tao He and Xiaofeng Li,https://www.sciencedirect.com/science/article/pii/S1047320319302755,https://doi.org/10.1016/j.jvcir.2019.102654,1047-3203,2019,102654,65,Journal of Visual Communication and Image Representation,Image quality recognition technology based on deep learning,article,HE2019102654
"To better deal with the partial occlusion issue and improve their efficiency of part-based and support vector machines (SVM) based trackers, we propose a novel part-based structural support correlation filter tracking method, which absorbs the strong discriminative ability from SVM and the excellent property of part-based tracking methods which is less sensitive to partial occlusion. Then, our proposed model can learn the support correlation filter of each part jointly by a star structure model, which preserves the spatial layout structure among parts and tolerates outliers of parts. In addition, our model introduces inter-frame consistencies of local parts to mitigate the drift problem. Finally, our model can accurately estimate the scale changes of object by the relative distance change among reliable parts. The extensive empirical evaluations on three benchmark datasets: OTB2015, TempleColor128 and VOT2015 demonstrate that the proposed method achieves comparable performance against several state-of-the-art trackers and runs in real time.","Object tracking, Support vector machines, Correlation filter, Structural learning, Temporal consistency, Scale estimation",Zhangjian Ji and Kai Feng and Yuhua Qian,https://www.sciencedirect.com/science/article/pii/S1047320319302238,https://doi.org/10.1016/j.jvcir.2019.102602,1047-3203,2019,102602,64,Journal of Visual Communication and Image Representation,Part-based visual tracking via structural support correlation filter,article,JI2019102602
"According to data released by the World Health Organization, malignant tumors have become the second leading cause of death among Chinese children. For malignant tumors, effective diagnostic information directly determines the subsequent treatment outcome. Because of the small age of children with childhood tumors and the specificity of their constitution, many diagnostic methods are not very effective in the diagnosis of children's tumors. Ultrasound-guided puncture is a widely used method. However, puncture and biopsy may lead to a series of serious complications such as bleeding, infection, organ damage and even tumor rupture. Regarding the complications caused by puncture and biopsy, most of the current studies use B-ultrasound imaging or even clinical observation to observe. Ultrasound is not applicable to bones and organs with more gas (such as intestinal tract). In response to these problems, this paper selects hospitalized children with solid tumors in the hospital oncology department, performs CT imaging examination on children who have undergone ultrasound-guided puncture, and conducts quality studies on CT imaging images to observe under ultrasound guidance. The complications after puncture were evaluated, and the feasibility of puncture biopsy in the diagnosis of childhood tumors was evaluated. The results show that the CT image after image quality control can meet the requirements of analyzing the condition and can effectively observe the complications after puncture. The results show that ultrasound-guided puncture can be used for the diagnosis of childhood tumors.","Ultrasound, Puncture, Childhood tumor, CT imaging, Image quality",Huiying Xu and Rui Xin and Yufei Zhao and Xianmei Jin,https://www.sciencedirect.com/science/article/pii/S1047320319302512,https://doi.org/10.1016/j.jvcir.2019.102630,1047-3203,2019,102630,65,Journal of Visual Communication and Image Representation,Image quality study of CT imaging examination in children with childhood tumors under ultrasound-guided puncture,article,XU2019102630
"Salient object detection is the process of identifying essential objects in an image. This paper solves this problem using background subtraction, Gabor filters, minimum directional backgroundness, and objectness. The first step is to calculate a backgroundness score for each region by calculating the difference between the feature vector of image boundary and image regions. This backgroundness map is then used for calculating the minimum directional background difference. The image is segmented using Gabor filters, and then the objectness criterion is used to choose the segment containing the salient object. The normalized foreground saliency map is then used to refine the selected segment. Further enhancement of this intermediate output is done using morphological operations, and boundary correction is done using the method of lazy snapping. The algorithm is tested on eight publicly available datasets and is compared against five algorithms. The performance is evaluated by PR-curve, F-Measure curve, and Mean Absolute Error.","Background subtraction, Gabor filters, Minimum directional backgroundness",Gargi Srivastava and Rajeev Srivastava,https://www.sciencedirect.com/science/article/pii/S1047320319301786,https://doi.org/10.1016/j.jvcir.2019.06.005,1047-3203,2019,330--339,62,Journal of Visual Communication and Image Representation,"Salient object detection using background subtraction, Gabor filters, objectness and minimum directional backgroundness",article,SRIVASTAVA2019330
"Object detection has been widely applied in modern intelligent systems, especially using convolutional neural networks (CNNs). Pedestrian detection is a key technique in video surveillance, which could automatically locate special pedestrian. However, conventional CNN based methods such as Fast/Faster R-CNN cannot handle pedestrian detection effectively due to the extremely similar of positives and hard negatives. In this paper, in order to solve hard negative problem in pedestrian detection, we incorporate classifier enhancement and representational ability of CNNs. More specifically, we first fuse multi-channel visual features (color, texture, semantic) for quality assessment. Then, we propose âReduction-adjustmentâ (RA) block which can enhance feature extraction and can be flexibly embedded into CNNs. In our implementation, we embed RA blocks into a base model such as VGG 16. Afterwards, we apply Faster R-CNN as a detection system to classify and locate pedestrians. Extensive experiments on Caltech, ETH and CityPersons datasets demonstrate that our deep model is feasible and effective for pedestrian detection.","Convolutional neural networks, Pedestrian detection, VGG-16 net, RA block, Faster R-CNN, Quality model",Peijia Yu and Yong Zhao and Jing Zhang and Xiaoyao Xie,https://www.sciencedirect.com/science/article/pii/S1047320319301944,https://doi.org/10.1016/j.jvcir.2019.102579,1047-3203,2019,102579,63,Journal of Visual Communication and Image Representation,Pedestrian detection using multi-channel visual feature fusion by learning deep quality model,article,YU2019102579
"Tampered images spread nowadays over any visual media influencing our judgement in many aspects of our life. This is particularly critical for face splicing manipulations, where recognizable identities are put out of context. To contrast these activities on a large scale, automatic detectors are required. In this paper, we present a novel method for automatic face splicing detection, based on computer vision, that exploits inconsistencies in the lighting environment estimated from different faces in the scene. Differently from previous approaches, we do not rely on an ideal mathematical model of the lighting environment. Instead, our solution, built upon the concept of histogram-based features, is able to statistically represent the current interaction of faces with light, untied from the actual and unknown reflectance model. Results show the effectiveness of our solution, that outperforms existing approaches on real-world images, being more robust to face shape inaccuracies.","Image forensics, Scene level analysis, Geometric constraints, Lighting environment, Face splicing detection",Marco Fanfani and Fabio Bellavia and Massimo Iuliani and Alessandro Piva and Carlo Colombo,https://www.sciencedirect.com/science/article/pii/S104732031930207X,https://doi.org/10.1016/j.jvcir.2019.102586,1047-3203,2019,102586,63,Journal of Visual Communication and Image Representation,FISH: Face intensity-shape histogram representation for automatic face splicing detection,article,FANFANI2019102586
"Improving the ability of imperceptibility, watermark capacity, and robustness at the same time still remains a challenge within the digital image watermarking community. By modeling the robust nonsubsampled Contourlet transform (NSCT) difference coefficients with vector based Cauchy distribution and employing locally most powerful (LMP) test, we propose a locally optimum image watermark decoder in NSCT domain. We first compute the difference coefficients according to the inter-scale dependency between NSCT coefficients, and investigate the robustness of the NSCT difference coefficients by subjective visual error and objective mean squared error (MSE) terms. We then embed the digital watermark into the significant NSCT difference subband with highest energy by modifying the robust NSCT difference coefficients. At the receiver, by combining the vector based Cauchy probability distribution and LMP test, we propose a locally optimum blind watermark decoder in the NSCT domain. Here, robust NSCT difference coefficients are firstly modeled by employing the vector based Cauchy probability density function (PDF), where the Cauchy marginal statistics and various strong dependencies of NSCT coefficients are incorporated. Then the statistical model parameters of vector based Cauchy PDF are estimated using second-kind statistics approach. And finally a blind image watermark decoder is developed using vector based Cauchy PDF and LMP decision rule. We conduct extensive experiments to evaluate the performance of the proposed blind watermark decoder, in which encouraging results validate the effectiveness of the proposed technique, in comparison with the state-of-the-art approaches recently proposed in the literature.","Image watermarking, NSCT difference coefficient, Vector based Cauchy distribution, Second-kind statistics, Locally most powerful test",Xiang-yang Wang and Si-yu Zhang and Li Wang and Hong-ying Yang and Pan-pan Niu,https://www.sciencedirect.com/science/article/pii/S1047320319301671,https://doi.org/10.1016/j.jvcir.2019.05.012,1047-3203,2019,309--329,62,Journal of Visual Communication and Image Representation,Locally optimum image watermark decoder by modeling NSCT domain difference coefficients with vector based Cauchy distribution,article,WANG2019309
"Despite the fact that traditional digital watermarking technology is relatively mature, there are still some areas that have not been fully involved in. For example, image watermarking technology and the certification are still in its infancy at early times. The bottleneck problem of digital product safety protection all solved theoretically with the combination of computing theory as well as traditional digital watermarking technology and point out a new direction for the research of information security industry. Inspired by the traditional algorithm of image watermarking and based on the Haar wavelet function along with algorithm of 2-D discrete wavelet transform and selection, this article presents the techniques of watermark embedding and extraction of color images. The main appraisal criteria of the watermark include invisibility and robustness, and some other standards. Image watermarking is relatively simple in the spatial domain, where it cannot resist geometrical attacks. In the transform domain, this approach can resist both geometrical attacks and image processing attacks. Only when the carrier image suffers from severe damage with the image quality hugely compromised will the extracted watermark become unrecognizable. As a result, the algorithm presented in this article can well embed the color image in the carrier image, and has good resistance to attack operations such as loss compression and adding of noise.","Color image watermarking, Discrete Wavelet Transform (DWT)",Jianyu Wang and Zhiguo Du,https://www.sciencedirect.com/science/article/pii/S1047320319302482,https://doi.org/10.1016/j.jvcir.2019.102627,1047-3203,2019,102627,64,Journal of Visual Communication and Image Representation,A method of processing color image watermarking based on the Haar wavelet,article,WANG2019102627
"A novel image indexing algorithm for Content Based Image Retrieval (CBIR) using Local Energy Oriented Patterns (LEOP) is proposed in this paper. LEOP encodes pixel level energy orientations to find minute spatial features of an image whereas existing methods use neighborhood relationship. LEOP maps four pixel progression orientations to find top two maximum energy changes for each reference pixel in the image i.e. for each reference 3Ã3 grid, two more 3Ã3 grids out of four pixel progression are extracted. Finally, LEOP encodes the relationship among pixels of three 3Ã3 local grids extracted. LEOP is applied on four different image databases named MESSIDOR, VIA/I-ELCAP, COREL and ImageNet Database using traditional CBIR framework. To test the robustness of proposed feature descriptor the experiment is extended to a learning based CBIR approach on COREL database. The LEOP outperformed state-of-the-art methods in both traditional as well as learning environments and hence it is a strong descriptor.","Local Binary Patterns (LBP), Local Mesh Patterns (LMeP), Local Directional Mask Maximum Edge Patterns (LDMaMEP)",G.M. Galshetwar and L.M. Waghmare and A.B. Gonde and S. Murala,https://www.sciencedirect.com/science/article/pii/S1047320319302366,https://doi.org/10.1016/j.jvcir.2019.102615,1047-3203,2019,102615,64,Journal of Visual Communication and Image Representation,Local energy oriented pattern for image indexing and retrieval,article,GALSHETWAR2019102615
"Media aesthetic assessment is a key technique in computer vision, which is widely applied in computer game rendering, video/image classification. Low-level and high-level features fusion-based video aesthetic assessment algorithms have achieved impressive performance, which outperform photo- and motion-based algorithms, however, these methods only focus on aesthetic features of single-frame while ignore the inherent relationship between adjacent frames. Therefore, we propose a novel video aesthetic assessment framework, where structural cues among frames are well encoded. Our method consists of two components: aesthetic features extraction and structure correlation construction. More specifically, we incorporate both low-level and high-level visual features to construct aesthetic features, where salient regions are extracted for content understanding. Subsequently, we develop a structure correlation-based algorithm to evaluate the relationship among adjacent frames, where frames with similar structure property should have a strong correlation coefficient. Afterwards, a kernel multi-SVM is trained for video classification and high aesthetic video selection. Comprehensive experiments demonstrate the effectiveness of our method.","Video aesthetic assessment, Structure correlation, SVM",Chao Zhang and Sitong Liu and Huizi Li,https://www.sciencedirect.com/science/article/pii/S1047320319302640,https://doi.org/10.1016/j.jvcir.2019.102643,1047-3203,2020,102643,71,Journal of Visual Communication and Image Representation,Quality-guided video aesthetics assessment with social media context,article,ZHANG2020102643
"First, this paper studies the MOH material mixing and paving equipment technology, and quantifies the correlations between each variable through the data fitting based on massive experimental data. Then, with the operation speed as the design variable and yielding the maximum mixing efficiency, minimum slip ratio and highest measuring accuracy as the three performance optimization objectives, an operation speed optimization model for the integrated mixer and paver has been built. Moreover, the principles and workflows of the cuckoo search are investigated, the population diversity and convergence rate of the cuckoo search are improved by using the fuzzy logic, and subsequently the operation speed optimization model is solved using the modified cuckoo search. At last, a simulation test based on MATLAB is carried out for validation. Research results show that the optimization results based on the modified cuckoo search algorithm excels those of frequently-used optimization algorithms such as the conventional cuckoo search and genetic algorithm. Also, the comparison between the cases of the optimized and standard operation speeds show that the mixing efficiency of the integrated mixing and paving machine can grow by 15.3%; the slip ratio drops by 54.2%; the measuring accuracy rises by 18.2%.","Integrated mixing and paving machine, Cooperative control, Cuckoo search, Fuzzy logic",Yikun Yang and Shengjie Jiao and Wenfa Wang,https://www.sciencedirect.com/science/article/pii/S1047320319302123,https://doi.org/10.1016/j.jvcir.2019.102591,1047-3203,2019,102591,63,Journal of Visual Communication and Image Representation,Cooperative media control parameter optimization of the integrated mixing and paving machine based on the fuzzy cuckoo search algorithm,article,YANG2019102591
"Image recognition aims to automatically search special objects in an image, such as human faces, vehicles, or buildings. In medical research, image recognition technique can also be applied for disease diagnosis and disease classification. Aiming at disadvantages of traditional methods in polycystic ovary syndrome (PCOS) recognition, we propose a probabilistic model for disease recognition using a deeply-learned image quality kernel. Specifically, we first segment training images into several equal-size grids for better cues discovery. Then, each grid within an image is quantitatively represented by a quality score according to grayscale and texture features. In this way, each image can be represented by a score matrix. Then, we leverage statistic based method to generate a long feature vector according to the score matrix. Afterward, we propose a probabilistic model to learn the distribution of obtained feature vector, which will be further fed into a SVM kernel for PCOS recognition. Experimental results show the effectiveness of our proposed method.","Probabilistic model, Image quality assessment, Image recognition",Dongyun He and Li Liu and Sheng Miao and Xiaoli Tong and Minjia Sheng,https://www.sciencedirect.com/science/article/pii/S1047320319302081,https://doi.org/10.1016/j.jvcir.2019.102587,1047-3203,2019,102587,63,Journal of Visual Communication and Image Representation,Probabilistic guided polycystic ovary syndrome recognition using learned quality kernel,article,HE2019102587
"Economic development requires energy support, but the economy that relies too much on high energy consumption and high pollution is not sustainable. However, due to blind investment and low-level expansion of some new energy companies, some new energy industry has overcapacity and low production efficiency, slowing down the pace of new energy industry development, and limiting the development space of new energy companies. Thus, this paper is based on the GIS technology and image quality processing design improvement, around this idea, completed the new system simulation analysis, system development, experimental testing and so on. This paper focuses on the evaluation of the distribution efficiency of new energy industry based on image processing. Natural images have their own unique statistical properties, and interference can cause them to deviate from the original ânatural statistical state.â According to the image itself and the characteristics of the human visual system, the expression of the image is adjusted, which is more conducive to the transmission, storage and understanding of information. Therefore, this paper proposes a new energy industry distribution efficiency evaluation method based on GIS technology and image processing from three aspects of the nature of natural images, the expression of image information and the characteristics of human visual system. The research shows that the method can objectively and effectively evaluate the distribution efficiency of the new energy industry, and has certain practicability, which can effectively alleviate the bottleneck problems faced above.","Deep learning, GIS technology, Image quality analysis, Model, Quality",Zhuolun Chen and Xiaowei Wu,https://www.sciencedirect.com/science/article/pii/S1047320319301816,https://doi.org/10.1016/j.jvcir.2019.06.008,1047-3203,2019,410--417,62,Journal of Visual Communication and Image Representation,Research on regional energy efficiency based on GIS technology and image quality processing,article,CHEN2019410
"Convolution neural networks based methods can derive deep features from training images. However, one challenge is that the dimension of the extracted image features increases dramatically with more network layers. To solve this problem, this paper focuses on the study of dimension reduction. After using deep learning to extract image features, the PCA algorithm is used to achieve dimension reduction. Specifically, we first leverage deep convolutional neural network to extract image features. Then, we introduce and leverage PCA algorithm to achieve dimension reduction. Aiming at the problem that it is difficult to process high-dimensional sparse big data based on PCA algorithm. This paper optimizes the PCA algorithm. After image preprocessing, the feasibility of PCA algorithm for dimension reduction of image feature extraction by deep learning is verified by simulation experiments. The efficiency of the proposed algorithm is proved by comparing the performance of PCA algorithm before and after optimization.","Deep learning, Feature extraction, Dimension reduction, PCA algorithm",Ji Ma and Yuyu Yuan,https://www.sciencedirect.com/science/article/pii/S1047320319301932,https://doi.org/10.1016/j.jvcir.2019.102578,1047-3203,2019,102578,63,Journal of Visual Communication and Image Representation,Dimension reduction of image deep feature using PCA,article,MA2019102578
"This paper focuses on learning the local image region representation via deep neural networks. Existing works mainly learn from matched corresponding image patches, with which the learned feature is too sensitive to the individual local patch matching result and cannot handle aggregation based tasks such as image level retrieval. Thus, we propose to use both the matched corresponding image patches and the clustering result as labels for the network training. To resolve the inconsistency between the matched correspondences and clustering results, we propose a semi-supervised iterative training scheme together with a dual margins loss. Moreover, a jointly learned spatial transform prediction network is utilized to obtain better spatial transform invariance of the learned local features. Using SIFT as the label initializer, experimental results show the comparable or even better performance than the hand-crafted feature, which sheds lights on learning local feature representation in an unsupervised or weakly supervised manner.","Local image representation, Local feature learning, Convolutional Neural Network (CNN), Semi-supervised learning, Spatial transform",Jianhan Mei and Xudong Jiang and Jianfei Cai,https://www.sciencedirect.com/science/article/pii/S1047320319302226,https://doi.org/10.1016/j.jvcir.2019.102601,1047-3203,2019,102601,63,Journal of Visual Communication and Image Representation,"Learning local feature representation from matching, clustering and spatial transform",article,MEI2019102601
"Traditional face recognition method usually faces the challenge of varying lighting condition. In this paper, we propose an illumination-invariant local binary descriptor learning method for face recognition. Unlike local binary descriptor (LBP) and its variants, which usually utilize the rigid sign function for binarization despite of data distributions. We first determine a dynamic thresholds strategy including the information of illumination variation to extract nonlinear multi-layer contrast features. Specially, Exponential Discriminant Analysis (EDA) is designed to act as preprocessing which can contribute to improve the discriminative ability of the face image by enlarging the margin between different classes relative to the same class. To further improve the recognition performance, we combined our preliminary work, the adaptive fuzzy fusion framework, to integrate the recognition results for multi-scale features spaces. Extensive experiments conducted on four face databases validate the effectiveness of the proposed method for illumination face recognition.","Illumination invariant face recognition, Local binary patterns, Local nonlinear multi-layer contrast patterns, Fuzzy fusion framework.",Lifang Zhou and Weisheng Li and Yuewei Du and Bangjun Lei and Shan Liang,https://www.sciencedirect.com/science/article/pii/S1047320319302627,https://doi.org/10.1016/j.jvcir.2019.102641,1047-3203,2019,102641,64,Journal of Visual Communication and Image Representation,Adaptive illumination-invariant face recognition via local nonlinear multi-layer contrast feature,article,ZHOU2019102641
"Human motion recognition based on computer vision plays an important role in many fields, such as video surveillance, virtual reality, and medical care. To solve the inaccurate multi-person pose estimation problem and improve the generalizability of the extracted features, this paper proposes a multi-person pose estimation method based on a deep convolutional neural network. This method mainly relies on a top-down structure which includes two stages. In the first stage, the bounding boxes that are likely to contain people are first detected by an improved faster R-CNN. Individuals in the complex scenario are then tailored by box cropping. In the second stage, we combine heatmap detection with coordinate regression to address the single person pose estimation problem. Specially, a deep convolutional ResNet is employed to produce heatmaps of human body. The precise location of each joint is achieved by the fully connected conditional random field. Experimental results demonstrate our method achieves comparable performance with the state-of-the-art ones.","Multi-person pose estimation, Improved faster R-CNN, Top-down structure, ResNet-152 model, Conditional random field",Peng Duan and Tingwei Wang and Maowei Cui and Hongyan Sang and Qun Sun,https://www.sciencedirect.com/science/article/pii/S1047320319301658,https://doi.org/10.1016/j.jvcir.2019.05.010,1047-3203,2019,245--252,62,Journal of Visual Communication and Image Representation,Multi-person pose estimation based on a deep convolutional neural network,article,DUAN2019245
"In this paper, we propose a novel approach for low-resolution face recognition, under uncontrolled settings. Our approach first decomposes a multiple of extracted local features into a set of representative basis (low-rank matrix) and sparse error matrix, and then learns a projection matrix based on our proposed sparse-coding-based algorithm, which preserves the sparse structure of the learned low-rank features, in a low-dimensional feature subspace. Then, a coefficient vector, based on linear regression, is computed to determine the similarity between the projected gallery and query imageâs features. Furthermore, a new morphological pre-processing approach is proposed to improve the visual quality of images. Our experiments were conducted on five available face-recognition datasets, which contain images with variations in pose, facial expressions and illumination conditions. Experiment results show that our method outperforms other state-ofâthe-art low-resolution face recognition methods in terms of recognition accuracy.","Face recognition, Feature fusion, Local features, Low rank approximation, Linear regression, Sparse coding",M. {Saad Shakeel} and Kin-Man Lam and Shun-Cheung Lai,https://www.sciencedirect.com/science/article/pii/S1047320319302111,https://doi.org/10.1016/j.jvcir.2019.102590,1047-3203,2019,102590,63,Journal of Visual Communication and Image Representation,Learning sparse discriminant low-rank features for low-resolution face recognition,article,SAADSHAKEEL2019102590
"Finite mixture models have been widely used for image segmentation in many computer vision and pattern recognition problems. While images of natural scenes are difficult to model, we can employ emerging concepts from statistical physics to achieve better representations. This paper introduces a new class of finite mixture models for solving such problems. The proposed non-extensive mixture models have real-valued power-law exponents that characterize the degree of correlations. The exponents are used to capture rare or frequent occurring patterns in the image. They can describe complex features found with a hierarchy of sizes in natural images: from small objects with a few dozen pixels to large ones that occupy the entire image. We also present a method to determine the parameters based on maximum likelihood estimation. Our numerical experiments indicate more robust and accurate capabilities of non-extensive mixture models for natural image segmentation than conventional mixture models.","Non-extensive statistics, Image segmentation, q-Gaussians, Finite mixture models",Dusan Stosic and Darko Stosic and Teresa Bernarda Ludermir and Tsang Ing Ren,https://www.sciencedirect.com/science/article/pii/S1047320319302196,https://doi.org/10.1016/j.jvcir.2019.102598,1047-3203,2019,102598,63,Journal of Visual Communication and Image Representation,Natural image segmentation with non-extensive mixture models,article,STOSIC2019102598
"Traditional image recognition technology currently cannot achieve the fast real-time high-accuracy performance necessary for road recognition in intelligent driving. Deep learning models have been recently emerging as promising tools to achieve this performance. The recognition performance of such models can be boosted using appropriate selection of the activation functions. This paper proposes a deep learning approach for the classification of road surface conditions, and constructs a new activation function based on the rectified linear unit Rectified Linear Units (ReLu) activation function. The experimental results show a classification accuracy of 94.89% on the road state database. Experiments on public datasets demonstrate that the proposed convolutional neural network model with the improved activation function has better generalization and excellent classification performance.","Deep learning, Road condition, Activation function, Image recognition, Intelligent driving",Lushan Cheng and Xu Zhang and Jie Shen,https://www.sciencedirect.com/science/article/pii/S1047320319302597,https://doi.org/10.1016/j.jvcir.2019.102638,1047-3203,2019,102638,64,Journal of Visual Communication and Image Representation,Road surface condition classification using deep learning,article,CHENG2019102638
We propose a novel inpainting process for color images. Our algorithm is based on the graph-based wavelet regularization and the non-local mean approach. At each step damaged structures are estimated by computing a graph of patches and applying a regularization model using a wavelet transform on graphs. Our approach uses color information of the image to reconstruct missing data according to local geometry. We show that the graph can be used to model geometry information in the frame of inpainting and to merge candidate pixels from a graph-based wavelet regularization. We provide details on numerical approaches and the results highlight an improvement of the geometrical information reconstruction of color images.,"Image processing, Inpainting, Color image",David Helbert and Mohamed Malek and Pascal Bourdon and Philippe CarrÃ©,https://www.sciencedirect.com/science/article/pii/S1047320319302354,https://doi.org/10.1016/j.jvcir.2019.102614,1047-3203,2019,102614,64,Journal of Visual Communication and Image Representation,Patch graph-based wavelet inpainting for color images,article,HELBERT2019102614
"Hashing is one of the most popular image retrieval technique since its fast-computational speed and low storage cost. Recently, deep hashing methods have greatly improved the image retrieval performance in contrast to traditional hashing method. However, the binary hashing representation is only generated from the global image region, which may result in sub-optimal hashing code. Inspired by the latest advance in spatial attention mechanism, we propose an novel end-to-end deep hashing framework which composes of two sub-networks. One sub-network uses spatial attention model to determine the local features from more specific region of interest, another sub-network extracts the global features from original image. By combining the local and global features with learnable hash functions, the proposed deep hashing framework can optimize the deep hash function and high-quality binary code jointly. Numerous experiments on two large scale image benchmarks datasets have shown that the proposed method is superior to other existing methods for image retrieval.","Hash function, Deep learning, Image retrieval, Convolutional neural network (CNN), Deep hashing",Lin-Wei Ge and Jun Zhang and Yi Xia and Peng Chen and Bing Wang and Chun-Hou Zheng,https://www.sciencedirect.com/science/article/pii/S1047320319301920,https://doi.org/10.1016/j.jvcir.2019.102577,1047-3203,2019,102577,63,Journal of Visual Communication and Image Representation,Deep spatial attention hashing network for image retrieval,article,GE2019102577
"Automatically reconstructing 3D sceneries from video sequences is an indispensable technique in computer 3D games, urban planning, and intelligent navigation. Many previous work relies on complicated and expensive equipment to fulfill 3D reconstruction under constrained environments. Nevertheless, such schemes are not readily to be applied for reconstructing 3D sport sceneries, such as basketball and mountain climbing. In this work, we propose a novel deep architecture: 3DSportNet, which reconstructs 3D sport sceneries by making use of multiple handheld videos captured by smart phones. In particular, given a rich of mobile videos captured by users, we extract multiple deep/shallow visual features from each sport video frame by leveraging the weakly-supervised semantic encoding. Afterward, a geometry-aware quality model is designed to summarize the multiple videos into multiple key frames from each single video, wherein the objective is that the selected key frames can maximally reconstruct the multiple sport videos. Based on this, we employ the key frames to reconstruct sport videos by utilizing the PMVS2 software. Comprehensive experimental comparisons and visualization results have shown that our method can produce very real 3D sport sceneries and athletes. Besides, the 3D reconstruction time consumption is reduced by 95% compared to conventional methods.","3D reconstruction, Quality model, Weakly-supervised learning",Zhenkun Liu,https://www.sciencedirect.com/science/article/pii/S104732031930272X,https://doi.org/10.1016/j.jvcir.2019.102651,1047-3203,2019,102651,65,Journal of Visual Communication and Image Representation,3DSportNet: 3D sport reconstruction by quality-aware deep multi-video summation,article,LIU2019102651
"Image quality assessment (IQA) is an indispensable technique in computer vision, which is widely applied in image classification, image clustering. With the development of deep learning, deep neural network (DNN)-based methods have shown impressive performance. Thus, in this paper, we propose a novel method for mechanical equipment fault diagnosis based on IQA. More specifically, we first conduct data acquisition base on our practice. Afterwards, we leverage image processing method for removing noise. Subsequently, we leverage CNN-based method for image classification. Finally, different mechanical equipment images will be grouped into different categories and fault detection can be achieved. Extensive experiments demonstrate the effectiveness and robustness of our method.","Deep learning, Mechanical equipment, Equipment maintenance, Image quality",Xue Chen and Lanyong Zhang and Tong Liu and M.M. Kamruzzaman,https://www.sciencedirect.com/science/article/pii/S1047320319301804,https://doi.org/10.1016/j.jvcir.2019.06.007,1047-3203,2019,402--409,62,Journal of Visual Communication and Image Representation,Research on deep learning in the field of mechanical equipment fault diagnosis image quality,article,CHEN2019402
"Image size reduction for energy-efficient transmission without losing quality is critical in Visual Sensor Networks (VSNs). The proposed method finds overlapping regions using camera locations, which eliminate unfocussed regions from the input images. The sharpness for the overlapped regions is estimated to find the Dominant Overlapping Region (DOR). The proposed model partitions further the DOR into sub-DORs according to capacity of the cameras. To reduce noise effects from the sub-DOR, we propose to perform a Median operation, which results in a Compressed Significant Region (CSR). For non-DOR, we obtain Sobel edges, which reduces the size of the images down to ambinary form. The CSR and Sobel edges of the non-DORs are sent by a VSN. Experimental results and a comparative study with the state-of-the-art methods shows that the proposed model outperforms the existing methods in terms of quality, energy consumption and network lifetime.","Visual sensor network, Image size reduction, Inter-redundancy, Intra-redundancy, Energy consumption, Quality of the image",Maryam Asadzadeh Kaljahi and Palaiahnakote Shivakumara and Mohd Yamani Idna Idris and Mohammad Hossein Anisi and Michael Blumenstein,https://www.sciencedirect.com/science/article/pii/S1047320319301889,https://doi.org/10.1016/j.jvcir.2019.102573,1047-3203,2019,102573,63,Journal of Visual Communication and Image Representation,A new image size reduction model for an efficient visual sensor network,article,KALJAHI2019102573
"This paper proposes a new charge structure, which can form a penetrator with enhanced lateral effect (PELE) or explosively formed projectile (EFP) by different initiation modes. The purpose of this paper is to study the dynamic response and aftereffect of the charge liner under explosive load in PELE mode. To this end, the computer modeling and simulation analysis method was used to simulate the forming and penetrating target progress of the PELE formed by charge liner. The results indicated that the ANSYS/LS-DYNA finite element software could accurately simulate the formation of PELE and the damage process to the target, and we could obtain exact forming and damage data by this method. The PELE damage element could be formed by the new charge structure liner under explosive load, its velocity was 2050â¯m/s, the head-to-tail velocity gradient was only 20â¯m/s, the length of damage element was 63.62â¯mm, and the length-diameter ratio was 1.79. And PELE damage element could effectively damage 15â¯mm target, the inlet opening hole diameter of the target was measured to be 68â¯mm, the outlet diameter was 72â¯mm. This finding could serve a technical basis for computer modeling and simulation technology of charge structure.","Charge structure, Computer modeling and simulation technology, PELE, Liner",J.P. Yin and Y.Y. Han and X.F. Wang and B.H. Chang and F.D. Dong and Y.J. Xu,https://www.sciencedirect.com/science/article/pii/S1047320319302342,https://doi.org/10.1016/j.jvcir.2019.102613,1047-3203,2019,102613,64,Journal of Visual Communication and Image Representation,A new charge structure based on computer modeling and simulation analysis,article,YIN2019102613
"Image captioning models successfully describe the visual contents of images using natural language. To generate more natural and diverse descriptions, a model must learn style-specific patterns and requires collecting style-specific datasets, which is time-consuming. To address this issue, we propose a semi-supervised deep generative model, Semi-supervised Conditional Variational Auto-Encoder (SCVAE). Our model is capable of leveraging more labelled and unlabelled data in the generative model schema. Extensive empirical results demonstrate that compared with the start-of-art models, our proposed method is able to generate more accurate image captions with more extensive styles.","VAE, Image caption, Generative models, Semi-supervised",Nikolai Zakharov and Hang Su and Jun Zhu and Jan GlÃ¤scher,https://www.sciencedirect.com/science/article/pii/S1047320319301890,https://doi.org/10.1016/j.jvcir.2019.102574,1047-3203,2019,102574,63,Journal of Visual Communication and Image Representation,Towards controllable image descriptions with semi-supervised VAE,article,ZAKHAROV2019102574
"Conditional Generative Adversarial Networks (CGANs) have been introduced to generate realistic images from extremely degraded inputs. However, these generative models without prior knowledge of spatial distributions has limited performance to deal with various complex scenes. In this paper, we proposed a image deblurring network based on CGANs to generate ideal images without any blurring assumption. To overcome adversarial insufficiency, an extended classifier with different attribute domains is formulated to replace the original discriminator of CGANs. Inspired by residual learning, a set of skip-connections are cohered to transfer multi-scaled spatial features to the following high-level operations. Furthermore, this adversary architecture is driven by a composite loss that integrates histogram of gradients (HoG) and geodesic distance. In experiments, an uniformed adversarial iteration is circularly applied to improve image degenerations. Extensive results show that the proposed deblurring approach significantly outperforms state-of-the-art methods on both qualitative and quantitative evaluations.","Image deblurring, Generative adversarial network, Residual learning, Prior distribution, Histogram of gradients",Meng Wang and Shengyu Hou and Huafeng Li and Fan Li,https://www.sciencedirect.com/science/article/pii/S104732031930269X,https://doi.org/10.1016/j.jvcir.2019.102648,1047-3203,2019,102648,65,Journal of Visual Communication and Image Representation,Generative image deblurring based on multi-scaled residual adversary network driven by composed prior-posterior loss,article,WANG2019102648
"In recent years, with the continuous development of network science and technology and the continuous promotion of âthree networks convergenceâ, IPTV (Interactive Network Television) has shown a rapid development trend. IPTV is different from the traditional one-way broadcasting mode of television, it can achieve interaction with the audience, and provide more personalized and diversified videos. With the rapid development of new media, massive video resources and a large number of video-related information are sweeping in. The evaluation of video can no longer be limited to the ratings of traditional platforms. Video playback on new media platforms, network impact, video content and other related data are also important indicators affecting video evaluation. Video quality is closely related to video content characteristics. Different videos have different sensitivity to the same packet loss rate. The IPTV video quality evaluation model based on content features considers the video content characteristics. Firstly, the QP, bit rate and motion vector (MV) information of the video is obtained by analyzing the video stream. Then, the time complexity of the video is calculated by using the obtained MV, and the spatial complexity of the video is calculated by quantization parameters and bit rate. Videos are classified by clustering analysis. On this basis, the BP neural network is used to establish the model and evaluate the video quality, so as to better reflect the visual perception of the human eye. The model has low computational complexity and is suitable for IPTV video quality assessment with certain computing power in network nodes.","BP neural network, IPTV video, Quality assessment, Clustering analysis",Ying Yuan and Cong Wang,https://www.sciencedirect.com/science/article/pii/S1047320319302500,https://doi.org/10.1016/j.jvcir.2019.102629,1047-3203,2019,102629,64,Journal of Visual Communication and Image Representation,IPTV video quality assessment model based on neural network,article,YUAN2019102629
"Video summarisation is characterised as the process of extracting meaningful frames or segments from a video that best represents the content of the whole video. The proposed framework surveys and categorizes the existing video summarisation models in the recent research works on the basis of genre. The most important phase of video summarisation is the detection of key frames or segments in the video. The strategy for identifying key frames or segments vary for each genre. The various genre analysed are user generated videos, movies and documentary, sports, surveillance, egocentric and informational talk videos with a total of more than 25 varying parameters significant to the respective genre. Comprehensive evaluations of the results obtained from the models are also included based on quantitative and qualitative parameters. The framework will help the user in deciding the technology to be adopted for video summarisation in a particular domain. The framework also aids in deciding the type of summary suitable for each genre and the available datasets in each genre for experimental analysis.","Video summarisation, Video summary, Genre-specific, Skim, Keyframe",M.U. Sreeja and Binsu C. Kovoor,https://www.sciencedirect.com/science/article/pii/S1047320319301774,https://doi.org/10.1016/j.jvcir.2019.06.004,1047-3203,2019,340--358,62,Journal of Visual Communication and Image Representation,Towards genre-specific frameworks for video summarisation: A survey,article,SREEJA2019340
"In a virtual assembly scenario, a semantic model of mid-air gesture interaction is established through a user-defined elicitation experiment. Considering the spatial-temporal continuity of the gesture movement, a fuzzy interaction scheme of gesture segments is proposed based on Trie Tree and Levenshtein distance. The experiment result proves that this design can effectively alleviate the effect of missing input sequences and recognition errors. Moreover, it helps relieve the memory load for users by establishing a close correlation of input actions.","Natural interaction, Gesture elicitation, Fuzzy match, Guessibility",Wenjun Hou and Guangyu Feng and Yiting Cheng,https://www.sciencedirect.com/science/article/pii/S1047320319302585,https://doi.org/10.1016/j.jvcir.2019.102637,1047-3203,2019,102637,64,Journal of Visual Communication and Image Representation,A fuzzy interaction scheme of mid-air gesture elicitation,article,HOU2019102637
"The multi-temporal high-resolution remote sensing (HRRS) images are usually acquired at different imaging angles, with serious noise interferences and obvious building shadows, so that detecting the changes of urban buildings is a problem. In order to address this challenge, a deep learning-based algorithm called ABCDHIDL is proposed to automatically detect the building changes from multi-temporal HRRS images. Firstly, an automatic selection method of labeled samples of building changes based on morphology (ASLSBCM) is proposed. Secondly, a deep learning model (DBN-ELM) for building changes detection based on deep belief network (DBN) and extreme learning machine (ELM) is proposed. A convolution operation is employed to extract the spectral, texture and spatial features and generate a combined low-level features vector for each pixel in the multi-temporal HRRS images. The unlabeled samples are introduced to pre-train the DBN, and the parameters of DBN-ELM are globally optimized by jointly using the ELM classifier and the labeled samples are offered by ASLSBCM to further improve the detection accuracy. In order to evaluate the performance of ABCDHIDL, four groups of double-temporal WorldView2 HRRS images in four different experimental regions are selected respectively as the test datasets, and five other representative methods are used and compared with ABCDHIDL in the experiments of buildings change detection. The results show that ABCDHIDL has higher accuracy and automation level than the other five methods despite its relatively higher time consumption.","Deep belief network, Building change detection, Morphological building index, Morphological shadow index, Extreme learning machine, Quality Assessment",Fenghua Huang and Ying Yu and Tinghao Feng,https://www.sciencedirect.com/science/article/pii/S1047320319302068,https://doi.org/10.1016/j.jvcir.2019.102585,1047-3203,2019,102585,63,Journal of Visual Communication and Image Representation,Automatic building change image quality assessment in high resolution remote sensing based on deep learning,article,HUANG2019102585
"Glaucoma is a progressive eye disease due to the increase in intraocular pressure. Accurate early detection may prevent vision loss. Most algorithms in the literature are not feasible for use in screening programs since they are not able to handle a wide diversity of images. We conducted an extensive study to determine the best set of features for image representation. Our feature extraction methodology included the following descriptors: LBP, GLCM, HOG, Tamura, GLRLM, morphology, and seven CNN architectures, that results in 30.682 features. Then, we used the gain ratio to order the features by importance and select the best set for glaucoma classification. Our tests were performed using 1675 images of DRISHTI, RIM-ONE, HRF, JSIEC, and ACRIMA databases. We concluded that a combination of the GLCM and pre-trained CNNâs has the potential to be used in a computer aid system for glaucoma detection. Our approach achieved an accuracy of 93.61%.","Glaucoma detection, Feature selection, Pre-trained CNNs, Transfer learning",MaÃ­la Claro and Rodrigo Veras and AndrÃ© Santana and FlÃ¡vio AraÃºjo and Romuere Silva and JoÃ£o Almeida and Daniel Leite,https://www.sciencedirect.com/science/article/pii/S1047320319302184,https://doi.org/10.1016/j.jvcir.2019.102597,1047-3203,2019,102597,64,Journal of Visual Communication and Image Representation,An hybrid feature space from texture information and transfer learning for glaucoma classification,article,CLARO2019102597
"In recent years, discriminative correlation filters based trackers have made remarkable achievements for single object tracking, while directly applying these trackers for multi-object tracking may encounter some problem in drifted results caused by occlusion and missing detection from the detector. Thus, we propose a weighted-correlation-filters framework with spatial-temporal attention mechanism for online multi-object tracking to solve the above problems. First, we use the weighted correlation filters with dynamic updating scheme to pre-track each object in the current frame, which helps to filter out the improper detection according to the position of pre-tack for each object and is capable of tracking objects of the false negative. Then, we introduce a spatial-temporal attention mechanism to produce a discriminative appearance model and calculate reliable similarity scores for data association. The proposed online algorithm achieves 48.4% in MOTA on challenging MOT17 benchmark dataset and better performance on MT and ML than some offline methods.","Multi-object tracking, Weighted correlation filters, Tracking by detection, Spatial-temporal attention mechanism",Sheng Tian and Lian Zou and Cian Fan and Liqiong Chen,https://www.sciencedirect.com/science/article/pii/S1047320319301919,https://doi.org/10.1016/j.jvcir.2019.102576,1047-3203,2019,102576,63,Journal of Visual Communication and Image Representation,Weighted correlation filters guidance with spatial-temporal attention for online multi-object tracking,article,TIAN2019102576
"The traditional method for reconstructing cityscape relies greatly on the subjective judgment of designers, which makes the cityscape simple and homogenized. This paper aims to propose a new integrated approach to protect and design cityscape based on virtual reality (VR) and eye tracking technology. Through the integration and quantification of the eye tracking data and the protocol analysis data in the VR environment, this research has revealed the mechanism of identifying the cityscape features, and discovered the differences in the perception of the cityscape features by different people, thus proposing the multi-cultural integrated strategy for protecting cityscape. This research is of great significance for building a human-oriented scientific planning and protection method and promoting the application of cutting-edge digital technology in the field of smart city governance.","Urban renewal, Three-dimensional eye tracking, Cityscape feature identification, Human-oriented, Smart city",Le-Min Zhang and Ruo-Xi Zhang and Tay-Sheng Jeng and Zi-Yuan Zeng,https://www.sciencedirect.com/science/article/pii/S1047320319302603,https://doi.org/10.1016/j.jvcir.2019.102639,1047-3203,2019,102639,64,Journal of Visual Communication and Image Representation,Cityscape protection using VR and eye tracking technology,article,ZHANG2019102639
"Content-sensitive superpixel segmentation generates small superpixels in content-dense regions and large superpixels in content-sparse regions. It achieves higher segmentation accuracy than traditional superpixels. In this paper, we propose a content-sensitive superpixel segmentation algorithm based on Self-Organization-Map (SOM) neural network. First, we propose a novel metric to measure the content-sensitiveness of superpixels. Second, by using this metric, we develop a sampling algorithm to sample pixels from image according to their content-sensitiveness. Finally, a SOM neutral network is trained with the sampled pixels and used to segment the image into content-sensitive superpixels. The Berkeley Image Segmentation database and INRIA database are used to evaluate the proposed method. The experiment results show that the proposed approach outperforms state-of-the-art methods.","Superpixel segmentation, Content sensitive, Self-Organization Map (SOM), Clustering",Murong Wang and Xiabi Liu and Nouman Q. Soomro and Guanhui Han and Weihua Liu,https://www.sciencedirect.com/science/article/pii/S1047320319301877,https://doi.org/10.1016/j.jvcir.2019.102572,1047-3203,2019,102572,63,Journal of Visual Communication and Image Representation,Content-sensitive superpixel segmentation via self-organization-map neural network,article,WANG2019102572
"Image quality optimization is a key technique in image processing, whose goal is to improve image quality by image enhancement or image format transform. This paper aims at optimizing image acquisition using Lidar registration, which can cope with disadvantages of conventional algorithms such as low-resolution. Specifically, we propose an iterative termination optimization strategy based on image quality perception features and local mean estimation. First, fuzzy images with different types and degrees of distortion are incorporated to form a representative natural image set, and feature maps of fuzzy images are extracted by the natural scene statistical method in the spatial domain. Noticeably, the proposed algorithm which performs iterative deblurring operation records the optimal iteration point based on recording the quality value FSIM of the restored image, and calibrates the corresponding feature vector in the sample library with the optimal iteration point (step number). Afterwards, we leverage LME method to implement an estimate of the number of iteration steps. Based on these two steps, the estimation of the initial iterative monitoring point is completed, so that the subsequent adaptive iterative termination work is more purposeful to monitor the defuzzification metric. The optimization operation can be completed faster effectively.","Laser radar, Registration image, Quality optimization, Image processing",Wanyi Zhang and Xiuhua Fu and Chunyang Wang,https://www.sciencedirect.com/science/article/pii/S104732031930255X,https://doi.org/10.1016/j.jvcir.2019.102634,1047-3203,2019,102634,64,Journal of Visual Communication and Image Representation,Image quality optimization towards lidar registration based on iterative termination,article,ZHANG2019102634
"Object detection and image classification are basic tasks in computer vision. In this paper, we introduce fault detection towards transmission line. Traditional fault detection methods in the transmission line are prone to be affected by the noise and transient magnitude. To overcome these limitations, we propose a novel fault zone detection method, where quality-aware fine-grained categorization model is well encoded for category cues discovery. The goal of our approach is to recognize the most discriminative image patches for classification. The key techniques of our method include quality-based discriminative feature extraction and wavelet-support vector machine. We extract the features of the line currents by leveraging Fast R-CNN based image samples decomposition, where quality module is utilized to choose the most discriminative regions. Afterwards, the extracted features are fed into a SVM to recognize the fault. We conduct comprehensive experiment on transmission line fault identification to verify the availability and superiority of our proposed method.","Fine-grained categorization, Fault recognition, Quality model, Fast R-CNN, SVM",Yanhai Wang and Qingquan Li and Bo Chen,https://www.sciencedirect.com/science/article/pii/S1047320319302688,https://doi.org/10.1016/j.jvcir.2019.102647,1047-3203,2019,102647,64,Journal of Visual Communication and Image Representation,Image classification towards transmission line fault detection via learning deep quality-aware fine-grained categorization,article,WANG2019102647
"Data-driven saliency estimation attracts increasing interests in recent years because of the establishment of large-scale annotated datasets and the evolution of deep convolutional neural networks (CNN). Although CNN-based models perform much better than traditional ones in saliency prediction, there is still a gap between computational models and human behavior. One reason is that existing approaches fail assigning correct saliency to different objects in scenes with multiple objects. In this paper, we propose a multiscale dilated dense convolutional network to handle instance-level attention competition for better saliency prediction. In the proposed architecture, dense connections encode inter- and intra-class features for instance-level attention competition, dilated convolution collects contextual information to enrich feature representations of instances, and shortcut connections provide multiscale features for attention competition across scales. According to evaluations on three challenging datasets, CAT2000, SALICON, and MIT1003, the proposed model achieves the state-of-the-art performance.","Saliency, Attention competition, Convolutional neural networks, Dense connections, Dilated convolution, Multiscale features",Hao Li and Fei Qi and Guangming Shi and Chunhuan Lin,https://www.sciencedirect.com/science/article/pii/S1047320319302329,https://doi.org/10.1016/j.jvcir.2019.102611,1047-3203,2019,102611,64,Journal of Visual Communication and Image Representation,A multiscale dilated dense convolutional network for saliency prediction with instance-level attention competition,article,LI2019102611
"Image appearance transfer is the process of transferring the appearance features from the user-supplied reference image to the target image. Since traditional appearance transfer methods are based on low-level feature, it is difficult to obtain natural effect in the case of complex illumination between the target image and the reference image. This is mainly because that the appearance mapping relation with complex illumination is a highly nonlinear problem. Although traditional methods are good at dealing with linear transformations, they are less suitable for solving the highly nonlinear problems caused by such complex illumination. Moreover, convolutional neural network (CNN) can extract the hierarchical abstraction features at different levels of the network layer, so it can be conveniently used to realize progressive transfer. In this paper, we propose a novel method for progressive appearance transfer for images with complex illumination. Firstly, we convert the input images from the RGB color space to the HSV color space. The illumination transfer is carried out only in the illumination channel of the image, and for the other two channels, the color distribution of the reference image is transferred to the target image. Secondly, CNN is specially designed to extract the hierarchical feature maps. To achieve progressive transfer, the histogram reshaping method is carried out by using the hierarchical feature maps extracted from the CNN. The appearance transfer results are obtained after the illumination transfer and color transfer. To optimize the transfer results, we adopt the joint bilateral filter to smooth the noises. The experimental results show that our method can effectively solve the problem of progressive appearance transfer for images with complex illumination.","Appearance transfer, Complex illumination, Histogram reshaping, Color transfer",Shiguang Liu and Zhichao Song and Xiaoli Zhang and Ting Zhu,https://www.sciencedirect.com/science/article/pii/S1047320319302573,https://doi.org/10.1016/j.jvcir.2019.102636,1047-3203,2019,102636,64,Journal of Visual Communication and Image Representation,Progressive complex illumination image appearance transfer based on CNN,article,LIU2019102636
"The current work of fine-grained classification generally depends on a large number of fine labels of images. However, these fine labels are much more difficult to annotate than the coarse labels, which generalize fine labels based on the hierarchy of categories. In this paper, we propose to make fine labels prediction under a weakly supervised setting where a subset of training data is labeled with fine labels and the others only have coarse labels. We aim to explore the hierarchy relationship between coarse classes and fine classes to achieve a better performance on fine-grained classification and meanwhile reduce the heavy dependence on fine labels. To this end, we use convolutional block attention module and multi-scale convolution kernel based feature fusion to generate more effective features from multi-scale convolution kernels and multi-level features. Besides, an adaptive classification module exploits the hierarchy relationship of categories to learn the fine-grained classifier automatically according to the available labels of the training data. Comprehensive experiments on the CIFAR100 dataset, a subset of ImageNet and CUB-200-2011 dataset demonstrate the better fine-grained classification performance of our model.","Fine-grained classification, Weakly labeled images, Convolutional block attention, Feature fusion",Qihan Jiao and Zhi Liu and Linwei Ye and Yang Wang,https://www.sciencedirect.com/science/article/pii/S1047320319302056,https://doi.org/10.1016/j.jvcir.2019.102584,1047-3203,2019,102584,63,Journal of Visual Communication and Image Representation,Weakly labeled fine-grained classification with hierarchy relationship of fine and coarse labels,article,JIAO2019102584
"As an important part of the integration of Beijing-Tianjin-Hebei, it is very important to analyze the seismic activity of active structures in Central north China. There are two sets of active faults belt in the lot, and there have been devastating earthquakes, which need to grasp the level of seismic activity. Located at the boundary of the third-order tectonic unit, there are a series of faults in the area, such as the north to the east Taihang mountain front fault, the north to the east Xinhe fault and the north to the west Cixian-daming fault, which intersect and cut each other to form fault depression basin. There are different scales of NE, NNE, and NW faults, which are considered to be the birthplace of the earthquake. At the same time, more than 6 magnitude earthquake magnitude have happened in the Cixian and Xingtai. The seismogenic structure of the research shows that these earthquakes associated with deep fault activities, the source location in the deep crust velocity structure mutation. In order to determine and analyze the P-wave velocity structure characteristics and the hypocenter distribution, and the activity characteristics of the deep space of active fault belt, the natural seismic data monitored by seismic network are collected and organized, which are used to analyze the relationship between seismic wave velocity and hypocenter position. Due to the deep migration of the crustal material and the horizontal principal compressive the NEE direction stress in North China, the crustal thickness on the west side of the Taihang mountain front fault is greater than that of the east side, from 1â¯km to 7â¯km. Along the trend, the epicenter of the small earthquake is mainly distributed in the crustal thickening area on the west side of this active fault, and the epicenter of the eastern plain is less distributed. The depth of the small earthquake is concentrated in the range of 8â20â¯Km. Comprehensive analysis shows that the seismic p-wave velocity structure characteristics can be divided into the sedimentary cover, upper crust, the earth's crust and the lower crust structure, thickness of different location have change, the thickness of the sedimentary cover Taihang uplift zone thickness 0.1â3â¯km, to 5â7â¯km in Handan fault depression; The thickness of the crystalline basement in the Taihang mountain uplift is 3â5â¯km, and the Handan fault depression basin is thickened to 7â10â¯km. The thickness of the crust on the west side of Taihang mountain front fault is significantly greater than that on the east side. The thickness of the crust on the west side is decreased from 36â40â¯km on the west side to 30â35â¯km on the east side and about 7â10â¯km on the east side. Due to the near east-west tension, the zone has disengaging movement, forming the characteristics of shovel-type normal fault combination. In the earth's crust with high-speed and low-speed layer between configuration characteristics, seismic horizon of earthquake preparation 12â18â¯km deep in the earth's crust, characterized by low speed and high speed layer mutation position, concentrated distribution of small earthquakes, the seismogenic layer a concentration distribution in the crust velocity structure conversion section. Seismic activity is concentrated in the west end of the Cixian-daming fault and the west side of the Xinhe fault, with an average depth of 12â18â¯km.","Active fault zone, Earthquake distribution, Taihang mountain front fault, Velocity inversion",Junjie Zhou and Guowei Zhu and Qingchao Zhang and Zhenqiang Yang and Pengfei Sun and Jiahao Liu,https://www.sciencedirect.com/science/article/pii/S1047320319302330,https://doi.org/10.1016/j.jvcir.2019.102612,1047-3203,2019,102612,65,Journal of Visual Communication and Image Representation,Analysis of active faults based on natural earthquakes in Central north China,article,ZHOU2019102612
"In recent years, geological disasters caused by surface deformation frequently occur, which seriously threatens the safety of people's lives and property. Therefore, it is of great significance to strengthen the monitoring of surface deformation. With the continuous advancement of science and technology, traditional monitoring technology is difficult to meet the development requirements of modern society. As a new type of space-to-earth observation technology, INSAR technology has the advantages of high precision and real-time dynamic monitoring, and has been obtained in surface deformation monitoring widely used. This paper briefly analyzes the basic working principle of INSAR technology and its specific application in surface deformation monitoring. The algorithm parallelism of the ground-based SAR deformation monitoring process is analyzed, and the CPUâ¯+â¯GPU heterogeneous platform is used to accelerate the implementation to improve the timeliness of deformation monitoring. BP imaging algorithm, interferogram generation, interferogram filtering and phase unwrapping algorithm are designed in parallel, and appropriate parallel granularity planning for multiple loops, adaptive division of optimal thread block size and use of shared memory to reduce duplicate data are adopted. Optimization strategies such as read time enable GPU acceleration processing. Compared with the implementation of CPU platform and CPUâ¯+â¯GPU heterogeneous platform, the acceleration effect from tens to hundreds of times is accelerated, and the feasibility of GPU to improve the timeliness of deformation monitoring is verified.","INSAR, Surface deformation, Monitoring, GPU acceleration",Zhengquan Hu and Bin Li and Yu Liu and Xiaowei Niu,https://www.sciencedirect.com/science/article/pii/S1047320319302731,https://doi.org/10.1016/j.jvcir.2019.102652,1047-3203,2019,102652,64,Journal of Visual Communication and Image Representation,Research on quality improvement method of deformation monitoring data based on InSAR,article,HU2019102652
"Internet of Things (IoT) is widely applied in modern power systems, which could establish the intelligent power grid systems and obtain considerable social and economic benefits. IoT plays an important role in power grid safety production, user interaction, and information collection. However, existing methods cannot address problems of IoT devices accurately and quickly, such as fault detection. Aiming at the shortcomings of current power IoT equipment fault detection methods, this paper proposes a multi-spectral image fusion based on deep learning to detect fault points of power IoT equipment. The deep convolutional neural network is trained by simulating the image of the power device. The results show that the multi-spectral image descriptor based on deep learning presented in this paper shows very high accuracy in block matching, and the effect of image fusion is remarkable. This indicates that the proposed method can accurately integrate multi-spectral images of power equipment, helping to locate fault points quickly and accurately.","Convolution neural network, IoT fault point detection, Deep learning, Multi-spectral image fusion",Hou Rui and Zhao Yunhao and Tian Shiming and Yang Yang and Yang Wenhai,https://www.sciencedirect.com/science/article/pii/S1047320319302214,https://doi.org/10.1016/j.jvcir.2019.102600,1047-3203,2019,102600,64,Journal of Visual Communication and Image Representation,Fault point detection of IOT using multi-spectral image fusion based on deep learning,article,RUI2019102600
"In this paper, removal of additive, signal-independent, correlated noise from images is considered. We consider the non-blind case, meaning that the stationary autocovariance function of the noise is assumed to be known. The denoising method is based on unrolled optimization where in the half-quadratic energy minimization each proximal step is replaced by a learnt convolutional neural network. The proximal steps take place in learnt transform domains. Functions producing the regularization parameters are also learnt. We assume that we have a distribution for autocovariance functions in order to be able to draw samples. For simplicity, we assume that the noise has low-pass spectral character and a typical autocovariance function has a relatively simple form. The experimental results demonstrate that in terms of PSNR values, the method performs better than two classical methods and a method based on a learnt patch prior.","Image denoising, Correlated noise, Convolutional neural networks, Deep learning",Juha Tiirola,https://www.sciencedirect.com/science/article/pii/S1047320319301762,https://doi.org/10.1016/j.jvcir.2019.06.003,1047-3203,2019,286--294,62,Journal of Visual Communication and Image Representation,"A learning based approach to additive, correlated noise removal",article,TIIROLA2019286
"Ordered porous materials, especially mesoporous materials, molecular sieves and organometallic framework materials, are widely used in the fields of adsorption, membrane separation and catalytic reaction processes due to their unique properties. Molecular sieve porous material is a new type of engineering material with excellent performance, and its microstructure is one of the key factors affecting macroscopic physical properties and use effect. At present, in order to further broaden its application in the fields of separation, catalysis, sensors and micro-devices, at present, microscopic molecular level control of porous material composition and structure, macroscopically controlling its appearance and appearance has become a research of porous materials. An important development direction. However, the quantitative characterization of the molecular structure of molecular sieve porous materials and its influence on physical properties has always been the focus and difficulty in the field of materials science and engineering. At the same time, the microstructure of porous materials of molecular sieves is the key factor affecting its macroscopic physical properties, and the analysis of spatial structure characteristics. Has important research significance. Based on this paper, a method for predicting effective diffusion coefficient in porous materials by convolutional neural network is proposed. The training samples of porous material microstructure are generated by computer stochastic simulation, and the corresponding effective diffusion coefficients are calculated by finite element method. The image quality subjective evaluation model is used to control the microscopic picture precision of the molecular sieve porous material. The combination of the two methods can quickly and accurately calculate the effective diffusion coefficient.","Image recognition, Image quality",Hua Wang and Cong Li,https://www.sciencedirect.com/science/article/pii/S1047320319302299,https://doi.org/10.1016/j.jvcir.2019.102608,1047-3203,2019,102608,64,Journal of Visual Communication and Image Representation,Quality guided image recognition towards industrial materials diffusion,article,WANG2019102608
"The temporal component of videos provides an important clue for activity recognition, as a number of activities can be reliably recognized based on the motion information. In view of that, this work proposes a novel temporal stream for two-stream convolutional networks based on images computed from the optical flow magnitude and orientation, named Magnitude-Orientation Stream (MOS), to learn the motion in a better and richer manner. Our method applies simple non-linear transformations on the vertical and horizontal components of the optical flow to generate input images for the temporal stream. Moreover, we also employ depth information to use as a weighting scheme on the magnitude information to compensate the distance of the subjects performing the activity to the camera. Experimental results, carried on two well-known datasets (UCF101 and NTU), demonstrate that using our proposed temporal stream as input to existing neural network architectures can improve their performance for activity recognition. Results demonstrate that our temporal stream provides complementary information able to improve the classical two-stream methods, indicating the suitability of our approach to be used as a temporal video representation.","Activity recognition, Convolutional neural networks (CNNs), Two-stream convolutional networks, Spatiotemporal information, Optical flow, Depth information",Carlos Caetano and Victor H.C. {de Melo} and FranÃ§ois BrÃ©mond and Jefersson A. {dos Santos} and William Robson Schwartz,https://www.sciencedirect.com/science/article/pii/S1047320319302172,https://doi.org/10.1016/j.jvcir.2019.102596,1047-3203,2019,102596,63,Journal of Visual Communication and Image Representation,Magnitude-Orientation Stream network and depth information applied to activity recognition,article,CAETANO2019102596
"Models play an important role in inverse problems, serving as the prior for representing the original signal to be recovered. REgularization by Denoising (RED) is a recently introduced general framework for constructing such priors using state-of-the-art denoising algorithms. Using RED, solving inverse problems is shown to amount to an iterated denoising process. However, as the complexity of denoising algorithms is generally high, this might lead to an overall slow algorithm. In this paper, we suggest an accelerated technique based on vector extrapolation (VE) to speed-up existing RED solvers. Numerical experiments validate the obtained gain by VE, leading to substantial savings in computations compared with the original fixed-point method.","Inverse problem, RED â REgularization by Denoising, Fixed-point, Vector extrapolation, Acceleration",Tao Hong and Yaniv Romano and Michael Elad,https://www.sciencedirect.com/science/article/pii/S1047320319301907,https://doi.org/10.1016/j.jvcir.2019.102575,1047-3203,2019,102575,63,Journal of Visual Communication and Image Representation,Acceleration of RED via vector extrapolation,article,HONG2019102575
"In this paper, we propose a novel Deep Spectral Feature Pyramid in the Frequency domain (DSFP) to share the merits of deep features and spectral approaches for long-term action recognition. More specifically, in the spatial domain, deep features of sparse sampled frames are extracted by Convolutional Neural Networks (CNNs) to cover long-term temporal structure. In the frequency domain, appearance features of sampled frames are partitioned recursively along the time dimension and spectral transform is applied to each partitioned feature respectively. All coefficients of partitioned features are then concatenated into a video-level feature to better model the spatio-temporal structure of actions in the form of a pyramid. So DSFP could model actions from both microcosmic and macroscopic aspects. Extensive experiments conducted on two challenging action benchmarks UCF101 and HMDB51 show that our proposed DSFP is effective for spatio-temporal representation of actions and achieves comparable performance with the state-of-the-arts.","Action recognition, Deep learning, Spectral feature, Video classification",Gaoyun An and Zhenxing Zheng and Dapeng Wu and Wen Zhou,https://www.sciencedirect.com/science/article/pii/S1047320319302718,https://doi.org/10.1016/j.jvcir.2019.102650,1047-3203,2019,102650,64,Journal of Visual Communication and Image Representation,Deep spectral feature pyramid in the frequency domain for long-term action recognition,article,AN2019102650
"With the development of science and technology, image processing is applied more and more widely. In power system, a large number of power equipment always work in harsh environment, which easily leads to equipment damage, not only affects the normal operation of equipment, but also may lead to accidents. In order to ensure the normal operation of power equipment, it is necessary to monitor the operation status of power equipment in real time. The key of real-time monitoring is how to analyze the quality of infrared image. In order to improve the quality of infrared image of power equipment, this paper analyses the traditional image processing technology, mainly using partial differential equation to optimize and improve the image enhancement algorithm and segmentation method, so as to improve the quality of infrared image of power equipment. Firstly, in the aspect of infrared image enhancement, partial differential equation is used to improve the shortcomings of the traditional contrast enhancement method, enhance the texture details of the image while effectively improving the brightness visual effect of the image; then, in image segmentation, the stopping function of GAC model of classical partial differential equation is improved to improve the effect of infrared image segmentation of power equipment. Through the analysis of simulation experiments, this paper improves the traditional method by using partial differential equation in image enhancement and image segmentation, which can effectively improve the quality and segmentation effect of infrared image of power equipment.","Partial differential equation, Power equipment, Infrared image, Quality model",Dun Nan Liu and Rui Hou and Wen Zhuo Wu and Jing Wen Hua and Xuan Yuan Wang and Bo Pang,https://www.sciencedirect.com/science/article/pii/S1047320319302317,https://doi.org/10.1016/j.jvcir.2019.102610,1047-3203,2019,102610,64,Journal of Visual Communication and Image Representation,Research on infrared image enhancement and segmentation of power equipment based on partial differential equation,article,LIU2019102610
"Image classification aims to automatically group a set of images into several categorizations, which is widely applied in scene categorization, image clustering. Lung cancer recognition can be achieved by using image classification technique, since there are distinct differences between healthy lung and sick lung images. In this paper, we propose lung cancer recognition based on image quality assessment, which can distinguish sick lung images from healthy lung images. First, our dataset is acquired using low-dose CT scan combined with full-mode iterative recombination (IMR). Then, we incorporate both low-level and high-level features to extract deep representation from obtained dataset. Specifically, our designed low-level features include color moment and texture feature, and CNN based method is leveraged for deep feature extraction. For reducing artifacts and noise of images, we assign quality score for each training image. And quality score and deep feature are fused to generate deep representation. Afterward, we propose a probabilistic model to learn the distribution of deep representation. Finally, lung cancer recognition can be achieved using learned model. We conduct comprehensive experiments and our proposed method is verified effective.","Image classification, Cancer recognition, Deep feature, CNN",Ying Liu and Haodong Wang and Yue Gu and Xiaohong Lv,https://www.sciencedirect.com/science/article/pii/S1047320319301865,https://doi.org/10.1016/j.jvcir.2019.06.012,1047-3203,2019,102570,63,Journal of Visual Communication and Image Representation,Image classification toward lung cancer recognition by learning deep quality model,article,LIU2019102570
"In recent years, the Kinect-based systems that enable users to be trained without the participation of teachers have been widely used in the field of physical education. In this paper, we propose a novel technique that helps the Kinect-based training system to select the subsequential training material for the users according to their realtime performance. An algorithm based on the Hidden Markov Model is demonstrated to generate the customized training pathes(training curriculums) for each individual. We present an edutainment gaming system for children in order to illustrate the feasibility of the training method. A user study of 10 children participants is conducted and the results show that the proposed technique enhances the effect of physical training significantly.","Kinect, Educational games",Mingliang Xu and Yafang Zhai and Yibo Guo and Pei Lv and Yafei Li and Meng Wang and Bing Zhou,https://www.sciencedirect.com/science/article/pii/S1047320319301622,https://doi.org/10.1016/j.jvcir.2019.05.007,1047-3203,2019,394--401,62,Journal of Visual Communication and Image Representation,Personalized training through Kinect-based games for physical education,article,XU2019394
"The identification model that employs softmax loss to minimize person identity classification errors has gradually gained popularity in person re-identification community due to its easy implementations. However, the softmax loss only encourages the separation of different identities. The intra-class differences caused by large view variations such as spatial misalignment and human pose change are not considered in the model training process. In this paper, we present a hybrid deep model that combines multiple loss functions to handle this problem. Specifically, the multi-loss function contains three terms, namely softmax loss, center loss, and a novel loss called inter-center loss. The center loss penalizes the distance between deep features and their center, aiming to reduce intra-class differences. The inter-center loss maximizes the distances between different class centers, aiming to further enlarge inter-class separation. Extensive experiments conducted on three public benchmark datasets including Market1501, CUHK03, and DukeMTMC-reID demonstrate the effectiveness of our method.","Person re-identification, Multi-loss training, Inter-center loss",Weilin Zhong and Tao Zhang and Linfeng Jiang and Jinsheng Ji and Zenghui Zhang and Huilin Xiong,https://www.sciencedirect.com/science/article/pii/S1047320319301749,https://doi.org/10.1016/j.jvcir.2019.06.001,1047-3203,2019,267--278,62,Journal of Visual Communication and Image Representation,Discriminative representation learning for person re-identification via multi-loss training,article,ZHONG2019267
"Image recognition is an indispensable technique in computer vision, such as face recognition, vehicle recognition. It can be also widely used in medical research. Coronary heart disease recognition is still a big challenge due to artifacts or noise. In addition, recognizing coronary heart disease manually is a huge workload. So in this paper, we exploit representative structure cues for coronary heart disease recognition. More specifically, we first leverage Iterative Reconstruction (IR) algorithm to reduce the radiation dose during CT angiography acquisition. For structure cues discovery, we propose grid-based image segmentation algorithm, where each grid is represented by an according image quality score. High probability grid of disease cues corresponds to high quality score. Then, each training image is represented using quality score matrix. Afterward, we incorporate low-level features (grayscale and texture) with quality score matrix. Finally, image matching algorithm is proposed for structure cues discovery from test images. Experimental results demonstrate the effectiveness of our proposed method.","Image recognition, Image quality assessment, Iterative reconstruction algorithm, Image match",Miao Liu and Xiaoli Rong and Tiechao Jiang,https://www.sciencedirect.com/science/article/pii/S1047320319302287,https://doi.org/10.1016/j.jvcir.2019.102607,1047-3203,2019,102607,64,Journal of Visual Communication and Image Representation,Representative discovery of structure cues for coronary heart disease recognition based on quality assessment,article,LIU2019102607
"Siamese network based similarity-learning algorithm is currently a significant branch of visual tracking. However, most of existing deep Siamese networks depend much on the offline-trained knowledge and always assume the same importance for different prediction views. In this paper, we first introduce a dynamic weighting module in Siamese framework, which could make the offline-trained network adapt to the current circumstance well and weight predictive response maps discriminatively. The thought stems from the basis that different maps have different predictive preference, which should not be treated equally. Secondly, in order to focus more on the accurate preference, we then introduce the residual structure to form the residual dynamic weighting module. Thirdly, we construct a simple online pyramid-redetection module to avoid local search and also consider the global viewpoint. Extensive experiments on both short-term and long-term tracking demonstrate that the proposed tracker possesses the competitive tracking performance over many mainstream state-of-the-art trackers.","Visual tracking, Siamese networks, Dynamic weighting, Residual structure, Convolutional neural networks, Pyramid-redetection",Yi Cao and Hongbing Ji and Wenbo Zhang and Fei Xue,https://www.sciencedirect.com/science/article/pii/S1047320319302561,https://doi.org/10.1016/j.jvcir.2019.102635,1047-3203,2019,102635,65,Journal of Visual Communication and Image Representation,Visual tracking via dynamic weighting with pyramid-redetection based Siamese networks,article,CAO2019102635
"Dynamic gesture recognition is a major topic for most real-time human-robot interaction applications. Recently, increasingly people have focused on dynamic gesture recognition based on 3D representation. In this paper, the gesture recognition is converted into the shortest path problem by transforming the feature matrix to an undirected graph and a novel dynamic gesture recognition algorithm of directional pulse coupled neuron network (DPCNN) is proposed for real time human-robot interactions. The DPCNN can select the firing direction by giving different excitations to neighbor neurons and reduce the effects of useless neurons. Furthermore, to reduce the recognition time, an early gesture recognition method based on the adaptive window is introduced to recognize the unfinished gestures. DPCNN reduces the computation time and achieves a high recognition rate on three public datasets compared with other algorithms which improves the efficiency of real-time dynamic gesture recognition and ensures a friendly experience for human-robot interactions.","Human-robot interaction, Dynamic gesture recognition, Graph theory problem, Directional pulse coupled neuron network (DPCNN), Early recognition",Jiaqi Dong and Zeyang Xia and Weiwu Yan and Qunfei Zhao,https://www.sciencedirect.com/science/article/pii/S1047320319302044,https://doi.org/10.1016/j.jvcir.2019.102583,1047-3203,2019,102583,63,Journal of Visual Communication and Image Representation,Dynamic gesture recognition by directional pulse coupled neural networks for human-robot interaction in real time,article,DONG2019102583
"Intelligent power grid systems is the trend of power development, since traditional methods of manually monitoring power equipment have been unable to meet the requirements of power systems. When an abnormal situation occurs in the operating environment, most monitoring devices cannot be quickly and accurately identified, which may have serious consequences. Aiming at the above problems, in this paper, we propose an anomaly detection algorithm for the monitoring environment of power IoT equipment operating environment based on deep learning from the perspective of personnel identification and fire smoke detection. The multi-stream CNN-based remote monitoring image personnel detection method and the deep convolutional neural network-based fire smoke detection method have achieved good results in personnel identification and fire smoke detection in the power equipment operating environment monitoring image, respectively. This provides a reference for monitoring image anomaly detection.","Operating environment monitoring, Image anomaly detection, Deep learning",Rui Hou and MingMing Pan and YunHao Zhao and Yang Yang,https://www.sciencedirect.com/science/article/pii/S1047320319302202,https://doi.org/10.1016/j.jvcir.2019.102599,1047-3203,2019,102599,64,Journal of Visual Communication and Image Representation,Image anomaly detection for IoT equipment based on deep learning,article,HOU2019102599
"A ghost-free multi-exposure image fusion technique using the dense SIFT descriptor and the guided filter is proposed in this paper. The results suggest that the presented scheme produces high-quality images using ordinary cameras and that too without the ghosting artifact. To do so, the dense SIFT descriptor is used to extract the local contrast information from source images. Whereas, for the dynamic scenes, the histogram equalization and median filtering are used to calculate the color dissimilarity feature. Three weighting terms: local contrast, brightness, and color dissimilarity feature are used to estimate the initial weights. The estimated initial weights contain discontinuities. Therefore, the guided filter is used to remove the noise and discontinuity in initial weights. Finally, the fusion is performed using a pyramid decomposition method. Experimental results prove the superiority of the proposed technique over existing state-of-the-art methods in terms of both subjective and objective evaluation.","Multi-exposure fusion, Dynamic range, Dense SIFT, Histogram, Quality measures, Pyramids",Naila Hayat and Muhammad Imran,https://www.sciencedirect.com/science/article/pii/S1047320319301750,https://doi.org/10.1016/j.jvcir.2019.06.002,1047-3203,2019,295--308,62,Journal of Visual Communication and Image Representation,Ghost-free multi exposure image fusion technique using dense SIFT descriptor and guided filter,article,HAYAT2019295
"With the rapid development of information technology and the popularity of computer and information technology, intelligent, digital has become a hot topic of discussion, industries in the daily work of the museum, there is a large amount of collection and collection information to process and maintenance staff to, so museum in various fields is also an urgent need to informationization, the digital revolution, allowing staff to liberate from the tedious work. One of the emphases of the daily work of museums is the management of the collections. Museum collection of computer management information system's purpose is to put as the basic object of information collection, through to the existing various CARDS, books and pictures, sounds, video, etc of traditional data processing and digital, set up a computer information management system, system including hardware and software of two parts. This paper starts from the background, elaborated the picture management system present situation, the existence superiority; Secondly, it is the system demand analysis, mainly introduced the system environment demand, performance demand and system role demand; Then is another part of the system outline design, including the overall system architecture design, use case design, database design; Finally is the system detailed design and the realization part, this part mainly is carries on the design to the system function module, and has carried on the programming realization.","The museum, Advertising pictures, C/S mode, The system design",Shih-Hsien Chin and Cheng Chen and Po-Chang Ko and Shih-Yang Lin,https://www.sciencedirect.com/science/article/pii/S1047320319302160,https://doi.org/10.1016/j.jvcir.2019.102595,1047-3203,2019,102595,63,Journal of Visual Communication and Image Representation,Design of museum advertisement picture management system based on web,article,CHIN2019102595
"Activation functions are of great importance for the performance and training of deep neural networks. High-performance activation function is expected to effectively prevent the gradient from vanishing and help network converge. This paper provides a novel smooth activation function, called Parameterized Self-circulating Gating Unit (PSGU), aiming to train an adaptive activation function to improve the performance of deep networks. Compared with other works, we propose and study the self-circulation gating property of activation function, and analyze its influence on the signal transmission in network by controlling the flow of information. Specifically, we theoretically analyze and propose the initialization based on PSGU, which adequately explores the properties in neighborhood of the origin. Finally, the proposed activation function and initialization are compared with other methods on commonly-used network architectures, the achieved performances of using PSGU alone or combining with our proposed initialization are over par with the state of the art.","Deep learning, Neural network, Activation function, PSGU, Initialization",Zhengze Li and Xiaoyuan Yang and Kangqing Shen and Fazhen Jiang and Jin Jiang and Huwei Ren and Yixiao Li,https://www.sciencedirect.com/science/article/pii/S1047320321001942,https://doi.org/10.1016/j.jvcir.2021.103294,1047-3203,2021,103294,80,Journal of Visual Communication and Image Representation,PSGU: Parametric self-circulation gating unit for deep neural networks,article,LI2021103294
"In this paper, a convolutional neural network (CNN) with multi-loss constraints is designed for stereoscopic image quality assessment (SIQA). A stereoscopic image not only contains monocular information, but also provides binocular information which is as identically crucial as the former. So we take the image patches of left-view images, right-view images and the difference images as the inputs of the network to utilize monocular information and binocular information. Moreover, we propose a method to obtain proxy label of each image patch. It preserves the quality difference between different regions and views. In addition, the multiple loss functions with adaptive loss weights are introduced in the network, which consider both local features and global features and constrain the feature learning from multiple perspectives. And the adaptive loss weights also make the multi-loss CNN more flexible. The experimental results on four public SIQA databases show that the proposed method is superior to other existing SIQA methods with state-of-the-art performance.","Image quality assessment, Binocular information, Multi-loss, Proxy label",Sumei Li and Yueyang Li and Yongtian Han,https://www.sciencedirect.com/science/article/pii/S1047320321001668,https://doi.org/10.1016/j.jvcir.2021.103255,1047-3203,2021,103255,79,Journal of Visual Communication and Image Representation,Stereoscopic image quality assessment considering visual mechanism and multi-loss constraints,article,LI2021103255
"The existing hashing methods mainly handle either the feature based nearest-neighbor search or the category-level image retrieval, whereas a few efforts are devoted to instance retrieval problem. In this paper, we propose a binary multi-view fusion framework for directly recovering a latent Hamming subspace from the multi-view features for instance retrieval. More specifically, the multi-view subspace reconstruction and the binary quantization are integrated in a unified framework so as to minimize the discrepancy between the original multi-view high-dimensional Euclidean space and the resulting compact Hamming subspace. Besides, our method is essentially an unsupervised learning scheme without any labeled data involved, and thus can be used in the cases when the supervised information is unavailable or insufficient. Experiments on public benchmark and large-scale datasets reveal that our method achieves competitive retrieval performance comparable to the state-of-the-arts and has excellent scalability in large-scale scenario.","Instance retrieval, Multi-view fusion, Hamming subspace, Unsupervised learning",Zhijian Wu and Jun Li and Jianhua Xu and Wankou Yang,https://www.sciencedirect.com/science/article/pii/S104732032100153X,https://doi.org/10.1016/j.jvcir.2021.103234,1047-3203,2021,103234,79,Journal of Visual Communication and Image Representation,Beyond ITQ: Efficient binary multi-view subspace learning for instance retrieval,article,WU2021103234
"We present PRNU-based image manipulation localization as a probabilistic labeling task in a flexible discriminative random field (DRF) setup. Instead of reaching local decisions independent of each other, discriminative random fields incorporate local inter-label dependencies while keeping the formulation general enough to make label assignments depend on both local and non-local image characteristics. With an improved form of association potential combining normalized correlation and the deviation of the measured correlation from the expected correlation and an interaction potential defined as the weighted L2 norm squared between intensities of neighboring pixels, we were able to localize even considerably small manipulations on realistic tampered images. We experimented with different combinations of window sizes to capture features to predict the correlation more accurately than already existing algorithms. Experimental results indicate that our algorithm outperforms recent state of the art methods based on multiscale analysis strategies. We also found that for inspecting manipulated images which are JPEG compressed, it helps to train the predictor with JPEG images rather than with uncompressed images and for all quality factors, it is possible to work with two predictors, one trained for images with lower quality factors and another for higher quality factors.","Photo-response non-uniformity, Discriminative random field, Manipulation localization, Correlation predictor",Sujoy Chakraborty and Matthias Kirchner,https://www.sciencedirect.com/science/article/pii/S1047320321001784,https://doi.org/10.1016/j.jvcir.2021.103273,1047-3203,2021,103273,80,Journal of Visual Communication and Image Representation,Sensor-based image manipulation localization with Discriminative Random fields and Graph Cut,article,CHAKRABORTY2021103273
"Dense 3D reconstruction is required for robots to safely navigate or perform advanced tasks. The accurate depth information of the image and its pose are the basis of 3D reconstruction. The resolution of depth maps obtained by LIDAR and RGB-D cameras is limited, and traditional pose calculation methods are not accurate enough. In addition, if each image is used for dense 3D reconstruction, the dense point clouds will increase the amount of calculation. To address these issues, we propose a 3D reconstruction system. Specifically, we propose a depth network of contour and gradient attention, which is used to complete and correct depth maps to obtain high-resolution and high-quality depth maps. Then, we propose a method of fusion of traditional algorithms and deep learning for pose estimation to obtain accurate localization results. Finally, we adopt the method of autonomous selection of keyframes to reduce the number of keyframes, the surfel-based geometric reconstruction is performed to reconstruct the dense 3D environment. On the TUM RGB-D, ICL-NIUM, and KITTI datasets, our method significantly improves the quality of the depth maps, the localization results, and the effect of 3D reconstruction. At the same time, we have also accelerated the speed of 3D reconstruction.","3D reconstruction, Depth completion and correction, Pose fusion estimation, Auto-selected keyframes",Fangzheng Tian and Yongbin Gao and Zhijun Fang and Jia Gu and Shuqun Yang,https://www.sciencedirect.com/science/article/pii/S1047320321001267,https://doi.org/10.1016/j.jvcir.2021.103199,1047-3203,2021,103199,79,Journal of Visual Communication and Image Representation,3D reconstruction with auto-selected keyframes based on depth completion correction and pose fusion,article,TIAN2021103199
"Enhancement of remotely sensed images is a challenging problem, since the enhanced image has to have an improved contrast and edge information while preserving the original radiance values as much as possible. In this paper, a scale aware enhancement method based on rolling guidance is proposed for remotely sensed images. For each scale, a guidance image is defined and the approximation image is provided by an iterative joint filtering of the approximation and guidance images. Then the extracted details are amplified through an adaptive scheme and added to the final level approximation layer to provide the resulting enhanced image. A comparative study between the proposed methods with classical edge preserving filters and traditional methods have been carried out by using several criteria. The proposed methods have an average of 12% improvement for contrast gain (CG) metric and 81% improvement for enhancement measurement (EME) metric compared to the closest comparison method.","Image enhancement, Remote sensing images, Rolling guidance filter, Edge preserving filters, Multiscale decomposition",N.H. Kaplan and I. Erer,https://www.sciencedirect.com/science/article/pii/S1047320321002078,https://doi.org/10.1016/j.jvcir.2021.103315,1047-3203,2021,103315,80,Journal of Visual Communication and Image Representation,Scale aware remote sensing image enhancement using rolling guidance,article,KAPLAN2021103315
"In the recent advancements in image and video analysis, the detection of salient regions in the image becomes the initial step. This plays a crucial role in deciding the performance of such algorithms. In this work, a Multi-Resolution Feature Extraction (MRFE) technique that makes use of Discrete Wavelet Convolutional Neural Network (DWCNN) for generating features is employed. An Enhanced Feature Extraction (EFE) module extracts additional features from the high level features of the DWCNN, which are used to frame both channel as well as spatial attention models for yielding contextual attention maps. A new hybrid loss function is also proposed, which is a combination of Balanced Cross Entropy (BCE) loss and Edge based Structural Similarity (ESSIM) loss that effectively identifies and segments the salient regions with clear boundaries. The method is tested exhaustively with five different benchmark datasets and is proved superior to the existing state-of-the-art methods with a minimum Mean Absolute error (MAE) of 0.03 and F-measure of 0.956.","Visual saliency detection, Discrete wavelet convolutional neural network, Edge structural similarity loss",Reshmi Sasibhooshan and Suresh Kumaraswamy and Santhoshkumar Sasidharan,https://www.sciencedirect.com/science/article/pii/S1047320321001541,https://doi.org/10.1016/j.jvcir.2021.103236,1047-3203,2021,103236,79,Journal of Visual Communication and Image Representation,WavNet â Visual saliency detection using Discrete Wavelet Convolutional Neural Network,article,SASIBHOOSHAN2021103236
"Distortion function is designed for evaluating the cost of modifications in adaptive steganography. UNIWARD is a successful and popular distortion scheme which achieves high performance both for spatial and JPEG images. In this paper, we analyze the UNIWARD scheme with some empirical rules of distortion function designation. Based on that we propose our scheme to improve UNIWARD distortion. In our scheme, we focus on the symmetric characteristic of UNIWARD, and suggest that not only use original wavelet filters but also their flippings to calculate sub-models of UNIWARD distortion to maintain its isotropic properties. Moreover, we design several schemes to merge sub-models, which could maintain its invariance regard to flipping or rotation and improve its security against steganalysis detection. Experimental results show our revised UNIWARD achieves better performance for spatial and JPEG image in comparison with original UNIWARD.","Steganography, Distortion, UNIWARD",Qingxiao Guan and Hefeng Chen and Weiming Zhang and Nenghai Yu,https://www.sciencedirect.com/science/article/pii/S1047320321002194,https://doi.org/10.1016/j.jvcir.2021.103333,1047-3203,2021,103333,81,Journal of Visual Communication and Image Representation,Improving UNIWARD distortion function via isotropic construction and hierarchical merging,article,GUAN2021103333
"We propose a novel online multi-object visual tracker using a Gaussian mixture Probability Hypothesis Density (GM-PHD) filter and deep appearance learning. The GM-PHD filter has a linear complexity with the number of objects and observations while estimating the states and cardinality of time-varying number of objects, however, it is susceptible to miss-detections and does not include the identity of objects. We use visual-spatio-temporal information obtained from object bounding boxes and deeply learned appearance representations to perform estimates-to-tracks data association for target labeling as well as formulate an augmented likelihood and then integrate into the update step of the GM-PHD filter. We also employ additional unassigned tracks prediction after the data association step to overcome the susceptibility of the GM-PHD filter towards miss-detections caused by occlusion. Extensive evaluations on MOT16, MOT17 and HiEve benchmark data sets show that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy and identification.","Online visual tracking, GM-PHD filter, Prediction, CNN features, Augmented likelihood, Re-identification",Nathanael L. Baisa,https://www.sciencedirect.com/science/article/pii/S1047320321001814,https://doi.org/10.1016/j.jvcir.2021.103279,1047-3203,2021,103279,80,Journal of Visual Communication and Image Representation,Occlusion-robust online multi-object visual tracking using a GM-PHD filter with CNN-based re-identification,article,BAISA2021103279
"Instance search is an interesting task as well as a challenging issue due to the lack of effective feature representation. In this paper, an instance level feature representation built upon fully convolutional instance-aware segmentation is proposed. The feature is ROI-pooled from the segmented instance region. So that instances in various sizes and layouts are represented by deep features in uniform length. This representation is further enhanced by the use of deformable ResNeXt blocks. Superior performance is observed in terms of its distinctiveness and scalability on a challenging evaluation dataset built by ourselves. In addition, the proposed enhancement on the network structure also shows superior performance on the instance segmentation task.","Instance search, Instance segmentation, CNN",Yu Zhan and Wan-Lei Zhao,https://www.sciencedirect.com/science/article/pii/S1047320321001656,https://doi.org/10.1016/j.jvcir.2021.103253,1047-3203,2021,103253,79,Journal of Visual Communication and Image Representation,Instance search via instance level segmentation and feature representation,article,ZHAN2021103253
"Crowd counting has become a hot topic because of its wide applications in video surveillance and public security. However, one main problem of the deep learning methods for crowd counting is that the location information about the crowd is degraded irreversibly due to the spatial down-sampling of convolutional neural networks, which degrades the quality of generated density maps. To remedy the above problem, we propose an attention guided feature pyramid network (AG-FPN) for crowd counting, which can adaptively generate a high-quality density map with accurate spatial locations by combining the high- and low-level features. An attention block is added to each encoder layer to further emphasize the crowd regions and suppress the background clutters in feature extraction. Experimental results on the ShanghaiTech, UCF_CC_50, WorldExpoâ10 and UCF-QNRF datasets demonstrate the superiority of the proposed method over state-of-the-art approaches.","Crowd counting, Feature pyramid network, Attention mechanism, Density map generation",Huanpeng Chu and Jilin Tang and Haoji Hu,https://www.sciencedirect.com/science/article/pii/S1047320321002108,https://doi.org/10.1016/j.jvcir.2021.103319,1047-3203,2021,103319,80,Journal of Visual Communication and Image Representation,Attention guided feature pyramid network for crowd counting,article,CHU2021103319
"While deep learning-based image compression methods have shown impressive coding performance, most existing methods are still in the mire of two limitations: (1) unpredictable compression efficiency gain when adopting convolutional neural networks with different depths, and (2) lack of an accurate model to estimate the entropy during the training process. To address these two problems, in this paper, a deep multi-stage representation based image compression (MSRIC) method is proposed. Owing to this architecture, the detail information of shallow stages and the compact information of deep stages can be utilized for image reconstruction. Furthermore, a data-dependent channel-wised factorized probability model (DCFPM) is adopted to increase the accuracy of entropy estimation. Experimental results indicate that the proposed method guarantees better perceptual performance at a wide range of bit-rates. Also, ablation studies are carried out to validate the above mentioned technologies.","Deep image compression, Multi-stage representation, Data-dependent probability model, Convolutional neural network",Zixi Wang and Guiguang Ding and Jungong Han and Fan Li,https://www.sciencedirect.com/science/article/pii/S1047320321001498,https://doi.org/10.1016/j.jvcir.2021.103226,1047-3203,2021,103226,79,Journal of Visual Communication and Image Representation,Deep image compression with multi-stage representation,article,WANG2021103226
"Currently, VR video delivery over 5G systems is still a very complicated endeavor. One of the major challenges for VR video streaming is the expectations for low latency that current mobile networks can hardly meet. Network caching can reduce the content delivery latency efficiently. However, current caching schemes cannot obtain ideal results for VR video since it requests the viewport interactively. In this paper, we propose a tiled scalable VR video caching scheme over 5G networks. VR chunks are first encoded into multi-granularity quality layers, and are then partitioned into tiles to facilitate viewport data access. By accommodating the 5G network infrastructure, the tiles are cooperatively cached in a three-level hierarchal system to reduce delivery latency. Furthermore, a quality-adaptive request routing algorithm is designed to cater for the 5G bandwidth dynamics. Experimental results show that the proposed scheme can reduce the transmission latency over conventional constant bitrate video caching schemes.","VR video, Caching, Video tile, 5G systems",Kedong Liu and Yanwei Liu and Jinxia Liu and Antonios Argyriou,https://www.sciencedirect.com/science/article/pii/S104732032100136X,https://doi.org/10.1016/j.jvcir.2021.103210,1047-3203,2021,103210,79,Journal of Visual Communication and Image Representation,Tile caching for scalable VR video streaming over 5G mobile networks,article,LIU2021103210
"We propose a novel video object segmentation method employing random walkers to travel on graphs constructed on two consecutive frames. First, we estimate the initial foreground and background distributions by minimising an energy function that incorporates the stationary distributions of the random walks. The random walkers frequently travel between similar nodes of the graph constructed on two adjacent frames, which enables the incorporation of the inter-frame information into the energy function effectively and elegantly. Then, we refine the initial results by simulating the movements of multiple random walkers. We process the sequence in a recursive manner, which naturally propagates the previous segmentation labels to the subsequent frames. Additionally, we develop a strategy for adjusting the superpixel number using region similarity and the average Frobenius norm of optical flow gradient. This strategy can improve performance significantly. Furthermore, we discuss the feature selection problem in the method to select a more effective feature representation. Extensive and comparable experiments on Segtrack and Segtrack v2 demonstrate that the proposed algorithm yields higher performance than several recent state-of-the-art approaches.","Random walks, Video object segmentation, Optical flow gradient, Spatiotemporal consistency",Hui Wang and Weibin Liu and Weiwei Xing,https://www.sciencedirect.com/science/article/pii/S1047320321001930,https://doi.org/10.1016/j.jvcir.2021.103293,1047-3203,2021,103293,80,Journal of Visual Communication and Image Representation,Video object segmentation via random walks on two-frame graphs comprising superpixels,article,WANG2021103293
"Tracking-by-detection (TBD) is a significant framework for visual object tracking. However, current trackers are usually updated online based on random sampling with a probability distribution. The performance of the learning-based TBD trackers is limited by the lack of discriminative features, especially when the background is full of semantic distractors. We propose an attention-driven data augmentation method, in which a residual attention mechanism is integrated into the TBD tracking network as supplementary references to identify discriminative image features. A mask generating network is used to simulate changes in target appearances to obtain positive samples, where attention information and image features are combined to identify discriminative features. In addition, we propose a method for mining hard negative samples, which searches for semantic distractors with the response of the attention module. The experiments on the OTB2015, UAV123, and LaSOT benchmarks show that this method achieves competitive performance in terms of accuracy and robustness.","Object tracking, Data augmentation, Attention mechanism, Deep learning",Zaifeng Shi and Cheng Sun and Qingjie Cao and Zhe Wang and Qiangqiang Fan,https://www.sciencedirect.com/science/article/pii/S1047320321002066,https://doi.org/10.1016/j.jvcir.2021.103312,1047-3203,2021,103312,80,Journal of Visual Communication and Image Representation,Residual attention-based tracking-by-detection network with attention-driven data augmentation,article,SHI2021103312
"Depth completion, which combines additional sparse depth information from the range sensors, substantially improves the accuracy of monocular depth estimation, especially using the deep-learning-based methods. However, these methods can hardly produce satisfactory depth results when the sensor configuration changes at test time, which is important for real-world applications. In this paper, the problem is tackled by our proposed novel two-stage mechanism, which decomposes depth completion into two subtasks, namely relative depth map estimation and scale recovery. The relative depth map is first estimated from a single color image with our designed scale-invariant loss function. Then the scale map is recovered with the additional sparse depth. Experiments on different densities and patterns of the sparse depth input show that our model always produces satisfactory depth results. Besides, our approach achieves state-of-the-art performance on the indoor NYUv2 dataset and performs competitively on the outdoor KITTI dataset, demonstrating the effectiveness of our method.","Depth estimation, Depth completion, Relative depth, Scale recovery, Geometry structure",Yangqi Long and Huimin Yu and Biyang Liu,https://www.sciencedirect.com/science/article/pii/S1047320321001772,https://doi.org/10.1016/j.jvcir.2021.103272,1047-3203,2021,103272,80,Journal of Visual Communication and Image Representation,Depth completion towards different sensor configurations via relative depth map estimation and scale recovery,article,LONG2021103272
"2D image-based 3D model retrieval has become a hotspot topic in recent years. However, the current existing methods are limited by two aspects. Firstly, they are mostly based on the supervised learning, which limits their application because of the high time and cost consuming of manual annotation. Secondly, the mainstream methods narrow the discrepancy between 2D and 3D domains mainly by the image-level alignment, which may bring the additional noise during the image transformation and influence cross-domain effect. Consequently, we propose a Wasserstein distance feature alignment learning (WDFAL) for this retrieval task. First of all, we describe 3D models through a series of virtual views and use CNNs to extract features. Secondly, we design a domain critic network based on the Wasserstein distance to narrow the discrepancy between two domains. Compared to the image-level alignment, we reduce the domain gap by the feature-level distribution alignment to avoid introducing additional noise. Finally, we extract the visual features from 2D and 3D domains, and calculate their similarity by utilizing Euclidean distance. The extensive experiments can validate the superiority of the WDFAL method.","3D model retrieval, Multi-view learning, Cross-domain retrieval",Yaqian Zhou and Yu Liu and Heyu Zhou and Wenhui Li,https://www.sciencedirect.com/science/article/pii/S1047320321001255,https://doi.org/10.1016/j.jvcir.2021.103197,1047-3203,2021,103197,79,Journal of Visual Communication and Image Representation,Wasserstein distance feature alignment learning for 2D image-based 3D model retrieval,article,ZHOU2021103197
"In recent years, discrete supervised hashing methods have attracted increasing attention because of their high retrieval efficiency and precision. However, in these methods, some effective semantic information is typically neglected, which means that all the information is not sufficiently utilized. Moreover, these methods often only decompose the first-order features of the original data, ignoring the more fine-grained higher-order features. To address these problems, we propose a supervised hashing learning method called discrete hashing with triple supervision learning (DHTSL). Specifically, we integrate three aspects of semantic information into this method: (1) the bidirectional mapping of semantic labels; (2) pairwise similarity relations; (3) second-order features from the original data. We also design a discrete optimization method to solve the proposed objective function. Moreover, an out-of-sample extension strategy that can better maintain the independence and balance of hash codes is employed to improve retrieval performance. Extensive experiments on three widely used datasets demonstrate its superior performance.","ANN search, Discrete hashing, Triple supervision",Shaohua Wang and Xiao Kang and Fasheng Liu and Xiushan Nie and Xingbo Liu,https://www.sciencedirect.com/science/article/pii/S1047320321002340,https://doi.org/10.1016/j.jvcir.2021.103355,1047-3203,2021,103355,81,Journal of Visual Communication and Image Representation,Discrete hashing with triple supervision learning,article,WANG2021103355
"Efficiently capturing shape and turbulent motions of dynamic textures (DTs) for video description is a challenge in real applications due to the negative influences of the well-known problems: environmental elements, illumination, scale, and noise. In this paper, we propose an efficient and simple framework for DT representation based on the oriented features of high-order Gaussian gradients. Firstly, 2D/3D Gaussian-based filtering kernels in high-order partial derivatives are taken into account the video analysis as a preprocessing step to obtain corresponding gradient-filtered images/volumes. After that, the oriented features, which are robust against the above issues, are extracted by decomposing the Gaussian derivative magnitudes into oriented components. Finally, a shallow local encoding is utilized for structuring spatio-temporal features from these oriented magnitudes. This allows constructing discriminative descriptors with promising performances compared to those based on the non-oriented ones. Experimental results for DT classification task on benchmark datasets have verified the interest of our proposal.","Dynamic textures, Gaussian-filtered derivatives, Oriented magnitudes, LBP, CLBP, Video representation",Thanh Tuan Nguyen and Thanh Phuong Nguyen and FrÃ©dÃ©ric Bouchara,https://www.sciencedirect.com/science/article/pii/S1047320321002182,https://doi.org/10.1016/j.jvcir.2021.103330,1047-3203,2021,103330,81,Journal of Visual Communication and Image Representation,Dynamic texture representation based on oriented magnitudes of Gaussian gradients,article,NGUYEN2021103330
"This article presents a new curved-based intra-frame prediction method for current and upcoming video coding standards. Our proposal extends conventional straight-line angular modes found on intra-prediction tools to model curved texture characteristics, enhancing the intra-frame prediction process. Our work targets the High Efficiency Video Coding (HEVC) standard for evaluation, although our curved-based method can be used by any other video coding standard. We model curved intra-frame prediction using an offset-based displacement calculation to each predicted sample. The proposal incurs a small bitstream overhead for transmitting the displacement information, which is offset by encoding efficiency gains. Experimental results demonstrate reduced residual energy; consequently, improving BD-Rate for the tested sequences. Evaluations applying eight curve displacement values show an average BD-Rate reduction of 2.69%, 2.49%, and 0.86% for All-Intra-8, All-Intra 10, and Random-Access configurations, respectively. The proposal allows further BD-Rate improvements, albeit at higher encoding complexity.","Intra-frame prediction, High Efficiency Video Coding (HEVC), Video coding, Predictive coding",Ramon Fernandes and Gustavo Sanchez and Rodrigo Cataldo and Luciano Agostini and CÃ©sar Marcon,https://www.sciencedirect.com/science/article/pii/S1047320321001929,https://doi.org/10.1016/j.jvcir.2021.103291,1047-3203,2021,103291,80,Journal of Visual Communication and Image Representation,Using curved angular intra-frame prediction to improve video coding efficiency,article,FERNANDES2021103291
"Video anomaly detection is usually studied by considering the spatial and temporal contexts. This paper focuses first on spatial context and shows that it can be a fast real-time solution. In the first part of this work there are two main contributions: employing a new deep network for reconstruction and introducing a new regularity scoring function. The new deep architecture is based on pyramid of input images and compared to UNet, the proposed architecture boosts AUC by 15% and the new regularity scoring function is based on SSIM. The second part employs a multiframe approach to distinguish temporal behavior anomalies. The second approach enhances the results by 7% compared to spatial anomaly detection. Comparing the two approaches, if computing power is limited and real time anomaly detection is looked for, single frame detection is preferred while multi frame analysis offers a much wider possibility of anomaly detection.","Anomaly detection, Deep learning, Convolutional autoencoder, Image reconstruction",Maedeh Bahrami and Majid Pourahmadi and Abbas Vafaei and Mohammad Reza Shayesteh,https://www.sciencedirect.com/science/article/pii/S1047320321001528,https://doi.org/10.1016/j.jvcir.2021.103232,1047-3203,2021,103232,79,Journal of Visual Communication and Image Representation,A comparative study between single and multi-frame anomaly detection and localization in recorded video streams,article,BAHRAMI2021103232
"Existing saliency prediction methods focus on exploring a universal saliency model for natural images, relatively few on advertising images which typically consists of both textual regions and pictorial regions. To fill this gap, we first build an advertising image database, named ADD1000, recording 57 subjectsâ eye movement data of 1000 ad images. Compared to natural images, advertising images contain more artificial scenarios and show stronger persuasiveness and deliberateness, while the impact of this scene heterogeneity on visual attention is rarely studied. Moreover, text elements and picture elements express closely related semantic information to highlight product or brand in ad images, while their respective contribution to visual attention is also less known. Motivated by these, we further propose a saliency prediction model for advertising images based on text enhanced learning (TEL-SP), which comprehensively considers the interplay between textual region and pictorial region. Extensive experiments on the ADD1000 database show that the proposed model outperforms existing state-of-the-art methods.","Saliency prediction, Advertising, OCR, Lightweight architecture",Song Liang and Ruihang Liu and Jiansheng Qian,https://www.sciencedirect.com/science/article/pii/S1047320321002339,https://doi.org/10.1016/j.jvcir.2021.103356,1047-3203,2021,103356,81,Journal of Visual Communication and Image Representation,Fixation prediction for advertising images: Dataset and benchmark,article,LIANG2021103356
"Weakly supervised temporal action localization is a challenging computer vision problem that uses only video-level labels and lacks the supervision of temporal annotations. In this task, the majority of existing methods usually identify the most discriminative snippets and ignore other relevant snippets. To address this problem, we propose a deep feature enhancing and selecting network. It generates multiple masks for both capturing more complete temporal interval of actions and keeping its high classification accuracy. After that, we further propose a novel selection strategy to balance the influence of multiple masks and improve the model performance. In the experiments, we evaluate the proposed method on the THUMOSâ14 and ActivityNet datasets, and the results show the effectiveness of our approach for weakly supervised temporal action localization.","Weakly supervised, Temporal action localization, Deep learning",Jiaruo Yu and Yongxin Ge and Xiaolei Qin and Ziqiang Li and Sheng Huang and Feiyu Chen,https://www.sciencedirect.com/science/article/pii/S1047320321001802,https://doi.org/10.1016/j.jvcir.2021.103276,1047-3203,2021,103276,80,Journal of Visual Communication and Image Representation,Deep feature enhancing and selecting network for weakly supervised temporal action localization,article,YU2021103276
"Multiple object tracking is one of the most fundamental tasks in computer vision, and it is still very challenging for real-world applications due to its severe occlusion and motion blur. Most of the existing methods solve these multiple object tracking issues by performing data association based on the deep features of the detections in consecutive frames, which only contain the spatial information of the detected objects. Therefore, the inaccuracy of data association would easily occur, especially in the severe occlusion scenes. In this paper, a novel multiple object tracking model named sequence-tracker (STracker) has been proposed, which combines both the temporal and spatial features to perform data association. We trained a sequence feature extraction network based on video pedestrian re-identification offline, fused the obtained sequence features with the depth features of the previous frame, and then implemented the Hungarian algorithm for data association. Experiments have been carried out to validate the effectiveness of the proposed algorithm and the corresponding results indicates that it can significantly improve the trajectory quality of our dataset in this paper. Remarkably, for the public detector results from MOT official website, the proposed algorithm can achieve up to 57.2% MOTA and 50.9% IDF1 on the MOT17 dataset.","Object tracking, Sequence features, Severe occlusion scene, Sequence-tracker",Xu Tan and Zhengwei Li and Qiaokang Liang and Wei Sun and Yaonan Wang and Dan Zhang,https://www.sciencedirect.com/science/article/pii/S1047320321001644,https://doi.org/10.1016/j.jvcir.2021.103250,1047-3203,2021,103250,79,Journal of Visual Communication and Image Representation,Sequence-tracker: Multiple object tracking with sequence features in severe occlusion scene,article,TAN2021103250
"Small object detection is challenging and far from satisfactory. Most general object detectors suffer from two critical issues with small objects: (1) Feature extractor based on classification network cannot express the characteristics of small objects reasonably due to insufficient appearance information of targets and a large amount of background interference around them. (2) The detector requires a much higher location accuracy for small objects than for general objects. This paper proposes an effective and efficient small object detector YOLSO to address the above problems. For feature representation, we analyze the drawbacks in previous backbones and present a Half-Space Shortcut(HSSC) module to build a background-aware backbone. Furthermore, a coarse-to-fine Feature Pyramid Enhancement(FPE) module is introduced for layer-wise aggregation at a granular level to enhance the semantic discriminability. For loss function, we propose an exponential L1 loss to promote the convergence of regression, and a focal IOU loss to focus on prime samples with high classification confidence and high IOU. Both of them significantly improves the location accuracy of small objects. The proposed YOLSO sets state-of-the-art results on two typical small object datasets, MOCOD and VeDAI, at a speed of over 200 FPS. In the meantime, it also outperforms the baseline YOLOv3 by a wide margin on the common COCO dataset.","Small object detection, Background-aware, Granular feature aggregation, Accurate location, High speed",Jinpu Zhang and Lei Zhang and Tianyu Liu and Yuehuan Wang,https://www.sciencedirect.com/science/article/pii/S1047320321002297,https://doi.org/10.1016/j.jvcir.2021.103348,1047-3203,2021,103348,81,Journal of Visual Communication and Image Representation,YOLSO: You Only Look Small Object,article,ZHANG2021103348
"Image conversion has attracted mounting attention due to its practical applications. This paper proposes a lightweight network structure that can implement unpaired training sets to complete one-way image mapping, based on the generative adversarial network (GAN) and a fixed-parameter edge detection convolution kernel. Compared with the cycle consistent adversarial network (CycleGAN), the proposed network features simpler structure, fewer parameters (only 37.48% of the parameters in CycleGAN), and less training cost (only 35.47% of the GPU memory usage and 17.67% of the single iteration time in CycleGAN). Remarkably, the cyclic consistency becomes not mandatory for ensuring the consistency of the content before and after image mapping. This network has achieved significant processing effects in some image translation tasks, and its effectiveness and validity have been well demonstrated through typical experiments. In the quantitative classification evaluation based on VGG-16, the algorithm proposed in this paper has achieved superior performance.","Lightweight generative adversarial network, Image conversion, Image-to-image translation, Unpaired image-to-image translation",Yijie Li and Qiaokang Liang and Zhengwei Li and Youcheng Lei and Wei Sun and Yaonan Wang and Dan Zhang,https://www.sciencedirect.com/science/article/pii/S104732032100119X,https://doi.org/10.1016/j.jvcir.2021.103187,1047-3203,2021,103187,78,Journal of Visual Communication and Image Representation,EdgeGAN: One-way mapping generative adversarial network based on the edge information for unpaired training set,article,LI2021103187
"Color-based particle filters have emerged as an appealing method for targets tracking. As the target may undergo rapid and significant appearance changes, the template (i.e. scale of the target, color distribution histogram) also needs to be updated. Traditional updates without learning contextual information may imply a high risk of distorting the model and losing the target. In this paper, a new algorithm utilizing the environmental information to update both the scale of the tracker and the reference appearance model for the purpose of object tracking in video sequences has been put forward. The proposal makes use of the well-established color-based particle filter tracking while differentiating the foreground and background particles according to their matching score. A roaming phenomenon that yields the estimation to shrink and diverge is investigated. The proposed solution is tested using both simulated and publicly available benchmark datasets where a comparison with six state-of-the-art trackers has been carried out. The results demonstrate the feasibility of the proposal and lie down foundations for further research on tackling complex visual tracking problems.","Object tracking, Video Analysis, Scale modification, Background learning",Jingjing Xiao and Mourad Oussalah,https://www.sciencedirect.com/science/article/pii/S1047320321001760,https://doi.org/10.1016/j.jvcir.2021.103270,1047-3203,2021,103270,79,Journal of Visual Communication and Image Representation,Robust model adaption for colour-based particle filter tracking with contextual information,article,XIAO2021103270
"Human detection in crowded scenes is challenging since the objects occlude and overlap each other. Compared to general pedestrian detection, there is also more variation in human posture. This paper proposes a real-time human detection network, Dynamic Dual-Peak Network (DDPNet), which specifically addresses human object detection in overlapping and crowded scenes. We design a deep cascade fusion module to enhance the feature extraction capability of the anchor-free model for small objects in crowded scenes. In the meantime, the headâbody dual-peak activation module is used to improve the prediction score of the central region of the occluded individual through low occlusion components. By this improvement strategy, the networkâs ability is enhanced to discriminate individuals in crowded scenes and alleviate the problem caused by individual posture variation. Ultimately, we propose a novel ExhaleâInhale method to adjust the feature mapping ranges for various scale objects dynamically. In the process of ground truth mapping, the overlapping of individual feature information is reduced. Our DDPNet achieves competitive performance on the CrowdHuman dataset and executes real-time inference of almost 3xâ¼7x faster than competitive methods.","Anchor free, Crowded scenes, CNN, Human detection",Yefan Xie and Jiangbin Zheng and Xuan Hou and Yue Xi and Fengming Tian,https://www.sciencedirect.com/science/article/pii/S1047320321001243,https://doi.org/10.1016/j.jvcir.2021.103195,1047-3203,2021,103195,79,Journal of Visual Communication and Image Representation,Dynamic Dual-Peak Network: A real-time human detection network in crowded scenes,article,XIE2021103195
"MV-HEVC can efficiently compress multiview video data captured from different viewpoints. To achieve high coding efficiency, it consists of not only inter coding but also interview coding. The inter coding includes a motion estimation (ME) process that reduces temporal redundancies between consecutive frames, and the interview coding performs a disparity estimation (DE) that reduces interview redundancies between neighboring views. As a result, MV-HEVC needs high encoding complexity to perform both ME and DE. In order to reduce the complexity, this paper proposes an adaptive fractional ME and DE skipping method in a partitioned inter prediction unit (PU) mode, based on a result of a 2Â NÂ ÃÂ 2Â N inter PU coding. Experimental results show that the proposed method efficiently reduces the encoding complexity with negligible coding loss, compared to conventional methods.","Encoding complexity, Disparity estimation (DE), Motion estimation (ME), Interview coding, MV-HEVC",Jin Young Lee and Sang-hyo Park,https://www.sciencedirect.com/science/article/pii/S1047320321001486,https://doi.org/10.1016/j.jvcir.2021.103223,1047-3203,2021,103223,79,Journal of Visual Communication and Image Representation,Adaptive fractional motion and disparity estimation skipping in MV-HEVC,article,LEE2021103223
"The latest deep neural networks for medical segmentation typically utilize transposed convolutional filters and atrous convolutional filters for spatial restoration and larger receptive fields, leading to dilution and inconsistency of visual semantics. To address such issues, we propose a novel attentional up-concatenation structure to build an auxiliary path for direct access to multi-level features. In addition, we employ a new structural loss to bring better morphological awareness and reduce the segmentation flaws caused by the semantic inconsistencies. Thorough experiments on the challenging optic cup/disc segmentation, cellular segmentation and lung segmentation tasks were performed to evaluate the proposed methods. Further ablation analysis demonstrated the effectiveness of the different components of the model and illustrated its efficiency. The proposed methods achieved the best performance and speed compared to the state-of-the-art models in three tasks on seven public datasets, including DRISHTI-GS, RIM-r3, REFUGE, MESSIDOR, TNBC, GlaS and LUNA.","Image segmentation, Convolutional neural network, Semantics, Deep learning",Yang Wen and Leiting Chen and Yu Deng and Jin Ning and Chuan Zhou,https://www.sciencedirect.com/science/article/pii/S1047320321002054,https://doi.org/10.1016/j.jvcir.2021.103311,1047-3203,2021,103311,80,Journal of Visual Communication and Image Representation,Towards better semantic consistency of 2D medical image segmentation,article,WEN2021103311
"Integral imaging is a kind of 3D display with no glasses, which represents the future developments. Elementary image array (EIA) is an essential component of integral imaging. Our coding framework includes pre-processing, modeling, and reconstruction. We acquire the sub-EIA from the original EIA and get the offsets between adjacent elementary images (EIs) through pre-processing. As for modeling, we get the optimal combination of 3-D Epanechnikov Mixture Regression (3-D EMR) or 3-D Gaussian Mixture Regression (3-D GMR) by Elementary Image Adaptive Model Selection (EI-AMLS) algorithm to achieve the best modeling of sub-EIA. Finally, the linear-based reconstruction is completed according to the correlation between adjacent EIs. Our decoded images realize a clearer outline reconstruction and more superior coding efficiency than HEVC and JPEG2000 below about 0.05bpp. Furthermore, the proposed method can achieve the same visual effect as HEVC with only 15% to 80% time consumed.","3-D Epanechnikov Kernel, 3-D Epanechnikov Mixture Regression, 3D holoscopic image compression, Integral imaging compression, Image modeling",Boning Liu and Yan Zhao and Xiaomeng Jiang and Shigang Wang and Jian Wei,https://www.sciencedirect.com/science/article/pii/S1047320321002170,https://doi.org/10.1016/j.jvcir.2021.103332,1047-3203,2021,103332,81,Journal of Visual Communication and Image Representation,3-D Epanechnikov Mixture Regression in integral imaging compression,article,LIU2021103332
"Scene text recognition has been a hot research topic in computer vision due to its various applications. The state-of-the-art solutions usually depend on the attention-based encoder-decoder framework that learns the mapping between input images and output sequences in a purely data-driven way. Unfortunately, there often exists severe misalignment between feature areas and text labels in real-world scenarios. To address this problem, this paper proposes a sequential alignment attention model to enhance the alignment between input images and output character sequences. In this model, an attention gated recurrent unit (AGRU) is first devised to distinguish the text and background regions, and further extract the localized features focusing on sequential text regions. Furthermore, CTC guided decoding strategy is integrated into the popular attention-based decoder, which not only helps to boost the convergence of the training but also enhances the well-aligned sequence recognition. Extensive experiments on various benchmarks, including the IIIT5k, SVT, and ICDAR datasets, show that our method substantially outperforms the state-of-the-art methods.","Scene text recognition, Attention-gated recurrent unit, Attention mechanism, Connectionist temporal classification",Yan Wu and Jiaxin Fan and Renshuai Tao and Jiakai Wang and Haotong Qin and Aishan Liu and Xianglong Liu,https://www.sciencedirect.com/science/article/pii/S1047320321001917,https://doi.org/10.1016/j.jvcir.2021.103289,1047-3203,2021,103289,80,Journal of Visual Communication and Image Representation,Sequential alignment attention model for scene text recognition,article,WU2021103289
"Double JPEG compression detection plays a vital role in multimedia forensics, to find out whether a JPEG image is authentic or manipulated. However, it still remains to be a challenging task in the case when the quality factor of the first compression is much higher than that of the second compression, as well as in the case when the targeted image blocks are quite small. In this work, we present a novel end-to-end deep learning framework taking raw DCT coefficients as input to distinguish between single and double compressed images, which performs superior in the above two cases. Our proposed framework can be divided into two stages. In the first stage, we adopt an auxiliary DCT layer with sixty-four 8Â ÃÂ 8 DCT kernels. Using a specific layer to extract DCT coefficients instead of extracting them directly from JPEG bitstream allows our proposed framework to work even if the double compressed images are stored in spatial domain, e.g. in PGM, TIFF or other bitmap formats. The second stage is a deep neural network with multiple convolutional blocks to extract more effective features. We have conducted extensive experiments on three different image datasets. The experimental results demonstrate the superiority of our framework when compared with other state-of-the-art double JPEG compression detection methods either hand-crafted or learned using deep networks in the literature, especially in the two cases mentioned above. Furthermore, our proposed framework can detect triple and even multiple JPEG compressed images, which is scarce in the literature as far as we know.","Multimedia forensics, Discrete cosine transform, Deep learning, Double JPEG compression, Convolutional neural network",Israr Hussain and Shunquan Tan and Bin Li and Xinghong Qin and Dostdar Hussain and Jiwu Huang,https://www.sciencedirect.com/science/article/pii/S1047320321001759,https://doi.org/10.1016/j.jvcir.2021.103269,1047-3203,2021,103269,80,Journal of Visual Communication and Image Representation,A novel deep learning framework for double JPEG compression detection of small size blocks,article,HUSSAIN2021103269
"Stereoscopic image quality assessment (SIQA) is of great significance to the development of modern three-dimensional (3D) display technology. In this work, by further mining the relationship between visual features and stereoscopic image quality perception, we build a new no-reference SIQA model, which combines the monocular and binocular features. Statistical quality-aware structural features from relative gradient orientation (RGO) map and texture features from the histogram of the weighted local binary pattern (LBP) in the texture image (TLBP) are not only extracted from both monocular view, but also extracted from binocular views to predict binocular quality perception. Meanwhile, the color statistical features ignored by most models and the binocularity feature is extracted to complement the monocular features and the above binocular features, respectively. Finally, all the extracted features and subjective scores are used to predict the objective quality score through the support vector regression (SVR) model. Experiments on four popular stereoscopic image databases show that the proposed model achieves high consistency with subjective assessment, and the performance of the model is very competitive with the latest models.","Stereoscopic image quality, Binocularity, Monocular feature, Binocular feature, Features extraction and regression",Yun Liu and Baoqing Huang and Hongwei Yu and Zhi Zheng,https://www.sciencedirect.com/science/article/pii/S1047320321002327,https://doi.org/10.1016/j.jvcir.2021.103354,1047-3203,2021,103354,81,Journal of Visual Communication and Image Representation,No-reference stereoscopic image quality evaluator based on human visual characteristics and relative gradient orientation,article,LIU2021103354
"Salient object detection (SOD) tasks aim to outline the most concerned part of human vision, which is widely used in computer vision fields. Due to possibility of the insufficient illumination in the application environment (such as night or dim indoor environment), RGB images from visible channels usually lose most of their performance, while thermal images can improve the detection performance. Therefore, it is in urgent need of a robust saliency detection method, which can handle complex illumination conditions and take use of features from multiple sources intelligently. Accordingly, we propose our âillumination based multi-source fused salient object detection networkâ (IAN-MF-SOD network). Taking the illumination condition as a quantitative reference, we guide features from two sources to fuse adaptively and intelligently, so that our method can enhance both of their advantages. For different illumination conditions, we distribute different fusion weights for each RGBâthermal image pair. Well fused images are generated as inputs to a trained SOD network to obtain saliency maps. Due to the analysis of our proposed IAN-score, our method performs favorably against traditional RGB-based SOD networks.","Multi-source, Illumination discrimination, Salient object detection, Deep learning",Chunxu Jiang and Yu Liu and Jinglin Sun and Jichang Guo and Wei Lu,https://www.sciencedirect.com/science/article/pii/S104732032100122X,https://doi.org/10.1016/j.jvcir.2021.103192,1047-3203,2021,103192,79,Journal of Visual Communication and Image Representation,Illumination-based adaptive saliency detection network through fusion of multi-source features,article,JIANG2021103192
"Recently, Siamese based methods have made a breakthrough in the visual tracking field. However, the existing trackers still cannot take full advantage of the deep features. In this work, we improve the performances of Siamese trackers by complementary learning with different types of matching features. Specifically, a Matching Activation Network (MAN) is firstly designed to highlight the matching regions of the search image given a template. Since only sparse parts of feature maps contribute to the matching result, an important design choice is to emphasize the weak-matching features by erasing the strong-matching ones and learn complementary classifiers from both types of features. Then we propose a novel complementary region proposal network (CoRPN) to take complementary features as inputs and their outputs complement to each other, which are fused to improve the performance. Experiments show that our proposed tracker achieves leading performances on five tracking datasets while retaining real-time speed.","Visual tracking, Matching visualization, Complementary learning, Siamese networks",Ke Tan and Ting-Bing Xu and Zhenzhong Wei,https://www.sciencedirect.com/science/article/pii/S104732032100198X,https://doi.org/10.1016/j.jvcir.2021.103299,1047-3203,2021,103299,80,Journal of Visual Communication and Image Representation,Learning complementary Siamese networks for real-time high-performance visual tracking,article,TAN2021103299
"Image inpainting is an important research direction of image processing. The generative adversarial network (GAN), which can reconstruct new reasonable content in the corrupted region, is the most interesting tool in current inpainting technologies. However, the previous deep methods generally need to be pre-added the binary mask representing the corruption location as the extra input. A novel inpainting algorithm which does not require additional external labels is proposed in this paper. The algorithm consists of two parts: corruption recognition module and content inpainting module, which can recognize and fill random corruption. In the recognizer, the salient object from the uncorrupted region is used as the prior for distinguishing corruption. In the inpainting module, a two-stage network is applied to reconstruct the image from coarse content to texture details. To avoid the misdetection in recognition which has a negative impact on the restoration in inpainting, we perform relative total variational filtering on the corrupted image, and use the salient map as the supervision of detail reconstruction. Qualitative and quantitative experiments on multiple datasets verify the effectiveness of our recognition module, the competitive advantage of our inpainting module, and the enlightening significance of our total algorithm in image inpainting.","Image inpainting, GAN, Corruption recognition, Salient prior, Relative total variation",Hang Shao and Yongxiong Wang,https://www.sciencedirect.com/science/article/pii/S1047320321001516,https://doi.org/10.1016/j.jvcir.2021.103231,1047-3203,2021,103231,79,Journal of Visual Communication and Image Representation,Generative image inpainting with salient prior and relative total variation,article,SHAO2021103231
"Although colorful information in natural scenes can be collected, due to the limitation of camera depth of field, it is hard to capture an image with all-in-focus. Sparse representation (SR)-based methods have shown their powerful potentiality and ability in multi-focus image fusion. However, because of sparse coding and information compress, the existing fusion methods based on SR are imperfect to seize the rich details and significant texture information in source images. As a result, a fusion method based on multi-scale sparse representation for registered multi-focus images (MIF-MsSR) is proposed in this paper, where an adaptive fusion rule for sparse coefficients is presented. At first, source images are processed by multi-scale decomposition and sub-images with different scales can be obtained. According to image features with different richness in these sub-images, dictionaries with different sizes and redundancy are thereby trained. By comprehensively considering the relationships of focused areas, out-of-focused areas and boundary areas between the source images, an adaptive fusion rule based on l0âmax and Sum Modified Laplacian (SML) is proposed. Finally, a fused image with all-in-focus can be obtained by sparse reconstruction and inverse multi-scale decomposition. Excessive experiments on multi-focus images have demonstrated that the proposed MIF-MsSR not only reserves the integrity of the information in source images, but also has better fusion performance on subjective and objective indicators than other state-of-the-art methods.","Multi-scale decomposition, Sparse representation, Adaptive fusion rule, Sum modified Laplacian, Multi-focus image fusion",Xiaole Ma and Zhihai Wang and Shaohai Hu,https://www.sciencedirect.com/science/article/pii/S1047320321002169,https://doi.org/10.1016/j.jvcir.2021.103328,1047-3203,2021,103328,81,Journal of Visual Communication and Image Representation,Multi-focus image fusion based on multi-scale sparse representation,article,MA2021103328
"Double random phase encryption (DRPE) system is a simple and powerful encoding technique that consists of only two lenses and two random phase masks. However, there are many issues for applying to actual security systems such as phase acquisition, vulnerability to phase retrieval techniques, and data throughput. Although various extensions of DRPE have addressed each issue, there is no comprehensive solution. To tackle all the issues of DRPE, we propose a new amplitude-based DRPE (ADRPE) system using deep learning. The encoding is the same as the current ADRPE system, and the decoding is achieved by an inverse ADRPE system using convolution neural networks. Our system can achieve a real-time end-to-end encryption system without any additional optical devices and exposure of the keys. To demonstrate our method, we applied it to simulations with various datasets such as passwords, Quick-Response (QR) codes, and fingerprints.","Optical encryption, Deep neural network, Amplitude-based double random phase encryption",Kotaro Inoue and Myungjin Cho,https://www.sciencedirect.com/science/article/pii/S1047320321001632,https://doi.org/10.1016/j.jvcir.2021.103251,1047-3203,2021,103251,79,Journal of Visual Communication and Image Representation,Amplitude based keyless optical encryption system using deep neural network,article,INOUE2021103251
"Video hashing is a useful technique of many multimedia systems, such as video copy detection, video authentication, tampering localization, video retrieval, and anti-privacy search. In this paper, we propose a novel video hashing with secondary frames and invariant moments. An important contribution is the secondary frame construction with 3D discrete wavelet transform, which can reach initial data compression and robustness against noise and compression. In addition, since invariant moments are robust and discriminative features, hash generation based on invariant moments extracted from secondary frames can ensure good classification of the proposed video hashing. Extensive experiments on 8300 videos are conducted to validate efficiency of the proposed video hashing. The results show that the proposed video hashing can resist many digital operations and has good discrimination. Performance comparisons with some state-of-the-art algorithms illustrate that the proposed video hashing outperforms the compared algorithms in classification in terms of receiver operating characteristic results.","Video hashing, Hash function, Secondary frame, Discrete wavelet transform (DWT), Invariant moments",Zhenjun Tang and Shaopeng Zhang and Xianquan Zhang and Zhixin Li and Zhenhai Chen and Chunqiang Yu,https://www.sciencedirect.com/science/article/pii/S1047320321001358,https://doi.org/10.1016/j.jvcir.2021.103209,1047-3203,2021,103209,79,Journal of Visual Communication and Image Representation,Video hashing with secondary frames and invariant moments,article,TANG2021103209
"Static hand gesture (HG) recognition for both user-dependent and user-independent is a challenging problem, especially when there are changes in lighting, hand position, and background, the recognition becomes more complex. To solve this problem, this paper proposes a static hand gesture recognition based on a set of image descriptors: Gradient Local Auto-Correlation (GLAC), Gabor Wavelet Transform (GWT), and Fast Discrete Curve Transform (FDCT). Principal Component Analysis (PCA) was used to reduce dimensionality. Tests were performed on three sign language datasets and one hand posture dataset using neural network classifiers, K-Nearest Neighbor (KNN) classifiers, and combined classifiers. The results obtained were compared to the state of the art and show an accuracy of 100% for user-independent and 98.33% for user-dependent gestures, despite the difficult acquisition conditions of the datasets.","Static hand gesture recognition, Sign language recognition, GLAC, Gabor wavelet, Curvelet transform, Combined classifiers",Khadidja Sadeddine and Fatma Zohra Chelali and Rachida Djeradi and Amar Djeradi and Sidahmed Benabderrahmane,https://www.sciencedirect.com/science/article/pii/S1047320321001231,https://doi.org/10.1016/j.jvcir.2021.103193,1047-3203,2021,103193,79,Journal of Visual Communication and Image Representation,Recognition of user-dependent and independent static hand gestures: Application to sign language,article,SADEDDINE2021103193
"The intelligent multimedia processing community has developed increasing interest in kernel entropy component analysis (KECA) due to its abilities in effective data transformation and dimensionality reduction. However, since only the unsupervised structural information of Renyi entropy from the given data set is utilized, KECA alone is incapable of generating high quality discriminant features. Aiming to develop a new and generic approach for feature representation learning, this paper proposes a discriminant kernel entropy-based framework, which integrates KECA and a complete discriminant strategy (consisting of regular and irregular discriminant information), to explore discriminant feature representations from the given data set. The framework is realized and further optimized to generate a more powerful discriminant descriptor for feature representation learning, leading to improved performance. Since the joint utilization of kernel entropy estimation and the complete discriminant strategy is able to reveal the distribution and semantic information of the given data, the proposed framework opens up a new front for discriminant feature representation learning via information theoretic learning (ITL). To demonstrate the generic nature and effectiveness of the proposed framework, experiments are conducted on two different data sources; the visual data source (e.g., University of California Irvine (UCI) database, Olivetti Research Lab (ORL) database, Caltech 256 database) and the audio data source (Ryerson Multimedia Lab (RML) audio emotion database). The results show this framework yields superior performance over other methods on the data sets evaluated for feature representation learning.","Kernel entropy component analysis, Discriminant analysis, Feature representation learning, Iris visualization, Face recognition, Emotion recognition, Object recognition",Lei Gao and Lin Qi and Ling Guan,https://www.sciencedirect.com/science/article/pii/S104732032100239X,https://doi.org/10.1016/j.jvcir.2021.103366,1047-3203,2021,103366,81,Journal of Visual Communication and Image Representation,A discriminant kernel entropy-based framework for feature representation learning,article,GAO2021103366
"In recent years, quaternion matrix completion (QMC) based on low-rank regularization has been gradually used in image processing. Unlike low-rank matrix completion (LRMC) which handles RGB images by recovering each color channel separately, QMC models retain the connection of three channels and process them as a whole. Most of the existing quaternion-based methods formulate low-rank QMC (LRQMC) as a quaternion nuclear norm (a convex relaxation of the rank) minimization problem. The main limitation of these approaches is that they minimize the singular values simultaneously such that cannot approximate low-rank attributes efficiently. To achieve a more accurate low-rank approximation, we introduce a quaternion truncated nuclear norm (QTNN) for LRQMC and utilize the alternating direction method of multipliers (ADMM) to get the optimization in this paper. Further, we propose weights to the residual error quaternion matrix during the update process for accelerating the convergence of the QTNN method with admissible performance. The weighted method utilizes a concise gradient descent strategy which has a theoretical guarantee in optimization. The effectiveness of our method is illustrated by experiments on real visual data sets.","Quaternion matrix completion, Low-rank, Quaternion truncated nuclear norm, Weights",Liqiao Yang and Kit Ian Kou and Jifei Miao,https://www.sciencedirect.com/science/article/pii/S1047320321002200,https://doi.org/10.1016/j.jvcir.2021.103335,1047-3203,2021,103335,81,Journal of Visual Communication and Image Representation,Weighted truncated nuclear norm regularization for low-rank quaternion matrix completion,article,YANG2021103335
"Spatialâtemporal information is easy to achieve in a practical surveillance scene, but it is often neglected in most current person reidentification (ReID) methods. Employing spatialâtemporal information as a constrain has been verified as beneficial for ReID. However, there is no effective modeling according to the pedestrian movement law. In this paper, we present a ReID framework with internal and external spatialâtemporal constraints, termed as IESC-ReID. A novel residual spatial attention module is proposed to build a spatialâtemporal constraint and increase the robustness to partial occlusions or camera viewpoint changes. A Laplace-based spatialâtemporal constraint is also introduced to eliminate irrelevant gallery images, which are gathered by the internal learning network. IESC-ReID constrains the attention within the functioning range of the channel space, and utilizes additional spatialâtemporal constrains to further constrain results. Intensive experiments show that these constraints consistently improve the performance. Extensive experimental results on numerous publicly available datasets show that the proposed method outperforms several state-of-the-art ReID algorithms. Our code is publicly available at https://github.com/jiaming-wang/IESC.","Person reidentification, Convolution neural network, Attention mechanism, Spatialâtemporal constraint",Zhenfeng Shao and Jiaming Wang and Tao Lu and Ruiqian Zhang and Xiao Huang and Xianwei Lv,https://www.sciencedirect.com/science/article/pii/S1047320321001991,https://doi.org/10.1016/j.jvcir.2021.103302,1047-3203,2021,103302,80,Journal of Visual Communication and Image Representation,Internal and external spatialâtemporal constraints for person reidentification,article,SHAO2021103302
"Monocular pipelines are convenient and cost-effective solutions for object distance estimation in 3D vision. Current methods for monocular object distance estimation either perform inaccurately or require heavy work on data collection. In this paper, we propose a network with R-CNN based structure to implement object detection and distance estimation simultaneously. We append an efficient branch to integrate the information of camera extrinsic parameters with RGB data in our network. Further, optimized multi-scale feature is utilized to enrich the representation power of deep feature, hence to enhance the estimation accuracy. Finally, several regression methods are explored to improve distance estimation results. We train and validate our network on KITTI object dataset, and compare with other methods to show that our method is accurate and easy to train. To prove the generality of our method under other scenarios, we construct a dataset of surveillance scenes, and conduct similar experiments on this dataset.","Monocular distance estimation, Object detection, Deep neural network, Surveillance",Yufeng Zhang and Lianghui Ding and Yuxi Li and Weiyao Lin and Mingbi Zhao and Xiaoyuan Yu and Yunlong Zhan,https://www.sciencedirect.com/science/article/pii/S1047320321001474,https://doi.org/10.1016/j.jvcir.2021.103224,1047-3203,2021,103224,79,Journal of Visual Communication and Image Representation,A regional distance regression network for monocular object distance estimation,article,ZHANG2021103224
"Animated GIF has become a key communication tool in contemporary social platforms thanks to highly compatible with affective performance, and it is gradually adopted in commercial applications. Therefore, the copyright protection of the animated GIF requires more attention. Digital watermarking is an effective method to embed invisible data into a digital medium that can identify the creator or authorized users. However, few works have been devoted to robust watermarking for the animated GIF. One of the main challenges is that the animated image also contains time frame dimension information compare with still images. This paper proposes a robust blind watermarking framework based 3D convolutional neural networks for the animated GIF image, which achieves watermark image embedding and extraction for the animated GIF. Also, noise simulation is developed in frame-level to ensure robustness for the attack of the temporal dimension in this framework. Furthermore, the invisibility of the watermarked animated image is optimized by adversarial learning. Experimental results provide the effectiveness of the proposed framework and show advantages over existing works.","Animated GIF images, Robust watermarking, 3D convolutional neural networks, Adversarial network",Xin Liao and Jing Peng and Yun Cao,https://www.sciencedirect.com/science/article/pii/S1047320321001590,https://doi.org/10.1016/j.jvcir.2021.103244,1047-3203,2021,103244,79,Journal of Visual Communication and Image Representation,GIFMarking: The robust watermarking for animated GIF based deep learning,article,LIAO2021103244
"Person re-identification (re-ID) based on unsupervised domain adaptation intends to distill knowledge from annotated source dataset to identify target persons in another dataset. Although the advanced UDA re-ID models are dominated by pseudo-label methods, they almost transform images from various camera views into the same feature space, without considering the camera distribution gaps, which may lead to generate considerably noisy pseudo-labels. In this study, we develop an IntraâInter Camera Affinity Domain Adaptation (I2CADA) to tackle these problems for UDA person re-ID. Precisely, I2CADA framework is composed of two modules. The first one is generative adversarial learning module, aiming to train a feature extractor that can map target data to source feature space by supervised learning and adversarial learning, which can relieve the distribution gap between different datasets (domains). The second one is affinity transfer learning module, which simultaneously considers intra-camera clustering and inter-camera separation among persons with similar appearances in the target domain, thus mitigating the distribution inconsistency among person images collected from multiple target camera views. Besides, comprehensive experiments exhibit that I2CADA outperforms the existing UDA person re-identification approaches.","IntraâInter Camera Affinity Domain Adaptation, Person re-identification, Generative adversarial learning, Affinity transfer learning",Guiqing Liu and Jinzhao Wu,https://www.sciencedirect.com/science/article/pii/S1047320321002042,https://doi.org/10.1016/j.jvcir.2021.103310,1047-3203,2021,103310,80,Journal of Visual Communication and Image Representation,Unsupervised person re-identification by IntraâInter Camera Affinity Domain Adaptation,article,LIU2021103310
"Single-view 3D shapes generation has achieved great success in recent years. However, current methods always blind the learning of shapes and viewpoints. The generated shape only fit the observed viewpoints and would not be optimal from unknown viewpoints. In this paper, we propose a novel encoderâdecoder based network which contains a disentangled transformer to generate the viewpoint-invariant 3D shapes. The differentiable and parametric Non-uniform B-spline (NURBS) surface generation and 3D-to-3D viewpoint transformation are incorporated to learn the viewpoint-invariant shape and the camera viewpoint, respectively. Our new framework allows us to learn the latent geometric parameters of shapes and viewpoints without knowing the ground truth viewpoint. That can simultaneously generate camera-viewpoint and viewpoint-invariant 3D shapes of the object. We analyze the effects of disentanglement and show both quantitative and qualitative results of shapes generated at various unknown viewpoints.","3D shape generation, Invariant viewpoint, Disentanglement, B-spline surfaces",Jinglun Yang and Youhua Li and Lu Yang,https://www.sciencedirect.com/science/article/pii/S1047320321002285,https://doi.org/10.1016/j.jvcir.2021.103345,1047-3203,2021,103345,81,Journal of Visual Communication and Image Representation,Shape transformer nets: Generating viewpoint-invariant 3D shapes from a single image,article,YANG2021103345
"Kernel based Sparse Representation Classifier (KSRC) can classify images with acceptable performance. In addition, Multiple Kernel Learning based SRC (MKL-SRC) computes the weighted sum of multiple kernels in order to construct a unified kernel while the weight of each kernel is calculated as a fixed value in the training phase. In this paper, an MKL-SRC with non-fixed kernel weights for dictionary atoms is proposed. Kernel weights are embedded as new variables to the main KSRC goal function and the resulted optimization problem is solved to find the sparse coefficients and kernel weights simultaneously. As a result, an atom specific multiple kernel dictionary is computed in the training phase which is used by SRC to classify test images. Also, it is proved that the resulting optimization problem is convex and is solvable via common algorithms. The experimental results demonstrate the effectiveness of the proposed approach.","Sparse Representation Classifier, Multiple Kernel Learning, Kernel local weighting, Dictionary learning, Image classification",Fatemeh Zamani and Mansour Jamzad and Hamid R. Rabiee,https://www.sciencedirect.com/science/article/pii/S1047320321001504,https://doi.org/10.1016/j.jvcir.2021.103228,1047-3203,2021,103228,79,Journal of Visual Communication and Image Representation,Atom specific multiple kernel dictionary based Sparse Representation Classifier for medium scale image classification,article,ZAMANI2021103228
"In this paper, we follow the trend of deep learning and make an attempt to investigate the potential benefit of using multispectral images via convolutional neural networks for background subtraction task. The major contributions of this work lie in two aspects, based on the impressive algorithm FgSegNet_v2. Firstly, we extract three channels out of the seven of the FluxData FD-1665 multispectral dataset to match the number of input channels of the VGG16 deep model. Some combinations of three-channel based multispectral images perform better than RGB images. Secondly, a new convolutional encoder is designed to use all the multispectral channels available to further explore the information of multispectral images. The results outperform the RGB images and also other approaches using the same multispectral dataset.","Background subtraction, Multispectral images, Deep learning, Convolutional neural networks",Rongrong Liu and Yassine Ruichek and Mohammed {El Bagdouri},https://www.sciencedirect.com/science/article/pii/S1047320321001747,https://doi.org/10.1016/j.jvcir.2021.103267,1047-3203,2021,103267,80,Journal of Visual Communication and Image Representation,Multispectral background subtraction with deep learning,article,LIU2021103267
"In this paper, we propose an effective method for quality assessment of screen content images (SCIs) based on multi-stage dictionary learning. To simulate the brainâs layered processing of signals, we proposed a hierarchical feature extraction strategy, which is called multi-stage dictionary learning, to simulate the hierarchical information processing of brain. First, the standard deviation of normalized map obtained from training image is used to select the training data in a certain proportion, which can ensure the learning efficiency and reduce the training burden. Next, the reconstructed map is weighted as the input of the next-stage dictionary learning. Then using the trained dictionary, the sparse representation is applied to extract features. Meanwhile, considering that some important features may be ignored in the process of multi-stage dictionary learning, we use Log Gabor filter to extract feature maps, and then calculate the correlation between feature maps as another kind of compensation features. Final, for the two feature sets, we choose SVR and feature codebook to learn two objective scores, and then use the adaptive weighting strategy to get the final objective quality score. Experimental results show that the proposed method is superior to several mainstream SCIs metrics on two publicly available databases.","Image quality assessment, Screen content images, Sparse dictionary, Hierarchical feature extraction",Yongli Chang and Sumei Li and Anqi Liu and Jie Jin,https://www.sciencedirect.com/science/article/pii/S1047320321001620,https://doi.org/10.1016/j.jvcir.2021.103248,1047-3203,2021,103248,79,Journal of Visual Communication and Image Representation,Quality assessment of screen content images based on multi-stage dictionary learning,article,CHANG2021103248
"Computer vision tasks are often expected to be executed on compressed images. Classical image compression standards like JPEG 2000 are widely used. However, they do not account for the specific end-task at hand. Motivated by works on recurrent neural network (RNN)-based image compression and three-dimensional (3D) reconstruction, we propose unified network architectures to solve both tasks jointly. These joint models provide image compression tailored for the specific task of 3D reconstruction. Images compressed by our proposed models, yield 3D reconstruction performance superior as compared to using JPEG 2000 compression. Our models significantly extend the range of compression rates for which 3D reconstruction is possible. We also show that this can be done highly efficiently at almost no additional cost to obtain compression on top of the computation already required for performing the 3D reconstruction task.","Image compression, 3D reconstruction, Deep learning, Recurrent neural networks",Alex Golts and Yoav Y. Schechner,https://www.sciencedirect.com/science/article/pii/S1047320321001346,https://doi.org/10.1016/j.jvcir.2021.103208,1047-3203,2021,103208,79,Journal of Visual Communication and Image Representation,Image compression optimized for 3D reconstruction by utilizing deep neural networks,article,GOLTS2021103208
"Sorting-based reversible data hiding (RDH) methods like pixel-value-ordering (PVO) can predict pixel values accurately and achieve an extremely low distortion on the embedded image. However, the excellent performance of these methods was not well explained in previous works, and there are unexploited common points among them. In this paper, we propose a general multi-predictor (GMP) framework to summarize PVO-based RDH methods and explain their high prediction accuracy. Moreover, by utilizing the proposed GMP framework, a more efficient sorting-based RDH method is given as an example to show the generality and applicability of our framework. Comparing with other PVO-based methods, the proposed example method can achieve significant improvement in embedding performance. It is hopeful that more efficient sorting-based RDH algorithms can be designed according to our proposed framework by designing better predictors and their combination methods.","Reversible data hiding, General multi-predictor framework, Pixel-value-ordering (PVO), Additional predictors, Adaptive complexity",Guojun Fan and Zhibin Pan and Quan Zhou and Xinyi Gao and Xiaoran Zhang,https://www.sciencedirect.com/science/article/pii/S1047320321002315,https://doi.org/10.1016/j.jvcir.2021.103349,1047-3203,2021,103349,81,Journal of Visual Communication and Image Representation,A comparative study between PVO-based framework and multi-predictor mechanism in reversible data hiding,article,FAN2021103349
"Featuring with more uniform sampling density in the sphere domain and less non-uniform geometric deformations in the planar domain, variants of cubemap projection (CMP) format enable the higher compression ratio in on-going 360-degree video coding standardization. Different from single-view videos, 360-degree CMP videos feature with content discontinuity combined with the abrupt change of motion vectors between some adjacent faces. However, there is few bit allocation scheme designed for rate control of video coding of CMP format. Thus, this paper proposes a region-level bit allocation scheme for rate control of interframe coding of CMP format. The proposed scheme consists of two parts. The first part is machine learning based high HEVC coding cost region detection for individual faces, where the feature descriptor of a CTU consists of the face based texture complexity, motion magnitude, motion density, and temporal coherence of motion vector. The second part is fitting function based region-level bit allocation. Different from previous work, bits are assigned to the high coding cost region and non-high coding cost region in individual faces of CMP format. Experimental results indicate that the proposed scheme achieves higher bitrate accuracy and larger BD-WS-PSNR compared with the original rate control scheme of the reference software of HEVC, HM16.16 with the 360Lib.","360-degree video coding, Cubemap projection (CMP), Rate control, Bit allocation, Machine learning, Detection of high HEVC coding cost regions",Yu-Chieh Nien and Chih-Wei Tang,https://www.sciencedirect.com/science/article/pii/S1047320321001589,https://doi.org/10.1016/j.jvcir.2021.103242,1047-3203,2021,103242,79,Journal of Visual Communication and Image Representation,Region-level bit allocation for rate control of 360-degree videos using cubemap projection,article,NIEN2021103242
"The visual system prioritizes emotional content in natural scenes, but it is unclear whether emotional objects are systematically more salient. We compare emotional maps - created by averaging multiple manual selections of the most meaningful regions in images of negative, positive, and neutral affective valence - with saliency maps generated by Graph-Based Visual Saliency, Proto-object, and SalGAN models. We found that similarity between emotional and saliency maps is modulated by the scenesâ arousal and valence ratings: the more negative and high-arousing content, the less it was salient. Simultaneously, the negative and high-arousing content was the easiest to identify by the participants, as shown by the highest inter-individual agreement in the selections. Our results support the âaffective gapâ hypothesis, i.e., decoupling of emotional meaning from imageâs formal features. The Emotional Maps Database created for this study, proven useful in gaze fixation prediction, is available online for scientific use.","Meaning maps, Saliency, Emotion, Arousal, Natural scenes, Key objects",Joanna Pilarczyk and Weronika Janeczko and RadosÅaw Sterna and MichaÅ Kuniecki,https://www.sciencedirect.com/science/article/pii/S1047320321001462,https://doi.org/10.1016/j.jvcir.2021.103221,1047-3203,2021,103221,79,Journal of Visual Communication and Image Representation,Are emotional objects visually salient? The Emotional Maps Database,article,PILARCZYK2021103221
"Visual Cryptography Scheme (VCS) is a secret-sharing scheme which aims to encrypt a secret message into multiple shares and transmit them to participants over an untrusted communication channel. Although human vision can easily reveal the secret message by stacking a sufficient number of shares, this scheme reduces the visual quality of recovered images. This paper presents a novel high-quality and printer-friendly VCS. When providing high-quality recovery, this scheme keeps the size of the shares the same as the secret image. Experimental results show that, compared with previous work, the proposed scheme significantly improves the performance of recovered images.","Visual cryptography scheme, Image security, Halftoning, Color VCS",Denghui Zhang and Hongbin Zhu and Shenglong Liu and Xu Wei,https://www.sciencedirect.com/science/article/pii/S1047320321001188,https://doi.org/10.1016/j.jvcir.2021.103186,1047-3203,2021,103186,78,Journal of Visual Communication and Image Representation,HP-VCS: A high-quality and printer-friendly visual cryptography scheme,article,ZHANG2021103186
"Image stitching is developed to generate wide-field images or panoramic images for virtual reality applications. However, the quality assessment of stitched images with respect to various stitching algorithms has been less studied. Effective stitched image quality assessment (SIQA) is advantageous to evaluate the performance of various stitching methods and optimize the design of stitching methods. In this paper, we propose a novel SIQA method by exploiting local measurement errors and global statistical properties for feature extraction. Comprehensive image attributes including ghosting, misalignment, structural distortion, geometric error, chromatic aberrations and blur are considered either locally or globally. The extracted local and global features are aggregated into an overall quality via regression. Experimental results on two benchmark databases demonstrate the superiority of the proposed metric over both the state-of-the-art quality models designed for natural images and stitched images.","Image stitching, Stitched image quality assessment, Structural distortion, Geometric error, Quality aggregation",Chongzhen Tian and Xiongli Chai and Feng Shao,https://www.sciencedirect.com/science/article/pii/S1047320321002157,https://doi.org/10.1016/j.jvcir.2021.103324,1047-3203,2021,103324,81,Journal of Visual Communication and Image Representation,Stitched image quality assessment based on local measurement errors and global statistical properties,article,TIAN2021103324
"Few-shot semantic segmentation (FSS) has drawn great attention in the community of computer vision, due to its remarkable potential for segmenting novel objects with few pixel-annotated samples. However, some interference factors, such as insufficient illumination and complex background, can impose more challenge to the segmentation performance than fully-supervised when the number of samples is insufficient. Therefore, we propose the visible and thermal (V-T) few-shot semantic segmentation task, which utilize the complementary and similar information of visible and thermal images to boost few-shot segmentation performance. As the first step, we build a novel outdoor city dataset Tokyo Multi-Spectral-4i for the V-T few-shot semantic segmentation task. In addition, a fusion architecture is proposed, which consists of an Edge Similarity fusion module (ES) and a Texture Edge Prototype module (TEP). The ES module fuses the bi-modal information by exploiting the edge similarity in the visible and thermal images. The TEP module extracts the prototype from two models by collaborating the representativeness and complementarity of the visible and thermal feature. Finally, extensive experiments conducted on the proposed datasets demonstrate that our architecture can achieve state-of-the-arts results.","V-T semantic segmentation, Thermal images, Few-shot semantic segmentation",Yanqi Bao and Kechen Song and Jie Wang and Liming Huang and Hongwen Dong and Yunhui Yan,https://www.sciencedirect.com/science/article/pii/S1047320321002030,https://doi.org/10.1016/j.jvcir.2021.103306,1047-3203,2021,103306,80,Journal of Visual Communication and Image Representation,Visible and thermal images fusion architecture for few-shot semantic segmentation,article,BAO2021103306
"In video-based action recognition, using videos with different frame numbers to train a two-stream network can result in data skew problems. Moreover, extracting the key frames from a video is crucial for improving the training and recognition efficiency of action recognition systems. However, previous works suffer from problems of information loss and optical-flow interference when handling videos with different frame numbers. In this paper, an augmented two-stream network (ATSNet) is proposed to achieve robust action recognition. A frame-number-unified strategy is first incorporated into the temporal stream network to unify the frame numbers of videos. Subsequently, the grayscale statistics of the optical-flow images are extracted to filter out any invalid optical-flow images and produce the dynamic fusion weights for the two branch networks to adapt to different action videos. Experiments conducted on the UCF101 dataset demonstrate that ATSNet outperforms previously defined methods, improving the recognition accuracy by 1.13%.","Two-stream network, Action recognition, Data skew",Chuanjiang Leng and Qichuan Ding and Chengdong Wu and Ange Chen,https://www.sciencedirect.com/science/article/pii/S1047320321002273,https://doi.org/10.1016/j.jvcir.2021.103344,1047-3203,2021,103344,81,Journal of Visual Communication and Image Representation,Augmented two stream network for robust action recognition adaptive to various action videos,article,LENG2021103344
"Graph methods have been widely employed in re-ranking for image retrieval. Although we can effectively find visually similar images through these methods, the ranking lists given by those approaches may contain some candidates which appear to be irrelevant to a query. Most of these candidates fall into two categories: (1) the irrelevant outliers located near to the query images in a graph; and (2) the images from another cluster which close to the query. Therefore, eliminating these two types of images from the ordered retrieval sets is expected to further boost the retrieval precision. In this paper, we build a Three Degree Binary Graph (TDBG) to eliminate the outliers and utilize a set-based greedy algorithm to reduce the influence of adjacent manifolds. Moreover, a multi-feature fusion method is proposed to enhance the retrieval performance further. Experimental results obtained on three public datasets demonstrate the superiority of the proposed approach.","Unsupervised re-ranking, Image retrieval, Multi-feature fusion, Graph learning",Guihong Lao and Shenglan Liu and Chenwei Tan and Yang Wang and Guangzhe Li and Li Xu and Lin Feng and Feilong Wang,https://www.sciencedirect.com/science/article/pii/S104732032100184X,https://doi.org/10.1016/j.jvcir.2021.103282,1047-3203,2021,103282,80,Journal of Visual Communication and Image Representation,Three Degree Binary Graph and Shortest Edge Clustering for re-ranking in multi-feature image retrieval,article,LAO2021103282
"A deep learning framework for 3D point cloud processing is proposed in this work. In a point cloud, local neighborhoods have various shapes, and the semantic meaning of each point is determined within the local shape context. Thus, we propose shape-adaptive filters (SAFs), which are dynamically generated from the distributions of local points. The proposed SAFs can extract robust features against noise or outliers, by employing local shape contexts to suppress them. Also, we develop the SAF-Nets for classification and segmentation using multiple SAF layers. Extensive experimental results demonstrate that the proposed SAF-Nets significantly outperform the state-of-the-art conventional algorithms on several benchmark datasets. Moreover, it is shown that SAFs can improve scene flow estimation performance as well.","Point cloud processing, Shape-adaptive filter, Deep learning",Seon-Ho Lee and Chang-Su Kim,https://www.sciencedirect.com/science/article/pii/S1047320321001619,https://doi.org/10.1016/j.jvcir.2021.103246,1047-3203,2021,103246,79,Journal of Visual Communication and Image Representation,SAF-Nets: Shape-Adaptive Filter Networks for 3D point cloud processing,article,LEE2021103246
"This paper introduces a fully automatic method for kinship verification from facial images. Recently, a number of methods have been proposed to verify kinship from facial images, however, most of these methods are needed to exactly align face images before feature extraction in a manual manner. Unlike these methods, our method does not depend on face alignment. Firstly, we localize several facial feature points by utilizing a facial feature detector to extract SIFT descriptor around each feature point of a face image. Lastly, two ways, feature combination and distance metric learning, are used to verify the kinship of a pair of face images. For feature combination, three simple strategies of feature combination and support vector machine classifier are used for kinship verification. For metric learning, we propose a component-based metric learning (CML) method to measure the distance of each face pair, which jointly learns multiple local distance metrics, and one specific distance metric for each facial feature point. Experimental results show the effectiveness of our proposed approach on two popular kinship datasets.","Kinship verification, Metric learning, Component, Feature combination, Facial image",Huishan Wu and Jiawei Chen and Xiao Liu and Junlin Hu,https://www.sciencedirect.com/science/article/pii/S1047320321001735,https://doi.org/10.1016/j.jvcir.2021.103265,1047-3203,2021,103265,79,Journal of Visual Communication and Image Representation,Component-based metric learning for fully automatic kinship verification,article,WU2021103265
"The power of convolutional neural networks (CNN) has demonstrated irreplaceable advantages in super-resolution. However, many CNN-based methods need large model sizes to achieve superior performance, making them difficult to apply in the practical world with limited memory footprints. To efficiently balance model complexity and performance, we propose a multi-scale attention network (MSAN) by cascading multiple multi-scale attention blocks (MSAB), each of which integrates a multi-scale cross block (MSCB) and a multi-path wide-activated attention block (MWAB). Specifically, MSCB initially connects three parallel convolutions with different dilation rates hierarchically to aggregate the knowledge of features at different levels and scales. Then, MWAB split the channel features from MSCB into three portions to further improve performance. Rather than being treated equally and independently, each portion is responsible for a specific function, enabling internal communication among channels. Experimental results show that our MSAN outperforms most state-of-the-art methods with relatively few parameters and Mult-Adds.","Super-resolution, Multi-scale, Attention mechanism, Lightweight",Li Wang and Jie Shen and E. Tang and Shengnan Zheng and Lizhong Xu,https://www.sciencedirect.com/science/article/pii/S1047320321001978,https://doi.org/10.1016/j.jvcir.2021.103300,1047-3203,2021,103300,80,Journal of Visual Communication and Image Representation,Multi-scale attention network for image super-resolution,article,WANG2021103300
"Vehicle re-identification (reID) aims to search the target vehicle in a non-overlapping multi-camera network, which is important for the intelligent analysis in large scale of surveillance videos. Many existing methods have employed various techniques to achieve discriminative information. However, those methods always focus on the description of one view for the same vehicle images. Hence, a generated multiple sparse information fusion method is proposed for exploiting latent features from multi-views, which employs three different deep networks to extract multiple features from coarse to fine. And these features are regarded as multi-view features. Besides, to fuse these features reasonably, the paper transfers various features into a common space for better seeking distinctive features. Especially, besides ResNet, two feature learning networks are proposed to learn different features, respectively. One is designed to learn robust feature by dropping some features randomly when training the reID model. Another is to combine various salient features from different layers, which forms strong features for the reID task. Moreover, comprehensive experimental results have demonstrated that our proposed method can achieve competitive performances on benchmark datasets VehicleID and VeRi-776.","Vehicle re-identification, Hierarchical attention network, Multi-views",Jinjia Peng and Guangqi Jiang and Huibing Wang,https://www.sciencedirect.com/science/article/pii/S1047320321001334,https://doi.org/10.1016/j.jvcir.2021.103207,1047-3203,2021,103207,79,Journal of Visual Communication and Image Representation,Generalized multiple sparse information fusion for vehicle re-identification,article,PENG2021103207
"Aggregation of local and global contextual information by exploiting multi-level features in a fully convolutional network is a challenge for the pixel-wise salient object detection task. Most existing methods still suffer from inaccurate salient regions and blurry boundaries. In this paper, we propose a novel edge-aware global and local information aggregation network (GLNet) to fully exploit the integration of side-output local features and global contextual information and utilization of contour information of salient objects. The global guidance module (GGM) is proposed to learn discriminative multi-level information with the direct guidance of global semantic knowledge for more accurate saliency prediction. Specifically, the GGM consists of two key components, where the global feature discrimination module exploits the inter-channel relationship of global semantic features to boost representation power, and the local feature discrimination module enables different side-output local features to selectively learn informative locations by fusing with global attentive features. Besides, we propose an edge-aware aggregation module (EAM) to employ the correlation between salient edge information and salient object information for generating estimated saliency maps with explicit boundaries. We evaluate our proposed GLNet on six widely-used saliency detection benchmark datasets by comparing with 17 state-of-the-art methods. Experimental results show the effectiveness and superiority of our proposed method on all the six benchmark datasets.","Salient object detection, Saliency, Multi-level feature aggregation, Attention, Salient edge",Qing Zhang and Liqian Zhang and Dong Wang and Yanjiao Shi and Jiajun Lin,https://www.sciencedirect.com/science/article/pii/S1047320321002303,https://doi.org/10.1016/j.jvcir.2021.103350,1047-3203,2021,103350,81,Journal of Visual Communication and Image Representation,Global and local information aggregation network for edge-aware salient object detection,article,ZHANG2021103350
"In this paper, we present a new approach for single image dehazing based on the proposed variational optimization. A hazy image captures the information about haze in terms of the transmission map and object details present in it. We propose to estimate the initial transmission map by performing the structure-aware smoothing of the hazy image. Further, we formulated a variational optimization for the estimation of final transmission, which refines the initial transmission of a hazy image. Atmospheric light can be considered to be constant throughout the scene for practical purposes. The uniform atmospheric light is computed from the dark channel of a hazy image. The exhaustive experimentation shows that the performance of the proposed method is comparable or better.","Image dehazing, Transmission, Atmospheric light, Haze",Kavinder Singh and Anil Singh Parihar,https://www.sciencedirect.com/science/article/pii/S1047320321001577,https://doi.org/10.1016/j.jvcir.2021.103241,1047-3203,2021,103241,79,Journal of Visual Communication and Image Representation,Variational optimization based single image dehazing,article,SINGH2021103241
"Understanding of indoor scenes has considerable value in computer vision. Most previous methods infer indoor scenes via manhattan assumption. However, attic ceilings do not satisfy manhattan assumption and understanding them remains a big challenge. Non-manhattan ceilings can be seen as compositions of spatial right-angles projections. In this paper, we presented a method to understand indoor scenes including both manhattan structures and non-manhattan attic ceilings from a single image. First, angle projections are detected and assigned to different clusters. Then vanishing points of attic ceilings can be estimated. Third, it is possible to determine the attic ceilings of non-manhattan surfaces. The proposed approach requires no prior training. We compared the estimated attic layout against the ground truth and measured the percentage of pixels that were incorrectly classified. Experimental results showed that the method can understand indoor scenes including both manhattan and non-manhattan attic ceilings, meeting the requirements of robot navigation.","Non-manhattan, Ceiling, Indoor scene, Spatial right-angle, Monocular vision",Luping Wang and Hui Wei,https://www.sciencedirect.com/science/article/pii/S1047320321002029,https://doi.org/10.1016/j.jvcir.2021.103307,1047-3203,2021,103307,80,Journal of Visual Communication and Image Representation,Indoor scene understanding based on manhattan and non-manhattan projection of spatial right-angles,article,WANG2021103307
"Dense depth completion is essential for autonomous driving and robotic navigation. Existing methods focused on attaining higher accuracy of the estimated depth, which comes at the price of increasing complexity and cannot be well applied in a real-time system. In this paper, a coarse-to-fine and lightweight network (S&CNet) is proposed for dense depth completion to reduce the computational complexity with negligible sacrifice on accuracy. A dual-stream attention module (S&C enhancer) is proposed according to a new finding of deep neural network-based depth completion, which can capture both the spatial-wise and channel-wise global-range information of extracted features efficiently. Then it is plugged between the encoder and decoder of the coarse estimation network so as to improve the performance. The experiments on KITTI dataset demonstrate that the proposed approach achieves competitive result with respect to state-of-the-art works but via an almost four times faster speed. The S&C enhancer can also be easily plugged into other existing works to boost their performances significantly with negligible additional computations.","Dense depth completion, Lightweight network, Coarse-to-fine, Attention module",Ziyang Liu and Lei Zhang and Weihai Chen and Xingming Wu and Zhengguo Li,https://www.sciencedirect.com/science/article/pii/S1047320321001450,https://doi.org/10.1016/j.jvcir.2021.103220,1047-3203,2021,103220,79,Journal of Visual Communication and Image Representation,S&CNet: A lightweight network for fast and accurate depth completion,article,LIU2021103220
"In this paper, a novel salient object detection framework based on Linear Quadratic Regulator (LQR) controller is proposed. The major goal of this research is to take advantage of optimal control theory for improving the performance of detecting salient objects in images. In this regard, for the sake of detection of salient and non-salient regions, two LQR-based control systems are employed. In the proposed framework, for the initialization of the control systems, background and foreground estimations have been done with two different strategies. Doing so, we would ultimately have more effective distinction between those regions. After the initialization step, the control systems refine both estimations in parallel until reaching a steady state for each of them. Within the mentioned process, by using optimal control concept, specifically LQR controller (for the first time in the field), control signals which are in charge of determining saliency values, would be constantly optimized. At the end, the raw saliency map will be generated by combination of background and foreground optimized initial maps. Finally, the integrated saliency map will be refined by using angular embedding method. The experimental evaluations on three benchmark datasets shows that the proposed framework performs well and introduces comparable results with some deep learning based methods.","Salient object detection, Control system, Linear quadratic regulator, Optimal control, Angular embedding",Morteza Moradi and Farhad Bayat and Mostafa Charmi,https://www.sciencedirect.com/science/article/pii/S1047320321001693,https://doi.org/10.1016/j.jvcir.2021.103259,1047-3203,2021,103259,79,Journal of Visual Communication and Image Representation,A salient object detection framework using linear quadratic regulator controller,article,MORADI2021103259
"Studying the well-known phenomenon âauroraâ plays a pivotal role in investigating the solarâterrestrial coupling mechanism. A special auroral spectrograph in Antarctic Zhongshan Station constitutes a auroral observation joint system with satellite-borne sensors of the Defense Meteorological Satellite Program. Multipoint observation by this system provides more essential information for relevant studies than single observation by each instrument, but also results in a multifold increased volume of data that are difficult to be either stored or transmitted. To address this difficulty, we develop a clustering-based, generic lossless data compression framework that combines the usage of various ultimate compressors with a hierarchical clustering algorithm to exert the strength of all the compressors in data reduction. This framework achieves an always-best compression performance for different-sized datasets with a reasonable time consumption, which promises the design of pipelines using it for real-time data transmission.","Joint auroral observation, Lossless compression, Hierarchical clustering, Auroral spectral data, DMSP SSJ5, DMSP SSUSI",Kun Shang and Wanqiu Kong and Tan Qu and Zejun Hu and Jiaji Wu and Witold Pedrycz,https://www.sciencedirect.com/science/article/pii/S1047320321001176,https://doi.org/10.1016/j.jvcir.2021.103185,1047-3203,2021,103185,78,Journal of Visual Communication and Image Representation,"A generic, cluster-centred lossless compression framework for joint auroral data",article,SHANG2021103185
"The spatial regularization weight of the correlation filter is not related to the object content and the model degradation in the tracking process. To solve this problem, a new multi-frame co-saliency spatio-temporal regularization correlation filters (MCSRCF) is proposed for visual object tracking. To the best our knowledge, this is the first application of co-saliency regularization to CF-based tracking. In MCSRCF, grayscale features, directional gradient histogram (HOG) features and CNN features are extracted to improve the tracking precision of the tracker. Secondly, the three-dimensional spatial saliency and semantic saliency are introduced to obtain the initial weight of the spatial regularization with object content information. Then, the heterogeneous saliency fusion method is exploited to add a co-saliency spatial regularization term to the objective function to make the spatial penalty weight learn the change of the object region. In additional, the temporal saliency regularization is introduced to learn the information between adjacent frames, which reduces the overfitting effect caused by inaccurate samples. A variety of evaluations are conducted on public benchmarks, and the experimental results show that the proposed tracker achieves good robustness against many state-of-the-art trackers in various complex scenarios.","Object tracking, Correlation filter, Co-saliency, Spatio-temporal regularization, Heterogeneous fusion",Xi Yang and Shaoyi Li and Jun Ma and Hao Liu and Jie Yan,https://www.sciencedirect.com/science/article/pii/S1047320321002145,https://doi.org/10.1016/j.jvcir.2021.103329,1047-3203,2021,103329,81,Journal of Visual Communication and Image Representation,Multi-frame co-saliency spatio-temporal regularization correlation filters for object tracking,article,YANG2021103329
"NLNet has been considered as one milestone in the study of capturing long-range dependencies. Many recent studies modify the internal structure of NLNet directly and apply them to video object detection and semantic segmentation tasks. The dependencies between local and global features have been well developed, but the dependencies between global features of different convolution layers are rarely considered. Convolution is a local operation, so the global features of different convolution layers cannot be directly related, resulting in the loss of dependencies between global features. Given the vulnerability, this study designs a network that can efficiently capture the dependencies between the global features of different convolution layers, potentially further improving the accuracy. Furthermore, for the calculation of the dependency matrix, based on the Dot-product used in NLNet, we propose RELU-Dot-product, which can achieve higher accuracy. We evaluatethe proposed method on image classification and object detection tasks. The data sets involved are CIFAR10, CIFAR100, Tiny-imagenet, VOC2007, VOC2012 and MS COCO. Experiments show that our method can significantly improve network performance by introducing a few parameters.","Deep learning, Object detection, Non-local neural network, Global features dependencies",Zhangwei Li and Anshun Hu and Xiaofei Wang and Jun Hu and Guijun Zhang,https://www.sciencedirect.com/science/article/pii/S1047320321002388,https://doi.org/10.1016/j.jvcir.2021.103360,1047-3203,2021,103360,81,Journal of Visual Communication and Image Representation,Learning to capture dependencies between global features of different convolution layers,article,LI2021103360
"This article presents a performance analysis of Versatile Video Coding (VVC) intra-frame prediction. VVC is the next generation of video coding standards, which has been developed to supply the demand of upcoming video applications. VVC brings several innovations and enhancements for the intra-frame prediction to improve the encoding efficiency. These improvements comprise larger block sizes, more flexible block partitioning, more angular intra-frame prediction modes, multiple transform selection, non-separable secondary transform, among others. This article provides a detailed description of these tools, discussing how they work together in the intra-frame coding flow to raise the compression performance. Moreover, this article presents encoding complexity, encoding usage distribution, and rate-distortion-complexity analyses of the intra-frame prediction tools over different quantization scenarios. Based on these analyses, this article provides support for future works focusing on VVC intra-frame coding, including complexity reduction, complexity control, and real-time hardware design.","Versatile video coding, VVC, Intra-frame coding, Performance analysis",MÃ¡rio Saldanha and Gustavo Sanchez and CÃ©sar Marcon and Luciano Agostini,https://www.sciencedirect.com/science/article/pii/S1047320321001292,https://doi.org/10.1016/j.jvcir.2021.103202,1047-3203,2021,103202,79,Journal of Visual Communication and Image Representation,Performance analysis of VVC intra coding,article,SALDANHA2021103202
"Currently, weakly supervised data augmentation network (WS-DAN) has been proved to be one of the state-of-the-art methods for fine-grained image classification due to its effectiveness on attention-guided data augmentation and bilinear attention pooling. Taking WS-DAN as the backbone, in this paper, we further propose a subtler WS-DAN recognition network, namely, SWS-DAN. Specifically, we first construct a novel âsalience-guided data augmentationâ scheme composed of cutblock, part-aware cropping, and SCutMix operations, which can more effectively expand the number of training dataset and improve the weakness addressed in the data augmentation procedure of WS-DAN. Meanwhile, the novel data-augmentation manner reduces background noise and mines more discriminative regions simultaneously, thereby avoiding the overfitting. In caring about the key issue in fine-grained image classification task is how to distinguish the extremely similar subclasses (e.g., Artic Tern, Elegant Tern, and Forsters Tern), we then design a âTop-kâ loss function that mainly focuses on the similar classes so as to find their extraordinary subtle differences. Extensive experiments carried out on common fine-grained image datasets demonstrate that SWS-DAN can surpass WS-DAN with a significant margin in the classification performance.","Fine-grained, Classification, WS-DAN, SWS-DAN, Data augmentation, Loss function",Zhen Yang and Zhipeng Wang and Lingkun Luo and Hongping Gan and Tao Zhang,https://www.sciencedirect.com/science/article/pii/S1047320321001607,https://doi.org/10.1016/j.jvcir.2021.103245,1047-3203,2021,103245,79,Journal of Visual Communication and Image Representation,SWS-DAN: Subtler WS-DAN for fine-grained image classification,article,YANG2021103245
"Hashing technology improves the search efficiency and reduces the storage space of data. However, building an effective modal with unsupervised cross modal retrieval and generating efficient binary code is still a challenging task, considering of some issues needed to be further discussed and researched for unsupervised multimodal hashing. Most of the existing methods ignore the discrete restriction, and manually or experientially determine the weights of each modality. These limitations may significantly reduce the retrieval accuracy of unsupervised cross-modal hashing methods. To solve these problems, we propose a robust hash modal that can efficiently learn binary code by employing a flexible and noise-resistant â2,1-loss with nonlinear kernel embedding. In addition, we introduce an intermediate state mapping that facilitate later modal optimization to measure the loss between the hash codes and the intermediate states. Experiments on several public multimedia retrieval datasets validate the superiority of the proposed method from various aspects.","Hashing, Robust, Cross-modal retrieval, Unsupervised learning",Yuzhi Fang,https://www.sciencedirect.com/science/article/pii/S104732032100167X,https://doi.org/10.1016/j.jvcir.2021.103256,1047-3203,2021,103256,79,Journal of Visual Communication and Image Representation,Robust multimodal discrete hashing for cross-modal similarity search,article,FANG2021103256
"Video action recognition is an important topic in computer vision tasks. Most of the existing methods use CNN-based models, and multiple modalities of image features are captured from the videos, such as static frames, dynamic images, and optical flow features. However, these mainstream features contain much static information including object and background information, where the motion information of the action itself is not distinguished and strengthened. In this work, a new kind of motion feature is proposed without static information for video action recognition. We propose a quantization of motion network based on the bag-of-feature method to learn significant and discriminative motion features. In the learned feature map, the object and background information is filtered out, even if the background is moving in the video. Therefore, the motion feature is complementary to the static image feature and the static information in the dynamic image and optical flow. A multi-stream classifier is built with the proposed motion feature and other features, and the performance of action recognition is enhanced comparing to other state-of-the-art methods.","Motion feature, Bag of features, Dynamic image, Action recognition",Jianyu Yang and Yao Huang and Zhanpeng Shao and Chunping Liu,https://www.sciencedirect.com/science/article/pii/S1047320321001723,https://doi.org/10.1016/j.jvcir.2021.103263,1047-3203,2021,103263,79,Journal of Visual Communication and Image Representation,Learning discriminative motion feature for enhancing multi-modal action recognition,article,YANG2021103263
"The game of soccer involves an act of one team trying to score a goal against the other. During the game, defending players constantly try to predict the pass of the attacking player to prevent a goal. So, pass prediction is an important facet to anticipate the game strategy of participating teams. Here we present a probabilistic framework for pass prediction. Aberrating the state-of-the-art notion of mutually independent decision models, the proposed framework predicts pass recipients by integrating two dependent models, designed from the coordinates of the players in abstract top-view visualization. To evaluate the real time efficacy of the proposed pass prediction framework, a soccer data set has been introduced. The proposed pass prediction algorithm is compared against recent methods and the ground truth available in the soccer data set. The proposed method outperforms the existing approaches by a noticeable margin.","Pass prediction, Dependent models, Soccer dataset",Samriddha Sanyal,https://www.sciencedirect.com/science/article/pii/S1047320321001206,https://doi.org/10.1016/j.jvcir.2021.103190,1047-3203,2021,103190,78,Journal of Visual Communication and Image Representation,Who will receive the ball? Predicting pass recipient in soccer videos,article,SANYAL2021103190
"Image Quality Assessment (IQA) is one of the fundamental problems in the fields of image processing, image/video coding and transmission, and so on. In this paper, a Blind Image Quality Assessment (BIQA) approach with channel attention based deep Residual Network (ResNet)and extended LargeVis dimensionality reduction is proposed. Firstly, ResNet50 with channel attention mechanism is used as the backbone network to extract the deep features from the image. In order to reduce the dimensionality of the deep features, LargeVis, which is originally designed for the visualization of large scale high-dimensional data, is extended by using Support Vector Regression (SVR) to perform on a single feature vector data. The extended LargeVis can remove the redundant information of the deep features so as to obtain a low-dimensional and discriminative feature representation. Finally, the quality prediction model is established by using SVR as the fitting method. The low-dimensional feature representation and quality score of the image form the pair-wise data samples to train the fitting model. Experimental results on authentic distortions datasets and synthetic distortions datasets show that our proposed method can achieve superior performance compared with the state-of-the-art methods.","Blind image quality assessment, ResNet-50, Channel attention mechanism, LargeVis dimensionality reduction",Han Han and Li Zhuo and Jiafeng Li and Jing Zhang and Meng Wang,https://www.sciencedirect.com/science/article/pii/S1047320321001966,https://doi.org/10.1016/j.jvcir.2021.103296,1047-3203,2021,103296,80,Journal of Visual Communication and Image Representation,Blind image quality assessment with channel attention based deep residual network and extended LargeVis dimensionality reduction,article,HAN2021103296
"Humans tend to allocate attention to semantic entities. Objects are important in fixation selection, but not all the objects are equally attractive. In this paper, we introduce the concept of attribute bias to characterize the influence of semantic attributes compared with low-level saliency on fixation distribution. Two different ways are adopted to get two sets of semantic attributes. In both cases, most semantic attributes have a positive influence on drawing attention and contribute more than low-level saliency in object areas. We also find that attribute bias is robust to low-level saliency and can consistently reflect the relative attractiveness of objects with different semantic attributes. It is demonstrated that such bias helps make better fixation predictions by distinguishing the importance of objects, although low-level saliency models with better performance are less dramatically improved by attribute bias. These findings indicate the role of conceptual meaning as opposed to features in visual attention.","Visual attention, Image saliency, Semantic attributes, Object importance",Aoqi Li and Zhenzhong Chen,https://www.sciencedirect.com/science/article/pii/S1047320321001322,https://doi.org/10.1016/j.jvcir.2021.103206,1047-3203,2021,103206,79,Journal of Visual Communication and Image Representation,Semantic meaning modulates object importance in human fixation prediction,article,LI2021103206
"This paper presents a continuous stereo disparity estimation method based on superpixel segmentation and graph-cuts. We re-parameterize the disparity with a 3D tangent plane, and propose two algorithms to optimize the Markov Random Field (MRF) energy. The first algorithm, called superpixel Î±-expansion, is built on superpixel segmentation to localize the label proposal and the expansion scope. Three levels of superpixels with increasing granularity are generated for acceleration. The second algorithm, called normal adjustment, optimizes the 3D planes for the regions with low texture and/or illumination changes. The normal adjustment is performed along a depth-first similarity path of superpixels. We evaluate our method on the Middlebury 3.0 evaluation benchmark and the Eth3d benchmark. Experimental results show that our method achieves high accuracy on both evaluation benchmarks. (Middlebury 3.0 evaluation benchmark: http://vision.middlebury.edu/stereo/eval3/. Eth3d benchmark: https://www.eth3d.net/low_res_two_view.)","Stereo matching, Superpixel, Alpha-expansion, Graph cuts, Normal adjustment",Penglei Ji and Jie Li and Hanchao Li and Xinguo Liu,https://www.sciencedirect.com/science/article/pii/S1047320321001565,https://doi.org/10.1016/j.jvcir.2021.103238,1047-3203,2021,103238,79,Journal of Visual Communication and Image Representation,Superpixel alpha-expansion and normal adjustment for stereo matching,article,JI2021103238
"Human action analysis has been an active research area in computer vision, and has many useful applications such as human computer interaction. Most of the state-of-the-art approaches of human action analysis are data-driven and focus on general action recognition. In this paper, we aim to analyze fitness actions with skeleton sequences and propose an efficient and robust fitness action analysis framework. Firstly, fitness actions from 15 subjects are captured and built to a fitness action dataset (Fitness-28). Secondly, skeleton information is extracted and made alignment with a simplified human skeleton model. Thirdly, the aligned skeleton information is transformed to an uniform human center coordinate system with the proposed spatialâtemporal skeleton encoding method. Finally, the action classifier and localâglobal geometrical registration strategy are constructed to analyze the fitness actions. Experimental results demonstrate that our method can effectively assess fitness action, and have a good performance on artificial intelligence fitness system.","Computer vision, Action assessment, Image processing, Action recognition, Intelligent sports, Performance analysis",Jianwei Li and Qingrui Hu and Tianxiao Guo and Siqi Wang and Yanfei Shen,https://www.sciencedirect.com/science/article/pii/S1047320321002017,https://doi.org/10.1016/j.jvcir.2021.103304,1047-3203,2021,103304,80,Journal of Visual Communication and Image Representation,What and how well you exercised? An efficient analysis framework for fitness actions,article,LI2021103304
"Since online hashing has the advantages of low storage and fast calculation ,it attracts the attention of many scholars. However, the learning of new data streams separates the similarity between new data and existing data in many online hashing methods, which leads to poor retrieval performance. In addition, the similarity measure ignores the expression of different similarity. In this paper, we propose a novel supervised method, namely Label Projection Online Hashing for Balanced Similarity (LPOH). Compared with existing online hashing methods, LPOH aims to solve the problem of the effective establishment of the projection between the label vector and the binary code, and the successful realization of description of different similarity between the same labeled data. Specifically, LPOH overcomes the problem of similarity deviation caused by data imbalance via establishing a mapping matrix to derive a relationship between the data label vector and the binary code. Furthermore, the error between the binary code and the hash function concerning data streams is described. Extensive experiments on widely-used three benchmark datasets demonstrate that LPOH outperforms the state-of-the-art online hashing methods.","Online hashing, Label semantic, Image retrieval, Balanced similarity",Yuzhi Fang and Huaxiang Zhang and Li Liu,https://www.sciencedirect.com/science/article/pii/S104732032100208X,https://doi.org/10.1016/j.jvcir.2021.103314,1047-3203,2021,103314,80,Journal of Visual Communication and Image Representation,Label projection online hashing for balanced similarity,article,FANG2021103314
"Feature detection has great importance in many applications such as vision navigation. Examining the developed detectors, it is found in many recent studies that most of the scale-invariant detectors are sensitive to illumination. In this work, we propose a novel detector that has good robustness to both scale and illumination. Motivated by the good robustness of Log-Gabor kernels toward light changes, we employ these kernels as a basis to construct the scale space. To detect potential features, we develop an effective interest points measure which is motivated by the concept of the autocorrelation and Hessian matrices. To confirm the good performance of our detector, we hold experiments on many datasets and with comparisons to common state-of-the-art methods. Furthermore, we evaluate the saliency of the detected features on a UAV attitude estimation task.","Feature detection, Scale invariance, Illumination invariance, UAV attitude estimation",Achraf Djerida and Zhonghua Zhao and Jiankang Zhao,https://www.sciencedirect.com/science/article/pii/S1047320321001681,https://doi.org/10.1016/j.jvcir.2021.103258,1047-3203,2021,103258,79,Journal of Visual Communication and Image Representation,Development of scale and illumination invariant feature detector with application to UAV attitude estimation,article,DJERIDA2021103258
"CNN (Convolutional Neural Network) steganalyzers achieve enormous improvements in detecting stego images. However, they are easily deceived by adversarial steganography, which combines adversarial attack and steganography. Currently, there are two kinds of adversarial steganography, function separation and cover enhancement. ADV-EMB (ADVersarial EMBedding) is a typical function separation method. It forces the steganographic modifications along side the gradient directions of the target CNN steganalyzer on partial image elements. It results in relatively low deceiving success rate against the target model. ADS (ADversarial Steganography) is the first adversarial steganography, which is based on cover enhancement. It introduces much distortions, so it can be easily detected by non-target steganalyzers. To overcome such defects of the previous works, in this paper, we propose a novel cover enhancement method, denoted as SPS-ENH (SParSe ENHancement). Through sparse Â±1 adversarial perturbations, we effectively compress the distortions caused in cover enhancement. In addition, a re-trying scheme is introduced to further reduce the distortion scale. Extensive experiments show that the proposed method outperforms the previous works in the average classification error rates under non-target steganalyzers and deceiving success rates against target CNN models. When combining with the minâmax strategy, the proposed method converges in less iterations and provides higher security level than ADV-EMB.","Steganography, Adversarial example, Deep neural network",Chuan Qin and Weiming Zhang and Xiaoyi Dong and Hongyue Zha and Nenghai Yu,https://www.sciencedirect.com/science/article/pii/S1047320321002133,https://doi.org/10.1016/j.jvcir.2021.103325,1047-3203,2021,103325,80,Journal of Visual Communication and Image Representation,Adversarial steganography based on sparse cover enhancement,article,QIN2021103325
"Recently, reversible data hiding (RDH) has emerged into a new class of data hiding methods that enables exact retrieving of both embedded data and cover medium. In the present study, a novel automatic RDH method with contrast enhancement is proposed, in which the data is embedded through two-sided histogram expansion. Two-sided histogram shifting doubles the number of bits embedded at each iteration. Moreover, it preserves the mean brightness of the cover image and prevents it from over enhancement with less calculation. Experimental results on two sets of images show that the proposed method enhances the image contrast at an appropriate level without using a mean brightness controller during data embedding and provides higher information security compared to the existing RDH approaches.","Reversible data hiding, Reversible contrast enhancement, Two-sided histogram shifting, Embedding capacity",Saeideh Mansouri and Hossein {Khaleghi Bizaki} and Mohammad Fakhredanesh,https://www.sciencedirect.com/science/article/pii/S1047320321002376,https://doi.org/10.1016/j.jvcir.2021.103359,1047-3203,2021,103359,81,Journal of Visual Communication and Image Representation,Reversible data hiding with automatic contrast enhancement using two-sided histogram expansion,article,MANSOURI2021103359
"Despite the notable successes of Generative adversarial networks (GANs) achieved to date, applying them to real-world problems still poses significant challenges. In real traffic surveillance scenarios, for the task of generating images of multiple color of truck heads and cars without changing textures and license plates, conditional image generation hardly manipulate the generated images by the color attribute. Image style transfer methods inevitably produce color smearing. Even state-of-the-art methods of disentangled representation learning (e.g. MixNMatch) cannot disentangle colors individually, ensuring that irrelevant factors, such as texture remain the same. To solve this problem, we present an approach called Multi-ColorGAN based on memory-augmented networks for multi-color real vehicle coloring/generation with limited data. In particular, our model could filter out unwanted color changes in specific areas with a simple but effective method called Fusion Module, and generate more natural color images. Experiments on three vehicle image benchmarks and a new truck image dataset are conducted to evaluate the proposed Multi-ColorGAN compared to state-of-the-art.","Memory-augmented network, Generative adversarial network, Image recoloring, Vehicle recoloring, Few-shot learning",Wei Zhou and Si-Bao Chen and Li-Xiang Xu and Bin Luo,https://www.sciencedirect.com/science/article/pii/S1047320321002091,https://doi.org/10.1016/j.jvcir.2021.103317,1047-3203,2021,103317,80,Journal of Visual Communication and Image Representation,Multi-ColorGAN: Few-shot vehicle recoloring via memory-augmented networks,article,ZHOU2021103317
"Deep neural networks, including deep auto-encoder (DAE) and generative adversarial networks (GAN), have been extensively applied for visual anomaly detection. These models generally assume that reconstruction errors should be lower for normal samples but higher for anomalies. However, it has been found that DAE based models can sometimes reconstruct anomalies very well and thus result in false alarms or misdetections. To address this problem, we propose a model using GAN with locality-preferred recoding, named LRGAN. LRGAN is inspired by the observation that both normal and abnormal samples are not completely scattered throughout the latent space but clustered separately at some local regions. Therefore, a locality-preferred recoding (LR) module is designed to compulsively represent the latent vectors of anomalies by normal ones. As a result, reconstructions of anomalies will approximate to normal samples and corresponding residuals can thus be enlarged. To partly avoid latent vectors of normal samples being recoded, we further present an improved model using GAN with an adaptive LR (ALR) module, named LRGAN+. ALR applies the clustering algorithm to generate a more compact codebook; more importantly, it helps LRGANâ¯+â¯automatically skip the LR module for possible normal samples with a threshold strategy. Our proposed method is evaluated on two public datasets (i.e., MNIST and CIFAR-10) and one real-world industrial dataset (i.e., Fasteners), considering both one-class and multi-class anomaly detection protocols. Experimental results demonstrate that LRGAN is comparable with state-of-the-art methods and LRGANâ¯+â¯outperforms these methods on all datasets.","Visual anomaly detection, GAN, Locality, Recoding",Jianzhu Wang and Wei Huang and Shengchun Wang and Peng Dai and Qingyong Li,https://www.sciencedirect.com/science/article/pii/S1047320321001280,https://doi.org/10.1016/j.jvcir.2021.103201,1047-3203,2021,103201,79,Journal of Visual Communication and Image Representation,LRGAN: Visual anomaly detection using GAN with locality-preferred recoding,article,WANG2021103201
"Although deep learning makes major breakthroughs in object detection, object detection still faces several limitations listed as follows: (1) Many works underplay the feature selection, leading to the resulting key features are not prominent enough and prone to noise; (2) Many works pass back features in a layer-by-layer manner to achieve multi-scale features. However, as the distance of layers from each other increases, the semantics are diluted, and the transfer of information between layers becomes difficult. To overcome these problems, we propose a new Interconnected Feature Pyramid Networks (IFPN) for feature enhancement. It can simultaneously select attentive features through the attention mechanism and realize the free flow of information. On the basis of the improvements, we design a new IFPN Detector. Experiments on COCO dataset and Smart UVM dataset show that our method can bring a significant improvement.","Attention mechanism, Feature Pyramid Networks, Object detection, Deep learning",Qiang Wang and Lukuan Zhou and Yuncong Yao and Yong Wang and Jun Li and Wankou Yang,https://www.sciencedirect.com/science/article/pii/S104732032100170X,https://doi.org/10.1016/j.jvcir.2021.103260,1047-3203,2021,103260,79,Journal of Visual Communication and Image Representation,An Interconnected Feature Pyramid Networks for object detection,article,WANG2021103260
"Video-level sign language recognition is still a challenging task due to the influence of sign language-independent factors and timing requirements. This paper constructs a sign language recognition framework based on global-local feature description, and proposes a three-dimensional residual global network model with attention layer and a local network model based on target detection. The global feature description is based on the whole video behavior for time series modeling. The improved timing conversion layer is used to explore the timing information of different periods and learn the video representations of different timings. In the local module the hand is located through the target detection network to highlight its key role in the whole sign language behavior, which strengthens the category differences, and compensates the global network. Experiments on two well-known Chinese sign language datasets (SLR_Dataset and DEVSIGN_D) show that the proposed method can obtain higher recognition accuracy (respectively 89.2%, 91%) and better generalization performance.","Sign language recognition, 3D convolution network, Global-local attention, Time series modeling",Shujun Zhang and Qun Zhang,https://www.sciencedirect.com/science/article/pii/S1047320321001838,https://doi.org/10.1016/j.jvcir.2021.103280,1047-3203,2021,103280,80,Journal of Visual Communication and Image Representation,Sign language recognition based on global-local attention,article,ZHANG2021103280
"Inspired by instance segmentation algorithms, researchers have proposed quantity of segmentation-based methods for text detection, achieving remarkable results on scene text with arbitrary orientation and large aspect ratios. Following their success, we believe cascade architecture and extracting contextual information in multiple aspects are powerful to boost performance on the basis of segmentation-based methods, especially in decreasing false positive texts in complex natural scene. Based on such consideration, we propose a multiple-context-aware and cascade CNN structure, which appropriately encodes multiple categories of context information into a cascade R-CNN framework. Specifically, the proposed method consists of two stages, i.e., feature generation and cascade detection. During the first stage, we define ISTK (Isolated Selective Text Kernel) module to refine feature map, which sequentially encodes channel-wise and kernel-size attention information by designing multiple branches and different kernel sizes in isolate form. Afterwards, we build long-range spatial dependencies in feature map via non-local operations. Built on contextual feature map, Cascade Mask R-CNN structure progressively refines accurate boundaries of text instances with multi-stage framework. We conduct comparative experiments on ICDAR2015 and 2017-MLT datasets, where the proposed method outperform comparative methods in terms of effectiveness and efficiency measurements.","Cascade R-CNN, Deep representation learning, Applications to robust image recognition, Multiple attention encoding, Scene text detection, Multi-oriented text",Yirui Wu and Wenxiang Liu and Shaohua Wan,https://www.sciencedirect.com/science/article/pii/S1047320321001711,https://doi.org/10.1016/j.jvcir.2021.103261,1047-3203,2021,103261,80,Journal of Visual Communication and Image Representation,Multiple attention encoded cascade R-CNN for scene text detection,article,WU2021103261
"With the development of image colorization technique, the recolored images (RIs) become more and more authentic, making it very difficult to visually distinguish from natural images (NIs). Recently, researchers have proposed the detection methods towards recolored images. However, the current detection still has limitations such as poor generalization, large-scale training samples, high-dimensional features for training, and high computation cost. To address those issues, this paper proposes a novel method based on the lateral chromatic aberration (LCA) inconsistency and its statistical differences. Generally, RIs have fewer numbers of LCA characteristics than that of NIs, that inspire us to design the classifier for distinguishing two types of images. In particular, we propose to adopt very low 5-dimensional features to feed a classical SVM mechanism. The baseline ImageNet and Oxford datasets are used to verify the effectiveness of the proposed method, in which the performance of our proposed method rivals the prior arts.","Image forensics, Recolored images detection, Lateral chromatic aberration",Yangxin Yu and Ning Zheng and Tong Qiao and Ming Xu and Jiasheng Wu,https://www.sciencedirect.com/science/article/pii/S1047320321001954,https://doi.org/10.1016/j.jvcir.2021.103295,1047-3203,2021,103295,80,Journal of Visual Communication and Image Representation,Distinguishing between natural and recolored images via lateral chromatic aberration,article,YU2021103295
"With the advancement of the camera-related technology in mobile devices, the vast amount of photos have been taken and shared in our daily life. However, many users still have unsatisfactory experiences with low-visible photos, which are frequently acquired under complicated real-world environments. In this paper, a novel yet simple method for low-light image enhancement has been proposed without any learning procedure. The key idea of the proposed method is to estimate properties of the scene illumination both in global and local manner by exploiting the diffusion pyramid with residuals. Specifically, the residual of each scale level in the diffusion pyramid is combined with the corresponding input. This restored result efficiently highlights local details across different scale spaces, thus it is helpful for preserving the boundary of illuminations. By conducting max-pooling with restored results from different levels of the diffusion pyramid, which are resized to the original resolution, the illumination component is accurately inferred from a given image. Compared to recent learning-based approaches, one important advantage of the proposed method is to effectively avoid the overfitting problem to the specific training dataset. Experimental results on various benchmark datasets demonstrate the efficiency and robustness of the proposed method for low-light image enhancement in real-world scenarios.","Low-light image enhancement, Scene illumination, Diffusion pyramid with residuals",Wonjun Kim,https://www.sciencedirect.com/science/article/pii/S1047320321002406,https://doi.org/10.1016/j.jvcir.2021.103364,1047-3203,2021,103364,81,Journal of Visual Communication and Image Representation,Low-light image enhancement by diffusion pyramid with residuals,article,KIM2021103364
"The creation of manipulated multimedia content involving human characters has reached in the last years unprecedented realism, calling for automated techniques to expose synthetically generated faces in images and videos. This work explores the analysis of spatio-temporal texture dynamics of the video signal, with the goal of characterizing and distinguishing real and fake sequences. We propose to build a binary decision on the joint analysis of multiple temporal segments and, in contrast to previous approaches, to exploit the textural dynamics of both the spatial and temporal dimensions. This is achieved through the use of Local Derivative Patterns on Three Orthogonal Planes (LDP-TOP), a compact feature representation known to be an important asset for the detection of face spoofing attacks. Experimental analyses on state-of-the-art datasets of manipulated videos show the discriminative power of such descriptors in separating real and fake sequences, and also identifying the creation method used. Linear Support Vector Machines (SVMs) are used which, despite the lower complexity, yield comparable performance to previously proposed deep models for fake content detection.","Manipulated videos, Deepfakes, Video forensics, Local Derivative Patterns",Mattia Bonomi and Cecilia Pasquini and Giulia Boato,https://www.sciencedirect.com/science/article/pii/S1047320321001553,https://doi.org/10.1016/j.jvcir.2021.103239,1047-3203,2021,103239,79,Journal of Visual Communication and Image Representation,Dynamic texture analysis for detecting fake faces in video sequences,article,BONOMI2021103239
"The inconsistency caused by different factors, such as different camera imaging methods, complex imaging environments, and changes in light, present a huge challenge to person re-identification (re-ID). Unsupervised domain adaptation (UDA) can solve the inconsistency issue to a certain extent, but different datasets may not have any overlapping of peopleâs identities. Therefore, it is necessary to pay attention to peopleâs identities in solving domain-dissimilarity. A camera imaging style transformation with preserved self-similarity and domain-dissimilarity (CSPSD) is proposed to solve the cross-domain issue in person re-ID. First, CycleGAN is applied to determine the style conversion between source and target domains. Intra-domain identity constraints are used to maintain identity consistency between source and target domains during the image style transformation process. Maximum mean difference (MMD) is used to reduce the difference in feature distribution between source and target domains. Then, a one-to-n mapping method is proposed to achieve the mapping between positive pairs and distinguish negative pairs. Any sample image from the source domain and its transformed image or a transformed image with the same identity information compose a positive pair. The transformed image and any image from the target domain compose a negative pair. Next, a circle loss function is used to improve the learning speed of positive and negative pairs. Finally, the proposed CSPSD that can effectively reduce the difference between domains and an existing feature learning network work together to learn a person re-ID model. The proposed method is applied to three public datasets, Market-1501, DukeMTMC-reID, and MSMT17. The comparative experimental results confirm the proposed method can achieve highly competitive recognition accuracy in person re-ID.","Unsupervised domain adaptation, Person re-identification, Cross-domain, Circle loss, Maximum mean discrepancy",Zhiqin Zhu and Yaqin Luo and Sixin Chen and Guanqiu Qi and Neal Mazur and Chengyan Zhong and Qiwang Li,https://www.sciencedirect.com/science/article/pii/S1047320321002005,https://doi.org/10.1016/j.jvcir.2021.103303,1047-3203,2021,103303,80,Journal of Visual Communication and Image Representation,Camera style transformation with preserved self-similarity and domain-dissimilarity in unsupervised person re-identification,article,ZHU2021103303
"In a bio-imaging context, the main issues which obstruct the CS (Compressed sensing) application are image reconstruction time and computational cost. This paper presents an effective compressed sensing-based MRI reconstruction through a hybrid optimization algorithm. Initially, the preprocessing stage is performed using Cross guided bilateral filter. Then the K-space is generated by the Fourier transform. The hybrid Walsh Hadamard Transform and Discrete Wavelet Transform (HWHDWT) is utilized for the compressive sensing of the images. Finally, the Hybrid Galactic Swarm Optimization and Grey Wolf Optimization (HGSGWO) algorithm are developed for MRI reconstruction. The dataset collected from a hospital which contains MRI images both in JPEG and DICOM format. The performance of SSIM (Structural Similarity Index), PSNR (Peak Signal to Noise Ratio), MSE (mean square error) and reconstruction time are evaluated for images and it is compared with the existing methods.","Compressed sensing, Cross guided bilateral filter, Quasi random sampling, Hybridized Galactic Swarm Optimization and Grey Wolf Optimization, Hybridized Walsh Hadamard Transform and Discrete Wavelet Transform",Shrividya Guruprasad and S.H. Bharathi and D. {Anto Ramesh Delvi},https://www.sciencedirect.com/science/article/pii/S1047320321001796,https://doi.org/10.1016/j.jvcir.2021.103274,1047-3203,2021,103274,80,Journal of Visual Communication and Image Representation,Effective compressed sensing MRI reconstruction via hybrid GSGWO algorithm,article,GURUPRASAD2021103274
"Recently convolutional neural networks (CNNs) have been employed to address the problem of hand pose estimation. In this work, we introduce an end-to-end deep architecture that can accurately estimate hand pose through the joint use of model-based and fine-tuning methods. In the model-based stage, we make use of the prior information in hand model geometry to ensure the geometric validity of the estimated poses. Next, we introduce a fine-tuning approach that learns to refine the errors between the model and observed hand. Our approach is validated on three challenging public datasets and achieves state-of-the-art performance.","Hand pose estimation, CNN, Model-based, Fine-tuning",Lu Ding and Yong Wang and Robert LaganiÃ¨re and Dan Huang and Shan Fu,https://www.sciencedirect.com/science/article/pii/S1047320321001279,https://doi.org/10.1016/j.jvcir.2021.103200,1047-3203,2021,103200,79,Journal of Visual Communication and Image Representation,A CNN model for real time hand pose estimation,article,DING2021103200
"Video summary technology based on keyframe extraction is an effective means to rapidly access video content. Traditional video summary generation technology requires high video resolution, which poses a problem as most existing studies have no targeted solutions for videos that are subject to privacy protection. We propose a novel keyframe extraction algorithm for video data in the visual shielding domain, named visual shielding compressed sensing coding and double-layer affinity propagation (VSCS-DAP). VSCS-DAP involves three main steps. First, the video is compressed by compressed sensing technology to provide a visual shielding effect (protecting the privacy of monitored figures), while the data volume is significantly reduced. Then, pyramid histogram of oriented gradients (PHOG) features are extracted from the compressed video to be clustered by the first step affinity propagation (AP) to gain the summaries of the first stage. Finally, the PHOG and Hist fusion features are extracted from the keyframes of the first stage, and they cluster the fused PHOG-Hist features by the second step AP algorithm to obtain the final output summaries. Experimental results obtained on two common video datasets show that our method exhibits advantages including low redundancy and few missing frames, low computational complexity, strong real-time performance, and robustness to vision-shielded video.","Visual shielding, Video summary technology, Compressed sensing, Double-layer affinity propagation, Feature fusion",Jixin Liu and Dan Yu and Zheng Tang,https://www.sciencedirect.com/science/article/pii/S1047320321002121,https://doi.org/10.1016/j.jvcir.2021.103321,1047-3203,2021,103321,81,Journal of Visual Communication and Image Representation,Video summary generation by visual shielding compressed sensing coding and double-layer affinity propagation,article,LIU2021103321
"To form a high-performance video quality predictor, we developed a framework for full-reference (FR) video quality assessment that integrates Spatio-temporal slice analysis (STS) to create a high-performance predictor of video quality. However, both gradient and Gabor are spatialâtemporal structural capturers used for the simultaneous extraction of both spatial and temporal features. In this paper, we proposed a novel VQA algorithm via a joint model of gradient magnitude and Gabor features (JMG) between the STS images of the reference videos and their distorted counterparts to assess the degradation of video quality effectively. Firstly, gradient magnitude and the Gabor filter were constructed to extract the spatiotemporal features of the video sequence. However, the two-feature model combined to predict the perceptual quality of frames. This new proposed VQA model is known as the horizontal and time STS (HT-JMG) model. To further investigate the influence of spatial dissimilarity, we combined the frame-by-frame spatial T-JMG(S) factor with the HT-JMG and propose another VQA model, called the time, horizontal, and vertical STS (THV-JMG) model. Finally, the results of the experiment showed that the proposed method has a strong correlation with subjective perception and is competitive with state-of-the-art full reference VQA models.","Full-reference (FR) video quality assessment, Gradient magnitude, Gabor filter",Daniel {Oppong Bediako} and Xuanqin Mou,https://www.sciencedirect.com/science/article/pii/S1047320321001310,https://doi.org/10.1016/j.jvcir.2021.103204,1047-3203,2021,103204,79,Journal of Visual Communication and Image Representation,Joint model of gradient magnitude and Gabor features via Spatio-Temporal slice,article,OPPONGBEDIAKO2021103204
"This paper presents a moving object detection scheme that incorporates three innovations. First, considering the inter-frame consistency of pixels, we extend the compact binary face descriptor (CBFD) to the temporal domain and propose a novel local binary descriptor named temporally-consistent local compact binary descriptor (TC-LCBD), which exploits the useful correlation of the intensities of inter-frame pixels to guarantee good performance in complex scenes. We do this mainly because the background scene between frames has a significant coherence. Second, both color and TC-LCBD features are modeled as a group of adaptive histograms for characterizing each pixel, which can enhance the robustness to dynamic backgrounds and illumination changes. Third, by comparing changes in histogram proximity between two adjacent frames, we can dynamically adjust the model sensitivity and adaptation rate without user intervention. Experimental results on well-known, challenging data sets demonstrate that the proposed method significantly outperforms many state-of-the-art methods.","Moving object detection, Background subtraction, Local compact binary descriptor, Multi-feature histogram",Wei He and Wujing Li and Guoyun Zhang and Bing Tu and Yong {Kwan Kim} and Jianhui Wu and Qi Qi,https://www.sciencedirect.com/science/article/pii/S1047320321001826,https://doi.org/10.1016/j.jvcir.2021.103278,1047-3203,2021,103278,80,Journal of Visual Communication and Image Representation,Detection of moving objects using adaptive multi-feature histograms,article,HE2021103278
"Applications on the cloud server have matured, and protecting the privacy of the content owner has attracted more attention. Privacy-Preserving Reversible data hiding (PP-RDH) is an efficient technique for embedding additional data into an encrypted image. In this paper, we propose a privacy-preserving reversible data hiding scheme using the quad-tree partition and Integer Wavelet Transform (IWT) techniques. Our scheme focuses on improving the embedding rate and quality of the recovered image when a 2Â ÃÂ 2-sized, block-based image encryption method is applied to ensure relative higher security. On this basis, the IWT technique transforms the encrypted image, and coefficients in three high frequency subbands are converted into 8-bit binary system. Then, the quad-tree partition technique encodes each 8Â ÃÂ 8-sized coefficient block, since there are many zeroes in the front bit planes. The experimental results indicated that our proposed scheme significantly improved the embedding rate, and guaranteed lossless image recovery and data extraction.","Reversible data hiding, Encrypted images, Quad-tree partition, IWT",Xu Wang and Ching-Chun Chang and Chia-Chen Lin and Chin-Chen Chang,https://www.sciencedirect.com/science/article/pii/S1047320321001309,https://doi.org/10.1016/j.jvcir.2021.103203,1047-3203,2021,103203,79,Journal of Visual Communication and Image Representation,Privacy-preserving reversible data hiding based on quad-tree block encoding and integer wavelet transform,article,WANG2021103203
"Detecting anomalous activity in video surveillance often suffers from limited availability of training data. Transfer learning may close this gap, allowing to use existing annotated data from some source domain. However, analyzing the source feature space in terms of its potential for transfer of learning to another context is still to be investigated. This paper reports a study on video anomaly detection, focusing on the analysis of feature embeddings of pre-trained CNNs with the use of novel cross-domain generalization measures that allow to study how source features generalize for different target video domains. This generalization analysis represents not only a theoretical approach, can be useful in practice as a path to understand which datasets allow better transfer of knowledge. Our results confirm this, achieving better anomaly detectors for video frames and allowing analysis of transfer learningâs positive and negative aspects.","Video, Transfer learning, Feature generalization, Anomaly detection",Fernando P. {dos Santos} and Leonardo S.F. Ribeiro and Moacir A. Ponti,https://www.sciencedirect.com/science/article/pii/S1047320319300926,https://doi.org/10.1016/j.jvcir.2019.02.035,1047-3203,2019,407--416,60,Journal of Visual Communication and Image Representation,Generalization of feature embeddings transferred from different video anomaly detection domains,article,DOSSANTOS2019407
"Online payment is becoming popular due to the development of e-commerce. So payment safety is more important. Since there is a fraudulent situation, an anti-fraud system is indispensable. GMM was leveraged in many anti-fraud applications, but it only takes positive sample into account. Convolutional neural network is a strong strategy for learning deep representation of samples. So in this paper, we propose a CNNs architecture to deal with this problem. And the distance metric method can effectively identify whether candidates are the same person. Experimental results show the effectiveness of our method.","Convolutional neural network, Anti-fraud, Distance metric",Bing Tu and Danbing He and Yongheng Shang and Chengle Zhou and Wujing Li,https://www.sciencedirect.com/science/article/pii/S1047320319300409,https://doi.org/10.1016/j.jvcir.2019.01.031,1047-3203,2019,253--256,59,Journal of Visual Communication and Image Representation,Deep feature representation for anti-fraud system,article,TU2019253
"Fine-grained categorization is challenging due to its small inter-class and large intra-class variance. Moreover, requiring domain expertise makes fine-grained labelled data much more expensive to acquire. Existing models predominantly require extra information such as bounding box and part annotation in addition to the image category labels, which involves heavy manual labor. In this paper, we propose a novel hierarchical deep transfer learning model, based on a compression convolutional neural network. Our model transfers the learned image representation from large-scale labelled fine-grained datasets to micro fine-grained datasets, which avoids using expensive annotations and realizes visual categorization task effectively. Firstly, we introduce a cohesion domain to measure correlation degree between source domain and target domain. Secondly, the source-domain convolutional neural network is adjusted according to its metrical feedback, in order to select task-specific features that are suitable for transferring to the target domain. Finally, we make most of perspective-class labels, which are inherent attributes of fine-grained data for multi-task learning and learn all the attributes through joint learning to extract more discriminative representations. The proposed model not only economizes training time effectively and achieves high categorization accuracy, but also verifies that the inter-domain feature transition can accelerate learning and optimization.","Fine-grained categorization, Convolutional neural network, Transfer learning, Multi-task learning, Model compression",Ronggui Wang and Xuchen Yao and Juan Yang and Lixia Xue and Min Hu,https://www.sciencedirect.com/science/article/pii/S1047320319301579,https://doi.org/10.1016/j.jvcir.2019.05.002,1047-3203,2019,129--139,62,Journal of Visual Communication and Image Representation,Hierarchical deep transfer learning for fine-grained categorization on micro datasets,article,WANG2019129
"This study analyzes the effectiveness of various loss functions on performance improvement for Single Image Super-Resolution (SISR) using Convolutional Neural Network (CNN) models by surrogating the reconstructive map between Low Resolution (LR) and High Resolution (HR) images with convolutional filters. In total, eight loss functions are separately incorporated with Adam optimizer. Through experimental evaluations on different datasets, it is observed that some parametric and non-parametric robust loss functions promise impressive accuracies whereas remaining ones are sensitive to noise that misleads the learning process and consequently resulting in lower quality HR outcomes. Eventually, it turns out that the use of either Difference of Structural Similarity (DSSIM), Charbonnier or L1 loss functions within the optimization mechanism would be a proper choice, by considering their excellent reconstruction results. Among them, Charbonnier and L1 loss functions are fastest ones when the computational time cost is examined during training stage.","Super-resolution, Convolutional neural networks, Loss functions",Yildiray Anagun and Sahin Isik and Erol Seke,https://www.sciencedirect.com/science/article/pii/S1047320319301336,https://doi.org/10.1016/j.jvcir.2019.03.027,1047-3203,2019,178--187,61,Journal of Visual Communication and Image Representation,SRLibrary: Comparing different loss functions for super-resolution over various convolutional architectures,article,ANAGUN2019178
"Aurora spectral data, the particular hyperspectral data of auroral spectra, have an irreplaceable research value in bridging the gap between solar activity and terrestrial evolution. Their requirement for high-volume storage and real-time transmission had been a great challenge until we proposed a CPU-paralleled and online-biprediction-based method. As it is no longer applicable because of the recent spectrograph reassembling, this paper presents a replacement strategy combines the unidirectional predictor with the entropy coder of the other dimension, as well as with using smoothing and outlier recognition. The hybrid encoders of Spat-SPCC and Spec-SPCC are developed distinguished by their respective prediction direction. Spat-SPCC tuned on one-day trial data is used for its better capability for compression, in the further comparison with various classical algorithms, it achieves the top-ranking compression performance and has the average processing time of 1 s per file so that its availability for the practical applications is validated.","Lossless compression, Aurora spectral data, Hyperspectral data, Prediction, Range coding, Outlier recognition",Wanqiu Kong and Jiaji Wu and Zejun Hu and Gwanggil Jeon,https://www.sciencedirect.com/science/article/pii/S1047320319301610,https://doi.org/10.1016/j.jvcir.2019.05.006,1047-3203,2019,174--181,62,Journal of Visual Communication and Image Representation,Lossless compression codec of aurora spectral data using hybrid spatial-spectral decorrelation with outlier recognition,article,KONG2019174
"The emergence of cost-effective depth sensors opens up a new dimension for RGB-D based human action recognition. In this paper, we propose a collaborative multimodal feature learning (CMFL) model for human action recognition from RGB-D sequences. Specifically, we propose a robust spatio-temporal pyramid feature (RSTPF) to capture dynamic local patterns around each human joint. The proposed CMFL model fuses multimodal data (skeleton, depth and RGB), and learns action classifiers using the fused features. The original low-level feature matrices are factorized to learn shared features and modality-specific features under a supervised fashion. The shared features describe the common structures among the three modalities while the modality-specific features capture intrinsic information of each modality. We formulate shared-specific features mining and action classifiers learning in a unified max-margin framework, and solve the formulation using an iterative optimization algorithm. Experimental results on four action datasets demonstrate the efficacy of the proposed method.","RGB-D action recognition, Multimodal data, Max-margin learning framework, Supervised matrix factorization",Jun Kong and Tianshan Liu and Min Jiang,https://www.sciencedirect.com/science/article/pii/S104732031930063X,https://doi.org/10.1016/j.jvcir.2019.02.013,1047-3203,2019,537--549,59,Journal of Visual Communication and Image Representation,Collaborative multimodal feature learning for RGB-D action recognition,article,KONG2019537
"How to correctly recognize and extract named entities, such as disease names, medical measurements and therapies, from online medical diagnosis data remains challenging. For the one hand, conventional natural language processing methods cannot be directly applied in the field of online medical diagnosis (OMD). Although existing supervised or unsupervised learning algorithms have offered strategies for OMD on open websites, such methods might extensively rely on specific knowledge sources or manually designed features. For the other hand, due to the large size of the data and the sophistication of the data structure, it is difficult to establish a robust NLP model for recognition and extraction of clinical named entities in paragraphs. Therefore, in the paper, we try to establish a new deep neural network (DNN), combine the bi-directional long short-term memory (Bi-LSTM) and the conditional random field (CRF), and utilize online medical diagnosis data for recognition and extraction of clinical named entities. Different from existing artificial neural networks (ANN), the proposed neural network well functions even without manual rules or features. The word representations input into the DNN is connected by character-based representations and continuous bags of word clusters (CBOWC) in an embedding way. We test the new DNN on different online medical diagnosis datasets obtained from a scalable web crawler, and compare it with the long short-term memory (LSTM) neural network linguistic model, the convolutional neural network (CNN) model and the most advanced Bi-LSTM-CRF. The fundamentals for the method selection is that they have been suggested to generate acceptable results. According to the comparison analyses, the DNN is proved to be a reliable tool and improves every benchmark performance of OMD.","Continuous bags of word clusters, Deep neural network, Online medical diagnosis data, Named entities",Xin Liu and Yanju Zhou and Zongrun Wang,https://www.sciencedirect.com/science/article/pii/S1047320319300525,https://doi.org/10.1016/j.jvcir.2019.02.001,1047-3203,2019,1--15,60,Journal of Visual Communication and Image Representation,Recognition and extraction of named entities in online medical diagnosis data based on a deep neural network,article,LIU20191
"Object counting is a challenging task in computer vision. In this paper, we propose an object counting network based on hierarchical context and feature fusion called HFNet. HFNet comprises a hierarchical context extraction module and an end-to-end convolution neural network. The hierarchical context extraction module extracts hierarchical features to the main network as context cues, aiming to provide more information to improve counting performance. The main network adds the relatively lower but naturally high-resolution feature maps into higher but semantic feature maps, whose benefits are: one is to reduce the risk of losing detailed information during multi-convolutions; the other is to against the scale variations in this task due to the fusion operation of the multi-scale feature maps. Experiments demonstrate HFNet achieves competitive results on crowd counting including UCF_CC_50 dataset and ShanghaiTech dataset and on vehicle counting including TRANCOS dataset. The contrast experiments also verify the structure rationality of HFNet.","Object counting, Density estimation, Feature fusion, Hierarchical context",Shihui Zhang and He Li and Weihang Kong and Lei Wang and Xiaofang Niu,https://www.sciencedirect.com/science/article/pii/S1047320319301580,https://doi.org/10.1016/j.jvcir.2019.05.003,1047-3203,2019,166--173,62,Journal of Visual Communication and Image Representation,An object counting network based on hierarchical context and feature fusion,article,ZHANG2019166
"With the continuous development of urbanization, urban population, economy and other factors have a close impact on the geometry and distribution of urban buildings. Obtaining information of urban buildings from aerial images or satellite images quickly and accurately is not only conducive to updating geospatial data, but also of great significance for effective monitoring of new thematic information such as new buildings. Moreover, in recent years, the research and improvement of building recognition and contour extraction algorithms based on satellite images or aerial images are helpful to the recognition and classification of urban buildings. It is of great significance to the acquisition of GIS data, the understanding of images, large-scale mapping and many other applications of remote sensing data. With the development of artificial intelligence and computer technology, the image processing of intelligent building environment based on pattern recognition technology has become an important research direction in the field of intelligent building image recognition. Based on the concept, principle and technology analysis of pattern recognition technology, this paper studies the application of pattern recognition technology in the image processing of intelligent building environment. In this paper, based on image processing of intelligent building as the basic theoretical platform, with the pattern recognition technology as the basic research means, three problems of image processing, image extraction and image recognition in image processing of building intelligent environment are studied respectively, and corresponding reasonable solutions are put forward.","Intelligent building, Image processing, Satellite imagery, Building contours, Edge detection",Wei Cai and Xiaodong Wen and Qiu Tu and Xiujuan Guo,https://www.sciencedirect.com/science/article/pii/S1047320319301063,https://doi.org/10.1016/j.jvcir.2019.03.014,1047-3203,2019,141--148,61,Journal of Visual Communication and Image Representation,Research on image processing of intelligent building environment based on pattern recognition technology,article,CAI2019141
"Because the image measurement technology based on machine vision has the advantages of high accuracy, high efficiency and non-contact measurement, this kind of measurement technology has gradually become the focus of attention in industrial production measurement and detection. Based on the analysis of image measurement technology, this paper studies the measurement method of mechanical parts size based on image recognition and improves related algorithms. Specific research work is as follows: Design a measurement method of machine parts size based on image recognition, study and analyze the formation of noise, types and corresponding denoising technology, select a fast median filtering algorithm to achieve filtering. Polynomial interpolation is applied to the sub-pixel edge location method to extract the edges accurately. Some classical operators are studied and analyzed with the specific part image to be tested as the experimental object. Several classical operators are compared and analyzed through many experiments. Experiments show that the improved morphological gradient operator can effectively refine the image edge. The experimental scheme proposed in this paper can better realize the measurement of mechanical parts size, and the improved algorithm has significantly improved the accuracy than before.","Image recognition, Machine vision, Size measurement, Mechanical parts",Laigang Zhang and Qili Yang and Qun Sun and Deying Feng and Ying Zhao,https://www.sciencedirect.com/science/article/pii/S1047320319300410,https://doi.org/10.1016/j.jvcir.2019.01.035,1047-3203,2019,425--432,59,Journal of Visual Communication and Image Representation,Research on the size of mechanical parts based on image recognition,article,ZHANG2019425
"Real-world video surveillance has increasing demand for person re-identification. Existing multi-shot works usually aggregate single sample features by computing the average features or using time series model. The Multi-image Joint Re-ranking framework with updateable Image Pool that we are proposing will give a different approach. First, we defined the term âImage Poolâ to store image samples for each pedestrian. Next, the updating rules of Image Pool has been defined in order to optimize the representativeness of it. Second, we compute initial ranking lists of every sample in Image Pool, and propose the âMultiple-image Joint Re-rankingâ algorithm to aggregate initial ranking lists. We calculate the rank score of partial elements of initial ranking lists. In the end, we get final ranking list by ascending the order of the rank scores. We validated our re-ranking results on Market-1501, iLIDS-VID, PRID-2011 and our ITSD datasets, and the results outperform other methods.","Person re-identification, Image pool, Multi-shot, Re-ranking",Mingyue Yuan and Dong Yin and Jinwen Ding and Zhipeng Zhou and Chengfeng Zhu and Rui Zhang and An Wang,https://www.sciencedirect.com/science/article/pii/S1047320319300495,https://doi.org/10.1016/j.jvcir.2019.01.041,1047-3203,2019,527--536,59,Journal of Visual Communication and Image Representation,A multi-image Joint Re-ranking framework with updateable Image Pool for person re-identification,article,YUAN2019527
"Although steganalysis has developed rapidly in recent years, it still faces many difficulties and challenges. Based on the theory of in-depth learning method and image-based general steganalysis, this paper makes a deep study of the hot and difficult problem of steganalysis feature expression, and tries to establish a new steganalysis paradigm from the idea of feature learning. The main contributions of this paper are as follows: 1. An innovative steganalysis paradigm based on in-depth learning is proposed. Based on the representative deep learning method CNN, the model is designed and adjusted according to the characteristics of steganalysis, which makes the proposed model more effective in capturing the statistical characteristics such as neighborhood correlation. 2. A steganalysis feature learning method based on global information constraints is proposed. Based on the previous research of steganalysis method based on CNN, this work focuses on the importance of global information in steganalysis feature expression. 3. A feature learning method for low embedding rate steganalysis is proposed. 4. A general steganalysis method for multi-class steganography is proposed. The ultimate goal of general steganalysis is to construct steganalysis detectors without distinguishing specific types of steganalysis algorithms.","Steganalysis, Steganography, Feature learning, Deep learning, Convolutional neural network, Transfer learning, Multitask learning",Ying Zou and Ge Zhang and Leian Liu,https://www.sciencedirect.com/science/article/pii/S1047320319300914,https://doi.org/10.1016/j.jvcir.2019.02.034,1047-3203,2019,266--275,60,Journal of Visual Communication and Image Representation,Research on image steganography analysis based on deep learning,article,ZOU2019266
"Moving objects may have various kinds of topological events when they move relative to a directed line, such as moving in/out, folding back and stopping. This research develops a trajectory-directed line query mode using sketch-based retrieval in order to realize the query of topological relationship between a trajectory and a directed line. The research also proposes a quantitative similarity calculation method for trajectory-directed line. The sketched stroke-directed line is used as input query condition and is described as a sequence of key points for semantic association. The trajectory-directed line to be queried is also described as a sequence of key points. The time sequence distance metric is used to calculate the similarity between them. For the trajectory-directed line with different number of key points, the similarity calculation is carried out using two kinds of distance matching methods. The results show that the method is simple and feasible, and can realize the quantitative query of trajectory data.","Trajectory, Directed line, Topological relationship, Key point, Similarity",Yue Zhang and Yaping Lin,https://www.sciencedirect.com/science/article/pii/S104732031930046X,https://doi.org/10.1016/j.jvcir.2019.01.040,1047-3203,2019,448--454,59,Journal of Visual Communication and Image Representation,Quantitative similarity calculation method for trajectory-directed line using sketch retrieval,article,ZHANG2019448
"The number of patients with Barretâs esophagus (BE) has increased in the last decades. Considering the dangerousness of the disease and its evolution to adenocarcinoma, an early diagnosis of BE may provide a high probability of cancer remission. However, limitations regarding traditional methods of detection and management of BE demand alternative solutions. As such, computer-aided tools have been recently used to assist in this problem, but the challenge still persists. To manage the problem, we introduce the infinity Restricted Boltzmann Machines (iRBMs) to the task of automatic identification of Barrettâs esophagus from endoscopic images of the lower esophagus. Moreover, since iRBM requires a proper selection of its meta-parameters, we also present a discriminative iRBM fine-tuning using six meta-heuristic optimization techniques. We showed that iRBMs are suitable for the context since it provides competitive results, as well as the meta-heuristic techniques showed to be appropriate for such task.","Barrettâs esophagus, Infinity Restricted Boltzmann Machines, Meta-heuristics, Deep learning",Leandro A. Passos and Luis A. {de Souza Jr.} and Robert Mendel and Alanna Ebigbo and Andreas Probst and Helmut Messmann and Christoph Palm and JoÃ£o Paulo Papa,https://www.sciencedirect.com/science/article/pii/S1047320319300513,https://doi.org/10.1016/j.jvcir.2019.01.043,1047-3203,2019,475--485,59,Journal of Visual Communication and Image Representation,Barrettâs esophagus analysis using infinity Restricted Boltzmann Machines,article,PASSOS2019475
"Gender classification aims at recognizing a personâs gender. Despite the high accuracy achieved by state-of-the-art methods for this task, there is still room for improvement in generalized and unrestricted datasets. In this paper, we advocate a new strategy inspired by the behavior of humans in gender recognition. Instead of dealing with the face image as a sole feature, we rely on the combination of isolated facial components and a contextual feature which we call the foggy face. Then, we use these features to train deep convolutional neural networks followed by an AdaBoost-based score fusion to infer the final gender class. We evaluate our method on four challenging datasets to demonstrate its efficacy in achieving better or on-par accuracy with state-of-the-art methods. In addition, we present a new face dataset that intensifies the challenges of occluded faces and illumination changes, which we believe to be a much-needed resource for gender classification research.","Gender classification, Deep convolutional neural networks, Face image dataset",Mahmoud Afifi and Abdelrahman Abdelhamed,https://www.sciencedirect.com/science/article/pii/S1047320319301567,https://doi.org/10.1016/j.jvcir.2019.05.001,1047-3203,2019,77--86,62,Journal of Visual Communication and Image Representation,AFIF4: Deep gender classification based on AdaBoost-based fusion of isolated facial features and foggy faces,article,AFIFI201977
"Recently, Yang et al. introduced a (k,n) approach called Reversible Absolute moment block truncation coding Visual Cryptography Scheme (RAVCS) (Yang et al., 2017) to conceal a secret image into n AMBTC shares. However, a large number of reference images is used in their method. To reduce the number of reference images, a (k,n) PRAVCS using one AMBTC reference image is introduced. In encoding phase, a binary secret image is shared into n AMBTC shadow images according to the base matrices generated by two proposed constructions. In decoding phase, the secret image is recovered by stacking sufficient bitmaps, and the AMBTC reference image is partially recovered as well. When n AMBTC shares are used, losslessly reconstruction for the reference image is obtained. Theoretical analysis and experiments by the proposed method are demonstrated to show the effectiveness. Moreover, Construction 1 has larger contrast, while Construction 2 achieves higher reversibility for more thresholds.","Visual cryptography, Block truncation coding, Partial reversible, Meaningful share, Contrast, Reference image",Xiaotian Wu and Dong Chen and Ching-Nung Yang and Yi-Yun Yang,https://www.sciencedirect.com/science/article/pii/S1047320319300598,https://doi.org/10.1016/j.jvcir.2019.02.008,1047-3203,2019,550--562,59,Journal of Visual Communication and Image Representation,"A (k,n) threshold partial reversible AMBTC-based visual cryptography using one reference image",article,WU2019550
"Automatic age estimation from face images is a topic of growing interest nowadays, because of its great value in various applications. The main challenge in automatic facial age estimation task comes from the large intra-class facial appearance variations due to both gender and race attributes. To this end, in this paper we propose a complete approach for age estimation based on demographic classification. The proposed approach consists of three main parts: (1) Automatic face detection and alignment to extract only the regions of interest. (2) Feature extraction from facial region images using Multi-level face representation. (3) Two-Stages age Estimation (TSE). The main idea of TSE is to classify the input face image into one of demographic classes, then estimate age within the identified demographic class. The experimental results demonstrate that our proposed approach can offer better performance for age estimation when compared to the state-of-the-art methods on MORPH-II, PAL and a subset of LFW databases.","Age estimation, Demographic classification, Feature extraction, SVR, SVM",Mohammed-En-nadhir Zighem and Abdelkrim Ouafi and Athmane Zitouni and Yassine Ruichek and Abdelmalik Taleb-Ahmed,https://www.sciencedirect.com/science/article/pii/S1047320319301324,https://doi.org/10.1016/j.jvcir.2019.03.025,1047-3203,2019,236--249,61,Journal of Visual Communication and Image Representation,Two-stages based facial demographic attributes combination for age estimation,article,ZIGHEM2019236
"One of the most efficient descriptions of image structure, which has been widely used in image quality assessment (IQA) studies, is the three-components model. Based on this model, the major structural components of an image are edges, textures and flat regions. We found that this model is basically derived from the abstract concept of image region smoothness. Indeed, each of these three components, is a particular region with special smoothness characteristics. Inspired by this fact, we developed an efficient general-purpose full-reference IQA technique, in which the amount of region smoothness degradation is gauged using our efficient MSER (maximally stable extremal region)-based region smoothness measure. For this, we build a block-based smoothness similarity map, and extract the image quality score, using a percentile averaging scheme. Experimental results are provided on popular benchmark databases, which confirm that the proposed approach has a reasonable prediction performance compared to the state-of-the-art image quality metrics.","Image quality assessment, Full reference, Image region smoothness, Maximally stable extremal region, Percentile averaging",Mohammad Hossein Khosravi and Hamid Hassanpour,https://www.sciencedirect.com/science/article/pii/S1047320318302906,https://doi.org/10.1016/j.jvcir.2018.11.019,1047-3203,2019,217--228,60,Journal of Visual Communication and Image Representation,Image quality assessment using a novel region smoothness measure,article,KHOSRAVI2019217
"In view of the situation that a large number of Thangka images are missing part of color information because of time and environmental factors, the existing image evaluation methods are inconsistent with the result of subjective evaluation. This paper aims at evaluating the damaged Thangka color image, and proposes a new method of image quality evaluation based on superpixel and color entropy. In this algorithm, we use the uniformity of Thangka color image to extract color feature based on CIE 1976 L* a* b* (CIELAB) color space and superpixel. Therefore, the loss of color information in the complex area of Thangka images is well handled. The color entropy is used to quantify the color distribution and structure characteristics of each superpixel, and then we can get the preliminarily evaluation score. In the end, large amounts of data are obtained through some operations such as image deformation and rotating by the Generative Adversarial Nets (GANs), which makes the final evaluation score more reliable. Experimental results show that this method can obtain a good consistency with the subjective results, and Spearman rank order the correlation coefficient (SROCC) and Pearson linear correlation coefficient (PLCC) of the new method already exceed 0.9.","Image quality, No reference assessment, Superpixel, Information entropy, Thangka image",Wenjin Hu and Yuqi Ye and Jiahao Meng and Fuliang Zeng,https://www.sciencedirect.com/science/article/pii/S1047320319300483,https://doi.org/10.1016/j.jvcir.2019.01.039,1047-3203,2019,407--414,59,Journal of Visual Communication and Image Representation,No reference quality assessment for Thangka color image based on superpixel,article,HU2019407
"Facial expressions are complex, progress over time, and are challenging to interpret. The research subject of automated emotion recognition associated with the facial expressions has been a mainstream topic in computer vision focused on image processing and pattern recognition. Numerous databases of facial expressions are available to the research community, and are used as fundamental tools for the evaluation of a wide range of algorithms for face expression recognition. In this paper, we assessed the existing collections of facial expression datasets as a basis for building and evaluating the largely unmapped facet of pain expressions. To accentuate this topic, the study provides the summary of different characteristics of expressions that are relevant and justifiable indicators of pain. A preliminary platform is tested with accuracy rate of 85.66% using the collected FER datasets as testing inputs. Common challenges in face recognition were discussed, as well as the different methods used to address them. Different variants of feature learning techniques were also compared, and why some methods outperforms other existing methods.","Facial expression recognition, Emotion database, Human pain detection, Feature learning",Reneiro {Andal Virrey} and Chandratilak {De Silva Liyanage} and Mohammad {Iskandar bin Pg Hj Petra} and Pg {Emeroylariffion Abas},https://www.sciencedirect.com/science/article/pii/S1047320319301294,https://doi.org/10.1016/j.jvcir.2019.03.023,1047-3203,2019,209--217,61,Journal of Visual Communication and Image Representation,Visual data of facial expressions for automatic pain detection,article,ANDALVIRREY2019209
"The model parameters of convolutional neural networks (CNNs) are determined by backpropagation (BP). In this work, we propose an interpretable feedforward (FF) design without any BP. The FF design adopts a data-centric approach. It derives network parameters of the current layer based on data statistics from the output of the previous layer in a one-pass manner. To construct convolutional layers, we develop a new signal transform, called the Saab (Subspace approximation with adjusted bias) transform. It is a variant of the principal component analysis with an added bias vector to annihilate activationâs nonlinearity. Multiple Saab transforms in cascade yield multiple convolutional layers. As to fully-connected layers, we construct them using a cascade of multi-stage linear least squared regressors. The classification and robustness performances of BP- and FF-designed CNNs applied to the MNIST and the CIFAR-10 datasets are compared. Finally, we comment on the relationship between BP and FF designs.","Interpretable machine learning, Convolutional neural networks, Principal component analysis, Linear least-squared regression, Cross entropy, Dimension reduction",C.-C. Jay Kuo and Min Zhang and Siyang Li and Jiali Duan and Yueru Chen,https://www.sciencedirect.com/science/article/pii/S104732031930104X,https://doi.org/10.1016/j.jvcir.2019.03.010,1047-3203,2019,346--359,60,Journal of Visual Communication and Image Representation,Interpretable convolutional neural networks via feedforward design,article,KUO2019346
"Conventionally, classifiers designed for face liveness detection are trained on real-world images, where real-face images and corresponding face presentation attacks (PA) are very much overlapped. However, a little research has been carried out in utilization of the combination of real-world face images and face images generated by deep convolutional neural networks (CNN) for face liveness detection. In this paper, we evaluate the adaptive fusion of convolutional-features learned by convolutional layers from real-world face images and deep CNN generated face images for face liveness detection. Additionally, we propose an adaptive convolutional-features fusion layer that adaptively balance the fusion of convolutional-features of real-world face images and face images generated by deep CNN during training. Our extensive experiments on the state-of-the-art face anti-spoofing databases, i.e., CASIA, OULU and Replay-Attack face anti-spoofing databases with both intra-database and cross-database scenarios indicate promising performance of the proposed method on face liveness detection compared to state-of-the-art methods.","Convolution neural networks, Face anti-spoofing, Face liveness detection, Adaptive fusion, Auto-encoder, DNG face images",Yasar Abbas Ur Rehman and Lai-Man Po and Mengyang Liu and Zijie Zou and Weifeng Ou and Yuzhi Zhao,https://www.sciencedirect.com/science/article/pii/S1047320319300641,https://doi.org/10.1016/j.jvcir.2019.02.014,1047-3203,2019,574--582,59,Journal of Visual Communication and Image Representation,Face liveness detection using convolutional-features fusion of real and deep network generated face images,article,REHMAN2019574
"A novel image interpolation methodology is proposed in this paper, called the predictor-corrector interpolation (PCI). Given a low-resolution (LR) image, our PCI scheme begins with the prediction stage, aiming to interpolate the LR-sized input image to a high-resolution (HR) image which is of the same size as the final interpolated image. In the subsequent correction stage, those salient pixels (e.g., edge pixels) of the predicted image are identified and then necessary corrections are made to them for further improving the image quality. To demonstrate the effectiveness of this PCI methodology, the sparse mixing estimator (SME) interpolation is selected as the predictor, and a modified version of the contrast-guided interpolation (CGI) is developed and exploited as the corrector. Hence, the proposed PCI algorithm is denoted as PCI(sme,hr-cgi), which shows a superior performance over a number of comparable state-of-the-art image interpolation algorithms in both objective and subjective image quality assessment.","Image interpolation, Predictor-corrector, Edge-guided, Contrast-guided, Directional filtering",Baojiang Zhong and Kai-Kuang Ma and Zhifang Lu,https://www.sciencedirect.com/science/article/pii/S1047320319301178,https://doi.org/10.1016/j.jvcir.2019.03.018,1047-3203,2019,50--60,61,Journal of Visual Communication and Image Representation,Predictor-corrector image interpolation,article,ZHONG201950
"This paper introduces computational tools for cell classification into normal and abnormal, as well as content-based-image-retrieval (CBIR) for cell recommendation. It also proposes the radial feature descriptors (RFD), which define evenly interspaced segments around the nucleus, and proportional to the convexity of the nuclear boundary. Experiments consider Herlev and CRIC image databases as input to classification via Random Forest and bootstrap; we compare 14 different feature sets by means of False Negative Rate (FNR) and Kappa (Îº), obtaining FNRâ¯=0.02 and Îº=0.89 for Herlev, and FNRâ¯=0.14 and Îº=0.78 for CRIC. Next, we sort and rank cell images using convolutional neural networks and evaluate performance with the Mean Average Precision (MAP), achieving MAPâ¯=0.84 and MAPâ¯=0.82 for Herlev and CRIC, respectively. Cell classification show encouraging results regarding RFD, including its sensitivity to intensity variation around the nuclear membrane as it bypasses cytoplasm segmentation.","Radial feature descriptors, Cell classification, Image retrieval, Convolutional neural networks",Romuere R.V. Silva and Flavio H.D. Araujo and Daniela M. Ushizima and Andrea G.C. Bianchi and Claudia M. Carneiro and Fatima N.S. Medeiros,https://www.sciencedirect.com/science/article/pii/S1047320319301452,https://doi.org/10.1016/j.jvcir.2019.04.012,1047-3203,2019,105--116,62,Journal of Visual Communication and Image Representation,Radial feature descriptors for cell classification and recommendation,article,SILVA2019105
"There are various discontinuities in geotechnical engineering problems, which can be roughly divided into two types: one is the discontinuities formed by various actions of rock and soil, such as joints, faults, fissures and shear failure surfaces in rock mass; The other is the contact surface between geotechnical structures such as various foundations, retaining structures, underground structures and geotechnical bodies. Therefore, we cannot ignore its existence in computation. At present, the finite element method and other numerical methods have been widely used to solve geotechnical engineering problems, but the solution of the above discontinuous deformation problems has not been well solved. In this paper, the constitutive model of rock and soil is established. Based on the theory of image processing technology, the common methods of dealing with discontinuous deformation problems in finite element method are pointed out, and the unreasonable points are pointed out. It is more reasonable to treat such problems as contact problems, because it can reflect the main characteristics of discontinuous deformation surface. Come on. The three-dimensional image of the constitutive model has three important functions: (1) it is helpful to enhance the theory of constitutive theory and lay a foundation for further mastery of the theory of constitutive theory; (2) Abstract constitutive theory is no longer just a bunch of boring formulas, which is helpful for beginners to improve their interest in learning the theory of constitutive theory. (3) Finally, a three-dimensional image constitutive model of rock and soil is established through experiments, and the program written by MATLAB is compared with the test results of conventional triaxial compression of rock and soil, which fully verifies the correctness and effectiveness of the constitutive model of rock and soil established in this paper.","Rock and soil, Elastoplasticity, Constitutive model, Numerical simulation",Fujiang Chen,https://www.sciencedirect.com/science/article/pii/S1047320319301051,https://doi.org/10.1016/j.jvcir.2019.03.012,1047-3203,2019,398--406,60,Journal of Visual Communication and Image Representation,Three dimensional image of stress space geotechnical constitutive model,article,CHEN2019398
"Several gold occurrences have been discovered in the Maizijing-Shulonggou area, Ningxia Hui autonomous region, China. The clues for gold prospecting in this area may relate to fractures and hydrothermal alterations, but it is very difficult to conduct a field investigation on the clues because of the alpine valleys. To explore the clues for gold prospecting, this study extracted geological information related to fracture zones and hydrothermal alterations using ALI, ASTER, and WorldView-2 data, and subsequently, explored potential relationships between extracted geological characteristics and gold mineralization. The WorldView-2 image in the study area was used for fracture interpretation. The azimuth and density of linear fractures were investigated, and the results showed that linear fractures with NE direction were most common, the gold occurrences except Au2 were all located in NE fracture zones, and all gold occurrences located in high fracture density areas (level 1â3 areas) where NE and NW fractures were the most common observations. This suggests that the high fracture density and NE and NW fractures are important indicators of potential gold mineralization. Hydrothermal alteration minerals in exposed bedrocks were mapped using ALI and ASTER imagery. Based on the spectral analysis, ASTER image in the study area were used to map quartz, illite and chlorite, while ALI image were used to map limonite. The spatial distribution of alteration minerals revealed regional specific alteration mineral in gold mineralization. In the northern Maizijing region, the composition of alteration mineral in the surrounding of Au1 was overwhelmed by quartz, while the alteration minerals in the surrounding of Au2-4 were dominated by limonite in the southern Shulonggou region, which showed different indicator minerals for gold prospecting in the two regions. As a result, the key areas for prospecting are the NE and NW fracture zones with quartz veins in the Maizijing region and the NE and NW fracture zones with limonitization in the Shulonggou region. In the Shulonggou region, pyrite in buried quartz-pyritization belt and illitization-pyritization belt is likely to be the precursor of limonite. Heavy mineral assemblages were analyzed for native gold content, and the results suggested high native gold content in the illitization-pyritization belt, followed by quartz-pyritization belt, which was consistent with the strong association of limonite and gold mineralization, thus the two belts shall be prioritized for gold prospecting in the Shulonggou region.","Remote sensing, Mineral prospecting, Fracture, Hydrothermal alteration, Gold mineralization, Maizijing-Shulonggou area",Yuanjin Xu and Pengyan Meng and Jianguo Chen,https://www.sciencedirect.com/science/article/pii/S1047320319300628,https://doi.org/10.1016/j.jvcir.2019.02.011,1047-3203,2019,192--205,60,Journal of Visual Communication and Image Representation,"Study on clues for gold prospecting in the Maizijing-Shulonggou area, Ningxia Hui autonomous region, China, using ALI, ASTER and WorldView-2 imagery",article,XU2019192
"Remote sensing satellite images are used widely in space imaging applications as they collect significant information of ground objects through capturing the ground surface in immense wavelength bands. The size of these images is typically enormous in quantity due to the bulky number of capturing wavelengths. The images need to transmit to the ground from the sensors for a specific application. Thus, the efficient compression techniques are required to fit the available bandwidth for reducing the transmission time. The data in the images are usually redundant spatially, spectrally and temporally which give an ample opportunity to compress the images in various domains. Most importantly, the data features have a strong correlation in the separate spectral area. As a result, the similarity-based band reordering strategy is used to increase the compression performance in comparison to the image having natural band order. However, finding the optimal band reordering is still a computationally challenging problem. In this paper, three different methods namely Band Reordering based on Consecutive Continuity Breakdown Heuristics (BRCCBH), Band Reordering based on Weighted-Correlation Heuristic (BRWCH) and Segmented BRCCBH have been proposed for the compression of multispectral, hyperspectral and hyperspectral sounder data. The presented methods are different on the number and type of heuristics used for obtaining the optimal band reordering. The performances of the proposed band reordering methods are tested using CCSDS 123 lossless predictor and lossless 3D-CALIC. The experimental results show the significant improvement on compression performance by using the proposed band ordering techniques for different types of real multispectral data (3â5% using CCSDS and 2â3% using 3D-CALIC), hyperspectral data (0.2â0.7% using CCSDS and 0.8â1% using 3D-CALIC) and hyperspectral sounder data (5.5â7% using CCSDS and 4â5% using 3D-CALIC).","Satellite image, Lossless compression, Band reordering, CCSDS, Correlation exploitation, 3D-CALIC",Masud Ibn Afjal and Md. Al Mamun and Md. Palash Uddin,https://www.sciencedirect.com/science/article/pii/S1047320319300501,https://doi.org/10.1016/j.jvcir.2019.01.042,1047-3203,2019,514--526,59,Journal of Visual Communication and Image Representation,Band reordering heuristics for lossless satellite image compression with 3D-CALIC and CCSDS,article,AFJAL2019514
"The goal of super-resolution (SR) is to recover a high-resolution (HR) image from its corresponding low-resolution (LR) image. It is an ill-posed problem. Most recent methods are based on external training data. They can reconstruct pleasing HR results, especially when the input patch has a similar counterpart within the training dataset. Other methods are driven by self-similarity and are called internal methods. They can produce visually plausible HR images when the input images contain abundant regular structures. In this paper, we propose a hybrid method for image SR that exploits the complementary advantages of external and internal SR methods. Each input LR patch is first super-resolved using convolutional neural network (CNN) for external SR and self-similarity for internal SR. Then, we calculate the perceptual similarity between the feature representations from the pre-trained VGG network to learn an adaptive weight. Finally, our algorithm automatically selects the optimal method on the basis of the calculated adaptive weight. The experimental results of our visual and quantitative evaluations verify the effectiveness of the proposed method, by comparing it with state-of-the-art methods.","Super-resolution, Hybrid method, Adaptive weight, Pre-trained VGG network",Yanxiang Chen and Huadong Tan and Luming Zhang and Jie Zhou and Qiang Lu,https://www.sciencedirect.com/science/article/pii/S1047320319300744,https://doi.org/10.1016/j.jvcir.2019.02.022,1047-3203,2019,229--235,60,Journal of Visual Communication and Image Representation,Hybrid image super-resolution using perceptual similarity from pre-trained network,article,CHEN2019229
"The Picture-level Just Noticeable Difference (PJND) for a given image and compression scheme reflects the smallest distortion level that can be perceived by an observer with respect to a reference image. Previous work has focused on the PJND of images and videos. In this paper, we study the PJND of symmetrically and asymmetrically compressed stereoscopic images for JPEG2000 and H.265 intra coding. We conduct interactive subjective quality assessment tests to determine the PJND point using both a pristine image and a distorted image as a reference. We find that the PJND points are highly dependent on the image content. In asymmetric compression, there exists a perceptual threshold in the quality difference between the left and right views due to the binocular masking effect. We generate two PJND-based stereo image datasets (one for symmetric compression and one for asymmetric compression) and make them accessible to the public.","Picture-level JND, Subjective quality assessment test, Stereoscopic image",Chunling Fan and Yun Zhang and Huan Zhang and Raouf Hamzaoui and Qingshan Jiang,https://www.sciencedirect.com/science/article/pii/S1047320319301555,https://doi.org/10.1016/j.jvcir.2019.04.016,1047-3203,2019,140--151,62,Journal of Visual Communication and Image Representation,Picture-level just noticeable difference for symmetrically and asymmetrically compressed stereoscopic images: Subjective quality assessment study and datasets,article,FAN2019140
"This paper presents a video coding scheme tailored for traffic surveillance videos, which features a pre-built library that is utilized in both encoder and decoder to pursue higher compression efficiency. We are motivated by the observation that, in traffic surveillance videos, not only the background is steady for a long while, but also the foreground (e.g. vehicles) contains high redundancy. For example, in the video taken by a traffic monitoring camera, we can observe that the passing-through vehicles are usually similar. However, the redundancy in the vehicles and the background is not fully exploited in the current video coding schemes. To address this problem, we propose a library-based video coding scheme. Specifically, for each static monitoring camera, we can collect video in a period, and build a library of vehicles and background from that video. When encoding the following video of that camera, we utilize the pre-built library by searching similar vehicles and background from the library and using the retrieved vehicles and background to help encode. Accordingly, the decoder also refers to the same library to reconstruct the video. We design efficient algorithms for building the library, searching in the library, as well as utilizing the library for encoding/decoding, to fulfill the proposed scheme. Our scheme is implemented upon the state-of-the-art video coding systemâHigh Efficiency Video Coding (HEVC), and is tested on our own collected traffic surveillance videos. Experimental results show that, compared to the HEVC anchor, our proposed scheme achieves as high as 47.0%, 37.1%, and 34.8% BD-rate reduction, under random-access, low-delay B, and low-delay P settings, respectively. Our scheme also achieves higher compression efficiency than the existing background-based methods for surveillance video coding.","Background, High Efficiency Video Coding (HEVC), Library-based coding, Traffic surveillance video, Vehicle",Changyue Ma and Dong Liu and Xiulian Peng and Li Li and Feng Wu,https://www.sciencedirect.com/science/article/pii/S1047320319301038,https://doi.org/10.1016/j.jvcir.2019.03.009,1047-3203,2019,426--440,60,Journal of Visual Communication and Image Representation,Traffic surveillance video coding with libraries of vehicles and background,article,MA2019426
"Poisson noise occurs in various applications including medical imaging and night vision. There exist many traditional Poisson denoising algorithms, particularly, one class of impressive algorithm is based on variance-stabilizing transformation (VST). Inspired by the traditional VST scheme, in this paper we propose a novel Poisson denoising model based on convolutional neural network, called variance-stabilizing transform network (VST-Net). VST-Net inherits the structures and strengths of the traditional VST scheme via optimizing the nonlinear transformation by means of network design and supervised learning. The whole VST-Net network contains three sub-networks. The first and third sub-networks simulate the forward and inverse Anscombe transforms, respectively. Meanwhile, the second sub-network is devoted to playing the role of approximate Gaussian denoising. Joint learning strategy and two-stage progressive learning strategy are exploited to investigate the rationality and strength of the VST scheme. Experimental results verify the great potential of the VST-driven network. Code is available at: https://github.com/yqx7150/VST-Net.","Poisson noise reduction, Deep learning, Variance-stabilizing transformation, Joint learning, Two-stage learning",Minghui Zhang and Fengqin Zhang and Qiegen Liu and Shanshan Wang,https://www.sciencedirect.com/science/article/pii/S1047320319301439,https://doi.org/10.1016/j.jvcir.2019.04.011,1047-3203,2019,12--22,62,Journal of Visual Communication and Image Representation,VST-Net: Variance-stabilizing transformation inspired network for Poisson denoising,article,ZHANG201912
"Due to the limited measurement range and occlusion of single-line structured light, it is impossible to detect the side data of the whole part. It is proposed that point cloud registration method obtained from multiple rotations of parts in frequency domain by Fourier transform. In the process of point cloud registration, cross-section point cloud data are restored to the corresponding size matrix firstly. Secondly, Fourier transform is carried out to calculate the point cloud data. When calculating the rotation angle, the polar coordinate transformation is carried out at first, and then the cross power spectrum of the two matrices is obtained, so that the rotation and translation matrix of the point cloud can also be obtained. In this process, considering point cloud noise existence, the Sinc function is approximately replaced by the non-noise inverse Fourier transform of cross power spectrum, so that the noise has no influence on the determination of registration parameters in frequency domain registration. The registration accuracy of point cloud is checked by high precision rotation and multiple measurements of mobile platform. Finally, the rotation matrix and translation values are obtained.","Fourier transform, Point cloud data, Frequency domain, Registration",Shuang Zhang and Hua Wang and Jin-gang Gao and Chun-qi Xing,https://www.sciencedirect.com/science/article/pii/S1047320319300987,https://doi.org/10.1016/j.jvcir.2019.03.005,1047-3203,2019,170--177,61,Journal of Visual Communication and Image Representation,Frequency domain point cloud registration based on the Fourier transform,article,ZHANG2019170
"This research proposes a method that generates viewpoint smooth switching by reducing the flickering artefacts that are observed at bullet-times generated from asynchronous multi-view videos using frame interpolation processing. When we asynchronously capture multi-view videos of an object moving at a high velocity, deviations occur in the observed position at the bullet-times. We apply a frame interpolation technique to smooth this problem. By selecting suitable interpolated images that produce the smallest movement of the subjectâs observed position, we smoothly generate a viewpoint-switched bullet-time video. In this paper, we examine the subjective evaluation of the video generated by the proposed method. And we also examine objective evaluation. Therefore, the effectiveness of the proposed method is shown. Furthermore, reproducibility was improved by considering the application conditions of the proposed method.","Free-viewpoint video, Bullet-time, Frame interpolation, Morphing, Asynchronous multi-view videos",Hidehiko Shishido and Aoi Harazaki and Yoshinari Kameda and Itaru Kitahara,https://www.sciencedirect.com/science/article/pii/S1047320319301440,https://doi.org/10.1016/j.jvcir.2019.04.010,1047-3203,2019,68--76,62,Journal of Visual Communication and Image Representation,Smooth switching method for asynchronous multiple viewpoint videos using frame interpolation,article,SHISHIDO201968
"The stay points of trajectory include important semantic information. Identifying the stay points is an indispensable step for mining and analyzing the trajectory data. Current methods for identifying stay points generally require pre-setting thresholds, which have a major impact on the identification results. Meanwhile, the setting of these thresholds is heavily reliant on empirical experience. There are many types of moving objects, such as vehicles, ships, airplanes, people and animals. These moving objects have a large variation in their trajectory data due to their different geometric/physical properties and navigation environments. Therefore, setting the appropriate thresholds for the identification process is by no means easy for non-professionals. To cope with this challenge, an interactive visualization method is suggested in this paper in order to help users to set the thresholds. In the proposed method, the Space-Time Cube (STC) is used to visualize the trajectory as the first step. In the next step, users interactively choose the typical stay points of the trajectory. The threshold is atomically determined using the geometric characteristics of the 3-dimensional bounding box of the space where the moving object stays on. Finally, the sliding window method is used to extract the stay points. Experimental results demonstrate that the proposed method is an efficient method for determining the thresholds in an intuitive and fast manner. As a result, it can significantly improve the efficiency of identifying the stay points of the trajectory for non-professionals.","Trajectory, Stay point, Space-time cube",Yue Zhang and Yaping Lin,https://www.sciencedirect.com/science/article/pii/S1047320319300471,https://doi.org/10.1016/j.jvcir.2019.01.038,1047-3203,2019,387--392,59,Journal of Visual Communication and Image Representation,An interactive method for identifying the stay points of the trajectory of moving objects,article,ZHANG2019387
"For copyright and integrity protection of stereo images, a stereo image reversible data hiding (SIRDH) method based on the convolutional neural network (CNN) is presented. To increase the pixel prediction, a CNN-based predictor is designed. Firstly, pixels are divided into two types. For predicting one type of pixels in one view, the other type of pixels in the same view is considered as the low-resolution (LR) image. Secondly, a LR difference view between the LR image and the other view is computed, since the difference view consists of the texture and depth information. Then, CNN is trained to recover the high-precision difference view from the LR difference view, and the high-resolution image is computed to predict pixels accurately. Moreover, the prediction error expansion (PEE) is employed to embed data. Experimental results show that the proposed method is superior to existing SIRDH methods.","Convolutional neural network (CNN), Predictor, Prediction error expansion (PEE), Reversible data hiding (RDH), Stereo image",Ting Luo and Gangyi Jiang and Mei Yu and Caiming Zhong and Haiyong Xu and Zhiyong Pan,https://www.sciencedirect.com/science/article/pii/S1047320319301166,https://doi.org/10.1016/j.jvcir.2019.03.017,1047-3203,2019,61--73,61,Journal of Visual Communication and Image Representation,Convolutional neural networks-based stereo image reversible data hiding method,article,LUO201961
"In this paper, we propose a novel reduced-reference quality assessment metric for image super-resolution (RRIQA-SR) based on the low-resolution (LR) image information. With the pixel correspondence, we predict the perceptual similarity between image patches of LR and SR images by two components: the energy change in low-frequency regions, which can be used to capture the global distortion in SR images, and texture variation in high-frequency regions, which can be used to capture the local distortion in SR images. The overall quality of SR images is estimated by perceptual similarity calculated by energy change and texture variation between local image patches of LR and HR images. Experimental results demonstrate that the proposed method can obtain better performance of quality prediction for SR images than other existing ones, even including some full-reference (FR) metrics.","Image quality assessment (IQA), Image super-resolution, Reduced-reference (RR) quality assessment, Energy change, Texture variation",Yuming Fang and Jiaying Liu and Yabin Zhang and Weisi Lin and Zongming Guo,https://www.sciencedirect.com/science/article/pii/S1047320318303614,https://doi.org/10.1016/j.jvcir.2018.12.035,1047-3203,2019,140--148,60,Journal of Visual Communication and Image Representation,Reduced-reference quality assessment of image super-resolution by energy change and texture variation,article,FANG2019140
"Object detection has always attracted a lot of attention in computer vision due to its practical applications, i.e., robotics engineering, autonomous vehicles, and surveillance systems. Recently deep learning approaches have successfully improved the performance of object detection by a significant amount. However, there exist many challenging objects in the images that state-of-the-art approaches still fail to detect. In this paper, we propose an efficient approach that intentionally learns to detect the unseen (missing) objects. In particular, we utilize a dual-level of deep networks to efficiently detect difficult objects in images. The extensive experiments on three benchmarking datasets, PASCAL VOC, KITTI, and MS-COCO, show the superiority of our approach over the state-of-the-art methods.","Deep learning, Dual-level deep networks, Object detection",Khanh-Duy Nguyen and Khang Nguyen and Duy-Dinh Le and Duc Anh Duong and Tam V. Nguyen,https://www.sciencedirect.com/science/article/pii/S1047320319300732,https://doi.org/10.1016/j.jvcir.2019.02.020,1047-3203,2019,206--216,60,Journal of Visual Communication and Image Representation,You always look again: Learning to detect the unseen objects,article,NGUYEN2019206
"As the main media of human communication and understanding of the world, image is one of the important information sources of human intelligence activities. With the development of the times, the demand for image processing technology is increasing day by day. The rapid development of computer technology also provides a platform for the application of image processing. In order to achieve better image processing effect, this paper focuses on the application of artificial intelligence algorithm in image processing. Image segmentation is a technology that decomposes images into regions with different characteristics and extracts useful targets. It can be regarded as a combinatorial optimization problem. It is completely feasible to apply artificial intelligence algorithm to optimization problems. Firstly, this paper introduces the ant colony algorithm in artificial intelligence algorithm, elaborates the basic principle and mathematical model of the ant colony algorithm. Secondly, in order to improve the ability of global search of ant colony algorithm, this paper introduces the crowding degree function of fish into the ant colony algorithm. Finally, the improved ant colony algorithm is used in image segmentation to improve the effect of image segmentation. The simulation results show that it is feasible to use ant colony algorithm in image segmentation. And the optimization improvement of ant colony algorithm is effective. The improved ant colony algorithm applied in image segmentation can significantly improve the segmentation performance.","Image processing, Artificial intelligence algorithm, Image segmentation, Ant colony algorithm",Xin Zhang and Wang Dahu,https://www.sciencedirect.com/science/article/pii/S1047320319300975,https://doi.org/10.1016/j.jvcir.2019.03.004,1047-3203,2019,42--49,61,Journal of Visual Communication and Image Representation,Application of artificial intelligence algorithms in image processing,article,ZHANG201942
"With the growing shortage of energy, wind power is receiving an increasing amount of attention worldwide as a clean source of renewable energy. One of the most important components of the wind turbine is the bird-wing-like blade under the working principle. Three kinds of bionic blades are designed by using bionic wing type and configuration based on the good aerodynamic performance of seagull wings, combined with the theory of wind turbine blade design-the blade element theory. Standard blade and aerodynamic performances of bionic blade are evaluated by means of numerical simulation. Under different wind speeds, the results show that the blade torque of the bionic blade of total improved airfoil increases by 10.2%, the bionic blade of partially improved airfoil increases by 14%, and the configuration improved blade increases by 7%. To verify the correctness of the numerical simulation results, we run the bionic blade and establish a corresponding experimental device in an actual experiment. The results of the experiment are consistent with the trend of the numerical simulation.","Numerical simulation, Source of renewable energy, Wind turbine blade",Xin Hua and Chunhua Zhang and Jinda Wei and Xingjun Hu and Hongliang Wei,https://www.sciencedirect.com/science/article/pii/S1047320319300458,https://doi.org/10.1016/j.jvcir.2019.01.037,1047-3203,2019,258--265,60,Journal of Visual Communication and Image Representation,Wind turbine bionic blade design and performance analysis,article,HUA2019258
"With the development of information technology, mechanical product design has become an important part of the enterprise and design community. In the traditional mechanical design process, the product not only has performance requirements, but also has many economic requirements such as manufacturing cost and sales profit. Many conditions also have great uncertainty. In addition, in design and evaluation, in addition to the existing theoretical analysis, in most cases, it is mainly judged based on the designerâs experience. In view of the deficiencies in mechanical design, this paper proposes a key technology research of remote design of mechanical products based on artificial intelligence. The wireless communication technology is used to build a client/server wireless communication platform, which is convenient for designers to transmit remote information. The application of image processing and pattern recognition technology to target the components involved in mechanical products helps designers grasp many parameters of mechanical products. The research on the key technology of remote design of mechanical products based on artificial intelligence in this paper has certain reference value for the application of artificial intelligence technology in the field of mechanical product design with insufficient knowledge.","Artificial intelligence, Machinery, Remote design, Image processing",Ya Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319300616,https://doi.org/10.1016/j.jvcir.2019.02.010,1047-3203,2019,250--257,60,Journal of Visual Communication and Image Representation,Research on key technologies of remote design of mechanical products based on artificial intelligence,article,ZHANG2019250
"One of the key challenge issues of deep-learning-based facial expression recognition (FER) is learning effective and robust features from variant samples. In this paper, Region-based Convolutional Fusion Network (RCFN) is proposed to solve this issue via three aspects. Firstly, a muscle movement model is built to segment out crucial regions of frontal face, providing well-unified patches with benefits of removing unrepresentative regions and greatly reducing interference caused by facial organs with varied sizes and positions among individuals. Secondly, a fast and practical network is constructed to extract robust triple-level features from low level to semantic level in each crucial region and fuse them for FER. Thirdly, constrained punitive loss is introduced to leverage the network training for boosting up FER performance. The experiment results show that RCFN is effective in commonly used datasets like KDEF, CK+, and Oulu-CASIA, and can achieve comparable performance with other state-of-the-art FER methods.","Facial expression recognition, Emotion recognition, Convolution neural network",Yingsheng Ye and Xingming Zhang and Yubei Lin and Haoxiang Wang,https://www.sciencedirect.com/science/article/pii/S1047320319301427,https://doi.org/10.1016/j.jvcir.2019.04.009,1047-3203,2019,1--11,62,Journal of Visual Communication and Image Representation,Facial expression recognition via region-based convolutional fusion network,article,YE20191
"In compressive sensing framework, the results of image reconstruction are sometimes not accurate enough due to the downsampled measurements, especially when the sampling rate is relatively small. This paper proposes a novel edge guided compressive sensing (EGCS) algorithm for natural image reconstruction based on two-stage l0 minimization, aiming to improve the reconstruction performance. Firstly, wavelet transform is utilized to provide sparsity and multiple sampling scheme is employed to acquire the down-sampled measurements. Then, in the first stage, we design an edge-preserving smoothing method by l0 gradient minimization to extract the important edge prior accurately, which can not only contribute a lot to improve the reconstruction accuracy of image structures but also reduce the computational complexity remarkably. Also, the use of multiple sampling scheme is beneficial to enhancing the guidance accuracy of multiple edge prior. In the second stage, under the guidance of multiple edge prior, the intelligent searching strategies are designed by taking advantages of intelligent optimization algorithms in solving combinatorial optimization problems and utilizing the superior performance of greedy algorithm on reconstruction speed, which is conductive to solve the joint sparse reconstruction based on l0 minimization essentially. The better reconstruction performance can be achieved based on the fact that it is more likely to find the global optimal solution accurately by the designed intelligent searching strategies, especially when the sampling rate is relatively small. Experimental results on natural image reconstruction demonstrate that our proposed method EGCS is superior to the state-of-the-art reconstruction algorithms, and can well preserve the important image structures at the same time.","Compressive sensing, Image reconstruction,  minimization, Edge prior, Multiple sampling scheme",Dan Li and Zhaojun Wu and Qiang Wang,https://www.sciencedirect.com/science/article/pii/S104732031930032X,https://doi.org/10.1016/j.jvcir.2019.01.025,1047-3203,2019,461--474,59,Journal of Visual Communication and Image Representation,Edge guided compressive sensing for image reconstruction based on two-stage l0 minimization,article,LI2019461
"It is a challenge to conduct natural language steganography on Online interactive platforms such as photo-sharing websites since the stego texts should be consistent with the content of the images. In this paper, a novel natural language steganographic framework based on an end-to-end generative network is proposed. A Convolution Neural Network (CNN) combined with Long Short-Term Memory (LSTM) is trained to generate stego descriptions. Word by Word Hiding (WWH) and Sentence by Sentence Hiding (SSH) schemes are proposed to achieve various embedding capacity under the premise of sharing model between the sender and the receiver. Furthermore, a blind extraction scheme called Hash Hiding (HH) is proposed in case that the model is unavailable for data extraction. Comparative experiments show the superiority of the proposed framework. It is verified that the proposed framework is an effective carrier-less steganographic framework with competitive embedding capacity, considerable text quality, and good reversibility.","Natural language steganography, Image description, Neural network, Embedding capacity, BLEU, Perplexity",Juan Wen and Xuejing Zhou and Mengdi Li and Ping Zhong and Yiming Xue,https://www.sciencedirect.com/science/article/pii/S1047320319301154,https://doi.org/10.1016/j.jvcir.2019.03.016,1047-3203,2019,157--169,61,Journal of Visual Communication and Image Representation,A novel natural language steganographic framework based on image description neural network,article,WEN2019157
"Automatic recognition of actions can be addressed by employing data from multiple sensors, such as RGB cameras, depth sensors or inertial measurement units. Recent studies show that multimodal representations of actions are effective in providing rich information about motion patterns. In this work, we propose a novel action descriptor, called Joint Motion History Context, which is based on depth and skeleton data. It improves action representation when used with previously introduced descriptors that are based on depth, skeleton and inertial data. A feature selection method is proposed as well, which ranks features on the basis of their inter-class discriminative power, while minimizing redundancy in the selected feature subset. Decision-level fusion, based on Support Vector Machines and Multilayer Perceptron is employed to effectively combine motion pattern information from multiple feature sets. Experimental results on two publicly available datasets, FFD and UTD-MHAD, demonstrated that the proposed methods outperform state-of-the-art algorithms.","Action recognition, Action descriptors, Depth maps, Feature selection, Multimodal representation, Decision-level fusion",Filip Malawski and Bogdan Kwolek,https://www.sciencedirect.com/science/article/pii/S1047320319301312,https://doi.org/10.1016/j.jvcir.2019.03.026,1047-3203,2019,198--208,61,Journal of Visual Communication and Image Representation,Improving multimodal action representation with joint motion history context,article,MALAWSKI2019198
"Single sample per person face recognition influenced by varying illumination is a tricky issue. Conventional techniques for illumination-invariant face recognition either realize illumination normalization on the whole face, or learn the illumination-invariant representation from the face image. This paper holds the opinion that deep learning method, which is more similar to the behavior of primate brain, can leverage the advantages of both the conventional techniques. Motivated by the success of generative adversarial network in image representation, this paper proposes IL-GAN model based on the basic structures of variational auto-encoder and generative adversarial network, generating the Controlled Illumination-level Face Image while preserves identity character as well performing a powerful latent representation from the face image, which encodes illumination-invariant signatures. Moreover, this model can be adopted in single sample per person face recognition. Meanwhile, this research proposes an novel illumination level estimation method based on singular value decomposition to generate the Controlled Illumination-level Face Image optionally. Finally, the performances of the proposed method and other state-of-the-art techniques are verified on the Extended Yale B, CMU PIE, IJB-A and our Self-built Driver Face databases. The experimental results indicate that the IL-GAN model outperforms previous approaches for single sample per person face recognition under varying illumination.","Single sample per person, Singular value decomposition, Generative adversarial network, Representation learning, Illumination-invariant face recognition",Yang Zhang and Changhui Hu and Xiaobo Lu,https://www.sciencedirect.com/science/article/pii/S1047320319300586,https://doi.org/10.1016/j.jvcir.2019.02.007,1047-3203,2019,501--513,59,Journal of Visual Communication and Image Representation,IL-GAN: Illumination-invariant representation learning for single sample face recognition,article,ZHANG2019501
"Gamut mapping is a key technology to achieve high-quality cross-media color reproduction. To optimize a gamut mapping algorithm, an important step is to conduct an accurate evaluation of its psycho-visual performance. This paper presents an objective blind image quality assessment (BIQA) metric for gamut-mapped images based on natural scene statistics. Considering both the local and global aspects of distortions in gamut-mapped images, two categories of statistics are analyzed. Specifically, the local statistical features are used to portray structural and color distortions and features extracted from global statistics are utilized to characterize the naturalness of image. The proposed metric does not need ground truth quality scores for training, thus it is âcompletelyâ blind. Experimental results on three gamut mapping databases demonstrate that our method outperforms the state-of-the-art general-purpose BIQA models. To further validate its effectiveness, the proposed metric is applied for benchmarking GMAs as an application and achieves encouraging performance.","Image quality assessment, Gamut mapping, Natural scene statistics",Hao Cai and Leida Li and Zili Yi and Minglun Gong,https://www.sciencedirect.com/science/article/pii/S1047320319301397,https://doi.org/10.1016/j.jvcir.2019.04.006,1047-3203,2019,250--259,61,Journal of Visual Communication and Image Representation,Blind quality assessment of gamut-mapped images via local and global statistical analysis,article,CAI2019250
Dual image Reversible Data Hiding (RDH) algorithms generate two stego images instead of one. These algorithms are considered more secure because the secret data can be recovered only if both stego images are available. This paper proposes a dual image RDH algorithm based on the Centre Folding Strategy (CFS) and the Shiftable Pixel Coordinate Selection Strategy. The proposed algorithm improves the visual quality of the two stego images while maintaining a high embedding rate. It uses a look-up table containing the shifts that the cover pixel will undergo after the data has been embedded. Some overhead information is required for accurate extraction of the hidden data and it is delivered to the users as a password after embedding. The results show that the proposed algorithm outperforms other methods in terms of visual quality with an average Peak-Signal-to-Noise-Ratio (PSNR) of 50.66â¯dB with an embedding rate of 1.56 bits per pixel.,"Reversible data hiding, Dual images, Centre folding strategy, Trinary assignment, High PSNR, High security",Shounak Shastri and V. Thanikaiselvan,https://www.sciencedirect.com/science/article/pii/S1047320319301270,https://doi.org/10.1016/j.jvcir.2019.03.022,1047-3203,2019,130--140,61,Journal of Visual Communication and Image Representation,Dual image reversible data hiding using trinary assignment and centre folding strategy with low distortion,article,SHASTRI2019130
"A single-exposure image may lose details because of the imaging dynamic range limitations of single camera sensor. Multi-image fusion techniques are often used to improve the image quality, but if there are moving objects in the scene, the fused images may result in ghost artifacts. In order to avoid this problem and enhance single-exposure images, this paper proposes a dual network cascade model for single image enhancement, including exposure prediction network and exposure fusion network. First, the exposure prediction network generates two under-/over-exposure images that differ from the input normal-exposure image so as to recover the lost details of the under-exposed/over-exposed regions. Then, the exposure fusion network fuses the input image and the generated under-/over-exposure images to generate the final enhanced image. The loss function constructed by a structural dissimilarity index is used to alleviate chessboard artifacts in the generated image. Further, through three-phase training, the model robustly generates enhanced images without any post-processing. The experimental results demonstrate that the proposed method can effectively improve the image contrast and reconstruct details of under-exposed/over-exposed regions in the original image.","Single image enhancement, Convolutional neural network, Dual network cascade model, Exposure prediction, Exposure fusion",Yeyao Chen and Mei Yu and Gangyi Jiang and Zongju Peng and Fen Chen,https://www.sciencedirect.com/science/article/pii/S1047320319301415,https://doi.org/10.1016/j.jvcir.2019.04.008,1047-3203,2019,284--295,61,Journal of Visual Communication and Image Representation,End-to-end single image enhancement based on a dual network cascade model,article,CHEN2019284
"Fashion recommendation has attracted much attention given its ready applications to e-commerce. Traditional methods usually recommend clothing products to users on the basis of their textual descriptions. Product images, although covering a large resource of information, are often ignored in the recommendation processes. In this study, we propose a novel fashion product recommendation method based on both text and image mining techniques. Our model facilitates two kinds of fashion recommendation, namely, similar product and mix-and-match, by leveraging text-based product attributes and image features. To suggest similar products, we construct a new similarity measure to compare the image colour and texture descriptors. For mix-and-match recommendation, we firstly adopt convolutional neural network (CNN) to classify fine-grained clothing categories and fine-grained clothing attributes from product images. Algorithm is developed to make mix-and-match recommendations by integrating the image extracted categories and attributes information are with text-based product attributes. Our comprehensive experimental work on a real-life online dataset has demonstrated the effectiveness of the proposed method.","Fashion recommendations, Image retrieval, Human parsing, Image features",Wei Zhou and P.Y. Mok and Yanghong Zhou and Yangping Zhou and Jialie Shen and Qiang Qu and K.P. Chau,https://www.sciencedirect.com/science/article/pii/S1047320319300963,https://doi.org/10.1016/j.jvcir.2019.03.003,1047-3203,2019,112--120,61,Journal of Visual Communication and Image Representation,Fashion recommendations through cross-media information retrieval,article,ZHOU2019112
"3D human pose estimation from a single image is a challenging problem due to occlusion, viewpoint variance, and the ill-posed nature of back projection. We follow a standard two-step pipeline which first detects 2D joint locations and uses them to infer 3D pose. For the first step, we use a recent deep learning-based detector. For the second step, we propose a novel exemplar-based algorithm to implicitly augment the exemplar set for 3D human pose estimation. The motivation of this algorithm is to well represent various poses in the real world with finite real exemplars. We achieve it by a strategy of synthesizing virtual candidate poses which ensures that the augmented exemplar set has much more variety. Moreover, we also present an effective approach to select the best exemplar from candidate set to well match the detected 2D pose. Experimental results show that our method achieves competitive performance on Human3.6M dataset.","Human pose estimation, Human pose recovery, Exemplar-based, Pose retrieval, Pose synthesis, Monocular",Jingjing Yang and Lili Wan and Wanru Xu and Shenghui Wang,https://www.sciencedirect.com/science/article/pii/S1047320319300446,https://doi.org/10.1016/j.jvcir.2019.01.033,1047-3203,2019,371--379,59,Journal of Visual Communication and Image Representation,3D human pose estimation from a single image via exemplar augmentation,article,YANG2019371
"Collective activity recognition, which analyses the behavior of groups of people in videos, is an important goal of video surveillance systems. In this paper, we focus on collective activity recognition problem and propose a new multi-stream convolutional neural network architecture that utilizes information extracted from multiple regions. The proposed method is the first work that uses a multi-stream network and multiple regions in this problem. Various strategies to fuse multiple spatial and temporal streams are explored. We evaluate the proposed method on two benchmark datasets, the Collective Activity Dataset and the Volleyball Dataset. Our experimental results show that the proposed method improves collective activity recognition performance when compared to the state-of-the-art approaches.","Collective activity recognition, Action recognition",Cemil Zalluhoglu and Nazli Ikizler-Cinbis,https://www.sciencedirect.com/science/article/pii/S1047320319300689,https://doi.org/10.1016/j.jvcir.2019.02.016,1047-3203,2019,170--179,60,Journal of Visual Communication and Image Representation,Region based multi-stream convolutional neural networks for collective activity recognition,article,ZALLUHOGLU2019170
"Among the challenges present in the design of retrieval systems, how to accurately assess their performance is perhaps one of the most important. Many applications such as rank aggregation or relevance feedback can be significantly improved with online effectiveness estimation of queries. Thus, developing methodologies that can estimate performance with minimal supervision and at query time is of utmost importance for improving the results of existing retrieval systems. In this work, we explore score-based, post-retrieval approaches for relevance prediction of search systems. We first introduce two statistical methods based on the Extreme Value Theory to estimate which of the top-k objects retrieved for a query are relevant. Our prediction approach uses this estimation as a method to infer the overall performance of a query. The two relevance prediction methods were evaluated in image datasets covering several modalities and scoring approaches. We conducted experiments comparing the ground-truth relevances of several ranks with predictions generated by our proposed approach, measuring their effectiveness by way of normalized accuracy and Matthews Correlation Coefficient. Furthermore, we also evaluate the precision deducted from our approaches with the systemâs expected performance. Those experiments show that the proposed approaches succeed in most relevance prediction scenarios of the top-ranked objects of a query, obtaining high accuracy.","Relevance prediction, Performance prediction, Extreme value theory, Weibull distribution, Information retrieval",Alberto Oliveira and Eric Oakley and Ricardo {da Silva Torres} and Anderson Rocha,https://www.sciencedirect.com/science/article/pii/S1047320319300720,https://doi.org/10.1016/j.jvcir.2019.02.019,1047-3203,2019,236--249,60,Journal of Visual Communication and Image Representation,Relevance prediction in similarity-search systems using extreme value theory,article,OLIVEIRA2019236
"Ideation is a source of innovation and creativity, and is commonly used in early stages of engineering design processes. This paper proposes an integrated approach for enhancing design ideation by applying artificial intelligence and data mining techniques. This approach consists of two models, a semantic ideation network and a visual concepts combination model, which provide inspiration semantically and visually based on computational creativity theory. The semantic ideation network aims to provoke new ideas by mining potential knowledge connections across multiple knowledge domains, and this was achieved by applying âstep-forwardâ and âpath-trackâ algorithms which assist in exploring forward given a concept and in tracking back the paths going from a departure concept through a destination concept. In the visual concepts combination model, a generative adversarial networks model is proposed for generating images which synthesize two distinct concepts. An implementation of these two models was developed and tested in a design case study, which indicated that the proposed approach is able to not only generate a variety of cross-domain concept associations but also advance the ideation process quickly and easily in terms of quantity and novelty.","Idea generation, Artificial intelligence in design, Data-driven design, Generative adversarial networks, Semantic network analysis, Network visualisation, Computational creativity",Liuqing Chen and Pan Wang and Hao Dong and Feng Shi and Ji Han and Yike Guo and Peter R.N. Childs and Jun Xiao and Chao Wu,https://www.sciencedirect.com/science/article/pii/S1047320319300604,https://doi.org/10.1016/j.jvcir.2019.02.009,1047-3203,2019,10--22,61,Journal of Visual Communication and Image Representation,An artificial intelligence based data-driven approach for design ideation,article,CHEN201910
"Facial expression recognition (FER) is the interesting research area that enables us to recognize the expression of the human face in the day-to-day life. Most of the traditional methods fail to recognize the expressions accurately as the expressions are based on the movements of the parts in the human face. The paper proposes the effective method of FER using the proposed Whale- Grasshopper Optimization algorithm based Multi-Support Vector Neural Network (W-GOA-based MultiSVNN). The features from the facial image is extracted using the Scale-Invariant Feature Transform (SIFT) and the proposed Scatter Local Directional Pattern (SLDP). The extracted features are classified using the proposed classifier to recognize the expression of the face. The proposed method of facial recognition enhances the recognition accuracy. The experimentation of the proposed algorithm is performed using the databases, such as Cohn-Kanade AU-Coded Expression Database and The Japanese Female Facial Expression (JAFFE) Database. The proposed algorithm outperforms the existing methods in terms of the accuracy, TPR, and FPR and the values are found to be 0.96, 0.96, and 0.009, respectively.","Facial expression recognition, MultiSVNN classifier, Grasshopper optimization algorithm, Whale optimization algorithm, Local Directional Pattern",I. {Michael Revina} and W.R. {Sam Emmanuel},https://www.sciencedirect.com/science/article/pii/S104732031930152X,https://doi.org/10.1016/j.jvcir.2019.04.013,1047-3203,2019,43--55,62,Journal of Visual Communication and Image Representation,Face Expression Recognition with the Optimization based Multi-SVNN Classifier and the Modified LDP Features,article,MICHAELREVINA201943
"With the rapid development of network and multimedia technology, a large number of sports and national fitness information are stored in various fitness guidance systems in the form of video and pictures. In order to better promote public fitness and facilitate learning and viewing, sports video has a variety of needs of editing, segmentation and integration. Aiming at the shortcomings of current sports video image segmentation methods, such as rough segmentation results and high spatial distortion rate, a sports video image segmentation method based on fuzzy clustering algorithm is proposed. This paper introduces the basic theory of fuzzy clustering algorithm, establishes second-order fuzzy attributes with normal distribution and gray value by means of time-domain difference images, assigns the fuzzy attribute S membership function, then performs fuzzy clustering on time-domain difference images, and obtains the segmentation results of moving video images by edge detection. The experimental results show that the method has high spatial accuracy, good noise iteration performance and low spatial distortion rate, and can accurately segment complex moving video images to obtain high-definition images.","Fuzzy clustering algorithm, Image segmentation, Membership, Motion video",Wu-Yeh Chang,https://www.sciencedirect.com/science/article/pii/S1047320319300847,https://doi.org/10.1016/j.jvcir.2019.02.033,1047-3203,2019,105--111,61,Journal of Visual Communication and Image Representation,Research on sports video image based on fuzzy algorithms,article,CHANG2019105
"In this paper, a Spare Representation (SR) based fusion quality evaluation and analysis method is proposed. This method employs Joint Sparse Representation (JSR) to extract the source image remnants after fusion. These atom-level remnants indicate the fusion quality intuitively, and permit the analysis of fusion effect in learned feature space. Our analysis results indicate that high salient atoms always present poor expressions in fusion results. An improved fusion rule is designed to emphasis high salient atoms accordingly. In experiments, the effectiveness of our method was verified and the characteristics of atom JSR remnants were investigated in detail first. Then the new fusion rule was tested to demonstrate the value of JSR remnant analysis. The objective and subjective comparison results indicate that the proposed analytical evaluation metric can measure fusion quality and analysis atom fusion effect accurately. The new fusion rule provides a valuable alternative for SR fusion algorithm design.","Image fusion, Quality evaluation, Sparse representation, Joint sparse representation, Atom remnant analysis",Yanxiang Hu and Qian Gao and Bo Zhang and Juntong Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319301385,https://doi.org/10.1016/j.jvcir.2019.04.005,1047-3203,2019,225--235,61,Journal of Visual Communication and Image Representation,On the use of joint sparse representation for image fusion quality evaluation and analysis,article,HU2019225
"The biggest disadvantage of using chain code techniques is the generation of low definition contour shapes, in this paper we present the Extended Slope Chain Code (ESCC) which is an improvement on the Slope Chain Code (SCC). The ESCC is focused on the representation of high definition contour shapes. Generally speaking, most chain codes hold the length of the straight-line segments which represent the contour shape as a constant. In this case, the contour shapes represented by ESCC are composed of variable segments, which allow us to have a better description of the contour shape. Thus, the length of the segments are a function of the slope changes, i.e. the length of the next segment depends on the value of the slope change at that point. Therefore, the ESCC is continuously adjusting to the curvature requirements of contour shapes, in order to have a better description of contour shapes.","Slope chain code, Extended slope chain code, High definition contour shapes, Reconfigurable chain code, Bird wings",Ernesto Bribiesca and Fernanda Bribiesca-Contreras and Ãngel Carrillo-Bermejo and Graciela Bribiesca-Correa and Nidiyare Hevia-Montiel,https://www.sciencedirect.com/science/article/pii/S1047320319301142,https://doi.org/10.1016/j.jvcir.2019.03.015,1047-3203,2019,93--104,61,Journal of Visual Communication and Image Representation,A chain code for representing high definition contour shapes,article,BRIBIESCA201993
"Robust feature matcher is usually inapplicable to real time computer vision applications as its high computational complexity, while fast feature matcher is short of robustness to geometric transformations. By studying the sampling pattern of binary descriptor and local geometric statistics of features, this paper proposes a fast and robust feature matching method with binary affine invariant descriptor and local geometric consistency check (BALG). The sampling pattern is adaptively adjusted according local affine transformation that can be effectively estimated from the local intensity moments. The affine sampling pattern improves the affine invariance of binary descriptor while enable fast processing. Furthermore, candidate matches are sorted into local groups along different orientations with sequence order constraint, and then followed by local geometric consistency check. False matches are efficiently filtered out with high recall. Extensive experiments on four publicly benchmark datasets prove the proposed method to be an alternative for time critical feature matcher.","Feature matching, Binary descriptor, Affine invariant, Sequence order, Local geometric check",Zhoudi Huang and Zhenzhong Wei and Guangjun Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319300690,https://doi.org/10.1016/j.jvcir.2019.02.017,1047-3203,2019,129--139,60,Journal of Visual Communication and Image Representation,BALG: An alternative for fast and robust feature matching,article,HUANG2019129
"Recently, reversible data hiding in encrypted images has been developed to transmit useful data while the original images can be perfectly recovered when needed. In this paper, a new method is proposed for homomorphic encrypted images so that part of the hidden data can be extracted in encrypted domain and the rest are extractable after image decryption. Specifically, a plain-text image is preprocessed by reversibly embedding the bit values of some pixels into the image. The preprocessed image is encrypted in Paillier cryptosystem and two embedding algorithms are applied on the encrypted image in succession. Compared with the state-of-the-art schemes, higher embedding capacity can be achieved by applying the proposed method, respectively for data extraction before and after image decryption. Compared with the schemes with similar properties, better performances are achieved with the proposed method in terms of quality of directly decrypted image with respect to data hiding rate.","Homomorphic encryption, Privacy protection, Paillier cryptosystem, Image quality, Reversible data hiding",Hao-Tian Wu and Yiu-ming Cheung and Zhiyuan Yang and Shaohua Tang,https://www.sciencedirect.com/science/article/pii/S1047320319301543,https://doi.org/10.1016/j.jvcir.2019.04.015,1047-3203,2019,87--96,62,Journal of Visual Communication and Image Representation,A high-capacity reversible data hiding method for homomorphic encrypted images,article,WU201987
"We present an approach on training classifiers or regressors using the latent embedding of variational auto-encoders (VAE), an unsupervised deep learning method, as features. Usually VAEs are trained using unlabeled data and independently from the classifier, whereas we investigate and analyze the performance of a classifier or regressor that is trained jointly with the variational deep network. We found that models trained this way can improve the embedding s.t. to increase classification performance, and also can be used for semi-supervised learning, building up the information extracting latent representation in an incremental fashion. The model was tested on two widely known computer vision benchmarks, and its generalization power was evaluated on an independent dataset. Additionally, generally applicable statistical methods are presented for evaluating similarly performing classifiers, and used to quantify the performance increase. The general applicability and ease-of-use of deep learning approaches allows for a wide applicability of the method.","Variational auto-encoder, Regularization, Knowledge representation, Perceptual data compaction, Semi-supervised learning, Statistical performance analysis",Ingo Kossyk and ZoltÃ¡n-Csaba MÃ¡rton,https://www.sciencedirect.com/science/article/pii/S1047320319301026,https://doi.org/10.1016/j.jvcir.2019.03.008,1047-3203,2019,121--129,61,Journal of Visual Communication and Image Representation,Discriminative regularization of the latent manifold of variational auto-encoders,article,KOSSYK2019121
"Few-shot learning aims to build a classifier that recognizes unseen new classes given only a few samples of them. Previous studies like prototypical networks utilized the mean of embedded support vectors to represent the prototype that is the representation of class and yield satisfactory results. However, the importance of these different embedded support vectors is not studied yet, which are valuable factors that could be used to push the limit of the few-shot learning. We propose a principal characteristic network that exploits the principal characteristic to better express prototype, computed by distributing weights based on embedded vectorsâ different importance. The high-level abstract embedded vectors are extracted from our eResNet embedding network. In addition, we proposed a mixture loss function, which enlarges the inter-class distance in the embedding space for accurate classification. Extensive experimental results demonstrate that our network achieves state-of-the-art results on the Omniglot, miniImageNet and Cifar100 datasets.","Few-shot learning, Principal characteristic, Mixture loss function, Embedding network, Fine-tuning",Yan Zheng and Ronggui Wang and Juan Yang and Lixia Xue and Min Hu,https://www.sciencedirect.com/science/article/pii/S1047320319300574,https://doi.org/10.1016/j.jvcir.2019.02.006,1047-3203,2019,563--573,59,Journal of Visual Communication and Image Representation,Principal characteristic networks for few-shot learning,article,ZHENG2019563
"Similarity-preserving hashing has become the mainstream of approximate nearest neighbor (ANN) search for large-scale image retrieval. Recent research shows that deep neural networks can produce efficient feature representation. Most existing deep hashing schemes simply utilize the middle-layer features of the deep neural networks to measure the similarity between query images and database images. However, these visual features are suboptimal for discriminating the semantic information of images, especially for complex images that contain multiple objects. In this paper, a deep framework is employed to learn multi-level non-linear transformations to obtain advanced image features, and then we combine these intermediate features and top layer visual information to implement image retrieval. Three criterions are enforced on these compact codes: (1) minimal quantization loss; (2) evenly distributed binary; (3) independent bits. The experimental results on five public large-scale datasets demonstrate the superiority of our method compared with several other state-of-the-art methods.","Large-scale image retrieval, Similarity comparison, Deep learning, Multi-label learning, Quantization error",Xiaofei Wang and Feifei Lee and Qiu Chen,https://www.sciencedirect.com/science/article/pii/S1047320319301300,https://doi.org/10.1016/j.jvcir.2019.03.024,1047-3203,2019,260--271,61,Journal of Visual Communication and Image Representation,Similarity-preserving hashing based on deep neural networks for large-scale image retrieval,article,WANG2019260
"Motivated by the human visual system, we propose an infrared dim target detection method that is based on a fuzzy accurate updating symmetric adaptive resonance theory network. From the bottom-up perspective, the regions of interest (ROIs) are extracted using a difference of Gaussians at multiple scales and our designed ROI model in a saliency map. From the top-down perspective, five feature categories are extracted using the ROI model, which are used to train the proposed Fuzzy AUSART network. The well-trained network realizes the true identification of all ROI candidates. The results of the receiver operating characteristic (ROC) curves verify that the proposed method can better adapt to different circumstances and targets in our experiment. The average detection accuracy of the Fuzzy AUSART is improved by 15.4%, and the average F1 index of the proposed method is higher than six typical comparison methods by more than a factor of 2.48.","Infrared dim target detection, Human vision system, Fuzzy accurate updating symmetric adaptive resonance theory, ROI model",Shuai Zhang and Fuyu Huang and Bingqi Liu and Hao Yu and Yichao Chen,https://www.sciencedirect.com/science/article/pii/S1047320319300719,https://doi.org/10.1016/j.jvcir.2019.02.018,1047-3203,2019,180--191,60,Journal of Visual Communication and Image Representation,Infrared dim target detection method based on the fuzzy accurate updating symmetric adaptive resonance theory,article,ZHANG2019180
"Perceptual quality evaluation of multiply distorted images has become a very challenging research topic. In this paper, we present a novel and efficient deep blind quality evaluator for multiply distorted images based on monogenic binary coding (MBC). Local complementary structural information and a deep learning method are employed to blindly evaluate the quality of multiply distorted images. First, a monogenic signal representation is utilized to decompose a multiply distorted image into three complementary components: orientation, phase, and magnitude. The quality-predictive features are then determined from the complementary components. Finally, the features are mapped to the human quality score of the multiply distorted image based on the deep neural network. The results on two newly established multiply distorted image subjective databases confirm that our metric has a better prediction performance than existing state-of-the-art full-reference and classical blind metrics.","Quality assessment, Monogenic binary coding, Local structural information, Blind prediction, Deep neural network",Wujie Zhou and Lu Yu and Yaguan Qian and Weiwei Qiu and Yang Zhou and Ting Luo,https://www.sciencedirect.com/science/article/pii/S104732031930094X,https://doi.org/10.1016/j.jvcir.2019.03.001,1047-3203,2019,305--311,60,Journal of Visual Communication and Image Representation,Deep blind quality evaluator for multiply distorted images based on monogenic binary coding,article,ZHOU2019305
"Compared with facial emotion estimation on categorical model, dimensional emotion estimation can describe numerous emotions more accurately. Most prior works of dimensional emotion estimation only considered laboratory data and used video, speech or other multi-modal features. Compared with other modal data, static images has superiorities of accessibility, which is more conducive to the emotion estimation in real world. In this paper, a two-level attention with two-stage multi-task learning (2Att-2Mt) framework is proposed for facial emotion estimation on only static images. Firstly, the features of corresponding region (position level features) are extracted and enhanced automatically by first-level attention mechanism. Then, we utilize Bi-directional Recurrent Neural Network (Bi-RNN) with self-attention (second-level attention) to make full use of the relationship features of different layers (layer-level features) adaptively. And then, we propose a two-stage multi-task learning structure, which exploits categorical representations to ameliorate the dimensional representations and estimate valence and arousal simultaneously in view of the inherent complexity of dimensional representations and correlation of the two targets. The quantitative results conducted on AffectNet dataset show significant advancement on Concordance Correlation Coefficient(CCC) and Root Mean Square Error (RMSE), illustrating the superiority of the proposed framework. Besides, extensive comparative experiments have also fully demonstrated the effectiveness of different components (2Att and 2Mt) in our framework.","Facial emotion recognition, Attention mechanism, Multi-task learning, Valence-arousal dimension",Wang Xiaohua and Peng Muzi and Pan Lijuan and Hu Min and Jin Chunhua and Ren Fuji,https://www.sciencedirect.com/science/article/pii/S1047320319301646,https://doi.org/10.1016/j.jvcir.2019.05.009,1047-3203,2019,217--225,62,Journal of Visual Communication and Image Representation,Two-level attention with two-stage multi-task learning for facial emotion recognition,article,XIAOHUA2019217
"In this paper, we propose a Reinforcement Learning (RL) based Coding Unit (CU) early termination algorithm for High Efficiency Video Coding (HEVC). RL is utilized to learn a CU early termination classifier independent of depths for low complexity video coding. Firstly, we model the process of CU decision as a Markov Decision Process (MDP) according to the Markov property of CU decision. Secondly, based on the MDP, a CU early termination classifier independent of depths is learned from trajectories of CU decision across different depths with the end-to-end actor-critic RL algorithm. Finally, a CU decision early termination algorithm is introduced with the learned classifier, so as to reduce computational complexity of CU decision. We implement the proposed scheme with different neural network structures. Two different neural network structures are utilized in the implementation of RL based video encoder, which are evaluated to reduce video coding complexity by 34.34% and 43.33%. With regard to BjÃ¸ntegaard delta peak signal-to-noise ratio and BjÃ¸ntegaard delta bit rate, the results are â0.033â¯dB and 0.85%, â0.099â¯dB and 2.56% respectively on average under low delay B main configuration, when compared with the HEVC test model version 16.5.","Coding tree unit, Early termination, High efficiency video coding, Markov decision processing, Actor-critic, Reinforcement learning",Na Li and Yun Zhang and Linwei Zhu and Wenhan Luo and Sam Kwong,https://www.sciencedirect.com/science/article/pii/S1047320319300677,https://doi.org/10.1016/j.jvcir.2019.02.021,1047-3203,2019,276--286,60,Journal of Visual Communication and Image Representation,Reinforcement learning based coding unit early termination algorithm for high efficiency video coding,article,LI2019276
"Text versus non-text region classification is an essential but difficult step in scene-image analysis due to the considerable shape complexity of text and background patterns. There exists a high probability of confusion between background elements and letter parts. This paper proposes a feature-based classification of image blocks using the color autocorrelation histogram (CAH) and the scale-invariant feature transform (SIFT) algorithm, yielding a combined scale and color-invariant feature suitable for scene-text classification. For the evaluation, features were extracted from different color spaces, applying color-histogram autocorrelation. The color features are adjoined with a SIFT descriptor. Parameter tuning is performed and evaluated. For the classification, a standard nearest-neighbor (1NN) and a support-vector machine (SVM) were compared. The proposed method appears to perform robustly and is especially suitable for Asian scripts such as Kannada and Thai, where urban scene-text fonts are characterized by a high curvature and salient color variations.","Text detection in scene images, Text/non-text classification, Color features, Color histogram autocorrelation",Bowornrat Sriman and Lambert Schomaker,https://www.sciencedirect.com/science/article/pii/S1047320319301403,https://doi.org/10.1016/j.jvcir.2019.04.007,1047-3203,2019,23--42,62,Journal of Visual Communication and Image Representation,Multi-script text versus non-text classification of regions in scene images,article,SRIMAN201923
"Recent research on deep residual neural network has made breakthrough in single image super-resolution. However, these studies ignore persistent memory of information in the propagation, which fail to infer plausible high-frequency sufficiently. To solve this problem, we propose a novel enhanced two-phase residual network (ETRN). The proposed ETRN progressively generate high-resolution image in two phases so that we can recover accurate texture. We also present dense residual unit to capture hierarchical features in the local residual learning phase. With the help of these high-level representations, we use global residual learning to improve reconstruction quality in the second phase. In order to reduce cumbersome computation, between two phases we introduce dilated convolution which enlarges receptive field without increasing additional parameters. Comprehensive experiments show that ETRN has a significant improvement in both quantitation and visual perception.","Super-resolution, Convolutional neural network, Deep residual learning, Dilated convolution",Juan Yang and Wenjing Li and Ronggui Wang and Lixia Xue and Min Hu,https://www.sciencedirect.com/science/article/pii/S104732031930135X,https://doi.org/10.1016/j.jvcir.2019.04.002,1047-3203,2019,188--197,61,Journal of Visual Communication and Image Representation,Enhanced two-phase residual network for single image super-resolution,article,YANG2019188
"The human-computer communication adds to worldwide access to training, information upgrade and the conveyance of value learning and instructions. Furthermore, the changing paradigm of machine perceiving the human emotions effectively is a technological advancement for the learning community. The automatic recognition of emotions reflected from speech and facial expressions for making the machine to understand the human verbal and non-verbal emotions is coined as âEmotion Recognitionâ. Emotion recognition systems, regardless, are not totally seen as subject independent dynamic features, so they are not sufficiently vigorous for constant acknowledgment assignments with subject assortment (human face), head movement, illumination change, speech, noise and so on when thought about exclusively. Hence, it is proposed to fuse the features of facial expressions and speech. The present framework utilizes the Speech (Mel Frequency Cepstral Coefficients) features and Facial (Maximally Stable Extremal Regions) features to predict the emotions of a person through a systematic and scientific study. Specifically, when combining MSER with MFCC, the recognition rates can be further improved by 2 to 3% on Indian Face Database and Berlin Speech Database.","Emotion recognition, MFCC, MSER, Speeded Up Robust Features (SURF), SVM, Viola Jones",K. {Prasada Rao} and M.V.P. {Chandra Sekhara Rao} and N. {Hemanth Chowdary},https://www.sciencedirect.com/science/article/pii/S1047320319300951,https://doi.org/10.1016/j.jvcir.2019.03.002,1047-3203,2019,339--345,60,Journal of Visual Communication and Image Representation,An integrated approach to emotion recognition and gender classification,article,PRASADARAO2019339
"Traditional saliency model usually utilize handcrafted image features and various prior knowledge to pop out salient regions from complex surroundings. In this paper, we propose a novel FCN-like deep convolutional neural network for pixel-wise salient object detection. Our deep network automatically learns multi-level feature from different convolutional layers of a pre-trained convolutional neural network. Moreover, deeper side outputs are connected to the shallower ones, which provides a better feature representation and helps shallow side outputs to accurately locate salient regions. In addition, we adopt a weighted-fusion module to combine different side outputs for utilizing multi-scale and multi-level features. Finally, a fully connected CRF model can be optimally incorporated to improve spatial coherence and contour localization in the fused saliency map. Both qualitative and quantitative evaluations on four publicly available datasets demonstrate the robustness and efficiency of our proposed approach against 17 state-of-the-art methods.","Saliency detection, Salient object detection, Fully convolutional neural network, Multi-scale features",Qing Zhang and Jiajun Lin and JingJing Zhuge and Wenhao Yuan,https://www.sciencedirect.com/science/article/pii/S1047320319300434,https://doi.org/10.1016/j.jvcir.2019.01.034,1047-3203,2019,415--424,59,Journal of Visual Communication and Image Representation,Multi-level and multi-scale deep saliency network for salient object detection,article,ZHANG2019415
"In recent years, with the rapid development of economy and the acceleration of urbanization process, building energy consumption has become an urgent problem to be solved. Its total energy consumption shows a trend of sustained growth, and the growth rate is faster and faster, so the requirements of building energy efficiency detection technology are higher and higher. Aiming at the problems of time-consuming and difficult detection of traditional detection methods, this paper introduces infrared image processing technology and proposes an energy-saving detection method for buildings based on image processing. The air tightness and heat transfer coefficient are studied respectively. Firstly, the feasibility of infrared method to detect air tightness is theoretically analyzed, and the corresponding technical scheme and detection process are given. Secondly, the infrared image is grayed and filtered to eliminate the influence of thermal defect area on the calculation of heat transfer coefficient after eliminating interference noise. Finally, based on the one-dimensional steady heat transfer theory, the principle of quantitative detection of heat transfer coefficient in infrared image is given by studying the heat transfer process of the wall, and the related correction method is designed. The feasibility and superiority of this method are illustrated by an example. Therefore, this method has good application prospects in building energy-saving detection.","Building energy efficiency, Image processing, Air tightness, Heat transfer coefficient",Wei Cai and Xiaodong Wen and Saisai Wang and Lijuan Wang,https://www.sciencedirect.com/science/article/pii/S1047320319300835,https://doi.org/10.1016/j.jvcir.2019.02.032,1047-3203,2019,295--304,60,Journal of Visual Communication and Image Representation,A real-time detection method of building energy efficiency based on image processing,article,CAI2019295
"Humanâs perception plays a very important role on image assessment, especially for stereoscopic images. In general, viewing stereoscopic 3D images will cause visual fatigue, eyestrain, dizziness or headache. Therefore, how to evaluate humanâs perception of visual quality on 3D images becomes an emerging topic. In this paper, we propose a no-reference assessment metric for stereoscopic image quality of experience (QoE). First, the stereoscopic image pairs are used to calculate the disparity maps by optical flow estimation. Then the depth information are extracted from the disparity map, called as disparity-depth map. Next, we extract four types of features based on pixel value and distribution of disparity-depth map. Two regression models are used to predict visual discomfort scores. Also, we test the proposed method on EPFL 3D image database and IEEE-SA stereoscopic image database, respectively. The experiment results show that our proposed QoE assessment metric achieves excellent performance compared with state-of-the-art methods.","Disparity-depth map, No reference, Quality of experience, Stereoscopic image, Visual discomfort",Tsung-Jung Liu and Kuan-Hsien Liu and Kuan-Hung Shen,https://www.sciencedirect.com/science/article/pii/S1047320319301373,https://doi.org/10.1016/j.jvcir.2019.04.004,1047-3203,2019,272--283,61,Journal of Visual Communication and Image Representation,Learning based no-reference metric for assessing quality of experience of stereoscopic images,article,LIU2019272
"A new (k,n) threshold secret image sharing scheme with two decoding options is introduced in this paper. The proposed scheme combines color-black-and-white visual cryptography scheme (CBW-VCS) and polynomial based secret image sharing (PSIS) together, to offer stacking-to-see decryption and lossless image reconstruction. To construct the color shares, a general (k,n) threshold CBW-VCS is firstly given. A grayscale secret image is converted to a p-radix image and a binary image. The p-radix image is encrypted by the (k,n) PSIS under mod p operation, and n p-radix shadows are obtained. Then, a color share generation algorithm with data embedding is proposed to construct n color shares. Theoretical analysis and sufficient experiments are provided to show the effectiveness and advantages of the proposed scheme.","Visual cryptography, Secret image sharing, Color, Embedding, Polynomial, Black and white",Xiaotian Wu and Ching-Nung Yang,https://www.sciencedirect.com/science/article/pii/S1047320319301130,https://doi.org/10.1016/j.jvcir.2019.03.020,1047-3203,2019,74--84,61,Journal of Visual Communication and Image Representation,A combination of color-black-and-white visual cryptography and polynomial based secret image sharing,article,WU201974
"Text line segmentation is a key step in Tibetan historical document recognition. A novel method for text line segmentation was proposed based on the baseline in uchen Tibetan, and a new dataset was released, which was used to evaluate the results of text line segmentation of uchen Tibetan historical documents. In this paper, there were two steps for the proposed method: baseline detection and text line segmentation using the baseline. In baseline detection, the upper edges of all characters in the document were obtained by a horizontal gradient operator, then an edge connectivity definition was proposed by which the upper edge set was divided into disjoint subsets. Eligible sets were selected from these subsets, and the edges in these sets were joined in turn to obtain the baseline. In text line segmentation, the document image was truncated at the baseline position, then the adhesion regions were segmented again. Each connected region in the image was assigned to its nearest baseline. All connected regions belonging to the same baseline formed a text line. Experiments on the proposed dataset showed that the method could effectively avoid document distortion, the accuracy of text line segmentation was high, and the text line adhesion could be handled.","Tibetan historical document, Text line segmentation, Baseline, Upper edge, Connected region analysis, Dataset, Image processing",Zhenjiang Li and Weilan Wang and Yang Chen and Yusheng Hao,https://www.sciencedirect.com/science/article/pii/S1047320319300288,https://doi.org/10.1016/j.jvcir.2019.01.021,1047-3203,2019,23--32,61,Journal of Visual Communication and Image Representation,A novel method of text line segmentation for historical document image of the uchen Tibetan,article,LI201923
"This paper investigates the possibility of avoiding the HEVC intra-frame drift effect induced by additive modifications of the luma DCT/DST coefficients. The main novelty consists in solving the intra-drift problem in its general form: starting from the equations defining the 35 intra-prediction modes and the DCT/DST computation, a mask multiplicative matrix cancelling the intra-frame drift effect is computed. The relevance of the advanced method with respect to the state of the art studies is illustrated, discussed and quantitatively assessed.","HEVC, Additive modification, Intra-drift free, Theoretical computation, Masking matrix",A. Arige and M. Mitrea and I. Boujelbane,https://www.sciencedirect.com/science/article/pii/S1047320319301531,https://doi.org/10.1016/j.jvcir.2019.04.014,1047-3203,2019,56--67,62,Journal of Visual Communication and Image Representation,HEVC intra-frame drift cancellation matrix,article,ARIGE201956
"Disparity estimation in ill-posed regions, such as occlusions, repetitive patterns and textureless regions, is a challenging problem in stereo matching. The initial disparities obtained in these regions tend to be regarded as outliers that must be detected and addressed. In this paper, two outlier detection methods are proposed, i.e., the efficient approach and the accurate approach. The efficient approach detects outliers by exploring the disparity map for the left image only and reduces runtime and memory costs. First, the match fixed point jumps (MFPJ) algorithm is proposed as an initial solution to detect outliers. Then, a high-probability outlier detection algorithm is proposed to accomplish denser outlier detection with less noise. The accurate approach first classifies outliers as occlusions or mismatches. Then, 3D label assignment is performed for occlusion outliers and normal-based plane fitting is conducted for mismatch outliers to refine the disparities of the outliers and to achieve an accurate stereo matching result. Evaluations of the Middlebury datasets demonstrate that the proposed methods effectively improve the stereo matching performance.","Outlier detection, Stereo matching, Match fixed point jumps, Normal-based plane fitting",Qicong Dong and Jieqing Feng,https://www.sciencedirect.com/science/article/pii/S1047320319301014,https://doi.org/10.1016/j.jvcir.2019.03.007,1047-3203,2019,380--390,60,Journal of Visual Communication and Image Representation,Outlier detection and disparity refinement in stereo matching,article,DONG2019380
"In this work, we present a fully connected convolutional encoder-decoder for defects detection in archived video. The proposed method handles the detection of two of the most common archived video-related defects, namely blotches and scratches. It consists of two stages: (1) pixel-level classification and description of each video frame into defects pixels or not, by means of a novel CNN-based encoder-decoder architecture, and (2) spatio-temporal analysis to group and fine-tune the detections. For blotch detection, the learned features, extracted from an intermediate stage of the network, are used to evaluate the dissimilarity between the pre-selected regions in consecutive frames. For scratches detection, the morphology of scratches is used to eliminate false alarms. The experiments are performed on various video sequences suffering synthetic and real scratches and blotches. The results demonstrate the effectiveness of our approach and significant improvement against the most recent detectors.","Digital archived video restoration, Defects detection, Convolutional neural network, Deep learning",Hamza Yous and Amina Serir and Sofiane Yous,https://www.sciencedirect.com/science/article/pii/S1047320319300562,https://doi.org/10.1016/j.jvcir.2019.02.005,1047-3203,2019,486--500,59,Journal of Visual Communication and Image Representation,CNN-based method for blotches and scratches detection in archived videos,article,YOUS2019486
"Traditional block compressed sensing (CS) schemes encode nature images via a fixed sampling rate without taking the sparsity level differences among the blocks into consideration. In order to improve sampling efficiency, permutation-based block CS (BCS) schemes are proposed. In these schemes, the crux is to find a good permutation strategy which can make the nonzero entries evenly distributed among the blocks. In order to make the nonzero entries distributed among the blocks as evenly as possible, a novel matrix permutation strategy is proposed in this paper. Then, a BCS scheme with matrix permutation (BCS-MP) is proposed, which can be utilized to encode nature images effectively. Simulation results show that the proposed approach gets a significant gain of peak signal-to-noise ratio (PSNR) of the reconstructed-images compared with the state-of-the-art permutation-based ones and the traditional non-permutation one at the cost of slightly increasing the encoding time.","Matrix permutation, Image compression, Image coding, Block compressed sensing",Bo Zhang and Yulin Liu and Jie Zhuang and Kai Wang and Yuqiang Cao,https://www.sciencedirect.com/science/article/pii/S1047320319300707,https://doi.org/10.1016/j.jvcir.2019.02.023,1047-3203,2019,69--78,60,Journal of Visual Communication and Image Representation,Matrix permutation meets block compressed sensing,article,ZHANG201969
"This paper presents a fast filter based on a modified Lee filter. The smoothing operation in the proposed filter is a linear combination of the center pixel value and the average of pixel values in a window. Based on the new definitions of edge and noise, the local gradient mean is introduced to compute the coefficient of the linear combination. Besides, a multi-scale method is employed to smooth the pixel values near edges. The main advantage of the proposed filter is its computational cost. In addition to some point operations, only several mean filters are taken in this filter. No matter how large the window size is, the time complexity is O(N) (N is the pixel number). Experimental results have shown that the proposed filter can effectively smooth images while keeping edges well, thus can be widely used in computer vision and image processing.","Filter, Linear combination, Multi-scale, Edge preserving",Wujing Li and Wei He and Xianfeng Ou and Wenjing Hu and Jianhui Wu and Guoyun Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319301634,https://doi.org/10.1016/j.jvcir.2019.05.008,1047-3203,2019,226--233,62,Journal of Visual Communication and Image Representation,Fast combination filtering based on weighted fusion,article,LI2019226
"Strain sensors are widely applied in the industry requiring specific signal amplifiers and signal readers. However, the devices are unable to read data quickly and intuitively. The present study introduces a highly accurate, repeatable, mold-free strain sensor comprising of an amplification mechanism and a smart-phone microscope. Generation of the amplification mechanism using rapid prototyping especially 3D printing is summarized and evaluated. The sensitivity of the proposed amplifier is (39.47â¯Â±â¯1.34) for nylon and (37.74â¯Â±â¯2.41) for ABS. A visual polydimethylsiloxane (PDMS) lens with a focal length of 7.23â¯mm is attached to the iphone6 camera, performing as a microscope for image acquisition, which provides an equivalent focal length of 6.7â¯mm and a resolution of 0.691â¯Âµm.","Strain measurement, Amplification mechanism, Visual PDMS lens, Smart-phone microscope",Qiu-Yue Du and Bin Tian,https://www.sciencedirect.com/science/article/pii/S1047320319300422,https://doi.org/10.1016/j.jvcir.2019.01.036,1047-3203,2019,380--386,59,Journal of Visual Communication and Image Representation,"A low-cost, accurate strain measurement using multi-view amplification mechanism and visual polydimethylsiloxane lens",article,DU2019380
"The rapid development of social media services has spawned abundant user generated contents (UGC), such as Sina Weibo, which is one of the biggest Chinese microblogging platforms. In order to enhance the quality and popularity of the posted weibo (the microblog), Weibo users usually embed some social information and images or micro-videos, namely the multimodal weibo, and we assume that there is a close correlation among the multimodal weibo data, especially between the visual data (image/micro-video) and its corresponding text, for a multimodal weibo of high quality. Hence, we try to evaluate the quality of multimodal weibo via analyzing the correlation in the multimodal weibo. This paper constructs the classification model based on back propagation (BP) neural network with genetic algorithm (GA), to automatically identify the correlation within the multimodal weibo, and investigates three kinds of features from multimodal weibo to uncover their contributions to the correlation. The experimental results verify the superiority of the GA-BP based classification model over the traditional BP neural network.","Multimodal weibo, Correlation classification model, Back propagation, Genetic algorithm, GA-BP",Maofu Liu and Weili Guan and Jie Yan and Huijun Hu,https://www.sciencedirect.com/science/article/pii/S1047320319300665,https://doi.org/10.1016/j.jvcir.2019.02.015,1047-3203,2019,312--318,60,Journal of Visual Communication and Image Representation,Correlation identification in multimodal weibo via back propagation neural network with genetic algorithm,article,LIU2019312
"Temporal action localization is an important task of computer vision. Though many methods have been proposed, it still remains an open question how to predict the temporal location of action segments precisely. Most state-of-the-art works train action classifiers on video segments pre-determined by action proposal. However, recent work found that a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. In this paper, we propose a Frame Segmentation Network (FSN) that places a temporal CNN on top of the 2D spatial CNNs. Spatial CNNs are responsible for abstracting semantics in spatial dimension while temporal CNN is responsible for introducing temporal context information and performing dense predictions. The proposed FSN can make dense predictions at frame-level for a video clip using both spatial and temporal context information. FSN is trained in an end-to-end manner, so the model can be optimized in spatial and temporal domain jointly. We also adapt FSN to use it in weakly supervised scenario (WFSN), where only video level labels are provided when training. Experiment results on public dataset show that FSN achieves superior performance in both frame-level action localization and temporal action localization.","Action detection, Temporal action localization, Convolutional Neural Network",Ke Yang and Xiaolong Shen and Peng Qiao and Shijie Li and Dongsheng Li and Yong Dou,https://www.sciencedirect.com/science/article/pii/S1047320319300549,https://doi.org/10.1016/j.jvcir.2019.02.003,1047-3203,2019,296--302,61,Journal of Visual Communication and Image Representation,Exploring frame segmentation networks for temporal action localization,article,YANG2019296
"A new saliency prediction method via extracting topological feature and calculating Mahalanobis distance on deep color components is presented in this paper. Specifically, four selectable schemes of color components are considered and a deep convolutional network is used to learn the best scheme. Then the topological feature maps of an input image are extracted on the learned color components by the analysis of connectivity and adjacency. To achieve the final saliency map, a new fusion method is proposed by calculating the Mahalanobis distance between the feature maps and their means with their covariance matrices rather than summating the feature maps linearly. The numerical and visual evaluation shows that a competitive performance compared with fourteen state-of-the-art models is achieved by the proposed method.","Saliency, Deep color components, Topological feature, Covariance matrix, Mahalanobis distance",Jiazhong Chen and Qingqing Li and Ping Li and Yu Han and Lei Wu and Hefei Ling and Weimin Wu,https://www.sciencedirect.com/science/article/pii/S104732031930077X,https://doi.org/10.1016/j.jvcir.2019.02.026,1047-3203,2019,149--157,60,Journal of Visual Communication and Image Representation,Saliency prediction by Mahalanobis distance of topological feature on deep color components,article,CHEN2019149
"The popularity of 3D screen content videos like 3D animations calls for better coding performance of screen content videos. The exiting R-Î» rate control model is designed to enable the coding performance of conventional nature videos, which can not accurately describe the Rate-Distortion (R-D) relationships of the screen content videos, especially for videos containing limited colors and sharper edges. This paper proposes a new rate control (RC) scheme which takes the divergent characteristics of text contents, screen images and nature images in screen content videos into consideration. In particular, the content-based rate control algorithm is developed at Coding Tree Unit (CTU) level, where CTUs are divided into three categories including Text-CTUs (T-CTUs), Screen Image-CTUs (SI-CTUs) and Nature Image-CTUs (NI-CTUs). Three R-Î» models reflecting different R-D relationships of different CTUs are established, and the model parameters are updated on the basis of their own models. Furthermore, in view of the fact that continuity and mutability simultaneously exist in screen content videos, two bit allocation schemes for regions containing only one content are determined in this paper. Experimental results show that the proposed RC scheme achieves 0.749â¯dB BDPSNR increase under the Low Delay configuration and 0.637â¯dB BDPSNR increase under the Random Access configuration on average, compared to the default R-Î» model in HEVC Screen Content Coding extension (HEVC-SCC) reference software.","Screen content video, Rate control, Parameter update, Bit allocation",Yang Yang and Liquan Shen and Hao Yang and Ping An,https://www.sciencedirect.com/science/article/pii/S1047320319300823,https://doi.org/10.1016/j.jvcir.2019.02.031,1047-3203,2019,328--338,60,Journal of Visual Communication and Image Representation,A content-based rate control algorithm for screen content video coding,article,YANG2019328
"Optical coherence tomography (OCT) can achieve the high-resolution 3D tomography imaging of the retina, which is crucial for the diagnosis of retinal diseases. Currently, the classification of retinal OCT images is mainly conducted by ophthalmologists, which is time consuming and subjective. In this paper, we propose an iterative fusion convolutional neural network (IFCNN) method for the automatic classification of retinal OCT image. In the convolutional neural network (CNN), different convolutional layers contain feature information from different scales. Therefore, the proposed network adopts an iterative fusion strategy, which iteratively combines features in current convolutional layer with those of all previous layers in the CNN network, and thus can jointly utilize the features of different convolutional layers to achieve accurate classification of OCT images. Experimental results on a real retinal OCT dataset and a musculoskeletal radiographs dataset demonstrate the superiority of the proposed method over the traditional CNN and several well-known OCT classification methods.","Classification, Convolutional neural network (CNN), Deep learning, Optical coherence tomography (OCT), Retinal",Leyuan Fang and Yuxuan Jin and Laifeng Huang and Siyu Guo and Guangzhe Zhao and Xiangdong Chen,https://www.sciencedirect.com/science/article/pii/S1047320319300306,https://doi.org/10.1016/j.jvcir.2019.01.022,1047-3203,2019,327--333,59,Journal of Visual Communication and Image Representation,Iterative fusion convolutional neural networks for classification of optical coherence tomography images,article,FANG2019327
"In this paper, we consider the problem of subspace clustering for image data under occlusion and gross spatially contiguous noise. The state of the art subspace clustering methods assume that the noise either follows independent Laplacian or Gaussian distributions. However, the realistic noise is much more complicated and exhibits different structures in different scales. To address this issue, we propose a multi-scale framework that extracts a clean self-expressive dictionary through an iterative approach and is capable of identifying probable corrupted elements in each sample. Using this information, not only we can estimate parameters of each subspace more accurately but also by optimizing a matrix completion problem based on group sparsity, we can recover corrupted regions more precisely and hence achieve higher clustering accuracy for corrupted samples. Numerical experiments on synthetic and real world data sets demonstrate the efficiency of our proposed framework in presence of occlusion and spatially contiguous noise.","Subspace estimation, Sparse representation, Sparse subspace clustering, Group lasso, Multi-scale estimation, Matrix completion",Maryam Abdolali and Mohammad Rahmati,https://www.sciencedirect.com/science/article/pii/S1047320319301361,https://doi.org/10.1016/j.jvcir.2019.04.003,1047-3203,2019,303--314,61,Journal of Visual Communication and Image Representation,Robust subspace clustering for image data using clean dictionary estimation and group lasso based matrix completion,article,ABDOLALI2019303
"Food plays an important role in several aspects of our daily life. Several computer vision approaches have been proposed for tackling food analysis problems, but very little effort has been done in developing methodologies that could take profit of the existent correlation between tasks. In this paper, we propose a new multi-task model that is able to simultaneously predict different food-related tasks, e.g. dish, cuisine and food categories. Here, we extend the homoscedastic uncertainty modeling to allow single-label and multi-label classification and propose a regularization term, which jointly weighs the tasks as well as their correlations. Furthermore, we propose a new Multi-Attribute Food dataset and a new metric, Multi-Task Accuracy. We prove that using both our uncertainty-based loss and the class regularization term, we are able to improve the coherence of outputs between different tasks. Moreover, we outperform the use of task-specific models on classical measures like accuracy or F1.","Multi-task models, Uncertainty modeling, Convolutional neural networks, Food image analysis, Food recognition, Food group recognition, Ingredients recognition, Cuisine recognition",Eduardo Aguilar and Marc BolaÃ±os and Petia Radeva,https://www.sciencedirect.com/science/article/pii/S1047320319301002,https://doi.org/10.1016/j.jvcir.2019.03.011,1047-3203,2019,360--370,60,Journal of Visual Communication and Image Representation,Regularized uncertainty-based multi-task learning model for food analysis,article,AGUILAR2019360
"Prediction of visual saliency in images and video is needed for video understanding, search and retrieval, coding, watermarking and other applications. The majority of prediction models are founded only on âbottom-upâ features. Nevertheless, the âtop-downâ component of human visual attention becomes prevalent as human observers explore the visual scene. Visual saliency which is always a mix of bottom-up and top-down cues can be predicted on the basis of seen data. In this paper, a model of prediction of visual saliency in video on the basis of Deep convolutional neural networks (CNNs) is proposed. A Deep CNN architecture is designed. Various input channels for a CNN architecture are studied: using the known sensitivity of human visual system to residual motion, pixel colour values are completed with residual motion map. The latter is a normalized energy of residual motion in video frames with regard to the estimated global affine motion model. The experiments show that the choice of the input features for the Deep CNN depends on visual task: for highly dynamic content, the proposed model with residual motion is more efficient and gives decent results with relatively shallow Deep architecture.","Visual attention prediction in video, Deep convolutional neural networks, Saliency map, Residual motion, Dynamic content",Souad Chaabouni and Jenny Benois-Pineau and Chokri {Ben Amar},https://www.sciencedirect.com/science/article/pii/S1047320319300550,https://doi.org/10.1016/j.jvcir.2019.02.004,1047-3203,2019,79--93,60,Journal of Visual Communication and Image Representation,ChaboNet : Design of a deep CNN for prediction of visual saliency in natural video,article,CHAABOUNI201979
"In many real-world applications, images are prone to be degraded by contrast distortions during image acquisition. Quality assessment for contrast-distorted images is vital for benchmarking and optimizing the contrast-enhancement algorithms. To this end, this paper proposes a no-reference quality metric for contrast-distorted images based on Multifaceted Statistical representation of Structure (MSS). The âMultifacetedâ has two meanings, namely (1) not only the luminance information, but also the chromatic information is used for structure representation. This is inspired by the fact that the chromatic information on the one hand affects the perception of image quality as well, and on the other hand it changes along with the contrast distortions. Therefore, the chromatic information should be integrated with the luminance information for quality assessment of contrast-distorted images, a fact most existing quality metrics overlook; (2) regarding structure representation, three aspects, i.e. spatial intensity, spatial distribution, and orientation of structures are calculated, which is enlightened by the fact that the human visual system (HVS) is sensitive to the three aspects of structures. Specifically, the image is first transformed from RGB to the S-CIELAB color space to obtain a representation that is more consistent with the characteristics of the HVS, as well as to separate the chromatic information from the luminance. Then the statistical structural features are extracted from both luminance and chromatic channels. Finally, the back propagation (BP) neural network is adopted to train a quality prediction model. Experimental results conducted on four public contrast-distorted image databases demonstrate the superiority of the proposed method to the relevant state-of-the-arts.","Quality assessment, No-reference, Contrast-distorted images, Structure representation, Back propagation (BP)",Yu Zhou and Leida Li and Hancheng Zhu and Hantao Liu and Shiqi Wang and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S1047320319300793,https://doi.org/10.1016/j.jvcir.2019.02.028,1047-3203,2019,158--169,60,Journal of Visual Communication and Image Representation,No-reference quality assessment for contrast-distorted images based on multifaceted statistical representation of structure,article,ZHOU2019158
"Aiming at the detection and segmentation of underwater Continuous wave-like (CW-like) signals of under strong noise sea background, this paper introduces a Gaussian mixture model clustering method by analyzing the signal spectrum features. First, the time domain original signal is converted to its frequency domain correspondence by Windowed Fast Fourier Transform. Second, by studying on the feature of target signal, we introduce a spectrum filtering to alleviate the background noise of ocean environment, which is analyzed and calculated with both time and frequency information. Then, the target echo location signals is constructed using a Gaussian mixture mode based binary clustering algorithm. Finally, we use the EM algorithm and adaptive binarization for solving and optimizing the clustering results. Experimental results have shown the accuracy and efficiency of our detection, which can be also simply modified and applied for detecting similar and specific signal from complex background noise.","CW-like signal, Signal spectrum analysis, Gaussian mixture model, Clustering analysis, Signal detection",Zhaolin Xiao and Meng Zhang and Lisheng Chen and Haiyan Jin,https://www.sciencedirect.com/science/article/pii/S1047320319300938,https://doi.org/10.1016/j.jvcir.2019.02.036,1047-3203,2019,287--294,60,Journal of Visual Communication and Image Representation,Detection and segmentation of underwater CW-like signals in spectrum image under strong noise background,article,XIAO2019287
"Ancient villages are the carrier of a nation's profound history and culture. They are the integration of history, culture, architecture and sculpture, and have many value attributes. With the development of society, the protection of ancient villages is very important. The establishment of digital archives is an important means to protect ancient villages. Because of the large number and wide distribution of ancient villages, crowd sourcing can quickly and widely access the digital resources of ancient villages. Because of the uneven quality and repetition of the images collected from ancient villages, it is necessary to screen the images of ancient villages. Therefore, this paper proposes a screening model of ancient villages based on SIFT and convolution neural network. Firstly, this paper chooses edge gray change rate and NIQE quality score to evaluate the quality of ancient village image; secondly, extracts SIFT features of image for matching, calculates matching similarity to determine whether the matched image is myopic repetition. Finally, the image is filtered or preserved by using convolution neural network with the edge gray change rate, NIQE quality score and some image attributes as features. Experiments show that the ancient village image screening model designed in this paper has higher accuracy and recall rate than other methods, and has better screening effect.","Ancient villages, Image screening, SIFT, Convolutional neural network",Ying Huang and Qingping Zhang,https://www.sciencedirect.com/science/article/pii/S104732031930080X,https://doi.org/10.1016/j.jvcir.2019.02.029,1047-3203,2019,33--41,61,Journal of Visual Communication and Image Representation,Research on image screening model of ancient villages,article,HUANG201933
"Hierarchical models have shown their effectiveness for action recognition. However, most of the existing hierarchy construction methods fail to model the complex motion patterns in videos, and thus are vulnerable to the interference of the inevitable noise in action videos. Therefore, we propose a Dynamic Hierarchical Tree (DHT) model to characterize such complex motion for better recognition performance. First, a minimum maximum DTW (mmDTW) is developed to produce more stable atomic actions by limiting the minimum and maximum lengths of atomic actions. Then an aggregation method is utilized to construct a DHT for each video by merging atomic actions from bottom to top. Not only the similarity between frames but also the compatibility of dynamic evolution between frames and segments is exploited for the mmDTW and the aggregation process, making the DHTs more suitable for modeling actions in videos. Finally, a k-Nearest Neighbors Edge Pairs (kNNEP) kernel is proposed to compare two DHTs by using the mean similarity of k nearest neighbors edge pairs.","Action recognition, Hierarchical modeling, Evolution, Tree kernel",Tingwei Wang and Peng Duan and Bingxian Ma and Peng Wu and Weizhi Lu,https://www.sciencedirect.com/science/article/pii/S1047320319301348,https://doi.org/10.1016/j.jvcir.2019.04.001,1047-3203,2019,315--325,61,Journal of Visual Communication and Image Representation,Action recognition using dynamic hierarchical trees,article,WANG2019315
"Sum-Product Network (SPN) are recently introduced deep tractable Probabilistic Graphical Models providing exact and tractable inference. SPN have been successfully employed as density estimators in some artificial intelligence fields, however, most of the proposed structure learning algorithms focus on improving the performance of a certain aspect of model, at the cost of reducing other performance. This is due to the fact that there is no effective balance between network width and depth during learning process. In this paper, we propose two clustering analysis algorithms to replace the clustering part of LearnSPN. We improve the structure quality of the generated model by deepening the network while adjusting the network width adaptively, trying to find a balance between the expressive power, representation ability, inference accuracy and simplicity. Experimental results prove that LearnSPN equipped by our clustering method has different degrees of improvement in various performances.","Machine learning, Deep learning, Sum-product network, Structure learning",Yang Liu and Tiejian Luo,https://www.sciencedirect.com/science/article/pii/S1047320319300653,https://doi.org/10.1016/j.jvcir.2019.02.012,1047-3203,2019,391--397,60,Journal of Visual Communication and Image Representation,The optimization of sum-product network structure learning,article,LIU2019391
"Person re-identification is a cross-camera retrieval task. Person re-identification performance in a single dataset has been significantly improved, but person re-identification model trained in one dataset usually can't work well in another dataset. To solve this problem, this paper proposes a method of image-to-image translation, CTGAN (Multi-Camera Transfer GAN), which can be performed on multiple camera domains of pedestrian dataset by using one single model. The marked training images are transferred to each camera of the target dataset. At the same time, for the feature learning model, this paper adopts the MSCDA (Mixed Selective Convolution Descriptor Aggregation) method, which can locate the main pedestrian objects in the image, filter out the background noise, and keep the useful depth descriptor. In the paper, experiments show that the method is effective.","Generative adversarial networks, Deep learning, Computer vision, Person re-identification",Shuren Zhou and Maolin Ke and Peng Luo,https://www.sciencedirect.com/science/article/pii/S1047320319300379,https://doi.org/10.1016/j.jvcir.2019.01.029,1047-3203,2019,393--400,59,Journal of Visual Communication and Image Representation,Multi-camera transfer GAN for person re-identification,article,ZHOU2019393
"In recent years, with the rapid development of computer technology and network technology, computer vision has been widely used in various scientific fields. Human motion recognition, as an important branch of computer vision, is essentially to classify human motion information in motion images correctly. It has great significance in intelligent monitoring and security, human-computer interaction, motion analysis and other fields. At present, there are still some problems in human motion recognition methods. Firstly, how to extract and characterize the motion information in images has been one of the difficulties in this field; secondly, with the appearance of kinect and other depth cameras, researchers have provided the depth information of human motion images, and how to effectively use these depth information to achieve human motion recognition and classification is also an important research issue; finally, when the amount of sample data is small, how to use the deep learning network model to achieve a higher human motion recognition rate? Based on UTD-MHAD database, this paper studies the human motion recognition of RGB image and depth image captured simultaneously by kinect, and carries out relevant discussion and analysis on the above problems, using micro-inertial sensors (MTi-G-700 developed by Xsens and Android mobile phones, tablets and other personal mobile devices come with MEMS gyroscopes and accelerometers) to correct the image to motion blur, build a new mathematical model, use the inertial data obtained by MIMU in a short time to estimate the position, attitude and speed of camera motion, correct the image pixel position, perform image de-motion blur processing, and then perform image processing such as denoising to solve the image motion blur problem. A new algorithm is developed and its science is verified by MATLAB simulation.","Deep learning, Image processing, Blurred image",Yanfen Chang,https://www.sciencedirect.com/science/article/pii/S1047320319300811,https://doi.org/10.1016/j.jvcir.2019.02.030,1047-3203,2019,371--379,60,Journal of Visual Communication and Image Representation,Research on de-motion blur image processing based on deep learning,article,CHANG2019371
"Extracting efficient features from the large volume of 3D facial data directly is extremely difficult in 3D face recognition (3D-FR) with the latest methods, which mostly require heavy computations and manual processing steps. This paper presents a computationally efficient 3D-FR system based on a novel Frenet frame-based feature that is derived from the 3D facial iso-geodesic curves. In terms of the evaluation of the proposed method, we conducted a number of experiments on the CASIA 3D face database, and a superior recognition performance has been achieved. The performance evaluation suggests that the pose invariance attribute of the features relieves the need of an expensive 3D face registration in the face preprocessing procedure, where we take less time to process conversely. Our experiments further demonstrate that the proposed method not only achieves competitive recognition performance when compared with some existing techniques for 3D-FR, but also is computationally efficient.","Facial curves, Frenet framework, 3D face recognition, Iso-geodesic, Pose invariant",Biao Shi and Huaijuan Zang and Rongsheng Zheng and Shu Zhan,https://www.sciencedirect.com/science/article/pii/S1047320319300537,https://doi.org/10.1016/j.jvcir.2019.02.002,1047-3203,2019,455--460,59,Journal of Visual Communication and Image Representation,An efficient 3D face recognition approach using Frenet feature of iso-geodesic curves,article,SHI2019455
"Facial expression recognition is an interesting and challenging problem in computer vision. So far, much research has been performed in this area; however, facial expression recognition in uncontrolled conditions has remained an unresolved problem. The widely-used feature descriptors in computer vision are often histogram data. In this paper, a new metric learning method is presented for histogram data classification. In this method, chi-squared distance is appropriately modified for metric learning. Then, a convex cost function is proposed to use in metric learning optimization. Moreover, the proposed algorithm is redefined as Local Metric Learning for facial expression recognition problem. In this definition, the proposed metric learning method is applied locally on facial sub-regions. Experimental results on four histogram datasets (dslr, webcam, amazon, and caltech) as well as controlled and uncontrolled facial expression recognition datasets (CK+, SFEW, and RAF-DB) show that the proposed method has superior performance compared to the state-of-art methods.","Metric learning, Local metric learning, Chi-squared distance, Histogram classification, Facial expression recognition in the wild",Hamid Sadeghi and Abolghasem-A. Raie,https://www.sciencedirect.com/science/article/pii/S1047320319301592,https://doi.org/10.1016/j.jvcir.2019.05.004,1047-3203,2019,152--165,62,Journal of Visual Communication and Image Representation,Histogram distance metric learning for facial expression recognition,article,SADEGHI2019152
"Wushu is an outstanding cultural heritage of the Chinese nation and one of the most extensive mass sports in China. As a traditional sports event in China, martial arts are undergoing rapid changes. However, martial arts are a systemic sport with high requirements for speed, explosiveness and coordination. In recent years, with the rapid development of martial arts, competitive competitions have become increasingly fierce. It is easy for athletes to suffer physical damage during the practice of difficult movements. This not only affects the normal exercise and physical health of martial arts enthusiasts, but also affects the improvement of sports level and teaching quality. Therefore, it studies the common parts of martial arts sports injuries. Distribution, looking for its causes, proposing preventive measures, rapid development of image processing technology, digital image has become an indispensable part of multimedia information technology. Digital image is an important carrier for people to obtain information and communicate. Under this background, the research of image quality evaluation has become a hot spot in the field of image processing. The purpose of this paper is to analyze the anatomical characteristics of knee joints of martial arts athletes, the mechanics of injury, the pathophysiological changes after injury, and establish a mathematical model by computer algorithm to accurately perceive the image quality of martial arts sports damage, and finally achieve the use of computer instead of human vision. The system goes to view and recognize images. In this paper, the application value of quantitative CT parameters of martial arts exercise in the evaluation of martial arts injury joints is based on image quality, in order to provide valuable reference for the treatment of martial arts injury selection and prognosis evaluation.","Image quality, Wushu, Joint damage, Quantitative CT",Zexiu Ai,https://www.sciencedirect.com/science/article/pii/S1047320319301075,https://doi.org/10.1016/j.jvcir.2019.03.013,1047-3203,2019,417--425,60,Journal of Visual Communication and Image Representation,Quantitative CT study of martial arts sports injuries based on image quality,article,AI2019417
"A medical fusion method can combine multiple data into one, which is very useful and convenient in the medical diagnosis. We investigated the fusion problem for 3D Magnetic Resonance Imaging (MRI) data and proposed a feature level MRI fusion method based on 3D Dual Tree Compactly Supported Shearlet Transform (CSST) and Structure Tensor. By employing a 3D version of the traditional CSST with dual tree (DT) structure, the 3D DT CSST is shift-invariant and has directional analysis capability for volume data. Utilizing the structural feature extraction capability of Structure Tensor, the proposed fusion rule can preserve the critical structure information of scanned organ in source data. Experimental results using 51 groups of MRI data show the effectiveness of the proposed fusion method.","Medical image fusion, Shearlet transform, Structure tensor",Chang Duan and Shuai Wang and Qihong Huang and Tao Wen and Ce Zhu and Yuanyuan Xu,https://www.sciencedirect.com/science/article/pii/S1047320319300781,https://doi.org/10.1016/j.jvcir.2019.02.027,1047-3203,2019,319--327,60,Journal of Visual Communication and Image Representation,Feature level MRI fusion based on 3D dual tree compactly supported Shearlet transform,article,DUAN2019319
"This paper proposes a new end-to-end depth-aware saliency model using three convolutional neural networks including color saliency network, depth saliency network and saliency fusion network, for saliency detection in RGBD images and stereoscopic images. Firstly, the color image is fed to the color saliency network to generate the color saliency map. Then, by sharing the weights of some layers in the color saliency network, the depth saliency network exploits the weight initialization and multi-layer pyramid feature fusion to learn effective depth features from the three-channel depth image, which is converted from the original depth map, and generates the depth saliency map. Finally, the saliency fusion network integrates the color saliency map with the depth saliency map into the final saliency map, which can highlight salient object regions and suppress background regions more effectively. Experimental results on five public datasets demonstrate that our model achieves the better performance compared with the state-of-the-art depth-aware saliency models.","Saliency detection, Convolutional neural networks, Depth saliency network, Saliency fusion network, RGBD images, Stereoscopic images",Yu Ding and Zhi Liu and Mengke Huang and Ran Shi and Xiangyang Wang,https://www.sciencedirect.com/science/article/pii/S104732031930118X,https://doi.org/10.1016/j.jvcir.2019.03.019,1047-3203,2019,1--9,61,Journal of Visual Communication and Image Representation,Depth-aware saliency detection using convolutional neural networks,article,DING20191
"In this paper, a new algorithm based on a combined neural network is proposed to improve salient object detection in the complex images. It consists of two main steps. The first step, an objective function which is optimized on a multi-layer graph structure is constructed to diffuse saliency from borders to salient objects, aiming to roughly estimate the location and extent salient objects of an image, meanwhile, color attribute is adopted to rapidly find a set of object-related regions in the image. The second step, establish a combined neural network with Region Net and Local-Global Net. Region Net is adopted to efficiently generate the salient map with the sharp object boundary. Then Local-Global Net based on multi-scale spatial context is proposed to provide strongly reliable multi-scale contextual information, and thus achieves an optimized performance. Experimental results and comparison analysis demonstrate that the proposed algorithm is more effective and superior than most low-level oriented prior methods in terms of precision recall curves, F-measure and mean absolute errors.","Machine vision, Saliency detection, Fast R-CNN, Region Net, Local-Global Net",Chao Ji and Xinbo Huang and Wen Cao and Yongcan Zhu and Ye Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319302949,https://doi.org/10.1016/j.jvcir.2019.102673,1047-3203,2019,102673,65,Journal of Visual Communication and Image Representation,Saliency detection using Multi-layer graph ranking and combined neural networks,article,JI2019102673
"Sparse representation methods have exhibited promising performance for pattern recognition. However, these methods largely rely on the data sparsity available in advance and are usually sensitive to noise in the training samples. To solve these problems, this paper presents sparsity adaptive matching pursuit based sparse representation for face recognition (SAMPSR). This method adaptively explores the valid training samples that exactly represent the test via iterative updating. Next, the test samples are reconstructed via the valid training samples, and classification is performed subsequently. The two-phase strategy helps to improve the discriminating power of class probability distribution, and thus alleviates effect of the noise from the training samples to some extent and correctly performs classification. In addition, the method solves the sparse coefficient by comparing the residual between the test sample and the reconstructed sample instead of using the sparsity. A large number of experiments show that our method achieves promising performance.","Face recognition, Sparse representation, Matching pursuit",Yuhong Wang and Yali Peng and Shigang Liu and Jun Li and Xili Wang,https://www.sciencedirect.com/science/article/pii/S1047320320300146,https://doi.org/10.1016/j.jvcir.2020.102764,1047-3203,2020,102764,67,Journal of Visual Communication and Image Representation,Sparsity adaptive matching pursuit for face recognition,article,WANG2020102764
"A dehazing method often only shows good results when processing the image for a certain haze concentration. So an adaptive hazy image dehazing method based on SVM is proposed. The innovation points are as follows: Firstly, combining the characteristics of the degraded images of haze weather, the dark channel histogram and texture features of the input images are extracted to form the feature vectors. These are trained by supervised learning through SVM algorithm to realize automatic binary classification of images; Secondly, the defined dehazing methods are called to process the classified result as a hazy image and the same quality evaluation indexes are used to evaluate each image output by different dehazing methods. Then, it outputs the highest evaluation image after haze removal. Finally, the output image is classified again by SVM until the image reaches the clearest it can be. The experimental results show that the proposed algorithm exhibits good contrast, brightness and color saturation from the visual effect. Also the scene adaptability and robustness of the algorithm are improved.","SVM, Adaptive dehazing, Automatic binary classification, Quality evaluation index",Bian Gui and Yuhua Zhu and Tong Zhen,https://www.sciencedirect.com/science/article/pii/S1047320320300420,https://doi.org/10.1016/j.jvcir.2020.102792,1047-3203,2020,102792,70,Journal of Visual Communication and Image Representation,Adaptive single image dehazing method based on support vector machine,article,GUI2020102792
"Peer-to-peer (P2P) lending platform plays a significant role in modern financial systems. However, due to improper supervision, credit risk is inevitable. In this paper, we analyze the traditional financial risk and information technology risk of P2P lending platform. In order to evaluate the performance of assessment algorithms, we present a BP neural network-based algorithm for lending risk assessment. To achieve our task, we crawled large-scale lending data for 2015â2019. Logistic regression is used to compare with BP neural network method. Experimental results show that BP neural network-based algorithm outperforms traditional Logistic regression algorithm and the proposed method can effectively reduce investor risk.","Peer to peer, Credit risk assessment, Logistic regression, BP neural network, Big data",Yiping Guo,https://www.sciencedirect.com/science/article/pii/S1047320319303517,https://doi.org/10.1016/j.jvcir.2019.102730,1047-3203,2020,102730,71,Journal of Visual Communication and Image Representation,Credit risk assessment of P2P lending platform towards big data based on BP neural network,article,GUO2020102730
"Although zero-watermarking can provide an effective and distortion-free scheme for image copyright protection, its robustness and discriminability do not meet expectations in existing methods. Some cannot resist effectively geometric attacks, others do not consider the discriminability and equalization. For that reason, this paper proposes a robust and distinguishable color image zero-watermarking algorithm based on polar harmonic transforms (PHTs) and compound chaotic map. In the proposed algorithm, firstly three PHTs moments of an image are computed simultaneously and accurate moments are selected for the robustness. Then, content-based binary feature sequence is acquired by judging the relation between magnitudes of adjacent moments for the discriminability. Finally, compound chaotic map is employed to encrypt copyright logo for ensuring security and scramble binary feature sequence for improving the equalization. Experimental results show that the proposed zero-watermarking algorithm has good equalization and discriminability, and an advantage in robustness compared with other zero-watermarking and traditional watermarking.","Polar harmonic transforms, Zero-watermarking, Geometric attacks, Chaos encryption",Xiaobing Kang and Fan Zhao and Yajun Chen and Guangfeng Lin and Cuining Jing,https://www.sciencedirect.com/science/article/pii/S1047320320300547,https://doi.org/10.1016/j.jvcir.2020.102804,1047-3203,2020,102804,70,Journal of Visual Communication and Image Representation,Combining polar harmonic transforms and 2D compound chaotic map for distinguishable and robust color image zero-watermarking algorithm,article,KANG2020102804
"In the nonlocal total variation (NLTV) model the constant regularization parameter Î» cannot adaptively control the balance between the regularization term and the fidelity term, which may results in over-smoothing and the more losing image details in non-flat areas when Î» is small, or insufficient noise removal in flat areas when Î» is large. It is better that Î» has different values according to the characteristics of image areas. In this paper, we introduce an adaptive regularization parameter Î»(x) which can recognize flat areas and non-flat areas of an image and propose an improved NLTV model by replacing regularization parameter Î» in NLTV model with the function Î»(x). In addition, we calculate the similarity weight function of our model from the pre-filtered image to reduce the influence of noise on it. Experimental results demonstrate our approach outperforms some existing methods in terms of objective criteria and subjective visual perception.","Adaptive regularization parameter, Image denoising, NLTV model, Split-Bregman method",Yan Jin and Xiaoben Jiang and Wenyu Jiang,https://www.sciencedirect.com/science/article/pii/S1047320319302822,https://doi.org/10.1016/j.jvcir.2019.102661,1047-3203,2019,102661,65,Journal of Visual Communication and Image Representation,An image denoising approach based on adaptive nonlocal total variation,article,JIN2019102661
"This paper proposes a saliency detection method via two-stage absorbing Markov chain based on background and foreground for detecting salient objects in images. Firstly, image preprocessing is performed, followed by convex hull construction and superpixel segmentation, to prepare for subsequent processing. Secondly, according to the boundary connectivity, the superpixels with lower background probability value in the candidate boundary background set B0 are deleted, and the boundary background set B1 is obtained. With the saliency values of the nodes in the boundary-prior saliency map Sbg1, the background seeds are added appropriately in the region outside the candidate boundary background set B0 and the convex hull H, and the background seed set B is obtained after update. Then, the background-absorbing Markov chain is constructed to generate background-absorbing saliency map Sbg2. By fusing the saliency maps Sbg1 and Sbg2, the first-stage background-based saliency map Sbg is obtained. Thirdly, in the range of the convex hull H, the foreground seed set F is determined according to the saliency map Sbg. Then, the foreground-absorbing Markov chain is constructed, to obtain the second-stage foreground-absorbing saliency map Sfg. Finally, the saliency maps Sbg and Sfg of the two stages are combined to obtain a fused saliency map S, and the final saliency map Sâ is obtained after optimization through smoothing mechanism. Compared with the traditional methods, the performance of the proposed method is significantly improved. The proposed method is tested on three public image datasets, and it shows great accuracy in detecting salient objects.","Saliency object detection, Markov chain, Background absorbing, Foreground absorbing",Wei Tang and Zhijian Wang and Jiyou Zhai and Zhangjing Yang,https://www.sciencedirect.com/science/article/pii/S1047320319303487,https://doi.org/10.1016/j.jvcir.2019.102727,1047-3203,2020,102727,71,Journal of Visual Communication and Image Representation,Salient object detection via two-stage absorbing Markov chain based on background and foreground,article,TANG2020102727
"Conventional face image generation using generative adversarial networks (GAN) is limited by the quality of generated images since generator and discriminator use the same backpropagation network. In this paper, we discuss algorithms that can improve the quality of generated images, that is, high-quality face image generation. In order to achieve stability of network, we replace MLP with convolutional neural network (CNN) and remove pooling layers. We conduct comprehensive experiments on LFW, CelebA datasets and experimental results show the effectiveness of our proposed method.","Face image generation, GAN, High-quality images",Zhixin Zhang and Xuhua Pan and Shuhao Jiang and Peijun Zhao,https://www.sciencedirect.com/science/article/pii/S1047320319303402,https://doi.org/10.1016/j.jvcir.2019.102719,1047-3203,2020,102719,71,Journal of Visual Communication and Image Representation,High-quality face image generation based on generative adversarial networks,article,ZHANG2020102719
"The main task of emotional facial recognition is to understand human emotion expression through the recognition of facial expressions, so as to achieve more effective communication and interpersonal communication. Therefore, facial recognition plays an important role in people's daily lives. In addition, the research of facial recognition is also helpful to understand the human perception processing mode, and promote the development of pattern recognition, cognitive science, neural network and other fields. With the development of cognitive science, facial recognition technology has been continuously improved, and emotional facial recognition tasks have received attention in the fields of pattern recognition and artificial intelligence, and have become a research hotspot. Among them, pattern recognition is a cognitive system applied to many fields. For the first time, we confirmed the effects of facial memory time, personal cognitive style, and emotions associated with the target face on facial recognition patterns. This study measured the impact of time, cognitive style, and emotional type of 62 qualified college students. The research results show that cognitive style and facial emotional content are of great significance for face pattern recognition. Specifically, students classified as âdependentâ have achieved good results in face pattern recognition, and positive and negative strong emotional faces have left behind those who show neutral emotions. A deeper impression. Finally, an unusual phenomenon was discovered, which indicates that the shorter the time spent on the face of the memory, the higher the recognition score.","Face recognition, Pattern recognition, Cognitive style, Face emotion",Shuna Peng and Yang Dong and Weisha Wang and Jieyi Hu and Weiyang Dong,https://www.sciencedirect.com/science/article/pii/S1047320319302950,https://doi.org/10.1016/j.jvcir.2019.102674,1047-3203,2019,102674,65,Journal of Visual Communication and Image Representation,The affective facial recognition task: The influence of cognitive styles and exposure times,article,PENG2019102674
"Service composition is a research hotspot with practical value. With the development of Web service, many Web services with the same functional attributes emerge. However, service composition optimization is still a big challenge since the complex and unstable composition environment. To solve this problem, we propose an adaptive service composition based on deep reinforcement learning, where recurrent neural network (RNN) is utilized for predicting the objective function, improving its expression and generalization ability, and effectively solving the shortcomings of traditional reinforcement learning in the face of large-scale or continuous state space problems. We leverage heuristic behavior selection strategy to divide the state set into hidden state and fully visible state. Effective simulation of hidden state space and fully visible state of the evaluation function can further improve the accuracy and efficiency of the combined results. We conduct comprehensive experiment and experimental results have shown the effectiveness of our method.","Service composition, Deep reinforcement learning, QoS, Behavior strategy",Jiang-Wen Liu and Li-Qiang Hu and Zhao-Quan Cai and Li-Ning Xing and Xu Tan,https://www.sciencedirect.com/science/article/pii/S1047320319303086,https://doi.org/10.1016/j.jvcir.2019.102687,1047-3203,2019,102687,65,Journal of Visual Communication and Image Representation,Large-scale and adaptive service composition based on deep reinforcement learning,article,LIU2019102687
"Histopathological image classification is a very challenging task because of the biological heterogeneities and rich geometrical structures. In this paper, we propose a novel histopathological image classification framework, which includes the discriminative feature learning and the mutual information-based multi-channel joint sparse representation. We first propose a stack-based discriminative prediction sparse decomposition (SDPSD) model by incorporating the class labels information to predict deep discriminant features automatically. Subsequently, a mutual information-based multi-channel joint sparse model (MIMCJSM) is presented to jointly encode the common component and particular components of the discriminative features. Especially, the main advantage of the MIMCJSM is the construction of a joint dictionary using a mutual information criterion, which contains a common sub-dictionary and three particular sub-dictionaries. Based on the joint dictionary, the MIMCJSM captures the relationship of multi-channel features, which can improve discriminative ability of joint sparse representation coefficients. Finally, the joint sparse representation coefficients of different levels can be aggregated using the spatial pyramid matching (SPM) model, and the linear support vector machine (SVM) is used as the classifier. Experimental results on ADL and BreaKHis datasets demonstrate that our proposed framework consistently performs better than popular existing classification frameworks. Additionally, it can show promising strong-robustness performance for histopathological image classification.","Discriminative feature learning, Stack-based discriminative prediction sparse decomposition (SDPSD), Mutual information-based Multi-channel joint sparse model (MIMJSM), Histopathological image classification",Xiao. Li and Hongzhong. Tang and Dongbo. Zhang and Ting. Liu and Lizhen. Mao and Tianyu. Chen,https://www.sciencedirect.com/science/article/pii/S1047320320300493,https://doi.org/10.1016/j.jvcir.2020.102799,1047-3203,2020,102799,70,Journal of Visual Communication and Image Representation,Histopathological image classification through discriminative feature learning and mutual information-based multi-channel joint sparse representation,article,LI2020102799
"Dictionary learning is one of the most important algorithms for face recognition. However, many dictionary learning algorithms for face recognition have the problems of small sample and weak discriminability. In this paper, a novel discriminative dictionary learning algorithm based on sample diversity and locality of atoms is proposed to solve the problems. The rational sample diversity is implemented by alternative samples and new error model to alleviate the small sample size problem. Moreover, locality can leads to sparsity and strong discriminability. In this paper, to enhance the dictionary discrimination and to reduce the influence of noise, the graph Laplacian matrix of atoms is used to keep the local information of the data. At the same, the relational theory is presented. A large number of experiments prove that the proposed algorithm can achieve more high performance than some state-of-the-art algorithms.","Dictionary learning, Face recognition, Locality constrained, Sample diversity",Shigang Liu and Yuhong Wang and Xiaosheng Wu and Jun Li and Tao Lei,https://www.sciencedirect.com/science/article/pii/S1047320320300134,https://doi.org/10.1016/j.jvcir.2020.102763,1047-3203,2020,102763,71,Journal of Visual Communication and Image Representation,â - Discriminative dictionary learning algorithm based on sample diversity and locality of atoms for face recognition,article,LIU2020102763
"Typical video captioning methods are developed based on the encoder-decoder architecture. To better exploit the local temporal information, e.g., details about objects and their corresponding actions, we propose a reinforcement learning based method to predict the adaptive sliding window size sequentially for better event exploration. More specifically, we introduce the single Monte-Carlo sample to approximate the gradient of reward-based loss function. And the self-critical strategy is employed to estimate baseline reward to diminish the variance of gradients. Moreover, temporal attention is utilized to selectively focus on a subset of temporal frame representations while generating each word. In addition, to better initialize the decoderâs state, we utilize the motion features extracted by 3D CNNs with mean pooling to endow the decoder with the prior knowledge of the entire video. To evaluate the proposed method, experiments are performed on three public benchmark datasets: Microsoft Video Description Corpus (MSVD), MSR Video to Text challenge (MSR-VTT) and Charades. The experimental results demonstrate the effectiveness of our method by comparing with state-of-the-art methods.","Local temporal information, Video captioning, Sliding windows, Reinforcement learning",Ran Wei and Li Mi and Yaosi Hu and Zhenzhong Chen,https://www.sciencedirect.com/science/article/pii/S1047320320300018,https://doi.org/10.1016/j.jvcir.2020.102751,1047-3203,2020,102751,67,Journal of Visual Communication and Image Representation,Exploiting the local temporal information for video captioning,article,WEI2020102751
"Human-computer interaction is the way in which humans and machines communicate information. With the rapid development of deep learning technology, the technology of human-computer interaction has also made a corresponding breakthrough. In the past, the way human-computer interaction was mostly relied on hardware devices. Through the coordinated work of multiple sensors, people and machines can realize information interaction. However, as theoretical technology continues to mature, algorithms for human-computer interaction are also being enriched. The popularity of convolutional neural networks has made image processing problems easier to solve. Therefore, real-time human-computer interaction can be performed by using image processing, and intelligent of human-computer interaction can be realized. The main idea of this paper is to use the real-time capture of face images and video information to image the face image information. We perform feature point positioning based on the feature points of the face image. We perform expression recognition based on the feature points that are located. At the same time, we perform ray tracing for the identified human eye area. The feature points of the face and the corresponding expressions and implementation movements represent the user's use appeal. Therefore, we can analyze the user's use appeal by locating the face feature area. We define the corresponding action information for specific user face features. We extract the user's corresponding information according to the user's face features, and perform human-computer interaction according to the user's information.","Human interaction, Face point detection, Expression recognition, Sight tracking",Yan Shi and Zijun Zhang and Kaining Huang and Wudi Ma and Shanshan Tu,https://www.sciencedirect.com/science/article/pii/S104732031930361X,https://doi.org/10.1016/j.jvcir.2019.102740,1047-3203,2020,102740,70,Journal of Visual Communication and Image Representation,Human-computer interaction based on face feature localization,article,SHI2020102740
"Video summaries produced by low level features are unaware of the viewerâs requirements and result in a semantic gap. Video content evokes certain emotions in a viewer, which can be measured and act as a strong source of information to generate summaries meeting viewerâs expectation. In this paper, we propose a personalized video summarization framework that classifies viewerâs emotion based on electroencephalography (EEG) signals, while watching a video to extract keyframes. Features are extracted from recorded EEG signals in time, frequency and wavelet domain to classify viewerâs emotions. Those frames are selected as keyframes from the video, where different emotions of viewer are evoked. Experiments are performed on 50 viewers and 50 video sequences to validate the effectiveness and efficiency of the proposed framework. It is evident from the results that the proposed method generates summaries with high precision, recall, F-measure, accuracy, and low error, hence reducing the semantic gap.","Video summarization, Emotion recognition, Electroencephalography, Feature extraction, Classification",Huma Qayyum and Muhammad Majid and Ehatisham ul Haq and Syed Muhammad Anwar,https://www.sciencedirect.com/science/article/pii/S1047320319302937,https://doi.org/10.1016/j.jvcir.2019.102672,1047-3203,2019,102672,65,Journal of Visual Communication and Image Representation,Generation of personalized video summaries by detecting viewerâs emotion using electroencephalography,article,QAYYUM2019102672
"Tensor completion aims to recover missing entries from partial observations for multi-dimensional data. Traditional tensor completion algorithms process the dimensional data by unfolding the tensor into matrices, which breaks the inherent correlation and dependencies in multiple channels and lead to critical information loss. In this paper, we propose a novel tensor completion model for visual multi-dimensional data completion under the tensor singular value decomposition (t-SVD) framework. In the proposed method, tensor is treated as a whole and a truncated nuclear norm regularization is employed to exploit the structural properties in a tensor and hidden information existing among the adjacent channels of a tensor. Besides, we introduce a weighted tensor to adjust the residual error of each frontal slices in consideration of their different recovery statistics. It does enhance the sparsity of all unfoldings of the tensor and accelerates the convergence of the proposed method. Experimental results on various visual datasets demonstrate the promising performance of the proposed method in comparison with the state-of-the-art tensor completion methods.","Tensor completion, Tensor singular value decomposition, Truncated tensor nuclear norm, Visual data restoration",Yun Song and Jie Li and Xi Chen and Dengyong Zhang and Qiang Tang and Kun Yang,https://www.sciencedirect.com/science/article/pii/S1047320320300419,https://doi.org/10.1016/j.jvcir.2020.102791,1047-3203,2020,102791,70,Journal of Visual Communication and Image Representation,An efficient tensor completion method via truncated nuclear norm,article,SONG2020102791
"Finance service based on big data faces many issues, such as fraud, credit. In this paper, we study the development of financial business model under the big data. We first analyze the impact mechanism of big data finance on customer information protection of commercial banks. Customer information has the characteristics of large amount of information, high value of data and strong destructive data leakage. Then, we propose two solutions towards issues of finance service including face anti-spoofing algorithm and financial risk evaluation. Experiments show the effectiveness of our proposed method in improving the reliability and security of modern big data finance.","Financial business, Big data, Neural network",Yupeng Wang,https://www.sciencedirect.com/science/article/pii/S1047320319303505,https://doi.org/10.1016/j.jvcir.2019.102729,1047-3203,2020,102729,71,Journal of Visual Communication and Image Representation,Analysis of financial business model towards big data and its applications,article,WANG2020102729
"Object tracking has been widely used in various intelligent systems, such as pedestrian tracking, autonomous vehicles. To solve the problem that appearance changes and occlusion may lead to poor tracking performance, we propose a multiple instance learning (MIL) based method for object tracking. To achieve this task, we first manually label the first several frames of video stream in image level, which can indicate that whether a target object in the video stream. Then, we leverage a pre-trained convolutional neural network that has rich prior information to extract deep representation of target object. Since the location of the same object in adjacent frames is similar, we introduce a particle filter to predict the location of target object within a specific region. Comprehensive experiments have shown the effectiveness of our proposed method.","Object tracking, Convolutional networks, Multiple Instance Learning",Chunyu Li and Gang Li,https://www.sciencedirect.com/science/article/pii/S104732031930358X,https://doi.org/10.1016/j.jvcir.2019.102737,1047-3203,2020,102737,71,Journal of Visual Communication and Image Representation,Learning multiple instance deep representation for objects tracking,article,LI2020102737
"Light field images can be efficiently compressed using standard video codecs, such as the High Efficiency Video Coding (HEVC). However, the huge amount of data combined with the high computational complexity of HEVC, poses limitations on high-speed light field capturing and storage. This paper presents a contribution for low complexity encoding of light fields, in different formats using HEVC, based on a Random Forests ensemble classifier. Optimal features for training the classifier are found through a score fusion based approach. Using the HEVC still image profile, the proposed method gives speed-up of 56.23% for sub-aperture images. For pseudo video format, the proposed method outperforms others available in the literature, yielding an average speed-up of 62.18%, 56.54% and 44.73% for Random Access, Low-delay Main and All-Intra profiles respectively, with negligible decrease in RD performance. These are novel results in fast coding of light fields, which are useful for further research and benchmarking.","Light fields, HEVC, Fast encoding, Machine learning, Random forests",Muhammad Tahir and Imtiaz A. Taj and Pedro A. Assuncao and Muhammad Asif,https://www.sciencedirect.com/science/article/pii/S1047320319303633,https://doi.org/10.1016/j.jvcir.2019.102742,1047-3203,2020,102742,66,Journal of Visual Communication and Image Representation,Low complexity high efficiency coding of light fields using ensemble classifiers,article,TAHIR2020102742
"Spectral images (SI) can be represented as 3D-arrays of spatial information across multiple wavelengths. Compressive Spectral Imaging (CSI) reduces sensing costs by sensing compressed versions of the scene, recovering a suitable version of the original SI solving a sparsity-inducing inverse problem. On the other hand, Convolutional Sparse Coding (CSC) has been successfully proved for representing gray-scale images, however it misses any correlation between images. This work considers the spatial-spectral correlation within SIs introducing an extension of the CSC signal model describing the SI as the sum of convolutions of 3D sparse coefficient maps with their respective 3D dictionary filters. Furthermore, we use the proposed CSC framework for recovering SIs from CSI measurements. The simulations results, using two different CSI acquisition architectures, show that the proposed CSC framework yields better representations of the SIs than those obtained under the traditional sparse signal representation approach, improving the quality of the recovered SIs.","Compressive spectral imaging, Convolutional sparse coding, Sparse representation, Spectral images",Crisostomo Barajas-Solano and Juan-Marcos Ramirez and Henry Arguello,https://www.sciencedirect.com/science/article/pii/S1047320319303116,https://doi.org/10.1016/j.jvcir.2019.102690,1047-3203,2020,102690,66,Journal of Visual Communication and Image Representation,Convolutional sparse coding framework for compressive spectral imaging,article,BARAJASSOLANO2020102690
"Video may be subject to various distortions during acquisition, processing, compression, storage, transmission, and reproduction, and it results in reduced visual quality. In complex sports scenes under big data environment, the human body's movements are even more so. The quality of human motion can intuitively affect the human visual experience. Therefore, it is necessary to determine an intelligent quality assessment model to evaluate human motion in complex motion scenarios under big data environment. It can be used to dynamically monitor and adjust video quality, and it can be used for algorithms and parameter settings in motion image processing systems. With the popularity of deep learning, convolutional neural networks have become a very important method in the field of computer vision research. Based on the 2D-CNN algorithm, we propose a 3D convolutional neural network model for human motion quality assessment in complex motion scenarios. The model captures the pose characteristics, motion trajectory, video brightness and contrast in time and space. The model feeds back the reference and distorted video pairs into the network, with each output layer acting as a feature map. The local similarity between the feature maps obtained from the reference video and the distorted video is then calculated and combined to obtain a global image quality score. Experiments show that the model can achieve competitive performance in big data environment for video quality assessment.","Quality assessment, Deep learning, Human activities, Big data, Neural networks",Yedong Li and Hongmei He and Zhixin Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319303232,https://doi.org/10.1016/j.jvcir.2019.102702,1047-3203,2020,102702,71,Journal of Visual Communication and Image Representation,Human motion quality assessment toward sophisticated sports scenes based on deeply-learned 3D CNN model,article,LI2020102702
"This work presents a secure and robust color image watermarking for copyright protection applications, that is based on exploiting the multi-spectral properties of the primary color components of the RGB image. The proposed scheme employs the interconnection between the subbands of the primary color components in the wavelet-packet domain. The scheme is constructed to be adaptive, in the sense that the watermark bits are embedded in safe locations, depending on the inter-layer energy of coefficients in the wavelet-packets. The scheme immunity to attacks is improved by applying a two-level security procedure. To validate the high performance of the proposed scheme, several experimental tests were conducted and a comparative analysis was provided. The obtained results have shown improved watermarking robustness against a wide range of attacks while preserving a high watermarking imperceptibility.","Color image watermarking, Wavelet-packets, Multispectral analysis",Hazem Munawer Al-Otum,https://www.sciencedirect.com/science/article/pii/S1047320319303475,https://doi.org/10.1016/j.jvcir.2019.102726,1047-3203,2020,102726,66,Journal of Visual Communication and Image Representation,Secure and robust host-adapted color image watermarking using inter-layered wavelet-packets,article,ALOTUM2020102726
"Most of the color image enhancement algorithms are implemented in two stages: gray scale image enhancement, which finds the target intensity, and then gamut mapping of the original color coordinates to the target. Therefore, hue preserving gamut mapping is an essential and crucial step, which influences colorfulness. In conventional color mapping methods, color saturation is reduced after intensity modification, which deteriorates subjective image quality. In this paper, a new color enhancement algorithm resulting in high color saturation is proposed. The proposed method employs multiplicative and additive color mapping to improve color saturation without clipping of a color component for increased target intensity as well as decreased cases. This new scheme is fast and effective, therefore, it can be employed to real time applications such as video signal processing.","Gamut mapping, Color enhancement, Hue preservation, Color saturation",Junhee Park and Byung-Uk Lee,https://www.sciencedirect.com/science/article/pii/S1047320320300092,https://doi.org/10.1016/j.jvcir.2020.102759,1047-3203,2020,102759,67,Journal of Visual Communication and Image Representation,Color image enhancement with high saturation using piecewise linear gamut mapping,article,PARK2020102759
"This paper proposes an image retrieval algorithm towards massive-scale multimedia data. In order to be consistent with human visual system, we first design a color attention function to describe the important of different image patches. Subsequently, we combine color and texture to construct candidate regions, which will be fed into a deep neural network (DNN) for deep representation extraction. Then, we design a similarity function to calculate the distance among different images, where top-ranking images are considered as the required images. Experimental results show the effectiveness and robustness of our proposed method.","Image retrieval, Visual feature, DNN",Hongpeng Zhu,https://www.sciencedirect.com/science/article/pii/S1047320319303591,https://doi.org/10.1016/j.jvcir.2019.102738,1047-3203,2020,102738,70,Journal of Visual Communication and Image Representation,Massive-scale image retrieval based on deep visual feature representation,article,ZHU2020102738
"Attention mechanism is a simple and effective method to enhance discriminative performance of person re-identification (Re-ID). Most of previous attention-based works have difficulty in eliminating the negative effects of meaningless information. In this paper, a universal module, named Cross-level Reinforced Attention (CLRA), is proposed to alleviate this issue. Firstly, we fuse features of different semantic levels using adaptive weights. The fused features, containing richer spatial and semantic information, can better guide the generation of subsequent attention module. Then, we combine hard and soft attention to improve the ability to extract important information in spatial and channel domains. Through the CLRA, the network can aggregate and propagate more discriminative semantic information. Finally, we integrate the CLRA with Harmonious Attention CNN (HA-CNN) and form a novel Cross-level Reinforced Attention CNN (CLRA-CNN) to optimize person Re-ID. Experiment results on several public benchmarks show that the proposed method achieves state-of-the-art performance.","Person re-identification, Features of different levels, Soft attention, Hard attention, Reinforced attention",Min Jiang and Cong Li and Jun Kong and Zhende Teng and Danfeng Zhuang,https://www.sciencedirect.com/science/article/pii/S1047320320300250,https://doi.org/10.1016/j.jvcir.2020.102775,1047-3203,2020,102775,69,Journal of Visual Communication and Image Representation,Cross-level reinforced attention network for person re-identification,article,JIANG2020102775
"The human visual recognition system is more efficient than any current robotic vision setting. One reason for this superiority is that humans utilize different fields of vision, depending on the recognition task. For instance, experiments on human subjects show that the peripheral vision is more useful than the central vision in recognizing scenes. We tested our recently-developed model, that is, the elastic net-regularized hierarchical MAX (En-HMAX), in recognizing objects and scenes. In various experimental conditions, images were occluded with windows and scotomas of varying sizes. With this model, classification accuracies of up to 90% for objects and scenes were possible. Modelling human experiments, window and scotoma analysis with the En-HMAX model revealed that object and scene recognition are sensitive to the availability of data in the centre and the periphery of the images, respectively. Similarly, results of deep learning models have shown that the classification accuracy diminishes dramatically in the absence of the peripheral vision. These differences led us to further analyse the performance of the En-HMAX model with the parafoveal versus peripheral areas of vision, in a second study. Results of the second study show that approximately 50% of the visual field would be sufficient to achieve 96% accuracy in the classification of unseen images. The En-HMAX model adopts a relative order of importance, similar to the human visual system, depending on the image category. We showed that utilizing the relevant regions of vision can significantly reduce the image processing time and size.","Visual recognition, Image understanding, Visual-data reduction, Biological visual-systems, Visual perception, Scene analysis",Ali Alameer and Patrick Degenaar and Kianoush Nazarpour,https://www.sciencedirect.com/science/article/pii/S1047320319303190,https://doi.org/10.1016/j.jvcir.2019.102698,1047-3203,2020,102698,66,Journal of Visual Communication and Image Representation,Objects and scenes classification with selective use of central and peripheral image content,article,ALAMEER2020102698
"Video shot boundary detection (VSBD) is one of the most essential criteria for many intelligent video analysis-related applications, such as video retrieval, indexing, browsing, categorization and summarization. VSBD aims to segment big video data into meaningful fragments known as shots. This paper put forwards a new pyramidal opponent colour-shape (POCS) model which can detect abrupt transition (AT) and gradual transition (GT) simultaneously, even in the presence of illumination changes, huge object movement between frames, and fast camera motion. First, the content of frames in the video subjected to VSBD is represented by the proposed POCS model. Consequently, the temporal nature of the POCS model is subjected to a suitable segment (SS) selection procedure in order to minimize the complexity of VSBD method. The SS from the video frames is examined for transitions within it using a bagged-trees classifier (BTC) learned on a balanced training set via parallel processing. To prove the superiority of the proposed VSBD algorithm, it is evaluated on the TRECVID 2001, TRECVID2007 and VIDEOSEG2004 data sets for classifying the basic units of video according to no transition (NT), AT and GT. The experimental evaluation results in an F1-score of 95.13%, 98.13% and 97.11% on the TRECVID 2001, TRECVID2007 and VIDEOSEG2004 data sets, respectively.","Shot boundary detection, Abrupt transition, Gradual transition, Opponent color space, Ensemble algorithm",A. Sasithradevi and S. {Mohamed Mansoor Roomi},https://www.sciencedirect.com/science/article/pii/S1047320320300043,https://doi.org/10.1016/j.jvcir.2020.102754,1047-3203,2020,102754,67,Journal of Visual Communication and Image Representation,A new pyramidal opponent color-shape model based video shot boundary detection,article,SASITHRADEVI2020102754
"Sentence semantic matching (SSM) always plays a critical role in natural language processing. Measuring the intrinsic semantic similarity among sentences is very challenging and has not been substantially addressed. The latest SSM research usually relies on a shallow text representation and interaction between sentence pairs, which might not be enough to capture the complex semantic features and lead to limited performance. To capture more semantic context features and interactions, we propose a hierarchical encoding model (HEM) for sentence representation, further enhanced by a hierarchical matching mechanism for sentence interaction. Given two sentences, HEM generates intermediate and final representations in encoding layer, which are further handled by a novel hierarchical matching mechanism to capture more multi-view interactions in matching layer. The comprehensive experiments demonstrate that our model is capable to capture more sentence semantic features and interactions, which significantly outperforms the existing state-of-the-art neural models on the public real-world dataset.","Hierarchical encoding model, Hierarchical measure mechanism, Sentence similarity, Semantic equivalence identification, Text representation",Wenpeng Lu and Xu Zhang and Huimin Lu and Fangfang Li,https://www.sciencedirect.com/science/article/pii/S1047320320300444,https://doi.org/10.1016/j.jvcir.2020.102794,1047-3203,2020,102794,71,Journal of Visual Communication and Image Representation,Deep hierarchical encoding model for sentence semantic matching,article,LU2020102794
"Current context-utilizing detectors are all based on two-stage approaches. However, their computational efficiency and context quality extremely depend on the accuracy of proposal-generating methods, which limits their performance and makes them hardly perform real-time detection. In this work, we present a context-exploited method that integrates features in different receptive fields to obtain contextual representation. Based on this idea, we put forward the multi-branch diverse receptive field modules (DRF modules) and their design principles to encode context. To further utilize contextual information for fast object detection, we propose a one-stage diverse receptive field network (DRFNet). In DRFNet, the DRF modules are first applied to capture rich context as the basis, then a parallel structure is constructed to exploit the context at different scales along with DRF modules. Comprehensive experiments indicate that the context introduced by our methods improves the detection performance and DRFNet achieves a good trade-off between speed and accuracy.","Object detection, Convolutional neural network, Context aggregation, Multi-scale contextual representations",Shaorong Xie and Chang Liu and Jiantao Gao and Xiaomao Li and Jun Luo and Baojie Fan and Jiahong Chen and Huayan Pu and Yan Peng,https://www.sciencedirect.com/science/article/pii/S1047320320300201,https://doi.org/10.1016/j.jvcir.2020.102770,1047-3203,2020,102770,70,Journal of Visual Communication and Image Representation,Diverse receptive field network with context aggregation for fast object detection,article,XIE2020102770
"Driver fatigue detection is a significant application in smart cars. In order to improve the accuracy and timeliness of driver fatigue detection, a fatigue detection algorithm based on deeply-learned facial expression analysis is proposed. Specifically, the face key point detection model is first trained by multi block local binary patterns (MB-LBP) and Adaboost classifier. Subsequently, the eyes and mouth state are detected by using the trained model to detect the 24 facial features. Afterwards, we calculate the number of two parameters that can describe the driver's fatigue state and the proportion of the closed eye time within the unit time (PERCLOS) and yawning frequency. Finally, the fuzzy inference system is utilized to deduce the driver's fatigue state (normal, slight fatigue, severe fatigue). Experimental results show that the proposed algorithm can detect driver fatigue degree quickly and accurately.","Fatigue detection, MB-LBP, PERCLOS, Fuzzy reasoning",Zhongmin Liu and Yuxi Peng and Wenjin Hu,https://www.sciencedirect.com/science/article/pii/S104732031930344X,https://doi.org/10.1016/j.jvcir.2019.102723,1047-3203,2020,102723,71,Journal of Visual Communication and Image Representation,Driver fatigue detection based on deeply-learned facial expression representation,article,LIU2020102723
"Image information may be distorted during acquisition, processing, compression, and transmission. It is necessary to propose an intelligent image quality assessment model toward big data environment to quantify the degree of distortion of the image. This paper proposes a quality assessment model for human motion images. In complex scenes, the human body's action posture can be taken as an important feature point. Usually, in different scenes, the parts that affect the quality of the human body's posture are different. In other words, the weights of feature points that affect quality are different in different scenarios. However, due to the categorization of human movements, we can learn the quality assessment methods of different types of movements through sample training. Inspired by feature learning in the field of machine learning, we propose a hierarchical quality learning approach. We cast quality assessment as quality feature learning and layer by layer. The hierarchical quality learning method is based on deep reinforcement learning. The key part is that the method focuses on the region that containing more information on the features of the quality and enlarges the region layer by layer. Finally, we can determine the part of the body that affects the quality assessment. We compare this method with the subjective quality assessment results of the human observers and find that the proposed method achieves effective performance in big data environment to evaluate human motion quality.","Quality assessment, Reinforcement learning, Human activities, Big data, Hierarchical networks",Zhiwei Huang and Yan Li and Shiguang Luo,https://www.sciencedirect.com/science/article/pii/S1047320319303219,https://doi.org/10.1016/j.jvcir.2019.102700,1047-3203,2020,102700,71,Journal of Visual Communication and Image Representation,Hierarchical Learning-Guided human motion quality assessment in big data environment,article,HUANG2020102700
"Blockchain is a key technique which can support Bitcoin. Blockchain is a decentralized infrastructure that uses chained data structure to verify and store data, and uses distributed node consensus mechanism to generate and update data. Blockchain has become a hot research topic since its attributes of decentralization, verifiability and anti-tampering. To stimulate the development of Blockchain, we conduct a comprehensive research on Blockchain. Specifically, we discuss various mainstream consensus mechanisms used in blockchain technology, and thoroughly analyze anonymity and privacy protection in digital currency. Aiming at data encryption mechanism, we discuss existing anonymity and privacy protection schemes. Our discussion can further promote the development of Blockchain.","Blockchain, Data encryption, Privacy preserving, Big data",Leiyong Guo and Hui Xie and Yu Li,https://www.sciencedirect.com/science/article/pii/S1047320319303621,https://doi.org/10.1016/j.jvcir.2019.102741,1047-3203,2020,102741,70,Journal of Visual Communication and Image Representation,Data encryption based blockchain and privacy preserving mechanisms towards big data,article,GUO2020102741
"With the development of the information age and the popularity of Internet computer technology, the application field of digital image technology has been further expanded. Digital image technology can not only buy the interchange of the current picture scene, the image adjustment, but also change the color texture of the image and the shape of the main body of the picture. The application of digital technology and imaging in graphic design has become an inevitable trend, and they play an invaluable role in graphic design. Graphic design is designed to meet the growing cultural needs of people, and it is clear that high standards of digital technology and imaging are not lacking. The development of graphic design is inseparable from the promotion of digital image technology, and there is a close relationship between the two. In the context of the rapid development of the current society, people's needs are increasing, and how to meet the visual needs of the social population has become a top priority. This paper takes the relationship between digital image technology and graphic design as the starting point, discusses the processing criteria of digital image technology in graphic design, and provides reference for relevant researchers.","Digital image technology, Digital image, Graphic design, Image adjustment",Zhenyu Li,https://www.sciencedirect.com/science/article/pii/S1047320319303104,https://doi.org/10.1016/j.jvcir.2019.102689,1047-3203,2019,102689,65,Journal of Visual Communication and Image Representation,Application research of digital image technology in graphic design,article,LI2019102689
"Image quality assessment (IQA) is a useful technique in computer vision and machine intelligence. It is widely applied in image retrieval, image clustering and image recognition. IQA algorithms generally rely on human visual system (HVS), which can reflect how human perceive salient regions in the image. In this paper, we leverage both low-level features and high-level semantic features to select salient regions, which will be concatenated to form GSPs by the designed saliency-constraint algorithm to mimic human visual system. We design an enhanced IQA index based on the GSPs to calculate the simialrity between reference image and test image to achieve image quality assessment. Experiments demonstrate that our IQA method can achieve satisfactory performance.","Image quality assessment, Human visual system, SSIM",Feng Zhao and Shiwang Huang and Renyan Long and Tiantian Zhang and Sang-Gyun Na,https://www.sciencedirect.com/science/article/pii/S1047320319303220,https://doi.org/10.1016/j.jvcir.2019.102701,1047-3203,2020,102701,70,Journal of Visual Communication and Image Representation,Perceptual visual quality assessment using deeply-learned gaze shifting kernel,article,ZHAO2020102701
"This paper evaluates the performance of contemporary gait identification systems. A time, erosion and neural inspired framework (TEN-FE) for gait identification was proposed to augment the performance of gait identification systems. Performance of TEN-FE framework was evaluated using CASIA and OU-ISIR large population dataset. Proposed framework relies on CNN and Reinforcement Learning to restrict the impact of confounding factors like baggage and bulky clothing on the accuracy of gait identification systems. Difference in gait signature due to time was also considered and normalized. The results observed a clear increase in systemâs performance with minimal complexity and least hardware requirements.","Convolutional neural network, OU-ISIR, CASIA, Erosion, Silhouette",Jaiteg Singh and Gaurav Goyal,https://www.sciencedirect.com/science/article/pii/S1047320319303463,https://doi.org/10.1016/j.jvcir.2019.102725,1047-3203,2020,102725,66,Journal of Visual Communication and Image Representation,"Identifying biometrics in the wild â A time, erosion and neural inspired framework for gait identification",article,SINGH2020102725
"Due to the increasing growth of surveillance data, high-efficiency surveillance video coding schemes are demanded. However, the existing conventional coding framework has difficulties in handling rotation and zooming whilst the recently proposed multisource surveillance video coding method is inflexible and prone to be affected by pose errors. To this end, we propose to combine conventional surveillance video coding and multisource surveillance video coding into one unified framework. First, global knowledge in the form of 3D model and initial textures is employed to construct a knowledge based reference frame. Meanwhile, a temporal reference frame is also generated from the reconstructed frames in decoded picture buffer. Then, they are fused to obtain the synthetic reference frame to exploit global and local information. Finally, we add the synthetic reference frame into reference picture list and rearrange the list to optimize the overall efficiency. Experimental results demonstrate the effectiveness of the proposed method over state-of-the-art anchors.","Surveillance video coding, Global knowledge, Local information, Reference frame",Yu Chen and Ruimin Hu and Jing Xiao and Zhongyuan Wang,https://www.sciencedirect.com/science/article/pii/S1047320319303062,https://doi.org/10.1016/j.jvcir.2019.102685,1047-3203,2019,102685,65,Journal of Visual Communication and Image Representation,Multisource surveillance video coding with synthetic reference frame,article,CHEN2019102685
"Individual recognition from locomotion is a challenging task owing to large intra-class and small inter-class variations. In this article, we present a novel metric learning method for individual recognition from skeleton sequences. Firstly, we propose to model articulated body on Riemannian manifold to describe the essence of human motion, which can reflect biometric signatures of the enrolled individuals. Then two spatia-temporal metric learning approaches are proposed, namely Spatio-Temporal Large Margin Nearest Neighbor (ST-LMNN) and Spatio-Temporal Multi-Metric Learning (STMM), to learn discriminant bilinear metrics which can encode the spatio-temporal structure of human motion. Specifically, the ST-LMNN algorithm extends the bilinear model into classical Large Margin Nearest Neighbor method, which learns a low-dimensional local linear embedding in the spatial and temporal domain, respectively. To further capture the unique motion pattern for each individual, the proposed STMM algorithm learns a set of individual-specific spatio-temporal metrics, which make the projected features of the same person closer to its class mean than that of different classes by a large margin. Beyond that, we present a new publicly available dataset for locomotion recognition to evaluate the influence of both internal and external covariant factors. According to the experimental results from the three public datasets, we believe that the proposed approaches are both able to achieve competitive results in individual recognition.","Individual recognition, Riemannian motion features, Spatio-Temporal Large Margin Nearest Neighbor (ST-LMNN), Spatio-Temporal Multi-Metric Learning (STMM)",Yong Su and Simin An and Zhiyong Feng and Meng Xing and Jianhai Zhang,https://www.sciencedirect.com/science/article/pii/S1047320320300031,https://doi.org/10.1016/j.jvcir.2020.102753,1047-3203,2020,102753,67,Journal of Visual Communication and Image Representation,Spatio-temporal metric learning for individual recognition from locomotion,article,SU2020102753
"Anomaly detection and location in crowded scenes have attracted a lot of attention in computer vision research community recently due to the increased applications of intelligent surveillance improve security in public. We propose a novel parallel spatial-temporal convolution neural networks model to detect and localize the abnormal behavior in video surveillance. Our approach contains two main steps. Firstly, considering the typical position of camera and the large number of background information, we introduce a novel spatial-temporal cuboid of interest detection method with varied-size cell structure and optical flow algorithm. Then, we use the parallel 3D convolution neural networks to describe the same behavior in different temporal-lengths. That step ensures that the most of behavior information in cuboids could be captured, also insures the reduction of information unrelated to the major behavior. The evaluation results on benchmark datasets show the superiority of our method compared to the state-of-the-art methods.","Abnormal detection, Video surveillance, Parallel 3D convolution neural networks, Spatial-temporal interest cuboids",Zheng-ping Hu and Le Zhang and Shu-fang Li and De-gang Sun,https://www.sciencedirect.com/science/article/pii/S1047320320300158,https://doi.org/10.1016/j.jvcir.2020.102765,1047-3203,2020,102765,67,Journal of Visual Communication and Image Representation,Parallel spatial-temporal convolutional neural networks for anomaly detection and location in crowded scenes,article,HU2020102765
"Color-black-and-white visual cryptography scheme (CBW-VCS) is a methodology that utilizes colors to alleviate the pixel expansion problem. In a general (k,n) CBW-VCS, when k and n become larger, the pixel expansion increases dramatically. In this paper, two constructions for constituting a (k,n) threshold probabilistic CBW-VCS (PCBW-VCS) are introduced, where the generated color shares are non-expansible. The two proposed constructions are proven to be valid constructions which satisfy the security and contrast conditions. Theoretical analysis and sufficient experiments are demonstrated to shown the effectiveness and advantages of the proposed PCBW-VCSs.","Visual cryptography, Probabilistic, Color, Black and white, Pixel expansion, Secret image sharing",Xiaotian Wu and Ching-Nung Yang,https://www.sciencedirect.com/science/article/pii/S1047320320300432,https://doi.org/10.1016/j.jvcir.2020.102793,1047-3203,2020,102793,70,Journal of Visual Communication and Image Representation,Probabilistic color visual cryptography schemes for black and white secret images,article,WU2020102793
"Object detection technique is widely applied in modern intelligent systems, such as pedestrian tracking, video surveillance. Key frames selection aims to select more informative frames and reduce amount of redundant information frames. Traditional methods leveraged SIFT feature, which have high key frame selection error rate. In this paper, we propose a novel key frames selection method based on object detection and image quality. Specifically, we first leverage object detector to detect object, such as pedestrian, vehicles. Then, each training frame will be assigned with a quality score, where frames contain objects have high quality score. Afterwards, we leverage CNN based AlexNet architecture for deep feature representation extraction. Our algorithm combines mutual information entropy and SURF image local features to extract key frames. Comprehensive experiments verify the feasibility of practicing the key frame extractor based on convolutional neural network by training the model, and conduct a quality assessment model study.","Convolutional neural network, Key frame, Object detection, SIFT characteristics, Quality assessment model",Mingju Chen and Xiaofeng Han and Hua Zhang and Guojun Lin and M.M. Kamruzzaman,https://www.sciencedirect.com/science/article/pii/S1047320319302998,https://doi.org/10.1016/j.jvcir.2019.102678,1047-3203,2019,102678,65,Journal of Visual Communication and Image Representation,Quality-guided key frames selection from video stream based on object detection,article,CHEN2019102678
"With tone mapping, high dynamic range (HDR) image contents can be displayed on low dynamic range (LDR) display devices, in which some important visual information may be distorted. Thus, the tone mapped image (TMI) quality assessment is one of important issues in HDR image/video processing fields. Considering the difference of visual distortion degrees between the flat and complex regions in TMI, and considering that high-quality TMI should preserve as much information as possible of its original HDR image especially in the high/low luminance regions, this paper proposes a new blind TMI quality assessment method with image segmentation and visual perception. First, we design different features to describe the distortion of TMIâs different regions with two kinds of TMI segmentation. Then, considering that there lacks an efficient algorithm to quantify the importance of features, a feature clustering scheme is designed to eliminate the poor effect feature components in the extracted features to improve the effectiveness of the selected features. Finally, considering the diversity of tone mapping operator (TMO), which may cause global and local distortion of TMI, some other global features are also combined. At last, a final feature vector is formed to synthetically describe the distortion in TMI and used to blindly predict the TMIâs quality. Experimental results in the public ESPL-LIVE HDR database show that the Pearson linear correlation coefficient and Spearman rank order correlation coefficient of the proposed method reach 0.8302 and 0.7887, respectively, which is superior to the state-of-the-art blind TMI quality assessment methods, and it means that the proposed method is highly consistent with human visual perception.","High dynamic range image, Tone mapped image, Image quality assessment, Image segmentation, Visual perception, Feature clustering",Biwei Chi and Mei Yu and Gangyi Jiang and Zhouyan He and Zongju Peng and Fen Chen,https://www.sciencedirect.com/science/article/pii/S104732032030002X,https://doi.org/10.1016/j.jvcir.2020.102752,1047-3203,2020,102752,67,Journal of Visual Communication and Image Representation,Blind tone mapped image quality assessment with image segmentation and visual perception,article,CHI2020102752
"Urban residential environment surveillance plays an important role in modern intelligent city. Satellite images have been applied in various fields, and the analysis and processing of satellite images has become an important means to obtain the information perceived by satellites. This paper focuses on city residential environment surveillance based on massive-scale visual information retrieval. Since the shortcomings of low contrast, blurred boundary, large amount of information and susceptibility to noise, the performance of satellite image segmentation is not satisfactory, which will affect residential environment surveillance. We design an improved rough set fuzzy C-means clustering algorithm combined with ant colony algorithm. More specifically, satellite images are classified based on the gradient of pixels according to the indistinguishable relation of the image combined with rough set theory. Then, the traditional fuzzy set-based fuzzy C-means clustering algorithm is applied to the satellite image segmentation technology. Subsequently, the improved algorithm-quantum ant colony algorithm and rough set fuzzy clustering C-means algorithm are combined to achieve accurate segmentation of satellite images. Afterwards, we propose a satellite image retrieval algorithm, which can assist city residential environment surveillance. Comprehensive experiment show that our proposed method is effective and robust in residential environment surveillance.","Satellite image, Land policy, Rough set, Fuzzy C-means clustering, Quantum ant colony algorithm",Yuzhe Wu and Zhiyi Xu,https://www.sciencedirect.com/science/article/pii/S1047320319303608,https://doi.org/10.1016/j.jvcir.2019.102739,1047-3203,2020,102739,70,Journal of Visual Communication and Image Representation,Massive-scale visual information retrieval towards city residential environment surveillance,article,WU2020102739
"In this work, we study the power of Saak features as an effort towards interpretable deep learning. Being inspired by the operations of convolutional layers of convolutional neural networks, multi-stage Saak transform was proposed. Based on this foundation, we provide an in-depth examination on Saak features, which are coefficients of the Saak transform, by analyzing their properties through visualization and demonstrating their applications in image classification. Being similar to CNN features, Saak features at later stages have larger receptive fields, yet they are obtained in a one-pass feedforward manner without backpropagation. The whole feature extraction process is transparent and is of extremely low complexity. The discriminant power of Saak features is demonstrated, and their classification performance in three well-known datasets (namely, MNIST, CIFAR-10 and STL-10) is shown by experimental results.","Saak transform, Interpretable machine learning, Image classification, Adversarial attacks",Abinaya Manimaran and Thiyagarajan Ramanathan and Suya You and C.-C. Jay Kuo,https://www.sciencedirect.com/science/article/pii/S1047320319303207,https://doi.org/10.1016/j.jvcir.2019.102699,1047-3203,2020,102699,66,Journal of Visual Communication and Image Representation,"Visualization, Discriminability and Applications of Interpretable Saak Features",article,MANIMARAN2020102699
"Internet attacks pose a severe threat to most of the online resources and are a prime concern of security administrators these days. In spite of many efforts, the security techniques are unable to detect the intrusions accurately. Most of the methods suffer from the limitations of a high false positive rate, low detection rate and provide one solution which lacks the classification trade-offs. In this work, an effective two-stage method is proposed to produce a pool of non-dominating solutions or Pareto optimal solutions as base models and their ensembles for detecting the intrusions accurately. It generates Pareto optimal solutions to a chromosome structure in stage 1 formulating Pareto front. Whereas, another approximation to the Pareto front of optimal solutions is made to obtain non-dominating ensembles in the second stage. The final prediction ensemble solutions are computed from individual predictions using majority voting approach. Applicability of the suggested method is validated using benchmark dataset NSL-KDD dataset. The experimental results show that the recommended method provides better results than conventional ensemble techniques. The recommended method is also adequate to generate Pareto optimal solutions that address the issue of improving detection accuracy for minority as well as majority attack classes along with handling classification tradeoff problem. The proposed method resulted detection accuracy of 97% with FPR of 2% for KDD dataset respectively. The most attractive feature of the proposed method is that both generation of base classifier and their ensemble thereof are multi-objective in nature addressing the issue of low detection accuracy and classification tradeoffs.","Artificial Intelligence, Ensembles, Pattern recognition, Internet attacks, Neural networks",Hongwei Zhao and Mingzhao Li and Haoyu Zhao,https://www.sciencedirect.com/science/article/pii/S1047320319303578,https://doi.org/10.1016/j.jvcir.2019.102736,1047-3203,2020,102736,71,Journal of Visual Communication and Image Representation,Artificial intelligence based ensemble approach for intrusion detection systems,article,ZHAO2020102736
"With the rapid development of artificial intelligence technology, text categorization technology is becoming more and more mature. However, text categorization in real situations still faces various unconstrained conditions. English text is an important part of text information, it is also an important way for people to get information from abroad. How can everyone get the desired content from the massive data quickly and accurately, it has become a hot issue in current research. This paper improves the current text categorization algorithm based on English quality-related text categorization. The design and implementation of text categorization system are illustrated with an example of English quality-related text categorization system, complete the research work of text categorization algorithm. The core work of this paper is to mine, classify and analyze large amounts of data in English text by using the method of combining cyclic neural network with quality. Finally, the essential features of high quality English texts are obtained. Traditional English text categorization algorithm if the amount of training data is large, it is easy to show some defects such as unclear feature items. In view of these problems, in order to improve the accuracy and flexibility of English text categorization, this paper proposes a quality-related English text categorization method based on cyclic neural network. A mechanism combining attention is proposed to improve the problem of label disorder and make the structure of the model more flexible. The model proposed in this paper is compared and optimized. Experiments show that the accuracy of neural text classification based on quality classification can reach about 96%.","Recurrent neural network, Text categorization, English categorization, Feature Word Categorization",Cheng Liu and Xiaofang Wang,https://www.sciencedirect.com/science/article/pii/S1047320319303451,https://doi.org/10.1016/j.jvcir.2019.102724,1047-3203,2020,102724,71,Journal of Visual Communication and Image Representation,Quality-related English text classification based on recurrent neural network,article,LIU2020102724
"The Image Foresting Transform (IFT) is a graph-based framework to develop image operators based on optimum connectivity between a root set and the remaining nodes, according to a given path-cost function. Its applications involve a variety of tasks, such as segmentation, boundary tracking, skeletonization, filtering, among others. The Differential Image Foresting Transform (DIFT) allows multiple IFT executions for different root sets and a same monotonically incremental path-cost function, making the processing time proportional to the number of modified nodes. In this paper, we extend the DIFT algorithm for non-monotonically incremental functions with root-based increases. This proposed extension, called Generalized DIFT (GDIFT), has been successfully used as the core part of some modern superpixels methods with state-of-the-art results. Experimental results show considerable efficiency gains over the sequential flow of IFTs for the generation of superpixels, also avoiding inconsistencies in image segmentation, which could occur with the regular DIFT algorithm.","Image foresting transform, Superpixels, Differential image foresting transform",Marcos A.T. Condori and FÃ¡bio A.M. Cappabianco and Alexandre X. FalcÃ£o and Paulo A.V. Miranda,https://www.sciencedirect.com/science/article/pii/S1047320319303694,https://doi.org/10.1016/j.jvcir.2019.102748,1047-3203,2020,102748,71,Journal of Visual Communication and Image Representation,An extension of the differential image foresting transform and its application to superpixel generation,article,CONDORI2020102748
"For the compression of special image, such as medical image and aerial image, the reconstructed image quality is of utmost importance in the performance analysis. In this paper, a high-resolution quantization scheme based on the exp-Golomb code is proposed, aiming at improving the reconstructed image quality and realizing high-resolution near-lossless compression. Rather than quantizing the whole image uniformly, which adopted by most popular quantization schemes, an adaptive quantizer with smaller distortion is designed. Both the quantization step size and the quantization proportion are determined adaptively. Compression algorithms based on our adaptive quantizer can provide better reconstructed image quality, and the high frequency information of the image, such as texture and edge, can be better preserved.","High-resolution quantization scheme, Adaptive quantizer, Exp-Golomb code, Medical image, Aerial image, Near-lossless compression",Xiaoying Song and Bing Liu and Qijun Huang and Ruihan Hu,https://www.sciencedirect.com/science/article/pii/S1047320319303050,https://doi.org/10.1016/j.jvcir.2019.102684,1047-3203,2019,102684,65,Journal of Visual Communication and Image Representation,Design of high-resolution quantization scheme with exp-Golomb code applied to compression of special images,article,SONG2019102684
"With the development of urban metro, the research on structural diseases of shield tunnels has been becoming a hot research topic, especially the leakage water diseases. Deep learning-based algorithms have shown impressive performance in image processing domain, such as image classification, image recognition or image retrieval. In this paper, we propose a novel image recognition algorithm for water leakage diseases of shield tunnels based on deep learning algorithm. Water leakage images are classified into six categories, each of which are extracted deep representation for image recognition. We compare our method with Otsu algorithm (OA), Region Growing Algorithm (RGA), and Watershed Algorithm (WA) to show the effectiveness of our proposed method.","Shield tunnel, Water leakage, Deep learning, Image recognition",Leijin Xiong and Dingli Zhang and Yu Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319303293,https://doi.org/10.1016/j.jvcir.2019.102708,1047-3203,2020,102708,71,Journal of Visual Communication and Image Representation,Water leakage image recognition of shield tunnel via learning deep feature representation,article,XIONG2020102708
"Fusing cross-modal features is significant for image understanding, which aims at describing objects inside an image by optimally combining multiple visual channels. In the literature, low-level based multimodal feature fusion have achieved impressive performance. However, the semantic gap is a big limitation, i.e., these methods cannot reflect the how humans perceive image semantic objects. Supervised learning-based methods require intolerably expensive manual labeling, which is not a good choice in practice. To alleviate these limitations, we present an image understanding method by learning weakly-supervised based cross-modal semantic translation. More specifically, we design a manifold embedding algorithm to automatically translate image-level text semantic labels into several pixel-level image regions. Subsequently, we leverage a three-level spatial pyramid model to extract both local and global features of objects from training images. Afterwards, these cross-modal features are seamlessly concatenated to form a multiple feature matrix. Afterwards, these cross-modal features are seamlessly concatenated to form a multiple feature matrix. The feature matrix can be employed to learn a kernel SVM and ranking SVM for image classification and retrieval respectively. Comprehensive experiments on image recognition, classification and retrieval have demonstrated the effectiveness of our method.","Image understanding, Cross-modal semantic translation, Weakly-supervised learning",Guorong Shen,https://www.sciencedirect.com/science/article/pii/S1047320320300390,https://doi.org/10.1016/j.jvcir.2020.102789,1047-3203,2020,102789,71,Journal of Visual Communication and Image Representation,Image understanding via learning weakly-supervised cross-modal semantic translation,article,SHEN2020102789
"In this paper, we present a new approach for dynamic hand gesture recognition. Our goal is to integrate spatiotemporal features extracted from multimodal data captured by the Kinect sensor. In case the skeleton data is not provided, we apply a novel skeleton estimation method to compute temporal features. Furthermore, we introduce an effective method to extract a fixed number of keyframes to reduce the processing time. To extract pose features from RGB-D data, we take advantage of two different approaches: (1) Convolutional Neural Networks and (2) Histogram of Cumulative Magnitudes. We test different integration methods to fuse the extracted spatiotemporal features to boost recognition performance in a linear SVM classifier. Extensive experiments prove the effectiveness and feasibility of the proposed framework for hand gesture recognition.","Hand gesture recognition, Spherical coordinates, Keyframe extraction, Pose and motion information, Convolucional neuronal networks, Histogram of cumulative magnitudes, Fusion schemes",Edwin Jonathan {Escobedo Cardenas} and Guillermo Camara Chavez,https://www.sciencedirect.com/science/article/pii/S1047320320300225,https://doi.org/10.1016/j.jvcir.2020.102772,1047-3203,2020,102772,71,Journal of Visual Communication and Image Representation,Multimodal hand gesture recognition combining temporal and pose information based on CNN descriptors and histogram of cumulative magnitudes,article,ESCOBEDOCARDENAS2020102772
"Image quality assessment is an indispensable in computer vision applications, such as image classification, image parsing. With the development of Internet, image data acquisition becomes more conveniently. However, image distortion is inevitable due to imperfect image acquisition system, image transmission medium and image recording equipment. Traditional image quality assessment algorithms only focus on low-level visual features such as color or texture, which could not encode high-level features effectively. CNN-based methods have shown satisfactory results in image quality assessment. However, existing methods have problems such as incomplete feature extraction, partial image block distortion, and inability to determine scores. So in this paper, we propose a novel framework for image quality assessment based on deep learning. We incorporate both low-level visual features and high-level semantic features to better describe images. And image quality is analyzed in a parallel processing mode. Experiments are conducted on LIVE and TID2008 datasets demonstrate the proposed model can predict the quality of the distorted image well, and both SROCC and PLCC can reach 0.92 or higher.","Deep learning, Image distortion, Image quality analysis, Parallelization",Jui-Chan Huang and Hao-Chen Huang and Hsin-Hung Liu,https://www.sciencedirect.com/science/article/pii/S104732031930330X,https://doi.org/10.1016/j.jvcir.2019.102709,1047-3203,2020,102709,71,Journal of Visual Communication and Image Representation,Research on the parallelization of image quality analysis algorithm based on deep learning,article,HUANG2020102709
"Locality preserving projection (LPP) is a widely used linear dimensionality reduction method, which preserves the locality structure of the original data. Motivated by the fact that kernel technique can capture nonlinear similarity of features and help to improve separability between nearby data points, this paper proposes locality preserving projection model based on Euler representation (named as ELPP). This model first projects the data into a complex space with Euler representation, then learns the dimensionality reduction projection with preserving locality structure in this complex space. We also extend ELPP to F-ELPP by replacing the squared F-norm with F-norm, which will weaken the exaggerated errors and be more robustness to outliers. The optimization algorithms of the two models are given, and the convergence of F-ELPP is proved. A large number of experiments on several public databases have demonstrated that the two proposed models have good robustness and feature extraction ability.","Locality preserving projection, Euler representation, Dimensionality reduction",Tianhang Long and Yanfeng Sun and Junbin Gao and Yongli Hu and Baocai Yin,https://www.sciencedirect.com/science/article/pii/S1047320320300468,https://doi.org/10.1016/j.jvcir.2020.102796,1047-3203,2020,102796,70,Journal of Visual Communication and Image Representation,Locality preserving projection based on Euler representation,article,LONG2020102796
"Variational Auto-Encoder (VAE) is an important probabilistic technology to model 1D vectorial data. However, when applying VAE model to 2D image, vectorization is necessary. Vectorization process may lead to dimension curse and lose valuable spatial information. To avoid these problems, we propose a novel VAE model based on matrix variables named as Matrix-variate Variational Auto-Encoder (MVVAE). In this model, input, hidden and latent variables are all in matrix form, therefore inherent spatial structure of 2D images can be maintained and utilized better. Especially, the latent variable is assumed to follow matrix Gaussian distribution which is more suitable for describing 2D images. To solve the weights and the posterior of latent variable, the variational inference process is given. The experiments are designed for three real-world application: reconstruction, denoising and completion. The experimental results demonstrate that MVVAE shows better performance than VAE and other probabilistic methods for modeling and processing 2D data.","Variational autoencoder, Matrix Gaussian distribution, Variational inference, Face completion, Image denoising",Jinghua Li and Huixia Yan and Junbin Gao and Dehui Kong and Lichun Wang and Shaofan Wang and Baocai Yin,https://www.sciencedirect.com/science/article/pii/S1047320319303712,https://doi.org/10.1016/j.jvcir.2019.102750,1047-3203,2020,102750,67,Journal of Visual Communication and Image Representation,Matrix-variate variational auto-encoder with applications to image process,article,LI2020102750
"Detecting and recognizing human action in natural scenarios, such as indoor and outdoor, is a significant technique in computer vision and intelligent systems, which is widely applied in video surveillance, pedestrian tracking and human-computer interaction. Conventional approaches have been proposed based on various features and achieved impressive performance. However, these methods failed to cope with partial occlusion and changes of posture. In order to address these limitations, we propose a novel human action recognition method. More specifically, in order to capture image spatial composition, we leverage a three-level spatial pyramid feature extraction scheme, where each pyramid is encoded by local features. Thereafter, regions generated by a proposal algorithm are fed into a dual-aggregation net for deep representation extraction. Afterwards, both local features and deep features are fused to describe each image. To describe human action category, we design a metric CXQDA based on Cosine measure and Cross-view Quadratic Discriminant Analysis (XQDA) to calculate the similarity among different action categories. Experimental results demonstrate that our proposed method can effectively cope with object scale variations, partial occlusion and achieve competitive performance.","Human action recognition, Spatial pyramid, Convolution neural networks, Cosine distance measure",Jihai Xiao and Xiaohong Cui and Feng Li,https://www.sciencedirect.com/science/article/pii/S1047320319303438,https://doi.org/10.1016/j.jvcir.2019.102722,1047-3203,2020,102722,71,Journal of Visual Communication and Image Representation,Human action recognition based on convolutional neural network and spatial pyramid representation,article,XIAO2020102722
"Multi-focus image fusion aims to produce an all-in-focus image by merging multiple partially focused images of the same scene. The main work is identifying the focused region and then composing all the focused regions. In this paper, a novel efficient multi-focus image fusion method based on distributed compressed sensing (DCS) is proposed. Firstly, the low-frequency and high-frequency images are obtained by comparing the variance of the source images, which are further utilized to get the low-frequency and high-frequency dictionaries. Secondly, DCS using joint sparsity model-1 (JSM-1) is applied to reconstruct the precise high-frequency images. Thirdly, the decision map is obtained based on all the high-frequency images and then improved by the morphological processing. Finally, the focused pixels are chosen from the source images through the decision map. Experimental results indicate that the proposed DCS-based method can be competitive with or even outperform some state-of-the-art methods in terms of both visual and quantitative metric evaluations.","Distributed compressed sensing, Decision map, Multi-focus image fusion, Joint-sparsity-model-1",Guan-Peng Fu and Shao-Hua Hong and Fu-Lin Li and Lin Wang,https://www.sciencedirect.com/science/article/pii/S1047320320300109,https://doi.org/10.1016/j.jvcir.2020.102760,1047-3203,2020,102760,67,Journal of Visual Communication and Image Representation,A novel multi-focus image fusion method based on distributed compressed sensing,article,FU2020102760
"Infrared (IR) saliency detection with high detection accuracy is a challenging task due to the complex background and low contrast of IR images. In this paper, an IR saliency detection method via a new visual attention framework is proposed, which comprises two phases. In the first phase, a Gray & Contrast Features (GCF) model is established, in which the IR image is processed in two feature channels, a gray feature channel and a contrast feature channel. And then a primary feature map can be obtained by fusing the gray and contrast features from these two channels, which is the basis of the second phase. In the second phase, a Similarity-based Bayes (SB) model is established, in which two prior probabilities and two likelihood functions are calculated according to the previously obtained primary feature map. Finally, the saliency map is calculated with the obtained prior probabilities and likelihood functions by Bayes formula. Experimental results indicate that the proposed method can effectively reduce noise and enhance contrast of IR images with complex background and low contrast, and obtain a higher detection accuracy and robustness than seven state-of-the-art methods.","Saliency detection, IR images, Bayes formula, Visual attention",Yufei Zhao and Yong Song and Xu Li and Muhammad Sulaman and Zhengkun Guo and Xin Yang and Fengning Wang and Qun Hao,https://www.sciencedirect.com/science/article/pii/S104732031930327X,https://doi.org/10.1016/j.jvcir.2019.102706,1047-3203,2020,102706,66,Journal of Visual Communication and Image Representation,IR saliency detection via a GCF-SB visual attention framework,article,ZHAO2020102706
"With the development of Internet, personalized recommendation has played an important role in human modern lives. Since the number of usersâ data is always large-scale, traditional algorithms cannot effectively cope with e-commerce personalized recommendation tasks. This paper proposes an e-commerce product personalized recommendation system based on learning clustering representation. Traditional kNN method has limitation in selecting adjacent object set. Thus, we introduce neighbor factor and time function and leverage dynamic selection model to select the adjacent object set. We combine RNN as well as attention mechanism to design the e-commerce product recommendation system. Comprehensive experimental results have shown the effectiveness of our proposed method.","Clustering algorithm, Deep learning, Recommendation system",Kai Wang and Tiantian Zhang and Tianqiao Xue and Yu Lu and Sang-Gyun Na,https://www.sciencedirect.com/science/article/pii/S1047320319303566,https://doi.org/10.1016/j.jvcir.2019.102735,1047-3203,2020,102735,71,Journal of Visual Communication and Image Representation,E-commerce personalized recommendation analysis by deeply-learned clustering,article,WANG2020102735
"Multiple object tracking is still a challenging problem in computer vision even though there have been several attempts lately to resolve the tracking problem in the framework of deep neural networks. In this paper, a novel method for multiple object tracking in soccer videos, which often contain complicated interactions between players with severe occlusions, is introduced. To do this, we propose to interpret the extracted foreground regions in a given frame as the topographic surface. This gives a great help to reliably chase target players by accurately providing the boundary lines of each object even with occlusions. Color similarity and spatial proximity are subsequently employed to refine the estimated position of target players for continuous tracking over whole video sequences. Experimental results on various soccer videos, which are taken of the actual games with the wide-angle camera, demonstrate that the proposed method is effective for tracking multiple players in the dynamic scene of the soccer video.","Multiple object tracking, Topographic surface, Color similarity, Spatial proximity",Wonjun Kim,https://www.sciencedirect.com/science/article/pii/S1047320319303049,https://doi.org/10.1016/j.jvcir.2019.102683,1047-3203,2019,102683,65,Journal of Visual Communication and Image Representation,Multiple object tracking in soccer videos using topographic surface analysis,article,KIM2019102683
"Spatiotemporal irregularities (i.e., the uncommon appearance and motion patterns) in videos are difficult to detect, as they are usually not well defined and appear rarely in videos. We tackle this problem by learning normal patterns from regular videos, while treating irregularities as deviations from normal patterns. To this end, we introduce a 3D fully convolutional autoencoder (3D-FCAE) that is trainable in an end-to-end manner to detect both temporal and spatiotemporal irregularities in videos using limited training data. Subsequently, temporal irregularities can be detected as frames with high reconstruction errors, and irregular spatiotemporal patterns can be detected as blurry regions that are not well reconstructed. Our approach can accurately locate temporal and spatiotemporal irregularities thanks to the 3D fully convolutional autoencoder and the explored effective architecture. We evaluate the proposed autoencoder for detecting irregular patterns on benchmark video datasets with weak supervision. Comparisons with state-of-the-art approaches demonstrate the effectiveness of our approach. Moreover, the learned autoencoder shows good generalizability across multiple datasets.","Spatiotemporal irregularity detection, Autoencoder, 3D convolution, Anomaly detection, Unsupervised learning, Real-time",Mengjia Yan and Jingjing Meng and Chunluan Zhou and Zhigang Tu and Yap-Peng Tan and Junsong Yuan,https://www.sciencedirect.com/science/article/pii/S1047320319303682,https://doi.org/10.1016/j.jvcir.2019.102747,1047-3203,2020,102747,67,Journal of Visual Communication and Image Representation,Detecting spatiotemporal irregularities in videos via a 3D convolutional autoencoder,article,YAN2020102747
"Multimedia covers a wide range. In general, digital audio production, animation video production, website production, and even game development can all be attributed to multimedia. The definition of multimedia narrowly defined, that is, the project with interactive program development as the main object of this paper, such as interactive CD production, touch screen presentation production, etc. Of course, there will still be a lot of content related to graphic design, animation, video processing, audio production and so on. The application of multimedia technology in water conservancy and hydro-power engineering is characterized by a variety of media means to represent the design, construction process and post-construction scene of water conservancy and hydro-power projects and to simulate the phenomenon in the project, such as the performance of water conservancy and hydro-power projects. Hub layout, structure of main buildings, dam flood discharge, rubber dam dam overflow, sluice dispatching process, ship lock crossing process, etc. In the water conservancy and hydro-power project, computer multimedia technology has been widely used from the general design proposal to the entire pivot project demonstration system. This paper mainly introduces the design and development of the multimedia demonstration system for water conservancy and hydro-power projects.","Multimedia, Water conservancy and cydro-power, Demonstration system, Information technology",Jingfeng Zhao and Jing Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319303281,https://doi.org/10.1016/j.jvcir.2019.102707,1047-3203,2020,102707,71,Journal of Visual Communication and Image Representation,Application of multimedia technology in water conservancy and hydropower engineering,article,ZHAO2020102707
"Though there have been many research works carried out on the grayscale secret image sharing schemes, there is not sufficient research work available on the field of color secret image sharing schemes, especially on the lossless secret image sharing schemes. In the current work, we propose a new lossless secret color image sharing scheme with small shadow size. We directly deal with the three components of the intensities of the RGB values of each color pixel of the secret image. Using a mathematical transformation, we first embed the RGB intensity values of each secret pixel as an element of a suitably chosen finite field F. Then the whole mathematical operations are carried over the finite field F to generate shares using k-1 degree polynomials in the polynomial ring F[x]. Unlike most of the image sharing schemes, we do not have the preprocessing stage in which the secret image is transferred into random image either by using Arnold Cat map or by using some chaotic maps to avoid residual image effect. From experimental simulations, it is evident that the measure of randomness of the shares generated using our algorithm is same as that of the shares generated using the preprocessing stage in which Arnold Cat map or chaotic map is used, resulting our scheme simpler and efficient. Moreover, our scheme produces shares having smaller size than the secret image. Finally the recovery of the secret image by the qualified set of participants is lossless.","Secret image sharing, -threshold scheme, Small shadow size, Lossless recovery, Color image",Md Kutubuddin Sardar and Avishek Adhikari,https://www.sciencedirect.com/science/article/pii/S1047320320300183,https://doi.org/10.1016/j.jvcir.2020.102768,1047-3203,2020,102768,68,Journal of Visual Communication and Image Representation,A new lossless secret color image sharing scheme with small shadow size,article,SARDAR2020102768
"This paper proposes a new method for estimating quantization steps (QSs) from an image that has been previously JPEG-compressed and stored in a lossless format. In this method, DCT coefficients of each frequency band of JPEG-compressed image are aggregated in the QS and its multiples. The entire estimation process can be grouped into two categories: alternating and direct current bands. Considering that DCT coefficients under different QSs show different periodicity, QS estimation for each band is then further divided into three steps, which involve identifying whether the QS is one, two, or another value. For each step, the periodicity of DCT coefficients can be well exploited with the analyses of the DCT-coefficient histogram and its corresponding frequency magnitude spectrum. Experimental results demonstrate the efficacy of the proposed method and the superiority in QS estimation for previously JPEG-compressed images, especially in the case that the actual QSs are higher than two.","Image forensics, JPEG compression, Quantization-step estimation, Periodicity analysis",Heng Yao and Hongbin Wei and Tong Qiao and Chuan Qin,https://www.sciencedirect.com/science/article/pii/S1047320320300456,https://doi.org/10.1016/j.jvcir.2020.102795,1047-3203,2020,102795,69,Journal of Visual Communication and Image Representation,JPEG quantization step estimation with coefficient histogram and spectrum analyses,article,YAO2020102795
"Advances in video acquisition and storage technologies have promoted a great demand for automatic recognition of actions. The use of cameras for security and surveillance purposes has applications in several scenarios, such as airports, parks, banks, stations, roads, hospitals, supermarkets, industries, stadiums, schools. An inherent difficulty of the problem is the complexity of the scene under usual recording conditions, which may contain complex background and motion, multiple people on the scene, interactions with other actors or objects, and camera motion. Most recent databases are built primarily with shared recordings on YouTube and with snippets of movies, situations where these obstacles are not restricted. Another difficulty is the impact of the temporal dimension since it expands the size of the data, increasing computational cost and storage space. In this work, we present a methodology of volume description using the Visual Rhythm (VR) representation. This technique reshapes the original volume of the video into an image, where two-dimensional descriptors are computed. We investigated different strategies for constructing the representation by combining configurations in several image domains and traversing directions of the video frames. From this, we propose two feature extraction methods, NaÃ¯ve Visual Rhythm (NaÃ¯ve VR) and Visual Rhythm Trajectory Descriptor (VRTD). The first approach is the straightforward application of the technique in the original video volume, forming a holistic descriptor that considers action events as patterns and formats in the visual rhythm image. The second variation focuses on the analysis of small neighborhoods obtained from the process of dense trajectories, which allows the algorithm to capture details unnoticed by the global description. We tested our methods in eight public databases, one of hand gestures (SKIG), two in first person (DogCentric and JPL), and five in third person (Weizmann, KTH, MuHAVi, UCF11 and HMDB51). The results show that the developed techniques are able to extract motion elements along with format and appearance information, achieving competitive accuracy rates compared to state-of-the-art action recognition approaches.","Action recognition, Visual rhythm, Video sequences, Computer vision",Thierry Pinheiro Moreira and David Menotti and Helio Pedrini,https://www.sciencedirect.com/science/article/pii/S1047320320300213,https://doi.org/10.1016/j.jvcir.2020.102771,1047-3203,2020,102771,71,Journal of Visual Communication and Image Representation,Video action recognition based on visual rhythm representation,article,MOREIRA2020102771
"High Efficiency Video Coding (HEVC) is the latest encoder that increased the intra modes from 9 to 35 to efficiently handle the contents of the video. The HEVCâs test model (HM) selects the optimal intra mode using the brute force method which increases the complexity of HEVC. This work, firstly, investigates the feasibility of firefly algorithm (FFA) due to its exploration and exploitation characteristics to expedite the intra mode decision in HEVC. Secondly, a novel objective function is formulated for FFA to efficiently compute the brightness for intra modes in FFA. Thirdly, the parameters of FFA are made dynamic to adjust according to the contents of video sequences. Simulation results demonstrate that the nature inspired algorithm, FFA, pays off by saving a minimum of 27% of the total coding time on average and doesnât sacrifice quality by limiting Bjontegaard delta bit rate (BD-BR) increase to only 0.98% on average.","HEVC, Intra mode, Firefly algorithm, Video coding, Most probable mode",Junaid Tariq and Ammar Armghan and Amir Ijaz and Imran Ashraf,https://www.sciencedirect.com/science/article/pii/S104732032030016X,https://doi.org/10.1016/j.jvcir.2020.102766,1047-3203,2020,102766,68,Journal of Visual Communication and Image Representation,Pure intra mode decision in HEVC using optimized firefly algorithm,article,TARIQ2020102766
"In this paper, we present a salient object detection method based on novel graph structure. Given image is segmented into small image regions as basic units, we firstly construct an effective background-based map, each image regionâs saliency value is determined by its feature contrast with the image boundary. Then, saliency propagation mechanism is used to update all regionsâ saliency values by introducing a novel graph structure to better exploit the relationship between adjacent image regions. Finally, we propose an optimization method to further highlight salient objects and suppress background noises. Experimental results demonstrate adequately the superiority of proposed approach.","Salient object detection, Background prior, Novel graph model, Saliency propagation, Optimization method",Yu Pang and Xiaosheng Yu and Ying Wang and Chengdong Wu,https://www.sciencedirect.com/science/article/pii/S1047320319302974,https://doi.org/10.1016/j.jvcir.2019.102676,1047-3203,2019,102676,65,Journal of Visual Communication and Image Representation,Salient object detection based on novel graph model,article,PANG2019102676
"A new machine learning methodology, called successive subspace learning (SSL), is introduced in this work. SSL contains four key ingredients: (1) successive near-to-far neighborhood expansion; (2) unsupervised dimension reduction via subspace approximation; (3) supervised dimension reduction via label-assisted regression (LAG); and (4) feature concatenation and decision making. An image-based object classification method, called PixelHop, is proposed to illustrate the SSL design. It is shown by experimental results that the PixelHop method outperforms the classic CNN model of similar model complexity in three benchmarking datasets (MNIST, Fashion MNIST and CIFAR-10). Although SSL and deep learning (DL) have some high-level concept in common, they are fundamentally different in model formulation, the training process and training complexity. Extensive discussion on the comparison of SSL and DL is made to provide further insights into the potential of SSL.","Machine learning, Subspace learning, Computer vision, Pattern recognition",Yueru Chen and C.-C. Jay Kuo,https://www.sciencedirect.com/science/article/pii/S1047320319303700,https://doi.org/10.1016/j.jvcir.2019.102749,1047-3203,2020,102749,70,Journal of Visual Communication and Image Representation,PixelHop: A successive subspace learning (SSL) method for object recognition,article,CHEN2020102749
"Hyperspectral imagery has been widely used in military and civilian research fields such as crop yield estimation, mineral exploration, and military target detection. However, for the limited imaging equipment and the complex imaging environment of hyperspectral images, the spatial resolution of hyperspectral images is still relatively low, which limits the application of hyperspectral images. So, studying the data characteristics of hyperspectral images deeply and improving the spatial resolution of hyperspectral images is an important prerequisite for accurate interpretation and wide application of hyperspectral images. The purpose of this paper is to deal with super-resolution of the hyperspectral image quickly and accurately, and maintain the spectral characteristics of the hyperspectral image, makes the spectral separability of the substrate in the original image remains unchanged after super-resolution processing. This paper first learns the mapping relationship between the spectral difference of low-resolution hyperspectral image and the spectral difference of the corresponding high-resolution hyperspectral image based on multiple scale convolutional neural network, Thus, apply this mapping relationship to the input low-resolution hyperspectral image generally, getting the corresponding high resolution spectral difference. Constrained space by using the image of reconstructed spectral difference, this requires the low-resolution hyperspectral image generated by the reconstructed image is to be close to the input low-resolution hyperspectral image in space, so that the whole process becomes a closed circulation system where the low-resolution hyperspectral image generation of high-resolution hyperspectral images, then back to low-resolution hyperspectral images. This innovative design further enhances the super-resolution performance of the algorithm. The experimental results show that the hyperspectral image super-resolution method based on convolutional neural network improves the input image spatial information, and the super-resolution performance of the model is above 90%, which can maintain the spectral information well.","Hyperspectral image, Multi-scale deep convolutional network, Quality research, Super-resolution processing",Lei Liu and Min Sun and Xiang Ren and Xiuxian Li and Qiaoru Zhang and Li Ma and Yongning Li and Mo Song,https://www.sciencedirect.com/science/article/pii/S1047320319303426,https://doi.org/10.1016/j.jvcir.2019.102721,1047-3203,2020,102721,71,Journal of Visual Communication and Image Representation,Hyperspectral image quality based on convolutional network of multi-scale depth,article,LIU2020102721
"Screen content image (SCI) is a composite image including textual and pictorial regions resulting in many difficulties in image quality assessment (IQA). Large SCIs are divided into image patches to increase training samples for CNN training of IQA model, and this brings two problems: (1) local quality of each image patch is not equal to subjective differential mean opinion score (DMOS) of an entire image; (2) importance of different image patches is not same for quality assessment. In this paper, we propose a novel no-reference (NR) IQA model based on the convolutional neural network (CNN) for assessing the perceptual quality of SCIs. Our model conducts two designs solving problems which benefits from two strategies. For the first strategy, to imitate full-reference (FR) CNN-based model behavior, a CNN-based model is designed for both FR and NR IQA, and performance of NR-IQA part improves when the image patch scores predicted by FR-IQA part are adopted as the ground-truth to train NR-IQA part. For the second strategy, image patch qualities of one entire SCI are fused to obtain the SCI quality with an adaptive weighting method taking account the effect of the different image patch contents. Experimental results verify that our model outperforms all test NR IQA methods and most FR IQA methods on the screen content image quality assessment database (SIQAD). On the cross-database evaluation, the proposed method outperforms the existing NR IQA method in terms of at least 2.4 percent in PLCC and 2.8 percent in SRCC, which shows high generalization ability and high effectiveness of our model.","Image quality assessment, Screen content image, No-reference, Convolutional neural network",Xuhao Jiang and Liquan Shen and Qing Ding and Linru Zheng and Ping An,https://www.sciencedirect.com/science/article/pii/S1047320319303669,https://doi.org/10.1016/j.jvcir.2019.102745,1047-3203,2020,102745,67,Journal of Visual Communication and Image Representation,Screen content image quality assessment based on convolutional neural networks,article,JIANG2020102745
"Localizing visitors in natural environments is challenging due to the unavailability of pre-installed cameras or other infrastructure such as WiFi networks. We propose to perform localization using egocentric images collected from the visitorâs point of view with a wearable camera. Localization can be useful to provide services to both the visitors (e.g., showing where they are or what to see next) and to the site manager (e.g., to understand what the visitors pay more attention to and what they miss during their visits). We collected and publicly released a dataset of egocentric videos asking 12 subjects to freely visit a natural site. Along with video, we collected GPS locations by means of a smartphone. Experiments comparing localization methods based on GPS and images highlight that image-based localization is much more reliable in the considered domain and small improvements can be achieved by combining GPS- and image-based predictions using late fusion.","Egocentric (First Person) vision, Localization, GPS, Multimodal data fusion",Filippo L.M. Milotta and Antonino Furnari and Sebastiano Battiato and Giovanni Signorello and Giovanni M. Farinella,https://www.sciencedirect.com/science/article/pii/S1047320319302858,https://doi.org/10.1016/j.jvcir.2019.102664,1047-3203,2019,102664,65,Journal of Visual Communication and Image Representation,Egocentric visitors localization in natural sites,article,MILOTTA2019102664
"This paper presents a novel end-to-end trainable deep architecture to learn an attentive dynamic map (ADM) for understanding human motion from skeleton data. An ADM intends not only to capture the dynamic information over the period of human motion, referred to as an action, as the conventional dynamic image/map does, but also to embed in it the spatio-temporal attention for the classification of the action. Specifically, skeleton sequences are encoded into sequences of Skeleton Joint Maps (STMs), each STM encodes both joint location (i.e. spatial) and relative temporal order (i.e. temporal) of the skeleton in the sequence. The STM sequences are fed into a customized 3DConvLSTM to explore the local and global spatio-temporal information from which a dynamic map is learned. This dynamic map is subsequently used to learn the spatio-temporal attention at each time-stamp. ADMs are then generated from the learned attention weights and all hidden states of the 3DConvLSTM and used for action classification. The proposed method achieved competitive performance compared with the state-of-the-art results on the Large Scale Combined dataset, MSRC-12 dataset and NTU RGB+D dataset.","Human-robot/machine interaction, Deep learning, Human action recognition",Chuankun Li and Yonghong Hou and Wanqing Li and Pichao Wang,https://www.sciencedirect.com/science/article/pii/S1047320319302615,https://doi.org/10.1016/j.jvcir.2019.102640,1047-3203,2019,102640,65,Journal of Visual Communication and Image Representation,Learning attentive dynamic maps (ADMs) for Understanding Human Actions,article,LI2019102640
"The existing video compressed sensing (CS) algorithms for inconsistent sampling ignore the joint correlations of video signals in space and time, and their reconstruction quality and speed need further improvement. To balance reconstruction quality with computational complexity, we introduce a structural group sparsity model for use in the initial reconstruction phase and propose a weight-based group sparse optimization algorithm acting in joint domains. Then, a coarse-to-fine optical flow estimation model with successive approximation is introduced for use in the interframe prediction stage to recover non-key frames through alternating optical flow estimation and residual sparse reconstruction. Experimental results show that, compared with the existing algorithms, the proposed algorithm achieves a peak signal-to-noise ratio gain of 1â3dB and a multi-scale structural similarity gain of 0.01â0.03 at a low time complexity, and the reconstructed frames not only have good edge contours but also retain textural details.","Compressed sensing, Group sparsity, Interframe estimation, Reconstruction algorithms",Jian Chen and Zhifeng Chen and Kaixiong Su and Zheng Peng and Nam Ling,https://www.sciencedirect.com/science/article/pii/S1047320319303554,https://doi.org/10.1016/j.jvcir.2019.102734,1047-3203,2020,102734,66,Journal of Visual Communication and Image Representation,Video compressed sensing reconstruction based on structural group sparsity and successive approximation estimation model,article,CHEN2020102734
"Digital images can be convincingly edited using image editing tools. In order to identify such image processing operations, various forensic techniques have been proposed. In response, anti-forensic operations designed as counter-measures have been devised. In this paper, we propose an anti-forensic technique to counter spatial domain forensic detectors and demonstrate its accuracy on popular image manipulation operations such as median filtering and contrast enhancement. The integrated anti-forensic attack is formulated as an optimization problem. The proposed optimization modifies the image so as to incorporate the median filtering or contrast enhancement operation while ensuring that its spatial characteristics do not change significantly. Through a series of experiments, we prove that the proposed algorithm can severely degrade the performance of median filtering and contrast enhancement detectors. The proposed algorithm also outperforms popular anti-forensic algorithms.","Anti-forensics, Median filtering, Contrast enhancement, Huber Markov random field",Shishir Sharma and Hareesh Ravi and A.V. Subramanyam and Sabu Emmanuel,https://www.sciencedirect.com/science/article/pii/S1047320319303037,https://doi.org/10.1016/j.jvcir.2019.102682,1047-3203,2020,102682,66,Journal of Visual Communication and Image Representation,Anti-forensics of median filtering and contrast enhancement,article,SHARMA2020102682
"To counter face presentation attacks in face recognition (FR), color texture has been successfully used for face presentation attack detection (PAD) in recent years. However, the existing research does not fully consider the correlation between different color channels as well as the optimization of classification for face PAD. To resolve these limitations, a face PAD scheme based on chromatic co-occurrence of local binary pattern (CCoLBP) and ensemble learning (EL) is proposed in this paper. A color distortion-based face PAD model is first built, and then the chromatic discrepancies between bona fide faces and artefacts are analyzed. After that, CCoLBP is extracted as the feature to characterize these discrepancies. Meanwhile, an EL based classifier is put forward to reduce the effect of class imbalance and to improve the generalization ability. Experimental results and analysis indicate that the proposed scheme can achieve an overall good performance. Moreover, it can achieve significant improvement in the cross-database test, and its computational complexity can meet the requirement of real time applications.","Presentation attack detection, Face recognition, Color distortion, Chromatic co-occurrence of local binary pattern, Ensemble learning",Fei Peng and Le Qin and Min Long,https://www.sciencedirect.com/science/article/pii/S1047320319303670,https://doi.org/10.1016/j.jvcir.2019.102746,1047-3203,2020,102746,66,Journal of Visual Communication and Image Representation,Face presentation attack detection based on chromatic co-occurrence of local binary pattern and ensemble learning,article,PENG2020102746
"Image modeling towards sport scenes plays an important role in sport image classification and analysis. Traditional algorithms for sport image modeling required carefully hand-crafted features, which cannot be popularized in practical application, especially with the emergence of massive-scale data. Weakly-supervised learning algorithms have shown effectiveness in modeling data with image-level labels. Thus, in this paper, we propose a weakly-supervised learning based method for sport image modeling without utilizing bounding box annotations, which can be used for various sport image applications. More specifically, we first collect large-scale sport images from existing datasets and Internet, and we annotate them at image-level labels. Subsequently, we leverage region proposal generation algorithm to select discriminative regions that can effectively represent the category of images. Each region is fed into a pre-trained CNN architecture to extract deep representation. Afterwards, we design an improved multiple discriminant analysis (MDA) algorithm to project these datapoints to a subspace that can more easily to distinguish different sport categories. Comprehensive experiments have shown the effectiveness and robustness of our proposed method.","Sport scene modeling, Weakly-supervised learning, MDA",Congsheng Lu and Feng Zhai,https://www.sciencedirect.com/science/article/pii/S1047320319303396,https://doi.org/10.1016/j.jvcir.2019.102718,1047-3203,2020,102718,71,Journal of Visual Communication and Image Representation,Weakly-supervised large-scale image modeling for sport scenes and its applications,article,LU2020102718
"Aerators are essential and crucial auxiliary devices in intensive culture, especially in industrial culture in China. In this paper, we propose a real-time expert system for anomaly detection of aerators based on computer vision technology and existing surveillance cameras. The expert system includes two modules, i.e., object region detection and working state detection. First, we present a small object region detection method based on the region proposal idea. Moreover, we propose a novel algorithm called reference frame Kanade-Lucas-Tomasi (RF-KLT) algorithm for motion feature extraction in fixed regions. Then, we describe a dimension reduction method of time series for establishing a feature dataset with obvious boundaries between classes. Finally, we use machine learning algorithms to build the feature classifier. The proposed expert system can realize real-time, robust and cost-free anomaly detection of aerators in both the actual video dataset and the augmented video dataset. Demo is available at https://youtu.be/xThHRwu_cnI.","Computer vision, Surveillance camera, Anomaly detection, Optical flow, Object region detection, Application",Yeqi Liu and Huihui Yu and Chuanyang Gong and Yingyi Chen,https://www.sciencedirect.com/science/article/pii/S1047320320300171,https://doi.org/10.1016/j.jvcir.2020.102767,1047-3203,2020,102767,68,Journal of Visual Communication and Image Representation,A real time expert system for anomaly detection of aerators based on computer vision and surveillance cameras,article,LIU2020102767
"With the tremendous success of the visual question answering (VQA) tasks, visual attention mechanisms have become an indispensable part of VQA models. However, these attention-based methods do not consider any relationship among regions, which is crucial for the thorough understanding of the image by the model. We propose local relation networks for generating context-aware image features for each image region, which contain information on the relationship among the other image regions. Furthermore, we propose a multilevel attention mechanism to combine semantic information from the LRNs and the original image regions, rendering the decision of the model more reasonable. With these two measures, we improve the region representation and achieve better attentive effect and VQA performance. We conduct numerous experiments on the COCO-QA dataset and the largest VQA v2.0 benchmark dataset. Our model achieves competitive results, proving the effectiveness of our proposed LRNs and multilevel attention mechanism through visual demonstrations.","Visual question answering, Relation network, Attention mechanism",Bo Sun and Zeng Yao and Yinghui Zhang and Lejun Yu,https://www.sciencedirect.com/science/article/pii/S1047320320300122,https://doi.org/10.1016/j.jvcir.2020.102762,1047-3203,2020,102762,73,Journal of Visual Communication and Image Representation,Local relation network with multilevel attention for visual question answering,article,SUN2020102762
"Image quality assessment (IQA) plays an important role in digital image forensics. Due to the occurrence of contrast distortion during image acquisition and manipulation, IQA for contrast is a major issue. And it is vital for benchmarking and optimizing the image tampering detection and contrast-enhancement algorithms. In this paper, a new no-reference/blind image quality assessment (IQA) metric is proposed for evaluating image contrast. This research seeks for the inter-relationship between contrast distortion and visual perception quality. The comprehensive quality metric is obtained by combining local binary pattern (LBP) descriptor on gradient domain with color moment on HSV color space. And a prediction model is trained with support vector regression (SVR). Extensive analysis and cross validation are performed on four contrast relevant image databases, which validates the superiority of our proposed blind technique over state-of-the-art no-reference IQA methods.","Digital image forensics, No-reference image quality assessment, Contrast distortion, Gradient domain, HSV color space",Wenjing Lyu and Wei Lu and Ming Ma,https://www.sciencedirect.com/science/article/pii/S104732032030047X,https://doi.org/10.1016/j.jvcir.2020.102797,1047-3203,2020,102797,69,Journal of Visual Communication and Image Representation,No-reference quality metric for contrast-distorted image based on gradient domain and HSV space,article,LYU2020102797
"Compared with the traditional image denoising method, although the convolutional neural network (CNN) has better denoising performance, there is an important issue that has not been well resolved: the residual image obtained by learning the difference between noisy image and clean image pairs contains abundant image detail information, resulting in the serious loss of detail in the denoised image. In this paper, in order to relearn the lost image detail information, a mathematical model is deducted from a minimization problem and an end-to-end detail retaining CNN (DRCNN) is proposed. Unlike most denoising methods based on CNN, DRCNN is not only focus to image denoising, but also the integrity of high frequency image content. DRCNN needs less parameters and storage space, therefore it has better generalization ability. Moreover, DRCNN can also adapt to different image restoration tasks such as blind image denoising, single image superresolution (SISR), blind deburring and image inpainting. Extensive experiments show that DRCNN has a better effect than some classic and novel methods.","Image denoising, Convolutional neural network, Detail retaining, Image restoration, Gaussian denoising",Xiaoxia Li and Juan Xiao and Yingyue Zhou and Yuanzheng Ye and Nianzu Lv and Xueyuan Wang and Shunli Wang and ShaoBing Gao,https://www.sciencedirect.com/science/article/pii/S1047320320300249,https://doi.org/10.1016/j.jvcir.2020.102774,1047-3203,2020,102774,71,Journal of Visual Communication and Image Representation,Detail retaining convolutional neural network for image denoising,article,LI2020102774
"Aiming at the time-consuming problem caused by large computational load of radar image retrieval, based on blocking histogram, Sobel edge detection operator and gray level co-occurrence matrix (GLCCM), new radar remote sensing image retrieval algorithm based on improved Sobel operator is proposed. Firstly, the Sobel edge detection algorithm is used to process the image, the edge image is acquired, the radar remote sensing image is analyzed from different angles, and then the different radar remote sensing images are transformed. Then, based on the above processing, Radar Remote Sensing Image Retrieval Algorithm is acquired; finally, the plurality of statistic of the matrix is recorded as a feature vector describing the radar image, and the image is retrieved according to the feature vector of the radar image. Through a large number of experiments, Radar Remote Sensing Image Retrieval algorithm can greatly reduce the retrieval time, and it also has a good retrieval effect for images with rich texture.","Radar image retrieval, Blocking histogram, Sobel operator, Gray level co-occurrence matrix (GLCCM)",Guobin Chen and Zhiyong Jiang and M.M. Kamruzzaman,https://www.sciencedirect.com/science/article/pii/S1047320319303414,https://doi.org/10.1016/j.jvcir.2019.102720,1047-3203,2020,102720,71,Journal of Visual Communication and Image Representation,Radar remote sensing image retrieval algorithm based on improved Sobel operator,article,CHEN2020102720
"Reversible data hiding in encrypted images is an effective technique to embed information in encrypted domain, without knowing the original content of the image or the encryption key. In this paper, a high-capacity reversible data hiding scheme for encrypted images based on MSB (most significant bit) prediction is proposed. Since the prediction is not always accurate, it is necessary to identify the prediction error and store this information in the location map. The stream cipher is then used to encrypt the original image directly. During the data hiding phase, up to three MSBs of each available pixel in the encrypted image are substituted by the bits of the secret message. At the receiving end, the embedded data can be extracted without any errors and the original image can be perfectly reconstructed by utilizing MSB prediction. Experimental results show that the scheme can achieve higher embedding capacity than most related methods.","Reversible data hiding, Image encryption, Image security, MSB prediction, High capacity",Bo Guan and Dawen Xu,https://www.sciencedirect.com/science/article/pii/S1047320319303657,https://doi.org/10.1016/j.jvcir.2019.102744,1047-3203,2020,102744,66,Journal of Visual Communication and Image Representation,An efficient high-capacity reversible data hiding scheme for encrypted images,article,GUAN2020102744
"6D object pose (3D rotation and translation) estimation from RGB-D image is an important and challenging task in computer vision and has been widely applied in a variety of applications such as robotic manipulation, autonomous driving, augmented reality etc. Prior works extract global feature or reason about local appearance from an individual frame, which neglect the spatial geometric relevance between two frames, limiting their performance for occluded or truncated objects in heavily cluttered scenes. In this paper, we present a dual-stream network for estimating 6D pose of a set of known objects from RGB-D images. Our novelty stands in contrast to prior work that learns latent geometric consistency in pairwise dense feature representations from multiple observations of the same objects in a self-supervised manner. We show in experiments that our method outperforms state-of-the-art approaches on 6D object pose estimation in two challenging datasets, YCB-Video and LineMOD.","Geometric consistency, Geometric reasoning, Pose estimation, Convolutional neural networks",Qingnan Li and Ruimin Hu and Jing Xiao and Zhongyuan Wang and Yu Chen,https://www.sciencedirect.com/science/article/pii/S1047320320300407,https://doi.org/10.1016/j.jvcir.2020.102790,1047-3203,2020,102790,70,Journal of Visual Communication and Image Representation,Learning latent geometric consistency for 6D object pose estimation in heavily cluttered scenes,article,LI2020102790
"Recent progresses in Convolutional Neural Networks (CNNs) and GPUs have greatly advanced the state-of-the-art performance for face recognition. However, training CNNs for face recognition is complex and time-consuming. Multiple factors need to be considered: deep learning frameworks, GPU platforms, deep network models, training datasets and test datasets. The deep models under different frameworks may perform differently. Based on this concern, we compare three deep learning frameworks and benchmark the performance of different CNN models on five GPU platforms. The scalability issue is also explored. Our findings can help researchers select appropriate face recognition models, deep learning frameworks, GPU platforms, and training datasets for their face recognition tasks.","Deep learning, Convolutional neural networks, Face recognition, GPU, PyTorch, TensorFlow, Caffe, AlexNet, ArcFace, Center-loss, CosFace, DenseNet, GoogLeNet, Inception-v3, LightCNN, ResNet, SphereFace, VGG",Qiangchang Wang and Guodong Guo,https://www.sciencedirect.com/science/article/pii/S1047320319302846,https://doi.org/10.1016/j.jvcir.2019.102663,1047-3203,2019,102663,65,Journal of Visual Communication and Image Representation,Benchmarking deep learning techniques for face recognition,article,WANG2019102663
"In many monitoring applications such as smart home and surveillance, deployment of multiple depth sensors increases monitoring area and offers better occlusion handling which is not sensitive to illumination condition in comparison with RGB sensors. However, multiple sensors also increase the volume of data associated with signal processing alongside the associated computational complexity and power consumption. In order to address these drawbacks, this paper proposes a novel change detection algorithm that can be used as a part of a sensor scheduler in a centralized (e.g. star) network configuration. Initially, each sensor in the network performs a unique single scan of the common environment in order to detect any incremental changes in the sensed depth signal. This initial change detection is then used as a basis for several follow-up tasks such as foreground segmentation, background detection, target detection, and tracking for monitoring tasks. Here, instead of processing a complete depth frame, we proposed to utilize a collection of 1D scans of the depth frames. A confidence function is defined that can be used to estimate the reliability of the detected changes in each sensor and to reduce any false positive events which can be triggered by the noise and outliers. Analysis of the proposed confidence function is carried out through performance analysis in the presence of sensor noise and other parameters which can affect the reliability of the sensed data of each sensor. Finally, a score function is defined based on the confidence of the detected parameters and sensor resolution in order to rank and match sensors with the associated objects to be tracked. It results in tracking target(s) by a sensor (or sensors) that offer a high tracking score. This approach offers many advantages such as decreasing the overall system power consumption by placing the sensors with a low confidence value on standby mode and reducing the overall computational overheads.","Change detection, Depth sensor, Network sensor, Sensor network scheduler, Background subtraction, RGBD tracking",Maryam S. Rasoulidanesh and Shahram Payandeh,https://www.sciencedirect.com/science/article/pii/S1047320319303542,https://doi.org/10.1016/j.jvcir.2019.102733,1047-3203,2020,102733,66,Journal of Visual Communication and Image Representation,A novel change-detection scheduler for a network of depth sensors,article,RASOULIDANESH2020102733
"With the rapid development of mobile Internet and digital technology, people are more and more keen to share pictures on social networks, and online pictures have exploded. How to retrieve similar images from large-scale images has always been a hot issue in the field of image retrieval, and the selection of image features largely affects the performance of image retrieval. The Convolutional Neural Networks (CNN), which contains more hidden layers, has more complex network structure and stronger ability of feature learning and expression compared with traditional feature extraction methods. By analyzing the disadvantage that global CNN features cannot effectively describe local details when they act on image retrieval tasks, a strategy of aggregating low-level CNN feature maps to generate local features is proposed. The high-level features of CNN model pay more attention to semantic information, but the low-level features pay more attention to local details. Using the increasingly abstract characteristics of CNN model from low to high. This paper presents a probabilistic semantic retrieval algorithm, proposes a probabilistic semantic hash retrieval method based on CNN, and designs a new end-to-end supervised learning framework, which can simultaneously learn semantic features and hash features to achieve fast image retrieval. Using convolution network, the error rate is reduced to 14.41% in this test set. In three open image libraries, namely Oxford, Holidays and ImageNet, the performance of traditional SIFT-based retrieval algorithms and other CNN-based image retrieval algorithms in tasks are compared and analyzed. The experimental results show that the proposed algorithm is superior to other contrast algorithms in terms of comprehensive retrieval effect and retrieval time.","Image retrieval, In-depth learning, Feature extraction, Convolutional neural network",Xushan Peng and Xiaoming Zhang and Yongping Li and Bangquan Liu,https://www.sciencedirect.com/science/article/pii/S1047320319303268,https://doi.org/10.1016/j.jvcir.2019.102705,1047-3203,2020,102705,69,Journal of Visual Communication and Image Representation,Research on image feature extraction and retrieval algorithms based on convolutional neural network,article,PENG2020102705
"Neural network based methods for fisheye distortion correction are effective and increasingly popular, although training network require a high amount of labeled data. In this paper, we propose an unsupervised fisheye correction network to address the aforementioned issue. During the training process, the predicted parameters are employed to correct strong distortion that exists in the fisheye image and synthesize the corresponding distortion using the original distortion-free image. Thus, the network is constrained with bidirectional loss to obtain more accurate distortion parameters. We calculate the two losses at the image level as opposed to directly minimizing the difference between the predicted and ground truth of distortion parameters. Additionally, we leverage the geometric prior that the distortion distribution depends on the geometric regions of fisheye images and the straight line should be straight in the corrected images. The network focuses more on the geometric prior regions as opposed to equally perceiving the whole image without any attention mechanisms. To generate more appealing corrected results in visual appearance, we introduce a coarse-to-fine inpainting network to fill the hole regions caused by the irreversible mapping function using distortion parameters. Each module of the proposed network is differentiable, and thus the entire framework is completely end-to-end. When compared with the previous supervised methods, our method is more flexible and shows better practical applications for distortion rectification. The experiment results demonstrate that our proposed method outperforms state-of-the-art methods on the correction performance without any labeled distortion parameters.","Unsupervised, Fisheye image correction, Bidirectional loss, Geometric prior, Inpainting, Deep learning",Shangrong Yang and Chunyu Lin and Kang Liao and Yao Zhao and Meiqin Liu,https://www.sciencedirect.com/science/article/pii/S104732031930313X,https://doi.org/10.1016/j.jvcir.2019.102692,1047-3203,2020,102692,66,Journal of Visual Communication and Image Representation,Unsupervised fisheye image correction through bidirectional loss with geometric prior,article,YANG2020102692
"This paper proposes to apply coarse-grained parallel genetic algorithm (CGPGA) to solve polygonal approximation problem. Chromosomes are used to represent digital curves and genes correspond to points of curves. This method divides the whole population into several subpopulations, each of which performs evolutionary process independently. After every migration interval number of generations, these subpopulations exchange their information with each other. Inspired by the designing theory of ensemble learning in machine learning, this paper further improves the basic CGPGA through adopting different but effective genetic algorithms, respectively, in different subpopulations. Both the diversity among different subpopulations and the accuracy in each individual subpopulation are ensured. Experimental results, based on four benchmark curves and four real image curves extracted from the lake maps, show that the basic CGPGA outperforms the used genetic algorithm, and further the improved CGPGA (ICGPGA) is more effective than the basic CGPGA, in terms of the quality of best solutions, the average solutions, and the variance of best solutions. Especially for those larger approximation problems, the ICGPGA is more remarkably superior to some representative genetic algorithms.","Polygonal approximation, Coarse-grained parallel genetic algorithms, Ensemble learning",Zhaobin Wu and Chunxia Zhao and Bin Liu,https://www.sciencedirect.com/science/article/pii/S1047320319303384,https://doi.org/10.1016/j.jvcir.2019.102717,1047-3203,2020,102717,71,Journal of Visual Communication and Image Representation,Polygonal approximation based on coarse-grained parallel genetic algorithm,article,WU2020102717
"Modeling expression patterns of Drosophila, in space and time, plays a critical role to understand the development of multicellular organisms. In confocal microscopy, to produce precise quantitative data it is frequently necessary to process and analyze large amounts of digital images. Automatic preprocessing is a crucial step in this scenario, essential to standardize significant features such as orientation, size, position, direction, lighting condition and texture of embryo images. Even though a lot of efforts have been made, a robust embryo standardization strategy is still needed. In this paper, we propose the method Embrystandar. It is designed to remove background artifacts and standardize the direction and orientation of a Drosophila embryo through a sequence of automatic operations. To test its potential for large-scale image processing, Embrystandar was applied in different databases. It showed to be robust and precise, reaching more than 90% success rate.","Embryo standardization, Anterior-posterior orientation, Dorsal-ventral orientation, Automatic embryo positioning,",Daniela Justiniano {de Sousa} and Maira Arruda Cardoso and Paulo Mascarello Bisch and Francisco JosÃ© Pereira Lopes and Bruno Augusto Nassif TravenÃ§olo,https://www.sciencedirect.com/science/article/pii/S1047320320300080,https://doi.org/10.1016/j.jvcir.2020.102758,1047-3203,2020,102758,71,Journal of Visual Communication and Image Representation,Automated standardization of images of Drosophila embryos,article,DESOUSA2020102758
"The large-scale surveillance videos analysis becomes important as the development of the intelligent city; however, the heavy computational resources necessary for the state-of-the-art deep learning model makes real-time processing hard to be implemented. As the characteristic of high scene similarity generally existing in surveillance videos, we propose an effective compression architecture called dynamic convolution, which can reuse the previous feature maps to reduce the calculation amount; and combine with filter pruning to further speed up the performance. In this paper, we tested the presented method on 45 surveillance videos with various scenes. The experimental results show that the hybrid pruning architecture can reduce up to 80.4% of FLOPs while preserving the precision within 1.34% mAP; furthermore, the method can improve the processing speed up to 2.8 times compared to the traditional Single Shot MultiBox Detection.","Optimize CNN, Dynamic convolution, Pruning, Smart surveillance application",Chun-Ya Tsai and De-Qin Gao and Shanq-Jang Ruan,https://www.sciencedirect.com/science/article/pii/S1047320320300481,https://doi.org/10.1016/j.jvcir.2020.102798,1047-3203,2020,102798,70,Journal of Visual Communication and Image Representation,An effective hybrid pruning architecture of dynamic convolution for surveillance videos,article,TSAI2020102798
"This paper proposes a novel model for saliency detection using the adversarial learning networks, in which the generator is used to generate the saliency map and the discriminator is deployed to guide the training process of overall network. Concretely, the training procedure of our model consists of three steps including the training of generator, the training of discriminator, and the training throughout the overall network. The key point of training process lies in the discriminator, which is designed to provide the feedback information for the acceleration of the generator and the refinement of saliency map. Therefore, during the training stage of overall network, the output of the generator, i.e. the coarse saliency map, is fed into the discriminator, yielding the corresponding feedback information. Following this way, we can obtain the final generator with a higher performance. For testing, the obtained generator is employed to perform saliency detection. Extensive experiments on four challenging saliency detection datasets show that our model not only achieves the favorable performance against the state-of-the-art saliency models, but also possesses the faster convergence speed when training the proposed model.","Adversarial learning, Generator, discriminator, Saliency detection, Feedback information",Yong Wu and Zhi Liu and Xiaofei Zhou,https://www.sciencedirect.com/science/article/pii/S1047320320300110,https://doi.org/10.1016/j.jvcir.2020.102761,1047-3203,2020,102761,67,Journal of Visual Communication and Image Representation,Saliency detection using adversarial learning networks,article,WU2020102761
"Based on transfer learning, feature maps of deep convolutional neural networks (DCNNs) have been used to predict human visual attention. In this paper, we conduct extensive comparisons to investigate effects of feature maps on the predictions of the human visual attention using a deep features based saliency model framework. The feature maps of seven pretrained DCNNs are investigated using classical and class activation maps approaches. The performances of various saliency implementations are evaluated over four datasets using three metrics. The results demonstrate that deep feature maps of the pretrained DCNNs can be used to create saliency maps for the prediction of human visual attention. The incorporation of multiple levels of blurred and multi-scale feature maps improves the extraction of salient regions. Moreover, DCNNs pretrained using the Places dataset provide more localized objects that can be beneficial to the top-down saliency maps.","Convolutional neural networks, Feature maps, Human fixation prediction, Saliency map, Transfer learning",Ali Mahdi and Jun Qin,https://www.sciencedirect.com/science/article/pii/S1047320319302834,https://doi.org/10.1016/j.jvcir.2019.102662,1047-3203,2019,102662,65,Journal of Visual Communication and Image Representation,An extensive evaluation of deep featuresof convolutional neural networks for saliency prediction of human visual attention,article,MAHDI2019102662
"This paper introduces a novel spline-like parametric model for an image representation obtained directly from compressive imaging (CI) measurements. As a representation basis we use Chebyshev polynomials. To avoid common problem of blocking artifacts in block-based reconstruction algorithms, a desired number of derivatives are equated on the block boundaries in a spline-like fashion. This introduces a new set of constraints that fits into CI setup. Unlike splines, the proposed system of equations is underdetermined to provide a necessary degree of freedom for achieving sparsity by solving an â1 optimization problem. Recovered coefficients of the parametric model can be further used for image processing where operations can be elegantly defined and calculated. This offers a new framework for acquisition and processing of analog signals without converting them into samples. Experiments on real measurements show that our model achieves sparse representation without visible blocking artifacts from a reduced set of CI measurements.","Polynomial representation of image, Chebyshev moments, Runge phenomenon, Sparse modeling, Compressive sensing, 2D-imaging",Tin VlaÅ¡iÄ and Ivan RalaÅ¡iÄ and Azra Tafro and Damir SerÅ¡iÄ,https://www.sciencedirect.com/science/article/pii/S1047320319303529,https://doi.org/10.1016/j.jvcir.2019.102731,1047-3203,2020,102731,66,Journal of Visual Communication and Image Representation,Spline-like Chebyshev polynomial model for compressive imaging,article,VLASIC2020102731
"Automatic License Plate Recognition (ALPR) is an important task with many applications in Intelligent Transportation and Surveillance systems. This work presents an end-to-end ALPR method based on a hierarchical Convolutional Neural Network (CNN). The core idea of the proposed method is to identify the vehicle and the license plate region using two passes on the same CNN, and then to recognize the characters using a second CNN. The recognition CNN massively explores the use of synthetic and augmented data to cope with limited training datasets, and our results show that the augmentation process significantly increases the recognition rate. In addition, we present a novel temporal coherence technique to better stabilize the OCR output in videos. Our method was tested with publicly available datasets containing Brazilian and European license plates, achieving accuracy rates better than competitive academic methods and a commercial system.","Convolutional neural networks, License plate, Deep learning",Sergio Montazzolli Silva and Claudio Rosito Jung,https://www.sciencedirect.com/science/article/pii/S1047320320300237,https://doi.org/10.1016/j.jvcir.2020.102773,1047-3203,2020,102773,71,Journal of Visual Communication and Image Representation,Real-time license plate detection and recognition using deep convolutional neural networks,article,SILVA2020102773
"In this paper, we present an accurate superpixel algorithm by region fusion with boundary constraint (RFBC). Superpixels with regular shape and high boundary adherence can be generated in weak boundary and complex texture regions through our algorithm. RFBC includes two steps which are initial segmentation and region fusion respectively. In initial segmentation, broken Canny edges are connected through edge closing algorithm. Subsequently, the closed Canny edges and SLIC superpixel edges are combined together to form the incipient superpixels. In region fusion, gray Gaussian distribution and adjacent relation are used as priori to compute the degree of similarity across incipient superpixels in GBP algorithm. For concreteness, the information of similarity is propagated between regions and the most similar regions are fused, which are accomplished alternatingly to preserve accurate boundaries. Extensive experiments on the Berkeley segmentation benchmark show that the proposed algorithm outperforms the most state-of-the-art algorithms.","Superpixel, Initial segmentation, Edge closing, Gaussian belief propagation, Region fusion",Li Zhao and Zhihui Li and Chaoguang Men and Yongmei Liu,https://www.sciencedirect.com/science/article/pii/S1047320319303645,https://doi.org/10.1016/j.jvcir.2019.102743,1047-3203,2020,102743,66,Journal of Visual Communication and Image Representation,Superpixels extracted via region fusion with boundary constraint,article,ZHAO2020102743
"Human action recognition from skeletal data is one of the most popular topics in computer vision which has been widely studied in the literature, occasionally with some very promising results. However, being supervised, most of the existing methods suffer from two major drawbacks; (1) too much reliance on massive labeled data and (2) high sensitivity to outliers, which in turn hinder their applications in such real-world scenarios as recognizing long-term and complex movements. In this paper, we propose a novel unsupervised 3D action recognition method called Sparseness Embedding in which the spatiotemporal representation of action sequences is nonlinearly projected into an unwarped feature representation medium, where unlike the original curved space, one can easily apply the Euclidean metrics. Our strategy can simultaneously integrate the characteristics of nonlinearity, sparsity, and space curvature of sequences into a single objective function, leading to a more robust and highly compact representation of discriminative attributes without any need to label information. Moreover, we propose a joint learning strategy for dealing with the heterogeneity of the temporal and spatial characteristics of action sequences. A set of extensive experiments on six publicly available databases, including UTKinect, TST fall, UTD-MHAD, CMU, Berkeley MHAD, and NTU RGB+D demonstrates the superiority of our method compared with the state-of-the-art algorithms.","Unsupervised action recognition, Time series analysis, Sparseness embedding, Human computer interaction",Hoda Mohammadzade and Mohsen Tabejamaat,https://www.sciencedirect.com/science/article/pii/S1047320319303128,https://doi.org/10.1016/j.jvcir.2019.102691,1047-3203,2020,102691,66,Journal of Visual Communication and Image Representation,Sparseness embedding in bending of space and time; a case study on unsupervised 3D action recognition,article,MOHAMMADZADE2020102691
"The advancement of science and technology has a positive effect on the development of law disciplines. The development of algorithms and artificial intelligence also has a certain impact on judicial practice. Image restoration is a significant technique in image processing. It aims to objectively restore the content or quality of the original image from the degraded image. Image degradation is always generated in image transmission, such as distortion, blur. In modern video surveillance system, image restoration is significant for criminal investigation. However, image restoration based on conventional filter algorithms cannot achieve satisfactory performance. Thus, we first introduce the image restoration algorithms based on different degradation model. Then, we propose some applications of fuzzy image restoration in criminal investigation. We conduct experiments on both degraded images and videos and experimental results have shown the effectiveness of fuzzy image restoration applying to the criminal investigation.","Fuzzy image restoration, Image degradation, Criminal investigation",Shuo Sun,https://www.sciencedirect.com/science/article/pii/S1047320319303256,https://doi.org/10.1016/j.jvcir.2019.102704,1047-3203,2020,102704,71,Journal of Visual Communication and Image Representation,Application of fuzzy image restoration in criminal investigation,article,SUN2020102704
"The batching system of the integrated mixing and spreading equipment for MOH material is a nonlinear system with large uncertainty. It is difficult for conventional control strategies to meet the requirements for system performance. This research combines generalized predictive control and active disturbance rejection technique to propose a new generalized predictive active disturbance rejection controller (GPADRC) used in the batching system of MOH material. For the nonlinearity and uncertainty of the batching system, the extended state observer in the active disturbance rejection technique is used for estimation and compensation. The batching system model is converted into an integrator form, based on which the use of generalized predictive control can greatly reduce the impact of nonlinear models and uncertainties on the controller. Aiming at the problem that the parameters of the proposed new controller are numerous and difficult to tune, the adaptive genetic algorithm is used to realize the automatic tuning of the parameters. The simulation experiment shows that the designed GPADRC can well adapt to the working conditions of the batching system and can meet the requirements for various control indicators. At the same time, the adaptive genetic algorithm can realize the rapid tuning of the controller parameters, which reduce the difficulty and time consumption of the tuning process, and improve the applicability and achievability of the designed controller.","Mixing and spreading equipment for MOH material, Batching system, Active disturbance rejection control, Generalized prediction, Adaptive genetic algorithm",Yikun Yang and Shengjie Jiao and Jiabo Li,https://www.sciencedirect.com/science/article/pii/S1047320319303499,https://doi.org/10.1016/j.jvcir.2019.102728,1047-3203,2020,102728,71,Journal of Visual Communication and Image Representation,Vision-based optimization of the generalized predictive active disturbance rejection controller,article,YANG2020102728
"Learning an intelligent lane detection system is significant to autonomous vehicles, which is a crucial module to smart cars. Although conventional approaches have achieved impressive performance, they suffer from the following limitations: (1) lane perception are confronted with different weather conditions and varied illumination. Existing methods lack a unified framework for characterizing different sceneries and (2) the inefficiency of utilizing images due to the potential label noise. To solve these limitations, we propose a lane detection framework towards autonomous vehicles by learning a full-reference quality-aware discriminative gradient deep model, where two types of deep networks are proposed. More specifically, we first design a gradient-guided deep convolutional network to detect the presence of lane, since the gradient value of lane edge is larger than that of other regions. We leverage full-reference image quality assessment (FR-IQA) method to discover more discriminative gradient cues, and geometric attributes are exploited simultaneously. Subsequently, a recurrent neural layer is designed to represent the spatial distribution of detected lanes whose visual cues are difficult to explicitly define. Noticeably, we only utilize a small proportion of the labeled images, whereas the noisy features are abandoned using sparsity penalty. Extensive experiments have demonstrated the effectiveness of our proposed method.","Lane detection, Full-reference IQA, CNN, RNN",Jingyi Liu,https://www.sciencedirect.com/science/article/pii/S1047320319302962,https://doi.org/10.1016/j.jvcir.2019.102675,1047-3203,2019,102675,65,Journal of Visual Communication and Image Representation,Learning full-reference quality-guided discriminative gradient cues for lane detection based on neural networks,article,LIU2019102675
"Image captured underwater often suffers from low contrast, color distortion and noise problems, which is caused by absorbing and scattering before the light reaches the camera when traveling through water. Underwater image enhancement and restoration from a single image is known to be an ill-posed problem. To overcome these limitations, we establish an underwater total variation (UTV) model relying on underwater dark channel prior (UDCP), in which UDCP is used to estimate the transmission map. We design the data item and smooth item of the unified variational model based on the underwater image formation model. We further employ the alternating direction method of multipliers (ADMM) to accelerate the solving procedure. Numerical experiential results demonstrate that our underwater variational method obtains a good outcome on dehazing and denoising. Furthermore, compared with several other state-of-the-art algorithms, the proposed approach achieves better visual quality, which is illustrated by examples and statistics.","Underwater image restoration, Dehazing and denoising, UTV, ADMM, UDCP",Guojia Hou and Jingming Li and Guodong Wang and Huan Yang and Baoxiang Huang and Zhenkuan Pan,https://www.sciencedirect.com/science/article/pii/S1047320319303530,https://doi.org/10.1016/j.jvcir.2019.102732,1047-3203,2020,102732,66,Journal of Visual Communication and Image Representation,A novel dark channel prior guided variational framework for underwater image restoration,article,HOU2020102732
"The recognition and detection of space debris has become one of significant research fields recently. Compared with natural images, effective information are very few contained in star images. In the past years, the gray values of star points and the continuity of sequential star images are utilized by numerous algorithms to carry out the recognition and detection through fusion of consecutive star images, which have been achieved good performance. However, with the rapid increase of star image data, those algorithms seem to be inadequate in recognition ability. In this paper, we propose one novel approach based on the full information vectors of star points to recognize moving targets with the machine learning method which is never utilized in space debris recognition field. Besides gray values, we further deeply excavate the characteristics of each star point in a single frame by the equal probability density curve of Gaussian distribution. The elliptical pattern characteristic vectors of star points can be input into the machine learning method for classification of static stars and moving targets in a single frame. Finally, trajectories of moving targets can be determined within 3 frames by the full information vectors. Therefore, traditional processing methods are abandoned and the proposed brand new approach redefines the recognition technical route of space debris. The experimental results demonstrate that moving targets can be successfully recognized in a single frame and the coverage rate of moving targets can reach 100%. Compared with other traditional methods, the proposed approach has better performance and more robustness.","Space debris recognition, Star image, Binary classifier, Equal probability density curve, Full information vector",Yun Du and Desheng Wen and Guizhong Liu and Shi Qiu and Dalei Yao and Hongwei Yi and Meiying Liu,https://www.sciencedirect.com/science/article/pii/S1047320319303372,https://doi.org/10.1016/j.jvcir.2019.102716,1047-3203,2020,102716,71,Journal of Visual Communication and Image Representation,A novel approach for space debris recognition based on the full information vectors of star points,article,DU2020102716
"Detection of salient objects in image and video is of great importance in many computer vision applications. In spite of the fact that the state of the art in saliency detection for still images has been changed substantially over the last few years, there have been few improvements in video saliency detection. This paper proposes a novel non-local fully convolutional network architecture for capturing global dependencies more efficiently and investigates the use of recently introduced non-local neural networks in video salient object detection. The effect of non-local operations is studied separately on static and dynamic saliency detection in order to exploit both appearance and motion features. A novel deep non-local fully convolutional network architecture is introduced for video salient object detection and tested on two well-known datasets DAVIS and FBMS. The experimental results show that the proposed algorithm outperforms state-of-the-art video saliency detection methods.","Video saliency detection, Deep learning, Non-local neural networks, Fully convolutional neural networks",Mohammad Shokri and Ahad Harati and Kimya Taba,https://www.sciencedirect.com/science/article/pii/S1047320320300195,https://doi.org/10.1016/j.jvcir.2020.102769,1047-3203,2020,102769,68,Journal of Visual Communication and Image Representation,Salient object detection in video using deep non-local neural networks,article,SHOKRI2020102769
"Different from traditional 2D video, the contents of 360 degree video are deformed due to the projection from 3D sphere to 2D plane. As a result, the traditional Angular Intra Prediction (AIP) with a linear pattern may not be always efficient. To further improve the coding performance of 360 degree video, a novel intra prediction method is presented in this paper, i.e., Circular Intra Prediction (CIP), which takes consideration of the spherical characteristics of 360 degree video. In specific, the proposed CIP is performed in a circular pattern, where the center of circle is located around the to-be-predicted block, and different centers of circle are able to produce different CIP modes. The distance between center of this circle and center of the to-be-predicted block is adaptively determined according to the degree of projection deformation, where stronger projection deformation needs shorter distance, and vice versa. As the increase of the distance, the CIP is more and more close to the traditional AIP. In addition, one additional binary flag is utilized to achieve better coding performance from the competition between AIP and CIP with the rate-distortion optimization. The proposed algorithm is implemented on the platform of Versatile video coding Test Model (VTM) 5.0Â +Â 360Lib 9.1. Extensive experiments show that the proposed method can achieve bit rate reduction on this platform for 360 degree video coding.","Circular intra prediction, Projection deformation, 360 degree video, Versatile video coding",Linwei Zhu and Yun Zhang and Na Li and Jinyong Pi and Shiqi Wang,https://www.sciencedirect.com/science/article/pii/S1047320320302169,https://doi.org/10.1016/j.jvcir.2020.103000,1047-3203,2021,103000,74,Journal of Visual Communication and Image Representation,Circular intra prediction for 360 degree video coding,article,ZHU2021103000
"Traditional lossless compression methods for satellite hyperspectral imagery focus on exploiting spatial and/or spectral redundancy. Those methods do not consider the temporal redundancy between images of the same area that are captured at different times. To exploit the temporal redundancy between hyperspectral images and reduce the amount of information to be transmitted from the space-satellite to the ground station via the downlink, this paper introduces a dual link distributed source coding (DLDSC) scheme for hyperspectral space-satellite communication. The proposed scheme employs the space-satellite dual link (i.e., the downlink and the uplink). The satellite onboard uses some side information from the ground station to calculate the hyperspectral image band coset values, and then, without syndrome coding, transmits to the ground station via the downlink. Coset coding is a typical technique used in distributed source coding (DSC), and here the coset values represent the timely hyperspectral image details. Typically, the coset values have lower entropy than that of the original source values. To exploit the temporal redundancy, the side information is computed in the ground station using the image captured at the previous time for the same area and transmitted to the space-satellite via the uplink. Hyperspectral images from the Hyperion satellite are used for the validation of the proposed scheme. The experimental results indicate that the proposed DLDSC scheme can reduce the original signal entropy by approximately 3.2 bits per sample (bps) and can achieve up to 1.0 bps and 1.6 bps gains over the lossless JPEG2000 standard and the state-of-art predictive CCSDS-123 method, respectively.","Hyperspectral image compression, Temporal redundancy in hyperspectral images, Dual link, Distributed source coding (DSC), Coset values, Lossless compression",Ahmed Hagag and Ibrahim Omara and Souleyman Chaib and Guangzhi Ma and Fathi E. Abd El-Samie,https://www.sciencedirect.com/science/article/pii/S1047320321000730,https://doi.org/10.1016/j.jvcir.2021.103117,1047-3203,2021,103117,78,Journal of Visual Communication and Image Representation,Dual link distributed source coding scheme for the transmission of satellite hyperspectral imagery,article,HAGAG2021103117
"Graph-based salient object detection methods have gained more and more attention recently. However, existing works fail to separate effectively salient object and background in some challenging scenes. Inspired by this observation, we propose an effective salient object detection method based on a novel boundary-guided graph structure. More specifically, the input image is firstly segmented into a series of superpixels. Then we integrate two prior cues to generate the coarse saliency map, a novel weighting mechanism is proposed to balance the proportion of two prior cues according to their performance. Secondly, we propose a novel boundary-guided graph structure to explore deeply the intrinsic relevance between superpixels. Based on the proposed graph structure, an iterative propagation mechanism is constructed to refine the coarse saliency map. Experimental results on four datasets show adequately the superiority of the proposed method than other state-of-the-art methods.","Salient object detection, Coarse saliency map, Weighting integration framework, Boundary-guided graph structure, Adaptive strategy, Iterative propagation mechanism",Yunhe Wu and Tong Jia and Yu Pang and Jiaduo Sun and Dingyu Xue,https://www.sciencedirect.com/science/article/pii/S1047320321000213,https://doi.org/10.1016/j.jvcir.2021.103048,1047-3203,2021,103048,75,Journal of Visual Communication and Image Representation,Salient object detection via a boundary-guided graph structure,article,WU2021103048
"In recent years, convolutional neural networks (CNNs) have accelerated the developments of video super resolution (SR) for achieving higher image quality. However, the computational cost of existing CNN-based video super-resolution is too heavy for real-time applications. In this paper, we propose a new video super-resolution framework using lightweight frame alignment module and well-designed up-sampling module for real-time processing. Specifically, our framework, which is called as Lightweight Shuffle Video Super-Resolution Network (LSVSR), combines channel shuffling, depthwise convolution and pointwise group convolution to significantly reduce the computational burden during frame alignment and high-resolution frame reconstruction. On the public benchmark datasets, our proposed network outperforms the state-of-the-art lightweight video SR networks in terms of objective (PSNR and SSIM) and subjective evaluations, number of network parameters and floating-point operations. Our network can achieve real-time 540P to 2160P 4Ã super-resolution for more than 60fps using desktop GPUs or mobile phones with neural processing unit.","Super-resolution, Lightweight alignment module, Channel shuffle, Residual networks",Zhijiao Xiao and Zhikai Zhang and Kwok-Wai Hung and Simon Lui,https://www.sciencedirect.com/science/article/pii/S104732032100016X,https://doi.org/10.1016/j.jvcir.2021.103038,1047-3203,2021,103038,75,Journal of Visual Communication and Image Representation,Real-time video super-resolution using lightweight depthwise separable group convolutions with channel shuffling,article,XIAO2021103038
"To obtain reliable depth images with high resolution, a novel method is proposed in this study that fuses data acquired from time-of-flight (ToF) and stereo cameras, through which the advantages of both active and passive sensing are utilised. Based on the classic error model of the ToF, gradient information is introduced to establish the likelihood distribution for all disparity candidates. The stereo likelihood is estimated in parallel based on a 3D adaptive support-weight approach. The two independent likelihoods are unified using a maximum likelihood estimation, a process also referred to as a joint depth filter herein. Conventional post-processing methods such as a mutual consistency check are also used after applying a joint depth filter. We also propose a novel hole-filling method based on the seed-growing algorithm to retrieve missing disparities. Experiment results show that the proposed fusion method can produce reliable high-resolution depth maps and outperforms other compared methods.","ToF, Stereo vision, Data fusion, 3D block matching, Seed-growing",Xuanyin Wang and Tianpei Lin and Xuesong Jiang and Ke Xiang and Feng Pan,https://www.sciencedirect.com/science/article/pii/S1047320320302200,https://doi.org/10.1016/j.jvcir.2020.103006,1047-3203,2021,103006,74,Journal of Visual Communication and Image Representation,Reliable fusion of ToF and stereo data based on joint depth filter,article,WANG2021103006
"Hyperspectral anomaly detection (HAD) is a branch of target detection which tries to locate pixels that are spectrally or spatially different from their background. In this paper, a visual attention approach is developed to leverage HAD. Traditional HAD methods often try to locate anomalous pixels based on spectral information. However, the spatial features of hyperspectral datasets provide valuable information. Here, we aim to fuse spatial and spectral anomaly features based on bottom-up (BU) and top-down (TD) visual attention mechanisms. Owe to the BU attention, spatial features are extracted by mimicking the primary visual cortex neurons functionality. Also, spectral information is obtained throughout a deep neural network that imitating the TD visual attention. The BU and TD approachesâ results are then integrated to provide both spectral and spatial information. The key findings of our results demonstrate the proposed method outperforms the six state-of-the-art AD methods based on different evaluation metrics.","Hyperspectral image, Visual attention, Anomaly detection, Bottom-up attention, Top-down attention",Ashkan Taghipour and Hassan Ghassemian,https://www.sciencedirect.com/science/article/pii/S1047320321000699,https://doi.org/10.1016/j.jvcir.2021.103113,1047-3203,2021,103113,77,Journal of Visual Communication and Image Representation,A bottom-up and top-down human visual attention approach for hyperspectral anomaly detection,article,TAGHIPOUR2021103113
"In this paper, we propose to analyze stable and unstable modes of black-box image denoisers through nonlinear eigenvalue analysis. We aim to find input images for which the denoiser output is proportional to the input. We treat this as a generalized nonlinear eigenproblem. Potential implications are wide, as most image processing algorithms can be viewed as black-box operators. We introduce a generalized nonlinear power-method to solve eigenproblems for such operators. This allows us to reveal stable modes of the denoiser: optimal inputs, achieving superior PSNR in noise removal. Analogously to the linear case, such stable modes show coarse structures and correspond to large eigenvalues. We also provide a method to generate unstable modes, which the denoiser suppresses strongly, which are textural with small eigenvalues. We validate the method using total-variation (TV) and demonstrate it on the EPLL (ZoranâWeiss) and the Non-local means denoisers. Finally, we suggest an encryptionâdecryption application.","Eigenfunctions, Nonlinear operators, Denoising, Power iteration, Total-variation, EPLL",Ester Hait-Fraenkel and Guy Gilboa,https://www.sciencedirect.com/science/article/pii/S1047320321000171,https://doi.org/10.1016/j.jvcir.2021.103041,1047-3203,2021,103041,75,Journal of Visual Communication and Image Representation,Revealing stable and unstable modes of denoisers through nonlinear eigenvalue analysis,article,HAITFRAENKEL2021103041
"Currently, video-based Sign language recognition (SLR) has been extensively studied using deep learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). In addition, using multi view attention mechanism along with CNNs could be an appealing solution that can be considered in order to make the machine interpretation process immune to finger self-occlusions. The proposed multi stream CNN mixes spatial and motion modelled video sequences to create a low dimensional feature vector at multiple stages in the CNN pipeline. Hence, we solve the view invariance problem into a video classification problem using attention model CNNs. For superior network performance during training, the signs are learned through a motion attention network thus focusing on the parts that play a major role in generating a view based paired pooling using a trainable view pair pooling network (VPPN). The VPPN, pairs views to produce a maximally distributed discriminating features from all the views for an improved sign recognition. The results showed an increase in recognition accuracies on 2D video sign language datasets. Similar results were obtained on benchmark action datasets such as NTU RGB D, MuHAVi, WEIZMANN and NUMA as there is no multi view sign language dataset except ours.","Multi view, Sign language recognition, Deep learning, Attention models, Motion modelled",Suneetha M. and Prasad M.V.D. and Kishore P.V.V.,https://www.sciencedirect.com/science/article/pii/S1047320321001024,https://doi.org/10.1016/j.jvcir.2021.103161,1047-3203,2021,103161,78,Journal of Visual Communication and Image Representation,Multi-view motion modelled deep attention networks (M2DA-Net) for video based sign language recognition,article,M2021103161
"Explicit reasoning over a spatial substrate, i.e., spaceâtime information structures underlying a spatial problem, simplifies reasoning. Diagrammatic reasoning makes use of diagrams for exploiting such underlying structures. This paper proposes a novel approach combining diagrammatic reasoning with qualitative spatial and temporal reasoning techniques to visualize and perceive spatio-temporal relations among objects in a video. The hybrid techniques explore information over the spatial substrate for relational extractions. Different relations among objects in transition define short-term activities. Mealy machines are learned over patterns of short-term activities as activity recognizers. The proposed representation and recognition mechanism is validated by conducting experiments for video activity recognition from DARPA Mindâs Eye and J-HMDB dataset.","Cognitive vision, Video analysis, Activity recognition, Qualitative spatial and temporal reasoning, Diagrammatic reasoning",Chayanika D. Nath and Shyamanta M. Hazarika,https://www.sciencedirect.com/science/article/pii/S1047320321000298,https://doi.org/10.1016/j.jvcir.2021.103061,1047-3203,2021,103061,76,Journal of Visual Communication and Image Representation,Activity recognition in video sequences over qualitative abstracts of a diagram-based representation schema,article,NATH2021103061
"Pixel-value-ordering (PVO) is an effective and promising method of reversible data hiding (RDH) and has received much attention in recent years. To improve performance, a pixel-based PVO (PPVO) method was recently introduced to predict the pixels to be embedded in a pixel-wise manner instead of the block-wise manner used by PVO. However, for PPVO, the surrounding neighbors of the predicted pixels are underutilized; moreover, its embedding does not adapt to the local complexity of the image to be embedded. To overcome the shortcomings of PPVO, this paper proposes a novel PVO method based on hybrid prediction for RDH. First, the surrounding neighbors of the pixel to be predicted are fully utilized by a hybrid prediction method, which combines rhombus prediction and pixel-wise prediction. Second, a modified embedding scheme based on multiple histograms is presented for adaptive embedding. Experimental results show the superior performance of the proposed method by comparing it with state-of-the-art RDH methods.","Reversible data hiding (RDH), Pixel-value-ordering (PVO), Rhombus prediction, Embedding capacity",Jie Chang and Feng Ding and Xiaolong Li and Guopu Zhu,https://www.sciencedirect.com/science/article/pii/S1047320321000572,https://doi.org/10.1016/j.jvcir.2021.103097,1047-3203,2021,103097,77,Journal of Visual Communication and Image Representation,Hybrid prediction-based pixel-value-ordering method for reversible data hiding,article,CHANG2021103097
"The Iterative Ant-tree for color quantization algorithm has recently been proposed to reduce the colors of an image at a low computational cost. It is a clustering-based method that generates good images compared to several well-known color quantization methods. This article proposes the modification of two features of the original algorithm: the value assigned to the parameter associated with the algorithm and the order in which the pixels of the image are processed. As a result, the new variant of the algorithm generates better images than the original and the results are less sensitive to the value selected for the parameter.","Color quantization, Clustering, Artificial ants",MarÃ­a-Luisa PÃ©rez-Delgado,https://www.sciencedirect.com/science/article/pii/S1047320321001140,https://doi.org/10.1016/j.jvcir.2021.103180,1047-3203,2021,103180,78,Journal of Visual Communication and Image Representation,Revisiting the Iterative Ant-tree for color quantization algorithm,article,PEREZDELGADO2021103180
"Existing NR-IIQA (no reference-based inpainted image quality assessment) algorithms assess the quality of an inpainted image via artificially designed unnaturalness expression, which often fail to capture inpainted artifacts. This paper presents a new deep rank learning-based method for NR-IIQA. The model adopts a siamese deep architecture, which takes a pair of inpainted images as input and outputs their rank order. Each branch utilizes a CNN structure to capture the global structure coherence and a patch-wise coherence assessment module (PCAM) to depict the local color and texture consistency in an inpainted image. To train the deep model, we construct a new dataset, which contains thousands of pairs of inpainted images with ground-truth quality ranking labels. Rich ablation studies are conducted to verify the key modules of the proposed architecture. Comparative experimental results demonstrate that our method outperforms existing NR-IIQA metrics in evaluating both inpainted images and inpainting algorithms.","Image inpainting, Rank learning, Image quality assessment, Siamese network",Xiangdong Meng and Wei Ma and Chunhu Li and Qing Mi,https://www.sciencedirect.com/science/article/pii/S1047320321001139,https://doi.org/10.1016/j.jvcir.2021.103176,1047-3203,2021,103176,78,Journal of Visual Communication and Image Representation,Siamese CNN-based rank learning for quality assessment of inpainted images,article,MENG2021103176
"Multi-task learning aims to tackle various tasks with branched feature sharing architectures. Considering its diversity and complexity, discriminative feature representations need to be extracted for each individual task. Fixed geometric structures as a limitation of convolutional neural networks (CNNs) in building models, is also exists and poses a severe challenge in multi-task learning since the geometric variations will augment when we deal with multiple tasks. In this paper, we go beyond these limitations and propose a novel multi-task network by introducing the deformable convolution. Our design, the Deformable Multi-Task Network (DMTN), starts with a single shared network for constructing a shared feature pool. Then, we present task-specific deformable modules to extract discriminative features to be tailored for each task from the shared feature pool. The task-specific deformable modules utilize two new parts, deformable part and alignment part, to extract more discriminative task-specific features while greatly enhancing the transformation modeling capability. Experiments conducted on various multi-task learning types demonstrate the effectiveness of the proposed method. On multiple classification tasks, semantic segmentation and depth estimation tasks, our DMTN exceeds state-of-the-art approaches against strong baselines.","Multi-task learning, Deformable convolution, Recognition",Jie Li and Lei Huang and Zhiqiang Wei and Wenfeng Zhang and Qibing Qin,https://www.sciencedirect.com/science/article/pii/S1047320321000687,https://doi.org/10.1016/j.jvcir.2021.103109,1047-3203,2021,103109,77,Journal of Visual Communication and Image Representation,Multi-task learning with deformable convolution,article,LI2021103109
"Abnormal behavior detection in surveillance videos is necessary for public monitoring and safety. In human-based surveillance systems, it requires continuous human attention and observation, which is a difficult task. The autonomous detection of such events is of essential significance. However, due to the scarcity of labeled data and the low occurrence probability of these events, abnormal event detection is a challenging vision problem. In this paper, we introduce a novel two-stage architecture for detecting anomalous behavior in videos. In the first stage, we propose a 3D Convolutional Autoencoder (3D-CAE) architecture to extract spatio-temporal features from normal event training videos. In 3D-CAE, the encoder and decoder architectures are based on 3D convolutions, which can learn both appearance and the motion features effectively in an unsupervised manner. In the second stage, we group the 3D spatio-temporal features into different normality clusters, and then remove the sparse clusters to represent a stronger pattern of normality. From these clusters, one-class SVM classifier is used to distinguish between normal and abnormal events based on the normality scores. Experimental results on four different benchmark datasets show significant performance improvement compared to state-of-the-art approaches while providing results in real-time.","Spatiotemporal latent features, 3D-CAE, Anomaly detection, Video analysis, Autonomous video surveillance",Mujtaba Asad and Jie Yang and Enmei Tu and Liming Chen and Xiangjian He,https://www.sciencedirect.com/science/article/pii/S1047320321000201,https://doi.org/10.1016/j.jvcir.2021.103047,1047-3203,2021,103047,75,Journal of Visual Communication and Image Representation,Anomaly3D: Video anomaly detection based on 3D-normality clusters,article,ASAD2021103047
"Accurate retinal vessel segmentation is a challenging problem in color fundus image analysis. An automatic retinal vessel segmentation system can effectively facilitate clinical diagnosis and ophthalmological research. In general, this problem suffers from various degrees of vessel thickness, perception of details, and contextual feature fusion in technique. For addressing these challenges, a deep learning based method has been proposed and several customized modules have been integrated into the well-known U-net with encoderâdecoder architecture, which is widely employed in medical image segmentation. In the network structure, cascaded dilated convolutional modules have been integrated into the intermediate layers, for obtaining larger receptive field and generating denser encoded feature maps. Also, the advantages of the pyramid module with spatial continuity have been taken for multi-thickness perception, detail refinement, and contextual feature fusion. Additionally, the effectiveness of different normalization approaches has been discussed on different datasets with specific properties. Finally, sufficient comparative experiments have been enforced on three retinal vessel segmentation datasets, DRIVE, CHASE_DB1, and the STARE dataset with unhealthy samples. As a result, the proposed method outperforms the work of predecessors and achieves state-of-the-art performance.","Retinal vessel segmentation, Color fundus image analysis, Semantic segmentation, Cascaded dilated module, Context fusion",Muyi Sun and Kaiqi Li and Xingqun Qi and Hao Dang and Guanhong Zhang,https://www.sciencedirect.com/science/article/pii/S1047320321000845,https://doi.org/10.1016/j.jvcir.2021.103134,1047-3203,2021,103134,77,Journal of Visual Communication and Image Representation,Contextual information enhanced convolutional neural networks for retinal vessel segmentation in color fundus images,article,SUN2021103134
"In this paper, we present a novel adversarial embedding scheme named Steganalytic Feature based Adversarial Embedding (SFAE), which is elaborately designed in a non-data-driven style. Firstly, a novel DCTR based adversary is designed to generate adversarial stego images which can not only resist feature based steganalysis but also deep learning based steganalysis. Specifically, our adversary consists of an end-to-end neural network structure, while its inner weights are set according to DCTR rather than learned from datasets. Secondly, we use the minimum distance to the cover in steganalytic space as the criterion to select the optimal adversarial stego image, rather than fooling the adversary. Last but not least, we present two SFAE implementations to adapt to different cases. One is Iterative SFAE, which needs to calculate gradients multiple times. Iterative SFAE is more secure but has higher complexity. It fits the case that the steganographer has adequate computing resources. Another implementation is Oneshot SFAE, which can calculate gradients once. Oneshot SFAE trades the security for lower complexity. It fits the steganographer that has stricter requirements for running time. Experiments demonstrate that SFAE is effective to improve the security of conventional steganographic schemes against the state-of-the-art steganalysis including both feature based steganalysis and deep learning based steganalysis.","Steganography, Adversarial embedding, Non-data-driven",Sai Ma and Xianfeng Zhao,https://www.sciencedirect.com/science/article/pii/S1047320321000328,https://doi.org/10.1016/j.jvcir.2021.103066,1047-3203,2021,103066,76,Journal of Visual Communication and Image Representation,Steganalytic feature based adversarial embedding for adaptive JPEG steganography,article,MA2021103066
"Concurrent multipath transmission provides an effective solution for streaming high-quality mobile videos in heterogeneous wireless networks. Rate control is commonly adopted in multimedia communication systems to fully utilize the available network bandwidth. This paper proposes a novel rate control for concurrent multipath video transmission. The existing rate control algorithms mainly adapt bit rate in the short-term pattern, i.e., without considering the long-term video transmission quality. We propose a long-term rate control scheme that takes into account the status of both the transmission buffer and video frames. First, a mathematical model is developed to formulate the non-convex problem of long-term quality maximization. Second, we develop a dynamic programming solution for online encoding bit rate control based on buffer status. The performance evaluation is conducted in a real test bed over LTE and Wi-Fi networks. Experimental results demonstrate that the proposed long-term rate control scheme achieves appreciable improvements over the short-term rate control schemes in terms of video quality and delay performance.","Visual communications, Video coding, Lyapunov optimization, Heterogeneous wireless networks, Multi-homing, Rate control",Feng Chen and Jie Zhang and Mingkui Zheng and Jiyan Wu and Nam Ling,https://www.sciencedirect.com/science/article/pii/S1047320320302157,https://doi.org/10.1016/j.jvcir.2020.102999,1047-3203,2021,102999,77,Journal of Visual Communication and Image Representation,Long-term rate control for concurrent multipath real-time video transmission in heterogeneous wireless networks,article,CHEN2021102999
"With the growing availability of hand-held cameras in recent years, more and more images and videos are taken at any time and any place. However, they usually suffer from undesirable blur due to camera shake or object motion in the scene. In recent years, a few modern video deblurring methods are proposed and achieve impressive performance. However, they are still not suitable for practical applications as high computational cost or using future information as input. To address the issues, we propose a sequentially one-to-one video deblurring network (SOON) which can deblur effectively without any future information. It transfers both spatial and temporal information to the next frame by utilizing the recurrent architecture. In addition, we design a novel Spatio-Temporal Attention module to nudge the network to focus on the meaningful and essential features in the past. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art deblurring methods, both quantitatively and qualitatively, on various challenging real-world deblurring datasets. Moreover, as our method deblurs in an online manner and is potentially real-time, it is more suitable for practical applications.","Video deblurring, Deblurring, Image quality enhancement",Chih-Hung Liang and Hung-Ting Su and Winston H. Hsu,https://www.sciencedirect.com/science/article/pii/S1047320321001012,https://doi.org/10.1016/j.jvcir.2021.103159,1047-3203,2021,103159,78,Journal of Visual Communication and Image Representation,Learn from the past â sequentially one-to-one video deblurring network,article,LIANG2021103159
"Detecting the objects of interesting from aerial images captured by UAVs is one of the core modules in the UAV-based applications. However, it is very difficult to detection objects from aerial images. The reason is that the scale of objects in the aerial images captured by UAVs varies greatly and needs to meet certain real-time performance in detection. To deal with these challenges, we proposed a lightweight model named DSYolov3. We made the following improvements to the Yolov3 model: 1) multiple scale-aware decision discrimination network to detect objects in different scales, 2) a multi-scale fusion-based channel attention model to exploit the channel-wise information complementation, 3) a sparsity-based channel pruning to compress the model. Extensive experimental evaluation has demonstrated the effectiveness and efficiency of our approach. By the proposed approach, we could not only achieve better performance than most existing detectors but also ensure the models practicable on the UAVs.","Aerial images, UAVs, Small-size targets, Mutil-scale aggregation, Attention, Model compression",Zhaokun Li and Xueliang Liu and Ye Zhao and Bo Liu and Zhen Huang and Richang Hong,https://www.sciencedirect.com/science/article/pii/S1047320321000286,https://doi.org/10.1016/j.jvcir.2021.103058,1047-3203,2021,103058,77,Journal of Visual Communication and Image Representation,A lightweight multi-scale aggregated model for detecting aerial images captured by UAVs,article,LI2021103058
"The fields of signal and image processing have been deeply influenced by the introduction of deep neural networks. Despite their impressive success, the architectures used in these solutions come with no clear justification, being âblack boxâ machines that lack interpretability. A constructive remedy to this drawback is a systematic design of networks by unfolding well-understood iterative algorithms. A popular representative of this approach is LISTA, evaluating sparse representations of processed signals. In this paper, we revisit this task and propose an unfolded version of a greedy pursuit algorithm for the same goal. More specifically, we concentrate on the well-known OMP algorithm, and introduce its unfolded and learned version. Key features of our Learned Greedy Method (LGM) are the ability to accommodate a dynamic number of unfolded layers, and a stopping mechanism based on representation error. We develop several variants of the proposed LGM architecture and demonstrate their flexibility and efficiency.","Sparse representation, Orthogonal Matching Pursuit, Unfolding pursuit algorithms, Interpretable image processing architectures, Denoising, Deraining",Rajaei Khatib and Dror Simon and Michael Elad,https://www.sciencedirect.com/science/article/pii/S1047320321000560,https://doi.org/10.1016/j.jvcir.2021.103095,1047-3203,2021,103095,77,Journal of Visual Communication and Image Representation,Learned Greedy Method (LGM): A novel neural architecture for sparse coding and beyond,article,KHATIB2021103095
"Deep learning based stereo matching algorithms have produced impressive disparity estimation for recent years; and the success of them has once overshadowed the conventional ones. In this paper, we intend to reverse this inferiority, by leveraging Stacking Learning with Coalesced Cost Filtering to make the conventional algorithms achieve or even surpass the results of deep learning ones. Four classical and Discriminative Dictionary Learning (DDL) algorithms are adopted as base-models for Stacking. For the former ones, four classical stereo matching algorithms are employed and regarded as âCoalesced Cost Filtering Moduleâ; for the latter supervised learning one, we utilize the Discriminative Dictionary Learning (DDL) stereo matching algorithm. Then three categories of features are extracted from the predictions of base-models to train the meta-model. For the meta-model (final classifier) of Stacking, the Random Forest (RF) classifier is selected. In addition, we also employ an advanced one-view disparity refinement strategy to compute the final refined results more efficiently. Performance evaluations on Middlebury v.2 and v.3 stereo data sets demonstrate that the proposed algorithm outperforms other four most challenging stereo matching algorithms. Besides, the submitted online results even show better results than deep learning ones.","Stereo matching, Stacking, Random Forest, One-view disparity refinement",Peng Yao and Jieqing Feng,https://www.sciencedirect.com/science/article/pii/S1047320321001097,https://doi.org/10.1016/j.jvcir.2021.103169,1047-3203,2021,103169,78,Journal of Visual Communication and Image Representation,Stacking learning with coalesced cost filtering for accurate stereo matching,article,YAO2021103169
"In recent years, stereo cameras have been widely used in various fields. Due to the limited resolution of real equipments, stereo image super-resolution (SR) is a very important and hot topic. Recent studies have shown that deep network structures can directly affect feature expression and extraction and thus influence the final results. In this paper, we propose a multi-atrous residual attention stereo super-resolution network (MRANet) with parallax extraction and strong discriminative ability. Specifically, we propose a multi-scale atrous residual attention (MARA) block to obtain receptive fields of different scales through a multi-scale atrous convolution and then combine them with attention mechanisms to extract more diverse and meaningful information. Moreover, we propose a stereo feature fusion unit for stereo parallax extraction and single viewpoint feature refinement and integration. Experiments on benchmark datasets show that MRANet achieves state-of-the-art performance in terms of quantitative metrics and visual quality compared with several SR methods.","Stereo cameras, Stereo image super-resolution, Discriminative ability, Parallax extraction, Attention mechanism",Luyao Ning and Anhong Wang and Lijun Zhao and Weimin Xue and Donghan Bu,https://www.sciencedirect.com/science/article/pii/S1047320321000717,https://doi.org/10.1016/j.jvcir.2021.103115,1047-3203,2021,103115,77,Journal of Visual Communication and Image Representation,MRANet: Multi-atrous residual attention Network for stereo image super-resolution,article,NING2021103115
"It is the most crucial problem to remove ghost in the multi-exposure image fusion of dynamic scene. The traditional fusion methods have good effects to remove weak ghosts. However, they cannot effectively remove strong ghosts. This paper proposes a new strong ghost removal method in multi-exposure image fusion using hole-filling with exposure congruency. First, analyzing the characteristics of strong ghosts, a detection scheme for strong ghost regions is designed by combining histogram matching and exposure difference detection. Subsequently, to effectively extract image local features, a multi-scale fusion network for non-strong ghost regions is designed to obtain a pre-fused image. Further, based on the distribution characteristics of strong ghosts, a hole-filling model with exposure congruency is designed to remove the strong ghosts. Experimental results show that compared with the state-of-the-art methods, the proposed method can obtain better performance in both of subjective and objective evaluation, particularly in terms of effectively removing strong ghosts.","Multi-exposure image fusion, Weak ghost, Strong ghost, Ghost removal, Exposure congruency, Hole-filling",Hua Shao and Mei Yu and Gangyi Jiang and Zhiyong Pan and Zongju Peng and Fen Chen,https://www.sciencedirect.com/science/article/pii/S1047320320302273,https://doi.org/10.1016/j.jvcir.2020.103017,1047-3203,2021,103017,75,Journal of Visual Communication and Image Representation,Strong ghost removal in multi-exposure image fusion using hole-filling with exposure congruency,article,SHAO2021103017
"Zero-shot learning (ZSL) aims to recognize unseen image classes without requiring any training samples of these specific classes. The ZSL problem is typically achieved by building up a semantic embedding space like attributes to bridge the visual features and class labels of images. Currently, most ZSL approaches focus on learning a visual-semantic alignment from seen classes using only the human-designed attributes, and then ZSL problem is solved by transferring semantic knowledge from seen classes to the unseen classes. However, few works indicate if the human-designed attributes are discriminative enough for image class prediction. To address this issue, we propose a semantic-aware dictionary learning (SADL) framework to explore these discriminative visual attributes across seen and unseen classes. Furthermore, the semantic cues are elegantly integrated into the feature representations via learned visual attributes for recognition task. Experiments conducted on two challenging benchmark datasets show that our approach outweighs other state-of-the-art ZSL methods.","Zero-shot learning, Human-designed attributes, Visual attributes, Semantic representation",Yurui Xie and Tiecheng Song and Wei Li,https://www.sciencedirect.com/science/article/pii/S1047320320302224,https://doi.org/10.1016/j.jvcir.2020.103010,1047-3203,2021,103010,74,Journal of Visual Communication and Image Representation,Semantic-aware visual attributes learning for zero-shot recognition,article,XIE2021103010
"Recently, Facial Expression Recognition (FER) has gained much attention in the research area for its various applications. In the facial expression recognition task, subject-dependent issue is predominant when a small-scale database is used for training the system. The proposed Auxiliary Classifier Generative Adversarial Network (AC-GAN) based model regenerates ten expressions (angry, contempt, disgust, embarrassment, fear, joy, neutral, pride, sad, surprise) from input face image and recognizes its expression. To alleviate the subject dependence issue, we train the model person-wise and generate all the above expressions for a person and allow the discriminator to classify the expressions. The generator of our model uses U-Net Architecture, and the discriminator uses Capsule Networks for improved feature extraction. The model has been evaluated on the ADFES-BIV dataset yielding an overall classification accuracy of 93.4%. We also compared our model with the existing methods by evaluating our model on commonly used datasets like CK+, KDEF.","Facial Expression Recognition (FER), Subject dependence, Conditional GAN(CGAN), Auxiliary Classifier GAN(ACGAN), U-Net, Capsule Network(capsuleNet)",Dharanya V. and Alex Noel {Joseph Raj} and Varun P. Gopi,https://www.sciencedirect.com/science/article/pii/S1047320321000675,https://doi.org/10.1016/j.jvcir.2021.103110,1047-3203,2021,103110,77,Journal of Visual Communication and Image Representation,Facial Expression Recognition through person-wise regeneration of expressions using Auxiliary Classifier Generative Adversarial Network (AC-GAN) based model,article,V2021103110
"In recent years, the light field (LF) as a new imaging modality has attracted wide interest. The large data volume of LF images poses great challenge to LF image coding, and the LF images captured by different devices show significant differences in angular domain. In this paper we propose a view prediction framework to handle LF image coding with various sampling density. All LF images are represented as view arrays. We first partition the views into reference view (RV) set and intermediate view (IV) set. The RVs are rearranged into a pseudo sequence and directly compressed by a video encoder. Other views are then predicted by the RVs. To exploit the four dimensional signal structure, we propose the linear approximation prior (LAP) to reveal the correlation among LF views and efficiently remove the LF data redundancy. Based on the LAP, a distortion minimization interpolation (DMI) method is used to predict IVs. To robustly handle the LF images with different sampling density, we propose an Iteratively Updating depth image based rendering (IU-DIBR) method to extend our DMI. Some auxiliary views are generated to cover the target region and then the DMI calculates reconstruction coefficients for the IVs. Different view partition patterns are also explored. Extensive experiments on different types LF images also valid the efficiency of the proposed method.","Light field image, View partition, Camera array, Image compression, View synthesis, Linear approximation",Shengyang Zhao and Zhibo Chen,https://www.sciencedirect.com/science/article/pii/S1047320321000158,https://doi.org/10.1016/j.jvcir.2021.103036,1047-3203,2021,103036,75,Journal of Visual Communication and Image Representation,Various density light field image coding based on distortion minimization interpolation,article,ZHAO2021103036
"The diversity of pedestrians detectors proposed in recent years has encouraged some works to fuse them to achieve a more accurate detection. The intuition behind it is to combine the detectors based on its spatial consensus. The hypothesis is that a location pointed by multiple detectors has a high probability of actually belonging to a pedestrian, while false positive regions have little consensus among detectors (small support) which allows discarding the false positives in these regions. We proposed a novel method called Content-Based Spatial Consensus (CSBC), which, in addition to relying on spatial consensus, considers the content of the detection windows to learn a weighted-fusion of pedestrian detectors. The result is a reduction in false alarms and an enhancement in the detection. In this work, we also demonstrated that there is small influence of the feature used to learn the contents of the windows of each detector, which enables our method to be efficient even employing simple features. The CSBC overcomes state-of-the-art fusion methods in the ETH dataset and the Caltech dataset. Particularly, our method is also more efficient, since fewer detectors are necessary to achieve expressive results.","Edestrian detection, Content-based fusion, Spatial consensus, Multiple detectors, Late fusion",Jessica Sena and Artur JordÃ£o and William Robson Schwartz,https://www.sciencedirect.com/science/article/pii/S1047320321000559,https://doi.org/10.1016/j.jvcir.2021.103091,1047-3203,2021,103091,77,Journal of Visual Communication and Image Representation,A content-based late fusion approach applied to pedestrian detection,article,SENA2021103091
"Recently, single image super-resolution (SISR) has been widely applied in the fields of underwater robot vision and obtained remarkable performance. However, most current methods generally suffered from the problem of a heavy burden on computational resources with large model sizes, which limited their real-world underwater robotic applications. In this paper, we introduce and tackle the super resolution (SR) problem for underwater robot vision and provide an efficient solution for near real-time applications. We present a novel lightweight multi-stage information distillation network, named MSIDN, for better balancing performance against applicability, which aggregates the local distilled features from different stages for more powerful feature representation. Moreover, a novel recursive residual feature distillation (RRFD) module is constructed to progressively extract useful features with a modest number of parameters in each stage. We also propose a channel interaction & distillation (CI&D) module that employs channel split operation on the preceding features to produce two-part features and utilizes the inter channel-wise interaction information between them to generate the distilled features, which can effectively extract the useful information of current stage without extra parameters. Besides, we present USR-2K dataset, a collection of over 1.6K samples for large-scale underwater image SR training, and a testset with an additional 400 samples for benchmark evaluation. Extensive experiments on several standard benchmark datasets show that the proposed MSIDN can provide state-of-the-art or even better performance in both quantitative and qualitative measurements.","Underwater image, Super-resolution, Visually-guided underwater robots, Convolutional neural network",Huan Wang and Hao Wu and Qian Hu and Jianning Chi and Xiaosheng Yu and Chengdong Wu,https://www.sciencedirect.com/science/article/pii/S1047320321000833,https://doi.org/10.1016/j.jvcir.2021.103136,1047-3203,2021,103136,77,Journal of Visual Communication and Image Representation,Underwater image super-resolution using multi-stage information distillation networks,article,WANG2021103136
"A system for automatic pain detection whereby pain-related features are extracted from facial images using a four-layer Convolutional Deep Belief Network (CDBN) is proposed in this study. The CDBN is trained by greedy layer-wise procedure whereby each added layer is trained as a Convolutional Restricted Boltzmann Machine (CRBM) by contrastive divergence. Since conventional CRBM is trained in a purely unsupervised manner, there is no guarantee that learned features are appropriate for the supervised task at hand. A discriminative objective based on between-class and within-class distances is proposed to adapt CRBM to learn task-related features. When discriminative and generative objectives are appropriately combined, a competitive classification performance can be achieved. Moreover, we introduced batch normalization (BN) units in the structure of the CRBM model to smooth optimization landscape and speed up the learning process. BN units come right before sigmoid units. Extracted features are then used to train a linear SVM to classify each frame into pain or no-pain classes. Extensive experiments on UNBC-McMaster Shoulder Pain database demonstrate the effectiveness of the proposed method for automatic pain detection.","Pain detection, Convolutional deep belief network, Discriminant Feature Learning, Representation learning, Batch Normalization",Reza Kharghanian and Ali Peiravi and Farshad Moradi and Alexandros Iosifidis,https://www.sciencedirect.com/science/article/pii/S1047320321000316,https://doi.org/10.1016/j.jvcir.2021.103062,1047-3203,2021,103062,76,Journal of Visual Communication and Image Representation,Pain detection using batch normalized discriminant restricted Boltzmann machine layers,article,KHARGHANIAN2021103062
"As an active forensic technology, perceptual image hash has important application in image content authenticity detection and integrity authentication. In this paper, we propose a hybrid-feature-based perceptual image hash method that can be used for image tampering detection and tampering localization. In the proposed method, we use the color features of image as global features, use point-based features and block-based features as local features, and combine with the structural features to generate intermediate hash code. Then we encrypt and randomize to generate the final hash code. Using this hash code, we present a coarse-to-fine grained forensics method for image tampering detection. The proposed method can realize object-level tampering localization. Abundant experimental results show that the proposed method is sensitive to content changes caused by malicious attacks, and the tampering localization precision achieves pixel level, and it is robust to a wide range of geometric distortions and content-preserving manipulations. Compared with the state-of-the-art schemes, the proposed scheme yields superior performance.","Perceptual image hash, Simple linear iterative clustering, Image forging detection, Image tempering localization",Xiaofeng Wang and Qian Zhang and Chuntao Jiang and Jianru Xue,https://www.sciencedirect.com/science/article/pii/S104732032100078X,https://doi.org/10.1016/j.jvcir.2021.103124,1047-3203,2021,103124,78,Journal of Visual Communication and Image Representation,Perceptual hash-based coarse-to-fine grained image tampering forensics method,article,WANG2021103124
"Semantic segmentation is a prominent problem in scene understanding expressed as a dense labeling task with deep learning models being one of the main methods to solve it. Traditional training algorithms for semantic segmentation models produce less than satisfactory results when not combined with post-processing techniques such as CRFs. In this paper, we propose a method to train segmentation models using an approach which utilizes classification information in the training process of the segmentation network. Our method employs the use of classification network that detects the presence of classes in the segmented output. These class scores are then used to train the segmentation model. This method is motivated by the fact that by conditioning the training of the segmentation model with these scores, higher order features can be captured. Our experiments show significantly improved performance of the segmentation model on the CamVid and CityScapes datasets with no additional post processing.","Scene understanding, Semantic segmentation, Computer vision, Deep learning",Ifham Abdul Latheef Ahmed and Mohamed Hisham Jaward,https://www.sciencedirect.com/science/article/pii/S1047320321001127,https://doi.org/10.1016/j.jvcir.2021.103177,1047-3203,2021,103177,78,Journal of Visual Communication and Image Representation,Classifier aided training for semantic segmentation,article,AHMED2021103177
"Understanding human emotions requires information from different modalities like vocal, visual, and verbal. Since human emotion is time-varying, the related information is usually represented as temporal sequences and we need to identify both emotion-related clues and their cross-modal interactions inside. However, emotion-related clues are sparse and misaligned in temporally unaligned sequences, making it hard for previous multi-modal emotion recognition methods to catch helpful cross-modal interactions. To this end, we present cross-modal dynamic convolution. To deal with sparsity, cross-modal dynamic convolution models the temporal dimension locally to avoid being overwhelmed by unrelated information. Cross-modal dynamic convolution is easy to stack, enabling it to model long-range cross-modal temporal interactions. Besides, models with cross-modal dynamic convolution are more stable during training than with cross-modal attention, bringing more possibilities in multi-modal sequential model designing. Extensive experiments show that our method can achieve competitive performance compared to previous works while being more efficient.","Artificial neural networks, Pattern recognition, Affective behavior, Multi-modal temporal sequences",Huanglu Wen and Shaodi You and Ying Fu,https://www.sciencedirect.com/science/article/pii/S1047320321001085,https://doi.org/10.1016/j.jvcir.2021.103178,1047-3203,2021,103178,78,Journal of Visual Communication and Image Representation,Cross-modal dynamic convolution for multi-modal emotion recognition,article,WEN2021103178
"This work examines inexpensive design choices for dehazing as an end-to-end image-to-image mapping problem without relying on the physical scattering model. The proposed TheiaNet is free from intermediate-computation of transmission map, enabling haze removal in a highly resource constrained environments. The simplicity of the network is augmented by a spatial cleaning bottleneck block, that adds faster feature extraction without adding to trainable parameters. We also analyze the effectiveness of multi-cue color space (RGB, HSV, LAB, YCbCr) over single cue color space (RGB) for end-to-end dehazing. A comprehensive set of experiments were conducted on HazeRD, D-Hazy and the more recent Reside datasets. The proposed TheiaNet significantly outperforms the existing CNN and GAN based state-of-the-art methods in terms of PSNR and SSIM on all these datasets. It also outperforms all existing methods in term of speed, compute and memory efficiency, making it more efficient. This work highlights how judicious application-specific components can augment simple CNNs to denoise faster, and more accurately than deeper heavier networks, which is supported by an ablation analysis as well.","Dehazing, Computation efficiency, Speed and memory tests, Design choices",Aryan Mehra and Pratik Narang and Murari Mandal,https://www.sciencedirect.com/science/article/pii/S1047320321000791,https://doi.org/10.1016/j.jvcir.2021.103137,1047-3203,2021,103137,77,Journal of Visual Communication and Image Representation,TheiaNet: Towards fast and inexpensive CNN design choices for image dehazing,article,MEHRA2021103137
"Copy Move is a technique widespreadly used in digital image tampering, meaning Copy Move Forgery Detection (CMFD) is still a significant research. In this paper, a novel CMFD method is proposed, including double matching process and region localizing process. In double matching process, the first matching is conducted on Delaunay triangles consisting of Local Intensity Order Pattern (LIOP) keypoints, to find the approximate location of suspicious regions. In order to find sufficient keypoint pairs, the existing set of matching triangles is expanded by adding their neighbors iteratively, covering the whole tampered regions, and the second matching with a looser threshold is conducted on the vertices. In the region localizing process, considering the case of multiple copies, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is used to classify the keypoint pairs described in a new model. Experimental results indicate that the proposed method, with good robustness, outperforms some state-of-the-art methods.","Digital forensics, Copy move forgery detection, Delaunay triangle, Double matching",Qiyue Lyu and Junwei Luo and Ke Liu and Xiaolin Yin and Jiarui Liu and Wei Lu,https://www.sciencedirect.com/science/article/pii/S1047320321000274,https://doi.org/10.1016/j.jvcir.2021.103057,1047-3203,2021,103057,76,Journal of Visual Communication and Image Representation,Copy Move Forgery Detection based on double matching,article,LYU2021103057
"In this paper, we present a novel deep generative facial parts swapping method: parts-swapping generative adversarial network (PSGAN). PSGAN independently handles facial parts, such as eyes (left eye and right eye), nose, mouth and jaw, which achieves facial parts swapping by replacing the target facial parts with source facial parts and reconstructing the entire face image with these parts. By separately modeling the facial parts in the form of region inpainting, the proposed method can successfully achieve highly photorealistic face swapping results, enabling users to freely manipulate facial parts. In addition, the proposed method is able to perform jaw editing based on sketch guidance information. Experimental results on the CelebA dataset suggest that our method achieves superior performance for facial parts swapping and provides higher user control flexibility.","Facial parts swapping, Generative adversarial network, Deep leaning",Jingtao Guo and Yi Liu,https://www.sciencedirect.com/science/article/pii/S1047320321000948,https://doi.org/10.1016/j.jvcir.2021.103152,1047-3203,2021,103152,78,Journal of Visual Communication and Image Representation,Facial parts swapping with generative adversarial networks,article,GUO2021103152
"Two of the most important premises of an ensemble are the diversity of its components and how to combine their votes. In this paper, we propose a multi-stream architecture based on the weighted voting of convolutional neural networks to deal with the problem of recognizing human actions in videos. A major challenge is how to include temporal aspects into this kind of approach. A key step in this direction is the selection of features that characterize the complexity of human actions in time. In this context, we propose a new stream, Optical Flow Rhythm, besides using other streams for diversity. To combine the streams, a voting system based on a new weighted average fusion method is introduced. In this scheme, the weights of classifiers are defined by an optimization process led by a metaheuristic. Experiments conducted on the UCF101 and HMDB51 datasets demonstrate that our method is comparable to state-of-the-art approaches.","Convolutional neural networks, Action recognition, Optical flow rhythm",AndrÃ© {de Souza Brito} and Marcelo {Bernardes Vieira} and Saulo {Moraes Villela} and Hemerson Tacon and Hugo {de Lima Chaves} and Helena {de Almeida Maia} and Darwin {Ttito Concha} and Helio Pedrini,https://www.sciencedirect.com/science/article/pii/S1047320321000705,https://doi.org/10.1016/j.jvcir.2021.103112,1047-3203,2021,103112,77,Journal of Visual Communication and Image Representation,Weighted voting of multi-stream convolutional neural networks for video-based action recognition using optical flow rhythms,article,DESOUZABRITO2021103112
"Prior image deraining works mainly have two problems: (1) they do not generalize well to various datasets; (2) too much detail information is lost in the heavy rain area of the rain image. To overcome these two problems, we propose a new two-stage Adversarial Residual Refinement Network (ARRN) to deal with heavy rain images. Specifically, for the first problem, we first introduce a new implicit rain model to model a rain image as a composition of a background image and a residual image. Based on the proposed implicit model, we then propose the ARRN which consists of an image decomposition stage and an image refinement stage. For the second problem, a new attention Wasserstein Generative Adversarial Networks (WGAN) loss in the refinement stage is introduced to force the network to focus on refining heavily degraded areas. Comprehensive experiments demonstrate the effectiveness of the proposed approach.","Image deraining, Deep learning, GAN",Wei Xu and Song Qiu and Kunyao Huang and Wei Liu and Junzhe Zuo and Haoming Guo,https://www.sciencedirect.com/science/article/pii/S104732032100081X,https://doi.org/10.1016/j.jvcir.2021.103133,1047-3203,2021,103133,77,Journal of Visual Communication and Image Representation,Image deraining with Adversarial Residual Refinement Network,article,XU2021103133
"Nowadays, image annotation has been a hot topic in the semantic retrieval field due to the abundant growth of digital images. The purpose of these methods is to realize the content of images and assign appropriate keywords to them. Extensive efforts have been conducted in this field, which effectiveness is limited between low-level image features and high-level semantic concepts. In this paper, we propose a Multi-View Robust Spectral Clustering (MVRSC) method, which tries to model the relationship between semantic and multi-features of training images based on the Maximum Correntropy Criterion. A Half-Quadratic optimization framework is used to solve the objective function. According to the constructed model, a few tags are suggested based on a novel decision-level fusion distance. The stability condition and bound calculation of MVRSC are analyzed, as well. Experimental results on real-world Flickr and 500PX datasets, and Corel5K confirm the superiority of the proposed method over other competing models.","Image annotation, Geo-tagged photos, Recommender systems, Maximum correntropy criterion, Multi-view spectral clustering, Geographical information",Mona Zamiri and Hadi {Sadoghi Yazdi},https://www.sciencedirect.com/science/article/pii/S1047320320302182,https://doi.org/10.1016/j.jvcir.2020.103003,1047-3203,2021,103003,74,Journal of Visual Communication and Image Representation,Image annotation based on multi-view robust spectral clustering,article,ZAMIRI2021103003
"Robust loop-closure detection is essential for visual SLAM. Traditional methods often focus on the geometric and visual features in most scenes but ignore the semantic information provided by objects. Based on this consideration, we present a strategy that models the visual scene as semantic sub-graph by only preserving the semantic and geometric information from object detection. To align two sub-graphs efficiently, we use a sparse KuhnâMunkres algorithm to speed up the search for correspondence among nodes. The shape similarity and the Euclidean distance between objects in the 3-D space are leveraged unitedly to measure the image similarity through graph matching. Furthermore, the proposed approach has been analyzed and compared with the state-of-the-art algorithms at several datasets as well as two indoor real scenes, where the results indicate that our semantic graph-based representation without extracting visual features is feasible for loop-closure detection at potential and competitive precision.","Loop closure detection, Object detection, Semantic, Simultaneous localization and mapping (SLAM), Graph matching",Cao Qin and Yunzhou Zhang and Yingda Liu and Guanghao Lv,https://www.sciencedirect.com/science/article/pii/S1047320321000389,https://doi.org/10.1016/j.jvcir.2021.103072,1047-3203,2021,103072,76,Journal of Visual Communication and Image Representation,Semantic loop closure detection based on graph matching in multi-objects scenes,article,QIN2021103072
"A novel specular highlight restoration algorithm has been proposed to remove the specular highlight in a real-time vision system. In this paper, the specular highlight region has been detected and it has been restored by image inpainting method. Most of specular highlight detection algorithms are effective only for a single image. And the auto-threshold algorithm has been implemented in real time, but, it has still low reliability with a heavy computational cost. The proposed system detects pixels corresponding to the specular highlight region in the HSI color space through a newly defined classification table. So, Specular highlight can be detected quickly with these simplified classification table. In addition, it has versatility because it can be combined with various existing high-performance image inpainting method. The superiority of the proposed algorithm is compared with the conventional specular highlight removal algorithm by combining proposed algorithm with two high-performance image inpainting techniques.","Specular highlight detection, Highlight identification, Pixel analysis, Pixel clustering, Image inpainting",Hosun Kang and Dokyung Hwang and Jangmyung Lee,https://www.sciencedirect.com/science/article/pii/S1047320321000663,https://doi.org/10.1016/j.jvcir.2021.103106,1047-3203,2021,103106,77,Journal of Visual Communication and Image Representation,Specular highlight region restoration using image clustering and inpainting,article,KANG2021103106
"Single image deraining is a challenging problem due to the presence of non-uniform rain densities and the ill-posedness of the problem. Moreover, over-/under-deraining can directly impact the performance of vision systems. To address these issues, we propose an end-to-end Context Aggregation Recurrent Network, called CARNet, to remove rain streaks from single images. In this paper, we assume that a rainy image is the linear combination of a clean background image with rain streaks and propose to take advantage of the context information and feature reuse to learn the rain streaks. In our proposed network, we first use the dilation technique to effectively aggregate context information without sacrificing the spatial resolution, and then leverage a gated subnetwork to fuse the intermediate features from different levels. To better learn and reuse rain streaks, we integrate a LSTM module to connect different recurrences for passing the information learned from the previous stages about the rain streaks to the following stage. Finally, to further refine the coarsely derained image, we introduce a refinement module to better preserve image details. As for the loss function, the L1-norm perceptual loss and SSIM loss are adopted to reduce the gridding artifacts caused by the dilated convolution. Experiments conducted on synthetic and real rainy images show that our CARNet achieves superior deraining performance both qualitatively and quantitatively over the state-of-the-art approaches.","Image deraining, Context awareness, Dilated convolution, Recurrent network, Perceptual loss",Qunfang Tang and Jie Yang and Haibo Liu and Zhiqiang Guo and Wenjing Jia,https://www.sciencedirect.com/science/article/pii/S1047320321000146,https://doi.org/10.1016/j.jvcir.2021.103039,1047-3203,2021,103039,75,Journal of Visual Communication and Image Representation,Single image deraining using Context Aggregation Recurrent Network,article,TANG2021103039
"Human gait represents an attractive biometric modality to re-identify a person as it requires non contact and it is perceivable at a distance. However, the view angle variation and the presence of covariate factors cause significant difficulties for recognizing gaits. In order to deal with such constraints, this paper presents a Part View Transformation Model (PVTM) for gait based applications. Compared with previous methods, the PVTM is applied on selected relevant parts chosen through a semantic classification step. Conducted on the CASIA-B gait database, experimental results show that the proposed method outperforms well known multi-view methods even under covariate factors (i.e. carrying bag, clothing).","Re-identification, Gait, Part View Transformation Model (PVTM), View angle variation, Covariate factors",Imen Chtourou and Emna Fendri and Mohamed Hammami,https://www.sciencedirect.com/science/article/pii/S1047320321000547,https://doi.org/10.1016/j.jvcir.2021.103093,1047-3203,2021,103093,77,Journal of Visual Communication and Image Representation,Person re-identification based on gait via Part View Transformation Model under variable covariate conditions,article,CHTOUROU2021103093
"High dynamic range (HDR) images greatly improve visual content quality, but pose challenges in processing, acquisition, and display. Images captured in real-world scenarios with multiple nonlinear cameras, extremely short unknown exposure time, and a shared light source present the additional challenges of incremental baseline and angle deviation amongst the cameras. The disparity maps in such conditions are not reliable; therefore, we propose a method that relies on the accurate detection and matching of feature points across adjacent viewpoints. We determine the exposure gain among the matched feature points in the involved views and design an image restoration method to restore multiview low dynamic range (MVLDR) images for each viewpoint. Finally, the fusion of these restored MVLDR images produces high-quality images for each viewpoint without capturing a series of bracketed exposure. Extensive experiments are conducted in controlled and uncontrolled conditions, and results prove that the proposed method competes for the state-of-the-arts.","Multi-view low-light images, Feature matching, Exposure fusion",Rizwan Khan and You Yang and Qiong Liu and Zahid Hussain Qaisar,https://www.sciencedirect.com/science/article/pii/S1047320321001115,https://doi.org/10.1016/j.jvcir.2021.103175,1047-3203,2021,103175,78,Journal of Visual Communication and Image Representation,A ghostfree contrast enhancement method for multiview images without depth information,article,KHAN2021103175
"To reduce the computational complexity of screen content video coding (SCC), a fast algorithm based on gray level co-occurrence matrix and Gabor feature model for HEVC-SCC, denoted as GGM, is proposed in this paper. By studying the correlation of non-zero number in gray level co-occurrence matrix with different partitioning depth, the coding unit (CU) size of intra coding can be prejudged, which selectively skips the intra prediction process of CU in other depth. With Gabor filter, the edge information reflecting the features of screen content images to the human visual system (HVS) are extracted. According to Gabor feature, CUs are classified into natural content CUs (NCCUs), smooth screen content CUs (SSCUs) and complex screen content CUs (CSCUs), with which, the calculation and judgment of unnecessary intra prediction modes are skipped. Under all-intra (AI) configuration, experimental results show that the proposed algorithm GGM can achieve encoding time saving by 42.13% compared with SCM-8.3, and with only 1.85% bit-rate increasement.","Screen content video coding, High efficiency video coding (HEVC), Intra prediction, Partition decision",Jing Chen and Jianshan Ou and Huanqiang Zeng and Canhui Cai,https://www.sciencedirect.com/science/article/pii/S1047320321000821,https://doi.org/10.1016/j.jvcir.2021.103128,1047-3203,2021,103128,78,Journal of Visual Communication and Image Representation,A fast algorithm based on gray level co-occurrence matrix and Gabor feature for HEVC screen content coding,article,CHEN2021103128
"In this paper, we propose a new reversible data hiding method in encrypted images. Due to spatial correlation, there is a large probability that the adjacent pixels of the image have small differences, which is especially obvious on the high four most significant bits (high nibbles) of the pixels. If the high nibble of each pixel is regarded as a 4-bit value, the differences between the high nibbles of the adjacent pixels are mostly concentrated in a small range. Based on this fact, Huffman coding was used to encode all the differences between the high nibbles of the adjacent pixels in order to compress the four most significant bit (MSB) planes efficiently and create a large-capacity room. After creating room, a stream cipher is used to encrypt the image, and the room is reserved in the encrypted image for data hiding without losing information. The experimental results showed that the proposed method can achieve a larger embedding rate and better visual quality of the marked decrypted image than other related methods.","Reversible data hiding, Image encryption, Huffman coding, High nibble",Chih-Cheng Chen and Chin-Chen Chang and Kaimeng Chen,https://www.sciencedirect.com/science/article/pii/S1047320321000304,https://doi.org/10.1016/j.jvcir.2021.103060,1047-3203,2021,103060,76,Journal of Visual Communication and Image Representation,High-capacity reversible data hiding in encrypted image based on Huffman coding and differences of high nibbles of pixels,article,CHEN2021103060
"Images with hazy scene suffer from low-contrast, which reduces the visible quality of the scene, thus making object detection a more challenging task. Low-contrast can result from foggy weather conditions during image acquisition. Dehazing is a process of removal of haze from the photography of a hazy scene. Single-image dehazing based on dark channel priors are well-known techniques in this field. However, the performance of such techniques is limited to priors or constraints. Moreover, this type of method fails when images have sky-region. So, a method is proposed, which can restore the visibility of hazy images. First, a hazy image is divided into blocks of size 32Â ÃÂ 32, then the score of each block is calculated to select a block having the highest score. Atmospheric light is calculated from the selected block. A new color channel is considered to remove atmospheric scattering, obtained channel value and atmospheric light are then used to calculate the transmission map in the second step. Third, radiance is computed using a transmission map and atmospheric light. The illumination scaling factor is adopted to enhance the quality of a dehazed image in the final step. Experiments are performed on six datasets namely, I-HAZE, O-HAZE, BSDS500, FRIDA, RESIDE dataset and natural images from Google. The proposed method is compared against 11 state-of-the-art methods. The performance is analyzed using fourteen quantitative evaluation metrics. All the results demonstrate that the proposed method outperforms 11 state-of-the-art methods in most of the cases.","Image dehazing, Atmospheric light, Radiance, Illuminance scaling factor",Geet Sahu and Ayan Seal and Ondrej Krejcar and Anis Yazidi,https://www.sciencedirect.com/science/article/pii/S1047320320302212,https://doi.org/10.1016/j.jvcir.2020.103008,1047-3203,2021,103008,74,Journal of Visual Communication and Image Representation,Single image dehazing using a new color channel,article,SAHU2021103008
"Recent studies have shown that super-resolution can be significantly improved by using deep convolution neural network. Although applying a larger number of convolution kernels can extract more features, increasing the number of feature mappings will dramatically increase the training parameters and time complexity. In order to balance the workload among all units and maintain appropriate time complexity, this paper proposes a new network structure for super-resolution. For the sake of making full use of context information, in the structure, the operations of division (S) and fusion (C) are added to the pyramidal bottleneck residual units, and the dense connected methods are used. The proposed network include a preliminary feature extraction net, seven residual units with dense connections, seven convolution layers with the size of 1Ã1 after each residual unit, and a deconvolution layer. The experimental results show that the proposed network has better performance than most existing methods.","Super-resolution, Convolution neural network, Pyramidal network, Residual-feature learning",Feilong Cao and Baijie Chen,https://www.sciencedirect.com/science/article/pii/S1047320320301887,https://doi.org/10.1016/j.jvcir.2020.102963,1047-3203,2021,102963,74,Journal of Visual Communication and Image Representation,Densely connected network with improved pyramidal bottleneck residual units for super-resolution,article,CAO2021102963
"In detecting sensitive media, violence is one of the hardest to define objectively, and thus, a significant challenge to detect automatically. While many studies were conducted in detecting aspects of violence, very few try to approach the general concept. We propose a method that aims to enable machines to understand a high-level concept of violence by first breaking it down into smaller, more objective ones, such as fights, explosions, blood, and gunshots, to combine them later, leading to a better understanding of the scene. For this, we leverage characteristics of each individual sub-concept of violence (relying upon custom-tailored convolutional neural networks) to guide how they should be described. A fight scene should incorporate temporal features that a scene with blood does not need to describe. A scene with explosions or gunshots should weigh more on its audio features. With this multimodal approach, we trained visual and auditory feature detectors and later combined them into a decision neural network to give us a violence detector that considers several different aspects of the problem. This robust and modular approach allows different cultures and users to adapt the detector to their specific needs.",,Bruno M. Peixoto and Bahram Lavi and Zanoni Dias and Anderson Rocha,https://www.sciencedirect.com/science/article/pii/S1047320321001073,https://doi.org/10.1016/j.jvcir.2021.103174,1047-3203,2021,103174,78,Journal of Visual Communication and Image Representation,"Harnessing high-level concepts, visual, and auditory features for violence detection in videos",article,PEIXOTO2021103174
"We propose a novel online multi-target visual tracker based on the recently developed Hypothesized and Independent Stochastic Population (HISP) filter. The HISP filter combines advantages of traditional tracking approaches like MHT and point-process-based approaches like PHD filter, and it has linear complexity while maintaining track identities. We apply this filter for tracking multiple targets in video sequences acquired under varying environmental conditions and targets density using a tracking-by-detection approach. We also adopt deep CNN appearance representation by training a verification-identification network (VerIdNet) on large-scale person re-identification data sets. We construct an augmented likelihood in a principled manner using this deep CNN appearance features and spatio-temporal information. Furthermore, we solve the problem of two or more targets having identical label considering the weight propagated with each confirmed hypothesis. Extensive experiments on MOT16 and MOT17 benchmark data sets show that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy.","Multiple target filtering, HISP filter, Online tracking, Appearance learning, CNN, MOT challenge",Nathanael L. Baisa,https://www.sciencedirect.com/science/article/pii/S1047320320301802,https://doi.org/10.1016/j.jvcir.2020.102952,1047-3203,2021,102952,77,Journal of Visual Communication and Image Representation,Robust online multi-target visual tracking using a HISP filter with discriminative deep appearance learning,article,BAISA2021102952
"Diffusion-based compactness is an effective method for foreground-based saliency detection, in which one key is the conventional graph construction. However, the conventional graph only displays the local structure but not preserves global relevance information. Therefore, diffusion-based compactness cannot highlight complete salient object which contains multiple areas with different features, and the extracted salient regions with weak homogeneous. Aiming to address these problems, we propose a saliency detection method via coarse-to-fine diffusion-based compactness with a weighted learning affinity matrix. Firstly, we construct multi-view conventional graphs to calculate the rough compactness cue. Secondly, we build a two-stage multi-view weighted graphs using a weighted learning affinity matrix and compute the coarse-to-fine compactness cue. Extensive experiments tested on three benchmark datasets, demonstrating the superior against several state-of-the-art methods.","Saliency detection, Diffusion-based compactness, Multi-view graphs, Weighted learning affinity matrix",Fan Wang and Guohua Peng,https://www.sciencedirect.com/science/article/pii/S1047320321000936,https://doi.org/10.1016/j.jvcir.2021.103151,1047-3203,2021,103151,78,Journal of Visual Communication and Image Representation,Saliency detection via coarse-to-fine diffusion-based compactness with weighted learning affinity matrix,article,WANG2021103151
"Identity management systems with biometric key binding make digital transactions secure and reliable. A novel methodology is proposed to develop an intelligent key management system using facial emotions. Key binding with facial emotions makes use of an intrinsic user specific trait facilitating a more natural computer to human interaction. The proposed system utilizes metaheuristic swarm intelligence based optimization techniques to extract optimal features. The work demonstrates key binding by encrypting an image with a secret key bound to optimal features extracted from facial emotions. Efficiency and correctness of proposed key management is validated by successful decryption at receiving end with any one of the enrolled emotions given as input. Deer Hunting Optimization Algorithm and Chicken Swarm Optimization are merged to select optimal features from facial emotions. The derived algorithm is called Fitness Sorted Deer Hunting Optimization Algorithm with Rooster Update. Seven facial emotions â anger, disgust, fear, happiness, sadness, surprise and neutral are used to extract optimal features from Japanese Female Facial Expressions and Yale Facial datasets to train the neural network. Proposed work achieved better performance results over state-of-art optimization algorithms such as whale optimization algorithm, grey wolf optimization, chicken swarm optimization and deer hunting optimization algorithm. Accuracy of proposed model is 2.2% better than deer hunting optimization algorithm and 12.3% better than chicken swarm optimization for a key length 80.","Identity management system, Facial emotions, Metaheuristic optimization",Suresh Padmanabhan and Radhika K.R.,https://www.sciencedirect.com/science/article/pii/S1047320320302170,https://doi.org/10.1016/j.jvcir.2020.103002,1047-3203,2021,103002,74,Journal of Visual Communication and Image Representation,Optimal feature selection-based biometric key management for identity management system: Emotion oriented facial biometric system,article,PADMANABHAN2021103002
"Transfer learning from natural image datasets, such as ImageNet, is common for applying deep learning to medical imaging. However, the modalities of natural and medical images differ considerably, and the reason for the latest medical research preferring ImageNet to medical data is questionable. In this study, we investigated the properties of medical pre-training and its transfer effectiveness on various medical tasks. Through an intuitive convolution-based analysis, we determined the modality characteristics of images. Surprisingly, medical pre-training showed exceptional performance for a classification task but not for a segmentation task since medical data are visually homogeneous and lack morphological information. Using data with diverse modalities helped overcome such drawbacks, resulting in medical pre-training achieving performance comparable to pre-training with ImageNet with considerably fewer samples than ImageNet for both aforementioned tasks. Finally, a study of learned representations and realistic scenarios indicated that while ImageNet is the best choice for medical imaging, medical pre-training has significant potential.","Transfer learning, Medical image analysis, Convolutional neural network, Survival prediction",Yang Wen and Leiting Chen and Yu Deng and Chuan Zhou,https://www.sciencedirect.com/science/article/pii/S1047320321000894,https://doi.org/10.1016/j.jvcir.2021.103145,1047-3203,2021,103145,78,Journal of Visual Communication and Image Representation,Rethinking pre-training on medical imaging,article,WEN2021103145
"Visual odometry aims to estimate the relative pose between frames, which is a fundamental task for visual SLAM. In this paper, we present a novel line-based visual odometry (VO) algorithm that fully utilizes the characteristic of line to estimate the projected line of adjacent frame by minimizing the local gradient fitness evaluation. In contrast to the current feature-based or line-based visual odometry, we donâ²t need to explicitly match points or lines of two frames, which is non-trivial and inaccurate in challenging scenarios such as texture-less scenes. In our method, the projected line is calculated simultaneously with the local gradient fitting function of pose estimation based on the constraint that the orientation of the projected line should be perpendicular to the gradient orientation of pixels of its local regions. The proposed method is more robust and reliable than other line-based VO since it fully uses the pixel orientations in the local regions to estimate the projected line and relative pose. We evaluate our method on the real-world RGB-D dataset and synthetic benchmark dataset. Experimental results show that our method achieves the state-of-the-art algorithms in indoors scenes, especially in texture-less scenes.","Visual odometry, Line, Gradient, Texture-less, RGB-D",Junxin Lu and Zhijun Fang and Yongbin Gao and Jieyu Chen,https://www.sciencedirect.com/science/article/pii/S1047320321000377,https://doi.org/10.1016/j.jvcir.2021.103071,1047-3203,2021,103071,77,Journal of Visual Communication and Image Representation,Line-based visual odometry using local gradient fitting,article,LU2021103071
"Haze is an aggregation of very fine, widely dispersed, solid and/or liquid particles suspended in the atmosphere. In this paper, we propose an end-to-end network for single image dehazing, which enhances the CycleGAN model by introducing a transformer architecture within the generator, which is specific for haze removal. The proposed model is trained in an unpaired fashion with clear and hazy images altogether and does not require pairs of hazy and corresponding ground-truth clear images. Furthermore, the proposed model does not depend on estimating the parameters of the atmospheric scattering model. Rather, it uses a K-estimation module as the generatorâs transformer for complete end-to-end modeling. The feature transformer introduced in the proposed generator model transforms the encoded features into desired feature space and then feeds them into the CycleGAN decoder to create a clear image. In the proposed model we further modified the cycle consistency loss to include the SSIM loss along with pixel-wise mean loss to produce a new loss function specific for the reconstruction task, which enhances the performance of the proposed model. The model performs well even on the high-resolution images provided in the NTIRE 2019 challenge dataset for single image dehazing. Further, we perform experiments on NYU-Depth and reside beta datasets. Results of our experiments show the efficacy of the proposed approach compared to the state-of-the-art in removing the haze from the input image.","CycleGAN, Cyclic consistency loss, AOD-NET, Single image dehazing, SSIM loss",B.S.N.V. Chaitanya and Snehasis Mukherjee,https://www.sciencedirect.com/science/article/pii/S1047320320302248,https://doi.org/10.1016/j.jvcir.2020.103014,1047-3203,2021,103014,74,Journal of Visual Communication and Image Representation,Single image dehazing using improved cycleGAN,article,CHAITANYA2021103014
"The fully convolutional siamese network based trackers achieve great progress recently. Most of these methods focus on improving the capability of siamese network to represent the target. In this paper, we propose our model which focuses on estimating the state of the target with our proposed novel IoU (intersection over union) loss function which is named AIoU. Our model consists of a siamese subnetwork for feature extraction and a target estimation subnetwork for state representation. The target estimation subnetwork contains a classification head for classifying background and foreground and a regression head for estimating target. In order to regress better bounding boxes, we further study the loss function utilized in the regression head and propose a powerful IoU loss function. Our tracker achieves competitive performance on OTB2015, VOT2018, and VOT2019 benchmarks with a speed of 180 FPS, which proves the effectiveness of our method.","Visual tracking, Siamese network, Target estimation network, Intersection over union (IoU) loss",Zhiyong Li and Chenming Hu and Ke Nai and Jin Yuan,https://www.sciencedirect.com/science/article/pii/S1047320321000651,https://doi.org/10.1016/j.jvcir.2021.103107,1047-3203,2021,103107,77,Journal of Visual Communication and Image Representation,Siamese target estimation network with AIoU loss for real-time visual tracking,article,LI2021103107
"Convolutional neural networks have achieved the state-of-the-art results across numerous applications, but recent work finds that these models can be easily fooled by adversarial perturbations. This is partially due to gradient calculation instability, which may be amplified throughout network layers (Liao etÂ al., 2018). To address this issue, we propose a novel AdvCapsNet derived from CapsuleÂ (Sabour etÂ al., 2017), which utilizes a significantly more complicated non-linearity, to defend against adversarial attacks. In this paper, we focus on the transfer-based black-box adversarial attacks, which are more practical than their white-box counterparts. Specifically, we investigate vanilla Capsuleâs robustness and boost its performance by introducing an adversarial loss function as regularization. The weight updating between capsule layers is implemented via dynamic routing regularized by the additional adversarial term. Extensive experiments demonstrate that the proposed AdvCapsNet can significantly boost Capsuleâs robustness and that AdvCapsNet is far more resistance to adversarial attacks than alternative baselines, including both CNN- and Capsule-based defense models.","Capsule, Adversarial, Defense, Robustness",Yueqiao Li and Hang Su and Jun Zhu,https://www.sciencedirect.com/science/article/pii/S1047320321000134,https://doi.org/10.1016/j.jvcir.2021.103037,1047-3203,2021,103037,75,Journal of Visual Communication and Image Representation,AdvCapsNet: To defense adversarial attacks based on Capsule networks,article,LI2021103037
"The ability to detect gun and gun held in hand or other body parts is a typical human skill. The same problem presents an imperative task for computer vision system. Automatic observer independent detection of hand held gun or gun held in the other body part, whether it is visible or concealed, provides enhance security in vulnerable places and initiates appropriate action there. Compare to the automatic object detection systems, automatic detection of gun has very few successful attempts. In the present scope of this paper, we present an extensive survey on automatic detection of gun and define a taxonomy for this particular detection system. We also describe the inherent difficulties related with this problem. In this survey of published papers, we examine different approaches used in state-of-the-art attempts and compare performances of these approaches. Finally, this paper concludes pointing to the possible research gaps in related fields.","Weapon detection, Survey, Security and surveillance, Multiple object detection, Visual and concealed",Rajib Debnath and Mrinal Kanti Bhowmik,https://www.sciencedirect.com/science/article/pii/S104732032100105X,https://doi.org/10.1016/j.jvcir.2021.103165,1047-3203,2021,103165,78,Journal of Visual Communication and Image Representation,"A comprehensive survey on computer vision based concepts, methodologies, analysis and applications for automatic gun/knife detection",article,DEBNATH2021103165
"The application of adversarial learning for semi-supervised semantic image segmentation based on convolutional neural networks can effectively reduce the number of manually generated labels required in the training process. However, the convolution operator of the generator in the generative adversarial network (GAN) has a local receptive field, so that the long-range dependencies between different image regions can only be modeled after passing through multiple convolutional layers. The present work addresses this issue by introducing a self-attention mechanism in the generator of the GAN to effectively account for relationships between widely separated spatial regions of the input image with supervision based on pixel-level ground truth data. In addition, the adjustment of the discriminator has been demonstrated to affect the stability of GAN training performance. This is addressed by applying spectral normalization to the GAN discriminator during the training process. Our method has better performance than existing full/semi-supervised semantic image segmentation techniques.","Self-Attention Mechanism, Adversarial Learning, Semi-Supervised Learning, Spectral Normalization, Semantic Image Segmentation",Jia Zhang and Zhixin Li and Canlong Zhang and Huifang Ma,https://www.sciencedirect.com/science/article/pii/S1047320321001103,https://doi.org/10.1016/j.jvcir.2021.103170,1047-3203,2021,103170,78,Journal of Visual Communication and Image Representation,Stable self-attention adversarial learning for semi-supervised semantic image segmentation,article,ZHANG2021103170
"The imperceptibility, robustness and data payload are widely considered as the three main properties vital for any image watermarking systems. They are complimentary to each other and hence challenging to attain the right balance between them. The statistical model-based multiplicative watermarking is an effective way to achieve the tradeoff among imperceptibility, robustness and data payload. Radial harmonic Fourier moments (RHFMs) is a strong tool in image processing, which has many advantages, such as lower noise sensitivity, powerful image description ability and geometric invariance feature. In this paper, we propose a new statistical image watermarking scheme using local RHFMs magnitudes and Beta exponential distribution model. Our image watermarking scheme consists of two parts, namely, embedding and extraction. In the embedding process, we divide the host image into no-overlapping blocks and compute the local RHFMs of image blocks, then insert the watermark signal into the robust local RHFMs magnitudes through multiplicative approach. In the extraction phase, robust local RHFMs magnitudes are firstly modeled by employing the Beta exponential distribution, where the statistical properties of local RHFMs magnitudes are captured accurately. Then the modified maximum likelihood parameter estimation (MMLE) approach is introduced to estimate the statistical parameters of Beta exponential distribution model. And finally an image watermark decoder for multiplicative watermarking is developed using Beta exponential distribution and maximum likelihood decision criterion. Experimental results on some test images and comparison with well-known existing methods demonstrate the efficacy and superiority of the proposed statistical image watermarking.","Digital watermarking, Local RHFMs magnitudes, Beta-exponential distribution, Modified maximum likelihood estimation, ML decision criterion",Xiang-yang Wang and Jing Tian and Jia-lin Tian and Pan-pan Niu and Hong-ying Yang,https://www.sciencedirect.com/science/article/pii/S1047320321000778,https://doi.org/10.1016/j.jvcir.2021.103123,1047-3203,2021,103123,77,Journal of Visual Communication and Image Representation,Statistical image watermarking using local RHFMs magnitudes and beta exponential distribution,article,WANG2021103123
"The objective of blind-image quality assessment (BIQA) research is the prediction of perceptual quality of images, without reference information. The humanâs perceptual assessment of quality of an image is the backbone of BIQA research. Therefore, human-provided, mean opinion score (perceptual quality) has been analyzed in detail, and it has been observed to follow the Gaussian distribution and thus can be ideally modeled by the same. In this paper, we have proposed an integrated two-stage Gaussian process-based hybrid-feature selection algorithm for the BIQA problem. Moreover, a new consolidated feature set (obtained from the proposed algorithm), consisting of momentous Natural Scene Statistics (NSS)-based features is used in combination with the Gaussian process regression algorithm for the design of a new blind-image quality evaluator, referred to as GPR-BIQA. The proposed evaluator is tested on eight IQA legacy databases, and it is found that the proposed evaluator proficiently correlate with the human opinion, and outperformed a substantial number of existing approaches.","Image quality assessment (IQA), No-reference (NR), Natural scene statistics, Feature selection, Gaussian process regression, Blind image quality assessment (BIQA)",Hassan Khalid and Dr. Muhammad Ali and Nisar Ahmed,https://www.sciencedirect.com/science/article/pii/S1047320321000535,https://doi.org/10.1016/j.jvcir.2021.103092,1047-3203,2021,103092,77,Journal of Visual Communication and Image Representation,Gaussian Process-based Feature-Enriched Blind Image Quality Assessment,article,KHALID2021103092
"At present, the main super-resolution (SR) method based on convolutional neural network (CNN) is to increase the layer number of the network by skip connection so as to improve the nonlinear expression ability of the model. However, the network also becomes difficult to be trained and converge. In order to train a smaller but better performance SR model, this paper constructs a novel image SR network of multiple attention mechanism(MAMSR), which includes channel attention mechanism and spatial attention mechanism. By learning the relationship between the channels of the feature map and the relationship between the pixels in each position of the feature map, the network can enhance the ability of feature expression and make the reconstructed image more close to the real image. Experiments on public datasets show that our network surpasses some current state-of-the-art algorithms in PSNR, SSIM, and visual effects.","Super-resolution, CNN, Attention mechanism, Channel attention, Spatial attention",Xin Yang and Xiaochuan Li and Zhiqiang Li and Dake Zhou,https://www.sciencedirect.com/science/article/pii/S1047320321000018,https://doi.org/10.1016/j.jvcir.2021.103019,1047-3203,2021,103019,75,Journal of Visual Communication and Image Representation,Image super-resolution based on deep neural network of multiple attention mechanism,article,YANG2021103019
"In classification tasks, the robustness against various image transformations remains a crucial property of CNN models. When acquired using the data augmentation it comes at the price of a considerable increase in training time and the risk of overfitting. Consequently, researching other ways to endow CNNs with invariance to various transformations is an intensive field of study. This paper presents a new reduced, rotation-invariant, classification model composed of two parts: a feature representation mapping and a classifier. We provide an insight into the principle and we show that the proposed model is trainable. The model we obtain is smaller and has angular prediction capabilities. We illustrate the results on the MNIST-rot and CIFAR-10 datasets. We achieve the state-of-the-art classification score on MNIST-rot, and improve by 20% the state of the art score on rotated CIFAR-10. In all cases, we can predict the rotation angle.","Image Classification, Convolutional neural network, Rotation invariance, Prediction of angle of rotation, Steerable filters",Rosemberg {Rodriguez Salas} and Petr DoklÃ¡dal and Eva Dokladalova,https://www.sciencedirect.com/science/article/pii/S1047320321000250,https://doi.org/10.1016/j.jvcir.2021.103054,1047-3203,2021,103054,75,Journal of Visual Communication and Image Representation,A minimal model for classification of rotated objects with prediction of the angle of rotation,article,RODRIGUEZSALAS2021103054
"Despite the promising progress made in recent years, automatically generating high-resolution realistic images from text descriptions remains a challenging task due to semantic gap between human-written descriptions and diversities of visual appearance. Most existing approaches generate the rough images with the given text descriptions, while the relationship between sentence semantics and visual content is not holistically exploited. In this paper, we propose a novel chained deep recurrent generative adversarial network (CDRGAN) for synthesizing images from text descriptions. Our model uses carefully designed chained deep recurrent generators that simultaneously recovers global image structures and local details. Specially, our method not only considers the logic relationships of image pixels, but also removes computational bottlenecks through parameters sharing. We evaluate our method on three public benchmarks: CUB, Oxford-102 and MS COCO datasets. Experimental results show that our method significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.","Text-to-image synthesis, Logic relationships, Computational bottlenecks, Parameters sharing",Min Wang and Congyan Lang and Songhe Feng and Tao Wang and Yi Jin and Yidong Li,https://www.sciencedirect.com/science/article/pii/S1047320320301838,https://doi.org/10.1016/j.jvcir.2020.102955,1047-3203,2021,102955,74,Journal of Visual Communication and Image Representation,Text to photo-realistic image synthesis via chained deep recurrent generative adversarial network,article,WANG2021102955
"In recent years, the increasing requirements in cloud storage and cloud computing have made it necessary to encrypt digital images for privacy protection. Meanwhile, many reversible data hiding (RDH) algorithms in the encrypted domain have been proposed. However, most of these algorithms are for gray-level images, and the intrinsic cross-channel correlations of color images cannot be utilized to improve the embedding capacity. In this paper, we propose a novel data hiding method for encrypted color images. In the encryption stage, the homomorphic property of encryption is achieved by basic modular addition. During the data hiding process, the cross-channel correlations between R, G and B channels are generated in encrypted domain, and data hiding is performed by the difference histogram shifting. Analysis and experiments demonstrate that the proposed method is secure and the RDH performance is superior.","Reversible data hiding, Image encryption, Cross-channel correlation, Homomorphism",Ming Li and Hua Ren and Yong Xiang and Yushu Zhang,https://www.sciencedirect.com/science/article/pii/S1047320321001061,https://doi.org/10.1016/j.jvcir.2021.103166,1047-3203,2021,103166,78,Journal of Visual Communication and Image Representation,Reversible data hiding in encrypted color images using cross-channel correlations,article,LI2021103166
"Performances of fine-grained recognition have been greatly improved thanks to the fast developments of deep convolutional neural networks (DCNN). DCNN methods often treat each image region equally. Besides, researchers often rely on visual information for classification. To solve these problems, we propose a novel discriminative semantic region selection method for fine-grained recognition (DSRS). We first select a few image regions and then use the pre-trained DCNN models to predict their semantic correlations with corresponding classes. We use both visual and semantic representations to represent image regions. The visual and semantic representations are then linearly combined for joint representation. The combination parameters are determined by considering both semantic distinctiveness and spatial-semantic correlations. We use the joint representations for classifier training. A testing image can be classified by obtaining the visual and semantic representations and encoded for joint representation and classification. Experiments on several publicly available datasets demonstrate the proposed method's superiority.","Fine-grained recognition, Discriminative region selection, Semantic correlation, Object categorization",Chunjie Zhang and Da-Han Wang and Haisheng Li,https://www.sciencedirect.com/science/article/pii/S1047320321000493,https://doi.org/10.1016/j.jvcir.2021.103084,1047-3203,2021,103084,77,Journal of Visual Communication and Image Representation,Discriminative semantic region selection for fine-grained recognition,article,ZHANG2021103084
"A point cloud is a representation of a 3D scene as a discrete collection of geometry plus other attributes such as color, normal, transparency associated with each point. The traditional acquisition process of a 3D point cloud, e.g. using depth information acquired directly by active sensors or indirectly from multi-viewpoint images, suffers from a significant amount of noise. Hence, the problem of point cloud denoising has recently received a lot of attention. However, most existing techniques attempt to denoise only the geometry of each point, based on the geometry information of the neighboring points; there are very few works at all considering the problem of denoising the color attributes of a point cloud. In this paper, we move beyond the state of the art and we propose a novel technique employing graph-based optimization, taking advantage of the correlation between geometry and color, and using it as a powerful tool for several different tasks, i.e. color denoising, geometry denoising, and combined geometry and color denoising. The proposed method is based on the notion that the correct location of a point also depends on the color attribute and not only the geometry of the neighboring points, and the correct color also depends on the geometry of the neighbors. The proposed method constructs a suitable k-NN graph from geometry and color and applies graph-based convex optimization to obtain the denoised point cloud. Extensive simulation results on both real-world and synthetic point clouds show that the proposed denoising technique outperforms state-of-the-art methods using both subjective and objective quality metrics.","Point cloud denoising, Color denoising, Convex optimization, Tikhonov regularization, Total variation, Graph signal processing",Muhammad Abeer Irfan and Enrico Magli,https://www.sciencedirect.com/science/article/pii/S1047320321000092,https://doi.org/10.1016/j.jvcir.2021.103027,1047-3203,2021,103027,75,Journal of Visual Communication and Image Representation,Exploiting color for graph-based 3D point cloud denoising,article,IRFAN2021103027
"Single image deblurring aims to restore the single blurry image to its sharp counterpart and remains an active topic of enduring interest. Recently, deep Convolutional Neural Network (CNN) based methods have achieved promising performance. However, two primary limitations mainly exist on those CNNs-based image deblurring methods: most of them simply focus on increasing the complexity of the network, and rarely make full use of features extracted by encoder. Meanwhile, most of the methods perform the deblurred image reconstruction immediately after the decoder, and the roles of the decoded features are always underestimated. To address these issues, we propose a single image deblurring method, in which two modules to fuse multiple features learned in encoder (the Cross-layer Feature Fusion (CFF) module) and manipulate the features after decoder (the Consecutive Attention Module (CAM)) are specially designed, respectively. The CFF module is to concatenate different layers of features from encoder to enhance rich structural information to decoder, and the CAM module is able to generate more important and correlated textures to the reconstructed sharp image. Besides, the ranking content loss is employed to further restore more realistic details in the deblurred images. Comprehensive experiments demonstrate that our proposed method can generate less blur and more textures in deblurred image on both synthetic datasets and real-world image examples.","Image deblurring, Cross-layer feature fusion, Consecutive attention",Yaowei Li and Ye Luo and Guokai Zhang and Jianwei Lu,https://www.sciencedirect.com/science/article/pii/S1047320321000924,https://doi.org/10.1016/j.jvcir.2021.103149,1047-3203,2021,103149,78,Journal of Visual Communication and Image Representation,Single image deblurring with cross-layer feature fusion and consecutive attention,article,LI2021103149
"Detecting hazardous activity during driving can be useful in curbing roadside accidents. Existing techniques utilizing image based features for encoding such activity can sometimes misclassify crucial scenarios. One particular work by Zhao etÂ al. (2013 [1], 2013 [2], 2011 [3]) suggests an image based feature set that encodes the driverâs pose, which is categorized into one of four activities. We bring more clarity in understanding the activity by proposing a richer, video based feature set that adeptly exploits spatiotemporal information of the driver. Our feature set encodes the driverâs pose, crucial variations in pose and interactions with objects within the vehicle. The feature set is tested on our newly created dataset since the ones used in literature are not publicly available. Our proposed feature set captures a larger number of activities and using standard classifiers and benchmarks it has shown significant improvements over the existing ones.","Driver activity recognition, Feature extraction, Spatiotemporal features, Driver activity recognition dataset",Humza Naveed and Fareed Jafri and Kashif Javed and Haroon Atique Babri,https://www.sciencedirect.com/science/article/pii/S1047320321000808,https://doi.org/10.1016/j.jvcir.2021.103135,1047-3203,2021,103135,77,Journal of Visual Communication and Image Representation,Driver activity recognition by learning spatiotemporal features of pose and human object interaction,article,NAVEED2021103135
"Object tracking based on the Convolutional Neural Networks (CNNs) with multiple feature correlation filter (CF) has become one of the best object tracking frameworks. In this paper, we propose a novel approach of CNNs based CF, which combines deep features from CNNs into low-dimensional features. To achieve the dimensionality reduction, random-projection is used due to its data-independence and superior computational efficiency over other widely used. In our proposed approach, the spectral graph theory is applied to generate a random projection matrix. This method bypasses the time-consuming GramâSchmidt orthogonalization, where the dimension of the feature is high. The combined features have very low dimensions, less than one tenth of the dimensions of the original deep features from CNNs, offering an improvement of tracking speed and without loss of performance simultaneously. Extensive experiments are conducted on large-scale benchmark datasets. The results demonstrate that the proposed algorithm outperforms the state-of-the-art methods.","Object tracking, Correlation filter, Deep features, Random-projection",Mingke Zhang and Long Xu and Jing Xiong and Xuande Zhang,https://www.sciencedirect.com/science/article/pii/S104732032100047X,https://doi.org/10.1016/j.jvcir.2021.103082,1047-3203,2021,103082,77,Journal of Visual Communication and Image Representation,Correlation filter via random-projection based CNNs features combination for visual tracking,article,ZHANG2021103082
"Providing adequate Quality of Experience (QoE) to end-users is crucial for streaming service providers. In this paper, in order to realize automatic quality assessment, a No-Reference (NR) bitstream Human-Vision-System-(HVS)-based video quality assessment (VQA) model is proposed. Inspired by discoveries from the neuroscience community, which suggest there is a considerable overlap between active areas of the brain when engaging in video quality assessment and saliency detection tasks, saliency maps are used in the proposed method to improve the quality assessment accuracy. To this end, saliency maps are first generated from features extracted from the HEVC bitstream. Then, saliency map statistics are employed to create a model of visual memory. Finally, a support vector regression pipeline learns an estimate of the video quality from the visual memory, saliency, and frame features. Evaluations on SJTU dataset indicate that the proposed bitstream based no-reference video quality assessment algorithm achieves a competitive performance.","No-Reference Video Quality, Visual Memory, Saliency Detection, Bitstream, HEVC",Mehdi Banitalebi-Dehkordi and Abbas Ebrahimi-Moghadam and Morteza Khademi and Hadi Hadizadeh,https://www.sciencedirect.com/science/article/pii/S1047320320302236,https://doi.org/10.1016/j.jvcir.2020.103011,1047-3203,2021,103011,75,Journal of Visual Communication and Image Representation,No-reference quality assessment of HEVC video streams based on visual memory modelling,article,BANITALEBIDEHKORDI2021103011
"Detecting the object with external occlusion has always been a hot topic in computer version, while its accuracy is always limited due to the loss of original object information and increase of new occlusion noise. In this paper, we propose a occluded object detection algorithm named GC-FRCN (Generative feature completing Faster RCNN), which consists of the OSGM (Occlusion Sample Generation Module) and OSIM (Occlusion Sample Inpainting Module). Specifically, the OSGM mines and discards the feature points with high category response on the feature map to enhance the richness of occlusion scenes in the training data set. OSIM learns an implicit mapping relationship from occluded feature map to real feature map adversarially, which aims at improving feature quality by repair the noisy object feature. Extensive experiments and ablation studies have been conducted on four different datasets. All the experiments demonstrate the GC-FRCN can effectively detect objects with local external occlusion and has good robustness for occlusion at different scales.","Occlusion, Object detection, Feature completing, Generative adversarial networks",Can Xu and Wenxi Lang and Rui Xin and Kaichen Mao and Haiyan Jiang,https://www.sciencedirect.com/science/article/pii/S1047320321001218,https://doi.org/10.1016/j.jvcir.2021.103189,1047-3203,2021,103189,78,Journal of Visual Communication and Image Representation,Generative detect for occlusion object based on occlusion generation and feature completing,article,XU2021103189
"Natural image prior is one of the most efficient ways to represent images for computer vision tasks. In the literature, filter response statistics prior and synthesis-based sparse representation are two dominant prior models, which have been investigated separately and our knowledge of the relation between these two methods remains limited. In this paper, we examine the inherent relationship between the Fields of Experts (FoE) and K-SVD methods in the pursuit of natural image priors. We theoretically analyze and show that these two prior models have a mutually complementary relationship in the pursuit of the structure of natural images space. Based on these findings, a novel joint statistical prior is proposed, in which adaptive filters are obtained by exploring clues from both priors and utilized to characterize the subtle structure of natural images subspace. Qualitative and quantitative experiments demonstrate that the proposed method achieves a more comprehensive and reliable estimation of natural image prior and is competitive to both alternative and state-of-the-art methods.","FoE, K-SVD, Adaptive filters, Joint statistical prior, Nature image priors",Feng Jiang and ZhiYuan Chen and Amril Nazir and WuZhen Shi and WeiXiang Lim and ShaoHui Liu and SeungMin Rho,https://www.sciencedirect.com/science/article/pii/S1047320321000882,https://doi.org/10.1016/j.jvcir.2021.103142,1047-3203,2021,103142,78,Journal of Visual Communication and Image Representation,Combining Fields of Experts (FoE) and K-SVD methods in pursuing natural image priors,article,JIANG2021103142
"Copy-move forgery is one of the most common image tampering schemes, with the potential use for misleading the opinion of the general public. Keypoint-based detection methods exhibit remarkable performance in terms of computational cost and robustness. However, these methods are difficult to effectively deal with the cases when 1) forgery only involves small or smooth regions, 2) multiple clones are conducted or 3) duplicated regions undergo geometric transformations or signal corruptions. To overcome such limitations, we propose a fast and accurate copy-move forgery detection algorithm, based on complex-valued invariant features. First, dense and uniform keypoints are extracted from the whole image, even in small and smooth regions. Then, these keypoints are represented by robust and discriminative moment invariants, where a novel fast algorithm is designed especially for the computation of dense keypoint features. Next, an effective magnitude-phase hierarchical matching strategy is proposed for fast matching a massive number of keypoints while maintaining the accuracy. Finally, a reliable post-processing algorithm is developed, which can simultaneously reduce false negative rate and false positive rate. Extensive experimental results demonstrate the superior performance of our proposed scheme compared with existing state-of-the-art algorithms, with average pixel-level F-measure of 94.54% and average CPU-time of 36.25Â s on four publicly available datasets.","Copy-move forgery detection, Complex-valued moment invariants, Magnitude-phase hierarchical matching, Adaptive clustering, Gaussian weighted similarity measure, Two-stage false matches filtering",P. Niu and C. Wang and W. Chen and H. Yang and X. Wang,https://www.sciencedirect.com/science/article/pii/S1047320321000365,https://doi.org/10.1016/j.jvcir.2021.103068,1047-3203,2021,103068,77,Journal of Visual Communication and Image Representation,Fast and effective Keypoint-based image copy-move forgery detection using complex-valued moment invariants,article,NIU2021103068
"A usual problem encountered during bad weather conditions is the degraded image quality due to haze/fog. In basic Gamma correction method there is always an uncertainty regarding the choice of a particular exponential factor, which improves the quality of the input image because of the nonlinearity involved in the process. This issue has been solved in this study by proposing a modified Gamma correction method, in which the exponential correction factor is varied incrementally to generate images. We also propose the implementation of an automatic image selection criterion for fusion which helps chose images with varied and distinct features. The implementation of the multi-exposure fusion framework is done in the hue-saturation-value color space which has close resemblance with the human vision. The intensity channel of the selected images is fused in the gradient domain which captures minute details and takes an edge as compared to other conventional fusion based methods. The fused saturation channel is obtained by averaging fusion followed by enhancement using a non-linear sigmoid function. The hue channel of the input hazy image is left unprocessed to avoid color distortion. The experimental analysis demonstrates that the proposed method outperforms most of the single image dehazing methods.","Image dehazing, Gamma correction model, Multi-exposure fusion",Avishek Kumar and Rajib Kumar Jha and Naveen K. Nishchal,https://www.sciencedirect.com/science/article/pii/S1047320321000766,https://doi.org/10.1016/j.jvcir.2021.103122,1047-3203,2021,103122,78,Journal of Visual Communication and Image Representation,An improved Gamma correction model for image dehazing in a multi-exposure fusion framework,article,KUMAR2021103122
"With the growing popularity of biometrics technology in the pattern recognition field, especiallyidentification of human has gained the attention of researchers from both academia and industry. One such type of biometric technique is Gait recognition, which is used to identify a human being based on their walking style. Generally, two types of approaches are adopted by any algorithm designed for gait recognition, namely model based and model free approaches. The key reason behind the popularity of gait recognition is that it can identify a person from a considerable distance while other biometrics has failed to do so. In this paper, the authors have conducted a survey of extant studies on gait recognition in consideration of gait recognition approaches and phases of a gait cycle. Moreover, some aspects like floor sensors, accelerometer based recognition, the influences of environmental factors, which are ignored by exiting surveys, are also covered in our survey study. The information of gait is usually obtained from different parts of silhouettes. This paper also describes different benchmark datasets for gait recognition. This study will provide firsthand knowledge to the researchers working on the gait recognition domain in any real-world field. It has been observed that work done on the gait recognition with sufficiently high accuracy is limited in comparison to research on various other biometric recognition systems and has enough potential for future research.","Gait recognition, Surveillance, Biometric, Person identification",Munish Kumar and Navdeep Singh and Ravinder Kumar and Shubham Goel and Krishan Kumar,https://www.sciencedirect.com/science/article/pii/S1047320321000249,https://doi.org/10.1016/j.jvcir.2021.103052,1047-3203,2021,103052,75,Journal of Visual Communication and Image Representation,Gait recognition based on vision systems: A systematic survey,article,KUMAR2021103052
"Hazy or foggy weather conditions significantly degrade the visual quality of an image in an outdoor environment. It also changes the color and reduces the contrast of an image. This paper introduces a novel single image dehazing technique to restore a hazy image without considering the physical model of haze formation. In order to find haze-free image, the proposed method does not require the transmission map and its costly refinement process. Since haze effect is dependent on the depth, it severely degrades the visibility of the objects located at a far distance. The objects close to the camera are unaffected. In this paper, we propose a fusion-based haze removal method based on the joint cumulative distribution function (JCDF) that treats faraway haze and nearby haze separately. The output images after the JCDF module, fused in the gradient domain to produce a haze-free image. The proposed method not only significantly enhances visibility but also preserves texture details. The proposed method is experimented and evaluated on a large set of challenging hazy images (large scene depth, night time, dense fog, etc.). Both qualitative and quantitative measures show that the performance of the proposed method is better than the state-of-the-art dehazing techniques.","Image dehazing, Joint cumulative distribution, Nearby haze, Faraway haze, Image fusion, Gradient domain",Subhash Chand Agrawal and Anand Singh Jalal,https://www.sciencedirect.com/science/article/pii/S1047320321000523,https://doi.org/10.1016/j.jvcir.2021.103087,1047-3203,2021,103087,77,Journal of Visual Communication and Image Representation,A joint cumulative distribution function and gradient fusion based method for dehazing of long shot hazy images,article,AGRAWAL2021103087
"In person re-identification (Re-ID) task, multi-branch networks acquire better performance by combining global features and local features. Obviously, local branch can obtain detailed information of person pictures but may work on invalid regions when person pictures have imprecise bounding boxes. On the contrary, global branch can be aware of the position of person but hard to acquire detailed information of person pictures. Meanwhile, lots of multi-branch networks ignore mutual information among different branches. Therefore, it is necessary to enhance interaction of global branch and local branch. For this purpose, we propose Interactive Information Module (IIM). IIM includes two components named Global-map Attention Module (GAM) and Labeled-class Mutual Learning (LML), respectively. GAM leverages heatmaps generated by global branch to guide calculation of local attention and obtains a composite global feature by combining local features. GAM relys more on the performance of global branch which decides the quality of heatmaps. To improve performance of global branch, we propose LML to promote convergent rate of global branch. Extensive experiments implemented on Market-1501, DukeMTMC-ReID, and CUHK03-NP datasets confirm that our method achieves state-of-the-art results.","Person re-identification, Interactive Information Module, Global-map Attention Module, Labeled-class Mutual Learning",Xudong Liu and Jun Kong and Min Jiang and Sha Li,https://www.sciencedirect.com/science/article/pii/S1047320321000122,https://doi.org/10.1016/j.jvcir.2021.103033,1047-3203,2021,103033,75,Journal of Visual Communication and Image Representation,Interactive information module for person re-identification,article,LIU2021103033
"Being captured by amateur photographers, reciprocally propagated through multimedia pipelines, and compressed with different levels, real-world images usually suffer from a wide variety of hybrid distortions. Faced with this scenario, full-reference (FR) image quality assessment (IQA) algorithms can not deliver promising predictions due to the inferior references. Meanwhile, existing no-reference (NR) IQA algorithms remain limited in their efficacy to deal with different distortion types. To address this obstacle, we explore a NR-IQA metric by predicting the perceptual quality of distorted-then-compressed images using a deep neural network (DNN). First, we propose a novel two-stream DNN to handle both authentic distortions and synthetic compressions and adopt effective strategies to pre-train the two branches of the network. Specifically, we transfer the knowledge learned from in-the-wild images to account for authentic distortions by utilizing a pre-trained deep convolutional neural network (CNN) to provide meaningful initializations. Meanwhile, we build a CNN for synthetic compressions and pre-train it on a dataset including synthetic compressed images. Subsequently, we bilinearly pool these two sets of features as the image representation. The overall network is fine-tuned on an elaborately-designed auxiliary dataset, which is annotated by a reliable objective quality metric. Furthermore, we integrate the output of the authentic-distortion-aware branch with that of the overall network following a two-step prediction manner to boost the prediction performance, which can be applied in the distorted-then-compressed scenario when the reference image is available. Extensive experimental results on several databases especially on the LIVE Wild Compressed Picture Quality Database show that the proposed method achieves state-of-the-art performance with good generalizability and moderate computational complexity.","Image quality assessment, Convolutional neural network, Full-reference IQA, No-reference IQA, Distorted-then-compressed",Bowen Li and Meng Tian and Weixia Zhang and Hongtai Yao and Xianpei Wang,https://www.sciencedirect.com/science/article/pii/S1047320320302194,https://doi.org/10.1016/j.jvcir.2020.103004,1047-3203,2021,103004,76,Journal of Visual Communication and Image Representation,Learning to predict the quality of distorted-then-compressed images via a deep neural network,article,LI2021103004
"The depth image-based rendering paves the path to success of 3-D video. However, one issue still remained in 3-D video is how to fill the disocclusion areas. To this end, Gaussian mixture model (GMM) is commonly employed to generate the background, and then to fill the holes. Nevertheless, GMM usually has poor performance for sequences with big foreground reciprocation. In this paper, we aim to enhance the synthesis performance. Firstly, we propose an expectation maximization based GMM background generation method, in which the pixel mixture distribution is derived. Secondly, we propose a refined foreground depth correlation approach, which recovers the background frame-by-frame based on depth information. Finally, we adaptively choose the background pixels from these two methods for filling. Experimental results show that the proposed method outperforms existing non-deep learning based hole filling methods by around 1.1Â dB, and significantly surpasses deep learning based alternative in terms of subjective quality.","Depth-image-based-rendering, Gaussian mixture model, Expectation maximization, Foreground depth correlation, Adaptive hole-filling",Pan Gao and Tiantian Zhu and Manoranjan Paul,https://www.sciencedirect.com/science/article/pii/S1047320321000912,https://doi.org/10.1016/j.jvcir.2021.103148,1047-3203,2021,103148,78,Journal of Visual Communication and Image Representation,Disocclusion filling for depth-based view synthesis with adaptive utilization of temporal correlations,article,GAO2021103148
"Bokeh effect is used in photography to capture images where the closer objects look sharp and everything else stays out-of-focus. Bokeh photos are generally captured using Single Lens Reflex cameras using shallow depth-of-field. Most of the modern smartphones can take bokeh images by leveraging dual rear cameras or a good auto-focus hardware. However, for smartphones with single-rear camera without a good auto-focus hardware, we have to rely on software to generate bokeh images. This kind of system is also useful to generate bokeh effect in already captured images. In this paper, an end-to-end deep learning framework is proposed to generate high-quality bokeh effect from images. The original image and different versions of smoothed images are blended to generate Bokeh effect with the help of a monocular depth estimation network. The model is trained through three phases to generate visually pleasing bokeh effect. The proposed approach is compared against a saliency detection based baseline and a number of approaches proposed in AIM 2019 Challenge on Bokeh Effect Synthesis. Extensive experiments are shown in order to understand different parts of the proposed algorithm. The network is lightweight and can process an HD image in 0.03Â s. This approach ranked second in AIM 2019 Bokeh effect challenge-Perceptual Track.","Bokeh effect, Blur kernel, AIM 2019, Deep learning",Saikat Dutta,https://www.sciencedirect.com/science/article/pii/S1047320321000511,https://doi.org/10.1016/j.jvcir.2021.103089,1047-3203,2021,103089,77,Journal of Visual Communication and Image Representation,Depth-aware blending of smoothed images for Bokeh effect generation,article,DUTTA2021103089
"Many deep neural networks are built by using stacked convolutional layers of fixed and single size (often 3Â ÃÂ 3) kernels. This paper describes a method for learning the size of convolutional kernels to provide varying size kernels in a single layer. The method utilizes a differentiable, and therefore backpropagation-trainable Gaussian envelope which can grow or shrink in a base grid. Our experiments compared the proposed adaptive layers to ordinary convolution layers in a simple two-layer network, a deeper residual network, and a U-Net architecture. The results in the popular image classification datasets such as MNIST, MNIST-CLUTTERED, CIFAR-10, Fashion, and âFaces in the Wildâ showed that the adaptive kernels can provide statistically significant improvements on ordinary convolution kernels. A segmentation experiment in the Oxford-Pets dataset demonstrated that replacing ordinary convolution layers in a U-shaped network with 7Â ÃÂ 7 adaptive layers can improve its learning performance and ability to generalize.","Adaptive convolution, Multi-scale convolution, Image classification, Residual networks",F. Boray Tek and Ä°lker Ãam and Deniz KarlÄ±,https://www.sciencedirect.com/science/article/pii/S104732032030225X,https://doi.org/10.1016/j.jvcir.2020.103015,1047-3203,2021,103015,75,Journal of Visual Communication and Image Representation,Adaptive convolution kernel for artificial neural networks,article,TEK2021103015
"Due to the lighting, translation, scaling and rotation, image matching is a challenge task in computer vision area. In the past decades, local descriptors (e.g. SIFT, SURF and HOG, etc.) and global features (e.g. HSV, CNN, etc.) play a vital role for this task. However, most image matching methods are based on the whole image, i.e., matching the entire image directly base on some image representation methods (e.g. BoW, VLAD and deep learning, etc.). In most situations, this idea is simple and effective, but we recognize that a robust image matching can be realized based on sub-images. Thus, a block-based image matching algorithm is proposed in this paper. First, a new local composite descriptor is proposed, which combines the advantages of local gradient and color features with spatial information. Then, VLAD method is used to encode the proposed composite descriptors in one block, and block-CNN feature is extracted at the same time. Second, a block-based similarity metric is proposed for similarity calculation of two images. Finally, the proposed methods are verified on several benchmark datasets. Compared with other methods, experimental results show that our method achieves better performance.","Composite descriptors, Block index, Vector of locally aggregated descriptors, Image matching, Feature fusion, Deep feature, Image retrieval",Yanhong Wang and Ruizhen Zhao and Liequan Liang and Xinwei Zheng and Yigang Cen and Shichao Kan,https://www.sciencedirect.com/science/article/pii/S1047320320302145,https://doi.org/10.1016/j.jvcir.2020.102998,1047-3203,2021,102998,74,Journal of Visual Communication and Image Representation,Block-based image matching for image retrieval,article,WANG2021102998
"The two-stream convolutional network has been proved to be one milestone in the study of video-based action recognition. Lots of recent works modify internal structure of two-stream convolutional network directly and put top-level features into a 2D/3D convolution fusion module or a simpler one. However, these fusion methods cannot fully utilize features and the way fusing only top-level features lacks rich vital details. To tackle these issues, a novel network called Diverse Features Fusion Network (DFFN) is proposed. The fusion stream of DFFN contains two types of uniquely designed modules, the diverse compact bilinear fusion (DCBF) module and the channel-spatial attention (CSA) module, to distill and refine diverse compact spatiotemporal features. The DCBF modules use the diverse compact bilinear algorithm to fuse features extracted from multiple layers of the base network that are called diverse features in this paper. Further, the CSA module leverages channel attention and multi-size spatial attention to boost key information as well as restraining the noise of fusion features. We evaluate our three-stream network DFFN on three public challenging video action benchmarks: UCF101, HMDB51 and Something-Something V1. Experiment results indicate that our method achieves state-of-the-art performance.","Three-stream action recognition, Diverse features fusion, DIverse Compact Bilinear, Channel-spatial Attention",Haoyang Deng and Jun Kong and Min Jiang and Tianshan Liu,https://www.sciencedirect.com/science/article/pii/S1047320321000754,https://doi.org/10.1016/j.jvcir.2021.103121,1047-3203,2021,103121,77,Journal of Visual Communication and Image Representation,Diverse Features Fusion Network for video-based action recognition,article,DENG2021103121
"With the deepening of social information, the panoramic image has drawn a significant interest of viewers and researchers as it can provide a very wide field of view (FoV). Since panoramic images are usually obtained by capturing images with the overlapping regions and then stitching them together, image stitching plays an important role in generating panoramic images. In order to effectively evaluate the quality of stitched images, a novel quality assessment method based on bi-directional matching is proposed for stitched images. Specifically, dense correspondences between the testing and benchmark stitched images are first established by bi-directional SIFT-flow matching. Then, color-aware, geometric-aware and structure-aware features are respectively extracted and fused via support vector regression (SVR) to obtain the final quality score. Experiments on our newly constructed database and ISIQA database demonstrate that the proposed method can achieve comparable performance compared with the conventional blind quality metrics and the quality metrics specially designed for stitched images.","Image quality assessment, Stitched image, Bi-directional matching, Color correction, Color aberration",Xuejin Wang and Xiongli Chai and Feng Shao,https://www.sciencedirect.com/science/article/pii/S1047320321000237,https://doi.org/10.1016/j.jvcir.2021.103051,1047-3203,2021,103051,75,Journal of Visual Communication and Image Representation,Quality assessment for color correction-based stitched images via bi-directional matching,article,WANG2021103051
"In this paper, we explore the inherent geometry of video tensors by modeling them as points in product of Riemannian matrix manifolds. A video tensor is decomposed into three modes (factors) using matrix unfolding operation and each mode is represented as a point in a product space of Grassmannian and symmetric positive definite (SPD) matrix manifold. Hence a video is represented as a point in the Cartesian product of three such product spaces. Being a manifold valued (non-Euclidean) representation, application of several state-of-the-art Euclidean machine learning algorithms lead to inferior results. To overcome this, we propose positive definite kernels which map the points from product manifold space to Hilbert space. The proposed kernel functions implicitly make use of geodesic distance on product manifold to obtain a similarity measure and generate a kernel-gram matrix. In addition, we generate a discriminative feature representation for each manifold valued point using kernel-gram matrix diagonalization. Classification is performed in a sparse framework. The proposed methodology is tested over three publicly available datasets for hand gesture, traffic signal and sign language recognition. Experimentation performed over these datasets show that the proposed methodology is powerful in terms of classification accuracy in comparison with the state-of-the-art methods.","Video tensor, Product manifold geometry, Sparse representation, Grassmann manifold, SPD manifold, Riemannian manifold, Kernel methods",Krishan Sharma and Renu Rameshan,https://www.sciencedirect.com/science/article/pii/S1047320321000195,https://doi.org/10.1016/j.jvcir.2021.103045,1047-3203,2021,103045,75,Journal of Visual Communication and Image Representation,Distance based kernels for video tensors on product of Riemannian matrix manifolds,article,SHARMA2021103045
"Automatic water body extraction from satellite images of various scenes is a classical and challenging task in remote sensing and image interpretation. Convolutional neural network (CNN) has become prominent option for performing image segmentation task in remote sensing applications. However, CNN-based networks have non-trivial issues for segmenting such as: (1) blurring boundary pixels; (2) large number of trainable parameters; and (3) huge number of training samples. In this paper, we propose an end-to-end multi-feature based CNN architecture, called as W-Net, to perform water body segmentation. W-Net consists of contracting/expanding networks and inception layers. W-Net takes advantage of contracting network to capture context information while localization is achieved with expanding network. With these networks, W-Net is able to train on less number of images and extract water pixels accurately. Use of inception layers reduces computational burden within the network by decreasing total number of trainable parameters. W-Net incorporated two refinement modules to enhance predicted results which mitigate blurring effect and to inspect continuity of boundary pixels. Dataset consisting 2671 images with manually annotated ground truths are built to validate performance and effectiveness of our proposed method. In addition, we evaluated our method on crack detection dataset where W-Net achieved competitive performance with Deepcrack. W-Net accomplished excellent performance on the water body dataset (IâU=0.9434 and Fâscore=0.9509).","Convolutional neural network, Deep learning, Refinement modules, Satellite image analysis, Water body extraction",Rishikesh G. Tambe and Sanjay N. Talbar and Satishkumar S. Chavan,https://www.sciencedirect.com/science/article/pii/S1047320321000870,https://doi.org/10.1016/j.jvcir.2021.103141,1047-3203,2021,103141,77,Journal of Visual Communication and Image Representation,Deep multi-feature learning architecture for water body segmentation from satellite images,article,TAMBE2021103141
"An image can be annotated from the local perspective, based on objects visually present. An image can also be annotated from the global perspective, based on implicit emotion or meanings derived from it. We propose three points relatively little studied before. First, semantics remain the same even if the image is manipulated by some geometric processes. Second, object correlation is important in image labelling. We propose to use a standard recurrent neural network to take object sequences in random orders. Third, we observe that some entity can be represented by multiple image samples, and multiple samples can be jointly considered to improve recognition performance. These three points are implemented in a network that jointly considers global and local information. With comprehensive evaluation studies, we verify that a simple network with these points is effective and is able to achieve competitive performance compared to the state of the arts.","Multi-label image recognition, Object correlation, Semantics consistency, Multiple samples",Wei-Ta Chu and Si-Heng Huang,https://www.sciencedirect.com/science/article/pii/S1047320321000353,https://doi.org/10.1016/j.jvcir.2021.103067,1047-3203,2021,103067,77,Journal of Visual Communication and Image Representation,"Multi-label image recognition by using semantics consistency, object correlation, and multiple samples",article,CHU2021103067
"The small, moderate, and large scale saliency patterns in images are valuable to be extracted in saliency detection. By the observation that the probability of small and large saliency patterns appearing in datasets is lower than that of moderate scale saliency patterns. As results, a deep saliency model trained on such datasets would converge to moderate scale saliency patterns, and it is hard to well infer the small and large scale saliency patterns because they are not encoded efficiently in the model for their low probability. Thus a novel but simple saliency detection method using cross-scale deep inference is presented in this paper. Moreover, a new network architecture, in which the attention mechanism is exploited by multiple layers, is proposed to improve the receptive fields of various scale saliency patterns in different scale images. The presented cross-scale deep inference could improve the representation power of small and large scale saliency patterns encoded in multiple scale images efficiently. The quantitative and qualitative evaluation demonstrates our deep model achieves a promising results across a wide of metrics.","Cross-scale deep inference, Multi-layer attention, Image saliency, Deep learning",Dakai Ren and Xiangming Wen and Tao Jia and Jiazhong Chen and Zongyi Li,https://www.sciencedirect.com/science/article/pii/S1047320321000110,https://doi.org/10.1016/j.jvcir.2021.103031,1047-3203,2021,103031,75,Journal of Visual Communication and Image Representation,Saliency detection via cross-scale deep inference,article,REN2021103031
"Facial expression recognition (FER) is a popular research field in cognitive interaction systems and artificial intelligence. Many deep learning methods achieve outstanding performances at the expense of enormous computation workload. Limiting their application in small devices or offline scenarios. To cope with this drawback, this paper proposes the Frequency Multiplication Network (FMN), a deep learning method operating in the frequency domain that significantly reduces network capacity and computation workload. By taking advantage of the frequency domain conversion, this novel deep learning method utilizes multiplication layers for effective feature extraction. In conjunction with the Uniform Rectangular Features (URF), our method further improves the performance and reduces the training effort. On three publicly available datasets (CK+, Oulu, and MMI), our method achieves substantial improvements in comparison to popular approaches.","Facial expression recognition, Uniform rectangular features, Frequency multiplication network",Jinzhao Zhou and Xingming Zhang and Yubei Lin and Yang Liu,https://www.sciencedirect.com/science/article/pii/S1047320320302261,https://doi.org/10.1016/j.jvcir.2020.103018,1047-3203,2021,103018,75,Journal of Visual Communication and Image Representation,Facial expression recognition using frequency multiplication network with uniform rectangular features,article,ZHOU2021103018
"Although attention mechanisms are exploited widely in encoder-decoder neural network-based image captioning framework, the relation between the selection of salient image regions and the supervision of spatial information on local and global representation learning was overlooked, thereby degrading captioning performance. Consequently, we propose an image captioning scheme based on adaptive spatial information attention (ASIA), extracting a sequence of spatial information of salient objects in a local image region or an entire image. Specifically, in the encoding stage, we extract the object-level visual features of salient objects and their spatial bounding-box. We obtain the global feature maps of an entire image, which are fused with local features and the fused features are fed into the LSTM-based language decoder. In the decoding stage, our adaptive attention mechanism dynamically selects the corresponding image regions specified by an image description. Extensive experiments conducted on two datasets demonstrate the effectiveness of the proposed method.","Image captioning, Encoder-decoder, Spatial information, Adaptive attention",Xian Zhong and Guozhang Nie and Wenxin Huang and Wenxuan Liu and Bo Ma and Chia-Wen Lin,https://www.sciencedirect.com/science/article/pii/S1047320321000869,https://doi.org/10.1016/j.jvcir.2021.103138,1047-3203,2021,103138,78,Journal of Visual Communication and Image Representation,Attention-guided image captioning with adaptive global and local feature fusion,article,ZHONG2021103138
"Automated person re-identification in a multi-camera surveillance setup is very important for effective tracking and monitoring crowd movement. In this paper, we propose an efficient hierarchical re-identification approach in which color histogram-based comparison is employed to find the closest matches in the gallery set, and next deep feature-based comparison is carried out using the Siamese network. Reduction in search space after the first level of matching helps in improving the accuracy as well as efficiency of prediction by the Siamese network by eliminating dissimilar elements. A silhouette part-based feature extraction scheme is adopted in each level of hierarchy to preserve the relative locations of the different body parts and make the appearance descriptors more discriminating. The proposed approach has been evaluated on five public data sets and also a new data set captured in our laboratory. Results reveal that it outperforms most state-of-the-art approaches in terms of overall accuracy.","Hierarchical re-identification approach, Color-based clustering, Silhouette part-based analysis, Siamese Convolution Box, IIT(BHU) re-identification data set",Nirbhay Kumar Tagore and Ayushman Singh and Sumanth Manche and Pratik Chattopadhyay,https://www.sciencedirect.com/science/article/pii/S1047320321000109,https://doi.org/10.1016/j.jvcir.2021.103029,1047-3203,2021,103029,75,Journal of Visual Communication and Image Representation,Person re-identification from appearance cues and deep Siamese features,article,TAGORE2021103029
"With the boom of artificial intelligence, facial manipulation technology is becoming more simple and more numerous. At the same time, the technology also has a large and profound negative impact on face forensics, such as Deepfakes. In this paper, in order to aggregate multiframe features to detect facial manipulation videos, we solve facial manipulated video detection from set perspective and propose a novel framework based on set, which is called set convolutional neural network (SCNN). Three instances of the proposed framework SCNN are implemented and evaluated on the Deepfake TIMIT dataset, FaceForensics++ dataset and DFDC Preview datset. The results show that the method outperforms previous methods and can achieve state-of-the-art performance on both datasets. As a perspective, the proposed method is a fusion promotion of single-frame digital video forensics network.","Digital video forensics, Deepfake, Set convolutional neural network, Set reduce",Zhaopeng Xu and Jiarui Liu and Wei Lu and Bozhi Xu and Xianfeng Zhao and Bin Li and Jiwu Huang,https://www.sciencedirect.com/science/article/pii/S1047320321000742,https://doi.org/10.1016/j.jvcir.2021.103119,1047-3203,2021,103119,77,Journal of Visual Communication and Image Representation,Detecting facial manipulated videos based on set convolutional neural networks,article,XU2021103119
"A new method for encoding a sequence of integers, named Binary Adaptive Sequential Coding with Return to Bias, is proposed in this paper. It extends the compressing pipeline for chain codesâ compression consisting of Burrows Wheeler Transform, Move-To-Front Transform, and Adaptive Arithmetic Coding. We also explain when to include the Zero-Run Transform into the above-mentioned pipeline. The Zero-Run Transform generates a sequence of integers corresponding to the number of zero-runs. This sequence is encoded by Golomb coding, Binary Adaptive Sequential Coding, and the new Binary Adaptive Sequential Coding with Return to Bias. Finally, a comparison is performed with the two state-of-the-art methods. The proposed method achieved similar compression efficiency for the Freeman chain code in eight directions. However, for the chain codes with shorter alphabets (Freeman chain code in four directions, Vertex Chain Code, and Three-OrThogonal chain code), the introduced method outperforms the referenced ones.","Lossless data compression, Run-Length Encoding, Integer coding, Golomb coding",Borut Å½alik and Domen Mongus and Krista Rizman Å½alik and David Podgorelec and Niko LukaÄ,https://www.sciencedirect.com/science/article/pii/S1047320321000225,https://doi.org/10.1016/j.jvcir.2021.103050,1047-3203,2021,103050,75,Journal of Visual Communication and Image Representation,Lossless chain code compression with an improved Binary Adaptive Sequential Coding of zero-runs,article,ZALIK2021103050
"We propose a novel subpixel rendering algorithm for diamond-shaped PenTile displays, which reduces color distortions while improving apparent resolutions. We develop two types of subpixel rendering filters: main filter and color distortion reduction (CDR) filters. To derive the filters, we formulate a quadratic program to minimize the difference between an original input image and a virtual image that the human visual system perceives. By imposing two constraints for filter size and coefficients, we obtain the main filter, which has a suitable size and is normalized. Then, we design the CDR filters based on the analysis of various patch patterns for image areas. We define the patch patterns to classify local areas with possible color distortions. By imposing additional constraints according to the patch patterns, we derive the CDR filters. Lastly, by matching local areas in the input image into the pre-defined patch patterns, we render the image using the main filter and the CDR filters, which are applied adaptively to the local areas. Experimental results demonstrate that the proposed subpixel rendering algorithm improves apparent resolutions and suppresses color distortions effectively, thereby outperforming conventional algorithms.","Subpixel rendering, Diamond-shaped PenTile displays, Color distortion reduction",Jae-Han Lee and Kyung-Rae Kim and Chang-Su Kim,https://www.sciencedirect.com/science/article/pii/S1047320321000900,https://doi.org/10.1016/j.jvcir.2021.103144,1047-3203,2021,103144,78,Journal of Visual Communication and Image Representation,Subpixel rendering for diamond-shaped PenTile displays using patch-based adaptive filters,article,LEE2021103144
"This paper proposes a multi-image-based reversible data hiding method using a rhombus magic matrix. It takes a pixel pair as a position coordinate of a 256Ã256 modulus function matrix and extracts a 5-order rhombus matrix. It first embeds a 5-ary secret digit by producing three shadow pixel pairs which satisfies the predefined distance condition. Then it embeds a 6-ary secret digit by permuting the three shadow pixel pairs and assigning them to three ordered shadow images. Third, it embeds another 5-ary secret digit by modifying the pixel pair within a 3-order rhombus magic matrix in a shadow image. The receiver can extract the secret data and recover the original cover image when obtaining all shadow images. It also introduces a new application scenario for hierarchical security data transmission. The experimental results and analysis show that the proposed security scheme provides high embedding capacity with good visual quality of shadow images.","Reversible data hiding, Three shadow images, Rhombus magic matrix, Hierarchical data security",Sisheng Chen and Chin-Chen Chang,https://www.sciencedirect.com/science/article/pii/S104732032100033X,https://doi.org/10.1016/j.jvcir.2021.103064,1047-3203,2021,103064,76,Journal of Visual Communication and Image Representation,Reversible data hiding based on three shadow images using rhombus magic matrix,article,CHEN2021103064
"For synchronously combining the dynamic semantic and visual information in the decoder part of image captioning, we propose a novel parallel-fusion LSTM (pLSTM) structure in this paper. Two parallel LSTMs with attributes and visual information of image are fused by the hidden states at every time step, which makes the attributes and visual information complementary or enhanced for generating more accurate captions. According to the different ways of integrating semantic information from attribute LSTM to visual LSTM, we propose two models pLSTM with attention (pLSTM-A) and pLSTM with guiding (pLSTM-G). pLSTM-A can automatically capture the crucial semantic and visual information to generate captions, and pLSTM-G directly adjusts the hidden state of visual LSTM by synchronous semantic information to the critical region. For verifying the effectiveness of our proposed pLSTM, we conduct a series of experiments on MSCOCO and Flickr30K datasets, and the experimental results outperform some state-of-the-art image captioning methods.","Image captioning, Parallel-fusion LSTM, Attention mechanism, Guiding LSTM",Jing Zhang and Kangkang Li and Zhe Wang,https://www.sciencedirect.com/science/article/pii/S1047320321000183,https://doi.org/10.1016/j.jvcir.2021.103044,1047-3203,2021,103044,75,Journal of Visual Communication and Image Representation,Parallel-fusion LSTM with synchronous semantic and visual information for image captioning,article,ZHANG2021103044
"As the linear weighted fractional-order Fourier transform (LWFRFT), an extension of the Fourier transform, has been widely studied, many linear weighted fractional-order transforms (LWFRTs) have been proposed consequently. Our research shows that the LWFRT has limitations when applied to image encryption. For example, its application to image encryption leads to the security risks of key invalidation. In this paper, we propose a new reformulation of the LWFRT which establishes the relation between many fractional-order transforms. With the help of the new reformulation, we point out the limitations of the LWFRT and analyze the reasons for key invalidation in image encryption. Finally, numerical simulation verifies our perspective.","Fractional-order Fourier transform, Weighted fractional-order transform, Image encryption",Tieyu Zhao and Lin Yuan and Yingying Chi,https://www.sciencedirect.com/science/article/pii/S1047320321000584,https://doi.org/10.1016/j.jvcir.2021.103098,1047-3203,2021,103098,77,Journal of Visual Communication and Image Representation,Image encryption using linear weighted fractional-order transform,article,ZHAO2021103098
"Air-writing is a new human and smart device communication approach, permits users to write inputs in a natural and relentless way. This touch-less way can prevent users fromvirus infection such as COVID-19. Compared with othermethods, air writing ismore challenging due to its unique characteristics such as redundant lifting strokes, multiplicity (different writing styles from various users), and confusion (different character types written in air are similar). Without the need of any starting trigger, a novel reverse time-ordered algorithm is proposed in this paper toefficiently filter out unnecessary lifting strokes, and thus simplifies the matching procedure. As to the second and third issues, a tiered arrangement structure is proposed by sampling the air-writing results with various sampling rates to solvethe multiplicity and confusion problems. Analyzed with other recently proposed air writing algorithms, the proposed approach reaches satisfactory recognition accuracy (above 94%) without any starting triggers.","Air writing recognition, Backward time-order stroke representation, 3D-sensor, Gesture-based interaction",Tsung-Hsien Tsai and Jun-Wei Hsieh and Chuan-Wang Chang and Chin-Rong Lay and Kuo-Chin Fan,https://www.sciencedirect.com/science/article/pii/S1047320321000341,https://doi.org/10.1016/j.jvcir.2021.103065,1047-3203,2021,103065,78,Journal of Visual Communication and Image Representation,Air-writing recognition using reverse time ordered stroke context,article,TSAI2021103065
"Convolutional neural networks (CNN) have achieved outstanding face recognition (FR) performance with increasing large-scale face datasets. With face dataset size grown, noisy data will inevitably increase, undoubtedly bringing difficulties to data cleaning. In this paper, the probability that the sample belongs to noise can be determined based on the cosine distance (cosÎ¸) of normalized angle center and face feature vector in the margin-based loss functions. According to this finding, we propose a two-step learning method integrated into the loss function. The new proposed directional margin loss function combines the noise probability with the label as the supervision information. Experiments show that our method can tolerate noisy data and get high FR accuracy when the training datasets mix with more than 30% noise. Our approach can also achieve a great result of 79.33% in MegaFace challenge one using a noisy training dataset.","Face recognition, Margin paradigm, Loss function, Noisy labels, Two-step learning",Yang Zhou and Xun Gong and Peng Yang,https://www.sciencedirect.com/science/article/pii/S1047320321001152,https://doi.org/10.1016/j.jvcir.2021.103182,1047-3203,2021,103182,78,Journal of Visual Communication and Image Representation,A directional margin paradigm for noise suppression in face recognition,article,ZHOU2021103182
"With modern e-healthcare developments, ambulatory healthcare has become a prominent requirement for physical or mental ailed, elderly, childhood people. One of the major challenges in such applications is timing and precision. A potential solution to this problem is the fog-assisted cloud computing architecture. The activity recognition task is performed with the hybrid advantages of deep learning and genetic algorithms. The video frames captured from vision cameras are subjected to the genetic change detection algorithm, which detects changes in activities of subsequent frames. Consequently, the deep learning algorithm recognizes the activity of the changed frame. This hybrid algorithm is run on top of fog-assisted cloud framework, fogbus and the performance measures including latency, execution time, arbitration time and jitter are observed. Empirical evaluations of the proposed model against three activity data sets shows that the proposed deep genetic algorithm exhibits higher accuracy in inferring human activities as compared to the state-of-the-art algorithms.","Deep genetic algorithm, Human activity recognition, Fog computing, Ambulatory healthcare",R. Raja Subramanian and V. Vasudevan,https://www.sciencedirect.com/science/article/pii/S1047320321000857,https://doi.org/10.1016/j.jvcir.2021.103132,1047-3203,2021,103132,77,Journal of Visual Communication and Image Representation,A deep genetic algorithm for human activity recognition leveraging fog computing frameworks,article,SUBRAMANIAN2021103132
"In this paper, a reversible data hiding in encrypted images (RDHEI) method combining GCC (group classification encoding) and SIBRW containing sixteen image-based rearrangement ways is proposed to achieve high-capacity data embedding in encrypted images. Each way of SIBRW aims at bringing strongly-correlated bits of each higher bit-plane together by rearranging each higher bit-plane. For each higher bit-plane, the optimal way achieving the most concentrated aggregation performance is selected from SIBRW to rearrange this bit-plane, and then, GCC compresses the rearranged bit-plane in group-by-group manner. By making full use of strong-correlation between adjacent groups, GCC can compress not only consecutive several groups whose bits are valued 1 (or 0) but also a single group so that a large embedding space is provided. The encryption method including the bit-level XOR-encryption and scrambling operations enhances the security. The experimental results show that the proposed scheme can achieve large embedding capacity and high security.","Reversible data hiding, Image encryption, SIBRW, GCC, Image recovery, Embedding performance",Shaowei Weng and Caiying Zhang and Tiancong Zhang and Kaimeng Chen,https://www.sciencedirect.com/science/article/pii/S1047320320301632,https://doi.org/10.1016/j.jvcir.2020.102932,1047-3203,2021,102932,75,Journal of Visual Communication and Image Representation,High capacity reversible data hiding in encrypted images using SIBRW and GCC,article,WENG2021102932
"With the rapid development of deep learning techniques, convolutional neural networks (CNN) have been widely investigated for the feature representations in the image retrieval task. However, the key step in CNN-based retrieval, i.e., feature aggregation has not been solved in a robust and general manner when tackling different kinds of images. In this paper, we present a deep feature aggregation method for image retrieval using the Fourier transform and low-pass filtering, which can adaptively compute the weights for each feature map with discrimination. Specifically, the low-pass filtering can preserve the semantic information in each feature map by transforming images to the frequency domain. In addition, we develop three adaptive methods to further improve the robustness of feature aggregation, i.e., Region of Interests (ROI) selection, spatial weighting and channel weighting. Experimental results demonstrate the superiority of the proposed method in comparison with other state-of-the-art, in achieving robust and accurate object retrieval under five benchmark datasets.","Image retrieval, Convolutional neural networks, Feature aggregation, Fourier transform, Low-pass filtering",Ziyao Zhou and Xinsheng Wang and Chen Li and Ming Zeng and Zhongyu Li,https://www.sciencedirect.com/science/article/pii/S1047320320301115,https://doi.org/10.1016/j.jvcir.2020.102860,1047-3203,2020,102860,72,Journal of Visual Communication and Image Representation,Adaptive deep feature aggregation using Fourier transform and low-pass filtering for robust object retrieval,article,ZHOU2020102860
"Person Re-identification (Re-ID) is an important technique in intelligent video surveillance. Because of the variations on camera viewpoints and body poses, there are some problems such as body misalignment, the diverse background clutters and partial bodies occlusion, etc. To address these problems, we propose the Global-Local Background_bias Net (GLBN), a novel network architecture that consists of Foreground Partial Segmentation Net (FPSN), Global Aligned Supervision Net (GASN) and Background_bias Constraint Net (BCN) modules. Firstly, to enhance the adaptability of foreground features and reduce the interference of the background, FPSN is applied to perform local segmentation on the foreground image. Secondly, global features generated by GASN are purposed to supervise the learning of local features. Finally, BCN constrains the background information to reduce the impact of background information again. Extensive experiments implemented on the mainstream evaluation datasets including Market1501, DukeMTMC-reID and CUHK03 indicate that our method is efficient and robust.","Person Re-identification, Body misalignment, Foreground features, Background information",Yuxiu Gong and Ronggui Wang and Juan Yang and Lixia Xue and Min Hu,https://www.sciencedirect.com/science/article/pii/S1047320320301875,https://doi.org/10.1016/j.jvcir.2020.102961,1047-3203,2021,102961,74,Journal of Visual Communication and Image Representation,Person Re-identification with Global-Local Background_bias Net,article,GONG2021102961
"It is a fundamental task of translating videos into natural language automatically by computer. At present, the models for video description based on deep learning have made a great breakthrough. However, the static information loss is serious during encoding stage for motion feature of videos, and the linguistic feature from LSTM network lack personalized expression, leading to inappropriate words and poor semantics in generation sentences. In this work, a model with enhanced features of visual and language is proposed to address the challenges. First, static features of video frames from the first LSTM layer are incorporated, then fed into another LSTM layer according by frame sequence. Second, the feature of word is combined with the output of LSTM network for predicted probability of candidate word on each time step. The experimental results demonstrate effectiveness of the proposed approach with competitive performance compared with other state-of-the-art methods on various metrics.","Video description, Feature enhancing, CNN, LSTM, Semantic",Pengjie Tang and Yunlan Tan and Jinzhong Li and Bin Tan,https://www.sciencedirect.com/science/article/pii/S1047320320301231,https://doi.org/10.1016/j.jvcir.2020.102875,1047-3203,2020,102875,72,Journal of Visual Communication and Image Representation,Translating video into language by enhancing visual and language representations,article,TANG2020102875
"Extracting accurate foreground objects from a scene is an essential step for many video applications. Traditional background subtraction algorithms can generate coarse estimates, but generating high quality masks requires professional softwares with significant human interventions, e.g., providing trimaps or labeling key frames. We propose an automatic foreground extraction method in applications where a static but imperfect background is available. Examples include filming and surveillance where the background can be captured before the objects enter the scene or after they leave the scene. Our proposed method is very robust and produces significantly better estimates than state-of-the-art background subtraction, video segmentation and alpha matting methods. The key innovation of our method is a novel information fusion technique. The fusion framework allows us to integrate the individual strengths of alpha matting, background subtraction and image denoising to produce an overall better estimate. Such integration is particularly important when handling complex scenes with imperfect background. We show how the framework is developed, and how the individual components are built. Extensive experiments and ablation studies are conducted to evaluate the proposed method.","Foreground extraction1, Alpha matting, Background subtraction, Video segmentation, Saliency detection, Plug-and-play ADMM, Consensus equilibrium",Xiran Wang and Jason Juang and Stanley H. Chan,https://www.sciencedirect.com/science/article/pii/S1047320320301474,https://doi.org/10.1016/j.jvcir.2020.102907,1047-3203,2020,102907,72,Journal of Visual Communication and Image Representation,Automatic foreground extraction from imperfect backgrounds using multi-agent consensus equilibrium,article,WANG2020102907
"In this paper, a novel RDH scheme by flipping pattern pairs with opposite center pixel (PPOCPs) in binary images is proposed, aiming at decreasing the distortion while increasing the embedding payload. First, 25 patterns in the 3Ã3 block are designed which construct the PPOCPs according to the distance level providing a guarantee for reversibility. Then, a balanced score is designed which considers both visual distortion and embedding payload to select the optimal PPOCP, and the secret messages are embedded in the optimal PPOCP. For the receiver, the secret messages can be extracted precisely and the original binary image can be recovered by scanning the optimal PPOCP. PPOCP is a novel RDH model which fully considers the visual distortion caused by flipping pixels. Experimental results demonstrate the feasibility of the proposed RDH method for binary images, and the visual quality is satisfactory under high embedding payload and smallest pure flipping rate.","Reversible data hiding, Binary images, Pattern pair with opposite center pixel, Visual quality, Embedding payload",Xiaolin Yin and Wei Lu and JunHong Zhang and Jianfei Chen and Wanteng Liu,https://www.sciencedirect.com/science/article/pii/S1047320320300663,https://doi.org/10.1016/j.jvcir.2020.102816,1047-3203,2020,102816,70,Journal of Visual Communication and Image Representation,Reversible data hiding in binary images by flipping pattern pair with opposite center pixel,article,YIN2020102816
"In this paper, we aim to develop an automatic system to monitor and evaluate workerâs efficiency for smart manufacturing based on human pose tracking and temporal action localization. First, we explore the generative adversarial networks (GANs) to achieve significantly improved estimation of human body joints. Second, we formulate the automated worker efficiency analysis into a temporal action localization problem in which the action video performed by the worker is matched against a reference video performed by a teacher. We extract invariant spatio-temporal features from the human body pose sequences and perform cross-video matching using dynamic time warping. Our proposed human pose estimation method achieves state-of-the-art performance on the benchmark dataset. Our automated work efficiency analysis is able to achieve action localization with an average IoU (intersection over union) score large than 0.9. This represents one of the first systems to provide automated worker efficiency evaluation.","Smart manufacturing, Deep learning, Human pose estimation, Dynamic time warping, Temporal activity localization, Generative adversarial networks",Hao Sun and Guanghan Ning and Zhiqun Zhao and Zhongchao Huang and Zhihai He,https://www.sciencedirect.com/science/article/pii/S1047320320301759,https://doi.org/10.1016/j.jvcir.2020.102948,1047-3203,2020,102948,73,Journal of Visual Communication and Image Representation,Automated work efficiency analysis for smart manufacturing using human pose tracking and temporal action localization,article,SUN2020102948
"In this paper, we present a bottom-up approach for robust spotting of texts in scenes. In the proposed technique, character candidates are first detected using our proposed character detector, which leverages on the strengths of an Extremal Region (ER) detector and an Aggregate Channel Feature (ACF) detector for high character detection recall. The real characters are then identified by using a novel convolutional neural network (CNN) filter for high character detection precision. A hierarchical clustering algorithm is designed which combines multiple visual and geometrical features to group characters into word proposal regions for word recognition. The proposed technique has been evaluated on several scene text spotting datasets and experiments show superior spotting performance.","Text spotting, CNN, Extremal region, Clustering",Jiayuan Fan and Tao Chen and Feng Zhou,https://www.sciencedirect.com/science/article/pii/S1047320320300948,https://doi.org/10.1016/j.jvcir.2020.102843,1047-3203,2020,102843,71,Journal of Visual Communication and Image Representation,BURSTS: A bottom-up approach for robust spotting of texts in scenes,article,FAN2020102843
"Small group detection and tracking in crowd scenes are basis for high level crowd analysis tasks. However, it suffers from the ambiguities in generating proper groups and in handling dynamic changes of group configurations. In this paper, we propose a novel delay decision-making based method for addressing the above problems, motivated by the idea that these ambiguities can be solved using rich temporal context. Specifically, given individual detections, small group hypotheses are generated. Then candidate group hypotheses across consecutive frames and their potential associations are built in a tree. By seeking for the best non-conflicting subset from the hypothesis tree, small groups are determined and simultaneously their trajectories are got. So this framework is called joint detection and tracking. This joint framework reduces the ambiguities in small group decision and tracking by looking ahead for several frames. However, it results in the unmanageable solution space because the number of track hypotheses grows exponentially over time. To solve this problem, effective pruning strategies are developed, which can keep the solution space manageable and also improve the credibility of small groups. Experiments on public datasets demonstrate the effectiveness of our method. The method achieves the state-of-the-art performance even in noisy crowd scenes.","Group tracking, Delay decision, Joint optimization, Multiple hypothesis tracking",Qiulin Ma and Qi Zou and Nan Wang and Qingji Guan and Yanting Pei,https://www.sciencedirect.com/science/article/pii/S104732032030122X,https://doi.org/10.1016/j.jvcir.2020.102876,1047-3203,2020,102876,72,Journal of Visual Communication and Image Representation,Looking ahead: Joint small group detection and tracking in crowd scenes,article,MA2020102876
"This paper presents a novel sparse context-aware spatio-temporal correlation filter tracker (SCAST) method for robust visual object tracking. Different from the existing trackers, this paper introduce an l1 multi-scale regularization parameter-based correlation filter that reduces the boundary effect due to partial occlusions, illumination and scale variations. At each iteration, the l1 regularization parameter is updated through spatial knowledge of each correlation filter coefficient. Besides, the contextual information acquired from the target region can lead to determining the accurate localization of the target. Moreover, contextual information has combined with spatio-temporal factor to achieve the better performance. Further, an objective function is designed with system constraints to ensure the applicability of the model and the optimal solution is derived by utilizing the alternating direction method of multiplier, which leads to low computational cost. Finally, the feasibility and superiority of proposed tracker algorithm is evaluated through three benchmark dataset: OTB-2013, OTB-2015, and TempleColor-128.","Context, ADMM, Spatio-temporal,  regularization, Visual tracking, Correlation filter",Dinesh Elayaperumal and Young Hoon Joo,https://www.sciencedirect.com/science/article/pii/S1047320320300705,https://doi.org/10.1016/j.jvcir.2020.102820,1047-3203,2020,102820,70,Journal of Visual Communication and Image Representation,Visual object tracking using sparse context-aware spatio-temporal correlation filter,article,ELAYAPERUMAL2020102820
"Saliency prediction can be regarded as the human spontaneous activity. The most effective saliency model should highly approximate the response of viewers to the perceived information. In the paper, we exploit the perception response for saliency detection and propose a heuristic framework to predict salient region. First, to find the perceptually meaningful salient regions, an orientation selectivity based local feature and a visual Acuity based global feature are proposed to jointly predict candidate salient regions. Subsequently, to further boost the accuracy of saliency map, we introduce a visual error sensitivity based operator to activate the meaningful salient regions from a local and global perspective. In addition, an adaptive fusion method based on free energy principle is designed to combine the sub-saliency maps from each image channel to obtain the final saliency map. Experimental results on five natural and emotional datasets demonstrate the superiority of the proposed method compared to twelve state-of-the-art algorithms.","Saliency prediction, Orientation selectivity, Visual acuity, Visual error sensitivity, Free energy principle",Yongfang Wang and Peng Ye and Yumeng Xia and Ping An,https://www.sciencedirect.com/science/article/pii/S1047320320301516,https://doi.org/10.1016/j.jvcir.2020.102913,1047-3203,2020,102913,73,Journal of Visual Communication and Image Representation,A heuristic framework for perceptual saliency prediction,article,WANG2020102913
"Image restoration problem is generally ill-posed, which can be alleviated by learning image prior. Inspired by the considerable performance of utilizing priors in pixel domain and wavelet domain jointly, we propose a novel transformed denoising autoencoder as prior (TDAEP). The core idea behind TDAEP is to enhance the classical denoising autoencoder (DAE) via transform domain, which captures complementary information from multiple views. Specifically, 1-level nonorthogonal wavelet coefficients are used to form 4-channel feature images. Moreover, a 5-channel tensor is obtained by stacking the original image under the pixel domain and 4-channel feature images under the wavelet domain. Then we train the transformed DAE (TDAE) with the 5-channel tensor as the network input. The optimized image prior is obtained based on the trained autoencoder, and it is incorporated into an iterative restoration procedure with the aid of the auxiliary variable technique. The resulting model is affiliationed by proximal gradient descent technique. Numerous experiments demonstrated that the TDAEP outperforms a set of image restoration benchmark algorithms.","Image restoration, Denoising autoencoder, Pixel domain, Wavelet domain",Jinjie Zhou and Zhuonan He and Xiaodong Liu and Yuhao Wang and Shanshan Wang and Qiegen Liu,https://www.sciencedirect.com/science/article/pii/S1047320320301590,https://doi.org/10.1016/j.jvcir.2020.102927,1047-3203,2020,102927,72,Journal of Visual Communication and Image Representation,Transformed denoising autoencoder prior for image restoration,article,ZHOU2020102927
"Non-maximum suppression (NMS) as a post-processing step for object detection is mainly used to remove redundant bounding boxes in the object and plays a vital role in many detectors. Its positioning accuracy mainly depends on the bounding box with the highest score, and this strategy is difficult to eliminate the false positive. In order to solve the problem, this paper regards the post-processing step as a combinatorial optimization problem and combines the chaotic whale optimization algorithm and non-maximum suppression. The chaotic search method is used to generate an initial combinatorial solution, and the whale optimization algorithm is discretized to create an updated combinatorial strategy. Under the guidance of the fitness function, the optimal combination is searched. In addition, the method of difference set area (DSA) is proposed to optimize the final detection result. The experiment uses the current mainstream framework Faster R-CNN as the detector on PASCAL VOC2012, COCO2017 and the Warships datasets. The experimental results show that the proposed method can significantly improve the average precision (AP) of detectors compared with the most advanced methods.","Post-processing step, Object detection, Non-maximum suppression",Guixian Wu and Yuancheng Li,https://www.sciencedirect.com/science/article/pii/S1047320320302042,https://doi.org/10.1016/j.jvcir.2020.102985,1047-3203,2021,102985,74,Journal of Visual Communication and Image Representation,Non-maximum suppression for object detection based on the chaotic whale optimization algorithm,article,WU2021102985
"The human visual system analyzes the complex scenes rapidly. It devotes the limited perceptual resources to the most salient subsets and/or objects of scenes while ignoring their less salient parts. Gaze prediction models try to predict the human eye fixations (human gaze) under free-viewing conditions while imitating the attentive mechanism. Previous studies on saliency benchmark datasets have shown that visual attention is affected by the salient objects of the scenes and their features. These features include the identity, the location, and the visual features of objects in the scenes, beside to the context of the input image. Moreover, the human eye fixations often converge to the specific parts of salient objects in the scenes. In this paper, we propose a deep gaze prediction model using object detection via image segmentation. It uses some deep neural modules to find the identity, location, and visual features of the salient objects in the scenes. In addition, we introduce a deep module to capture the prior bias of human eye fixations. To evaluate our model, several challenging saliency benchmark datasets are used in the experiments. We also conduct an ablation study to show the effectiveness of our proposed modules and its architecture. Despite its fewer parameters, our model has comparable, or even better performance on some datasets, to the state-of-the-art saliency models.","Human visual system, Deep gaze prediction model, Image segmentation, Object detection, Prior bias, Convolutional network structure",Samad Zabihi and Eghbal Mansoori and Mehran Yazdi,https://www.sciencedirect.com/science/article/pii/S1047320320301620,https://doi.org/10.1016/j.jvcir.2020.102931,1047-3203,2020,102931,73,Journal of Visual Communication and Image Representation,Exploiting object features in deep gaze prediction models,article,ZABIHI2020102931
"This paper introduces a novel method for segmentation of clustered partially overlapping convex objects in silhouette images. The proposed method involves three main steps: pre-processing, contour evidence extraction, and contour estimation. Contour evidence extraction starts by recovering contour segments from a binarized image by detecting concave points. After this the contour segments which belong to the same objects are grouped. The grouping is formulated as a combinatorial optimization problem and solved using the branch and bound algorithm. Finally, the full contours of the objects are estimated by a Gaussian process regression method. The experiments on a challenging dataset consisting of nanoparticles demonstrate that the proposed method outperforms three current state-of-art approaches in overlapping convex objects segmentation. The method relies only on edge information and can be applied to any segmentation problems where the objects are partially overlapping and have a convex shape.","Segmentation, Overlapping objects, Convex objects, Image processing, Computer vision, Gaussian process, Kriging, Branch and bound",Sahar Zafari and Mariia Murashkina and Tuomas Eerola and Jouni Sampo and Heikki KÃ¤lviÃ¤inen and Heikki Haario,https://www.sciencedirect.com/science/article/pii/S1047320320301863,https://doi.org/10.1016/j.jvcir.2020.102962,1047-3203,2020,102962,73,Journal of Visual Communication and Image Representation,Resolving overlapping convex objects in silhouette images by concavity analysis and Gaussian process,article,ZAFARI2020102962
"In the task of skeleton-based action recognition, CNN-based methods represent the skeleton data as a pseudo image for processing. However, it still remains as a critical issue of how to construct the pseudo image to model the spatial dependencies of the skeletal data. To address this issue, we propose a novel convolutional neural network with adaptive inferential framework (AIF-CNN) to exploit the dependencies among the skeleton joints. We particularly investigate several initialization strategies to make the AIF effective with each strategy introducing the different prior knowledge. Extensive experiments on the dataset of NTU RGB+D and Kinetics-Skeleton demonstrate that the performance is improved significantly by integrating the different prior information. The source code is available at: https://github.com/hhe-distance/AIF-CNN.","Skeleton-based action recognition, Pseudo image, Adaptive inferential framework, Different prior information",Hongâen Huang and Hang Su and Zhigang Chang and Mingyang Yu and Jialin Gao and Xinzhe Li and Shibao Zheng,https://www.sciencedirect.com/science/article/pii/S1047320320301589,https://doi.org/10.1016/j.jvcir.2020.102925,1047-3203,2020,102925,73,Journal of Visual Communication and Image Representation,Convolutional neural network with adaptive inferential framework for skeleton-based action recognition,article,HUANG2020102925
"The producing, sharing and consuming life cycle of video content creates massive amount of duplicates in video segments due to variable bit rate representation and fragmentation in the playbacks. The inefficiency of this duplicates to storage and communication motivate researchers in both academia and industry to come up with computationally efficient video deduplication solutions for storage and CDN providers. Moreover, the increasing demands of high resolution and quality aggravate the status of heavy burden of cluster storage side and restricted bandwidth resources. Hence, video de-duplication in storage and transmission is becoming an important feature for video cloud storage and Content Delivery Network (CDN) service providers. Despite of the necessity of optimizing the multimedia data de-duplication approach, it is a challenging task because we should match as many as possible duplicated videos under not removing videos by mistake. The current video de-duplication schemes mostly relies on the URL based solution, which is not able to deal with non-cacheable content like video, which the same piece of content may have totally different URL identification and fragmentation and different quality representations further complicate the problem. In this paper, we propose a novel content based video segmentation identification scheme that is invariant to the underlying codec and operational bit rates, it computes robust features from a triplet loss deep learning network that captures the invariance of the same content under different coding tools and strategy, while a scalable hashing solution is developed based on Fisher Vector aggregation of the convolutional features from the Triplet loss network. Our simulation results demonstrate the great improvement in terms of large scale video repository de-duplication compared with state-of-the-art methods.","Binary hash, Binary tree, Fisher vector, Triplet loss, Video de-duplication",Wei Jia and Li Li and Zhu Li and Shuai Zhao and Shan Liu,https://www.sciencedirect.com/science/article/pii/S1047320320301462,https://doi.org/10.1016/j.jvcir.2020.102908,1047-3203,2020,102908,72,Journal of Visual Communication and Image Representation,Scalable Hash From Triplet Loss Feature Aggregation For Video De-duplication,article,JIA2020102908
"Empowered by 5G mobile communication networks, multimedia processing has been considered as a very promising application of Internet-of-Things (IoT). Stereoscopic image quality assessment (SIQA), as an important part of 3D capture system, can be embedded in the cloud or fog servers to automatically monitor the perceptual quality of the collected stereoscopic images. In this paper, a novel blind image quality assessment method towards IoT-based 3D capture systems is developed for multiply-distorted stereoscopic images (MDSIs), in which five complementary channels, including left view, right view, cyclopean map, summation map and difference map, are jointly considered in dictionary learning for characterizing the monocular receptive field (MRF) and binocular receptive field (BRF) properties of the visual cortex in response to MDSIs. Additionally, the high order statistics scheme is adopted by utilizing the statistical differences between the codebook and images to ensure the stable and robust quality prediction performance for MDSIs. The proposed method shows competitive prediction performances on four benchmark databases compared with the existing SIQA metrics.","Internet of things, Image quality assessment, Multiply-distorted stereoscopic images, High order statistics",Xuejin Wang and Meiling Qi and Feng Shao and Qiuping Jiang and Xiangchao Meng,https://www.sciencedirect.com/science/article/pii/S1047320320301188,https://doi.org/10.1016/j.jvcir.2020.102868,1047-3203,2020,102868,71,Journal of Visual Communication and Image Representation,Blind quality assessment for multiply distorted stereoscopic images towards IoT-based 3D capture systems,article,WANG2020102868
"Multi-modal canonical correlation analysis (MCCA) is an important joint dimension reduction method and has been widely applied to clustering tasks of multi-modal data. MCCA-based clustering is usually dimension reduction of high-dimensional data followed by clustering of low-dimensional data. However, the two-stage clustering is difficult to ensure the adaptability of dimension reduction and clustering, which will affect the final clustering performance. To solve the issue, we propose a novel clustering adaptive multi-modal canonical correlations (CAMCCs) method, which constructs a unified optimization model of multi-modal correlation learning and clustering. The method not only realizes discriminant learning of correlation projection directions under unsupervised cases, but also is able to directly obtain class labels of multi-modal data. Additionally, the method also realizes out-of-sample extension in class labels. Solutions of CAMCCs are optimized by an iterative way, and we analyze its convergence. Extensive experimental results on various datasets have demonstrated the effectiveness of the method.","Canonical correlation analysis, Joint dimension reduction, Clustering adaptive, High-dimensional data",Shuzhi Su and Xianjin Fang and Gaoming Yang and Bin Ge and Ping Zheng,https://www.sciencedirect.com/science/article/pii/S1047320320300651,https://doi.org/10.1016/j.jvcir.2020.102815,1047-3203,2020,102815,71,Journal of Visual Communication and Image Representation,Clustering adaptive canonical correlations for high-dimensional multi-modal data,article,SU2020102815
"Face aging has been widely considered in many studies regarding all the potential applications. However, the de-aging known as the rejuvenation or backward modeling has recently received more attention. Previous studies mainly focused on rejuvenating faces from aged adults into young adults using two-dimensional (2D) models. In this work, we propose an extension of a previous 2D adult-child B-FAM into 3D model. This model allows a digital face appearance rejuvenation within a range of [75â3] years old. To evaluate the performances of the proposed approach, first, we proposed two performance evaluation modes, namely: Generic Perception Based and Biometric Verification Mode. Then, the performances have been evaluated over our own 3D database, called Face Time-Machine database constructed using 75 females and 70 males, leading to 500 textured surface meshes. Finally, results show that they are perceptually satisfying and system performance increases by using the faces obtained from our model.","3D modeling, Face aging, Textured 3D mesh, Biometrics, Anthropometry, Depth perception, Forensics",Farnaz Majid Zadeh Heravi and Amine Nait-Ali,https://www.sciencedirect.com/science/article/pii/S1047320320300535,https://doi.org/10.1016/j.jvcir.2020.102803,1047-3203,2020,102803,72,Journal of Visual Communication and Image Representation,Adult-child 3D backward face aging model (3D B-FAM),article,HERAVI2020102803
"Recently, Hand-Gesture-Recognition (HGR) systems has appreciably change the way of interaction between humans and computers thanks to advanced sensor technologies like the Leap-Motion-Controller (LMC). Despite the success achieved by many state-of-the-art methods, they have not worked on the rich temporal information existing in the sequential hand gesture data and characterizing the discriminative representation of different hand gesture classes. In this paper, we suggest a novel Chronological-Pattern-Indexing (CPI) approach which encodes the temporal orders of patterns for hand gesture time series data acquired by the LMC sensor. We extract a set of temporal patterns from different optimized projections. Then, we compare their temporal order and we encode the whole sequence with the index of the first coming pattern. We repeat these steps until we generate an efficient feature vector modeling the chronological dynamics of the hand gesture. The experiments demonstrate the potential of the proposed CPI approach for HGR systems.","Leap Motion, Hand gesture recognition, Feature extraction, Time series data, Chronological indexing",Safa Ameur and Anouar {Ben Khalifa} and Med Salim Bouhlel,https://www.sciencedirect.com/science/article/pii/S1047320320300936,https://doi.org/10.1016/j.jvcir.2020.102842,1047-3203,2020,102842,70,Journal of Visual Communication and Image Representation,Chronological pattern indexing: An efficient feature extraction method for hand gesture recognition with Leap Motion,article,AMEUR2020102842
"Depth camera-based virtual rehabilitation systems are gaining attention in occupational therapy for cerebral palsy patients. When developing such a system, domain-specific exercise recognition is vital. To design such a gesture recognition method, some obstacles need to be overcome: detection of gestures not related to the defined exercise set and recognition of incorrect exercises performed by the patients to compensate for their lack of ability. We propose a framework based on hidden Markov models for the recognition of upper extremity functional exercises. We determine critical compensation mistakes together with restrictions for classifying these mistakes with the help of occupational therapists. We first eliminate undefined gestures by evaluating two models that produce adaptive threshold values. Then we utilize specific negative models based on feature thresholding and train them for each exercise to detect compensation mistakes. We perform various tests using our method in a laboratory environment under the supervision of occupational therapists.","Gesture recognition, Cerebral palsy, Occupational therapy, Compensation mistake, Hidden Markov model, Virtual rehabilitation",Mehmet Faruk Ongun and UÄur GÃ¼dÃ¼kbay and Selim Aksoy,https://www.sciencedirect.com/science/article/pii/S1047320320301905,https://doi.org/10.1016/j.jvcir.2020.102970,1047-3203,2020,102970,73,Journal of Visual Communication and Image Representation,Recognition of occupational therapy exercises and detection of compensation mistakes for Cerebral Palsy,article,ONGUN2020102970
"Compared with deadzone hard-decision quantization (HDQ), rate-distortion optimized quantization (RDOQ) in HEVC brings non-negligible coding gain, however consumes considerable computations caused by exhaustive search over multiple candidates to determine optimal output level. Benefiting from efficient prediction in HEVC, transform blocks are frequently quantized to all zero, especially in small-size blocks. It is worthwhile to detect all zero block (AZB) for transform blocks to bypass subsequent computation-intensive RDOQ. Traditional thresholding based AZB detection algorithms are well-suited for deadzone quantized blocks, however miss partial optimal results in RDOQ and suffer from more or less accuracy degradation in RDOQ. This paper proposes a novel multi-stage AZB detection algorithm for RDOQ blocks with good tradeoff between complexity and accuracy. At the first stage, genuine all zero blocks (G_AZB) which are quantized to all zero both in HDQ and RDOQ are prejudged by comparison with conservative threshold determined by mathematical derivation for deadzone HDQ. At the second stage, an adaptive threshold model is built using adaptive deadzone offset by simulating the behavior patterns existing in RDOQ, aiming to further detect the pseudo AZB (P_AZB) which are quantized to all zero in RDOQ however not all zero in HDQ. At the final stage, machine learning based detection is proposed to classify the remaining âcunningâ all zero blocks using eight distinguished RDO-related features, by which subtle working mechanism in RDOQ is leveraged. The experimental results demonstrate that the proposed algorithm achieves up to 7.471% total coding computation saving with 0.064% BD-RATE increment compared with RDOQ on average. Moreover, the average FNR and FPR detection accuracies are 6.3% and 6.5% respectively.","Multi-stage AZB detection, Rate-distortion optimization, Soft-decision quantization, Machine learning",Haibing Yin and Haoyun Yang and Xiaofeng Huang and Hongkui Wang and Chenggang Yan,https://www.sciencedirect.com/science/article/pii/S1047320320301747,https://doi.org/10.1016/j.jvcir.2020.102945,1047-3203,2020,102945,73,Journal of Visual Communication and Image Representation,Multi-stage all-zero block detection for HEVC coding using machine learning,article,YIN2020102945
"In realistic outdoor scenarios, image sensors tend to suffer from various weather conditions (e.g., haze, rain, etc.),which make the images of the same scene taken at different times may be different. Therefore, one should be able to securely embed secret messages into these images by making use of the variations of the weather effects. Inspired by some recent natural steganography algorithms, this paper presents a novel haze image steganography method, which embeds messages through adjusting the weather effects of an input haze image, making it resemble the same image captured under another weather condition. The proposed steganography method consists of three parts: (1) model parameter estimation of the input haze image, (2) haze effects adjustment according to the atmospheric scattering model, (3) message embedding using the floating-point adjusted haze image. 10,000 haze images captured under different haze conditions in various scenarios were used to test the proposed steganography algorithm. The experimental results show that the proposed steganography algorithm is more secure than S-UNIWARD and HILL for steganalyzers who only have raw haze images.","Steganography, Haze image, Natural steganography, UNIWARD, Steganalysis",Baojun Qi and Chunfang Yang and Lei Tan and Xiangyang Luo and Fenlin Liu,https://www.sciencedirect.com/science/article/pii/S104732032030064X,https://doi.org/10.1016/j.jvcir.2020.102814,1047-3203,2020,102814,70,Journal of Visual Communication and Image Representation,A novel haze image steganography method via cover-source switching,article,QI2020102814
"In the field of affective computing (AC), coarse-grained AC has been developed and widely applied in many fields. Electroencephalogram (EEG) signals contain abundant emotional information. However, it is difficult to develop fine-grained AC due to the lack of fine-grained labeling data and suitable visualization methods for EEG data with fine labels. To achieve a fine mapping of EEG data directly to facial images, we propose a conditional generative adversarial network (cGAN) to establish the relationship between EEG data associated with emotions, a coarse label, and a facial expression image in this study. In addition, a corresponding training strategy is also proposed to realize the fine-grained estimation and visualization of EEG-based emotion. The experiments prove the reasonableness of the proposed method for the generation of fine-grained facial expressions. The image entropy of the generated image indicates that the proposed method can provide a satisfactory visualization of fine-grained facial expressions.","Affective computing, Electroencephalography, Generative adversarial network, Fine-grained",Boxun Fu and Fu Li and Yi Niu and Hao Wu and Yang Li and Guangming Shi,https://www.sciencedirect.com/science/article/pii/S1047320320302030,https://doi.org/10.1016/j.jvcir.2020.102982,1047-3203,2021,102982,74,Journal of Visual Communication and Image Representation,Conditional generative adversarial network for EEG-based emotion fine-grained estimation and visualization,article,FU2021102982
"Visual cryptography scheme (VCS) shares a binary secret image into multiple shadows, only qualified set of shadows can reveal the secret image by stacking operation. However, VCS suffers the problems of low visual quality of the revealed image and large shadow size. A (t, k, n) XOR-based visual cryptography scheme (XVCS) shares the secret image into n shadows including t essentials and n-t non-essentials. A qualified set of shadows contains any k shadows including t essentials. The revealing process is implemented by XOR operation on the involved shadows. In this paper, we propose a construction method for (t, k, n)-XVCS with essential shadows. The secret image can be revealed perfectly, and the shadow size is small compared with VCS. Theoretical analysis and experimental results show the security and effectiveness of the proposed scheme.","XOR operation, Essential participants, Visual cryptography, Secret image sharing",Peng Li and Jianfeng Ma and Quan Ma,https://www.sciencedirect.com/science/article/pii/S1047320320301498,https://doi.org/10.1016/j.jvcir.2020.102911,1047-3203,2020,102911,72,Journal of Visual Communication and Image Representation,"(t, k, n) XOR-based visual cryptography scheme with essential shadows",article,LI2020102911
"Image segmentation with a volume constraint is an important prior for many real applications. In this work, we present a novel volume preserving image segmentation algorithm, which is based on the entropy and Total Variation (TV) regularized optimal transport theory. The volume and classification constraints can be regarded as two measures preserving constraints in the optimal transport. By studying the dual problem, we develop a simple but efficient dual algorithm for our model. Moreover, to be different from many variational based image segmentation algorithms, the proposed algorithm can be directly unrolled to a new Volume Preserving and TV regularized softmax (VPTV-softmax) layer for semantic segmentation in the popular Deep Convolution Neural Network (DCNN). The experiment results show that our proposed model is very competitive and can improve the performance of many semantic segmentation networks such as the popular U-net and DeepLabv3+.","Image segmentation, DCNN, Volume preserving, Optimal transport, Entropic regularization, TV regularization",Haifeng Li and Jun Liu and Li Cui and Haiyang Huang and Xue-Cheng Tai,https://www.sciencedirect.com/science/article/pii/S1047320320300961,https://doi.org/10.1016/j.jvcir.2020.102845,1047-3203,2020,102845,71,Journal of Visual Communication and Image Representation,Volume preserving image segmentation with entropy regularized optimal transport and its applications in deep learning,article,LI2020102845
"A two-in-one secret image sharing scheme (TiOSISS) is the combination of two different secret image sharing schemes (SISSs), which has advantages of both schemes, such as the simple stacking-to-see property and precise recovery with computing devices available. Since most of current TiOSISSs depend on steganography techniques, it results in several drawbacks, such as large pixel expansion and poor visual quality with shares stacking. Besides, researchers ignore the independence between two different SISSs, that is, both SISSs should deal with irrelevant secret images with different thresholds. By controlling the randomness of the sharing phase according to constraints from both SISSs, we combine polynomial-based SISS (PSISS) and random-grid-based visual cryptography scheme (RGVCS) together, and propose an ideal (k1,k2,n)-threshold TiOSISS for multiple secrets. The proposed TiOSISS not only overcomes drawbacks above, but also has high scalability to improve its performances by utilizing different RGVCSs. Sufficient analyses and experiments are provided to verify its security and effectiveness.","Ideal secret sharing, RGVCS, Polynomial-based SISS, TiOSISS, Multiple secrets, -threshold",Lintao Liu and Yuliang Lu and Xuehu Yan,https://www.sciencedirect.com/science/article/pii/S1047320320301930,https://doi.org/10.1016/j.jvcir.2020.102971,1047-3203,2021,102971,74,Journal of Visual Communication and Image Representation,"A novel (k1,k2,n)-threshold two-in-one secret image sharing scheme for multiple secrets",article,LIU2021102971
"In this paper, we proposed a semi-supervised common representation learning method with GAN-based Asymmetric Transfer Network (GATN) for cross modality retrieval. GATN utilizes the asymmetric pipeline to guarantee the semantic consistency and adopt (Generative Adversarial Network) GAN to fit the distributions of different modalities. Specifically, the common representation learning across modalities includes two stages: (1) the first stage, GATN trains source mapping network to learn the semantic representation of text modality by supervised method; and (2) the second stage, GAN-based unsupervised modality transfer method is proposed to guide the training of target mapping network, which includes generative network (target mapping network) and discriminative network. Experimental results on three widely-used benchmarks show that GATN have achieved better performance comparing with several existing state-of-the-art methods.","Cross-modal retrieval, Modality gap, Generative adversarial network",Lei Zhang and Leiting Chen and Weihua Ou and Chuan Zhou,https://www.sciencedirect.com/science/article/pii/S1047320320301413,https://doi.org/10.1016/j.jvcir.2020.102899,1047-3203,2020,102899,73,Journal of Visual Communication and Image Representation,Semi-supervised cross-modal representation learning with GAN-based Asymmetric Transfer Network,article,ZHANG2020102899
"The rapid development of deep learning has prompted the development of video action detection technology. However, the accuracy of current video action detection algorithms can be improved further. Previous work has improved feature extraction by optimizing the network structure. In addition, the features of the candidate regions have been optimized by changing the representation of the regions. Although these methods have achieved promising results, they fail to consider the correlation among different candidate regions, generating uninformative (even redundant) candidate regions, and thus usually decrease the detection performance in practice. To address this problem, in this paper we propose a self-attention mechanism for candidate regions, which can help pursue the most informative regions. We obtain the region correlation by simultaneously determining the spatial and temporal correlation among different candidate regions. In addition, we focus on how to apply the correlation to optimize the original candidate region features and improve video action detection accuracy. The experimental results show the promising improvement achieved by our method over the state-of-the-art solutions.","Deep learning, Action detection, Region correlation, Self-attention mechanism",Yeguang Li and Mingyuan Zhang and Liang Hu and Jun Li and Deqing Wang,https://www.sciencedirect.com/science/article/pii/S1047320320300687,https://doi.org/10.1016/j.jvcir.2020.102818,1047-3203,2020,102818,71,Journal of Visual Communication and Image Representation,Candidate region correlation for video action detection,article,LI2020102818
"The just noticeable distortion (JND) in the contour and orderly regions is easy to be overestimated and that in the disorderly areas is usually underestimated. In order to estimate the JND threshold more accurately, this paper proposes an improved DCT-based JND estimation model considering multiple masking effects properly. The contributions of this paper are characterized by twofold. On the one hand, a mean absolute difference based (MAD-based) block classification method is developed at first to classify image blocks into plain, contour and texture types accurately and quickly. And the JND model for contrast masking effect (CM-JND) is constructed as a modulation factor based on the MAD of each block. On the other hand, we propose a distance-based disorder evaluation metric to measure the disorder intensity in block level. Then, the JND model for the disorderly concealment effect (DC-JND) is proposed based on our psychological experiment. Finally, the total JND estimation threshold is modeled by fusing the spatial contrast sensitivity function, the luminance adaptation effect, the CM and DC effects. Experimental results show that the proposed DCT-based JND estimation model outperforms existing models in performance and complexity. Specifically, the proposed model shows more tolerance for distortions, lower computational complexity with better perceptual quality than other JND models.","DCT-based JND estimation model, Contrast masking, Disorderly concealment effect",Hongkui Wang and Li Yu and Haibing Yin and Tiansong Li and Shengwei Wang,https://www.sciencedirect.com/science/article/pii/S1047320320301012,https://doi.org/10.1016/j.jvcir.2020.102850,1047-3203,2020,102850,71,Journal of Visual Communication and Image Representation,An improved DCT-based JND estimation model considering multiple masking effects,article,WANG2020102850
"Expression recognition relies on intensity, edges, and geometry that overlooks the actual shape curvatures of facial regions. This paper presents a novel two-stage approach to distinguish seven expressions on the basis of eleven different facial areas. The combination of contour and region harmonics is used to develop the interrelationship of sub-local areas in the human face for expression recognition. We applied a multi-class support vector machine (SVM) with subject dependent k-fold cross-validation to classify the human emotions into expressions. We tested our proposed method on three public facial expression datasets for sub-local regions in human face and achieved 94.90%, 93.43%, and 92.57% recognition rate for the CK+, CFEE, and MUG datasets respectively. Experiments show that the contour and region harmonics have high classification power and can be computed efficiently. Our method provides higher accuracy, less computing time, and less memory space than existing techniques, including deep learning.","Contour description, Facial expression recognition, Local facial shape harmonics, Region description",Ali {Raza Shahid} and Sheheryar Khan and Hong Yan,https://www.sciencedirect.com/science/article/pii/S1047320320301772,https://doi.org/10.1016/j.jvcir.2020.102949,1047-3203,2020,102949,73,Journal of Visual Communication and Image Representation,Contour and region harmonic features for sub-local facial expression recognition,article,RAZASHAHID2020102949
"Correlation filter-based trackers (CFTs) have recently shown remarkable performance in the field of visual object tracking. The advantage of these trackers originates from their ability to convert time-domain calculations into frequency domain calculations. However, a significant problem of these CFTs is that the model is insufficiently robust when the tracking scenarios are too complicated, meaning that the ideal tracking performance cannot be acquired. Recent work has attempted to resolve this problem by reducing the boundary effects from modeling the foreground and background of the object target effectively (e.g., CFLB, BACF, and CACF). Although these methods have demonstrated reasonable performance, they are often affected by occlusion, deformation, scale variation, and other challenging scenes. In this study, considering the relationship between the current frame and the previous frame of a moving object target in a time series, we propose a temporal regularization strategy to improve the BACF tracker (denoted as TRBACF), a typical representative of the aforementioned trackers. The TRBACF tracker can efficiently adjust the model to adapt the change of the tracking scenes, thereby enhancing its robustness and accuracy. Moreover, the objective function of our TRBACF tracker can be solved by an improved alternating direction method of multipliers, which can speed up the calculation in the Fourier domain. Extensive experimental results demonstrate that the proposed TRBACF tracker achieves competitive tracking performance compared with state-of-the-art trackers.","Visual tracking, Correlation filters, BACF tracker, Temporal regularization, ADMM",Di Yuan and Xiu Shu and Zhenyu He,https://www.sciencedirect.com/science/article/pii/S1047320320301255,https://doi.org/10.1016/j.jvcir.2020.102882,1047-3203,2020,102882,72,Journal of Visual Communication and Image Representation,TRBACF: Learning temporal regularized correlation filters for high performance online visual object tracking,article,YUAN2020102882
"Unsupervised video object segmentation is a crucial application in video analysis when there is no prior information about the objects. It becomes tremendously challenging when multiple objects occur and interact in a video clip. In this paper, a novel unsupervised video object segmentation approach via distractor-aware online adaptation (DOA) is proposed. DOA models spatiotemporal consistency in video sequences by capturing background dependencies from adjacent frames. Instance proposals are generated by the instance segmentation network for each frame and they are grouped by motion information as positives or hard negatives. To adopt high-quality hard negatives, the block matching algorithm is then applied to preceding frames to track the associated hard negatives. General negatives are also introduced when there are no hard negatives in the sequence. The experimental results demonstrate these two kinds of negatives are complementary. Finally, we conduct DOA using positive, negative, and hard negative masks to update the foreground and background segmentation. The proposed approach achieves state-of-the-art results on two benchmark datasets, the DAVIS 2016 and the Freiburg-Berkeley motion segmentation (FBMS)-59.","Unsupervised video object segmentation, Pseudo ground truth, Motion saliency, Hard negative mining, Online adaptation",Ye Wang and Jongmoo Choi and Yueru Chen and Siyang Li and Qin Huang and Kaitai Zhang and Ming-Sui Lee and C.-C. Jay Kuo,https://www.sciencedirect.com/science/article/pii/S1047320320301814,https://doi.org/10.1016/j.jvcir.2020.102953,1047-3203,2021,102953,74,Journal of Visual Communication and Image Representation,Unsupervised video object segmentation with distractor-aware online adaptation,article,WANG2021102953
"Protein structure prediction is an important issue in computational biology, and protein secondary structure prediction is the basis for protein three-dimensional structure prediction. A protein secondary structure prediction method based on convolutional neural networks (CNN) and Long Short-Term Memory (LSTM) is proposed in this paper. The architecture of CNN has two convolutional layers, one max-pooling layer and one ReLU activation layer. The feature maps extracted from second convolutional layer are used to feed to softmax classifier, and the first probability output is obtained. The LSTM model has a sequence layer and a last layer. The feature is extracted from last layer and input to random forest classifier to get the second probability output. The two probabilistic outputs are weighted and integrated to obtain the prediction model EN-CSLR in this paper. Based on the advantages of integration of the two models, cross-validation experiments are performed on the 25pdb dataset, and Q3 reaches 80.18%, which is higher than using only one model. The experimental results show that the features extracted from CNN and LSTM models can effectively improve the accuracy of protein secondary structure prediction.","Protein secondary structure prediction, Convolution neural networks, Long short-term memory, Softmax, Random forest",Jinyong Cheng and Yihui Liu and Yuming Ma,https://www.sciencedirect.com/science/article/pii/S104732032030095X,https://doi.org/10.1016/j.jvcir.2020.102844,1047-3203,2020,102844,71,Journal of Visual Communication and Image Representation,Protein secondary structure prediction based on integration of CNN and LSTM model,article,CHENG2020102844
"Micro-expressions are spontaneous emotions appearing on a face that is hard to conceal and thus making them different from normal facial expressions both in duration and subtlety. This paper investigates a challenging issue in micro-expression, where not all facial regions contribute equally to effective representation. Consequently, we proposed a multi-scale active patches fusion-based spatiotemporal LBP-TOP descriptor that considers the active contributions for different region area in faces. For the feature procedure, we exploit the average value of all patches under each scale to obtain the threshold that selectively fuses the local and global features. On the other hand, an improved weighted sparse representation based dual augmented Lagrange multiplier is adopted for the classification to remit the problem of sparse coefficients obtained by the traditional sparse representation algorithm. We conduct comprehensive experiments on CASME II and SAMM datasets and the accuracies respectively reach 77.30% and 58.82% using LOSO cross-validation.","Micro-expression, Multi-scale active patches, Weighted sparse representation, LBP-TOP",Zhe Sun and Zheng-ping Hu and Mengyao Zhao and Shufang Li,https://www.sciencedirect.com/science/article/pii/S1047320320301139,https://doi.org/10.1016/j.jvcir.2020.102862,1047-3203,2020,102862,71,Journal of Visual Communication and Image Representation,Multi-scale active patches fusion based on spatiotemporal LBP-TOP for micro-expression recognition,article,SUN2020102862
"The automatic detection and extraction of tumor area in Magnetic Resonance Imaging (MRI) is an important and challenging task. This paper presents a fully automatic and unsupervised method for fast and accurate extraction of brain tumor area from MR images. The proposed method named as Saliency Based Segmentation (SBS) is based on visual saliency. The saliency model detects the pathologically important area and then fuzzy thresholding is used for extraction of the detected region. The performance of SBS is compared with Adaptively Regularized Kernel-Based Fuzzy C-Means Clustering, Mean Shift and Fuzzy C-Means clustering with Level Set Method. The experimental evaluation validated on BRATS database using Jaccard index (0.84Â Â±Â 0.04), Dice Index (0.91Â Â±Â 0.02), Execution time (2.99Â Â±Â 0.29), Precision (0.82Â Â±Â 0.16), Recall (0.97Â Â±Â 0.03) and F-measure (0.88Â Â±Â 0.10) demonstrates that SBS achieves better segmentation results even in the presence of noise and uneven illumination in images.","Saliency, Fuzzy, Segmentation, ROI, Medical Images",Paramveer Kaur Sran and Savita Gupta and Sukhwinder Singh,https://www.sciencedirect.com/science/article/pii/S1047320320301899,https://doi.org/10.1016/j.jvcir.2020.102964,1047-3203,2021,102964,74,Journal of Visual Communication and Image Representation,Integrating saliency with fuzzy thresholding for brain tumor extraction in MR images,article,SRAN2021102964
"Unmanned aerial vehicle system (UAVs) imaging has become a challenging area of research due to the dynamic atmospheric environment. The images captured by UAVs are often deteriorated by factors such as clouds occlusion, poor atmospheric illumination, and limited capability of the imaging system. To tackle problems, this paper presents a novel visibility restoration scheme for UAVs images by considering the following two assumptions: (1) The actual scene radiance of a UAVs image is bounded. (2) Pixels sharing the same appearance must have the same transmission value in a local neighborhood. Inspired by above assumptions, an image boundary constraint utilizing the median filter has been imposed on the RGB channel for the rough estimation of transmission-map in aerial images. Furthermore, a graph-model based optimization technique has been used for the transmission-map refinement. The experimental results demonstrate the efficiency of the proposed method in terms of metrics correspond to the human-visual-system (HVS).","Remote sensing, Satellite imaging, Unmanned aerial vehicle imaging, Image restoration, Image enhancement, Image dehazing",Sidharth Gautam and Tapan Kumar Gandhi and B.K. Panigrahi,https://www.sciencedirect.com/science/article/pii/S1047320320302108,https://doi.org/10.1016/j.jvcir.2020.102993,1047-3203,2021,102993,74,Journal of Visual Communication and Image Representation,A Model-based dehazing scheme for unmanned aerial vehicle system using radiance boundary constraint and graph model,article,GAUTAM2021102993
"This paper presents an approach to design Indian Sign Language (ISL) recognition system for complex background. In many applications, Histogram of Oriented Gradients (HOG) have been proved to be effective. However, it is observed that the choice of HOG parameters affects the feature vector size and its classification capability. The objective is to select the parameter values in order to have maximal accuracy at a minimal computational time and reduced feature vector size. A combined Taguchi and Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) based decision-making technique is applied to determine the values of these parameters. Results show that the combined TOPSIS-Taguchi based technique is effective in selecting the parameter combination to get high overall performance. For the acquired ISL complex background dataset, the selected values of parameters are further used to obtain multi-level HOG resulting in the overall accuracy of 92% for 280 features.","Histogram of Oriented Gradients (HOG), Taguchi, TOPSIS, Sign Language Recognition System (SLRS), Complex background, Indian Sign Language (ISL)",Garima Joshi and Sukhwinder Singh and Renu Vig,https://www.sciencedirect.com/science/article/pii/S1047320320300845,https://doi.org/10.1016/j.jvcir.2020.102834,1047-3203,2020,102834,71,Journal of Visual Communication and Image Representation,Taguchi-TOPSIS based HOG parameter selection for complex background sign language recognition,article,JOSHI2020102834
"Fully convolutional networks (FCNs) have been efficiently applied in splicing localization. However, the existing FCN-based methods still have three drawbacks: (a) their performance in detecting image details is unsatisfactory; (b) deep FCNs are difficult to train; (c) results of multiple FCNs are merged using fixed parameters to weigh their contributions. So, an improved method is proposed. Firstly, both the original spliced image and its corresponding residual image are regarded as the inputs of the network. Secondly, the residual block is introduced into FCN as residual-based FCN (RFCN) to make the network easier to optimize. Thirdly, three different RFCNs are merged to enhance locating maps with two learnable weight parameters. Besides, condition random field is introduced into the whole network to improve the results further. Experimental results on five datasets show that the proposed method performs better than some existing methods in localization ability, generalization ability, and robustness against additional operations.","Splicing localization, Fully convolutional network, Residual block, Residual image, Condition random field",Beijing Chen and Xiaoming Qi and Yang Zhou and Guanyu Yang and Yuhui Zheng and Bin Xiao,https://www.sciencedirect.com/science/article/pii/S1047320320301929,https://doi.org/10.1016/j.jvcir.2020.102967,1047-3203,2020,102967,73,Journal of Visual Communication and Image Representation,Image splicing localization using residual image and residual-based fully convolutional network,article,CHEN2020102967
"The nonlocal self-similarity of images means that groups of similar patches have low-dimensional property. The property has been previously used for image denoising, with particularly notable success via sparse coding. However, only a few studies have focused on the varying statistics of noise in different similar patches during the iterative denoising process. This has motivated us to introduce an improved weighted sparse coding for gray-level image denoising in this paper. On the basis of traditional sparse coding, we introduce a weight matrix to account for the noise variation characteristics of different similar patches, while introduce another weight matrix to make full use of the sparsity priors of natural images. The Maximum A-Posterior estimation (MAP) is used to obtain the closed-form solution of the proposed method. Experimental results demonstrate the competitiveness of the proposed method compared with that of state-of-the-art methods in both the objective and perceptual quality.","Image denoising, Nonlocal self-similarity, Weight matrix, Weighted sparse coding",Yang Ou and Jianqiao Luo and Bailin Li and M.N.S. Swamy,https://www.sciencedirect.com/science/article/pii/S1047320320301401,https://doi.org/10.1016/j.jvcir.2020.102895,1047-3203,2020,102895,72,Journal of Visual Communication and Image Representation,Gray-level image denoising with an improved weighted sparse coding,article,OU2020102895
"The past few years have witnessed a surge of interest in many topics at the intersection of natural language processing and computer vision. In particular, using objects together with their attributes and relations to represent images or interpret languages has been proved useful across a wide variety of applications. The goal of this work is to provide an improved RDF-based model to represent images for enhancing textual based image retrieval. We use natural language processing tools to obtain a set of objects, attributes and relations; and then model them into graphical structures with RDF-based model. We also conduct some preliminary experiments to show how to handle textual based image retrieval for complex queries or multilingual queries. The experimental results show that our approach improves the representation of image descriptions, which is suitable for enhancing image retrieval with high-level semantics.","Image representation, RDF, Image retrieval, Cross-lingual retrieval, Semantic image retrieval",Hua Chen and AiBin Guo and Wenlong Ni and Yan Cheng,https://www.sciencedirect.com/science/article/pii/S1047320320301644,https://doi.org/10.1016/j.jvcir.2020.102934,1047-3203,2020,102934,73,Journal of Visual Communication and Image Representation,Improving the representation of image descriptions for semantic image retrieval with RDF,article,CHEN2020102934
"The discrepancy between an image and its âreblurredâ version indicates the extent of blur in the image. This paper presents a novel no-reference image sharpness evaluator leveraging the discrepancy measures of structural degradation in both the spatial and wavelet domains. Specifically, local structural degradation of an input image is characterized by the discrepancy measures of orientation selectivity-based visual patterns and log-Gabor filter responses between the image and its corresponding reblurred version respectively. Considering the influence of viewing distance on image quality, the global sharpness discrepancy is measured through inter-resolution self-similarities. Finally, the computed discrepancies are utilized as sharpness-aware features and then a support vector regressor is employed to map the feature vectors into quality scores. The performance of the proposed method is evaluated on six public image quality databases, including two real blurred image databases. Experimental results demonstrate that our proposed method achieves state-of-the-art performances across all these databases.","Image sharpness assessment, Structural degradation, No-reference, Orientation selectivity mechanism",Hao Cai and Mingjie Wang and Wendong Mao and Minglun Gong,https://www.sciencedirect.com/science/article/pii/S1047320320301127,https://doi.org/10.1016/j.jvcir.2020.102861,1047-3203,2020,102861,71,Journal of Visual Communication and Image Representation,No-reference image sharpness assessment based on discrepancy measures of structural degradation,article,CAI2020102861
"Existing metric learning methods often do not consider different granularity in visual similarity. However, in many domains, images exhibit similarity at multiple granularities with visual semantic concepts, e.g.fashion demonstrates similarity ranging from clothing of the exact same instance to similar looks/design or common category. Therefore, training image triplets/pairs inherently possess different degree of information. Nevertheless, the existing methods often treat them with equal importance which hinder capturing underlying granularities in image similarity. In view of this, we propose a new semantic granularity metric learning (SGML) that develops a novel idea of detecting and leveraging attribute semantic space and integrating it into deep metric learning to capture multiple granularities of similarity. The proposed framework simultaneously learns image attributes and embeddings with multitask-CNN where the tasks are linked by semantic granularity similarity mapping to leverage correlations between the tasks. To this end, we propose a new soft-binomial deviance loss that effectively integrates informativeness of training samples into metric-learning on-the-fly during training. Compared to recent ensemble-based methods, SGML is conceptually elegant, computationally simple yet effective. Extensive experiments on benchmark datasets demonstrate its superiority e.g., 1â4.5%-Recall@1 improvement over the state-of-the-arts (Kim etÂ al., 2018; Cakir etÂ al., 2019) on DeepFashion-Inshop dataset.","Deep learnin, Metric learning, Metric loss functions, Semantic similarity, Visual search",Dipu Manandhar and Muhammet Bastan and Kim-Hui Yap,https://www.sciencedirect.com/science/article/pii/S104732032030119X,https://doi.org/10.1016/j.jvcir.2020.102871,1047-3203,2020,102871,72,Journal of Visual Communication and Image Representation,Semantic granularity metric learning for visual search,article,MANANDHAR2020102871
"Abstract The latest video coding standard Versatile Video Coding (VVC) obtains superior coding efficiency compared to the High Efficiency Video Coding (HEVC), which is achieved by incorporating more effective and complex new coding tools. In this paper, we propose a novel fast intra mode decision algorithm for VVC, including following two strategies: (1) the correlation between the optimal modes of the adjacent blocks and the modes selected in the rough modes decision (RMD) process is analyzed and applied to reduce the modes in the candidate list; (2) modes in the candidate list are sorted in ascending order according to the modesâ cost calculated in the RMD process. An early termination method is proposed for terminating the optimal prediction mode decision process based on this new order early. These two strategies are incorporated into intra coding to reduce the coding complexity. Since these two strategies do not add any additional computational complexity, the proposed fast algorithm can achieve more complexity reduction. The experimental results show that the complexity reduction of the proposed algorithm is up to 44.74% compared to VVC reference software VTM2.0, and averagely 30.59% encoding time saving with 0.86% BDBR increase.","Versatile video coding, Intra mode decision, Mode correlation, Sorted candidate list, Early termination",Yamei Chen and Li Yu and Hongkui Wang and Tiansong Li and Shengwei Wang,https://www.sciencedirect.com/science/article/pii/S1047320320301000,https://doi.org/10.1016/j.jvcir.2020.102849,1047-3203,2020,102849,71,Journal of Visual Communication and Image Representation,A novel fast intra mode decision for versatile video coding,article,CHEN2020102849
"This paper addresses a new approach to learn perceptual grouping of the extracted features of the convolutional neural network (CNN) to represent the structure contained in the image. In CNN, the spatial hierarchies between the high-level features are not taken into account. To do so, the perceptual grouping of features is utilized. To consider the intra-relationship between feature maps, modified Guided Co-occurrence Block (mGCoB) is proposed. This block preserves the joint co-occurrence of two features in the spatial domain and it prevents the co-adaptation. Also, to preserve the interrelationship in each feature map, the principle of common region grouping is utilized which states that the features which are located in the same feature map tend to be grouped together. To consider it, an MFC block is proposed. To evaluate the proposed approach, it is applied to some known semantic segmentation and image classification datasets that achieve superior performance.","Perceptual grouping, Modified Guided Co-occurrence Block (mGCoB), Map-wise Fully Connected Block (MFC), Spatial hierarchies of high-level features",Parvin Razzaghi and Karim Abbasi and Pegah Bayat,https://www.sciencedirect.com/science/article/pii/S1047320320300675,https://doi.org/10.1016/j.jvcir.2020.102817,1047-3203,2020,102817,70,Journal of Visual Communication and Image Representation,Learning spatial hierarchies of high-level features in deep neural network,article,RAZZAGHI2020102817
"Pansharpening is a process to fuse a low spatial resolution multispectral image and a high spatial resolution panchromatic image to produce a high-resolution multispectral image. Quality assessment of pansharpened images is challenging due to without actual reference images. There are two main types of assessment methods: reduced resolution (RR) assessment based on Waldâs protocol, and full resolution (FR) assessment without reference. Currently, it is lack of large-scale benchmark databases for subjective and objective performance evaluation of different image pansharpening methods. In this paper, we construct a large-scale database named Pansharpened Remote Sensing Image Quality Database (PRSIQD) from both qualitative and quantitative perspectives, which contains 13,620 pansharpened images acquired from IKONOS, QuickBird, Gaofen-1, WorldView-2, WorldView-3 and WorldView-4 satellite sensors. In addition, we have comprehensively analyzed the advantages and disadvantages of the existing pansharpening quality assessment methods on different satellite sensors, thematic datasets and bands.","Pansharpened image quality assessment, Pansharpened Remote Sensing Image Quality Database (PRSIQD), Panchromatic, Multispectral, Pansharpening, Remote sensing",Yiming Xiong and Feng Shao and Xiangchao Meng and Qiuping Jiang and Weiwei Sun and Randi Fu and Yo-Sung Ho,https://www.sciencedirect.com/science/article/pii/S1047320320301760,https://doi.org/10.1016/j.jvcir.2020.102947,1047-3203,2020,102947,73,Journal of Visual Communication and Image Representation,A large-scale remote sensing database for subjective and objective quality assessment of pansharpened images,article,XIONG2020102947
"Compared to traditional 2D images, light field images record both spatial and angular information of the scene, which can provide more data for image fusion. In this paper, a light field all-in-focus image fusion algorithm based on spatially-guided angular information is proposed. In the proposed method, the initial weight maps carrying the angular information are calculated by comparing the block variance of the 4D light field data. The initial weight maps are then guided by digital refocused images carrying the spatial information to obtain the refined weight maps. In the refocused image multi-scale decomposition, the micro-lens calibration error is considered and the additional edge layers are extracted to suppress the edge artifacts. Experiments demonstrate the effectiveness of the proposed algorithm. Quantitative evaluation results show that the proposed algorithm performs the best in the feature-based index and structural similarity-based index without sacrificing the information and perceptual sharpness of the fused image.","Light field camera, Micro-lens array, All-in-focus image fusion, Spatial information, Angular information, Guided filtering",Yingchun Wu and Yumei Wang and Jie Liang and Ivan V. BajiÄ and Anhong Wang,https://www.sciencedirect.com/science/article/pii/S1047320320301243,https://doi.org/10.1016/j.jvcir.2020.102878,1047-3203,2020,102878,72,Journal of Visual Communication and Image Representation,Light field all-in-focus image fusion based on spatially-guided angular information,article,WU2020102878
"Image steganography aims to securely embed secret information into cover images. Until now, adaptive embedding algorithms such as S-UNIWARD or Mi-POD, were among the most secure and most often used methods for image steganography. With the arrival of deep learning and more specifically, Generative Adversarial Networks (GAN), new steganography techniques have appeared. Among them is the 3-player game approach, where three networks compete against each other. In this paper, we propose three different architectures based on the 3-player game. The first architecture is proposed as a rigorous alternative to two recent publications. The second takes into account stego noise power. Finally, our third architecture enriches the second one with a better interaction between embedding and extracting networks. Our method achieves better results compared to existing works Hayes and Danezis (2017), Zhu etÂ al. (2018), and paves the way for future research on this topic.","Steganalysis, Deep learning, CNN, GAN",Mehdi Yedroudj and FrÃ©dÃ©ric Comby and Marc Chaumont,https://www.sciencedirect.com/science/article/pii/S1047320320301486,https://doi.org/10.1016/j.jvcir.2020.102910,1047-3203,2020,102910,72,Journal of Visual Communication and Image Representation,Steganography using a 3-player game,article,YEDROUDJ2020102910
"In this paper, we propose an end-to-end hierarchical-based multi-mode attention network and adaptive fusion (HMAN-HAF) strategy to learn different-level salient features for re-ID tasks. First, according to each layerâs characteristics, a hierarchical multi-mode attention network (HMAN) is designed to adopt different attention models for different-level salient feature learning. Specifically, refined channel-wise attention (CA) is adopted to capture high-level valuable semantic information, an attentive region model (AR) is used to detect salient regions in the low layer, and fused attention (FA) is designed to capture the salient regions of valuable channels in the middle layer. Second, a hierarchical adaptive fusion (HAF) is constructed to fulfill the complementary strengths of different-level salient features. Experimental results demonstrate that the proposed method outperforms the state-of-the-art methods on the following challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03.","Pedestrian re-identification, Hierarchical, Multi-mode attention network, Hierarchical adaptive fusion, Fused attention",Yanbing Geng and Yongjian Lian and Mingliang Zhou and Yixue Kong and Yinong Zhu,https://www.sciencedirect.com/science/article/pii/S1047320320301528,https://doi.org/10.1016/j.jvcir.2020.102914,1047-3203,2020,102914,73,Journal of Visual Communication and Image Representation,Exploiting multigranular salient features with hierarchical multi-mode attention network for pedestrian re-IDentification,article,GENG2020102914
"The traditional RDH method for JPEG bitstream is conducted by building the mapping between the variable length codes (VLC). However, the capacity is limited, and the file size may not be well preserved as the capacity is increased. This is because that the trade-off between the capacity and the file size has not been deeply investigated, neither explicitly formulated nor appropriately optimized. In this paper, we propose to take the file size preservation into consideration and minimize the file size increase for a given capacity. We use the value transfer matrix to simulate a theoretical model and then design some optimization rules to reach the reversible solution. Consequently, a better reversible VLC mapping can be obtained in terms of both the capacity and the file size preservation. The experimental results show that the proposed method can increase the capacity with a relatively low cost of file size increase.","Reversible data hiding, JPEG bitstream, VLC mapping, File size preservation",Cheng Zhang and Bo Ou and Huawei Tian and Zheng Qin,https://www.sciencedirect.com/science/article/pii/S1047320320300717,https://doi.org/10.1016/j.jvcir.2020.102821,1047-3203,2020,102821,71,Journal of Visual Communication and Image Representation,Reversible data hiding in JPEG bitstream using optimal VLC mapping,article,ZHANG2020102821
"In this paper, we focus on recognizing person-person interactions using skeletal data captured from depth sensors. First, we propose a novel and efficient view transformation scheme. The skeletal interaction sequence is re-observed under a new coordinate system, which is invariant to various setups and capturing views of depth cameras as well as the position or facing orientation exchange between two persons. Second, we propose concise and discriminative interaction representations simply composed of the joint locations from two persons. Proposed representations are efficient to describe both the holistic interactive scene and individual poses performed by each subject separately. Third, we introduce the graph convolutional networks(GCN) to directly learn proposed skeletal interaction representations. Moreover, we design a multiple GCN-based model to provide the final class score. Extensive experimental results on three skeletal action datasets NTU RGB+D 60, NTU RGB+D 120 and SBU consistently demonstrate the superiority of our interaction recognition method.","Activity analysis, Human interaction recognition, Skeleton data, Graph Convolution, Relative view",Xing Liu and Yanshan Li and Tianyu Guo and Rongjie Xia,https://www.sciencedirect.com/science/article/pii/S1047320320300833,https://doi.org/10.1016/j.jvcir.2020.102833,1047-3203,2020,102833,70,Journal of Visual Communication and Image Representation,Relative view based holistic-separate representations for two-person interaction recognition using multiple graph convolutional networks,article,LIU2020102833
"Copy-move forgery detection (CMFD) is the process of determining the presence of copied areas in an image. CMFD approaches are mainly classified into two groups: keypoint-based and block-based techniques. In this paper, a new CMFD approach is proposed on the basis of both block and keypoint based approaches. Initially, the forged image is partitioned into non overlapped segments utilizing adaptive watershed segmentation, wherein adaptive H-minima transform is used for extracting the markers. Also, an Adaptive Galactic Swarm Optimization (AGSO) algorithm is used to select optimal gap parameter while selecting the markers for reducing the undesired regional minima, which can increase the segmentation performance. After that, the features from every segment are extracted as segment features (SF) using Hybrid Wavelet Hadamard Transform (HWHT). Then, feature matching is performed using adaptive thresholding. The false matches or outliers can be removed with the help of Random Sample Consensus (RANSAC) algorithm. Finally, the Forgery Region Extraction Algorithm (FREA) is utilized for detecting the copied portion from the host image. Experimental results indicate that the proposed scheme find out image forgery region with PrecisionÂ =Â 92.45%; RecallÂ =Â 93.67% and F1Â =Â 92.75% on MICC-F600 dataset and PrecisionÂ =Â 94.52%; RecallÂ =Â 95.32% and F1Â =Â 93.56% on Bench mark dataset at pixel level. Also, it outperforms the existing approaches when the image undergone certain geometrical transformation and image degradation.","Copy-move forgery detection, Segments, Adaptive Galactic Swarm Optimization, RANSAC, Adaptive thresholding",Sreenivasu Tinnathi and G. Sudhavani,https://www.sciencedirect.com/science/article/pii/S1047320320301917,https://doi.org/10.1016/j.jvcir.2020.102966,1047-3203,2021,102966,74,Journal of Visual Communication and Image Representation,An efficient copy move forgery detection using adaptive watershed segmentation with AGSO and hybrid feature extraction,article,TINNATHI2021102966
"Video steganography forms a covert communication channel by data embedding in cover elements. To consider inter-frame mutual embedding impacts, this paper proposes a payload allocation strategy in video steganography based on motion vector modification distortion analysis. Firstly, the motion vector modification distortion caused by data embedding is analyzed. Then, a rateâdistortion model reflecting the residue deviation propagation in successive inter-coded frames is derived. According to this model, the residue deviation propagation weight of each inter-coded frame can be computed. Finally, an inter-frame payload allocation strategy is designed in order to restrain the residue deviation propagation. Experimental results demonstrate that the proposed payload allocation strategy can enhance existing motion vector-based video steganographic methods in terms of undetectability and video coding performance. Besides, the lower computational complexity can be achieved.","Video steganography, Motion vector, Payload allocation, Motion vector modification distortion, Residue deviation propagation",Yuanzhi Yao and Nenghai Yu,https://www.sciencedirect.com/science/article/pii/S1047320320302054,https://doi.org/10.1016/j.jvcir.2020.102986,1047-3203,2021,102986,74,Journal of Visual Communication and Image Representation,Motion vector modification distortion analysis-based payload allocation for video steganography,article,YAO2021102986
"Adaptive quantization proves to be an effective tool to improve coding performance. In this paper, we propose an adaptive spatiotemporal perception aware quantization algorithm to increase subjective coding performance. To measure the spatiotemporally perceptual redundancy, the perceptual complexity models are firstly established with spatial and temporal characteristics respectively. With the help of the models, the adaptive spatial and temporal quantization parameter (QP) offsets are then calculated for each coding tree unit (CTU), respectively. Finally, the perceptually optimal Lagrange multiplier of each CTU is determined with the spatialâtemporal QP offset. Experimental results show that the proposed algorithm reduces 8.6% and 8.4% Bjontegaard-Delta Rate (BD-Rate) with Structural Similarity Index Metric (SSIM) in average over the second generation of Audio Video Coding Standard (AVS2) reference software RD17.0 in Low-Delay-P (LDP) and Random-Access (RA) configurations, respectively. The subjective assessment proves that the proposed algorithm can reduce the bitrates with the same subjective quality significantly.","Adaptive quantization, Perceptual video coding (PVC), Just noticeable distortion (JND), Spatially perceptual complexity, Temporally perceptual complexity, Subjective quality",Yunyao Yan and Guoqing Xiang and Yuan Li and Xiaodong Xie and Huizhu Jia,https://www.sciencedirect.com/science/article/pii/S1047320320301553,https://doi.org/10.1016/j.jvcir.2020.102917,1047-3203,2020,102917,73,Journal of Visual Communication and Image Representation,An adaptive spatio-temporal perception aware quantization algorithm for AVS2,article,YAN2020102917
"In recent years, deep learning has been successfully applied to medical image segmentation. However, as the network extends deeper, the consecutive downsampling operations will lead to more loss of spatial information. In addition, the limited data and diverse targets increase the difficulty for medical image segmentation. To address these issues, we propose a multi-path connected network (MCNet) for medical segmentation problems. It integrates multiple paths generated by pyramid pooling into the encoding phase to preserve semantic information and spatial details. We utilize multi-scale feature extractor block (MFE block) in the encoder to obtain large and multi-scale receptive fields. We evaluated MCNet on three medical datasets with different image modalities. The experimental results show that our method achieves better performance than the state-of-the-art approaches. Our model has strong feature learning ability and is robust to capture different scale targets. It can achieve satisfactory results while using only 0.98 million (M) parameters.","Medical image segmentation, Multi-path connections, Convolutional neural networks, Encoder-decoder structure",Dan Wang and Guoqing Hu and Chengzhi Lyu,https://www.sciencedirect.com/science/article/pii/S1047320320301036,https://doi.org/10.1016/j.jvcir.2020.102852,1047-3203,2020,102852,71,Journal of Visual Communication and Image Representation,Multi-path connected network for medical image segmentation,article,WANG2020102852
"Sparsity-based single image super resolution method generates the High-Resolution (HR) output via a corresponding dictionary from the Low-Resolution (LR) input. However, most of these existing methods ignore the complementary information from color channels, which causes the loss of a valid prior and the limitation of HR image quality improvement. In this paper, hypergraph regularization is first incorporated with Joint Color Dictionary Training (JCDT) model and HR image reconstruction (HRIR) model. A novel Hypergraph-regularized Sparse coding-based Super Resolution (HG-ScSR) is proposed. This regularization can not only focus on the illuminance information, but also exploit the self-channel and cross-channel information of three color RGB channels from high-resolution image patches. Especially, the complex relationship is explored among every color image patch pixel and the consistency of the similar pixels is enforced. Both simulated and real data experiments verify the higher performance of the proposed HG-ScSR.","Color image super resolution, Alternating Direction Method of Multipliers (ADMM), Joint Color Dictionary Training (JCDT), Hypergraph regularization, Self-channel and cross-channel information",Minghua Wang and Qiang Wang,https://www.sciencedirect.com/science/article/pii/S1047320320301796,https://doi.org/10.1016/j.jvcir.2020.102951,1047-3203,2021,102951,74,Journal of Visual Communication and Image Representation,Hypergraph-regularized sparse representation for single color image super resolution,article,WANG2021102951
"The RGB-T trackers based on correlation filter framework have been extensively investigated for that they can track targets more accurately in most complex scenes. However, the performance of these trackers is limited when facing some specific challenging scenarios, such as occlusion and background clutter. For different tracking targets, most of these trackers utilize fixed regularization constraint to build the filter model, which is obviously unreasonable to effectively present the appearance changes and characteristics of a specific target. In addition, they adopt a simple model update mechanism based on linear interpolation, which can easily lead to model degradation in challenging scenarios, resulting in tracker drift. To solve the above problems, we propose a novel adaptive spatial-temporal regularized correlation filter model to learn an appropriate regularization for achieving robust tracking and a relative peak discriminative method for model updating to avoid the model degradation. Besides, to make better integrate the unique advantages of the two modes and adapt the changing appearance of the target, an adaptive weighting ensemble scheme and a multi-scale search mechanism are adopted, respectively. To optimize the proposed model, we designed an efficient ADMM algorithm, which greatly improved the efficiency. Extensive experiments have been carried out on two available datasets, RGBT234 and RGBT210, and the experimental results indicate that the tracker proposed by us performs favorably in both accuracy and robustness against the state-of-the-art RGB-T trackers.","Object tracking, Correlation filters, Adaptive spatial-temporal regularization, ADMM, Model updating",Mingzheng Feng and Kechen Song and Yanyan Wang and Jie Liu and Yunhui Yan,https://www.sciencedirect.com/science/article/pii/S1047320320301279,https://doi.org/10.1016/j.jvcir.2020.102881,1047-3203,2020,102881,72,Journal of Visual Communication and Image Representation,Learning discriminative update adaptive spatial-temporal regularized correlation filter for RGB-T tracking,article,FENG2020102881
"Existing Extended Progressive Visual Cryptography scheme (EPVCS) suffers from the problem of pixel expansion, poor quality of reconstructed image and residual trace of cover images in the reconstructed image. Hence in this paper, Two in One Image secret sharing scheme for EPVCS is proposed which decodes the encrypted image in two stages. The proposed scheme provides k,n threshold construction using meaningful shares without pixel expansion and demonstrates that the reconstructed image has improved quality compared to the existing schemes. To reduce the computational complexity, the proposed scheme uses simple Modular Arithmetic operations instead of Galois field. The proposed scheme has the additional advantages of supporting any value of k and n, no overhead in resizing the secret image and no residual trace of cover image. Simulation results and performance analysis show the effectiveness of proposed scheme with improved contrast, 99% Structural Similarity of the reconstructed image and good progressive reconstruction.","Visual Cryptography, Progressive, Polynomial, Meaningful shares and Modular Arithmetic",Srividhya Sridhar and Gnanou Florence Sudha,https://www.sciencedirect.com/science/article/pii/S1047320320302121,https://doi.org/10.1016/j.jvcir.2020.102996,1047-3203,2021,102996,74,Journal of Visual Communication and Image Representation,Two in One Image Secret Sharing Scheme (TiOISSS) for extended progressive visual cryptography using simple modular arithmetic operations,article,SRIDHAR2021102996
"Single image dehazing is a critical image pre-processing step for many practical vision systems. Most existing dehazing methods solve this problem utilizing various of hand-crafted priors or by supervised training on the synthetic hazy image informationÂ (such as haze-free image, transmission map and atmospheric light). However, the assumptions on the hand-crafted priors are easily violated and collecting realistic transmission map and atmospheric light are unpractical. In this paper, we propose a novel weakly supervised network based on the multi-level multi-scale block. The proposed network reduces the constraint on the training data and automatically estimates the transmission map and the atmospheric light as well as the intermediate haze-free image without using any realistic transmission map and atmospheric light as supervision. Moreover, the estimated intermediate haze-free image helps to generate accurate transmission map and atmospheric light by embedding the physical-model, which presents reliable restoration of the final haze-free image. In particular, our network also can be trained on the real-world dataset to fine-tune the model and the fine-tuning operation improves the dehazing performance on the real-world dataset. Quantitative and qualitative experimental results demonstrate the proposed method performs on par with the supervised methods.","Image dehazing, Weakly supervised, Convolutional neural network (CNN), Multi-level multi-scale block",Cong Wang and Wanshu Fan and Yutong Wu and Zhixun Su,https://www.sciencedirect.com/science/article/pii/S1047320320301395,https://doi.org/10.1016/j.jvcir.2020.102897,1047-3203,2020,102897,72,Journal of Visual Communication and Image Representation,Weakly supervised single image dehazing,article,WANG2020102897
"Sonar images are usually suffering from speckle noise which results in poor visual quality. In order to improve the sonar imaging quality, removing or reducing these speckle noises is a very important and arduous task. In this paper, the imaging principle and noise characteristics of the side-scan sonar (SSS) are analyzed, and five typical probability distribution functions are used to fit the seabed reverberation. Through experiment comparison, the Gamma distribution is selected to simulate the noise of the SSS image caused by the reverberation. Simultaneously, the fields of experts denoising algorithm based on the Gamma distribution (Gamma FoE) is proposed for SSS image denoising. In order to perceive and measure the denoising effect better, evaluation indexes of Fast Noise Variance Estimation (FNVE, an image noise estimation method) and Blind Referenceless Image Spatial Quality Evaluator (BRISQUE, an image quality evaluation method) are selected for image quality perception. The final results of the SSS image denoise experiment show that the Gamma FoE denoise algorithm has a better effect on SSS image denoise application than other denoise algorithms.","Sonar image, Denoising, Gamma distribution, Fields of Experts",Fei Yuan and Fengqi Xiao and Kaihan Zhang and Yifan Huang and En Cheng,https://www.sciencedirect.com/science/article/pii/S104732032030211X,https://doi.org/10.1016/j.jvcir.2020.102995,1047-3203,2021,102995,74,Journal of Visual Communication and Image Representation,Noise reduction for sonar images by statistical analysis and fields of experts,article,YUAN2021102995
"We develop an image similarity descriptor for an image pair, based on deep features. The development consists of two parts - selecting the deep layer whose features are to be included in the descriptor, and a representation of the similarity between the images in the pair. The selection of the deep layer follows a sparse representation of the feature maps followed by multi-output support vector regression. The similarity representation is based on a novel correlation between the histograms of the feature maps of the two images. Experiments to demonstrate the effectiveness of the proposed descriptor are carried out on four applications that can be cast as classification tasks.","Image similarity, Similarity representation, Deep features selection, Correlational descriptor",Liangliang Wang and Deepu Rajan,https://www.sciencedirect.com/science/article/pii/S1047320320300985,https://doi.org/10.1016/j.jvcir.2020.102847,1047-3203,2020,102847,71,Journal of Visual Communication and Image Representation,An image similarity descriptor for classification tasks,article,WANG2020102847
"The number of people collecting photos has surged owing to social media and cloud services in recent years. A typical approach to summarize a photo collection is dividing it into events and selecting key photos from each event. Despite the fact that a certain event comprises several sub-events, few studies have proposed sub-event segmentation. We propose the sentiment analysis-based photo summarization (SAPS) method, which automatically summarizes personal photo collections by utilizing metadata and visual sentiment features. For this purpose, we first cluster events using metadata of photos and then calculate the novelty scores to determine the sub-event boundaries. Next, we summarize the photo collections using a ranking algorithm that measures sentiment, emotion, and aesthetics. We evaluate the proposed method by applying it to the photo collections of six participants consisting of 5,480 photos in total. We observe that our sub-event segmentation based on sentiment features outperforms the existing baseline methods. Furthermore, the proposed method is also more effective in finding sub-event boundaries and key photos, because it focuses on detailed sentiment features instead of general content features.","Personal photo collection, Event segmentation, Key photo selection, Summarization, Sentiment analysis",Junghyun Bum and Joyce Jiyoung Whang and Hyunseung Choo,https://www.sciencedirect.com/science/article/pii/S1047320320301954,https://doi.org/10.1016/j.jvcir.2020.102973,1047-3203,2021,102973,74,Journal of Visual Communication and Image Representation,Sentiment-based sub-event segmentation and key photo selection,article,BUM2021102973
"Bringing a single still image into reality is a challenging topic in computer animation because the driven and structural information in single still image is inadequate. In this paper, we present an image animating method for enhancing single still image in social media with virtual realistic and animated motions without prior information. We imitate the interaction between the active objects in an image and their neighboring passive objects. The existing actions in the image and the virtual specified force are employed to animate the active objects. Observing that the change between two subsequent motions of the active objects derives a motion tendency, we can calculate a virtual driving force based on the motion tendency. By virtue of the virtual driving force, the stochastic motion texture is used to animate the passive objects. Finally, the convolutional neural network is employed to optimize the virtual motion animations. In this way, the proposed method produces visually natural results while guaranteeing motion harmony between active objects and passive objects. To demonstrate the applicability and rationality of virtual animation driving force, our method generates several animations from still images in Social Media.","Animation, Convolutional neural network, Image motion analysis, Shape context, Stochastic motion texture",Tao Hu and Chao Liang and Geyong Min and Keqin Li and Chunxia Xiao,https://www.sciencedirect.com/science/article/pii/S1047320320300626,https://doi.org/10.1016/j.jvcir.2020.102812,1047-3203,2020,102812,71,Journal of Visual Communication and Image Representation,Generating video animation from single still image in social media based on intelligent computing,article,HU2020102812
"Image stitching is a traditional but challenging computer vision task, aiming to obtain a seamless panoramic image. Recently, researchers begin to study the image stitching task using deep learning. However, the existing learning methods assume a relatively fixed view during the image capturing, thus show a poor generalization ability to flexible view cases. To address the above problem, we present a cascaded view-free image stitching network based on a global homography. This novel image stitching network does not have any restriction on the view of images and it can be implemented in three stages. In particular, we first estimate a global homography between two input images from different views. And then we propose a structure stitching layer to obtain the coarse stitching result using the global homography. In the last stage, we design a content revision network to eliminate ghosting effects and refine the content of the stitching result. To enable efficient learning on various views, we also present a method to generate synthetic datasets for network training. Experimental results demonstrate that our method can achieve almost 100% elimination of artifacts in overlapping areas at the cost of acceptable slight distortions in non-overlapping areas, compared with traditional methods. In addition, the proposed method is view-free and more robust especially in a scene where feature points are difficult to detect.","41A05, 41A10, 65D05, 65D17",Lang Nie and Chunyu Lin and Kang Liao and Meiqin Liu and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S1047320320301784,https://doi.org/10.1016/j.jvcir.2020.102950,1047-3203,2020,102950,73,Journal of Visual Communication and Image Representation,A view-free image stitching network based on global homography,article,NIE2020102950
"For patients with glioblastoma multiform (GBM), differentiating pseudoprogression (PsP) from true tumor progression (TTP) is a challenging and time-consuming task for radiologists. Although deep neural networks can automatically diagnose PsP and TTP, lacking of interpretability has always been its major drawback. To overcome these shortcomings and produce more reliable outcomes, we propose a transparency-guided ensemble convolutional neural network (CNN) to automatically discriminate PsP and TTP in magnetic resonance imaging (MRI). A total of 84 patients with GBM were enrolled in the study. First, three typical convolutional neutral networks, namely VGG, ResNet and DenseNet, were trained to distinguish PsP and TTP. Subsequently, we used class-specific gradient information from convolutional layers to highlight the important regions in MRI scans. And radiologists selected the most lesion-relevant layer for each CNN. Finally, the selected layers are utilized to guide the construction of a multi-scale ensemble CNN whose classification accuracy reached 90.20%, and whose specificity is promoted 20% than that of a single CNN. The results demonstrate the presented network can enhance the reliability and accuracy of CNNs.","Pseudo progression, Glioblastoma multiforme, Diffusion tensor imaging (DTI), Convolutional networks understanding, Ensemble CNN",Xiaoming Liu and Xiaobo Zhou and Xiaohua Qian,https://www.sciencedirect.com/science/article/pii/S1047320320301267,https://doi.org/10.1016/j.jvcir.2020.102880,1047-3203,2020,102880,72,Journal of Visual Communication and Image Representation,Transparency-guided ensemble convolutional neural network for the stratification between pseudoprogression and true progression of glioblastoma multiform in MRI,article,LIU2020102880
"Recently, video action recognition about two-stream network is still a popular research topic in computer vision. However, most of current two-stream-based methods have two redundancy issues, including: inter-frame redundancy and intra-frame redundancy. To solve the above problems, a Spatial-Temporal Saliency Action Mask Attention network (STSAMANet) is built for action recognition. First, this paper introduces a key-frame mechanism to eliminate inter-frame redundancy. This mechanism can compute key frames on each video sequence to get the greatest difference between frames. Then, Mask R-CNN detection technology is introduced to build a saliency attention layer to eliminate intra-frame redundancy. This layer is to focus on the saliency human body and objects for each action class. We experiment on two public video action datasets, i.e., the UCF101 dataset and Penn Action dataset to verify the effectiveness of our method in action recognition.","Action recognition, Two-stream, Saliency attention, Key-frame",Min Jiang and Na Pan and Jun Kong,https://www.sciencedirect.com/science/article/pii/S1047320320300973,https://doi.org/10.1016/j.jvcir.2020.102846,1047-3203,2020,102846,71,Journal of Visual Communication and Image Representation,Spatial-temporal saliency action mask attention network for action recognition,article,JIANG2020102846
"Bag-of-Visual Words (BoVW) and deep learning techniques have been widely used in several domains, which include computer-assisted medical diagnoses. In this work, we are interested in developing tools for the automatic identification of Parkinsonâs disease using machine learning and the concept of BoVW. The proposed approach concerns a hierarchical-based learning technique to design visual dictionaries through the Deep Optimum-Path Forest classifier. The proposed method was evaluated in six datasets derived from data collected from individuals when performing handwriting exams. Experimental results showed the potential of the technique, with robust achievements.","Parkinsonâs disease, Optimum-path forest, Handwriting dynamics, Hierarchical representation",Luis C.S. Afonso and Clayton R. Pereira and Silke A.T. Weber and Christian Hook and Alexandre X. FalcÃ£o and JoÃ£o P. Papa,https://www.sciencedirect.com/science/article/pii/S1047320320300730,https://doi.org/10.1016/j.jvcir.2020.102823,1047-3203,2020,102823,71,Journal of Visual Communication and Image Representation,Hierarchical learning using deep optimum-path forest,article,AFONSO2020102823
"Depth segmentation has the challenge of separating the objects from their supporting surfaces in a noisy environment. To address the issue, a novel segmentation scheme based on disparity analysis is proposed. First, we transform a depth scene into the corresponding U-V disparity map. Then, we conduct a region-based detection method to divide the object region into several targets in the processed U-disparity map. Thirdly, the horizontal plane regions may be mapped as slant lines in the V-disparity map, the Random Sample Consensus (RANSAC) algorithm is improved to fit such multiple lines. Moreover, noise regions are reduced by image processing strategies during the above processes. We respectively evaluate our approach on both real-world scenes and public data sets to verify the flexibility and generalization. Sufficient experimental results indicate that the algorithm can efficiently segment and label a full-view scene into a group of valid regions as well as removing surrounding noise regions.","Depth scene segmentation, UâV disparity map, Projection characteristics analysis, Object detection, RANSAC algorithm",Xiaohan Li and Lu Chen and Shuang Li and Xiang Zhou,https://www.sciencedirect.com/science/article/pii/S1047320320301541,https://doi.org/10.1016/j.jvcir.2020.102920,1047-3203,2020,102920,73,Journal of Visual Communication and Image Representation,Depth segmentation in real-world scenes based on UâV disparity analysis,article,LI2020102920
"While some denoising methods based on deep learning achieve superior results on synthetic noise, they are far from dealing with photographs corrupted by realistic noise. Denoising on real-world noisy images faces more significant challenges due to the source of it is more complicated than synthetic noise. To address this issue, we propose a novel network including noise estimation module and removal module (NERNet). The noise estimation module automatically estimates the noise level map corresponding to the information extracted by symmetric dilated block and pyramid feature fusion block. The removal module focuses on removing the noise from the noisy input with the help of the estimated noise level map. Dilation selective block with attention mechanism in the removal module adaptively not only fuses features from convolution layers with different dilation rates, but also aggregates the global and local information, which is benefit to preserving more details and textures. Experiments on two datasets of synthetic noise and three datasets of realistic noise show that NERNet achieves competitive results in comparison with other state-of-the-art methods.","Image denoising, Convolutional neural networks, Attention mechanism, Dilated convolution, Dilation rate selecting",Bingyang Guo and Kechen Song and Hongwen Dong and Yunhui Yan and Zhibiao Tu and Liu Zhu,https://www.sciencedirect.com/science/article/pii/S1047320320301024,https://doi.org/10.1016/j.jvcir.2020.102851,1047-3203,2020,102851,71,Journal of Visual Communication and Image Representation,NERNet: Noise estimation and removal network for image denoising,article,GUO2020102851
"Sketch based image retrieval (SBIR), which uses free-hand sketches to search the images containing similar objects/scenes, is attracting more and more attentions as sketches could be got more easily with the development of touch devices. However, this task is difficult as the huge differences between sketches and images. In this paper, we propose a cross-domain representation learning framework to reduce these differences for SBIR. This framework aims to transfer sketches to images with the information learned both in the sketch domain and image domain by the proposed domain migration generative adversarial network (DMGAN). Furthermore, to reduce the representation gap between the generated images and natural images, a similarity learning network (SLN) is also proposed with the new designed loss function incorporating semantic information. Extensive experiments have been done from different aspects, including comparison with state-of-the-art methods. The results show that the proposed DMGAN and SLN really work for SBIR.","Sketch based image retrieval, Cross-domain learning, Generative adversarial learning, Similarity learning",Cong Bai and Jian Chen and Qing Ma and Pengyi Hao and Shengyong Chen,https://www.sciencedirect.com/science/article/pii/S1047320320300857,https://doi.org/10.1016/j.jvcir.2020.102835,1047-3203,2020,102835,71,Journal of Visual Communication and Image Representation,Cross-domain representation learning by domain-migration generative adversarial network for sketch based image retrieval,article,BAI2020102835
"Neural machine translation has improved the translation accuracy greatly and received great attention of the machine translation community. Tree-based translation models aim to model the syntactic or semantic relation among long-distance words or phrases in a sentence. However, it faces the difficulties of expensive manual annotation cost and poor automatic annotation accuracy. In this paper, we focus on how to encode a source sentence into a vector in a unsupervised-tree way and then decode it into a target sentence. Our model incorporates Gumbel Tree-LSTM, which can learn how to compose tree structures from plain text without any tree annotation. We evaluate the proposed model on both spoken and news corpora, and show that the performance of our proposed model outperforms the attentional seq2seq model and the Transformer base model.","Neural machine translation, Tree to sequence, Gumbel Tree-LSTM",Chao Su and Heyan Huang and Shumin Shi and Ping Jian and Xuewen Shi,https://www.sciencedirect.com/science/article/pii/S1047320320300614,https://doi.org/10.1016/j.jvcir.2020.102811,1047-3203,2020,102811,71,Journal of Visual Communication and Image Representation,Neural machine translation with Gumbel Tree-LSTM based encoder,article,SU2020102811
"Story visualization is a novel and challenging topic that intersects computer vision and natural language processing, which needs to generate sequential images based on a story. It is related to text-to-image generation and video generation. Apart from ensuring the quality of the results, the synthesized images of story visualization are supposed to be consistent with each other and reflect the input story. In order to improve the performance of generated sequential images, we have developed the baseline model StoryGAN. Firstly, we use Dilated Convolution in the discriminators to expand the receptive field of the convolution kernel in the feature maps, thus enhancing the quality of the generated sequential images. In addition, Weighted Activation Degree (WAD) is introduced in the discriminators to provide a robust evaluation in view of similarity between the generated images and the target story, which results in enhancement on the consistency between the generated images and the target story. Last but not least, Bi-GRU stores the historical and future information of each sentence to effectively extract the textual features. Whatâs more, in order to make full use of the features of the long story features, Gated Convolution is used to replace the original MLP in the Initial State Encoder to improve the consistence between the generated sequential images. Experimental results and visual sequential images demonstrate the outperformance of the model we develop, compared with the other models.","Story visualization, Weighted Activation Degree (WAD), Dilated Convolution, Gated Convolution",Chunye Li and Liya Kong and Zhiping Zhou,https://www.sciencedirect.com/science/article/pii/S1047320320301826,https://doi.org/10.1016/j.jvcir.2020.102956,1047-3203,2020,102956,73,Journal of Visual Communication and Image Representation,Improved-StoryGAN for sequential images visualization,article,LI2020102956
"In clinical analysis and diagnosis, high resolution (HR) computed tomography (CT) images are required for proper treatment of a patient. Developing HR medical images by X-ray CT devices require extended radiation exposure with large radiative dosages, putting the patient at potential risk of inducing cancer. So, radiation exposure should be reduced. However, photon starvation and beam hardening in low-dose X-rays will cause severe artifacts. Thus, an accurate reconstruction of low-dose X-ray CT images is required. To this end, we propose a wavelet based multi-channel and multi-scale cross connected residual-in-dense grouped convolutional neural network (WCRDGCNN) for accurate super resolution (SR) of medical images. The adopted filter groups reduce the connection weights, thereby reducing the computational complexity. Gradient vanishing problem is tackled by using residual and dense skip connections. The extensive experimentation results on benchmark datasets show that our method outperforms the state-of-the-art SR methods.","Grouped convolution, Low-dose X-ray CT, Residual-in-dense, Super resolution, Wavelet sub-bands",Gadipudi Amaranageswarao and S. Deivalakshmi and Seok-Bum Ko,https://www.sciencedirect.com/science/article/pii/S1047320320300699,https://doi.org/10.1016/j.jvcir.2020.102819,1047-3203,2020,102819,70,Journal of Visual Communication and Image Representation,Wavelet based medical image super resolution using cross connected residual-in-dense grouped convolutional neural network,article,AMARANAGESWARAO2020102819
"A lot of image steganographic techniques (also called data hiding) conceal secret data by utilizing a reference matrix (RM), and some new types of RMs, such as the turtle shell, an octagon-shaped shell, the Sudoku table, and so on, have been proposed in recent years. In this article, we present a novel type of RM called anisotropic RM. By employing a full search strategy, we have found the optimal parameters for constructing the anisotropic RM. To judge the performance of a parameter set quickly, a theoretical peak signal-to-noise ratio (PSNR) evaluation method is proposed. In addition, we extend the RM to three-dimensional (3D) space. Experimental results show that our data hiding scheme, based on the anisotropic RM, has a better quality stego image than previous methods. Moreover, the 3D RM works better than the traditional 2D RM using the same embedding capacity.","Anisotropic reference matrix, Data hiding, Steganography, Visual quality, Embedding capacity",Juan Lin and Ji-Hwei Horng and Yanjun Liu and Chin-Chen Chang,https://www.sciencedirect.com/science/article/pii/S1047320320301942,https://doi.org/10.1016/j.jvcir.2020.102969,1047-3203,2021,102969,74,Journal of Visual Communication and Image Representation,An anisotropic reference matrix for image steganography,article,LIN2021102969
"Sepsis is the third-highest mortality disease in intensive care units (ICUs). In this paper, we proposed a deep learning model for predicting the severity of sepsis patients. Most existing models based on attention mechanisms do not fully utilize knowledge graph based information for different organ systems, such that might constitute crucial features for predicting the severity of sepsis patients. Therefore, we have employed a medical knowledge graph as a reliable and robust source of side information. End-to-end neural networks that incorporate analyses of various organ systems simultaneously and intuitively were developed in the proposed model to reflect upon the condition of patients in a timely fashion. We have developed a pre-training technique in the proposed model to combine it with labeled data by multi-task learning. Experimental results on real-world clinical datasets, MIMIC-III and eIR, demonstrate that our model outperforms state-of-the-art models in predicting the severity of sepsis patients.","Deep neural networks, Sepsis, Intensive care units, Clinical informatics, Illness severity prediction, Knowledge graph",Qing Li and Lili Li and Jiang Zhong and L. Frank Huang,https://www.sciencedirect.com/science/article/pii/S1047320320301425,https://doi.org/10.1016/j.jvcir.2020.102901,1047-3203,2020,102901,72,Journal of Visual Communication and Image Representation,Real-time sepsis severity prediction on knowledge graph deep learning networks for the intensive care unit,article,LI2020102901
"Object-specific edge detection (OSED) aims to detect object edges in an image along with classify the edge into object or non-object. It prunes edges which are not belonging to the object class for following processing, such as, feature matching for object detection, localization and three-dimensional reconstruction. In this paper, an OSED method that combines region proposal detectors with deep supervision nets to identify object-specific edges is proposed. It minimizes errors of object proposal by learning from hidden layers. Additionally, it combines features from different scales to detect object edges. In order to evaluate the performance of the OSED, we present two datasets which are captured in real scenes. The OSED method demonstrates a high accuracy of 90% and a high speed of 0.5 s for an image whose size is 512Â ÃÂ 448 pixels on the proposed datasets.","Region proposal, Edge detection, Deep supervision, Convolutional neural network",Ling Xiao and Bo Wu and Youmin Hu,https://www.sciencedirect.com/science/article/pii/S104732032030153X,https://doi.org/10.1016/j.jvcir.2020.102918,1047-3203,2020,102918,72,Journal of Visual Communication and Image Representation,OSED: Object-specific edge detection,article,XIAO2020102918
"Data hiding in multiple images has been a significant research direction in information security. How to reasonably design the embedding strategy to spread the payload among multiple images is still an open issue. In this paper, we propose an embedding strategy on fusing multiple features. We utilize the typical characteristic parameters of gray level co-occurrence matrix, the image entropy and the shape parameter to describe image complexity. Furthermore, we combine with the number of cover images, the number of cover images assigned to steganographer and the size of cover image to estimate the steganographic capacity of each image. The strategy is implemented together with some state-of-the-art single image steganographic algorithms. Experimental results demonstrate that the security performance of the proposed strategy is higher than that of the state-of-the-art embedding strategy against the blind universal pooled steganalysis.","Multiple images steganography, Embedding strategy, Multiple image features, Image complexity, Steganographic capacity",Junxue Yang and Xin Liao,https://www.sciencedirect.com/science/article/pii/S1047320320300729,https://doi.org/10.1016/j.jvcir.2020.102822,1047-3203,2020,102822,71,Journal of Visual Communication and Image Representation,An embedding strategy on fusing multiple image features for data hiding in multiple images,article,YANG2020102822
"In this work, a framework that can automatically create cartoon images with low computation resources and small training datasets is proposed. The proposed system performs region segmentation and learns a region relationship tree from each learning image. The segmented regions are clustered automatically with an enhanced clustering mechanism with no prior knowledge of number of clusters. According to the topology represented by region relationship tree and clustering results, the regions are reassembled to create new images. A swarm intelligence optimization procedure is designed to coordinate the regions to the optimized sizes and positions in the created image. Rigid deformation using moving least squares is performed on the regions to generate more variety for created images. Compared with methods based on Generative Adversarial Networks, the proposed framework can create better images with limited computation resources and a very small amount of training samples.","Image creation, Clustering, Convolutional neural networks",Hsu-Yung Cheng and Chih-Chang Yu,https://www.sciencedirect.com/science/article/pii/S1047320320301140,https://doi.org/10.1016/j.jvcir.2020.102863,1047-3203,2020,102863,71,Journal of Visual Communication and Image Representation,Low-resource automatic cartoon image creation from limited samples,article,CHENG2020102863
"Long-term tracking is one of the most challenging problems in computer vision. In this paper, we make full use of the Discriminative Correlation Filter (DCF), and propose a real-time long-term tracker by exploiting a joint trackingâverificationâdetectionârefinement framework. We utilize a DCF which is updated aggressively to estimate translation and scale variation of the target. Subsequently, a passively updated DCF checks the reliability of the tracking result. Once the result is not reliable, we evoke the proposed optimized candidate detector to generate a small number of relatively high quality candidates. Finally, one DCF with an adaptive online learning rate is adopted to refine the predictions that the sparse candidates inferred. In addition, we employ a selection mechanism for the correlation responses to maintain reliable samples effectively. Extensive experiments show that the proposed method performs favorably against lots of state-of-the-art methods while running more than 30 frames per second on single CPU.","Correlation filter, Occlusion, Long-term, Detector, Real-time",Jiawen Liao and Chun Qi and Jianzhong Cao and Long Ren and Gaopeng Zhang,https://www.sciencedirect.com/science/article/pii/S1047320320301383,https://doi.org/10.1016/j.jvcir.2020.102896,1047-3203,2020,102896,72,Journal of Visual Communication and Image Representation,Real-time long-term tracker with trackingâverificationâdetectionârefinement,article,LIAO2020102896
"Differentiable renderer is widely used in optimization-based 3D reconstruction which requires gradients for optimization. The existing differentiable renderers obtain gradients via numerical techniques. However, these methods are inaccurate and inefficient. Motivated by this fact, we propose a differentiable renderer with analytical gradients. The main obstacle of traditional renderer being differentiable is the discrete sampling operation of rasterization. To obtain a differentiable rasterization renderer, we define pixel intensity as a double integral over the pixel grid, and then derive the analytical gradients with respect to vertices. 3D pose estimation by multi-viewpoint silhouettes is conducted to reveal the effectiveness and efficiency of the proposed method. Experimental results show that 3D pose estimation without 3D and 2D joint supervision can produce competitive results. The findings also indicate that the proposed method has higher accuracy and efficiency than previous differentiable renderers.","Inverse graphics, Differentiable renderer, 3D pose estimation",Zaiqiang Wu and Wei Jiang and Hongyan Yu,https://www.sciencedirect.com/science/article/pii/S1047320320301851,https://doi.org/10.1016/j.jvcir.2020.102960,1047-3203,2020,102960,73,Journal of Visual Communication and Image Representation,Analytical derivatives for differentiable renderer: 3D pose estimation by silhouette consistency,article,WU2020102960
"Early activity recognition is a classification task before the completion of activity. The study of early activity recognition is beneficial to avoid serious result. Previous studies have focused on extracting effective activity features and modeling for quick and accurate classification. It is challenging because of lack of available information. In order to get a firm basis for judgment, this paper adds an activity prediction module prior to recognition module. The main task of the module is to predict subsequent motions according to observed motions. To avoid motion blur, the structure of GAN (Generative Adversarial Networks) is used to generate the predicted motions. Compared with the traditional deep learning model, dilated neural network has advantages in large-span spatiotemporal feature modeling. The dilated RNN (Recurrent Neural Networks) and CNN (Convolutional Neural Networks) are introduced to the recognition module. In order to make the activity prediction and recognition modules work together, this paper designs and introduces a hard class mining mechanism to improve the learning ability of hard class samples. The proposed method is validated on four skeletal activity datasets and achieves state-of-the-art accuracy.","Early activity recognition, Activity prediction, Skeleton, GAN",Ran Cui and Gang Hua and Jingran Wu,https://www.sciencedirect.com/science/article/pii/S1047320320301577,https://doi.org/10.1016/j.jvcir.2020.102923,1047-3203,2020,102923,73,Journal of Visual Communication and Image Representation,AP-GAN: Predicting skeletal activity to improve early activity recognition,article,CUI2020102923
"Zero-shot classification methods have attracted considerable attention in recent years. Existing ZSC methods encounter domain shift, hubness and visualâsemantic gap problems. To address these problems, we propose a low-rank embedded orthogonal subspace learning method (LEOSL) for ZSC. Many previous works project visual features to the semantic space. However, they often suffer from the visualâsemantic gap problem. To handle this problem, we project the visual representations and semantic representations to the common subspace. To address the domain shift problem, we restrict the mapping functions with a low-rank constraint. To handle the hubness problem, we introduce the class similarity term so that samples of the same class are located near each other, while samples of different classes are located far away. Furthermore, we restrict the shared representations in the subspace with an orthogonal constraint to remove the correlation between samples. The results show the superiority of LEOSL compared to many state-of-the-art methods.","Zero shot classification, Low-rank, Class similarity, Manifold structure",Xiao Li and Min Fang and Jichuan Liu,https://www.sciencedirect.com/science/article/pii/S1047320320302029,https://doi.org/10.1016/j.jvcir.2020.102981,1047-3203,2021,102981,74,Journal of Visual Communication and Image Representation,Low-rank embedded orthogonal subspace learning for zero-shot classification,article,LI2021102981
"Tracking of moving vehicles and pedestrians is the most important application in traffic surveillance videos. This study develops a highly efficient and fast multi-object tracking method using three-frame differencing-combined-background subtraction (TFDCBS)-coupled-automatic and fast histogram-entropy-based thresholding (HEBT) method together with GMPFM-GMPHD filters and VGG16-LSTM classifier. Here TFDCBS-HEBT methods identify the targeted objects with enclosed 3D bounding boxes and extracts multiple features from the raw images. Maximum number of error-free extracted multiple features (key points, multiple local convolutions, corners, and descriptors) are processed subsequently for object tracking by GMPFM-GMPHD Filters and an upgraded VGG16- LSTM classifier. The proposed method has been validated on KITTI 3D bounding box-dataset and its performance compared with three state-of-the-art tracking methods. Highest values of several performance parameters and the lowest computation time clearly demonstrate the promising feature of our new method for its application towards a fast and effective multi-target tracking of moving objects.","Tracking, Vehicles, Pedestrians, Gaussian mixture particle filter model, VGG16 and LSTM",K. Silpaja Chandrasekar and P. Geetha,https://www.sciencedirect.com/science/article/pii/S1047320320301450,https://doi.org/10.1016/j.jvcir.2020.102905,1047-3203,2020,102905,72,Journal of Visual Communication and Image Representation,Multiple objects tracking by a highly decisive three-frame differencing-combined-background subtraction method with GMPFM-GMPHD filters and VGG16-LSTM classifier,article,CHANDRASEKAR2020102905
"Convolutional neural networks (CNNs) with large model size and computing operations are difficult to be deployed on embedded systems, such as smartphones or AI cameras. In this paper, we propose a novel structured pruning method, termed the structured feature sparsity training (SFST), to speed up the inference process and reduce the memory usage of CNNs. Unlike other existing pruning methods, which require multiple iterations of pruning and retraining to ensure stable performance, SFST only needs to fine-tune the pretrained model with additional regularization on the less important features and then prune them, no multiple pruning and retraining needed. SFST can be deployed to a variety of modern CNN architectures including VGGNet, ResNet and MobileNetv2. Experimental results on CIFAR, SVHN, ImageNet and MSTAR benchmark dataset demonstrate the effectiveness of our scheme, which achieves superior performance over the state-of-the-art methods.","Convolutional neural network, CNN compression, Structured sparsity, Pruning criterion",Wei Wang and Liqiang Zhu,https://www.sciencedirect.com/science/article/pii/S1047320320301176,https://doi.org/10.1016/j.jvcir.2020.102867,1047-3203,2020,102867,71,Journal of Visual Communication and Image Representation,Structured feature sparsity training for convolutional neural network compression,article,WANG2020102867
"Weakly supervised instance segmentation is a new research topic in the field of computer vision. Compared with fully supervised instance segmentation, weakly supervised methods use weaker data annotations such as points, scribbles or class labels which are easy to obtain. Among these annotations, image-level instance segmentation using only class labels as supervision is the most challenging task. In this paper, we propose a novel weakly supervised instance segmentation framework using a multi-stage erasing refinement method and a saliency-guided proposals ordering method. Firstly, the multi-stage erasing refinement method is exploited to enhance the instance representation by iteratively discovering separate object-related regions, so as to obtain more complete discriminative regions. Then, the saliency-guided proposals ordering method utilizes the saliency map to alleviate the background noise and better select the object proposals for generating the instance segmentation result. Experimental results on the PASCAL VOC 2012 dataset and the COCO dataset demonstrate that our framework achieves superior performance compared with the state-of-the-art weakly supervised instance segmentation models and the ablation study shows the effectiveness of the proposed two methods.","Weakly supervised instance segmentation, Image-level annotations, Multi-stage erasing refinement, Saliency-guided proposals ordering",Zheng Hu and Zhi Liu and Gongyang Li and Linwei Ye and Lei Zhou and Yang Wang,https://www.sciencedirect.com/science/article/pii/S104732032030184X,https://doi.org/10.1016/j.jvcir.2020.102957,1047-3203,2020,102957,73,Journal of Visual Communication and Image Representation,Weakly supervised instance segmentation using multi-stage erasing refinement and saliency-guided proposals ordering,article,HU2020102957
"Despite excellent performance in image classification researches, the training of the deep neural networks (DNN) needs a large set of clean data with accurate annotations. The collection of a dataset is easy, but annotating the collected data is difficult on the contrary. There are many image data on the websites, which contain inaccurate annotations, but trainings on these datasets may make networks easier to over-fit noisy data and cause performance degradation. In this work, we propose an improved joint optimization framework for noise correction, which uses the Combination of Mix-up entropy and Kullback-Leibler entropy (CMKL) as the loss function. The new loss function can achieve better fine-tuning results after updating all label annotations. The experimental results on publicly available CIFAR-10 dataset and Clothing1M dataset show superior performance of our approach compared with other state-of-the-art methods.","Noisy labels, KL entropy, Mix-up loss, DNN",Qian Zhang and Feifei Lee and Ya-gang Wang and Ran Miao and Lei Chen and Qiu Chen,https://www.sciencedirect.com/science/article/pii/S1047320320301619,https://doi.org/10.1016/j.jvcir.2020.102930,1047-3203,2020,102930,72,Journal of Visual Communication and Image Representation,An improved noise loss correction algorithm for learning from noisy labels,article,ZHANG2020102930
"Hand Pose Estimation aims to predict the position of joints on a hand from an image, and it has become popular because of the emergence of VR/AR/MR technology. Nevertheless, an issue surfaces when trying to achieve this goal, since a hand tends to cause self-occlusion or external occlusion easily as it interacts with external objects. As a result, there have been many projects dedicated to this field for a better solution of this problem. This paper develops a system that accurately estimates a hand pose in 3D space using depth images for VR applications. We propose a data-driven approach of training a deep learning model for hand pose estimation with object interaction. In the convolutional neural network (CNN) training procedure, we design a skeleton-difference loss function, which effectively can learn the physical constraints of a hand. Also, we propose an object-manipulating loss function, which considers knowledge of the hand-object interaction, to enhance performance. In the experiments we have conducted for hand pose estimation under different conditions, the results validate the robustness and the performance of our system and show that our method is able to predict the joints more accurately in challenging environmental settings. Such appealing results may be attributed to the consideration of the physical joint relationship as well as object information, which in turn can be applied to future VR/AR/MR systems for more natural experience.","Hand pose estimation, Deep learning, Convolutional neural network, Spherical part model",Min-Yu Wu and Pai-Wen Ting and Ya-Hui Tang and En-Te Chou and Li-Chen Fu,https://www.sciencedirect.com/science/article/pii/S1047320320300523,https://doi.org/10.1016/j.jvcir.2020.102802,1047-3203,2020,102802,70,Journal of Visual Communication and Image Representation,Hand pose estimation in object-interaction based on deep learning for virtual reality applications,article,WU2020102802
"For stereo matching based on patch comparing using convolutional neural networks (CNNs), the matching cost estimation is highly dependent on the network structure, and the patch comparing is time consuming for traditional CNNs. Accordingly, we propose a stereo matching method based on a novel shrinking residual CNN, which consists of convolutional layers and skip-connection layers, and the size of the fully connected layers decreases progressively. Firstly, a layer-by-layer shrinking size model is adopted for the full-connection layers to greatly increase the running speed. Secondly, the convolutional layer and the residual structure are fused to improve patch comparing. Finally, the Loss function is re-designed to give higher weights to hard-classified examples compared with the standard cross entropy loss. Experimental results on KITTI2012 and KITTI2015 demonstrate that the proposed method can improve the operation speed while maintaining high accuracy.","Stereo matching, Matching cost, Residual convolutional neural network",Junfeng Lei and Yuxuan Dong and Tao Zhao and Jinsheng xiao and Yunhua Chen and Bijun Li,https://www.sciencedirect.com/science/article/pii/S1047320320301218,https://doi.org/10.1016/j.jvcir.2020.102872,1047-3203,2020,102872,72,Journal of Visual Communication and Image Representation,Novel shrinking residual convolutional neural network for efficient accurate stereo matching,article,LEI2020102872
"Blind image deblurring aims to recover the sharp image from a blurry image. The problem is seriously ill-conditioned and many existing algorithms based on kernel estimation require heuristic parameter adjustments and high computational cost, and cannot perform well on non-uniform motion blurs. To address this issue, image deblurring is viewed as an image translation problem in this paper. The authors solve it based on a conditional generative adversarial network (GAN), where the sharp image is restored by an end-to-end trainable neural network. Different from the generative network in basic conditional GAN, the proposed generator is based on dense blocks and residual network (DenseResNet), aiming to mitigate the problems of overfitting and vanishing gradient, and strengthen the blur feature propagation. To generate clear structure, the basic conditional GAN formulation is further revised by introducing joint VGG features andâL1-based gradient loss. Extensive experimental results demonstrate the superior performance of the proposed method.","Image deblurring, Conditional GAN, DenseResNet, -based gradient loss",Hongtian Zhao and Di Wu and Hang Su and Shibao Zheng and Jie Chen,https://www.sciencedirect.com/science/article/pii/S1047320320301565,https://doi.org/10.1016/j.jvcir.2020.102921,1047-3203,2021,102921,74,Journal of Visual Communication and Image Representation,Gradient-based conditional generative adversarial network for non-uniform blind deblurring via DenseResNet,article,ZHAO2021102921
"3D Human Pose Reconstruction (HPR) is a challenging task due to less availability of 3D ground truth data and projection ambiguity. To address these limitations, we propose a three-stage deep network having the workflow of 2D Human Pose Estimation (HPE) followed by 3D HPR; which utilizes the proposed Frame Specific Pose Estimation (FSPE), Multi-Stage Cascaded Feature Connection (MSCFC) and Feature Residual Connection (FRC) Sub-level Strategies. In the first stage, the FSPE concept with the MSCFC strategy has been used for 2D HPE. In the second stage, the basic deep learning concepts like convolution, batch normalization, ReLU, and dropout have been utilized with the FRC Strategy for spatial 3D reconstruction. In the last stage, LSTM deep architecture has been used for temporal refinement. The effectiveness of the technique has been demonstrated on MPII, Human3.6M, and HumanEva-I datasets. From the experiments, it has been observed that the proposed method gives competitive results to the recent state-of-the-art techniques.","Human Pose Estimation (HPE), Human Pose Reconstruction (HPR), Frame Specific Pose Estimation (FSPE), Multi-Stage Cascaded Feature Connection (MSCFC), Feature Residual Connection (FRC)",Pratishtha Verma and Rajeev Srivastava,https://www.sciencedirect.com/science/article/pii/S1047320320301164,https://doi.org/10.1016/j.jvcir.2020.102866,1047-3203,2020,102866,71,Journal of Visual Communication and Image Representation,Three stage deep network for 3D human pose reconstruction by exploiting spatial and temporal data via its 2D pose,article,VERMA2020102866
"This paper presents a technique for semi-automatic 2D-to-3D stereo video conversion, which is known to provide user intervention in assigning foreground/background depths for key frames and then get depth maps for non-key frames via automatic depth propagation. Our algorithm treats foreground and background separately. For foregrounds, kernel pixels are identified and then used as the seeds for graph-cut segmentation for each non-key frame independently, resulting in results not limited by objectsâ motion activity. For backgrounds, all video frames, after foregrounds being removed, are integrated into a common background sprite model (BSM) based on a relay-frame-based image registration algorithm. Users can then draw background depths for BSM in an integrated manner, thus reducing human efforts significantly. Experimental results show that our method is capable of retaining more faithful foreground depth boundaries (by 1.6â2.7Â dB) and smoother background depths than prior works. This advantage is helpful for 3D display and 3D perception.","Stereo video conversion, Depth propagation, Graph cut, Background sprite model",Wen-Nung Lie and Shao-Ting Chiu and Yi-Kai Chen and Jui-Chiu Chiang,https://www.sciencedirect.com/science/article/pii/S1047320320300511,https://doi.org/10.1016/j.jvcir.2020.102801,1047-3203,2020,102801,70,Journal of Visual Communication and Image Representation,Semi-automatic 2D-to-3D video conversion based on background sprite generation,article,LIE2020102801
"Saliency prediction on RGB-D images is an underexplored and challenging task in computer vision. We propose a channel-wise attention and contextual interaction asymmetric network for RGB-D saliency prediction. In the proposed network, a common feature extractor provides cross-modal complementarity between the RGB image and corresponding depth map. In addition, we introduce a four-stream feature-interaction module that fully leverages multiscale and cross-modal features for extracting contextual information. Moreover, we propose a channel-wise attention module to highlight the feature representation of salient regions. Finally, we refine coarse maps through a corresponding refinement block. Experimental results show that the proposed network achieves a performance comparable with state-of-the-art saliency prediction methods on two representative datasets.","RGB-D image, Saliency prediction, Attention mechanism, Contextual interaction",Xinyue Zhang and Ting Jin and Wujie Zhou and Jingsheng Lei,https://www.sciencedirect.com/science/article/pii/S1047320320302133,https://doi.org/10.1016/j.jvcir.2020.102997,1047-3203,2021,102997,74,Journal of Visual Communication and Image Representation,Attention-based contextual interaction asymmetric network for RGB-D saliency prediction,article,ZHANG2021102997
"As a challenging task of video classification, action recognition has become a significant topic of computer vision community. The most popular methods based on two-stream architecture up to now are still simply fusing the prediction scores of each stream. In that case, the complementary characteristics of two streams cannot be fully utilized and the effect of shallower features is often overlooked. In addition, the equal treatment to features may weaken the role of the feature contributing significantly to the classification. Accordingly, a novel network called Multiple Depth-levels Features Fusion Enhanced Network (MDFFEN) is proposed. It improves on two aspects of two-stream architecture. In terms of the two-stream interaction mechanism, multiple depth-levels features fusion (MDFF) is formed to aggregate spatialâtemporal features extracted from several sub-modules of original two streams by spatialâtemporal features fusion (STFF). And with respect to further refining the spatiotemporal features, we propose a group-wise spatial-channel enhance (GSCE) module to highlight the meaningful regions and expressive channels automatically by priority assignment. The competitive results are achieved after we validate MDFFEN on three public challenging action recognition datasets, HDMB51, UCF101 and ChaLearn LAP IsoGD.","Action recognition, Two-stream, Multiple depth-levels features fusion, Group-wise spatial-channel enhance",Shengquan Wang and Jun Kong and Min Jiang and Tianshan Liu,https://www.sciencedirect.com/science/article/pii/S1047320320301607,https://doi.org/10.1016/j.jvcir.2020.102929,1047-3203,2020,102929,73,Journal of Visual Communication and Image Representation,Multiple depth-levels features fusion enhanced network for action recognition,article,WANG2020102929
"In this paper, we propose an NCC-based object tracking deep framework, which can be well initialized with the limited target samples in the first frame. The proposed framework contains a pretrained model, online feature fine-tuning layers and tracking processes. The pretrained model provides rich feature representations while online feature fine-tuning layers select discriminative and generic features for the tracked object. We choose normalized cross-correlation as a template tracking layer to perform the tracking process. To enable the learned features representation closely coordinated to the tracked target, we jointly train the feature representation network and tracking processes. In online tracking, an adaptive template and a fixed template are fused to find the optimal tracking results. Scale estimation and a high-confidence model update scheme are perfectly integrated into the framework to adapt to the target appearance changes. The extensive experiments demonstrate that the proposed tracker achieves superior performance compared with other state-of-the-art trackers.","Visual tracking, End-to-end framework, NCC, Template matching",Kaiheng Dai and Yuehuan Wang,https://www.sciencedirect.com/science/article/pii/S104732032030050X,https://doi.org/10.1016/j.jvcir.2020.102800,1047-3203,2020,102800,70,Journal of Visual Communication and Image Representation,End-to-end DeepNCC framework for robust visual tracking,article,DAI2020102800
"Rate-distortion optimization (RDO) is conventionally based on the analysis of rate-distortion (R-D) curve to minimize the coding distortion under the coding bits constraint. However, it is necessary to consider the computational complexity in the RDO process. In this paper, we obtain the Confidence LEvel - Computational complexity (CLEC) curves which indicate the characteristics of coding tree units (CTUs). Based on the CLEC curves, a rate-distortion-complexity optimization (RDCO) algorithm is proposed to optimize R-D under given computational complexity and achieve the optimal coding performance for x265. Experimental results demonstrate that the proposed algorithm can achieve a wide range of encoding speed under a given quantization parameter (QP) whereas the original x265 can only achieve a few fixed encoding speeds, and the proposed algorithm can reduce the BD-rate and increase the BD-PSNR by 6.59% and 0.13Â dB on average under the same requirements of encoding speeds as the original x265.","Rate-distortion-complexity optimization (RDCO), X265, High Efficiency Video Coding (HEVC), Computational complexity allocation",Saiping Zhang and Fuzheng Yang and Shuai Wan,https://www.sciencedirect.com/science/article/pii/S1047320320301206,https://doi.org/10.1016/j.jvcir.2020.102870,1047-3203,2020,102870,71,Journal of Visual Communication and Image Representation,Rate-distortion-complexity optimization for x265,article,ZHANG2020102870
"No-reference quality assessment of images has received considerable attention. However, the accuracy of such assessment remains questionable because of its weak biological basis. In this paper, we propose a novel quality assessment model based on the superpixel index and biological binocular mechanisms. The technical contributions of our model are the introduction of local monocular superpixel features and three global binocular visual features. We utilize monocular superpixel segmentation to extract two types of entropies as the local visual features for accurate quality-aware feature extraction. In addition, natural scene statistics features are extracted from the binocular visual information to complement the local monocular features and quantify the naturalness of the stereoscopic images. Finally, a regression model is learned to evaluate the quality of the stereoscopic images. Experimental results from three popular databases demonstrate that the proposed model has a more reliable performance than earlier models in terms of prediction accuracy and generalizability.","Image quality evaluation, Superpixel visual patches, Human visual system, Support vector regression",Zhi Zheng and Yun Liu and Yun Liu and Baoqing Huang and Hongwei Yu,https://www.sciencedirect.com/science/article/pii/S1047320320300997,https://doi.org/10.1016/j.jvcir.2020.102848,1047-3203,2020,102848,71,Journal of Visual Communication and Image Representation,No-reference stereoscopic images quality assessment method based on monocular superpixel visual features and binocular visual featuresâ,article,ZHENG2020102848
"In this paper, we propose gradual flash fusion, a new imaging concept that enables acquisition of pseudo multi-exposure images in a passive manner. This means that our gradual flash capture does not require any user-side manipulation (taking multiple shots or varying camera settings). Continuous high-speed capture naturally contains different intensities of flash in a single shooting. The captured gradual flash images, containing different information of the same scene, are fused to generate higher-quality images, especially in a low light scenario. For gradual flash fusion, we use a Generative Adversarial Network (GAN) based approach, where the generator is a tailored convolutional Auto-Encoder for image fusion. For the training, we build a custom dataset comprising gradual flash images and corresponding ground truths. This enables supervised learning, unlike most conventional image fusion studies. Experimental results demonstrate that gradual flash fusion achieves artifact-free and noise-free results resembling ground truth, owing to supervised adversarial fusion.","Image fusion, Flash fusion, Pseudo multi-exposure, Auto-encoder, GAN, Low light enhancement",Jae-Woo Kim and Je-Ho Ryu and Jong-Ok Kim,https://www.sciencedirect.com/science/article/pii/S1047320320301449,https://doi.org/10.1016/j.jvcir.2020.102903,1047-3203,2020,102903,72,Journal of Visual Communication and Image Representation,Deep gradual flash fusion for low-light enhancement,article,KIM2020102903
"In this paper we provide a lightweight video compression scheme, Low Overhead Spatio-Temporal Video compression (Lost-Vision) scheme which is done through inter-frame and intra-frame compression. In inter-frame compression redundant frames are removed by a proposed interpolation search-based method and a lightweight edge detection technique. Then intra-frame compression is done by a proposed adaptive column dropping technique modifying an existing technique namely ICCD. At the receiver end, we propose two reconstruction filters targeting to improve reconstruction quality. Performance of our scheme in terms of energy efficiency and reconstruction quality is evaluated both theoretically and practically. In practical implementation, the proposed video compression scheme is assessed in a real environment with different terrains using a smartphones/tablet-based DTN-like network. A Comparison of our scheme with three recent works on video compression shows our scheme's dominance over the competing works with 52%, 45.6% and 53% energy in saving yet maintaining acceptable reconstruction quality.","Colour filter array, Sobel filter, Inter-frame compression, Intra-frame compression, Delay tolerant network, Wi-fi direct",Tamal Pal and Sipra Das Bit,https://www.sciencedirect.com/science/article/pii/S1047320320300638,https://doi.org/10.1016/j.jvcir.2020.102813,1047-3203,2020,102813,70,Journal of Visual Communication and Image Representation,Low overhead spatiotemporal video compression over smartphone based Delay Tolerant Network,article,PAL2020102813
"Multiview video summarization plays a crucial role in abstracting essential information form multiple videos of the same location and time. In this paper, we propose a new approach for the multiview summarization. The proposed approach uses the BIRCH clustering algorithm for the first time on the initial set of frames to get rid of the static and redundant. The work presents a new approach for shot boundary detection using frame similarity measures Jaccard and Dice. The algorithm performs effectively synchronized merging of keyframes from all camera-views to obtain the final summary. Extensive experimentation conducted on various datasets suggests that the proposed approach significantly outperforms most of the existing video summarization approaches. To state a few, a 1.5% improvement on video length reduction, 24.28% improvement in compression ratio, and 6.4% improvement in quality assessment ratio is observed on the lobby dataset.","Video summarization, Surveillance videos, Multiview video, Video partition, Clustering",Anil {Singh Parihar} and Joyeeta Pal and Ishita Sharma,https://www.sciencedirect.com/science/article/pii/S1047320320302091,https://doi.org/10.1016/j.jvcir.2020.102991,1047-3203,2021,102991,74,Journal of Visual Communication and Image Representation,Multiview video summarization using video partitioning and clustering,article,SINGHPARIHAR2021102991
"Image information is usually propagated along horizontal and vertical directions, and it is usually modeled as two-dimensional (2-D) Roesser systems. For Roesser systems, stability ensures itâs normal operation, and it is regarded as a key issue. However, note that the mentioned stability above ignores the running conditions of the system in a finite-region, which probably destroy the systemâ normal operation before the system reaches a steady state. In this paper, many attentions are focused on the classical discrete 2-D Roesser model, and output signal is analyzed in detail on some finite region. This issue is formulated as input-out finite-domain stability (I-O FRS) and it can effectively analyze output signals' transient behavior of discrete 2D Roesser model. First, for discrete 2-D Roesser model, I-O FRS concept is established under considering the effect of exogenous disturbance set W. In particular, when exogenous disturbances W are regarded as W2 and Wâ, sufficient I-O FRS criteria are formulated respectively, which are described as linear matrix inequality (LMIs) conditions. Subsequently, by designing state feedback controller, I-O finite-region stabilization is realized also under exogenous disturbances W2 and Wâ, and compact LMIs criteria are proposed accordingly. Finally, ranges of examples are described for supporting the correctness of research results.","Two-dimensional Roesser model, Input-output finite region stability, Controllers of state feedback",Guangchen Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318302979,https://doi.org/10.1016/j.jvcir.2018.11.024,1047-3203,2018,253--261,57,Journal of Visual Communication and Image Representation,Input-output finite-region stability and stabilization for discrete the 2-D Roesser model,article,ZHANG2018253
"Saliency detection has become an active topic in both computer vision and multimedia fields. In this paper, we propose a novel computational model for saliency detection by integrating the holistic center-directional map with the principal local color contrast (PLCC) map. In the proposed framework, perceptual directional patches are firstly detected based on discrete wavelet frame transform (DWFT) and sparsity criterion, then the center of the spatial distribution of the extracted directional patches are utilized to locate the salient object in an image. Meanwhile, we proposed an efficient local color contrast method, called principal local color contrast (PLCC), to compute the color contrast between the salient object and the image background, which is sufficient to highlight and separate salient objects from complex background while dramatically reduce the computational cost. Finally, by incorporating the complementary visual cues of the global center-directional map with the PLCC map, a final compounded saliency map can be generated. Extensive experiments performed on three publicly available image databases, verify that the proposed scheme is able to achieve satisfactory results compared to other state-of-the-art saliency-detection algorithms.","Saliency detection, Wavelet frame transform, Principal local color contrast, Directional patches",Muwei Jian and Wenyin Zhang and Hui Yu and Chaoran Cui and Xiushan Nie and Huaxiang Zhang and Yilong Yin,https://www.sciencedirect.com/science/article/pii/S104732031830244X,https://doi.org/10.1016/j.jvcir.2018.10.008,1047-3203,2018,1--11,57,Journal of Visual Communication and Image Representation,Saliency detection based on directional patches extraction and principal local color contrast,article,JIAN20181
"Traffic image analysis is an important application in intelligent transportation. For local featuresâ robustness to image variances, such as scale changes and occlusions, they are widely used in image classification. However, how to integrate these local features for modeling traffic images optimally is still a crucial challenge. In this paper, a novel deep learning method, geometric discriminative feature fusion (GDFF), is proposed to tackle this problem. First, we use a variety of data sets to train the general convolutional neural network (CNN), which is used to extract the features of the training and test set after deep level. Deep architecture makes it possible for people to learn more abstract and internal features that are robust to changes in viewpoint and illumination. It can fuse image geometric related local features, such as local regionsâ RGB histograms, into high level discriminative features, which can be used for better classifying complex scene images. Our frameworkâs central task is to build a structural kernel, called discriminative topological kernel. Firstly, we segment the traffic images into several regions and use a region connected graph (RCG) to model regions location relationships. We use frequent sub graph mining algorithm to mine all frequent sub structures (topologies) occurs in all training RCGs. And a selection algorithm is designed to select the k qualified topologies from the entire mined frequent topologies. We call these selected topologies geometric feature fusers, which are both high discriminative and low redundant structures in all training RCGs. Finally, given a pair of RCGs and to each geometric fuser, we extract all pairs of sub graphs sharing the same topology and calculate distance between them. All k distances are accumulated for the final kernel. The experimental result demonstrates the effectiveness of our method.","Deep features, Traffic image analysis",Haibo Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318302736,https://doi.org/10.1016/j.jvcir.2018.10.029,1047-3203,2018,163--171,57,Journal of Visual Communication and Image Representation,Geometric discriminative deep features for traffic image analysis,article,ZHANG2018163
"Motion compensation is the key technique to reduce temporal redundancy in video coding. Interpolation filters are adopted to generate the inter frame prediction for motion compensation with fractional pixel accuracy. In existing video coding standards such as H.264/AVC and HEVC, a set of predefined interpolation filters is adopted in motion compensation. However, predefined interpolation filters cannot adapt to the video content, which may compromise the coding efficiency. In this paper, a content adaptive interpolation scheme is proposed for motion compensation. In the proposed scheme, a set of adaptive interpolation filters is derived for each frame as additional interpolation filters to minimize the inter prediction difference. Rate-distortion optimization is employed to choose between the predefined interpolation filters and the derived adaptive interpolation filters to achieve the best coding performance at the low bit rates. The proposed scheme is implemented into the HM 12.1 software. Experimental results show that the proposed scheme achieves 5.13 percent, 3.42 percent and 4.07 percent bit rate saving on average compared with HEVC under the âlow delay Pâ, the âlow delay Bâ and the ârandom accessâ configurations respectively.","HEVC, Interpolation, Adaptive, Filter, Fractional samples",Xiaojie Liu and Wenpeng Ding and Yunhui Shi and Baocai Yin,https://www.sciencedirect.com/science/article/pii/S1047320318302219,https://doi.org/10.1016/j.jvcir.2018.09.006,1047-3203,2018,131--138,56,Journal of Visual Communication and Image Representation,Content adaptive interpolation filters based on HEVC framework,article,LIU2018131
"The perceptual quality of images is often affected by applied image processing techniques. Their evaluation requires tests which involve human subjects. However, in most cases, image quality assessment (IQA) should be automatic and reproducible. Therefore, in this paper, a novel no-reference IQA method is proposed. The method uses high-order derivatives to extract detailed structure deformation present in distorted images. Furthermore, it employs local features, considering that only some regions of an image carry interesting information. Then, statistics of local features are used by a support vector regression technique to provide an objective quality score. To improve the quality prediction, luminance and chrominance channels of the image are processed. Experimental results on six large-scale public IQA image datasets show that the proposed method outperforms the state-of-the-art hand-crafted and deep-learning techniques in terms of the visual quality prediction accuracy. Furthermore, the method is better than popular full-reference approaches (i.e., SSIM and PSNR).","Image quality assessment, No-reference, Local features, Support vector regression",Mariusz Oszust,https://www.sciencedirect.com/science/article/pii/S1047320318302062,https://doi.org/10.1016/j.jvcir.2018.08.019,1047-3203,2018,15--26,56,Journal of Visual Communication and Image Representation,No-reference image quality assessment with local features and high-order derivatives,article,OSZUST201815
"The images and videos captured in bad weather usually have low quality caused by reduced contrast and faded color. However, traditional techniques are not sufficient to solve the problems of halo artifacts and brightness distortion. In this paper, a multi-focus fusion method for single fog image restoration is proposed. Firstly, we estimate the global atmospheric light only in the sky regions to minimize interference from other regions. Secondly, we introduce a novel fast local Laplacian filtering with adaptive boundary constraint to optimize the transmission properly so as to reduce the halo artifacts. Finally, we remove the haze and produce a more natural effect on visual recovery by using a new multi-focus image fusion method. Experimental results show that the proposed method outperforms state-of-the-art haze removal methods in terms of efficiency and dehazing visual effect.","Image restoration, Histogram analysis, Adaptive boundary constraint, Multi-focus image fusion",Yin Gao and Yijing Su and Qiming Li and Jun Li,https://www.sciencedirect.com/science/article/pii/S1047320318301652,https://doi.org/10.1016/j.jvcir.2018.07.004,1047-3203,2018,586--595,55,Journal of Visual Communication and Image Representation,Single fog image restoration with multi-focus image fusion,article,GAO2018586
"Person re-identification aims at matching individuals across multiple camera views under surveillance systems. The major challenges lie in the lack of spatial and temporal cues, which makes it difficult to cope with large variations of lighting conditions, viewing angles, body poses and occlusions. How to extract multimodal features including facial features, physical features, behavioral features, color features, etc is still a fundamental problem in person re-identification. In this paper, we propose a novel Convolutional Neural Network, called Asymmetric Filtering-based Dense Convolutional Neural Network (AF D-CNN) to learn powerful features, which can extract different levelsâ features and take advantage of identity information. Moreover, instead of using typical metric learning methods, we obtain the ranking lists by merging Joint Bayesian and re-ranking techniques which do not need dimensionality reduction. Finally, extensive experiments show that our proposed architecture performs well on four popular benchmark datasets (CUHK01, CUHK03, Market-1501, DukeMTMC-reID).","Person re-identification, Joint Bayesian, Deep convolutional neural networks, Multimodal features",Shengke Wang and Xiaoyan Zhang and Long Chen and Huiyu Zhou and Junyu Dong,https://www.sciencedirect.com/science/article/pii/S1047320318302864,https://doi.org/10.1016/j.jvcir.2018.11.013,1047-3203,2018,262--271,57,Journal of Visual Communication and Image Representation,Asymmetric filtering-based dense convolutional neural network for person re-identification combined with Joint Bayesian and re-ranking,article,WANG2018262
"Graph-based multi-view feature extraction has attracted much attention in literature. However, conventional solutions generally rely on a manually defined affinity graph matrix, which is hard to capture the intrinsic sample relations in multiple views. In addition, the graph construction and feature extraction are separated into two independent processes which may result in sub-optimal results. Furthermore, the raw data may contain adverse noises that reduces the reliability of the affinity matrix. In this paper, we propose a novel Unsupervised Multi-view Feature Extraction with Dynamic Graph Learning (UMFE-DGL) to solve these limitations. We devise a unified learning framework which simultaneously performs dynamic graph learning and the feature extraction. Dynamic graph learning adaptively captures the intrinsic multiple view-specific relations of samples. Feature extraction learns the projection matrix that could accordingly preserve the dynamically adjusted sample relations modelled by graph into the low-dimensional features. Experimental results on several public datasets demonstrate the superior performance of the proposed approach, compared with state-of-the-art techniques.","Multi-view feature extraction, Intrinsic sample relations, Dynamic graph learning",Dan Shi and Lei Zhu and Zhiyong Cheng and Zhihui Li and Huaxiang Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318302347,https://doi.org/10.1016/j.jvcir.2018.09.019,1047-3203,2018,256--264,56,Journal of Visual Communication and Image Representation,Unsupervised multi-view feature extraction with dynamic graph learning,article,SHI2018256
"Recent studies have shown that facial attributes provide useful cues for a number of applications such as face verification. However, accurate facial attribute interpretation is still a formidable challenge in real life due to large head poses, occlusion and illumination variations. In this work, we propose a general-to-specific deep convolutional network architecture for predicting multiple attributes from a single image in the wild. First, we model the interdependencies among all attributes by joint learning them all. Second, task-aware learning is adopted to explore the disparity regarding each attribute. Finally, an attribute-aware face cropping scheme is proposed to extract more discriminative features from where a certain attribute naturally shows up. The proposed learning strategy ensures both robustness and performance of our model. Extensive experiments on two challenging publicly available datasets demonstrate the effectiveness of our architecture and the superiority to state-of-the-art alternatives.","Facial attribute, Deep convolutional network, Joint learning, Task-aware learning",Yuechuan Sun and Jun Yu,https://www.sciencedirect.com/science/article/pii/S1047320318302104,https://doi.org/10.1016/j.jvcir.2018.09.003,1047-3203,2018,83--91,56,Journal of Visual Communication and Image Representation,General-to-specific learning for facial attribute classification in the wild,article,SUN201883
"In this paper, we propose a novel expression-targeted feature learning (ETFL) method for effective facial expression recognition, which takes advantage of multi-task learning for discriminative feature learning. Specifically, the common features are firstly extracted from the lower layers of CNN. Then, based on the common features, the expression-specific features (ESF) are respectively learned for each facial expression via multi-task learning. In order to enhance the discriminability of ESF, we develop a joint loss (the combination of the center loss and a novel inter-class loss) to explicitly reduce intra-class variations while enlarging inter-class differences. Furthermore, we introduce the sample-sensitive weights and the soft-expression weights to balance the joint loss for better performance. Finally, all ESFs are combined for final classification. ETFL effectively exploits the relationship among all facial expressions, which leads to superiority feature discriminability. Experiments on public facial expression databases demonstrate the effectiveness of ETFL compared with several state-of-the-art methods.","Facial expression recognition, Multi-task learning, Feature learning, Convolutional neural network",Ying Huang and Yan Yan and Si Chen and Hanzi Wang,https://www.sciencedirect.com/science/article/pii/S1047320318301895,https://doi.org/10.1016/j.jvcir.2018.08.002,1047-3203,2018,677--687,55,Journal of Visual Communication and Image Representation,Expression-targeted feature learning for effective facial expression recognition,article,HUANG2018677
"Most existing local sparse trackers are prone to drifting away as they do not make use of discriminative information of local patches. In this paper, we propose an effective context-aware local sparse appearance model to alleviate the drift problem caused by background clutter and occlusions. First, considering that different local patches should have different impacts on the likelihood computation, we present a novel Impact Allocation Strategy (IAS) with integration of the spatial-temporal context. Varying positive impact factors are adaptively assigned to different local patches based on their ability distinguishing the spatial context, which provides discriminative information to prevent the tracker from drifting. Furthermore, we exploit temporal context to introduce some historical information for more accurate locating. Second, we present a new patch-based dictionary update method being able to update each patch independently with the validation of effectiveness. On the one hand, we introduce sparsity concentration index to check whether the local patch to be updated is a valid local patch from the target object. On the other hand, spatial context is further employed to eliminate the effect of the background. Experimental results show the superiority and competitiveness of the proposed method on the benchmark data set compared to other state-of-the-art algorithms.","Visual tracking, Local sparse representation, Spatial-temporal context, Dictionary update",Guiji Li and Manman Peng and Ke Nai and Zhiyong Li and Keqin Li,https://www.sciencedirect.com/science/article/pii/S1047320318302189,https://doi.org/10.1016/j.jvcir.2018.09.004,1047-3203,2018,92--105,56,Journal of Visual Communication and Image Representation,Visual tracking via context-aware local sparse appearance model,article,LI201892
"MEDLINE is one of the largest databases of biomedical literatures. The search results from MEDLINE for medical terms are in the form of lists of articles with PubMed IDs. To further explore and select articles that may help identify potentially interesting interactions between terms, users need to navigate through the lists of URLs to retrieve and read actual articles to find relevancies among these terms. Such work becomes extremely time consuming and unbearably tedious when each query returns tens of thousands results with an uncertain recall rate. To overcome this problem, we develop a topic-specific image indexing and presentation method for discovering interactions or relatedness of medical terms from MEDLINE, based on which a prototype tool is implemented to help discover interactions between terms of types of diseases. The merits of the method is illustrated by search examples using the tool and MEDLINE abstract dataset.","MEDLINE, Data visualization, Customized retrieval, Image presentation",Ye Wang and Lan Huang and Shuyu Guo and Leiguang Gong and Tian Bai,https://www.sciencedirect.com/science/article/pii/S1047320318302980,https://doi.org/10.1016/j.jvcir.2018.11.022,1047-3203,2019,130--137,58,Journal of Visual Communication and Image Representation,A novel MEDLINE topic indexing method using image presentation,article,WANG2019130
"A large mount of data is indispensable in deep learning. The learning results can be different because of the noise or contaminated tags. So in this paper, a controller design method is proposed to reduce the influence due to noise or damaged label. Our method is based on backstepping control method and observer. In our work, an adaptive function is designed to eliminate the influence of the unmodelable part of the system because of the contaminated tags. For the noise, the observer is used to accurately estimated and effectively compensated. Experimental results show the effectiveness of our method. Our modified system has good performance and can accurately response the input training data in the case of the unmodelable part of the system and the external noise.","Robustness, Adaptive function, Observer",Li JiGuang and Chen Xin,https://www.sciencedirect.com/science/article/pii/S1047320318302463,https://doi.org/10.1016/j.jvcir.2018.10.010,1047-3203,2018,34--38,57,Journal of Visual Communication and Image Representation,A robust enhancement system based on observer-backstepping controller,article,JIGUANG201834
"Pedestrian tracking technique is now widely used in many intelligent systems, such as video surveillance, security regions. But many methods suffer from illumination, human posture or human appendant. With the development of Convolutional Neural Networks (CNNs), deep feature can be learned. In this paper, training images will be divided into subregions to reduce the influence of human appendant, such as bags. The remain regions are almost fixed regions. Then these fixed regions will be fed into our CNNs for learning deep features. In order to copy with different sizes of training images, an arbitrarily-sized pooling layer is developed in our CNN architecture. Then, these deeply-learned feature vector can be used in pedestrian recognition. In our work, optical flow is used for pedestrian tracking. Experimental results show our proposed method can achieve pedestrian tracking effectively.","Pedestrian tracking, Convolutional neural networks, Optical flow",Honghe Huang and Yi Xu and Yanjie Huang and Qian Yang and Zhiguo Zhou,https://www.sciencedirect.com/science/article/pii/S1047320318302748,https://doi.org/10.1016/j.jvcir.2018.11.001,1047-3203,2018,172--175,57,Journal of Visual Communication and Image Representation,Pedestrian tracking by learning deep features,article,HUANG2018172
"In recent years, with the raise of the neural network and deep learning, significant progress has been achieved in the field of image recognition. Convolutional Neural Network (CNN) has been widely used in multiple image recognition tasks, but the recognition accuracy still has a lot of room for improvement. In this paper, we proposed a hybrid model CNN-GRNN to improve recognition accuracy. The model uses CNN to extract multilayer image representation and it uses General Regression Neural Network (GRNN) to classify image using the extracted feature. The CNN-GRNN model replace Back propagation (BP) neural network inside CNN with GRNN to improve generalization and robustness of CNN. Furthermore, we validate our model on the Oxford-IIIT Pet Dataset database and the Keck Gesture Dataset, the experiment result indicate that our model is superior to Gray Level Co-occurrency (GLCM),HU invariant moments, CNN and CNN_SVM on small sample dataset. Our model has favorable real-time characteristic at the same time.","Image recognition, Convolutional Neural Network (CNN), General Regression Neural Network (GRNN), Small sample, Real-time",Jiajia Zhang and Kun Shao and Xing Luo,https://www.sciencedirect.com/science/article/pii/S1047320318301810,https://doi.org/10.1016/j.jvcir.2018.07.011,1047-3203,2018,640--647,55,Journal of Visual Communication and Image Representation,Small sample image recognition using improved Convolutional Neural Network,article,ZHANG2018640
"In (t,n)-multi secret image sharing (MSIS) schemes, a number of secret images are shared among n users so that participation of at least t of them is needed to recover the shared images. Due to the high volume of images and computing complexity of secret sharing schemes, recent Boolean-based approaches are highly desirable. Unfortunately, to the best of our knowledge, existing literature on Boolean-based MSIS schemes only supports two cases: (2,n) and (n,n). In (n,n)-schemes, we lose fault tolerancy such that in the absence of even one share, secret images can not be recovered. On the other hand, (2,n)-MSIS seems to be quite restrictive for the wide range of applications that might occur in practice. It is therefore a challenging problem to propose a Boolean-based (t,n)-MSIS for tâ 2,n. The aim of this paper is to solve this problem. We further provide formal proofs of security as well as comparison with existing literature.","-secret image sharing scheme, Multi-secret image sharing, Boolean operations, Security",Saeideh Kabirirad and Ziba Eslami,https://www.sciencedirect.com/science/article/pii/S1047320318302505,https://doi.org/10.1016/j.jvcir.2018.10.014,1047-3203,2018,39--47,57,Journal of Visual Communication and Image Representation,"A (t,n)-multi secret image sharing scheme based on Boolean operations",article,KABIRIRAD201839
"In application of hyperspectral remote sensing, alteration zones are primarily detected by identifying alteration mineral assemblages, but the interpretation of alteration mineral maps is often complicated by surface materials and by minerals not directly associated with alteration. This study was conducted in the Dapingliang skarn copper deposit and its surrounding area, the Shanshan County of the Xinjiang Uygur autonomous region, China. In order to successfully detect alteration zones associated with skarns, this study identified skarns rather than alteration minerals using field spectra of skarn outcrops. In this study, skarn in pixels was identified from spectral overall shape and spectral shapes of absorption-bands; SAM (spectral angle mapper) was applied in the identification. When SAM scores of spectral overall shape were less than 0.022â¯rad, the identified skarns were mainly distributed in the contact zones of intrusive rocks and carbonates; in particular, the three identified skarns areas (R1, R2 and R3) were consistent with the skarn areas in the geological map of the study area. The field inspection of skarns showed that the identified objects of the three marked targets (A, B and C) were almost consistent with the ground objects. These obtained results demonstrated that using the field spectra, it was possible to identify skarns in the hyperspectral image. To evaluate the identified skarn zones for use in mineral exploration, the end-member spectra extracted from the image were analysed, and alteration zones were detected using these end-member spectra. Compared with these alteration zones, the identified skarn zones were more reliable for mineral exploration of the study area.","Hyperspectral remote sensing, Detection of alteration zones, Dapingliang skarn copper deposit, Spectral matching",Yuanjin Xu and Jianguo Chen and Pengyan Meng,https://www.sciencedirect.com/science/article/pii/S1047320318303043,https://doi.org/10.1016/j.jvcir.2018.11.032,1047-3203,2019,67--78,58,Journal of Visual Communication and Image Representation,"Detection of alteration zones using hyperspectral remote sensing data from Dapingliang skarn copper deposit and its surrounding area, Shanshan County, Xinjiang Uygur autonomous region, China",article,XU201967
"Recent years have witnessed great progress of salient object detection methods. However, due to the emerging complex scenes, two problems should be solved urgently: one is on the fast locating of the foreground while preserving the precision, and the other is about reducing the noise near the foreground boundary in saliency maps. In this paper, a hybrid method is proposed to ameliorate the above two issues. At first, to reduce the essential runtime of integrating the prior knowledge, a novel Prior Knowledge Learning based Region Classification (PKL-RC) method is proposed for classifying image regions and preliminarily locating foreground; furthermore, to generate more accurate saliency, a Locality-constrained Linear self-Coding based Region Clustering (LLsC-RC) model is proposed to improve the adjacency structure of the similarity graph for Manifold Ranking (MR). Experimental results demonstrate the effectiveness and superiority of the proposed method in both higher precision and better smoothness.","Salient object detection, Complex scene, Locality-constrained Linear Coding (LLC), Manifold ranking, Region classification, Region clustering",Chunlei Yang and Xiangluo Wang and Jiexin Pu and Guo-Sen Xie and Zhonghua Liu and Yongsheng Dong and Lingfei Liang,https://www.sciencedirect.com/science/article/pii/S1047320318302074,https://doi.org/10.1016/j.jvcir.2018.08.017,1047-3203,2018,27--37,56,Journal of Visual Communication and Image Representation,Hybrid of extended locality-constrained linear coding and manifold ranking for salient object detection,article,YANG201827
"3D information of an environment using stereo cameras is important information for navigation of intelligent systems. The cost, power, accuracy, and speed are four important parameters in these systems. In this article, an accurate, real-time, low-power and low-cost system is provided to extract disparity maps in a stereo vision, using FPGA hardware platform. First, a new transform based on directional graphs is proposed. Then, benefiting from this graph transform and cross-based matching method, disparity map is computed. By using optimized hardware for the proposed transform and algorithm, we have obtained an accurate, low-cost, low-power and fast stereo vision system. The proposed system is fully implemented on relatively low cost FPGA platform, XC7K160t, in order to operate as a Standalone system. This system uses 40â¯K registers, 31â¯K LUTs, 215 memory blocks, and 258 DSP blocks of this FPGA. The proposed system is tested and evaluated in Middlebury dataset. The results show that the proposed stereo system can process a HD quality video at 60 frames per second for 64 disparity levels with only 7.1% error in the final disparity map. The total power consumption of the proposed stereo vision core is about 1â¯W.","FPGA, Stereo vision, Hardware architecture, Disparity map",M. Dehnavi and M. Eshghi,https://www.sciencedirect.com/science/article/pii/S1047320318302116,https://doi.org/10.1016/j.jvcir.2018.09.002,1047-3203,2018,106--115,56,Journal of Visual Communication and Image Representation,Cost and power efficient FPGA based stereo vision system using directional graph transform,article,DEHNAVI2018106
"With the development of science and technology and the progress of computing level, the research field based on video is getting more and more attention. Video understanding is a hot and challenging topic in computer vision area. Human action recognition means to automatically analyze the ongoing actions from an unknown video or image sequence and classify them correctly. Action recognition technology is widely used in many areas, including patient monitoring system, human-computer interaction, intelligent video surveillance, virtual reality, intelligent security and other aspects. In addition, video retrieval based on content and intelligent image compression have broad application prospects, among which many methods of action recognition are used. With the increasing demand for technology, there are still some challenges of action recognition in traffic context. In this paper, we briefly review the development process of action recognition technology, and classify some relevant methods, then we summarize the overall development trend of this technology. Finally, we also discussed the practical significance of it in the future in traffic context.","Traffic, Action recognition, iDT, Deep learning, C3D",Haibo Zhang,https://www.sciencedirect.com/science/article/pii/S104732031830258X,https://doi.org/10.1016/j.jvcir.2018.10.022,1047-3203,2019,63--66,58,Journal of Visual Communication and Image Representation,The literature review of action recognition in traffic context,article,ZHANG201963
"Automatic face landmarking has received a lot of attention in the past decades. It is now mature enough to be implemented in fully autonomous video systems. As cascade-of-regression based algorithms have become state of the art in such systems, two major (and still relevant) sources of interest have slowly faded away: the need for semantic-driven learning beyond ground truth annotation, and full video chain performance i.e. tracking efficiency, which in the case of said methods strongly relates to their robustness towards shape initialization before fitting. In this paper, we investigate how data sampling using face priors can affect their performance in terms of convergence and robustness. We propose new strategies based on said priors to overcome inconsistencies observed during cascade-of-regression learning on purely random sampling-based stages. We will show that simple choices can be easily integrated within regression-based face tracking systems to increase accuracy and robustness.","Face landmarking, Regression, Sampling, Data augmentation",Romuald Perrot and Pascal Bourdon and David Helbert,https://www.sciencedirect.com/science/article/pii/S1047320318301664,https://doi.org/10.1016/j.jvcir.2018.07.006,1047-3203,2018,841--852,55,Journal of Visual Communication and Image Representation,Sampling strategies for performance improvement in cascaded face regression,article,PERROT2018841
"Structured selection of settlements is a decision-making problem. The application of an entropy-based multi-attribute decision-making method to structured selection of settlements is actually the integration of the attribute method with the geometric method, which internalizes the consideration of geometric factors into attributes. Through avoiding the inaccuracy of subjective weighting via objective weighting and quantifying the importance degree of each settlement, it can solve the difficulties in structured selection of settlement to some extent.","Settlement, Cartographic generalization, Structured selection, Entropy method, Multi-attribute decision-making",Duo Tianlin and Guo Jianzhong and Wu Fang and Zhai Renjian,https://www.sciencedirect.com/science/article/pii/S1047320318302992,https://doi.org/10.1016/j.jvcir.2018.11.026,1047-3203,2019,220--232,58,Journal of Visual Communication and Image Representation,Application of entropy-based multi-attribute decision-making method to structured selection of settlement,article,TIANLIN2019220
"Minimax algorithm is widely used for adversarial searching. It is commonly believed that searching depth and winning chance has a positive correlation, but Dana S. Nau pointed out in his 1982 research that in Pearlâs game, pathology occurs. Minimax shows a decrease of winning chance as searching depth increases. Our research proposes a possible way to fix the pathology by taking the opponentâs strategy into consideration in some specific cases. The experiment proves that the accumulation of incorrect predictions at least partially causes the pathology. Our modified version of Minimax successfully overcomes the pathology and continues to present the power of Minimax in Pearlâs game.","Minimax algorithm, Modified Minimax algorithm, Pearlâs game",Haoyang Cai and Haodong Ma and Linyu Li and Luming Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318302475,https://doi.org/10.1016/j.jvcir.2018.10.011,1047-3203,2018,23--27,57,Journal of Visual Communication and Image Representation,Can modified minimax win in Pearlâs game?,article,CAI201823
"Artificial Intelligence has attracted much of researchersâ attention in recent years. A question we always ask is: âCan machines replace human beings to some extent?â This paper aims to explore the knowledge learning for an image-annotation framework, which is an easy task for humans but a tough task for machines. This paperâs research is based on an assumption that machines have two systems of thinking, each of which handles the labels of images at different abstract levels. Based on this, a new hierarchical model for image annotation is introduced. We explore not only the relationships between the labels and the features used, but also the relationships between labels. More specifically, we divide labels into several hierarchies for efficient and accurate labeling, which are constructed using our Associative Memory Sharing method, proposed in this paper.","Image annotation, Multi-labeling, Hierarchical tree structure, Feature-pool selection",Jiwei Hu and Kin-Man Lam and Ping Lou and Quan Liu and Wupeng Deng,https://www.sciencedirect.com/science/article/pii/S1047320318302232,https://doi.org/10.1016/j.jvcir.2018.09.008,1047-3203,2018,275--286,56,Journal of Visual Communication and Image Representation,"Can a machine have two systems for recognition, like human beings?",article,HU2018275
"A photon-limited image can be represented as a pixel matrix limited by the relatively small number of collected photons. The image can also be seen as being contaminated by Poisson noise because the total number of photons follows the Poisson distribution. Through exploitation of the inherent properties of observation combined with application of a denoising method, an image can be significantly restored. In this paper, a hybrid clustering and low-rank regularization-based model (HCLR) is proposed based on the essential features of patch clustering and noise. An efficient Newton-type method is designed to optimize this biconvex problem. Experimental results demonstrate that HCLR achieves competitive denoising performance, especially for high noise levels, compared with state-of-the-art Poisson denoising algorithms.","Image restoration, Low rank, Poisson denoising, Newtonâs method",Xiaolong Zhu and Xiangchu Feng and Weiwei Wang and Xixi Jia and Rui Zhang and Ruiqiang He and Chen Xu,https://www.sciencedirect.com/science/article/pii/S1047320318302517,https://doi.org/10.1016/j.jvcir.2018.10.015,1047-3203,2018,61--68,57,Journal of Visual Communication and Image Representation,HCLR: A hybrid clustering and low-rank regularization-based method for photon-limited image restoration,article,ZHU201861
"Video object detection (VID) is a more challenging task compared with still-image object detection, which not only needs to detect objects accurately per frame but also needs to track objects for a long period of time. In order to detect objects from videos, we propose a Detection And Tracking (DAT) based tubelet generation framework. Under this framework, we first propose a detection-based tubelet generation method which can generate tubelets with more accurate bounding boxes compared with traditional tracking-based methods. On the other hand, the latter can produce a higher recall of bounding boxes than the former in general. To take advantage of their complementary attributes, we further propose a novel tubelet fusion method to combine these multi-modal information (appearance information in independent images and contextual information in videos). Our extensive experiments on the well-known ILSVRC 2016 VID dataset show that our proposed method can achieve state-of-the-art performances.","Object detection, Tubelet generation, Tubelet fusion",Bin Wang and Sheng Tang and Jun-Bin Xiao and Quan-Feng Yan and Yong-Dong Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318302876,https://doi.org/10.1016/j.jvcir.2018.11.014,1047-3203,2019,102--111,58,Journal of Visual Communication and Image Representation,Detection and tracking based tubelet generation for video object detection,article,WANG2019102
"Recently, considerable efforts have been made in feature selection to improve the original feature subspace. In this paper, we proposed a graph regularized low-rank tensor representation (GRLTR) for feature selection. We jointly incorporated the low-rank representation and the graph embedding into a unified learning framework to preserve the intrinsic global low-dimension structure and local geometrical structure of data together. According to the wide presence of multidimensional data, our proposed framework is based on tensor, which can faithfully maintain the information. To improve the performance of specific clustering task, we employed the idea of embedded-based feature selection into our model for optimizing the feature representation and clustering result simultaneously. Experimental results on six available datasets suggest our proposed approach produces superior performances compared with several state-of-the-art methods.","Unsupervised feature selection, Low-rank tensor representation, Graph embedding, Subspace clustering",Yuting Su and Xu Bai and Wu Li and Peiguang Jing and Jing Zhang and Jing Liu,https://www.sciencedirect.com/science/article/pii/S1047320318302359,https://doi.org/10.1016/j.jvcir.2018.09.020,1047-3203,2018,234--244,56,Journal of Visual Communication and Image Representation,Graph regularized low-rank tensor representation for feature selection,article,SU2018234
"Moving cast shadows detection and removal are indispensable for object detection and are the problems in visual surveillance applications which have been studied over the years. However, finding an efficient model that can handle the issue of moving cast shadow in various situations is still challenging. Unlike prior methods, we use a data-driven method without strong parametric assumptions or complex models to address the problem of moving cast shadow. In this paper, we propose a novel feature-extracting framework called Scale-Relation Multi-Layer Pooling Feature Extracting (SMPF) which includes two main tasks: (1) Scale-Relation Scheme (SRS), (2) Multi-Layer Pooling Scheme (MLPS). By leveraging the scale space, SRS firstly decomposes feature images of each shadow properties into various scales and further considers the relationship between adjacent scaled feature images of each shadow properties to extract the scale-relation features. Then, we design the multi-layer pooling scheme (MLPS) to integrate the features in a local region and to reduce the dimension of extracting features. After that, the density map is generated for various properties of shadow with low dimension. Finally, to seek the criteria for discriminating moving cast shadow, we use random forest algorithm as the ensemble decision scheme. The main contributions of this study are (1) we design the features with multi-scale which can provide abundant information to describe the moving cast shadow, (2) the multi-layer pooling scheme generates the density map to integrate and reduce the dimensions of features. Experiments on the popular benchmarks and the proposed dataset with benchmarks demonstrate that the proposed method can achieve the performances of the popular methodologies.","Shadow removal, Moving cast shadow, Scale-relation, Multi-layer pooling, Ensemble decision scheme",Chih-Wei Lin,https://www.sciencedirect.com/science/article/pii/S1047320318301548,https://doi.org/10.1016/j.jvcir.2018.06.028,1047-3203,2018,504--517,55,Journal of Visual Communication and Image Representation,Moving cast shadow detection using scale-relation multi-layer pooling features,article,LIN2018504
"This paper proposes two reversible data hiding schemes in encrypted images that provide lossless recovery of directly decrypted images. The modified homomorphic encryption is introduced to encrypt the original image so that the private-key homomorphism and public-key homomorphism are both available. To embed secret message into encrypted image, a part of encrypted pixels are replaced with new ciphertexts by homomorphic property. Finally, the original image and embedded secret message can be restored by direct decryption. Optimal visual quality is obtained by the proposed schemes, since the directly decrypted pixel value is equal to the original pixel value. And the embedding rate is further improved. In addition, the proposed schemes are suitable for compressed multimedia, which extends the application scenarios. Experimental results are presented to demonstrate the effectiveness and superiority of the proposed schemes.","Reversible data hiding, Encrypted images, Lossless recovery, Homomorphic encryption",Bing Chen and Xiaotian Wu and Yun-Shan Wei,https://www.sciencedirect.com/science/article/pii/S1047320318302918,https://doi.org/10.1016/j.jvcir.2018.11.017,1047-3203,2018,272--282,57,Journal of Visual Communication and Image Representation,Reversible data hiding in encrypted images with private-key homomorphism and public-key homomorphism,article,CHEN2018272
"In this paper an improved face recognition algorithm under degrading conditions is proposed. The proposed algorithm uses a combination of preprocessing techniques coupled with discriminative feature extractors to obtain the best distinctive features for classification. Preprocessing approach is the fusion of multi-scale Weber and enhanced complex wavelet transform. Combination of multiple feature extraction based on Gabor filters, block-based local phase quantization (LPQ) coupled with principal component analysis (PCA) proved to be very effective to improve correct rate of recognition. We have also used two known classifiers, extreme learning machine (ELM), and sparse classifier (SC), and fused their outputs to obtain best recognition rate. Experimental results show improved performance of proposed algorithm under poor illumination, partial occlusion and low-quality images in uncontrolled conditions. Our best recognition results using second version of face recognition grand challenge (FRGC 2.0.4) which is the most challenging database, indicated more than 28% improvement over previous works.","Face recognition, Global based, Block based, Decision fusion",Soodeh Nikan and Majid Ahmadi,https://www.sciencedirect.com/science/article/pii/S1047320318301949,https://doi.org/10.1016/j.jvcir.2018.08.007,1047-3203,2018,742--755,55,Journal of Visual Communication and Image Representation,A modified technique for face recognition under degraded conditions,article,NIKAN2018742
"This paper addresses the issue of robust visual tracking, in which an effective tracker based on multi-feature fusion under a collaborative local-global layer visual model is proposed. In the local layer, we implement a novel block tracker using structural local color histograms feature based on the foreground-background discrimination analysis approach. In the global layer we implement a complementary correlation filters-based tracker using HOG feature. Finally, the local and global trackers are linearly merged in the response maps level. We choose the different merging factors according to the reliability of each combined tracker, and when both of the combined trackers are unreliable, an online trained SVM detector is activated to re-detect the target. Experiments conducted on challenging sequences show that our final merged tracker achieves favorable tracking performance and outperforms several state-of-the-art trackers. Besides, performance of the implemented block tracker is evaluated by comparing with some relevant color histograms-based trackers.","Collaborative visual model, Block color tracking, Correlation filter tracking, Response maps fusion, Online re-detection",Haoyang Zhang and Guixi Liu and Zhaohui Hao,https://www.sciencedirect.com/science/article/pii/S1047320318302086,https://doi.org/10.1016/j.jvcir.2018.08.018,1047-3203,2018,1--14,56,Journal of Visual Communication and Image Representation,Robust visual tracking via multi-feature response maps fusion using a collaborative local-global layer visual model,article,ZHANG20181
"In this paper, we propose a pseudo long short-term memory (LSTM) classifier for single image vehicle classification. The proposed pseudo-LSTM (P-LSTM) uses spatially divided images rather than time-series images. In other words, the proposed method considers the divided images to be time-series frames. The divided images are formed by cropping input images using two-level spatial pyramid region configuration. Parallel convolutional networks are used to extract the spatial pyramid features of the divided images. To explore the correlations between the spatial pyramid features, we attached an LSTM classifier to the end of the parallel convolutional network and treated each convolutional network as an independent timestamp. Although LSTM classifiers are typically used for time-dependent data, our experiments demonstrated that they can also be used for non-time-dependent data. We attached one fully connected layer to the end of the network to compute a final classification decision. Experiments on an MIO-TCD vehicle classification dataset show that our proposed classifier produces a high evaluation score and is comparable with several other state-of-the-art methods.","Pseudo-LSTM classifier, Vehicle classification, Deep convolutional network",Reza Fuad Rachmadi and Keiichi Uchimura and Gou Koutaki and Kohichi Ogata,https://www.sciencedirect.com/science/article/pii/S1047320318302360,https://doi.org/10.1016/j.jvcir.2018.09.021,1047-3203,2018,265--274,56,Journal of Visual Communication and Image Representation,Single image vehicle classification using pseudo long short-term memory classifier,article,RACHMADI2018265
"Hypergraph matching utilizes high order constraints rather than unary or pairwise ones, which aims to establish a more reliable correspondence between two sets of image features. Although many hypergraph matching methods have been put forward over the past decade, it remains a challenging problem to be solved due to its combinatorial nature. Most of these methods are based on tensor marginalization, where tensor entries representing joint probabilities of the assignment are fixed during the iterations meanwhile the individual assignment probabilities evolving. This will cause some incomplete information which may hurt the matching performance. Addressing this issue, we propose a novel hypergraph matching algorithm based on tensor refining, accompanied with an alternative adjustment method to accelerate the convergence. We make a comparison between the proposed approach and several outstanding matching algorithms on three commonly used benchmarks. The experimental results validate the superiority of our method on both matching accuracy and robustness against noise and deformation.","Hypergraph matching, Probabilistic, Tensor refining",Jun Zhou and Tao Wang and Congyan Lang and Songhe Feng and Yi Jin,https://www.sciencedirect.com/science/article/pii/S1047320318302487,https://doi.org/10.1016/j.jvcir.2018.10.012,1047-3203,2018,69--75,57,Journal of Visual Communication and Image Representation,A novel hypergraph matching algorithm based on tensor refining,article,ZHOU201869
"Among the rapidly growing three-dimensional technologies, multiview displays have drawn great research interests in three-dimensional television due to their adaption to the motion parallax and wider viewing angles. However, multiview displays still suffer from dazzling discomfort on the border of viewing zones. Leveraging on the separability of scene via foreground segmentation, we propose a novel virtual view synthesis method for depth-image-based rendering to alleviate the discomfort. Foreground objects of interest are extracted to segment the whole image into multiple layers, which are further warped to the virtual viewpoint in order. To alleviate the visual discomfort, global disparity adjustments and local depth control are performed for specific objects in each layer. For the post-processing, we improve an exemplar-based inpainting algorithm to tackle the disoccluded areas. Experimental results demonstrate that our method achieves effective disparity control and generates high-quality virtual view images.","Virtual view synthesis, DIBR, Disparity control, Exemplar-based inpainting, Foreground object extraction",Dongxue Han and Hui Chen and Changhe Tu and Yanyan Xu,https://www.sciencedirect.com/science/article/pii/S1047320318302402,https://doi.org/10.1016/j.jvcir.2018.10.004,1047-3203,2018,287--295,56,Journal of Visual Communication and Image Representation,View synthesis using foreground object extraction for disparity control and image inpainting,article,HAN2018287
"The availability of high-speed 3D video sensors has greatly facilitated 3D shape acquisition of dynamic and deformable objects, but high frame rate 3D reconstruction is always degraded by spatial noise and temporal fluctuations. This paper presents a simple yet powerful dynamic 3D reconstruction improvement algorithm based on intensity video guided multi-frame 4D fusion. Temporal tracking of intensity image points (of moving and deforming objects) allows registration of the corresponding 3D model points, whose 3D noise and fluctuations are then reduced by spatio-temporal multi-frame 4D fusion. We conducted simulated noise tests and real experiments on four 3D objects using a 1000â¯fps 3D video sensor. The results demonstrate that the proposed algorithm is effective at reducing 3D noise and is robust against intensity noise. It outperforms existing algorithms with good scalability on both stationary and dynamic objects.","High-speed 3D video sensor, Multi-frame 4D fusion, Intensity tracking, Dynamic object, Noise reduction",Jie Zhang and Christos Maniatis and Luis Horna and Robert B. Fisher,https://www.sciencedirect.com/science/article/pii/S1047320318301676,https://doi.org/10.1016/j.jvcir.2018.07.007,1047-3203,2018,540--547,55,Journal of Visual Communication and Image Representation,Dynamic 3D reconstruction improvement via intensity video guided 4D fusion,article,ZHANG2018540
"How to retrieve the desired images quickly and accurately from the large scale image database has become a hot topic in the field of multimedia research. Many content-based image retrieval (CBIR) technologies already exist, but they are not always satisfactory. In many applications, the CBIR model based on machine learning relies heavily on the distance metric between samples. Although the traditional distance metric methods are simple and convenient, it is not always appropriate for CBIR tasks. In this paper, a novel distance metric learning (DML) method based on cost sensitive learning (CSL) is studied, and then it is used in a large margin distribution learning machine (LDM) to replace the traditional kernel functions. The improved LDM also takes into account CSL, and which is called CS-DLDM. Finally, CS-DLDM model is applied to CBIR tasks for implementation classification. We compare the proposed CS-DLDM model with other classifiers based on CSL. The experimental results show that the proposed CS-DLDM model not only has satisfactory classification performance but also the lowest misclassification cost, can effectively avoid the class imbalance of sample.","Content-based image retrieval, Distance metric learning, Cost sensitive learning, Classification performance, Misclassification cost, Class imbalance",Cong Jin and Shu-Wei Jin,https://www.sciencedirect.com/science/article/pii/S1047320318301950,https://doi.org/10.1016/j.jvcir.2018.08.009,1047-3203,2018,720--728,55,Journal of Visual Communication and Image Representation,Content-based image retrieval model based on cost sensitive learning,article,JIN2018720
"Emotional state analysis of facial expression is an important research content of emotion recognition. At the same time, in the medical field, the auxiliary early screening tools for depression are also urgently needed by clinics. Whether there are differences in facial expression changes between depressive patients and normal people in the same situation, and whether the characteristics can be obtained and recognized from the video images of depressive patients, so as to help doctors detect and diagnose potential depressive patients early are the contents of this study. In this paper, we introduce the videos collection process of depression patients and control group at Shandong Mental Health Center in China. The key facial features are extracted from the collected facial videos by person specific active appearance model. On the basis of locating facial features, we classified depression with the movement changes of eyes, eyebrows and corners of mouth by support vector machine. The results show that these features are effective for automatic classification of depression patients.","Depression detection, Facial expression, Video processing, Eye movement, Feature extraction",Qingxiang Wang and Huanxin Yang and Yanhong Yu,https://www.sciencedirect.com/science/article/pii/S1047320318302761,https://doi.org/10.1016/j.jvcir.2018.11.003,1047-3203,2018,228--233,57,Journal of Visual Communication and Image Representation,Facial expression video analysis for depression detection in Chinese patients,article,WANG2018228
"The detection of moving objects and the subtraction of the scene background are significant tasks for intelligent video surveillance systems as it is one among the fundamental steps. Inspired by the challenging cases yet to be resolved in Moving Object Detection (MOD), a new formulation is done to detect moving objects from video sequences based on Robust Principal Component Analysis (RPCA) principle by adopting the regularization of Total Variation (TV) norm using a convergent convex optimization algorithm. While the nuclear norm exploits the low-rank property of background, the sparsity is enhanced by the l1-norm and the foreground spatial smoothness is explored by TV regularization. The goodness of this method lies in the reduced computational complexity, quickness and on the superiority acquired in quantitative evaluation based on F-measure, Recall and Precision with respect to the state of the art methods.","Moving object detection, Low rank recovery, Background subtraction, Robust principle component analysis",B. Shijila and Anju Jose Tom and Sudhish N. George,https://www.sciencedirect.com/science/article/pii/S1047320318302244,https://doi.org/10.1016/j.jvcir.2018.09.009,1047-3203,2018,188--200,56,Journal of Visual Communication and Image Representation,Moving object detection by low rank approximation and l1-TV regularization on RPCA framework,article,SHIJILA2018188
"Compared with standard dynamic range (SDR) video, the high dynamic range (HDR) video can provide us significantly enhanced viewing experience. In particular, compared to SDR video, the HDR video has better contrast and preserves more details for the same scene. With the rapid development of HDR video compression technology, there is a lack of trusted quality measure of HDR video compression. In order to facilitate the future development of objective HDR quality assessment, we build a HDR video quality assessment database, in which the bitstream is created by compressing a series of HDR video sequences. In the compression, the quantization parameters (QP) are set to 12 levels according to the configuration of the codec. The subjective quality of each bitstream is rated by 22 viewers. It is revealed that the subject viewers have arrived at a reasonable agreement on the subjective quality of different QP levels. This paper presents the results of subjective quality assessment of HDR compressed video, which also exhibits that there is significant room to further improve the objective HDR video quality assessment algorithms.","High dynamic range (HDR), Subjective quality assessment, Video compression",Xiaofei Pan and Jiaqi Zhang and Shanshe Wang and Shiqi Wang and Yun Zhou and Wenhua Ding and Yahui Yang,https://www.sciencedirect.com/science/article/pii/S1047320318302529,https://doi.org/10.1016/j.jvcir.2018.10.016,1047-3203,2018,76--83,57,Journal of Visual Communication and Image Representation,HDR video quality assessment: Perceptual evaluation of compressed HDR video,article,PAN201876
"Recently, many graph-based algorithms are applied in the research of saliency detection, which use the border of an image as a background query. This frequently leads to undesired errors and retrieval outputs when the boundaries of the salient objects concerned touch, or connect with, the imageâs border. In this paper, a novel bottom-up saliency-detection algorithm is proposed to tackle and overcome the above issue. First, we utilize object proposals to collect the background seeds reliably. Then, the Extended Random Walk (ERW) algorithm is adopted to propagate the prior background labels to the rest of the pixels in an image. Finally, we refine the saliency map by taking both the textural and structure-information into consideration simultaneously. Experiments on publicly available data sets show that our proposed method achieves competitive results against the state-of-the-art approaches.","Saliency detection, Object proposals, Background seeds, Extended random walk",Muwei Jian and Runxia Zhao and Xin Sun and Hanjiang Luo and Wenyin Zhang and Huaxiang Zhang and Junyu Dong and Yilong Yin and Kin-Man Lam,https://www.sciencedirect.com/science/article/pii/S1047320318302803,https://doi.org/10.1016/j.jvcir.2018.11.007,1047-3203,2018,202--211,57,Journal of Visual Communication and Image Representation,Saliency detection based on background seeds by object proposals and extended random walk,article,JIAN2018202
"Many real-world applications require the estimation of human body joints for higher-level tasks as, for example, human behaviour understanding. In recent years, depth sensors have become a popular approach to obtain three-dimensional information. The depth maps generated by these sensors provide information that can be employed to disambiguate the poses observed in two-dimensional images. This work addresses the problem of 3D human pose estimation from depth maps employing a Deep Learning approach. We propose a model, named Deep Depth Pose (DDP), which receives a depth map containing a person and a set of predefined 3D prototype poses and returns the 3D position of the body joints of the person. In particular, DDP is defined as a ConvNet that computes the specific weights needed to linearly combine the prototypes for the given input. We have thoroughly evaluated DDP on the challenging âITOPâ and âUBC3Vâ datasets, which respectively depict realistic and synthetic samples, defining a new state-of-the-art on them.","3D human pose, Body limbs, Depth maps, ConvNets",Manuel J. MarÃ­n-JimÃ©nez and Francisco J. Romero-Ramirez and Rafael MuÃ±oz-Salinas and Rafael Medina-Carnicer,https://www.sciencedirect.com/science/article/pii/S1047320318301718,https://doi.org/10.1016/j.jvcir.2018.07.010,1047-3203,2018,627--639,55,Journal of Visual Communication and Image Representation,3D human pose estimation from depth maps using a deep combination of poses,article,MARINJIMENEZ2018627
"Iris segmentation is a critical step for improving the accuracy of iris recognition, as well as for medical concerns. Existing methods generally use whole eye images as input for network learning, which do not consider the geometric constrain that iris only occur in a specific area in the eye. As a result, such methods can be easily affected by irrelevant noisy pixels outside iris region. In order to address this problem, we propose the ATTention U-Net (ATT-UNet) which guides the model to learn more discriminative features for separating the iris and non-iris pixels. The ATT-UNet firstly regress a bounding box of the potential iris region and generated an attention mask. Then, the mask is used as a weighted function to merge with discriminative feature maps in the model, making segmentation model pay more attention to iris region. We implement our approach on UBIRIS.v2 and CASIA.IrisV4-distance, and achieve mean error rates of 0.76% and 0.38%, respectively. Experimental results show that our method achieves consistent improvement in both visible wavelength and near-infrared iris images with challenging scenery, and surpass other representative iris segmentation approaches.","Iris segmentation, U-Net, Attention",Sheng Lian and Zhiming Luo and Zhun Zhong and Xiang Lin and Songzhi Su and Shaozi Li,https://www.sciencedirect.com/science/article/pii/S1047320318302372,https://doi.org/10.1016/j.jvcir.2018.10.001,1047-3203,2018,296--304,56,Journal of Visual Communication and Image Representation,Attention guided U-Net for accurate iris segmentation,article,LIAN2018296
"Prototype learning aims to eliminate redundancy of large-scale data by selecting an informative subset. It is at the center of visual data analysis and processing. However, due to intrinsic structures among sample groups, the learnt prototypes are generally less representative and diversified. To alleviate this issue, we develop in this paper a structurally regularized model via âp,1-norm grouping, in which both the intra-group and inter-group structures of source data in object-space are rationally exploited. Thus, while the learnt representative prototypes are prone to distribute in different groups at the inter-group level, the grouping constraint via âp,1-norm will enforce the greatest diversity for intra-group prototypes. Considering the convexity in the formulated model, an alternative re-weighting solver is presented to efficiently solve the proposed optimization problem. Experimental results on video summarization, scene categorization and handwriting recognition demonstrate that the proposed method is considerably superior to the state-of-the-art methods in prototype learning.","Prototype, Subset selection, -norm, Sparse learning, Video summarization",Xingxing Zhang and Zhenfeng Zhu and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S1047320318302499,https://doi.org/10.1016/j.jvcir.2018.10.013,1047-3203,2018,192--201,57,Journal of Visual Communication and Image Representation,"Sparsity induced prototype learning via âp,1-norm grouping",article,ZHANG2018192
"Considering the bottleneck in improving the performance of the existing multi-temporal hyperspectral remote sensing (HSRS) image change detection methods, a HSRS image change detection solution based on tensor and deep learning is proposed in this study. At first, a tensor-based information model (TFS-Cube) of underlying features change in HSRS images is established. The wavelet texture feature change, spectral feature change and spatio-temporal autocorrelation coefficient of different-temporal related pixels are combined with three-order tensor, so as to make full use of the underlying features change information of HSRS images, optimize the organization mode and maintain the integrity of constraints between different underlying features. Secondly, a restricted Boltzmann Machine based on three-order tensor (Tensor3-RBM) is designed. The input, output and unsupervised learning of TFS-Cube tensor data are realized by multi-linear operations in Tensor3-RBMs. A large number of unlabeled samples are trained layer by layer through multilayer Tensor3-RBMs. Finally, the traditional BP neural network on the top layer of deep belief network (DBN) is replaced with support tensor machine (STM), and a deep belief network with multi-layer Tensor3-RBM and STM (TRS-DBN) is constructed. A small number of labeled samples are used for supervised learning and TRS-DBN global parameters optimization to improve the accuracy of TRS-DBN change detection. Two types of HSRS images from different sensors, AVIRIS and EO-1 Hyperion, are used as the data sources (double-temporal). Four representative experimental regions are randomly selected from the two areas covered by AVIRIS and EO-1 Hyperion HSRS images respectively (two regions in each area) to detect the land use changes. Experimental results demonstrate that TRS-DBN has higher change detection accuracy than similar methods and a good automation level.","Tensor model, Deep learning, Support tensor machine, Hyperspectral remote sensing images, Change detection",Fenghua Huang and Ying Yu and Tinghao Feng,https://www.sciencedirect.com/science/article/pii/S1047320318302773,https://doi.org/10.1016/j.jvcir.2018.11.004,1047-3203,2019,233--244,58,Journal of Visual Communication and Image Representation,Hyperspectral remote sensing image change detection based on tensor and deep learning,article,HUANG2019233
"Superpixel segmentation has been widely applied in hyperspectral image (HSI) classification. In this letter, a weighted spatial correlation representation (WSCR) method for HSI classification is proposed where an effective metric spatial correlation representation (SCR) that measures the correlation coefficient (CC) among different pixels in the superpixels is described, which fully utilizes the spatial information and structural features of superpixels. In addition, considering that the contribution of each SCR is different, the Gaussian weighted is considered. The proposed method includes the following steps: First, a superpixels image is obtained from HSI based on the entropy rate superpixel (ERS) algorithm. Second, the WSCRs for the training and test samples are calculated. Then, a joint sparse representation (JSR) classification is used to obtain the representation residuals of different pixels. Finally, the class label of each pixel is determined by the defined decision function that combines the WSCR and JSR. Experimental results obtained on two real HSI datasets demonstrate the superiority of the proposed methods compared to other widely used methods in terms of classification accuracy.","Hyperspectral image, Superpixel, Joint sparse representation, Correlation coefficient",Bing Tu and Nanying Li and Leyuan Fang and Hongyan Fei and Danbing He,https://www.sciencedirect.com/science/article/pii/S1047320318302256,https://doi.org/10.1016/j.jvcir.2018.09.010,1047-3203,2018,160--166,56,Journal of Visual Communication and Image Representation,Classification of hyperspectral images via weighted spatial correlation representation,article,TU2018160
"This paper presents a multi-task based object tracking algorithm via a collaborative model. Under the framework of particle filtering, we develop a multi-task sparse learning based generative and discriminative classifier model. In the generative model, we propose a histogram based subspace learning method which takes advantage of adaptive templates update. In the discriminative model, we introduce an effective method to compute the confidence value which assigns more weights to the foreground than the background. A decomposition model is employed to take the common features and outliers of each particle into consideration. The alternating direction method of multipliers (ADMM) algorithm guarantees the optimization problem can be solved robustly and accurately. Qualitative and quantitative comparisons with nine state-of-the-art methods demonstrate the effectiveness and efficiency of our method in handling various challenges during tracking.","Collaborative model, Alternating direction method of multipliers, Multi-task sparse learning, Generative model, Discriminative model",Yong Wang and Xinbin Luo and Lu Ding and Shiqiang Hu,https://www.sciencedirect.com/science/article/pii/S1047320318301962,https://doi.org/10.1016/j.jvcir.2018.08.008,1047-3203,2018,698--710,55,Journal of Visual Communication and Image Representation,Multi-task based object tracking via a collaborative model,article,WANG2018698
"To improve action recognition performance, a novel discriminative spectral clustering method is firstly proposed, by which the candidate parts with the internal trajectories being close in spatial position, consistent in appearance and similar in motion velocity are mined. Furthermore, the discriminative constraint is introduced to select discriminative parts. Meanwhile, by fully considering the local and global distributions of data, a new similarity matrix is constructed, which enhances clustering effect. Secondly, the spatio-temporal interaction descriptor and causal interaction descriptor are constructed respectively, which fully mine the spatio-temporal and implicit causal interactive relationships between parts. Finally, a new framework is proposed. By associating the discriminative parts, spatio-temporal and causal interaction descriptors together as the inputs of Latent Support Vector Machine (LSVM), the correlations between action categories and action parts as well as interaction descriptors are mined. Consequently, accuracy is enhanced. The extensive and adequate experiments demonstrate the effectiveness of the proposed method.","Action recognition, Spectral clustering, Discriminative constraint, Action part, Spatio-temporal relationship, Causal relationship",Ming Tong and Yiran Chen and Mengao Zhao and Weijuan Tian,https://www.sciencedirect.com/science/article/pii/S1047320318302098,https://doi.org/10.1016/j.jvcir.2018.09.001,1047-3203,2018,116--130,56,Journal of Visual Communication and Image Representation,"A new framework of action recognition with discriminative parts, spatio-temporal and causal interaction descriptors",article,TONG2018116
"In this paper, we propose an edge detector based on feature re-extraction (FRE) of a deep convolutional neural network to effectively utilize features extracted from each stage, and design a new loss function. The proposed detector is mainly composed of three modules: backbone, side-output, and feature fusion. The backbone module provides preliminary feature extraction; the side-output module makes network architecture more robustly map features from different stages of the backbone network to edge-pixel space by applying residual learning, and the feature fusion module generates the edge map. Generalization ability on the same distribution is verified using the BSDS500 dataset, achieving optimal dataset scale (ODS) F-scoreâ¯=â¯0.804. Cross-distribution generalization ability is verified on the NYUDv2 dataset, achieving ODS F-scoreâ¯=â¯0.701. In addition, we find that freezing backbone network can significantly speed up training process, without much overall accuracy loss (ODS F-score of 0.791 after 5.4k iterations).","Edge detection, Feature re-extract, Deep convolutional neural network, Generalization ability",Changbao Wen and Pengli Liu and Wenbo Ma and Zhirong Jian and Changheng Lv and Jitong Hong and Xiaowen Shi,https://www.sciencedirect.com/science/article/pii/S1047320318302530,https://doi.org/10.1016/j.jvcir.2018.10.017,1047-3203,2018,84--90,57,Journal of Visual Communication and Image Representation,Edge detection with feature re-extraction deep convolutional neural network,article,WEN201884
"In this paper, we couple effective dynamic background modeling with fast deep learning classification to develop an accurate scheme for human-animal detection from camera-trap images with cluttered moving objects. We introduce a new block-wise background model, named as Minimum Feature Difference (MFD), to model the variation of the background of the camera-trap sequences and generate the foreground object proposals. We then develop a region proposals verification to reduce the number of false alarms. Finally, we perform complexity-accuracy analysis of DCNN to construct a fast deep learning classification scheme to classify these region proposals into three categories: human, animals, and background patches. The optimized DCNN is able to maintain high level of accuracy while reducing the computational complexity by 14 times, which allows near real-time implementation of the proposed method on CPU machines. Our experimental results demonstrate that the proposed method outperforms existing methods on our and Alexander von Humboldt Institute camera-trap datasets in both foreground segmentation and object detection.","Human-animal detection, Camera-trap images, Background subtraction, Deep convolutional neural networks, Wildlife monitoring",Hayder Yousif and Jianhe Yuan and Roland Kays and Zhihai He,https://www.sciencedirect.com/science/article/pii/S1047320318302013,https://doi.org/10.1016/j.jvcir.2018.08.013,1047-3203,2018,802--815,55,Journal of Visual Communication and Image Representation,Object detection from dynamic scene using joint background modeling and fast deep learning classification,article,YOUSIF2018802
"Recently, Lin et al. [29] introduced a (k, n) multi-factor cheating prevention visual cryptographic scheme (MCPVCS) by hiding verification images into shadow images. Through authenticating the verification image and the reconstructed image simultaneously, Lin et al.âs MCPVCS can prevent cheating attack that dishonest participants may collude and cheat the honest one to get a wrong secret image. However, the large n reduces the size of verification image and makes the verification difficult. In this paper, a (k, n)-MCPVCS with the large size of verification image is proposed. There are three main contributions for the proposed (k, n)-MCPVCS: (i) the verification condition in (k, n)-MCPVCS is redefined, (ii) a new minimum (k, n)-connected graph problem is introduced, on which the minimum number of verification images is determined, and (iii) the proposed (k, n)-MCPVCS with the large size of verification image can more easily authenticate the unambiguity of verification image.","Visual cryptography, Cheating attack, Cheating prevention, Verification image, Shadow image",Ching-Nung Yang and Fu-Heng Wu and Sheng-Lung Peng,https://www.sciencedirect.com/science/article/pii/S1047320318301846,https://doi.org/10.1016/j.jvcir.2018.07.012,1047-3203,2018,660--676,55,Journal of Visual Communication and Image Representation,"Enhancing multi-factor cheating prevention in visual cryptography based minimum (k, n)-connected graph",article,YANG2018660
"Semantic segmentation plays an important role in a series of high-level computer vision applications. In the state-of-the-art semantic segmentation methods based on fully convolutional neural networks, all label variables are predicted independently from each other, and the restricted field-of-views of the convolutional filters are difficult to capture the long-range information. In this paper, a novel post-processing method based on GAN (Generative Adversarial Network) is explored to reinforce spatial contiguity in the output label maps. With the help of fully connected layers in the discriminator, the GAN can capture the long-range information, and provide an auxiliary higher-order potential loss to the segmentation model, thus the segmentation model has the ability of correcting higher order inconsistencies. Furthermore, the optimization scheme in Wasserstein GAN (WGAN) is adopted to the training process of our model to get better performance and stability. Extensive experiments on public benchmarking database demonstrate the effectiveness of the proposed method.","Semantic segmentation, Generative adversarial network (GAN), Wasserstein distance, Auxiliary higher-order potential loss",Xiaobin Zhu and Xinming Zhang and Xiao-Yu Zhang and Ziyu Xue and Lei Wang,https://www.sciencedirect.com/science/article/pii/S1047320318302931,https://doi.org/10.1016/j.jvcir.2018.11.020,1047-3203,2019,532--543,58,Journal of Visual Communication and Image Representation,A novel framework for semantic segmentation with generative adversarial network,article,ZHU2019532
"Existing image fusion approaches are not so efficient to seize significant edges, texture and fine features of the source images due to ineffective and non-adaptive fusion structure. Also for objective evaluation of fusion algorithms, there is a need of a metric to measure source image features which are preserved in the fused image. To address these issues, an optimized non-subsampled shearlet transform (NSST) is developed, which is applied to decompose the source images into low- and high frequency bands. The low frequency bands are fused using proposed descriptor obtained from superposition of scale multiplied Canny edge detector features and Hessian features. The high frequency bands are fused using unsharp masking based fusion rule. Moreover, a metric QE is formulated on the basis of Karhunen-Loeve transform (KLT). The information of image pixel variance for both source and fused images can be measured by using the proposed metric QE, and it gives an indication of the amount of variance information transferred from the source images to the fused image. Both subjective and objective analysis show the efficacy of the proposed fusion structure and the metric QE.","Image fusion, Kaiser window, Optimized NSST, Canny edge detector, Hessian features, Unsharp masking",Amit Vishwakarma and M.K. Bhuyan and Yuji Iwahori,https://www.sciencedirect.com/science/article/pii/S1047320318302414,https://doi.org/10.1016/j.jvcir.2018.10.005,1047-3203,2018,48--60,57,Journal of Visual Communication and Image Representation,An optimized non-subsampled shearlet transform-based image fusion using Hessian features and unsharp masking,article,VISHWAKARMA201848
"Video clipping system is very important in many intelligent applications. In order to shorten the time of video and extract the framework of the video, many methods have been proposed. But these methods just considered videos without taking sound into account. As we know, sound is also an important information for image processing. For example, many sport match videos include rich sound of audience and commentator such as NBA. In addition, human pay more attention to some video clips of interest (VCOI) such as scoring time instead of pause. So in this paper, we propose a sound-based video clipping framework toward specific sports scenes. First, we convert sound of sport videos to sonogram. For some aesthetically-pleasing images (APIM) such as slam dunk or jump shot, a set of object patches are selected using BING feature. Then, these object patches are ordered by our active object patches ranking algorithm. After that, ordered object patches and sonogram are fed into CNN respectively to obtain patch-level deep feature. In order to obtain image-level deep representation, deep feature extracted from ordered object patches are aggregated statistically into a deep representation. Finally, probabilistic model is used to select VCOI and APIM. Experiments on some NBA basketball matches have shown the effectiveness of our video clipping framework.","Sonogram, Deep representation, Convolutional neural network",Qunchao Mi and Dali Xue,https://www.sciencedirect.com/science/article/pii/S1047320318301688,https://doi.org/10.1016/j.jvcir.2018.07.008,1047-3203,2018,648--653,55,Journal of Visual Communication and Image Representation,A sound-based video clipping framework toward sports scenes,article,MI2018648
"Shape From Focus refers to the inverse problem of recovering the depth in every point of a scene from a set of differently focused 2D images. Recently, some authors stated it in the variational framework and solved it by minimizing a non-convex functional. However, the global optimality on the solution is not guaranteed and evaluations are often application-specific. To overcome these limits, we propose to globally and efficiently minimize a convex functional by decomposing it into a sequence of binary problems using graph cuts. To illustrate the genericity of such a decomposition-based approach, data-driven strategies are considered, allowing us to optimize (in terms of reconstruction error) the choice of the depth values for a given number of possible depths. We provide qualitative and quantitative evaluation on Middlebury datasets and we show that, according to classic statistics on error values, the proposed approach exhibits high performance and robustness against corrupted data.","Shape from focus, Depth map estimation, Graph cuts, Multi-labels",Christophe Ribal and Nicolas LermÃ© and Sylvie {Le HÃ©garat-Mascle},https://www.sciencedirect.com/science/article/pii/S104732031830155X,https://doi.org/10.1016/j.jvcir.2018.06.029,1047-3203,2018,529--539,55,Journal of Visual Communication and Image Representation,Efficient graph cut optimization for shape from focus,article,RIBAL2018529
"Joint image encryption and compression schemes have shown their great potential values in protecting compressed images. To achieve the protection, a trade-off between encryption power and compression ability needs to be considered. In this paper, we propose two new joint encryption and compression schemes, where one scheme emphasizes compression performance, another highlights protection performance. For a given plain-image, we first raster scan it into non-overlapping 16â¯Ãâ¯16 blocks, then apply various encryption techniques to it. In the first scheme, encryption operations are conducted at the transformation stage and quantization stage of JPEG. As for the second scheme, we add the run/size and value (RSV) pairsâ shuffling operation at JPEGâs entropy coding stage after first schemeâs encryption operations. Performance evaluations using various criteria are conducted to show that the first scheme has better compression efficiency, while the second scheme has better defense ability against the statistical attack.","Image encryption, JPEG compression, 16â¯â¯16 DCT, Statistical attack",Peiya Li and Kwok-Tung Lo,https://www.sciencedirect.com/science/article/pii/S104732031830292X,https://doi.org/10.1016/j.jvcir.2018.11.018,1047-3203,2019,12--24,58,Journal of Visual Communication and Image Representation,Joint image encryption and compression schemes based on 16â¯Ãâ¯16 DCT,article,LI201912
"This paper presents an image quality assessment (IQA) model exploring the multi-order derivative feature, called Multi-order Derivative Feature-based Model (MDFM), for evaluating the perceptual quality of light field image (LFI). In our approach, for the input reference and distorted LFIs, the multi-order derivative features are extracted by using the discrete derivative filter to represent the image details in different degrees. Then, the similarities of the extracted derivative features are measured independently. Finally, the weight map is established through the maximum value of the second-order derivative feature of reference and distorted LFIs, which is further utilized to pool the similarity map for generating the final score. Extensive simulation results have demonstrated that the proposed MDFM is more consistent with the perception of the HVS on the evaluation of LFI than the classical and state-of-the-art IQA methods.","Light field image, Image quality assessment, Multi-order derivative feature",Yu Tian and Huanqiang Zeng and Lu Xing and Jing Chen and Jianqing Zhu and Kai-Kuang Ma,https://www.sciencedirect.com/science/article/pii/S1047320318302785,https://doi.org/10.1016/j.jvcir.2018.11.005,1047-3203,2018,212--217,57,Journal of Visual Communication and Image Representation,A multi-order derivative feature-based quality assessment model for light field image,article,TIAN2018212
"Facial expression involves a dynamic process, leading to the variation of different facial components over time. Thus, dynamic descriptors are essential for recognising facial expressions. In this paper, we extend the spatial pyramid histogram of gradients to spatio-temporal domain to give 3-dimensional facial features. To enhance the spatial information, we divide the whole face region into a group of smaller local regions to extract local 3D features, and a weighting strategy based on fisher separation criterion is proposed to enhance the discrimination ability of local features. A multi-class classifier based on support vector machine is applied for recognising facial expressions. Experiments on the CK+ and MMI datasets using leave-one-out cross validation scheme show that the proposed framework perform better than using the descriptor of simple concatenation. Compared with state-of-the-art methods, the proposed framework demonstrates a superior performance.","Histogram of gradients, Facial expression, Feature extraction",Xijian Fan and Xubing Yang and Qiaolin Ye and Yin Yang,https://www.sciencedirect.com/science/article/pii/S1047320318302268,https://doi.org/10.1016/j.jvcir.2018.09.011,1047-3203,2018,182--187,56,Journal of Visual Communication and Image Representation,A discriminative dynamic framework for facial expression recognition in video sequences,article,FAN2018182
"VR video is increasingly popular due to the recent advances in VR technology and hardware. The bulky size of VR video, however, impose new challenges in its storage and processing. In this paper, we focus on the research problems of VR video representation and objective quality assessment. Distinct from traditional 2D video, a VR video is displayed and represented in a form of spherical surface. For encoding purpose, a VR video frame needs to be projected to a 2D flat plane. Existing projection methods usually lead to much redundancy or significant violation in the correlation of neighbor pixels, which are not encoding friendly or are creating visible edge artifacts. To alleviate these problems, we propose in this paper our Quadrangle Affine Square Projection (QASP). QASP is a novel representation for VR video frames, which can reduce the redundant pixels over the traditional projections. In particular, all the inner pixels in QASP remain connected, i.e. the correlations between neighbors pixels are well maintained, which is a desirable feature for video encoding and edge-artifact-free viewing experience. Besides, we also investigated in predicting the optimal rotation angle for QASP frames for higher encoding efficiency. In addition to VR video representation, we also investigate in this paper accurate objective quality measurement for VR video. The traditional video quality measurements, e.g. PSNR, are not suitable to measure the quality of VR videos since they will take the redundant pixels into account. In this paper, we propose a new quality measurement for VR videos, named as Resized-PSNR (R-PSNR). With R-PSNR, only the âmeaningfulâ pixels are considered for quality measurement while the redundant pixels are discarded. To evaluate QASP and R-PSNR, experiments are conducted based on standard VR video sequences. The experimental results show that the proposed projection method achieves noticeable improvement over traditional methods, and the proposed quality assessment method outperforms the traditional measurements in terms of consistency with subjective evaluation.","Virtual reality, Cinematic VR, Immersive media, Video coding, Quality assessment",Shilin Wu and Xiaoming Chen and Jun Fu and Zhibo Chen,https://www.sciencedirect.com/science/article/pii/S1047320318302542,https://doi.org/10.1016/j.jvcir.2018.10.018,1047-3203,2018,107--117,57,Journal of Visual Communication and Image Representation,Efficient VR Video Representation and Quality Assessment,article,WU2018107
"The aim of this paper is to model long-term evolution of an action video with temporal multi-scale representation. This task is tough due to huge intra-class variations in motion speed. Most of the existing methods consider evolution modeling and multi-scale feature fusion in two separated phases, which generates sub-optimal representation. To address this issue, this paper proposes a novel method to integrate the evolution modeling and multi-scale representation into a unified framework. The core idea is to introduce a temporal multi-scale smoothing vector, which is used to define how the representations at different temporal scales are combined together for frame smoothing. By formulating the smoothing vector learning, evolution modeling and classifier training jointly, our method can learn a discriminative and flexible representation of multi-scale rather than a single scale or a fixed multi-scale smoothing. Experimental results on three datasets demonstrate the effectiveness of our method.","Action recognition, Multi-scale representation, Rank pooling, Evolution modeling, Dynamics",Tingwei Wang and Chuancai Liu and Liantao Wang and Bingxian Ma and Xingjian Gu,https://www.sciencedirect.com/science/article/pii/S1047320318302025,https://doi.org/10.1016/j.jvcir.2018.08.014,1047-3203,2018,778--788,55,Journal of Visual Communication and Image Representation,Evolution modeling with multi-scale smoothing for action recognition,article,WANG2018778
"The development of emerging 3D display brings increasing attention of stereoscopic techniques such as quality assessment, re-targeting, and compression of 3D image, that require new saliency detection methods to deal with stereoscopic data. In this paper, we present a disparity tuning guided stereoscopic saliency (DTSS) model which apply the disparity tuning mechanism of visual cortical neurons into visual attention modeling. First, we investigate the rationality of converting features from physical quantity into perception quantity before computing saliency. Second, we discuss the biological principles that depth affects visual attention and consider both absolute and relative depth tuning mechanisms to model visual attention. Then, we propose a diffusion saliency feature combining depth and RGB features. Specifically, we use RGB contrast as primitive labels to diffuse a saliency map for depth map and using depth contrast as primitive labels to diffuse a saliency map for RGB image. Finally, an adaptively weighted fusion method is proposed for the integration of feature maps. Experiments demonstrate that the proposed model performs well against to the state-of-art methods on the task of 3D eye fixation prediction.","Stereoscopic saliency, Disparity tuning, Diffusion-based saliency",Di Liu and Zhenzhong Chen,https://www.sciencedirect.com/science/article/pii/S1047320318302384,https://doi.org/10.1016/j.jvcir.2018.10.002,1047-3203,2018,218--227,57,Journal of Visual Communication and Image Representation,Disparity tuning guided stereoscopic saliency detection for eye fixation prediction,article,LIU2018218
"Eye detection is a very useful technique in many intelligent applications. Since the importance of eyes to human beings, eye detection technique is an indispensable component in intelligent systems, e.g., emotional analysis, iris detection and gaze estimation. Recently, there have been proposed a large number of methods for eye detection, wherein good performances have been achieved. But these methods cannot take human visual perception into account, that is to say, human beings will first pay attention to the eyes when they are communicating with each other, and then nose, then mouth. In addition, their geometric positions are almost fixed, i.e., eyes are above the nose and mouth, and eyes are on both sides of the nose. So in our work, a novel method for eye detection is proposed using human visual perception. More specifically, we first derive object patches from a large quantity of training images. Then, a geometry-preserved object patches ranking method is designed to effectively mimic human visual mechanism when human beings are communicating with each other. After that, these ordered object patches will be fed into CNN to extract patch-level deep features, then patch-level deep features will be represented by deep representations. Finally, eye detection can be achieved using learned deep representation. Experimental results on different database show that our method can achieve high efficiency and accuracy of eye detection.","Eye detection, Gaze shifting path, Deep feature",Shujun Zhang and Li Shen and Ruoyi Zhang and Yihan Yang and Youqian Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318301858,https://doi.org/10.1016/j.jvcir.2018.07.013,1047-3203,2018,654--659,55,Journal of Visual Communication and Image Representation,Robust eye detection using deeply-learned gaze shifting path,article,ZHANG2018654
"A huge amount of image data is being collected in real world sectors. Image data analytics provides information about important facts and issues of a particular domain. But, it is challenging to handle voluminous, unstructured and unlabeled image collection. Clustering provides groups of homogeneous unlabeled data. Therefore, it is used quite often to access the interesting data easily and quickly. Image clustering is a process of partitioning image data into clusters on the basis of similarities. Whereas, features extracted from images are used for the computation of similarities among them. In this paper, significant feature extraction approaches and clustering methods applied on the image data from nine important applicative areas are reviewed. Medical, 3D imaging, oceanography, industrial automation, remote sensing, mobile phones, security and traffic control are considered applicative areas. Characteristics of images, suitable clustering approaches for each domain, challenges and future research directions for image clustering are discussed.","Image clustering, Feature extraction, Real world applications",Seema Wazarkar and Bettahally N. Keshavamurthy,https://www.sciencedirect.com/science/article/pii/S104732031830172X,https://doi.org/10.1016/j.jvcir.2018.07.009,1047-3203,2018,596--626,55,Journal of Visual Communication and Image Representation,A survey on image data analysis through clustering techniques for real world applications,article,WAZARKAR2018596
"In this paper, we propose an deep intensity guidance based compression artifacts reduction model (denoted as DIG-Net) for depth map. The proposed DIG-Net model can learn an end-to-end mapping from the color image and distorted depth map to the uncompressed depth map. To eliminate undesired artifacts such as discontinuities around object boundary, the proposed model is with three branches, which extracts the high frequency information from color image and depth maps as priors. Based on the modified edge preserving loss function, the deep multi-scale guidance information are learned and fused in the model to make the edge of depth map sharper. Experimental results show the effectiveness and superiority of our proposed model compared with the state-of-the-art methods.","Convolutional neural network, Compression artifacts reduction, JPEG compression, Depth map",Xu Wang and Pingping Zhang and Yun Zhang and Lin Ma and Sam Kwong and Jianmin Jiang,https://www.sciencedirect.com/science/article/pii/S1047320318302827,https://doi.org/10.1016/j.jvcir.2018.11.008,1047-3203,2018,234--242,57,Journal of Visual Communication and Image Representation,Deep intensity guidance based compression artifacts reduction for depth map,article,WANG2018234
"2D pose estimation have achieve remarkable performance with deep convolutional neural networks. However 3D pose estimation is current constrained by the limited datasets of 3D annotations. Meanwhile most annotated images are captured using Motion Capture system in lab or certain studio, which has large variations with large-scale monocular 2D pose datasets. We propose an adversarial learning framework, which can learn invariant human pose latent from 3D annotated datasets to optimize the estimation of monocular images with only 2D annotations. However there is large difference in observation coordinates between 2D datasets and 3D datasets, and this viewpoints issue should be separated from invariant pose latent. We add a viewpoints invariant module to automatically regulate observation viewpoints for generated 3D pose, which transforming the generated pose to more suitable observation in the 3D datasets. Our method achieve competitive results on both 2D and 3D benchmarks.","3D pose estimation, Adversarial learning",Yimeng Li and Jun Xiao and Di Xie and Jian Shao and Jinlong Wang,https://www.sciencedirect.com/science/article/pii/S1047320318302943,https://doi.org/10.1016/j.jvcir.2018.11.021,1047-3203,2019,374--379,58,Journal of Visual Communication and Image Representation,Adversarial learning for viewpoints invariant 3D human pose estimation,article,LI2019374
"Cross-modal hashing has made a great development in cross-modal retrieval since its vital reduction in computational cost and storage. Generally, projections for each modality that map heterogeneous data into a common space are used to bridge the gap between different modalities. However, category-specific distributions are usually be ignored during the projection. To address this issue, we propose a novel cross-modal hashing, termed as Category Structure Preserving Hashing (CSPH), for cross-modal retrieval. In CSPH, category-specific distribution is preserved by a structure-preserving regularization term during the hash learning. Compared with existing methods, CSPH not only preserves the local structure of each category, but also generates more stable hash codes with less time for training. Extensive experiments conducted on three benchmark datasets, and the experimental results demonstrate the superiority of CSPH under various cross-modal scenarios.","Cross-modal retrieval, Supervised hashing, Category-specific structure preserving",Fei Dong and Xiushan Nie and Xingbo Liu and Leilei Geng and Qian Wang,https://www.sciencedirect.com/science/article/pii/S1047320318302426,https://doi.org/10.1016/j.jvcir.2018.10.006,1047-3203,2018,28--33,57,Journal of Visual Communication and Image Representation,Cross-modal hashing based on category structure preserving,article,DONG201828
"Although high dynamic range (HDR) videos are a very fascinating way to represent real-world scenes, they need a huge amount of memory to store and transmit due to the high bit-depth. Thus, it is a major challenge for HDR video coding to efficiently compress them without sacrificing perceptual quality. Perceptual Quantizer (PQ) transfer function provides a solution to this problem, which is adopted as the HEVC Main 10 Profile-based Anchor. However, PQ is not adaptive to HDR contents, thus reducing the coding efficiency. In this paper, we propose adaptive transfer function based on PQ for HDR video compression, called adaptive PQ. Different from PQ which uses a fixed mapping curve from luminance to luma, the proposed transfer function adaptively maps luminance to luma according to HDR contents. Thus, adaptive PQ is able to efficiently utilize possible luma values. Moreover, adaptive PQ achieves better perceptual uniformity in the luminance range than PQ. Experimental results demonstrate that adaptive PQ achieves a significant performance improvement in HDR video coding over PQ in terms of visual quality and bitrate.","HEVC, Adaptive transfer function, High dynamic range, Perceptual quantizer, Perceptual uniformity",Shengtao Yu and Cheolkon Jung,https://www.sciencedirect.com/science/article/pii/S104732031830289X,https://doi.org/10.1016/j.jvcir.2018.11.016,1047-3203,2019,25--36,58,Journal of Visual Communication and Image Representation,Adaptive perceptual quantizer for high dynamic range video compression,article,YU201925
"Inspired by the application of denoising autoencoding priors (DAEP) to image restoration tasks, we propose a single image super-resolution (SISR) method via introducing multi-denoising autoencoding priors (MDAEP). On the basis of the naive DAEP, the proposed MDAEP integrates multi-DAEPs from different noisy inputs into the iterative restoration process. The combined strategy avails to alleviate the instability of the denoising autoencoders, and thus to avoid falling into local solutions. Furthermore, compared with the existing SISR methods based on end-to-end mapping, MDAEP is only trained once and applied to different magnification factors, but also can effectively preserve high-frequency information and reduce ringing effects of the reconstructed images. Both quantitative and qualitative assessments of the benchmark datasets show that the ability and the stability of the network are improved effectively. The proposed method performs better than the state-of-the-art algorithms including the basic DAEP, in terms of PSNRs and visual comparisons.","Single image super-resolution, Denoising autoencoder, Multi-denoising autoencoding priors, Alternative iteration",Yankun Wang and Qiegen Liu and Huilin Zhou and Yuhao Wang,https://www.sciencedirect.com/science/article/pii/S1047320318302700,https://doi.org/10.1016/j.jvcir.2018.10.028,1047-3203,2018,152--162,57,Journal of Visual Communication and Image Representation,Learning multi-denoising autoencoding priors for image super-resolution,article,WANG2018152
"Accurately locating the video target in the process of occlusion and recurrence will be very important for effective follow-up of the target. For the problem of poor applicability of Mean Shift and its improved algorithm when the target is heavily occluded, this paper proposes an anti-occlusion video target tracking algorithm based on prediction and re-matching strategy. Firstly, dynamically combining the Mean Shift algorithm with the Kalman filter, this paper achieves stable tracking of un-occluded target. Secondly, when the target is occluded, Kalman filter is combined with the target prior information to predict the position of the occluded target. Finally, in the recurrence process of occluded targets, the target is re-matched through the normalized cross-correlation method to obtain target optimal position, and then the target can be quickly and accurately located. The simulation results show that the proposed method has strong anti-occlusion and reliability tracking in the video target tracking process.","Anti-occlusion, Mean Shift algorithm, Kalman filter, Normalized cross correlation matching",Zhen-tao Hu and Lin Zhou and Ya-nan Yang and Xian-xing Liu and Yong Jin,https://www.sciencedirect.com/science/article/pii/S1047320318302554,https://doi.org/10.1016/j.jvcir.2018.10.019,1047-3203,2018,176--182,57,Journal of Visual Communication and Image Representation,Anti-occlusion tracking algorithm of video target based on prediction and re-matching strategy,article,HU2018176
"Considering each of the visual features as one modality in image annotation task, efficient fusion of different modalities is essential in graph-based learning. Traditional graph-based methods consider one node for each image and combine its visual features into a single descriptor before constructing the graph. In this paper, we propose an approach that constructs a subgraph for each modality in such a way that edges of subgraph are determined using a search-based approach that handles class-imbalance challenge in the annotation datasets. Multiple subgraphs are then connected to each other to have a supergraph. This follows by introducing a learning framework to infer the tags of unannotated images on the supergraph. The proposed approach takes advantages of graph-based semi-supervised learning and multi-modal representation simultaneously. We evaluate the performance of the proposed approach on different datasets. The results reveal that the proposed approach improves the accuracy of annotation systems.","Image annotation, Tag, Manifold, Multi-modal representation, Graph-based learning, Supergraph",S. Hamid Amiri and Mansour Jamzad,https://www.sciencedirect.com/science/article/pii/S1047320318302037,https://doi.org/10.1016/j.jvcir.2018.08.012,1047-3203,2018,816--828,55,Journal of Visual Communication and Image Representation,Leveraging multi-modal fusion for graph-based image annotation,article,AMIRI2018816
"In this paper, a simple yet quite useful hyperspectral images (HSI) classification method based on adaptive total variation filtering (ATVF) is proposed. The proposed method consists of the following steps: First, the spectral dimension of the HSI is reduced with principal component analysis (PCA). Then, ATVF is employed to extract image features which not only reduces the noise in the image, but also effectively exploits spatialâspectral information. Therefore, it can provide an improved representation. Finally, the efficient extreme learning machine (ELM) with a very simple structure is used for classification. This paper analyzes the influence of different parameters of the ATVF and ELM algorithm on the classification performance in detail. Experiments are performed on three hyperspectral urban data sets. By comparing with other HSI classification methods and other different feature extraction methods, the proposed method based on the ATVF algorithm shows outstanding performance in terms of classification accuracy and computational efficiency when compared with other hyperspectral classification methods.","Hyperspectral image classification, Principal component analysis, Adaptive total variation filtering, Extreme learning machine",Guoyun Zhang and Jinping Wang and Xiaofei Zhang and Hongyan Fei and Bing Tu,https://www.sciencedirect.com/science/article/pii/S1047320318302311,https://doi.org/10.1016/j.jvcir.2018.09.016,1047-3203,2018,150--159,56,Journal of Visual Communication and Image Representation,Adaptive total variation-based spectral-spatial feature extraction of hyperspectral image,article,ZHANG2018150
"A multiscale Galerkin method (MGM) was proposed recently by the same authors in order to solve second-order boundary value problems of Fredholm integro-differential equation. Although, the numerical solution of MGM is always stable because of the multiscale bases properties, obligatory of considerable computational cost and huge memory for achieving great approximation accuracy, are the main draw backs. To overcome MGM problems, in this paper, a new multilevel augmentation method (MAM) in order to solve discrete linear system is proposed. Applying the special matrix splitting techniques, approximate solution is obtained by (1) solving a linear system only at an initial lower level; (2) compensating the error by directly computing the product of matrices and vectors at the higher level without any iterations. Theoretical and experimental results approve that MAM and MGM have similar and optimum convergence orders, though MAM is more efficient than MGM.","Multiscale Galerkin method (MGM), Multilevel augmentation method (MAM), Boundary value problems, Fredholm integro-differential equation",Jian Chen and Minfan He and Taishan Zeng,https://www.sciencedirect.com/science/article/pii/S1047320318303006,https://doi.org/10.1016/j.jvcir.2018.11.027,1047-3203,2019,112--118,58,Journal of Visual Communication and Image Representation,A multiscale Galerkin method for second-order boundary value problems of Fredholm integro-differential equation II: Efficient algorithm for the discrete linear system,article,CHEN2019112
"With the increasing tendency of using images to express opinions and share experiences, sentiment analysis of visual content has aroused considerable attention interests in the past few years. Traditional sentiment analysis methods mainly focus on predicting the most dominant sentiment category of images while neglecting the sentiment ambiguity problem restricted by various factors such as environment, subjectivity, and cultural background. To tackle this problem, visual sentiment distribution prediction has been put forward to characterize images by distributions over a set of sentiment labels instead of a single distinct label or multiple distinct labels. Nevertheless, existing approaches usually separate feature embedding and distribution prediction. In this paper, we propose a novel supervised visual sentiment distribution prediction model, termed as low-rank regularized multi-view inverse-covariance estimation, in which feature embedding and distribution prediction are jointly performed. Specifically, our proposed model contains two main components: multi-view embedding and inverse-covariance estimation terms. The multi-view embedding term is restricted by low-rank constraints to seek the lowest-rank representation of samples. The inverse-covariance estimation term is restricted by structured sparsity regularization to learn a more reasonable distribution prediction model. We develop an alternative heuristic optimization algorithm to solve the objective function of the proposed model. Experiment results performed on three publicly available datasets demonstrate the effectiveness of our proposed scheme compared with state-of-the-art algorithms.","Image sentiment, Label distribution learning, Structured sparsity, Low-rank",Anan Liu and Yingdi Shi and Peiguang Jing and Jing Liu and Yuting Su,https://www.sciencedirect.com/science/article/pii/S1047320318302797,https://doi.org/10.1016/j.jvcir.2018.11.006,1047-3203,2018,243--252,57,Journal of Visual Communication and Image Representation,Low-rank regularized multi-view inverse-covariance estimation for visual sentiment distribution prediction,article,LIU2018243
"With the fast development of information network, the scale of social network has become very significant, and it has become more difficult to obtain the information of entire network. In addition, because current mining method for complicated network community utilizes the information of node link or property, which cannot effectively detect the community with dense member links and highly similar properties. As a result, most current algorithms are impractical for online social network with large scale, and we propose a community detection algorithm for multi-layer social network based on local random walk (MRLCD); this algorithm determines the core node based on the repeatability of multi-layer nodes. It expands from a core node, has local random walk in multi-layer network, identifies and controls the random walk scope of node based on the intra-layer and interlayer trust. During the walk process, the clustering coefficient of nodes to be combined is comprehensively compared to further complete a local community search, and the optimal local community search is obtained through multiple iterations. Finally, the multi-layer modularity is used as the indicator for measurement and evaluation of algorithm performance, and its performance is compared with other network clustering algorithms such as GL, LART and PMM through four actual multi-layer network datasets. The MRLCD algorithm can autonomously explore the local community structure of given node, and effectively improve the stability and accuracy for local community detection in multi-layer social network.","Multi-layer social network, Local community detection, Trust, Multi-layer local random walk",XiaoMing Li and Guangquan Xu and Minghu Tang,https://www.sciencedirect.com/science/article/pii/S1047320318302396,https://doi.org/10.1016/j.jvcir.2018.10.003,1047-3203,2018,91--98,57,Journal of Visual Communication and Image Representation,Community detection for multi-layer social network based on local random walk,article,LI201891
"This paper presents an algorithm for extracting PRNU noise from video files taken by a smartphone camera. We consider video because they are quite prevalent in our life, but are not often involved in the existing research works. Unlike most prior arts tending to extract PRNU noise from completely decompressed video frames, our proposed algorithm leaves out some procedures in the video decoding process to reduce the computational complexity. Besides, we design a maximum-likelihood-estimation algorithm for extracting PRNU noise from partly decoded video frames, and analyze the algorithmâs suitability as well in theory. Experimental results further prove the validity and effectiveness of the proposed algorithm.","Forensics, Smartphone, Camera, Sensor noise, PRNU",Jian Li and Bin Ma and Chunpeng Wang,https://www.sciencedirect.com/science/article/pii/S1047320318302670,https://doi.org/10.1016/j.jvcir.2018.10.023,1047-3203,2018,183--191,57,Journal of Visual Communication and Image Representation,Extraction of PRNU noise from partly decoded video,article,LI2018183
"In this work we aim to automatically recognize the artistic movement from a digitized image of a painting. Our approach uses a new system that resorts to descriptions induced by color structure histograms and by novel topographical features for texture assessment. The topographical descriptors accumulate information from the first and second local derivatives within four layers of finer representations. The classification is performed by two layers of ensembles. The first is an adapted boosted ensemble of support vector machines, which introduces further randomization over feature categories as a regularization. The training of the ensemble yields individual experts by isolating initially misclassified images and by correcting them in further stages of the process. The solution improves the performance by a second layer build upon the consensus of multiple local experts that analyze different parts of the images. The resulting performance compares favorably with classical solutions and manages to match the ones of modern deep learning frameworks.","Randomized boosted SVMs, Multi-scale topography, Painting style recognition, Consensus of experts, Ensembles",Corneliu Florea and Fabian Gieseke,https://www.sciencedirect.com/science/article/pii/S104732031830230X,https://doi.org/10.1016/j.jvcir.2018.09.015,1047-3203,2018,220--233,56,Journal of Visual Communication and Image Representation,Artistic movement recognition by consensus of boosted SVM based experts,article,FLOREA2018220
"In (k,n) progressive secret image sharing (PSIS) schemes, a secret image is shared into n shadows in such way that: (1) fewer than k-1 shadows get no information on the secret image; (2) k to n shadows can progressively recover the secret image. In most PSIS schemes, the shadows are noise-like images which would cause suspicious of attackers. The combination of secret image sharing and steganography can share a secret image into meaningful shadow images that can reduce attention of attackers. However most existing secret image sharing schemes with meaningful shadow images do not possess progressive property, a group of shadow images can either reconstruct entire secret image or get nothing on it. In this paper, we construct a new (k,n) PSIS with meaningful shadow images using GEMD and RGEMD. The property of progressive reconstruction is proved both in theoretical analysis and experimental results. The approaches of GEMD and RGEMD enable our Scheme high embedding capacity and resistance of RS detection. Comparing with other PSISs with meaningful shadow images, our Scheme has advantages in shadow size and shadow visual quality.","Progressive secret sharing scheme, Meaningful shadow image, GEMD, RGEMD",Yan-Xiao Liu and Ching-Nung Yang and Yung-Shun Chou and Song-Yu Wu and Qin-Dong Sun,https://www.sciencedirect.com/science/article/pii/S1047320318301901,https://doi.org/10.1016/j.jvcir.2018.08.003,1047-3203,2018,766--777,55,Journal of Visual Communication and Image Representation,"Progressive (k,n) secret image sharing Scheme with meaningful shadow images by GEMD and RGEMD",article,LIU2018766
"Visual comparison is that given two images, we can predict which one exhibits a particular visual attribute more than the other. The existing relative attribute methods rely on ranking SVM functions to conduct visual comparison; however, the ranking SVM functions are sensitive to the support vectors. When there are rarely effective samples, the performance of the ranking SVM model will be greatly discounted. To address this issue, we propose the pairwise relative attribute method for visual comparison by training the Linear Regression Model (LRM), which can be formulated by learning a mapping function between a vector-formed feature input with pairwise image difference and a scalar-valued output. In addition, we propose a novel feature reduction method based on the Linear Discriminant Analysis (LDA) in order to obtain a low dimensional and discriminant feature. Experimental results on the three databases of UT-Zap50K-1, OSR and PubFig demonstrate the advantages of the proposed method.","Visual comparison, LDA, Linear regression, SVM, Pairwise relative attributes",Hanqin Shi and Liang Tao,https://www.sciencedirect.com/science/article/pii/S1047320318302712,https://doi.org/10.1016/j.jvcir.2018.10.026,1047-3203,2018,118--124,57,Journal of Visual Communication and Image Representation,Visual comparison based on linear regression model and linear discriminant analysis,article,SHI2018118
"Person re-identification is an emerging research field in computer vision. Our paper aims to study how to improve the discrimination of person features. We find that some peculiarities of people have not been better attention in the semantic features of deep learning. However, some features obtained by traditional methods can better express the color, and these features are an important clue for re-identification. Therefore, in this paper, we combine traditional Gaussian features with deep semantic features to enhance the discrimination of overall features. At last, we have achieved good performance on two public datasets (Market1501 and VIPeR) in three main distance method learning (DML). In addition, we applied this model to the task of vehicle re-identification. Experiments show that our method has a great improvement on the VeRi vehicle dataset. We compare the results with the current high level results, which indicates the effectiveness of our model.","Deep learning, Fusion strategy, Re-identification",Yongge Liu and Nan Song and Yahong Han,https://www.sciencedirect.com/science/article/pii/S1047320318302955,https://doi.org/10.1016/j.jvcir.2018.11.023,1047-3203,2019,46--52,58,Journal of Visual Communication and Image Representation,Multi-cue fusion: Discriminative enhancing for person re-identification,article,LIU201946
"With the development of 3D sensors, it will be much easier for us to obtain 3D models, which is prevailing in our future daily life, but up to now, although many 3D object recognition algorithms have been proposed, there are some limitations, including the lack of training samples, hand-crafted feature representation, feature extraction and recognition separately. In this work, we propose a novel pairwise Multi-View Convolutional Neural Network for 3D Object Recognition (PMV-CNN for short), where automatic feature extraction and object recognition are put into a unify CNN architecture. Moreover, since the pairwise network architecture is utilized in PMV-CNN, thus, the requirement of the number of training samples in the original dataset is not severe. In addition, the latent complementary relationships from different views can be highly explored by view pooling. Large scale experiments demonstrate that the pairwise architecture is very useful when the number of labeled training samples is very small. Moreover, it also makes more robust feature extraction. Furthermore, since the end-to-end network architecture is employed in PMV-CNN, thus, the extracted feature is very suitable for 3D object recognition, whose performance is much better than that of hand-crafted features. In a word, the performance of our proposed method outperforms state-of-the-art methods.","3D object recognition, CNN, Multi-view, PMV-CNN, Pairwise, End-to-end",Z. Gao and D.Y. Wang and Y.B. Xue and G.P. Xu and H. Zhang and Y.L Wang,https://www.sciencedirect.com/science/article/pii/S1047320318302438,https://doi.org/10.1016/j.jvcir.2018.10.007,1047-3203,2018,305--315,56,Journal of Visual Communication and Image Representation,3D object recognition based on pairwise Multi-view Convolutional Neural Networks,article,GAO2018305
"We describe the spatial correlation problem of noise in colour digital images and analyse its cause. Pixel-correlated image processing procedures, such as CFA colour interpolation and colour space transformation, mainly lead to this problem. Considering this problem, we propose a new noise model based on a joint Gaussian probability distribution. Furthermore, we present an algorithm that makes the revised noise model fit the existing image deconvolution well. The parameters of our algorithm depend only on the image processing procedures of the imaging system. Finally, we apply the proposed algorithm to revise two typical image deconvolution methods and perform simulations and real-world experiments. Both the quantitative indicators and visual performance of the image deblurring results show that the revised deconvolution methods based on our noise model behave better in reducing the noise and ringing artefacts, thus improving the image quality compared with the methods that use the original noise model.","Spatial correlation coefficient, Noise probability distribution model, Colour interpolation, Colour space transformation, Correction algorithm, Image deblurring",Chenwei Yang and Huajun Feng and Zhihai Xu and Qi Li and Yueting Chen,https://www.sciencedirect.com/science/article/pii/S1047320318302281,https://doi.org/10.1016/j.jvcir.2018.09.013,1047-3203,2018,167--176,56,Journal of Visual Communication and Image Representation,The spatial correlation problem of noise in imaging deblurring and its solution,article,YANG2018167
"This paper outlines a simplistic formulation for doublet constrained discriminative metric learning framework for face verification. The Mahalanobis distance metric of the framework is formulated by leveraging the within-class scatter matrix of the doublet and a quadratic kernel function. Unlike existing metric learning methods, the proposed framework admits efficient solution attributed to the convexity nature of the kernel machines. We demonstrate three realizations of the proposed framework based on the well-known kernel machine instances, namely Support Vector Machine, Kernel Ridge Regression and Least Squares Support Vector Machine. Due to wide availability of off-the-shelf kernel learner solvers, the proposed method can be easily trained and deployed. We evaluate the proposed discriminative kernel-based metric learning with two types of face verification setup: standard and unconstrained face verification through three benchmark datasets. The promising experimental results corroborate the feasibility and robustness of the proposed framework.","Face verification, Metric learning, Kernel machine, Discriminant analysis",Siew-Chin Chong and Thian-Song Ong and Andrew Beng Jin Teoh,https://www.sciencedirect.com/science/article/pii/S1047320318302323,https://doi.org/10.1016/j.jvcir.2018.09.017,1047-3203,2018,207--219,56,Journal of Visual Communication and Image Representation,Discriminative kernel-based metric learning for face verification,article,CHONG2018207
"In recent years, information technology is developing continuously and set off a burst of artificial intelligence boom in the field of science. The development of advanced technologies such as unmanned driving and AI chips, is the extensive application of artificial intelligence. Face-related technologies have a wide range of applications because of intuitive results and good concealment. Since 3D face information can provide more comprehensive facial information than 2D face information, and it can solve many difficulties that cannot be solved in 2D face recognition. Therefore, more and more researchers have studied 3D face recognition in recent years. Under the new circumstances, the research on face are experiencing all kinds of challenges. With the tireless of many scientists, the new technology is also making a constant progress, and in the development of many technologies it still maintained its leading position. In this paper, we simply sort out the present development process of facial correlation technology, and the general evolution of this technology is outlined. Finally, the practical significance of this technology development is briefly discussed.","Face recognition, Face enhancement, 3D face reconstruction, Deep learning",Hongyan Fei and Bing Tu and Ququ Chen and Danbing He and Chengle Zhou and Yishu Peng,https://www.sciencedirect.com/science/article/pii/S104732031830227X,https://doi.org/10.1016/j.jvcir.2018.09.012,1047-3203,2018,139--143,56,Journal of Visual Communication and Image Representation,An overview of face-related technologies,article,FEI2018139
"In this paper, we develop a joint foveation-depth just-noticeable-difference (FD-JND) model to quantify the perceptual redundancy of image in the VR display environment. The proposed FD-JND model is developed with considerations on the effects of both foveation and depth. More specifically, experiments for the VR environment on synthesized stimuli are conducted based on luminance masking and contrast masking and the FD-JND model is developed accordingly. Subjective quality discrimination experiments between the noise contaminated images and original ones validate favorableness of the proposed FD-JND model.","Just-noticeable-difference, Depth, Foveation, Stereoscopic images, Virtual reality environment",Di Liu and Yingbin Wang and Zhenzhong Chen,https://www.sciencedirect.com/science/article/pii/S1047320318301871,https://doi.org/10.1016/j.jvcir.2018.07.015,1047-3203,2018,73--82,56,Journal of Visual Communication and Image Representation,Joint foveation-depth just-noticeable-difference model for virtual reality environment,article,LIU201873
"Joint image super-resolution refers to methods to enhance the resolution of an image with the guidance of a higher resolution image. It is similar to image completion, which is shown to benefit from larger receptive fields in recent deep neural network based methods. However, larger receptive fields increase the depths and parameters of the network, which may cause degradation and large memory consumption. To this end, we propose a joint residual pyramid model by introducing residual blocks and linear interpolation layers into the convolutional neural pyramid (CNP), and adopting the CNP in the joint super-resolution framework. Our model consists of three sub-networks, two for feature extraction concatenated by another for image reconstruction. Experimental results show that our model outperforms existing state-of-the-art algorithms not only on data pairs of RGB/depth images, but also on data pairs like color/saliency and color-scribbles/colorized images, without significantly sacrificing computation efficiency and memory space.","Deep learning, Neural convolutional pyramid, Joint super-resolution, Residual block",Yan Zheng and Xiang Cao and Yi Xiao and Xianyi Zhu and Jin Yuan,https://www.sciencedirect.com/science/article/pii/S1047320318303018,https://doi.org/10.1016/j.jvcir.2018.11.028,1047-3203,2019,53--62,58,Journal of Visual Communication and Image Representation,Joint residual pyramid for joint image super-resolution,article,ZHENG201953
"Panoramic visual tracking has high application value in many situations, but its visual distortion is likely to cause low tracking robustness or loss of target. This paper presents a panoramic visual tracking method based on adaptive feature fusion method, the size of the trapezoidal frame is calibrated with target moving in the method, and the linear model of trapezoidal frame parameters is fitted, then according to the model trapezoidal area of target extracted, and are modified to target trapezoidal region through the affine transformation; Based on filter tracking framework, the fusion of color and shape information is as the main characteristics of the target tracking, and Bayesian fusion recursive formula is used for calculating the particle weights. The experimental results show that the proposed algorithm is better than the existing methods in tracking precision and anti occlusion, and effectively improves the robustness of panoramic visual tracking.","Panoramic vision, Target tracking, Particle filtering, Feature fusion, Adaption",Long Liu and Zijing Yan and Qing Liu,https://www.sciencedirect.com/science/article/pii/S1047320318302566,https://doi.org/10.1016/j.jvcir.2018.10.020,1047-3203,2018,99--106,57,Journal of Visual Communication and Image Representation,Panoramic visual tracking based on adaptive mechanism,article,LIU201899
"Light field depth estimation has become a mature research topic and there are numerous algorithms introduced by various research groups. However, comprehensive and fair benchmark is difficult to apply because there are large step variances of the introduced algorithms. It is essential to analyze each step in the light field depth estimation so that it could help design better and more robust algorithms. Thus, a thorough analysis of cost aggregation is conducted in this paper to analyze the performance of various cost aggregation methods on light field depth estimation. A study on the parameter setting for each cost aggregation method is performed. Then, each cost aggregation with its optimal parameters is evaluated individually. Instead of using the standard rank system, this paper utilizes the weighted rank system based on the score difference on each criterion. Experimental results confirm that the guided-filter based method outperforms other methods in most evaluation criteria.","Light field, Depth estimation, Cost aggregation, Weighted rank, Benchmark",Williem and In Kyu Park,https://www.sciencedirect.com/science/article/pii/S1047320318302049,https://doi.org/10.1016/j.jvcir.2018.08.015,1047-3203,2018,38--51,56,Journal of Visual Communication and Image Representation,Cost aggregation benchmark for light field depth estimation,article,WILLIEM201838
"Due to the lack of accurate state judgment and health analysis of equipment operation, track circuit implements the repair and maintenance strategy of fault repair or planned repair. For this reason, a novel track circuit fault prediction method is proposed based on grey theory and expert system. In the proposed method, the feature of grey prediction model is to establish dynamic differential equation and then predict its own development according to its own data. The dynamic prediction model with equal dimension is applied to improve original grey model. Based on the gray models, the expert system is used to simulate human experts to solve the problems in a professional field. It contains man-computer interface, inference engine, knowledge library, knowledge management system, interpretation module and dynamic database. The measurement data show this system can effectively predict several typical faults of HVAP track circuit, and prove the proposed system structure is effective. Such condition-based fault prognostic maintenance mechanism provides an effective solution to improve equipment maintenance efficiency, reduce maintenance cost and reduce equipment fault rate.","Track circuit, Fault prediction, Grey theory, Expert system",Li-Qiang Hu and Chao-Feng He and Zhao-Quan Cai and Long Wen and Teng Ren,https://www.sciencedirect.com/science/article/pii/S1047320318302682,https://doi.org/10.1016/j.jvcir.2018.10.024,1047-3203,2019,37--45,58,Journal of Visual Communication and Image Representation,Track circuit fault prediction method based on grey theory and expert system,article,HU201937
"Correlation filter based tracking approach has been an important branch of visual tracking. However, most correlation filter based trackers fail to work under occlusion due to their frame-by-frame model update strategy, and the tracking performance can be further enhanced by optimizing the energy equation. The target appearance during tracking is nearly moving on a manifold. So, the classification scores should be similar on the target manifold. K Nearest Neighbor graphs are constructed and the classification scores on the neighborhood are regularized to have similar values. Through the local score propagation on the graph, the learned Graph Regularized Kernel Correlation Filer can represent different appearances of the object. Furthermore, in the proposed Multi-Memory Voting scheme, occlusion problem is addressed by voting from multiple target snapshots in the memory pool. An extensive evaluation on two recent benchmarks shows that the proposed tracker achieves competitive performance compared to nine other state-of-the-art trackers.","Visual object tracking, Graph Regularized Kernel Correlation Filer, Multi-Memory Voting, Drift handling",Weiwei Zheng and Huimin Yu and Wei Huang,https://www.sciencedirect.com/science/article/pii/S1047320318301913,https://doi.org/10.1016/j.jvcir.2018.08.004,1047-3203,2018,688--697,55,Journal of Visual Communication and Image Representation,Visual tracking via Graph Regularized Kernel Correlation Filer and Multi-Memory Voting,article,ZHENG2018688
"Background subtraction (BS) is a popular approach for detecting moving objects in video sequences for visual surveillance applications. In this paper, a new multi-channel and multi-resolution Wronskian change detection model (MCMRWM) based codebook background subtraction is proposed for moving object detection in the presence of dynamic background conditio ns. In the prooed MCMRWM, the multi-channel information helps to reduce the false negative of the foreground object; and the multi-resolution data suppresses the background noise resulting in reduced false positives. The proposed algorithm considers the ratio between feature vectors of current frame to the background model or its reciprocal in an adaptive manner, depending on the l2 norm of the feature vector, which helps to detect the foreground object completely without any false negatives. Extensive experiments are carried out with challenging video sequences to show the efficacy of the proposed algorithm against state-of-the-art BS techniques.","Visual surveillance, Moving object detection, Background subtraction, Dynamic backgrounds, Wronskian change detection model, Codebook model",Deepak Kumar Panda and Sukadev Meher,https://www.sciencedirect.com/science/article/pii/S104732031830186X,https://doi.org/10.1016/j.jvcir.2018.07.014,1047-3203,2018,52--72,56,Journal of Visual Communication and Image Representation,A new Wronskian change detection model based codebook background subtraction for visual surveillance applications,article,PANDA201852
"With the capability of fusing varying features from a specific image region, the Region Covariance Matrices (RCM) image descriptor has been evidenced plausible in face recognition. However, a systematic study for RCM, regarding which features to be fused in particular, remains absent. This paper therefore explores several features derived from the orthogonal filter ensembles, i.e., Identity Transform, Discrete Haar Transform, Discrete Cosine Transform, and Karhunen-LoÃ¨ve Transform, for feature encoding in RCM. Aside from that, we also outline a RCM variant, dubbed Region Log-TiedRank Covariance Matrices (RLTCM) in this paper. The RLTCM descriptor, on average, exhibits dramatic performance gain over RCM as well as state-of-the-art descriptors, especially when probe sets far deviated from the face gallery. Furthermore, we discern that the RLTCM descriptor defined based on Identity Transform, i.e., the simplest form of orthogonal filters, and other learning-free orthogonal filters yield impressive performance on par with the learning-based counterparts.","Orthogonal filters, Region covariance matrices, Log-TiedRank, Face recognition",Cong Jie Ng and Cheng Yaw Low and Kar-Ann Toh and Jaihie Kim and Andrew Beng Jin Teoh,https://www.sciencedirect.com/science/article/pii/S1047320318301639,https://doi.org/10.1016/j.jvcir.2018.07.002,1047-3203,2018,548--560,55,Journal of Visual Communication and Image Representation,Orthogonal filter banks with region Log-TiedRank covariance matrices for face recognition,article,NG2018548
"With the rapid development of internet technology, mining and retrieving the information from internet accurately is an urgent problem, among which, cross media retrieval becomes a hot spot of current research. This paper proposes a cross media retrieval approach, which learns two couples of projections based on different retrieval tasks. We first learn a common subspace to project heterogeneous media data to the isomorphic subspace, to measure the similarity of the heterogeneous media data in the isomorphic subspace. Second, we build isomorphic and heterogeneous adjacent graphs to preserve the correlations of the cross media data. Then we combine the two processes together to learn a common subspace. We also consider intra-class and inter-class similarity of images or texts in the unified framework. Third, the L2 norm is used to perform feature selection for different media data. Experimental results on three datasets demonstrate the effectiveness of the proposed approach.","Cross media retrieval, Subspace learning, Supervised graph regularization",Meijia Zhang and Huaxiang Zhang and Junzheng Li and Li Wang and Yixian Fang and Jiande Sun,https://www.sciencedirect.com/science/article/pii/S1047320318302967,https://doi.org/10.1016/j.jvcir.2018.11.025,1047-3203,2019,1--11,58,Journal of Visual Communication and Image Representation,Supervised graph regularization based cross media retrieval with intra and inter-class correlation,article,ZHANG20191
"The majority of existing objective Image Quality Assessment (IQA) methods are designed for evaluation of images corrupted by single distortion types. However, images may be degraded with multiple distortions during processing stages. In this paper, we propose a reduced-reference IQA algorithm to predict the quality of multiply-distorted images. An image is first decomposed into predicted and disorderly portions based on the internal generative mechanism theory. The structural information is captured from the predicted image by using a shearlet representation and RÃ©nyi directional entropy is deployed to measure the disorderly information changes. Finally, we introduce the application of a framework namely Learning Using Privileged Information (LUPI) to build a quality model and obtain quality scores. During training, the LUPI framework utilizes a set of additional privileged data to learn an improved quality model. Experimental results on multiply-distorted image datasets (MLIVE and MDID2015) confirm the effectiveness of the proposed IQA model.","Image quality, Multiply-distortion types, Shearlet transform, Entropy analysis, Support vector regression, Privileged information",Saeed Mahmoudpour and Peter Schelkens,https://www.sciencedirect.com/science/article/pii/S1047320318302724,https://doi.org/10.1016/j.jvcir.2018.10.027,1047-3203,2018,125--137,57,Journal of Visual Communication and Image Representation,Reduced-reference quality assessment of multiply-distorted images based on structural and uncertainty information degradation,article,MAHMOUDPOUR2018125
"With the rapid development of biometric identification technology, face recognition has been one of the most widely used as its important component. It facilitates a series of applications such as security, military, transportation, education and other fields. The demand for face feature recognition is increasing. However the current techniques still exist some deficiencies. In this paper, we proposed a face-mask recognition method for fraud prevention based on Gaussian Mixture Model. We address the problem of identifying the face and mask in the area of financial security precaution. And we show how to combine opencv with dlib to recognition face and extract it. We use Gaussian Mixture Model (GMM) to construct the model of human faces. According to this, we calculate the similarity between the face sample and the model. By analyzing and learning the features of faces, we can predict whether the image of which we test is a human face or a mask. Compared with other traditional method of face recognition, our approach has been targeted to strengthen the ability to recognize abnormal faces such as sunglasses, masks and respirator, and reduce the potential danger of these unusual faces in the security field. It is simple to be calculated and has a higher accuracy. In addition, our method have enhanced the robustness of the algorithm about mask recognition.","Gussian Mixture Model (GMM), Dlib, Deep learning, Fraud prevention",Ququ Chen and Lei Sang,https://www.sciencedirect.com/science/article/pii/S1047320318302050,https://doi.org/10.1016/j.jvcir.2018.08.016,1047-3203,2018,795--801,55,Journal of Visual Communication and Image Representation,Face-mask recognition for fraud prevention using Gaussian mixture model,article,CHEN2018795
"Availability of low-cost range sensors and the development of relatively robust algorithms for the extraction of skeleton joint locations have inspired many researchers to develop human activity recognition methods using 3-D data. In this paper, an effective method for the recognition of human activities from the normalized joint trajectories is proposed. We represent the actions as multidimensional signals and introduce a novel method for generating action templates by averaging the samples in a âdynamic timeâ sense. Then, in order to deal with the variations in speed and style of performing actions, we warp the samples with action templates by an efficient algorithm and employ wavelet filters to extract meaningful spatiotemporal features. The proposed method is also capable of modeling the human-object interactions, by performing the template generation and temporal warping procedure via the joint and object trajectories simultaneously. Experimental evaluations on several challenging datasets demonstrates the effectiveness of our method compared to the state-of-the-arts as well as its robustness against different sources of noise.","Human activity recognition, RGB-D sensors, Trajectory-based representation, Action template, Dynamic time warping (DTW), Human object interaction",Saeed Ghodsi and Hoda Mohammadzade and Erfan Korki,https://www.sciencedirect.com/science/article/pii/S1047320318301883,https://doi.org/10.1016/j.jvcir.2018.08.001,1047-3203,2018,729--741,55,Journal of Visual Communication and Image Representation,Simultaneous joint and object trajectory templates for human activity recognition from 3-D data,article,GHODSI2018729
"With the advancement in the digital camera technology, the use of high resolution images and videos has been widespread in the modern society. In particular, image and video frame registration is frequently applied in computer graphics and film production. However, conventional registration approaches usually require long computational time for high resolution images and video frames. This hinders the application of the registration approaches in the modern industries. In this work, we first propose a new image representation method to accelerate the registration process by triangulating the images effectively. For each high resolution image or video frame, we compute an optimal coarse triangulation which captures the important features of the image. Then, we apply a surface registration algorithm to obtain a registration map which is used to compute the registration of the high resolution image. Experimental results suggest that our overall algorithm is efficient and capable to achieve a high compression rate while the accuracy of the registration is well retained when compared with the conventional grid-based approach. Also, the computational time of the registration is significantly reduced using our triangulation-based approach.","Triangulated image, Image registration, Coarse triangulation, Map interpolation",Chun Pang Yung and Gary P.T. Choi and Ke Chen and Lok Ming Lui,https://www.sciencedirect.com/science/article/pii/S1047320318301640,https://doi.org/10.1016/j.jvcir.2018.07.005,1047-3203,2018,561--571,55,Journal of Visual Communication and Image Representation,Efficient feature-based image registration by mapping sparsified surfaces,article,YUNG2018561
"The scanner-dependent variations effect fluctuation of intensities of MR images, even under the fixed condition of key parameters: the scanner, the patient, the body region and the type of MRI protocol. The inherent variation causes the lack of a standard and quantifiable interpretation of image intensities. Moreover, the unbalanced distribution of intensity values lowers accuracy and sensitivity of automatic analysis and segmentation based on MR images. As such, we proposed a uniformizing method to make the distribution even while ensuring that similar intensities of MR images reflect the same tissue after processed. Our experiments based on the 3D brain tumor MR images proved that this method can significantly improve labeling and segmentation accuracy as compared to conventional preprocessed methods.","MR images, Intensity value, Distribution uniformization",Jie Chang and Naijie Gu and Xiaoci Zhang and Li Yang and Chuanwen Lin and Zengshi Huang and Junjie Su,https://www.sciencedirect.com/science/article/pii/S1047320318302578,https://doi.org/10.1016/j.jvcir.2018.10.021,1047-3203,2018,138--151,57,Journal of Visual Communication and Image Representation,An uniformizing method of MR image intensity transformation,article,CHANG2018138
"The last few decades have witnessed rapid development of visual saliency detection, as it can detect object-of-interest from clutter environments to substantially facilitate a wide range of applications. However, traditional visual saliency detection models primarily rely on image features, which may face great challenges in low contrast video stream captured from low lighting scenarios. This paper proposes a dynamic multimodal fusion based visual saliency detection model towards low contrast videos, which combines saliency information from spatial, frequency, and temporal domains. In spatial domain, superpixel covariance is utilized to compute the region dissimilarity under low lighting scenarios; in frequency domain, the amplitude spectrum tuned method is used to suppress the background noise; in temporal domain, the incremental learning is employed to efficiently update background model from high dimensional video streams. Extensive experiments have been conducted to validate the effectiveness of the proposed model.","Salient object detection, Low contrast videos, Multimodal fusion, Region covariance, Incremental learning",Nan Mu and Xin Xu and Xiaolong Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318302852,https://doi.org/10.1016/j.jvcir.2018.11.012,1047-3203,2019,79--88,58,Journal of Visual Communication and Image Representation,A spatial-frequency-temporal domain based saliency model for low contrast video sequences,article,MU201979
"Medical doctors use diagnostic imaging techniques such as X-rays, CT scans and MRI, for detecting diseases or narrowing down possible causes of pain. This often require sharing and transmitting medical images over public channels. In this work we adapt Shamirâs secret sharing paradigm to propose a novel lossless scheme for secure sharing of medical images. The proposed scheme takes advantage of the redundancy in typical medical images to reduce share sizes, and hence facilitate storing and sharing. To this end, we employ a customized run-length encoding method to compress the medical image. We conduct an extensive performance analysis on the proposed scheme, including a comparison with some existing Shamir-type secret image sharing schemes.","Secret sharing, Secret image sharing, -threshold scheme, Chaos",A. Kanso and M. Ghebleh,https://www.sciencedirect.com/science/article/pii/S1047320318302335,https://doi.org/10.1016/j.jvcir.2018.09.018,1047-3203,2018,245--255,56,Journal of Visual Communication and Image Representation,An efficient lossless secret sharing scheme for medical images,article,KANSO2018245
"Multi-view data has become commonplace in today's computer vision applications, for the same object can be sampled through various viewpoints or by different instruments. The large discrepancy between distinct even heterogenous views bring the challenge of handling multi-view data. To obtain intrinsic common representation shared by all views, this paper proposes a novel multi-view algorithm called Multiview Marginal Discriminant Projection (MMDP), which is a supervised dimensionality reduction method for searching latent common subspace across multiple views. MMDP takes both inter-view and intra-view discriminant information into account and can preserve the global geometric structure and local discriminant structure of data manifold. Furthermore, the performance of MMDP is improved via imposing graph embedding as a regularization term to give a penalization of the local data geometric structure violation, which is called Graph regularized Multiview Marginal Discriminant Projection (GMMDP). The extensive experimental results on face recognition tasks demonstrate the effectiveness and robustness of MMDP and GMMDP. Finally, this paper excavates a new application scenario of multi-view learning and introduce it including the proposed GMMDP into solving hyperspectral image classification (HIC) problem, which leads to a satisfactory result.","Marginal discriminant projection, Graph Laplacian, Manifold regularization, Multiview learning, Dimensionality reduction, Hyperspectral images classification",Heng Pan and Jinrong He and Yu Ling and Lie Ju and Guoliang He,https://www.sciencedirect.com/science/article/pii/S1047320318302451,https://doi.org/10.1016/j.jvcir.2018.10.009,1047-3203,2018,12--22,57,Journal of Visual Communication and Image Representation,Graph regularized multiview marginal discriminant projection,article,PAN201812
"One of the potential 3D imaging techniques relies on the use of stereoscopic systems. The great interest in these systems has resulted in huge amount of data which needs to be compressed for storage and transmission purposes. In this context, vector lifting scheme has been found to be an efficient approach for stereo image coding. For instance, the coding performance depends on the design of the involved lifting operators referred to as prediction and update filters. For this reason, while a non separable vector lifting structure is retained, we investigate different techniques for optimizing sparse criteria to design the filters used with both views. More precisely, an independent full optimization algorithm as well as a joint algorithm will be developed and studied. Simulations performed on different stereo images demonstrate the effectiveness of the proposed sparse optimization algorithms in terms of quality of reconstruction and bitrate saving.","Stereo images, Adaptive coding, Vector lifting scheme, Non separable transform, Optimization",I. Bezzine and M. Kaaniche and S. Boudjit and A. Beghdadi,https://www.sciencedirect.com/science/article/pii/S1047320318302694,https://doi.org/10.1016/j.jvcir.2018.10.025,1047-3203,2018,283--293,57,Journal of Visual Communication and Image Representation,Sparse optimization of non separable vector lifting scheme for stereo image coding,article,BEZZINE2018283
"In this era, due to the widespread availability of digital devices, various open source and commercially available image editing tools have made authenticity of image contents questionable. Copy-move forgery (CMF) is a common technique to produce tampered images by concealing undesirable objects or replicating desirable objects in the same image. Therefore, means are required to authenticate image contents and identify the tampered areas. In this paper, a robust technique for CMF detection and localization in digital images is proposed. The technique extracts stationary wavelet transform (SWT) based features for exposing the forgeries in digital images. SWT is adopted because of its impressive localization properties, in both spectral and spatial domains. More specifically approximation subband of the stationary wavelet transform is utilized as this subband holds most of the information that is best suited for forgery detection. The dimension of the feature vectors is reduced by applying discrete cosine transform (DCT). To evaluate the proposed technique, we use two standard datasets namely, the CoMoFoD and the UCID for experimentations. The experimental results reveal that the proposed technique outperforms the existing techniques in terms of true and false detection rate. Consequently, the proposed forgery detection technique can be applied to detect the tampered areas and the benefits can be obtained in image forensic applications.","Copy-move forgery, Tampered images, Forgery detection, Authenticity, Passive authentication",Toqeer Mahmood and Zahid Mehmood and Mohsin Shah and Tanzila Saba,https://www.sciencedirect.com/science/article/pii/S1047320318300713,https://doi.org/10.1016/j.jvcir.2018.03.015,1047-3203,2018,202--214,53,Journal of Visual Communication and Image Representation,A robust technique for copy-move forgery detection and localization in digital images via stationary wavelet and discrete cosine transform,article,MAHMOOD2018202
"Nonparametric Bayesian dictionary learning has shown a powerful potential in image restoration. However, it still lacks exploiting image structure to improve the performance. In this work, we propose a sparse Bayesian dictionary learning framework with structure prior called nonlocal structured beta process factor analysis (NLS-BPFA) which connects nonlocal self-similarity and sparse Bayesian dictionary learning. A nonlocal structured beta process is proposed to introduce the nonlocal self-similarity as a structure prior for image denoising and inpainting. Unlike most of the existing image denoising methods, our proposed method does not need to know noise variance in advance like an unsupervised learning. The experimental results demonstrate the effectiveness of our proposed model.","Nonparametric Bayesian, Beta process, Image restoration, Nonlocal structure prior, Dictionary learning",Zhou Liu and Lei Yu and Hong Sun,https://www.sciencedirect.com/science/article/pii/S1047320318300439,https://doi.org/10.1016/j.jvcir.2018.02.011,1047-3203,2018,159--169,52,Journal of Visual Communication and Image Representation,Image restoration via Bayesian dictionary learning with nonlocal structured beta process,article,LIU2018159
"In this paper, we formulate the soccer video event detection task as a sparse representation problem by learning a supervised, discriminative and event-oriented dictionary based on learned weighted local features. To this end, we present a novel framework based on two ideas: First, we propose an approach for computing the representativeness of each video frame for each soccer event. Second, we propose an Adaptive Label-Consistent K-SVD (ALC-KSVD) algorithm to learn an event-oriented and discriminative dictionary based on the computed representativeness of frames to transfer video frames to a sparse space. To improve discrimination among frames of different events, we proposed a weighting method to identify local features that are more representative in each event category. Next, the representativeness score of each frame is calculated by aggregating the weighted local features within each frame. The calculated representativeness score of each frame indicates its belonging degree to each event. The representativeness score matrix, being a discriminative term, is combined with the reconstruction error to form an objective function to improve the discrimination ability in the sparse representation during the dictionary learning process. The obtained objective function is efficiently and optimally solved by the K-SVD algorithm. The representativeness score matrix, which is automatically calculated based on the training samples, defines an adaptive correspondence between the dictionary atoms and the labels of the frames. We demonstrate the effectiveness of the proposed framework on the detection and classification of several soccer events based on an extensive experimental investigation that was conducted using a large collection of video data. The experimental results indicate that our approach maintains good classification performance and outperforms the state-of-the-art methods.","Soccer videos, Event detection, Sparse coding, Supervised dictionary learning, Adaptive discriminative dictionary learning",Babak Fakhar and Hamidreza Rashidy Kanan and Alireza Behrad,https://www.sciencedirect.com/science/article/pii/S1047320318301408,https://doi.org/10.1016/j.jvcir.2018.06.014,1047-3203,2018,489--503,55,Journal of Visual Communication and Image Representation,Learning an event-oriented and discriminative dictionary based on an adaptive label-consistent K-SVD method for event detection in soccer videos,article,FAKHAR2018489
"As one of the most commonly used dimension reduction approaches, discriminant non-negative matrix factorization (NMF) has been widely used for data representation in the pattern classification task. However, the previous discriminant NMFs emphasize the Fisher criterion or maximum margin criterion which has high requirement to the distribution of data. Therefore, this work proposes a discriminative label embedded NMF (LENMF) algorithm. LENMF takes into account the discriminative label embedding to obtain the low-dimensional projected data and orthogonal property of the non-negative basis to strength the ability of parts-based representation. Besides, LENMF is extended in the kernel space to explore the nonlinear relations of data. By integrating the non-negative constraint, discriminative label embedding, and the orthogonal property into the proposed objective, the multiplicative updating rules have been given in this work. Experiment results on the challenging face, object, document, and digit databases illustrate the performance of the proposed algorithm.","Non-negative matrix factorization (NMF), Discriminative label embedding, Orthogonality constraint, Pattern classification",Wenjie Zhu and Yunhui Yan,https://www.sciencedirect.com/science/article/pii/S1047320318301524,https://doi.org/10.1016/j.jvcir.2018.06.030,1047-3203,2018,477--488,55,Journal of Visual Communication and Image Representation,Non-negative matrix factorization via discriminative label embedding for pattern classification,article,ZHU2018477
"First-person videos (FPVs) or egocentric videos provide a huge amount of data for visual lifelogs. The quality assessment of frames in FPVs serves as an important tool, feature or evaluation baseline for not only structuring but also analyzing lifelogs. To develop a frame-quality measure for FPVs, we introduce a new strategy for image quality estimation, called mutual reference (MR), which uses one or more pseudo-reference images to evaluate a test image. We then propose a MR quality estimator, called Local Visual Information (LVI), that primarily measures the relative blur between two images. To apply the MR strategy to FPVs, we propose a mutual reference frame quality assessment for FPVs (MRFQAFPV) framework which incorporates LVI. Our results, using both real and synthetic distortions and objective and subjective tests, demonstrate both methods perform better than existing NR QEs at measuring the quality of frames in FPVs.","First-person videos, Local Visual Information (LVI), Mutual reference, Image quality assessment, Pseudo-reference",Chen Bai and Amy R. Reibman,https://www.sciencedirect.com/science/article/pii/S1047320318301007,https://doi.org/10.1016/j.jvcir.2018.05.005,1047-3203,2018,123--132,54,Journal of Visual Communication and Image Representation,Image quality assessment in first-person videos,article,BAI2018123
"Enhanced Local Tone Mapping (ELTM) is a flexible tone mapping operator designed to provide a good global and local contrast simultaneously over various test scenes. Also, it has intuitive and decoupled tuning interface, providing the user with full control over final image appearance. ELTM is based on detail/base layer decomposition compressing the base plane in both linear and logarithmic domain. This provides robustness to ELTM, while modified tone compression function provides good local contrast. Results were validated using set of images with various content, brightness and resolution. In this testing ELTM performed as the best tone mapping operator, among 7 state-of-the-art global and local tone mapping operators. Even better overall results are achieved by using proposed brightness control, to handle extreme scenes. Robustness and flexibility to achieve desired appearance makes ELTM suitable for applications where user experience is the primary concern as is the case with consumer electronics products.",,Dragomir M. {El Mezeni} and Lazar V. Saranovac,https://www.sciencedirect.com/science/article/pii/S1047320318300555,https://doi.org/10.1016/j.jvcir.2018.03.007,1047-3203,2018,122--133,53,Journal of Visual Communication and Image Representation,Enhanced local tone mapping for detail preserving reproduction of high dynamic range images,article,ELMEZENI2018122
"Quasi-flat zones are morphological operators which partition the image into homogeneous regions with respect to certain criteria. They are used in grayscale and multivariate image simplification and segmentation. However, they often induce an oversegmentation of the image, taking the shape of narrow transition regions between objects and small regions which are a few pixels wide. Various methods have been devised in order to reduce this oversegmentation, which remove the unwanted zones according to some criteria and then grow the remaining regions. In this paper we propose improvements in transition region and area threshold filtering. We also combine the two filtering methods for further-improved results. We apply the proposed approaches in color image segmentation and hyperspectral pixel classification.","Quasi-flat zones, Mathematical morphology, Color image segmentation, Hyperspectral pixel classification, EDICS: 4.4 morphological image analysis",Radu-Mihai Coliban and Mihai Ivanovici,https://www.sciencedirect.com/science/article/pii/S1047320318300798,https://doi.org/10.1016/j.jvcir.2018.04.003,1047-3203,2018,281--293,53,Journal of Visual Communication and Image Representation,Reducing the oversegmentation induced by quasi-flat zones for multivariate images,article,COLIBAN2018281
"This paper addresses the co-segmentation problem using feature visualization for CNNs. Visualization is exploited as an auxiliary information to discriminate salient image regions (dubbed as âheat-regionsâ) from non-salient ones. Region occlusion sensitivity is proposed for feature visualization. The co-segmentation problem is formulated via a convex quadratic optimization which is initialized by the heat-regions. The information obtained through the visualization is considered as an extra energy term in the cost function. The results of the visualization demonstrate that there exist some heat-regions which are not productive in the co-segmentation. To detect helpful regions among them, an adaptive strategy in the form of an iterative algorithm is proposed according to the consistency among all images. Comparison experiments conducted on two benchmark datasets, iCoseg and MSRC, illustrate the superior performance of the proposed approach over state-of-the-art algorithms.","Co-segmentation, Convolutional Neural Network (CNN), Feature visualization, Occlusion sensitivity, Adaptive learning",Zahra Kamranian and Federico Tombari and Ahmad Reza {Naghsh Nilchi} and Amirhassan Monadjemi and Nassir Navab,https://www.sciencedirect.com/science/article/pii/S1047320318301093,https://doi.org/10.1016/j.jvcir.2018.05.014,1047-3203,2018,201--214,55,Journal of Visual Communication and Image Representation,Co-segmentation via visualization,article,KAMRANIAN2018201
"In this study, an interactive Chinese portrait rendering system was developed. This portrait rendering system can generate a user-lookalike ink portrait by blending the userâs face with a selected Chinese ink painting. It first automatically analyzes the userâs facial features and then integrates them into a selected Chinese painting. This system comprises two processes: an offline process and an online process. During the offline process, a collection of Chinese portrait paintings is configured (e.g., the face masks and facial coordinates of the paintings are determined). Subsequently, blending-ready templates (faces without facial features) are prepared for the online process. During the online process, the user integrates their photograph into our rendering system. The system automatically analyzes the face orientation, color, and facial features and adjusts the attributes of the photograph to match the templateâs configuration. The produced facial image is blended into a selected template, which preserves the textures of the original Chinese painting. The results reveal that our system preserved both the user characteristics and original painting styles. In this study, user-portrait matching was experimentally evaluated, and a questionnaire survey on satisfaction with painting style was conducted.","Active shape model, Chinese portrait, Facial feature extraction, Non-photorealistic rendering",Pei-Ying Chiang and Chun-Von Lin and Cheng-Hua Tseng,https://www.sciencedirect.com/science/article/pii/S1047320318300282,https://doi.org/10.1016/j.jvcir.2018.02.002,1047-3203,2018,33--44,52,Journal of Visual Communication and Image Representation,Generation of Chinese ink portraits by blending face photographs with Chinese ink paintings,article,CHIANG201833
"The number of users using multimedia streaming services in various environments is increasing. Accordingly, HyperText Transfer Protocol (HTTP) adaptive streaming has attracted attention. Recently, there have been many studies on schemes for improving the Quality of Experience (QoE) by using multipaths in HTTP adaptive streaming. Downloading segments at the same time using multipaths can take up a high amount of bandwidth and make the transmission robust to data loss. However, when the bandwidths of the multipaths are aggregated, the variation of the bandwidth fluctuation increases, so quality changes occur more frequently. Additionally, it takes time to reorder the segments due to different segment download times received through the multipath. To this end, we propose a multipath-based transmission scheme to improve the QoE of HTTP adaptive streaming in the multipath environment. The proposed scheme improves the QoE by increasing the average video quality and reducing the frequency of video quality changes.","HTTP adaptive streaming, Quality of experience, Multipath",Yunho Kim and Kwangsue Chung,https://www.sciencedirect.com/science/article/pii/S1047320318301135,https://doi.org/10.1016/j.jvcir.2018.05.017,1047-3203,2018,12--20,55,Journal of Visual Communication and Image Representation,Multipath-based transmission scheme for improving the QoE of HTTP adaptive streaming,article,KIM201812
"We present a scalable pipeline for Free-Viewpoint Video (FVV) content creation, considering also visualisation in Augmented Reality (AR) and Virtual Reality (VR). We support a range of scenarios where there may be a limited number of handheld consumer cameras, but also demonstrate how our method can be applied in professional multi-camera setups. Our novel pipeline extends many state-of-the-art techniques (such as structure-from-motion, shape-from-silhouette and multi-view stereo) and incorporates bio-mechanical constraints through 3D skeletal information as well as efficient camera pose estimation algorithms. We introduce multi-source shape-from-silhouette (MS-SfS) combined with fusion of different geometry data as crucial components for accurate reconstruction in sparse camera settings. Our approach is highly flexible and our results indicate suitability either for affordable content creation for VR/AR or for interactive FVV visualisation where a user can choose an arbitrary viewpoint or sweep between known views using view synthesis.","Free-viewpoint video, 3D reconstruction, Texturing, View synthesis, Augmented reality, Virtual reality",R. PagÃ©s and K. Amplianitis and D. Monaghan and J. OndÅej and A. SmoliÄ,https://www.sciencedirect.com/science/article/pii/S1047320318300683,https://doi.org/10.1016/j.jvcir.2018.03.012,1047-3203,2018,192--201,53,Journal of Visual Communication and Image Representation,Affordable content creation for free-viewpoint video and VR/AR applications,article,PAGES2018192
"This paper presents an efficient and effective interactive segmentation scheme for extracting the region of a foreground object in an image. Our goal is to design an interactive segmentation algorithm that unifies the bounding-box-based, seed-based, and query-based interaction mechanisms for pursuing (i) high efficiency in simple interaction mechanism, (ii) few interaction rounds, and (iii) short response time. The proposed algorithm starts with a user-provided bounding box and obtains candidate background superpixels for inferring the foreground object. Our algorithm tolerates imprecise bounding boxes and provides two kinds of interactions for acquiring correct labels from the user. The user can either input the seed/scribble annotations or label the algorithm-queried regions. Our algorithm selects the most uncertain region as a query, and this query-based interaction mechanism reduces the burden of the user on deciding suitable annotation locations. The average response time per-interaction of our algorithm is merely 0.014â¯s. Our experiments demonstrate that the algorithm achieves an efficient unified scheme for interactive image segmentation.","Interactive, Image segmentation, Manifold ranking, Machine-assisted",Ding-Jie Chen and Hwann-Tzong Chen and Long-Wen Chang,https://www.sciencedirect.com/science/article/pii/S1047320318301378,https://doi.org/10.1016/j.jvcir.2018.06.011,1047-3203,2018,393--403,55,Journal of Visual Communication and Image Representation,Toward a unified scheme for fast interactive segmentation,article,CHEN2018393
"Background subtraction has been widely discussed in video surveillance, but it still has open challenges such as dynamic background, illumination variation. To address these challenges a novel Cut set-based Dynamic Key frame selection (CDK) and Adaptive Layer-based Background Modeling (ALBM) approach for background subtraction is proposed which adaptively changes layers in the background model for each scenario such as static, dynamic background and high illumination. The concept of key frame is used to choose representative frames from the video. In order to capture the invariant directional codes of each spatio-temporal patch symmetric operators such as line and rotational symmetry are used. The proposed method identifies highly similar static spatio-temporal patches and sets it as background there by reducing the computational complexity in the foreground detection step. Both qualitative and quantitative evaluations on challenging video sequences demonstrate that the proposed algorithm performs background subtraction more favorably than the state-of-the-art methods.","Cut set-based Dynamic Key frame selection, Adaptive Layer-based Background Modeling, Background subtraction, Object tracking",D. Jeyabharathi and  Dejey,https://www.sciencedirect.com/science/article/pii/S1047320318301494,https://doi.org/10.1016/j.jvcir.2018.06.024,1047-3203,2018,434--446,55,Journal of Visual Communication and Image Representation,Cut set-based Dynamic Key frame selection and Adaptive Layer-based Background Modeling for background subtraction,article,JEYABHARATHI2018434
"This paper proposes a sparsity constraint nearest subspace classifier (SNSC) for target recognition of synthetic aperture radar (SAR) images. Unlike optical images, SAR images are highly sensitive to target azimuth. Therefore, the global dictionary collaborated by samples from different classes has high between-class correlation, which will impair the performance of sparse representation-based classification (SRC). Furthermore, even on the subspace spanned by a single class, only a small number of samples with similar azimuths to the test image are highly correlated with the test image. Thus, the linear coefficients over the subspace are actually sparse ones. Therefore, in this paper we impose sparsity constraint on nearest subspace classifier (NSC) classifier and apply it to SAR target recognition. The target label of the test sample is decided to be the class with the minimum reconstruction error. The proposed method is tested on moving and stationary target acquisition and recognition (MSTAR) dataset and compared with several state-of-the-art methods and the experimental results verify the validity and robustness of the proposed method.","Syntheticaperture radar (SAR), Target recognition, Sparsity constraint nearest subspace classifier (SNSC), Sparse representation based classification (SRC)",Baiyuan Ding and Gongjian Wen,https://www.sciencedirect.com/science/article/pii/S1047320318300440,https://doi.org/10.1016/j.jvcir.2018.02.012,1047-3203,2018,170--176,52,Journal of Visual Communication and Image Representation,Sparsity constraint nearest subspace classifier for target recognition of SAR images,article,DING2018170
"Tracking-by-detection (TBD) is widely used in visual object tracking. However, many TBD-based methods ignore the strong motion correlation between current and previous frames. In this work, a motion-guided convolutional neural network (MGNet) solution to online object tracking is proposed. The MGNet tracker is built upon the multi-domain convolutional neural network with two innovations: (1) a motion-guided candidate selection (MCS) scheme based on a dynamic prediction model is proposed to accurately and efficiently generate the candidate regions and (2) the spatial RGB and temporal optical flow are combined as inputs and processed in an unified end-to-end trained network, rather than a two-branch processing network. We compare the performance of the MGNet, the MDNet and several state-of-the-art online object trackers on the OTB and the VOT benchmark datasets, and demonstrate that the temporal correlation between any two consecutive frames in videos can be more effectively captured by the MGNet via extensive performance evaluation.","Object tracking, Online tracking, Convolutional neural network, Optical flow, Multi-domain learning",Weihao Gan and Ming-Sui Lee and Chi-hao Wu and C.-C. (Jay) Kuo,https://www.sciencedirect.com/science/article/pii/S1047320318300725,https://doi.org/10.1016/j.jvcir.2018.03.016,1047-3203,2018,180--191,53,Journal of Visual Communication and Image Representation,Online object tracking via motion-guided convolutional neural network (MGNet),article,GAN2018180
"To encode the depth maps efficiently, 3D-HEVC introduced three intra-frame prediction tools: (i) Depth Intra Skip (DIS), (ii) Depth Modeling Modes (DMMs), and (iii) Segment-wise DC Coding (SDC) that raise the encoding effort. Therefore, we analyzed the most time-consuming steps at the intra-frame prediction and proposed a model-level scheme for reducing the encoding time. The most time-consuming encoding steps were the DMM-1 wedgelet search and the Rate-Distortion (RD) list evaluation in Transform-Quantization and SDC flows. Consequently, we proposed a scheme composed of two solutions for speeding up the DMM-1 and one solution for reducing the RD-list size, accelerating the RD-list evaluation. The DMM-1 speeding up solutions use the information of neighbor-encoded blocks and the data contained in the border of the encoding block to accelerate the wedgelet search. The proposed scheme reduces 31.9% the encoding time with an impact of 0.272% in the Bjontegaard Delta-rate (BD-rate), surpassing the related works.","3D-HEVC, Intra-frame prediction, Depth maps, 3D video coding",Gustavo Sanchez and Luciano Agostini and CÃ©sar Marcon,https://www.sciencedirect.com/science/article/pii/S1047320318300968,https://doi.org/10.1016/j.jvcir.2018.05.003,1047-3203,2018,193--203,54,Journal of Visual Communication and Image Representation,A reduced computational effort mode-level scheme for 3D-HEVC depth maps intra-frame prediction,article,SANCHEZ2018193
"Change blindness is a type of visual masking which affects our ability to notice changes introduced in visual stimuli (e.g. change in the colour or position of an object). In this paper, we propose to use it as a means to identify image attributes that are less important than others. We propose a model of visual awareness based on low-level saliency detection and image inpainting, which identifies textured regions within images that are the most prone to change blindness. Results from a user study demonstrate that our model can generate alternative versions of natural scenes which, while noticeably different, have the same visual quality as the original. We show an example of practical application in image compression.","Perception, Visual awareness, Visual attention, Change blindness, Salience, Image quality",Steven {Le Moan} and Ivar Farup and Jana BlahovÃ¡,https://www.sciencedirect.com/science/article/pii/S1047320318300841,https://doi.org/10.1016/j.jvcir.2018.04.008,1047-3203,2018,31--38,54,Journal of Visual Communication and Image Representation,Towards exploiting change blindness for image processing,article,LEMOAN201831
"In this paper, we propose a novel framework for underwater image saliency detection by exploiting Quaternionic Distance Based Weber Descriptor (QDWD), pattern distinctness, and local contrast. Our proposed algorithm incorporates quaternion number system and principal components analysis (PCA) simultaneously, so as to achieve superior performance. In our algorithm, QDWD, which was initially designed for detecting outliers in color images, is used to represent the directional cues in an underwater image. Then, PCA coordinate system is employed to compute pattern distinctness. Meanwhile, we utilize local contrast to further highlight salient regions and suppress background regions. Finally, by integrating QDWD, pattern distinctness, and local contrast, a reliable saliency map for underwater images can be computed and estimated. Experimental results, based on the publicly available OUC-VISION underwater image database, show that the proposed method can produce reliable and promising results, compared to other state-of-the-art saliency-detection models.","Underwater image, Saliency detection, QDWD, Pattern distinctness, Local contrast",Muwei Jian and Qiang Qi and Junyu Dong and Yilong Yin and Kin-Man Lam,https://www.sciencedirect.com/science/article/pii/S1047320318300567,https://doi.org/10.1016/j.jvcir.2018.03.008,1047-3203,2018,31--41,53,Journal of Visual Communication and Image Representation,Integrating QDWD with pattern distinctness and local contrast for underwater saliency detection,article,JIAN201831
"Tracking a target of interest in both sparse and crowded environments is a challenging problem, not yet successfully addressed in the literature. In this paper, we propose a new long-term visual tracking algorithm, learning discriminative correlation filters and using an online classifier, to track a target of interest in both sparse and crowded video sequences. First, we learn a translation correlation filter using a multi-layer hybrid of convolutional neural networks (CNN) and traditional hand-crafted features. Second, we include a re-detection module for overcoming tracking failures due to long-term occlusions using online SVM and Gaussian mixture probability hypothesis density (GM-PHD) filter. Finally, we learn a scale correlation filter for estimating the scale of a target by constructing a target pyramid around the estimated or re-detected position using the HOG features. We carry out extensive experiments on both sparse and dense data sets which show that our method significantly outperforms state-of-the-art methods.","Visual tracking, Correlation filter, CNN features, Hybrid features, Online learning, GM-PHD filter",Nathanael L. Baisa and Deepayan Bhowmik and Andrew Wallace,https://www.sciencedirect.com/science/article/pii/S1047320318301536,https://doi.org/10.1016/j.jvcir.2018.06.027,1047-3203,2018,464--476,55,Journal of Visual Communication and Image Representation,Long-term correlation tracking using multi-layer hybrid features in sparse and dense environments,article,BAISA2018464
"Most existing cross-modal retrieval methods ignore the discriminative semantics embedded in multi-modal data and the unique characteristics of different sub-retrieval tasks. To address the problem, we propose a novel approach in this paper, which is named Joint Feature selection and Graph regularization for Modality-dependent cross-modal retrieval (JFGM). The key idea of JFGM is learning modality-dependent subspaces for different sub-retrieval tasks while simultaneously preserving the semantic consistency of multi-modal data. Specifically, besides to the shared subspace learning between different modalities, a linear regression term is introduced to further correlate the discovered modality-dependent subspace with the explicit semantic space. Furthermore, a multi-model graph regularization term is formulated to preserve the inter-modality and intra-modality semantic consistency. In order to avoid over-fitting problems and select the discriminative features, l2,1-norm is imposed on the projection matrices. Experimental results on several publicly available datasets demonstrate the superiority of the proposed method compared with several state-of-the-art approaches.","Cross-modal retrieval, Feature selection, Subspace learning, Graph regularization",Li Wang and Lei Zhu and Xiao Dong and Li Liu and Jiande Sun and Huaxiang Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318301019,https://doi.org/10.1016/j.jvcir.2018.05.006,1047-3203,2018,213--222,54,Journal of Visual Communication and Image Representation,Joint feature selection and graph regularization for modality-dependent cross-modal retrieval,article,WANG2018213
"Image segmentation with clustering approach is widely used in biomedical application. Accurate brain Magnetic Resonance (MR) image segmentation is a challenging task due to the complex anatomical structure of brain tissues in addition to the existence of intensity inhomogeneity, partial volume effects and noise. In this study, a spatial modified bias corrected FCM algorithm is applied to brain MRI for the purpose of segmentation into White Matter (WM), Gray Matter (GM) and Cerebrospinal fluid (CSF) in MR images. So to overcome the uncertainty caused by the above effects, a modified Fuzzy C-Means (m-FCM) algorithm for MR brain image segmentation is presented in this paper. Also FCM suffers from initialization sensitivity, to overcome this we have used chaos theory based firefly algorithm. This paper presents a novel application of FCM clustering by using Firefly algorithm with a chaotic map to initialize the population of fireflies and tune the absorption coefficient (Î»), for increasing the global search mobility. This algorithm is called chaotic firefly integrated Fuzzy C-means (C-FAFCM) algorithm, which embeds chaos map in the Firefly Algorithm. The proposed technique is applied to several simulated and real T1-weighted for normal magnetic resonance brain images, taken from IBSR and BrainWeb database. The algorithm is realized by incorporating the spatial neighborhood information into the standard FCM algorithm and modifying the membership weighting of each cluster by regularizing it by Total Variation (TV) denoising. The experimental results on both simulated and real brain MRI datasets demonstrate that our proposed method (C-FAFCM) has satisfactory outputs in comparison with some other state of the art, based on FCM and non FCM based algorithms.","FCM, FAFCM, En-FAOFCM, Bias field, Spatial information, Total variation, PVE, Tanimoto coefficient and dice similarity",Partha Ghosh and Kalyani Mali and Sitansu Kumar Das,https://www.sciencedirect.com/science/article/pii/S104732031830083X,https://doi.org/10.1016/j.jvcir.2018.04.007,1047-3203,2018,63--79,54,Journal of Visual Communication and Image Representation,Chaotic firefly algorithm-based fuzzy C-means algorithm for segmentation of brain tissues in magnetic resonance images,article,GHOSH201863
"In this paper, we present a novel method to restore the visual quality of images from scenes immersed in participating media, in particular water. Our method builds upon existing physics-based model and estimates the scene radiance by removing the medium interference on light propagation. Our approach requires a single image as input and, by combining a physics-based model for light propagation and a set of quality metrics, reduces the artifacts and degradation imposed by the attenuation, forward scattering, and backscattering effects. We show that the resulting images produced by our technique from underwater images are amenable to be directly used as input to algorithms which do not assume disturbances from the media. Our experiments demonstrate that, as far as visual image quality is concerned, our methodology outperforms both traditional image based restoration approaches and the state-of-the-art methods. Our approach brings advantages regarding descriptor distinctiveness which enables the use of underwater images in legacy non-participating media algorithms such as keypoint detection and description.","Image restoration, Underwater vision, Feature-preserving, Visibility, Inverse problem",Wagner Barros and Erickson R. Nascimento and Walysson V. Barbosa and Mario F.M. Campos,https://www.sciencedirect.com/science/article/pii/S1047320318301421,https://doi.org/10.1016/j.jvcir.2018.06.018,1047-3203,2018,363--373,55,Journal of Visual Communication and Image Representation,Single-shot underwater image restoration: A visual quality-aware method based on light propagation model,article,BARROS2018363
"The emergence of low-cost high-quality personal wearable cameras combined with the increasing storage capacity of video-sharing websites have evoked a growing interest in first-person videos, since most videos are composed of long-running unedited streams which are usually tedious and unpleasant to watch. State-of-the-art semantic fast-forward methods currently face the challenge of providing an adequate balance between smoothness in visual flow and the emphasis on the relevant parts. In this work, we present the Multi-Importance Fast-Forward (MIFF), a fully automatic methodology to fast-forward egocentric videos facing these challenges. The dilemma of defining what is the semantic information of a video is addressed by a learning process based on the preferences of the user. Results show that the proposed method keeps over 3 times more semantic content than the state-of-the-art fast-forward. Finally, we discuss the need of a particular video stabilization technique for fast-forward egocentric videos1https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/jvci2018/.1.","Semantic information, First-person video, Fast-forward, Egocentric stabilization",Michel M. Silva and Washington L.S. Ramos and Felipe C. Chamone and JoÃ£o P.K. Ferreira and Mario F.M. Campos and Erickson R. Nascimento,https://www.sciencedirect.com/science/article/pii/S1047320318300452,https://doi.org/10.1016/j.jvcir.2018.02.013,1047-3203,2018,55--64,53,Journal of Visual Communication and Image Representation,Making a long story short: A multi-importance fast-forwarding egocentric videos with the emphasis on relevant objects,article,SILVA201855
"A novel approach exploiting facial landmarks and depth warping is proposed for robust cross-pose face recognition. Unlike the existing 3-D reconstruction based cross-pose recognition algorithms, the proposed algorithm utilizes the automatically identified extensive facial landmarks to replace the computationally expensive 3-D reconstruction procedure, by depth warping. The given face is thereby registered to the most similar 3-D reference model. When matching to a probe face image, the registered depth-warped faces in the gallery are rotated to align to the orientation of the probe image, and sparse regression is then used to identify the correct person. Further, to handle the more challenging cases with eyeglasses, we devise and employ an enhanced Regressive Tree Structured Model (RTSM) combined with inpainting procedure, prior to depth warping. The proposed robust cross-pose recognition (RCPR) algorithm is rigorously validated on PIE and Multi-PIE databases, and compared with state-of-the-art contemporary approaches to demonstrate its superior efficacy.","Face recognition, Cross-pose, Facial landmarks, Depth warping, Sparse reconstruction, RTSM, PIE and Multi-PIE database",Gee-Sern (Jison) Hsu and ArulMurugan Ambikapathi and Sheng-Luen Chung and Hung-Cheng Shie,https://www.sciencedirect.com/science/article/pii/S1047320318300695,https://doi.org/10.1016/j.jvcir.2018.03.013,1047-3203,2018,273--280,53,Journal of Visual Communication and Image Representation,Robust cross-pose face recognition using landmark oriented depth warping,article,HSU2018273
"Despite the practical importance of photographic composition for improving or assessing the aesthetical quality of photographs, only a few simple composition rules have been considered for its classification. In this work, we propose novel techniques to classify photographic composition rules of outdoor scenes and detect dominant geometric elements, called composition elements, for each composition class. Specifically, we first categorize composition rules of outdoor photographs into nine classes: RoT, center, horizontal, symmetric, diagonal, curved, vertical, triangle, and pattern. Then, we develop a photographic composition classification algorithm using a convolutional neural network (CNN). To train the CNN, we construct a photographic composition database, which is publicly available. Finally, for each composition class, we propose an effective scheme to locate composition elements, i.e., bounding boxes for main subjects, leading lines, axes of symmetry, triangles, and sky regions. Extensive experimental results demonstrate that the proposed algorithm classifies composition classes reliably and detects composition elements accurately.","Image classification, Photographic composition, Composition element detection, Geometric element detection, Sky detection, Rule of thirds",Jun-Tae Lee and Han-Ul Kim and Chul Lee and Chang-Su Kim,https://www.sciencedirect.com/science/article/pii/S1047320318301147,https://doi.org/10.1016/j.jvcir.2018.05.018,1047-3203,2018,91--105,55,Journal of Visual Communication and Image Representation,Photographic composition classification and dominant geometric element detection for outdoor scenes,article,LEE201891
"Optical flow techniques have been applied to motion object detection under a moving camera over the years. In this paper, we propose an effective motion object detection method based on optical flow estimation, characterized in that the complete boundary of the motion object can be extracted from the combination of the optimized horizontal flow with the optimized vertical flow. The optimized horizontal flow and the optimized vertical flow can be obtained as follows: introducing a third frame into the two frames to obtain the horizontal flow and the vertical flow, and then optimizing the horizontal flow and the vertical flow by applying gradient function and threshold method, respectively. The complete boundary of the motion object, after being subjected to region filling, could be optimized by Gaussian filtering technique to obtain the final detection results. Our proposed method is tested on the following datasets consisting of Cdnet2014, KITTI2015, MPI Sintel datasets, natural YouTube sequences and the collected data, respectively. The results show that our proposed method outperforms state-of-the-art significantly.","Optical flow estimation, The moving camera, The horizontal flow, The vertical flow, Motion object detection, Motion object boundary",Yugui Zhang and Jin Zheng and Chi Zhang and Bo Li,https://www.sciencedirect.com/science/article/pii/S1047320318301263,https://doi.org/10.1016/j.jvcir.2018.06.006,1047-3203,2018,215--228,55,Journal of Visual Communication and Image Representation,An effective motion object detection method using optical flow estimation under a moving camera,article,ZHANG2018215
"This research proposes a method for 3D face recognition in various conditions using 3D constrained local model (CLM-Z). In this method, a combination of 2D images (RGBs) and depth images (Ds) captured by Kinect has been used. After detecting the face and smoothing the depth image, CLM-Z model has been used to model and detect the important points of the face. These points are described using Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), and 3D Local Binary Patterns (3DLBP). Finally, each face is recognized by a Support Vector Machine (SVM). The challenging situations are changes of lighting, facial expression and head pose. The results on CurtinFaces and IIIT-D datasets demonstrate that the proposed method outperformed state-of-the-art methods under illumination, expression and pitch pose conditions and comparable results were obtained in other cases. Additionally, our proposed method is robust even when the training data has not been carefully collected.","3D face recognition, Depth image, Face model, 3D constrained local model, Head pose, Lighting, Facial expression, Kinect, Feature descriptor",Nastaran {Nourbakhsh Kaashki} and Reza Safabakhsh,https://www.sciencedirect.com/science/article/pii/S1047320318300294,https://doi.org/10.1016/j.jvcir.2018.02.003,1047-3203,2018,66--85,52,Journal of Visual Communication and Image Representation,RGB-D face recognition under various conditions via 3D constrained local model,article,NOURBAKHSHKAASHKI201866
"A window-adaptive video filter for removal of impulse noise from grayscale videos is proposed. The new method is based on local orientation estimation. The dominant orientation of the pattern in a local spatial neighborhood is computed by minimizing an expression of directional derivatives, and at the same time the orientation strength is also computed. Based on the local spatial orientation and its strength, the size, shape, and orientation of 3D filter window are adaptively determined, which leads to the proposed window-adaptive 3D median filter. To further enhance denoising performance, a new noise detection mechanism is developed and integrated to the proposed video filter. By using this noise detector, video pixels are classified into noise-free and noisy ones. For the noisy pixels detected, the proposed window-adaptive 3D filter is performed. Experimental results show that the proposed method outperforms other state-of-the-art video denoising methods in both objective measure and visual evaluation.","Video denoising, Window-adaptive filter, Orientation estimation, Impulse noise",Lianghai Jin and Hong Liu and Wenhua Zhang and Enmin Song,https://www.sciencedirect.com/science/article/pii/S1047320318301020,https://doi.org/10.1016/j.jvcir.2018.05.007,1047-3203,2018,1--11,55,Journal of Visual Communication and Image Representation,Video oriented filter for impulse noise reduction,article,JIN20181
"Anti-fraud system is very useful in many intelligent applications. With the development of the financial field, anti-fraud system is becoming more and more important. But conventional face recognition techniques cannot distinguish real faces and masks effectively from the video stream. In addition, computing the whole video stream is redundant and time-consuming. So computing some key frames selected from video stream is more effective, but many frames gotten randomly from video stream donât contain any faces. So in this paper, we propose a new method only with a camera to estimate a personâs behavior to detect whether a person is fraudulent. A camera-based anti-fraud system requires a series of representative video frames (i.e. key frames) from the video stream. First, a set of key frames are extracted from the video stream using our active key frame selection algorithm. The criterion is that the contents of the video stream are maximally covered by these frames. Then, face features are obtained using Dlib. Finally, a probabilistic model is proposed to estimate a personâs behavior. Experimental results have demonstrated that: 1) our key frame selection algorithm can reduce redundant frames effectively; 2) our system can estimate a personâs behavior in real time.","Anti-fraud, Key frames, Face recognition",Lizong Zhang and Zepeng Wang,https://www.sciencedirect.com/science/article/pii/S104732031830141X,https://doi.org/10.1016/j.jvcir.2018.06.016,1047-3203,2018,263--269,55,Journal of Visual Communication and Image Representation,A multi-view camera-based anti-fraud system and its applications,article,ZHANG2018263
"In this paper, we propose a novel visual saliency detection algorithm. The saliency of image region is defined as its global and local information. Firstly, we construct background-based map based on a novel multi-feature similarity metric by adjusting the weight of different features varied with image content, then integrated with center prior and Objectness measure into global saliency map. Secondly, a robust locality-based coding method is used to extract image local saliency cues by introducing effective codebooks selection rule and codebook elementâs reliability into reconstruction. Finally, we propose a novel integration mechanism to incorporate global and local saliency map for performance improvement. In terms of experimental results analysis on four benchmark datasets, the superiority of proposed algorithm is adequately demonstrated.","Feature similarity metric, Global and local information, Locality-based coding method, Integration mechanism",Ming Zhang and Yunhe Wu and Yue Du and Lei Fang and Yu Pang,https://www.sciencedirect.com/science/article/pii/S1047320318300737,https://doi.org/10.1016/j.jvcir.2018.03.019,1047-3203,2018,215--223,53,Journal of Visual Communication and Image Representation,Saliency detection integrating global and local information,article,ZHANG2018215
"Color is a rich source of visual information for the effective characterization of image content. The recognition of texture or shape elements in images is strongly associated with the analysis of the image color layout. This paper presents a contextual color descriptor designed especially to be applied to CBIR tasks in heterogeneous image databases. The proposed color uniformity descriptor (CUD) clusters perceptually similar image color regions according to the uniformity analysis of their neighbor pixels. CUD produces vast color image details with a thin histogram, whilst preserving the balance between uniqueness and robustness. CUD is computationally efficient and can achieve high precision and throughput rates when used in CBIR. Experimental results show that CUD performs comparably against local features and multiple features state-of-the-art approaches that require more complex data manipulation. Results demonstrate that CUD provides strong image discrimination even in the presence of significant content variation.","Image retrieval, Image representation, Contextual features, Color uniformity descriptor, Lab color space",Carolina Reta and Jose A. Cantoral-Ceballos and Ismael Solis-Moreno and Jesus A. Gonzalez and Rogelio Alvarez-Vargas and Nery Delgadillo-Checa,https://www.sciencedirect.com/science/article/pii/S1047320318300853,https://doi.org/10.1016/j.jvcir.2018.04.009,1047-3203,2018,39--50,54,Journal of Visual Communication and Image Representation,Color uniformity descriptor: An efficient contextual color representation for image indexing and retrieval,article,RETA201839
"In this paper a steganographic method is proposed to improve the capacity of the hidden secret data and to provide an imperceptible stego-image quality. The proposed steganography algorithm is based on the wavelet packet decomposition (WPD) and neutrosophic set. First, an original image is decomposed into wavelet packet coefficients. Second, the generalized parentâchild relationships of spatial orientation trees for wavelet packet decomposition are established among the wavelet packet subbands. An edge detector based on the neutrosophic set named (NSED) is then introduced and applied on a number of subbands. This leads to classify each wavelet packet tree into edge/non-edge tree to embed more secret bits into the coefficients in the edge tree than those in the non-edge tree. The embedding is done based on the least significant bit substitution scheme. Experimental results demonstrate that the proposed method achieves higher embedding capacity with better imperceptibility compared to the published steganographic methods.","Image steganography, Wavelet packet transformation, Neutrosophic set, Edge detection",Randa Atta and Mohammad Ghanbari,https://www.sciencedirect.com/science/article/pii/S1047320318300579,https://doi.org/10.1016/j.jvcir.2018.03.009,1047-3203,2018,42--54,53,Journal of Visual Communication and Image Representation,A high payload steganography mechanism based on wavelet packet transformation and neutrosophic set,article,ATTA201842
"Medical image segmentation remains a challenged problem because of intensity inhomogeneity and surrounding complex background. In this paper, we propose a novel method for medical image segmentation by integrating support vector machine and graph cuts. Particularly, a novel localized training scheme is proposed to train a classifier for each pixel based on the target image information, and then a novel graph cuts-based segmentation method that combines the constraint information of machine learning result, the edge information, the local information, and the remote-local information is proposed for post-processing. Instead of delineating an initialized curve around the object boundary, we directly draw a narrowband mask for the initialization in the paper. Experiments on synthetic and medical images demonstrate that the proposed method can achieve better performance than the state-of-the-art.","Support vector machine, Graph cuts, Medical image segmentation",Qiang Zheng and Honglun Li and Baode Fan and Shuanhu Wu and Jindong Xu,https://www.sciencedirect.com/science/article/pii/S1047320318301305,https://doi.org/10.1016/j.jvcir.2018.06.005,1047-3203,2018,157--165,55,Journal of Visual Communication and Image Representation,Integrating support vector machine and graph cuts for medical image segmentation,article,ZHENG2018157
"This paper presents a sophisticated patch-based visual tracking algorithm using an omnidirectional camera with distortion adaptation. The omnidirectional camera is modeled using the equivalent projection theory, so that a nonlinear deformed neighbourhood can be accurately estimated in the image plane, which significantly facilitates feature coding. In order to improve the omnidirectional tracking performance, a patch-based multi-feature matching method is proposed under a probability framework. In particular, the distributions of patches covering key parts of the target are weighted adaptively according to their joint-feature response, which is able to track target robustly and filter out the outliers effectively. Extensive experiments have been conducted to verify the performance of the proposed omnidirectional tracking algorithm, which obtains promising results on challenging datasets and outperforms many state-of-the-art methods.","Visual tracking, Omnidirectional camera, Multi-feature integration",Yazhe Tang and Zhi Gao and Feng Lin and Y.F. Li and Fei Wen,https://www.sciencedirect.com/science/article/pii/S1047320318301391,https://doi.org/10.1016/j.jvcir.2018.06.015,1047-3203,2018,253--262,55,Journal of Visual Communication and Image Representation,Visual adaptive tracking for monocular omnidirectional camera,article,TANG2018253
"Sparse representation theory shows effectiveness in single image super-resolution (SR). Existing image super-resolution methods usually make use of l1-regularization, l2-regularization or their combination to restrict the sparsity. However, the nonlocal similarity of images, which can be helpful to image SR, is often neglected. In order to utilize the nonlocal similarity and improve SR results in this paper, we propose a new single image super-resolution method by combining the adaptive sparse representation and robust principal component analysis (RPCA). Furthermore, we adopt the self-similarity learning framework to construct the dictionary pair. In our method, we first compute the sparse coefficient of each testing image patch through adaptive sparse representation with the constructed dictionary. Then, for each testing image block, we search for its similar patches and use RPCA as a low-rank optimization strategy to the corresponding coefficients. Extensive experiment results demonstrate that the proposed method can possesses better performance compared with some state-of-the-art methods.","Super-resolution, Adaptive sparse representation, Self-similarity learning, Robust principal component analysis",Xuesong Li and Guo Cao and Youqiang Zhang and Bisheng Wang,https://www.sciencedirect.com/science/article/pii/S104732031830138X,https://doi.org/10.1016/j.jvcir.2018.06.012,1047-3203,2018,319--330,55,Journal of Visual Communication and Image Representation,Single image super-resolution via adaptive sparse representation and low-rank constraint,article,LI2018319
"State-of-the-art techniques for Camera Model Identification operate by extracting different features from the training image set and incorporating those features to predict the source of test images using machine learning. Though the existing approaches perform efficiently for images captured in natural daylight or bright illumination conditions, the state-of-the-art lacks sufficient experiments and results to evaluate efficiency of such schemes for images captured in dark illumination conditions. In this paper, we present a set of experiments to assess the impact of illumination conditions, on image source classification problem, and also propose an image filtering based technique to eliminate the adverse effects of scene illumination on source classification accuracy. Our experimental results prove that the performance efficiency of existing feature based source classification techniques, is indeed dependent on the illumination conditions. The proposed strategy enables our source classification model to achieve high efficiency as compared to the state-of-the-art, under all illumination conditions.","Camera model identification, Classification, Digital forensics, Image features, Illumination dependency, Overfitting, Scene content",Udaya Sameer Venkata and Ruchira Naskar,https://www.sciencedirect.com/science/article/pii/S1047320318300221,https://doi.org/10.1016/j.jvcir.2018.01.015,1047-3203,2018,24--32,52,Journal of Visual Communication and Image Representation,Eliminating the effects of illumination condition in feature based camera model identification,article,VENKATA201824
"Bit starvation resulted from inefficient rate control of 3D video coding deteriorates visual quality of synthesized views. Most region based rate control schemes allocate higher bitrate to regions of interest (ROIs), but bits may be still consumed by early coded units. This paper avoids reducing coding bits in high-cost regions of 3D-HEVC. Instead of ROIs that are determined by humans, high-cost regions sensitive to bit starvation are detected. Region-level bit allocation is achieved by curve fitting of coding statistics, and recursive Taylor expansion (RTE) based bit allocation is adopted to optimally estimates the target bitrate and the QP related Î» of LCUs in each region. Based on retained earnings in economics, bits are reserved for LCUs that are sensitive to distortions. Experimental results show that the proposed scheme outperforms both the R-Î» model and RTE model in bitrate accuracy with similar R-D performance.","3D-HEVC, Rate control, Bit allocation, Bit starvation, Retained bit earnings, R-Î» model",Che-Chien Wang and Chih-Wei Tang,https://www.sciencedirect.com/science/article/pii/S1047320318300981,https://doi.org/10.1016/j.jvcir.2018.05.001,1047-3203,2018,108--122,54,Journal of Visual Communication and Image Representation,Region-based rate control for 3D-HEVC based texture video coding,article,WANG2018108
"In this paper, a chaotic particle filter method is introduced to improve the performance of particle filter based on chaos theory. The methodology of the algorithm includes two steps. First, the global motion estimation is used to predict target position using dynamical information of object movement over frames. Then, the color-based particle filter method is employed in the local region obtained from global motion estimation to localize the target. The algorithm significantly reduces the number of particles, search space, and the filter divergence because of high-order estimation. To verify the efficiency of the tracker, the proposed method is applied to two datasets, consisting of particle filter-based methods under the Bonn Benchmark on Tracking (BoBoT), the large Tracking Benchmark (TB), and Visual Object Tracking (VOT2014). The results demonstrate that the chaotic particle filter method outperforms other state-of-the-art methods on the abrupt motion, occlusion, and out of view. The precision of the proposed method is about 10% higher than that of other particle filter algorithms with low computational cost.","Object tracking, Chaos theory, Particle filter, Global motion estimation, Occlusion, Fast motion",Marjan Firouznia and Karim Faez and Hamidreza Amindavar and Javad Alikhani Koupaei,https://www.sciencedirect.com/science/article/pii/S1047320318300464,https://doi.org/10.1016/j.jvcir.2018.02.014,1047-3203,2018,1--12,53,Journal of Visual Communication and Image Representation,Chaotic particle filter for visual object tracking,article,FIROUZNIA20181
"Image forensics technology based on image manipulation detection has aroused great interest of researchers in recent decades. By revealing the traces of image manipulations, it could contribute to the location of tampered areas. This paper addresses the detection of image gamma transformation and its application in image splicing detection. A 5-dimensional feature vector is constructed based on the effects of gamma transformation on the histogram, and then Support Vector Machine (SVM) is trained to detect gamma transformation. For splicing tampered area locating, the investigated image is divided into overlapping blocks based on the sliding window, and then each pixel gets a probability of being gamma transformed according to the detection results in image blocks, based on which the tampered area is located. The experimental results validate the effectiveness of the proposed method when no post-operations are applied. Meanwhile, the proposed method is robust to the attack of rank filtering.","Blind forensics, Gamma transformation, Manipulation detection, Splicing, Tampered area location",Ping Wang and Fenlin Liu and Chunfang Yang and Xiangyang Luo,https://www.sciencedirect.com/science/article/pii/S1047320318301159,https://doi.org/10.1016/j.jvcir.2018.05.020,1047-3203,2018,80--90,55,Journal of Visual Communication and Image Representation,Blind forensics of image gamma transformation and its application in splicing detection,article,WANG201880
"Recently, correlation filter-based trackers have achieved the competitive performance both on accuracy and robustness. To learn a classifier effectively, these methods exploit a periodic assumption of the training samples to model the processing of dense sampling in Fourier domain. However, the periodic assumption introduces unwanted boundary effects, which severely degrades the performance of tracking model. To lower the boundary effects, we propose a multi-scale â1 regularized correlation filter tracker (MSL1CFT), which leverages the different regularization parameters to penalize each correlation filter coefficient in the learning process. Our method can learn the correlation filter model on a significantly larger set of negative training samples, without worsening the positive samples. We further present a fast solver to our model utilizing the Alternating Direction Method of Multipliers (ADMM) technique. The extensive empirical evaluations on two benchmark datasets: OTB2013 and VOT2015 demonstrate that our method outperforms the state-of-the-art approaches in tracking accuracy and robustness.","Object tracking, Correlation filter, Adaptive  regularization, Occlusion handling",Zhangjian Ji and Weiqiang Wang,https://www.sciencedirect.com/science/article/pii/S1047320318301433,https://doi.org/10.1016/j.jvcir.2018.06.017,1047-3203,2018,354--362,55,Journal of Visual Communication and Image Representation,Correlation filter tracker based on sparse regularization,article,JI2018354
"Automatic image captioning has been studied extensively over the last few years, driven by breakthroughs in deep learning-based image-to-text translation models. However, most of this work has considered captioning web images from standard data sets like MS-COCO, and has considered single images in isolation. To what extent can automatic captioning models learn finer-grained contextual information specific to a given personâs day-to-day visual experiences? In this paper, we consider captioning image sequences collected from wearable, life-logging cameras. Automatically-generated captions could help people find and recall photos among their large-scale life-logging photo collections, or even to produce textual âdiariesâ that summarize their day. But unlike web images, photos from wearable cameras are often blurry and poorly composed, without an obvious single subject. Their content also tends to be highly dependent on the context and characteristics of the particular camera wearer. To address these challenges, we introduce a technique to jointly caption sequences of photos, which allows captions to take advantage of temporal constraints and evidence across time, and we introduce a technique to increase the diversity of generated captions, so that they can describe a photo from multiple perspectives (e.g., first-person versus third-person). To test these techniques, we collect a dataset of about 8000 realistic lifelogging images, a subset of which are annotated with nearly 5000 human-generated reference sentences. We evaluate the quality of image captions both quantitatively and qualitatively using Amazon Mechanical Turk, finding that while these algorithms are not perfect, they could be an important step towards helping to organize and summarize lifelogging photos.","Lifelogging, First-person, Image captioning, Diary, Privacy",Chenyou Fan and Zehua Zhang and David J. Crandall,https://www.sciencedirect.com/science/article/pii/S1047320318301032,https://doi.org/10.1016/j.jvcir.2018.05.008,1047-3203,2018,40--55,55,Journal of Visual Communication and Image Representation,Deepdiary: Lifelogging image captioning and summarization,article,FAN201840
"Due to the typical challenges including image noise or blurriness, intensity inhomogeneity or various image modalities, image segmentation is still an open problem. In this paper, a new variational model is proposed for multiphase segmentation of gray and color images corrupted by noise or blur. Based on the aspects of image restoration, the coupled fidelity terms are utilized in order to effectively and robustly tackle images with a high level of noise or blurriness. For intensity inhomogeneous images, we use the bias-corrected fuzzy c-means method to eliminate the effect of bias field before our implementation. A partial result for the energy minimization problem is established. For solving the new variational model, the alternating minimization algorithm is studied. Experiments demonstrate that our method gives excellent results in terms of segmentation quality in comparison with other state-of-the-art segmentation methods.","Image segmentation, Image restoration, Mumford-Shah model",Qianting Ma and Dexing Kong,https://www.sciencedirect.com/science/article/pii/S1047320318300580,https://doi.org/10.1016/j.jvcir.2018.03.010,1047-3203,2018,224--234,53,Journal of Visual Communication and Image Representation,A new variational model for joint restoration and segmentation based on the Mumford-Shah model,article,MA2018224
"With the availability of whole-slide imaging in pathology, high-resolution images offer a more convenient disease observation but also require content-based retrieval of large scans. The bag-of-visual-words methodology has shown a high ability to describe the image content for recognition and retrieval purposes. In this work, a variant of the bag-of-visual-words with multiple dictionaries for histopathology image classification is proposed and tested on the image dataset Kimia Path24 with more than 27,000 patches of size 1000â¯Ãâ¯1000 belonging to 24 different classes. Features are extracted from patches and clustered to form multiple codebooks. The histogram intersection approach and support vector machines are exploited to build multiple classifiers. At last, the majority voting determines the final classification for each patch. The experiments demonstrate the superiority of the proposed method for histopathology images that surpasses deep networks, LBP and other BoW results.","Image retrieval, Image representation, Histopathology, Wholeslide imaging, Bag-of-words, Dictionary learning, LBP, SVM, Deep learning",Shujin Zhu and Yuehua Li and Shivam Kalra and H.R. Tizhoosh,https://www.sciencedirect.com/science/article/pii/S1047320318301275,https://doi.org/10.1016/j.jvcir.2018.06.001,1047-3203,2018,243--252,55,Journal of Visual Communication and Image Representation,Multiple disjoint dictionaries for representation of histopathology images,article,ZHU2018243
"Many methods of image acquisition from medical multidimensional data rely on continuous techniques whereas in fact they are used in a finite discrete field. The discretization step is often accompanied by residuals diminishing the quality of the produced images. In addition, the acquisition phase does not occur in an ideal way and may cause artifacts and nonstandard noise. Therefore, denoising is mandatory for many algorithms in computer vision and image processing. In this paper, we propose a new denoising strategy for the tomographic image reconstruction. The method is based on a coupling of the wavelet techniques with the well-known Non Local Means (NLM) filter and operates adaptively during the data acquisition stage. Unlike other well-known denoising techniques, which are mainly based on the smoothing of the resultant image, this approach is instead based on the sinogram preprocessing. The numerical simulations show that the tomographic reconstruction based on the new denoising strategy is able to reduce enough noises present in various forms in the data. Additional robustness tests prove that the proposed approach is more stable than the basic NLM and other homologous methods.","Denoising, Wavelets, Non-local means, Radon transform, Tomography, Medical imaging, Simulation",Hana Rabbouch and Foued SaÃ¢daoui,https://www.sciencedirect.com/science/article/pii/S104732031830097X,https://doi.org/10.1016/j.jvcir.2018.05.004,1047-3203,2018,115--130,55,Journal of Visual Communication and Image Representation,A wavelet-assisted subband denoising for tomographic image reconstruction,article,RABBOUCH2018115
"Most existing saliency detection algorithms concentrate on obtaining good results for images with single salient object, while it produces poor generalization power when tested on more realistic images. In this paper, we present a novel framework to detect saliency in object-level through fusing objectness estimation into the process of salient object detection. Different from most existing methods that evaluate saliency via aggregation of adjacent pixels or regions, our approach peels background regions step by step via evaluating each regionâs saliency, objectness and background, until all the independent foreground objects are left. Instead of extracting from saliency map, the proposed method can obtain salient objects directly, and different salient scores can be assigned to different salient objects. Experimental results show that the proposed method is effective and achieves state-of-the-art performance in several benchmark datasets, especially on PASCAL_S and SED2 that offer salient objects in more complicated scenes.","Object-level, Salient object detection, Saliency, Objectness, Background prior, Peeling",Jianhua Zhang and Yanzhu Zhao and Shengyong Chen,https://www.sciencedirect.com/science/article/pii/S1047320318300506,https://doi.org/10.1016/j.jvcir.2018.03.002,1047-3203,2018,102--112,53,Journal of Visual Communication and Image Representation,Object-level saliency: Fusing objectness estimation and saliency detection into a uniform framework,article,ZHANG2018102
"Salient object detection is a fundamental problem in both pattern recognition and image processing tasks. Previous salient object detection algorithms usually involve various features based on priors/assumptions about the properties of the objects. Inspired by the effectiveness of recently developed deep feature learning, we propose a novel Salient Object Detection via a Local and Global method based on Deep Residual Network model (SOD-LGDRN) for saliency computation. In particular, we train a deep residual network (ResNet-G) to measure the prominence of the salient object globally and extract multiple level local features via another deep residual network (ResNet-L) to capture the local property of the salient object. The final saliency map is obtained by combining the local-level and global-level saliency via Bayesian fusion. Quantitative and qualitative experiments on six benchmark datasets demonstrate that our SOD-LGDRN method outperforms eight state-of-the-art methods in the salient object detection.","Salient object detection, Deep residual network, Local and global features",Dandan Zhu and Ye Luo and Lei Dai and Xuan Shao and Qiangqiang Zhou and Laurent Itti and Jianwei Lu,https://www.sciencedirect.com/science/article/pii/S1047320318300749,https://doi.org/10.1016/j.jvcir.2018.03.017,1047-3203,2018,1--9,54,Journal of Visual Communication and Image Representation,Salient object detection via a local and global method based on deep residual network,article,ZHU20181
"In this paper, we propose a novel steganalytic scheme based on local texture pattern (LTP) to detect binary image steganography. We first assess how the expanded LTPs capture embedding distortions exactly. Considering curse of dimensionality when expanding LTPs, we employ Manhattan distance to measure the pixels correlation in a 5Ã5 sized block and select the pixels with closely correlation to remove some LTPs that are not interested. Although the stego image can maintain good visual quality, steganography scheme changes the inter-pixels correlation of binary image. Therefore we utilize totally 8192 LTPs histogram to define a 8192-dimensional steganalytic feature set. Original images and stego images are classified by ensemble classifier. Experimental results show that the proposed steganalytic method can more effectively detect state-of-the-art binary image steganography schemes compared with other steganalytic schemes.","Binary image steganalysis, Local texture pattern, Manhattan distance, Ensemble classifier",Jialiang Chen and Wei Lu and Yanmei Fang and Xianjin Liu and Yuileong Yeung and Yingjie Xue,https://www.sciencedirect.com/science/article/pii/S1047320318301317,https://doi.org/10.1016/j.jvcir.2018.06.004,1047-3203,2018,149--156,55,Journal of Visual Communication and Image Representation,Binary image steganalysis based on local texture pattern,article,CHEN2018149
"We propose a sparse representation based model to restore an image corrupted by blurring and Rician noise. Our model is composed of a nonconvex data-fidelity term and two regularization terms involving a sparse representation prior and a nonconvex total variation. The sparse representation prior, using image patches, provides restored images with well-preserved repeated patterns and small details, whereas the non-convex total variation enables the preservation of edges. Moreover, the regularization terms are mutually complementary in removing artifacts. To realize our nonconvex model, we adopt the penalty method and the alternating minimization method. The K-SVD algorithm is utilized for learning dictionaries. Numerical experiments demonstrate that the proposed model is superior to state-of-the-art models, in terms of visual quality and certain image quality measurements.","Rician denoising, Deblurring, Sparse representation, Dictionary learning, Nonconvex total variation, Penalty method, Alternating minimization method",Myeongmin Kang and Miyoun Jung and Myungjoo Kang,https://www.sciencedirect.com/science/article/pii/S1047320318300865,https://doi.org/10.1016/j.jvcir.2018.04.010,1047-3203,2018,80--99,54,Journal of Visual Communication and Image Representation,Rician denoising and deblurring using sparse representation prior and nonconvex total variation,article,KANG201880
"According to the nonlocal self-similarity property of natural images, group-based simultaneous sparse coding (GSSC) model assumes that nonlocal similar patches have similar sparse representations in a given dictionary and have been widely used in various image inverse problems. Inspired by the success of the GSSC mode in image restoration problems, this paper proposes a weighted group-based simultaneous sparse coding (WGSSC) model based on the spherically contoured exponential scale mixture (SCESM) prior for image restoration. Compared with traditional GSSC models, which often characterize the similar sparse coefficients by a common set of zero supports and lack spatial adaption and principled fashion, the proposed model considers the dependent relation and adaptivity of similar sparse coefficient, so it is more rational than the GSSC model. The similar sparse coefficients and scale variables can be jointly estimated by the alternating minimization algorithm with the SCESM prior. Based on the estimated sparse coefficients, we can reconstruct the clear patch group and obtain the global denoised image by averaging these patch groups. We refer this denoised method as WGSSC-SCESM based denoiser prior, which can be effectively plugged into general image restoration problems by the alternating direction method of multipliers (ADMM) techniques. Extensive experiments on various types of image restoration problems, e.g., image denoising, inpainting, deblurring and single image super-resolution, demonstrate that the proposed method outperforms many state-of-the-art restored methods in term of the objective and subjective metrics.","Image restoration, Nonlocal self-similarity, Spherically contoured exponential scale mixture, Weighted group-based simultaneous sparse coding, ADMM",Xiaolei Lu and Xuebin LÃ¼ and Yongsheng Zuo,https://www.sciencedirect.com/science/article/pii/S1047320318301160,https://doi.org/10.1016/j.jvcir.2018.05.021,1047-3203,2018,374--392,55,Journal of Visual Communication and Image Representation,Spherically contoured exponential scale mixture prior based nonlocal image restoration with ADMM framework,article,LU2018374
"Human detection is a very important research problem due to its relevance to a wide range of applications. This paper proposes a new method for human detection in RGB-D images. Based on the observation that human head often forms a distinguishable blob-like region in depth image and its physical size and height are in well-known ranges, we propose a physical blob detector to efficiently locate candidate human regions. Since color information and 3D physical structure information are both important cues for characterizing human upper body, we propose to incorporate these two sources of information and construct a novel Multi-Channel Color Shape Descriptor (MCSD) to further verify the candidate regions. The experimental results on four publicly available datasets consistently show that the proposed method can reliably detect humans in RGB-D video in real time.","Human detection, RGB-D camera, Physical blob detector, Multi-Channel Color Shape Descriptor",Guyue Zhang and Jun Liu and Ye Liu and Jingwen Zhao and Luchao Tian and Yan Qiu Chen,https://www.sciencedirect.com/science/article/pii/S1047320318300191,https://doi.org/10.1016/j.jvcir.2018.01.013,1047-3203,2018,13--23,52,Journal of Visual Communication and Image Representation,Physical blob detector and Multi-Channel Color Shape Descriptor for human detection,article,ZHANG201813
"Despite the recent advances of image object proposals (IOPs) and video object proposals (VOPs), it still remains a challenge to apply them to online video object/action detection. To address this problem, we propose a novel form of image object proposals, Temporally Enhanced Image Object Proposals (TE-IOPs), for online video object/action detection. The proposed TE-IOPs augment the existing IOPs at every frame by their temporal dynamics in the past few frames. We develop a dynamic programming scheme to efficiently search for such TE-IOPs in an online manner. Compared with existing VOPs that cannot run online, our TE-IOPs can be used for online detection. Compared with IOPs, our TE-IOPs bring rich temporal dynamics with minor computational cost. Experiments on benchmark datasets validate the superior performance of the proposed TE-IOPs over existing IOPs and VOPs, in terms of both the proposal re-ranking and the application of online action detection.","Video, Proposal, Online, Detection, Temporal",Jiong Yang and Junsong Yuan,https://www.sciencedirect.com/science/article/pii/S1047320318300750,https://doi.org/10.1016/j.jvcir.2018.03.018,1047-3203,2018,245--256,53,Journal of Visual Communication and Image Representation,Temporally enhanced image object proposals for online video object and action detections,article,YANG2018245
"Gait recognition is an important issue currently. In this paper, we propose to combine deep features and hand-crafted representations into a globally trainable deep model. Specifically, a set of deep feature vectors are firstly extracted by a pre-trained CNN model from the input sequences. Then, a kernel function with respect to the fully connected vector is trained as the guiding weight of the respective receptive fields of the input sequences. Therefore, the hand-crafted features are extracted based on the guiding weight. Finally, the hand-crafted features and the deep features are combined into a unified deep network to complete classification. The optimized gait descriptor, termed as deep convolutional location weight descriptor (DLWD), is capable of effectively revealing the importance of different body parts to gait recognition accuracy. Experiments on two gait data sets (i.e., CASIA-B, OU-ISIR) show that our method outperforms the other existing methods for gait recognition.","Gait recognition, Deep learning, Convolutional neural network, Weighted receptive field",Huimin Wu and Jian Weng and Xin Chen and Wei Lu,https://www.sciencedirect.com/science/article/pii/S1047320318301445,https://doi.org/10.1016/j.jvcir.2018.06.019,1047-3203,2018,424--432,55,Journal of Visual Communication and Image Representation,Feedback weight convolutional neural network for gait recognition,article,WU2018424
"This paper proposes a multilevel reversible data hiding scheme in encrypted domain by utilizing the controllable redundancy of learning with error public key cryptography. Messages can be embedded into multilevel sub-regions of ciphertext by quantifying the encrypted domain and recoding its redundancy. We recode redundancy based on the characteristics of cipherâs distribution. Extraction and decryption processes are separated by dividing the encrypted domain into multilevel sub-regions and introducing different quantification standards. Original plaintext can be losslessly recovered from the marked ciphertext by using the decryption key; with a specific level data-hiding key, only the message hiding in the corresponding level can be extracted, while plaintext and other messages remain secret. We provide theoretical analysis and experimental results on the feasibility, reversibility, and security of the proposed scheme. The capacity and encryption blow up factor are discussed. The experimental results demonstrate the maximum embedding rate can exceed 0.3000â¯bpb of ciphertext.","Information security, Reversible data hiding, Multilevel embedding, Public key cryptography, Learning with Error",Yan Ke and Min-qing Zhang and Jia Liu and Ting-ting Su and Xiao-yuan Yang,https://www.sciencedirect.com/science/article/pii/S1047320318300993,https://doi.org/10.1016/j.jvcir.2018.05.002,1047-3203,2018,133--144,54,Journal of Visual Communication and Image Representation,A multilevel reversible data hiding scheme in encrypted domain based on LWE,article,KE2018133
"Recent saliency models rely on propagation to compute the saliency map. Previous propagation methods are single directional, where foreground propagation and background propagation are separate (e.g., only foreground propagation, or background propagation after foreground propagation). Different from the previous approaches, we propose a bi-directional propagation model (BIP) for saliency detection. The BIP model propagates from the labeled foreground superpixels and the labeled background superpixels to the unlabeled ones in the same iteration. A difficulty-based rule is adopted to manipulate the prorogation sequence, which considers both the distinctness of the superpixel to its neighboring ones and its connectivity to the labeled sets. The BIP model outperforms fourteen state-of-the-art saliency models on four challenging datasets, and largely enhances the propagation efficiency compared to single directional propagation models.","Saliency detection, Bi-directional, Propagation",Yingyue Xu and Xiaopeng Hong and Xin Liu and Guoying Zhao,https://www.sciencedirect.com/science/article/pii/S1047320318300476,https://doi.org/10.1016/j.jvcir.2018.02.015,1047-3203,2018,113--121,53,Journal of Visual Communication and Image Representation,Saliency detection via bi-directional propagation,article,XU2018113
"We propose a super-resolution image reconstruction method using multi-source low resolution images. The proposed method includes a hierarchical structure that combines a neighborhood expansion process with the surface fitting technique. In the proposed method, a series of nested neighborhoods are created to collect LR pixels, and a purification algorithm is put forward to remove the outliers. Then we fit with a surface in each neighborhood to obtain a value at the location of estimated high resolution grid site. These values are pooled to a MAP frame to reconstruct high resolution pixels. Therefore, a reconstructed pixel is associated with the pixel correlation, pixel intensity and the spatial structure. Moreover, our method is non-iterative and does not suffer from convergence problem. Comparing with the state-of-the-art schemes, the proposed method provides superior effect and computational efficiency. Experimental results demonstrate the superiority of the proposed method in both visual fidelity and numerical measures.","Super-resolution image reconstruction, Neighborhood expansion, Multi-surface fitting, Hierarchical structure, MAP estimation",Xiaofeng Wang and Didong Zhou and Nengliang Zeng and Xina Yu and Shaolin Hu,https://www.sciencedirect.com/science/article/pii/S1047320318300592,https://doi.org/10.1016/j.jvcir.2018.03.011,1047-3203,2018,65--75,53,Journal of Visual Communication and Image Representation,Super-resolution image reconstruction using surface fitting with hierarchical structure,article,WANG201865
"Video abstraction is an interesting topic that aims at briefly representing the entire video stream by producing a short summary either statically or dynamically. In this paper, we present an optimal static video summarization method based on keyframe extraction, termed as MSKVS. The proposed MSKVS has three major components: A new feature representation is exploited to describe the visual content of the video, a simple and fast algorithm is proposed to eliminate most similar and redundant frames, and an adaptive mean shift algorithm is used to select the most representative keyframes. We further develop a novel verification technique to measure the amount of information preserved by the produced summary and to make sure that it deserves to present the entire video stream regardless of human opinion impact. We report experimental results on six challenging datasets using different evaluation metrics, showing that MSKVS achieves state-of-the-art performances in a short computation time.","Keyframe extraction, Video summarization, Mean shift, Features extraction, Summarization quality, Objective video summary evaluation",Rachida Hannane and Abdessamad Elboushaki and Karim Afdel,https://www.sciencedirect.com/science/article/pii/S1047320318301287,https://doi.org/10.1016/j.jvcir.2018.06.002,1047-3203,2018,179--200,55,Journal of Visual Communication and Image Representation,MSKVS: Adaptive mean shift-based keyframe extraction for video summarization and a new objective verification approach,article,HANNANE2018179
"It is know that face detection as a kind of artificial intelligence (AI) technology has become an indispensable tool in our daily life, which produce effects on every aspect of us. The demand for detection and recognition is higher accuracy and higher speed in different areas. So a new video frame-based face detection system is designed to help us making good safety precautions in recognition between normal face and abnormal face. Abnormal face means that face is partially occluded by some objects such as mask, sunglass and so on. Since these abnormal faces are easily recognized as normal faces in previous detection systems, they are often ignored. And it brings us some potential dangers, especially in the area of residential face detection access, bank business login and other security areas. This system provides a complete set of process for detecting faces from video and distinction them, which achieves a good real-time performance in accuracy and speed. We adopt libfacedetection to detect faces from each frame. In addition, we introduce a dlib library which is a deep learning tools to help aligning face and extract the characteristic value. And a GMM clustering algorithm is provided to train and test images for the system. This system can help us to make a distinction between normal face and abnormal face, which is of great significance to the security field in the future.","Video frame-based face detection, Libfacedetection, Deep learning, Gaussian mixture model",Gang Niu and Ququ Chen,https://www.sciencedirect.com/science/article/pii/S1047320318301561,https://doi.org/10.1016/j.jvcir.2018.07.001,1047-3203,2018,457--463,55,Journal of Visual Communication and Image Representation,Learning an video frame-based face detection system for security fields,article,NIU2018457
"Nowadays, face biometric based access control systems are becoming ubiquitous in our daily life while they are still vulnerable to spoofing attacks. So developing robust and reliable methods to prevent such frauds is unavoidable. As deep learning techniques have achieved satisfactory performances in computer vision, they have also been applied to face spoofing detection. However, the numerous parameters in these deep learning based detection methods cannot be updated to optimum due to limited data. Local Binary Pattern (LBP), effective features for face recognition, have been employed in face spoofing detection and obtained promising results. Considering the similarities between LBP extraction and convolutional neural network (CNN) that the former can be accomplished by using fixed convolutional filters, we propose a novel end-to-end learnable LBP network for face spoofing detection. Our network can significantly reduce the number of network parameters by combing learnable convolutional layers with fixed-parameter LBP layers that are comprised of sparse binary filters and derivable simulated gate functions. Compared with existing deep leaning based detection methods, the parameters in our fully connected layers are up to 64x savings. Conducting extensive experiments on two standard spoofing databases, i.e., Relay-Attack and CASIA-FA, our proposed LBP network substantially outperforms the state-of-the-art methods.","Face spoofing detection, Deep learning, Local binary pattern",Lei Li and Xiaoyi Feng and Zhaoqiang Xia and Xiaoyue Jiang and Abdenour Hadid,https://www.sciencedirect.com/science/article/pii/S1047320318301044,https://doi.org/10.1016/j.jvcir.2018.05.009,1047-3203,2018,182--192,54,Journal of Visual Communication and Image Representation,Face spoofing detection with local binary pattern network,article,LI2018182
"Recent developments in light field acquisition and computational photography are driving new research efforts on light field encoding methods, capable of exploiting the specific features of this type of visual data. This paper presents a research study of lossless light field image compression, using Minimum-Rate Predictors (MRP) and mainstream image and video encoders. The research is focused on three light field representation formats: lenslet images, stack of sub-aperture images and epipolar images. The main contributions of this work are the âSpiral-blackendâ serialization method and the use of MRP for the lossless compression of light fields with joint encoding of RGB data. The results show that the lenslet format yields lower compression efficiencies than other formats. Furthermore, it is demonstrated that the MRP algorithm consistently outperforms HEVC-RExt, JPEG2000, JPEG-LS and CALIC when light fields are represented by either a stack of sub-aperture or epipolar images.","Image coding, Lossless compression, Light field coding",JoÃ£o M. Santos and Pedro A.A. Assuncao and Luis A. {da Silva Cruz} and Luis M.N. Tavora and Rui Fonseca-Pinto and Sergio M.M. Faria,https://www.sciencedirect.com/science/article/pii/S1047320318300518,https://doi.org/10.1016/j.jvcir.2018.03.003,1047-3203,2018,21--30,54,Journal of Visual Communication and Image Representation,Lossless coding of light field images based on minimum-rate predictors,article,SANTOS201821
"We describe an effective and efficient strategy building steganography detector for patch synthesis based steganography, one case of which is reversible texture synthesis based steganography method proposed by Wu et al. (2015). By exploiting the observation that steganography destroys optimization of matching extent between the synthetic patch and optimal candidate patch, we reconstruct the two patches from an overlapped region to extract the existence of optimality, which are distinct between cover and stego images, to form features. Support vector machine (SVM) is implemented for classification. Meanwhile, a variant of Wu et al.âs steganographic method is proposed with reinforced security, by padding redundant regions carrying no message around the periphery of the synthesized image and generating additional candidate patches to increase capacity. Experiments demonstrate that the modified algorithm offers not only better resistance against the state-of-the-art steganalysis methods and steganalytic attack we developed, but also a larger embedding capacity.","Texture image, Steganalysis, Texture synthesis, Steganography",Hang Zhou and Kejiang Chen and Weiming Zhang and Zhenxing Qian and Nenghai Yu,https://www.sciencedirect.com/science/article/pii/S1047320318300877,https://doi.org/10.1016/j.jvcir.2018.04.011,1047-3203,2018,100--107,54,Journal of Visual Communication and Image Representation,Targeted attack and security enhancement on texture synthesis based steganography,article,ZHOU2018100
"Relating instrumentally measured to visually perceived colour-differences is one of the challenges of advanced colorimetry. Lately, the use of color difference formulas is becoming more important in the computer vision field as it is a key tool in advancing towards perceptual image processing and understanding. In the last decades, the study of contours of equal color-differences around certain color centers has been of special interest. In particular, the contour of threshold level difference that determines the just noticeable differences (JND) has been deeply studied and, as a result, a set of 19 different ellipsoids of suprathreshold color-difference is available in the literature. In this paper we study whether this set of ellipsoids could be used to compute any color difference in any region of the color space. To do so, we develop a fuzzy multi-ellipsoid model using the ellipsoids information along with two different metrics. We see that the performance of the two metrics vary significantly for very small, small, medium and large color differences. Therefore, we also study how to adapt two metric parameters to optimize performance. The obtained results outperform the currently CIE-recommended color-difference formula CIEDE2000.","Color-difference formulas, Fuzzy logic, Fuzzy Metrics, STRESS",Samuel Morillas and Mark D. Fairchild,https://www.sciencedirect.com/science/article/pii/S1047320318301172,https://doi.org/10.1016/j.jvcir.2018.05.022,1047-3203,2018,142--148,55,Journal of Visual Communication and Image Representation,Using suprathreshold color-difference ellipsoids to estimate any perceptual color-difference,article,MORILLAS2018142
"With the growing popularity of high dynamic range (HDR) imaging, efficient compression techniques are demanded, as HDR video entails typically higher raw data rate than traditional video. For this purpose, we introduce a hybrid spatially and temporally constrained content-adaptive tone mapping operator (TMO) to convert the input HDR video into a tone mapped video sequence, which is then encoded using the high efficiency video coding (HEVC) standard. The proposed TMO simultaneously exploits intra-frame spatial redundancies and preserves inter-frame temporal coherence of the tone mapped video sequence. Extensive experimental results show that the developed spatio-temporal TMO (ST-TMO) solution yields higher coding performance than existing frame-by-frame TMOâs, and compares favorably with state-of-the-art methods based on a fixed transfer function.","High dynamic range, Video compression, Tone mapping operator, Convex optimization",Cagri Ozcinar and Paul Lauga and Giuseppe Valenzise and FrÃ©dÃ©ric Dufaux,https://www.sciencedirect.com/science/article/pii/S1047320318301299,https://doi.org/10.1016/j.jvcir.2018.06.003,1047-3203,2018,166--178,55,Journal of Visual Communication and Image Representation,Spatio-temporal constrained tone mapping operator for HDR video compression,article,OZCINAR2018166
"Multi-scale exposure fusion is an efficient approach to fuse multiple differently exposed images of a high dynamic range (HDR) scene directly for displaying on a conventional low dynamic range (LDR) display device without generating an intermediate HDR image. It can produce images with higher quality than single-scale exposure fusion, but has a risk of producing halo artifacts and cannot preserve details in brightest or darkest regions well in the fused image. In this paper, an edge-preserving smoothing pyramid is introduced for the multi-scale exposure fusion. Benefiting from the edge-preserving property of the filter used in the algorithm, the details in the brightest/darkest regions are preserved well and no halo artifacts are produced in the fused image. The complexity of the proposed edge-preserving smoothing pyramid could be an issue. A hybrid smoothing pyramid is proposed to obtain a good trade-off between the complexity of algorithm and the quality of fused images. The experimental results prove that the proposed algorithms produce better fused images than the state-of-the-art algorithms both qualitatively and quantitatively.","Exposure fusion, High dynamic range, Image pyramid, Gradient domain guided image filter, Edge-preserving smoothing",Fei Kou and Zhengguo Li and Changyun Wen and Weihai Chen,https://www.sciencedirect.com/science/article/pii/S1047320318300762,https://doi.org/10.1016/j.jvcir.2018.03.020,1047-3203,2018,235--244,53,Journal of Visual Communication and Image Representation,Edge-preserving smoothing pyramid based multi-scale exposure fusion,article,KOU2018235
"Satellite imagery is a kind of typical remote sensing data, which holds preponderance in large area imaging and strong macro integrity. However, for most commercial space usages, such as virtual display of urban traffic flow, virtual interaction of environmental resources, one drawback of satellite imagery is its low spatial resolution, failing to provide the clear image details. Moreover, in recent years, synthesizing the color for grayscale satellite imagery or recovering the original color of camouflage sensitive regions becomes an urgent requirement for large spatial objects virtual reality interaction. In this work, unlike existing works which solve these two problems separately, we focus on achieving image super-resolution (SR) and image colorization synchronously. Based on multi-task learning, we provide a novel deep neural network model to fulfill single satellite imagery SR and colorization simultaneously. By feeding back the color feature representations into the SR network and jointly optimizing such two tasks, our deep model successfully achieves the mutual cooperation between imagery reconstruction and image colorization. To avoid color bias, we not only adopt the non-satellite imagery to enrich the color diversity of satellite image, but also recalculate the prior color distribution and the valid color range based on the mixed data. We evaluate the proposed model on satellite images from different data sets, such as RSSCN7 and AID. Both the evaluations and comparisons reveal that the proposed multi-task deep learning approach is superior to the state-of-the-art methods, where image SR and colorization can be accomplished simultaneously and efficiently.","Image super-resolution, Satellite image colorization, Deep neural networks, Multi-task learning",Heng Liu and Zilin Fu and Jungong Han and Ling Shao and Hongshen Liu,https://www.sciencedirect.com/science/article/pii/S1047320318300488,https://doi.org/10.1016/j.jvcir.2018.02.016,1047-3203,2018,20--30,53,Journal of Visual Communication and Image Representation,Single satellite imagery simultaneous super-resolution and colorization using multi-task deep neural networks,article,LIU201820
"In 3D video coding systems, depth maps are not displayed to the viewers, but provide the geometric information to generate virtual views. To ensure the quality of virtual views, the rate-distortion optimization (RDO) in depth map coding adopts the virtual view distortion as the distortion item. The virtual view distortion comes from the reconstructed color video distortion and depth distortion. It is usually recognized that the virtual view distortion caused by reconstructed color video distortion is independent of that in depth map coding. Preliminary experiments reveal that the virtual view distortion in depth map coding is also influenced by the reconstructed color video distortion. Therefore, we proposed a novel distortion criterion of depth map coding in which the reconstructed color video distortion is modeled and joins into the virtual view distortion calculation. Correspondingly, the associated Lagrange multiplier is also proposed. Experimental results demonstrate that the method by integrating the proposed distortion criterion into RDO for depth map coding can achieve an average 12.72% bitrate saving compared with SSD based RDO method and can also lead a bitrate reduction (0.64%) compared with the existing distortion estimation method in the current 3D-HEVC reference software. With the associated Lagrange multiplier, the proposed distortion criterion can achieve 12.98% bitrate saving compared with SSD based RDO method on average.","Three dimensional video (3DV), Multiview video plus depth (MVD), Rate-distortion optimization, Virtual view distortion, 3D-HEVC",Ziqi Zheng and Junyan Huo and Bingbing Li and Hui Yuan and Weisi Lin,https://www.sciencedirect.com/science/article/pii/S1047320318301056,https://doi.org/10.1016/j.jvcir.2018.05.010,1047-3203,2018,145--154,54,Journal of Visual Communication and Image Representation,A novel distortion criterion of rate-distortion optimization for depth map coding,article,ZHENG2018145
"To support good video quality of experiences in heterogeneous environments, transcoding an existed HEVC (high efficiency video coding) video bitstream to a SHVC (scalability extension of HEVC) bitstream with quality scalability is highly required. A straightforward way is to first fully decode the input HEVC bitstream and then fully re-encode it with the SHVC encoder, which requires a tremendous computational complexity. To solve the problem, in this paper, a coding unit complexity (CUC)-based prediction method for predictions of CU (coding unit) depth and PU (prediction unit) mode for efficient HEVC-to-SHVC transcoding with quality scalability is proposed to significantly reduce the transcoding complexity. The proposed method contains two prediction techniques, including (i) early termination and (ii) adaptive confidence interval, and predicts the CU depth and PU mode relying on the decoded information from the input HEVC bitstream. Experimental results have shown that the proposed method significantly outperforms the traditional HEVC-to-SHVC method by 74.14% on average in reductions of encoding time for SHVC enhancement layer.","HEVC (high efficiency video coding), SHVC (scalability extension of HEVC), Video transcoding, Scalable video coding, Early termination, Coding unit complexity",Chia-Hung Yeh and Wen-Yu Tseng and Li-Wei Kang and Cheng-Wei Lee and Kahlil Muchtar and Mei-Juan Chen,https://www.sciencedirect.com/science/article/pii/S1047320318301330,https://doi.org/10.1016/j.jvcir.2018.06.008,1047-3203,2018,342--351,55,Journal of Visual Communication and Image Representation,Coding unit complexity-based predictions of coding unit depth and prediction unit mode for efficient HEVC-to-SHVC transcoding with quality scalability,article,YEH2018342
"Existing multi-view facial expression recognition algorithms are not fully capable of finding discriminative directions if the data exhibits multi-modal characteristics. This research moves toward addressing this issue in the context of multi-view facial expression recognition. For multi-modal data, local preserving projection (LPP) or local Fisher discriminant analysis (LFDA)-based approach is quite appropriate to find a discriminative space. Also, the classification performance can be enhanced by imposing uncorrelated constraint onto the discriminative space. So for multi-view (multi-modal) data, we proposed an uncorrelated multi-view discriminant locality preserving projection (UMvDLPP)-based approach to find an uncorrelated common discriminative space. Additionally, the proposed UMvDLPP is implemented in a hierarchical fashion (H-UMvDLPP) to obtain an optimal performance. Extensive experiments on BU3DFE dataset show that UMvDLPP performs slightly better than the existing methods. However, an improvement of approximately 3% as compared to the existing state-of-the-art multi-view learning-based approaches is achieved by our H-UMvDLPP. This improvement is due to the fact that the proposed method enhances the discrimination between the classes more effectively, and classifies expressions category-wise followed by classification of the basic expressions embedded in each of the subcategories (hierarchical approach).",Facial expression recognition,Sunil Kumar and M.K. Bhuyan and Brian C. Lovell and Yuji Iwahori,https://www.sciencedirect.com/science/article/pii/S1047320318300889,https://doi.org/10.1016/j.jvcir.2018.04.013,1047-3203,2018,171--181,54,Journal of Visual Communication and Image Representation,Hierarchical uncorrelated multiview discriminant locality preserving projection for multiview facial expression recognition,article,KUMAR2018171
"Confocal laser endomicroscopy (CLE) is an advanced optical fluorescence technology undergoing assessment for applications in brain tumor surgery. Many of the CLE images can be distorted and interpreted as nondiagnostic. However, just one neat CLE image might suffice for intraoperative diagnosis of the tumor. While manual examination of thousands of nondiagnostic images during surgery would be impractical, this creates an opportunity for a model to select diagnostic images for the pathologists or surgeons review. In this study, we sought to develop a deep learning model to automatically detect the diagnostic images. We explored the effect of training regimes and ensemble modeling and localized histological features from diagnostic CLE images. The developed model could achieve promising agreement with the ground truth. With the speed and precision of the proposed method, it has potential to be integrated into the operative workflow in the brain tumor surgery.","Neural network, Unsupervised localization, Ensemble modeling, Brain tumor, Confocal laser endomicroscopy, Surgical vision",Mohammadhassan Izadyyazdanabadi and Evgenii Belykh and Michael Mooney and Nikolay Martirosyan and Jennifer Eschbacher and Peter Nakaji and Mark C. Preul and Yezhou Yang,https://www.sciencedirect.com/science/article/pii/S1047320318300804,https://doi.org/10.1016/j.jvcir.2018.04.004,1047-3203,2018,10--20,54,Journal of Visual Communication and Image Representation,"Convolutional neural networks: Ensemble modeling, fine-tuning and unsupervised semantic localization for neurosurgical CLE images",article,IZADYYAZDANABADI201810
"In this paper, we propose guided filtering based data fusion for light field depth estimation with L0 gradient minimization. Stereo disparity produces good depth edge, while defocus response yields good depth information in homogeneous regions. We fuse stereo disparity and defocus response from light filed data in a guided filtering framework. In the guided filtering framework, we adopt L0 gradient minimization as the regularization term instead of penalizing linear coefficients to consider depth characteristics that have similar depth in the same object. Moreover, we utilize edge direction in stereo matching to prevent the confusion caused by occlusion. Experimental results on both synthetic and real light field datasets show that the proposed method achieves clearer edge and less error in depth than state-of-the-arts.","Data fusion, Guided filtering, Light field,  gradient minimization, Defocus response, Occlusion, Stereo matching",Qihui Han and Cheolkon Jung,https://www.sciencedirect.com/science/article/pii/S1047320318301457,https://doi.org/10.1016/j.jvcir.2018.06.020,1047-3203,2018,449--456,55,Journal of Visual Communication and Image Representation,Guided filtering based data fusion for light field depth estimation with L0 gradient minimization,article,HAN2018449
"Depth-image-based-rendering (DIBR) is the most popular view synthesis method. The rendering process in DIBR introduces artifacts to synthesized views. Temporal inconsistency of these artifacts causes flickering, which is one of the dominant distortions in DIBR-synthesized videos. We propose a no-reference quality index for DIBR-synthesized videos. A two-stage flickering region detection method is first proposed by calculating the gradient distance between the matching blocks in adjacent frames followed by a refinement operation. Subsequently, the distortion intensity of flickering regions is measured in singular value decomposition domain. The average flickering intensity is computed as the quality score. Experimental results show that our method outperforms the state-of-the-arts on the IRCCyN/IVC database and ranks the second on the SIAT database, which is only slightly worse than the best-performing one. Furthermore, the presented method is applied to improve the performances of existing metrics and benchmark DIBR algorithms, which achieves very promising results for both tasks.","Video quality evaluation, View synthesis, DIBR, Flickering, Singular value decomposition",Yu Zhou and Leida Li and Shiqi Wang and Jinjian Wu and Yun Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318301184,https://doi.org/10.1016/j.jvcir.2018.05.023,1047-3203,2018,30--39,55,Journal of Visual Communication and Image Representation,No-reference quality assessment of DIBR-synthesized videos by measuring temporal flickering,article,ZHOU201830
"The paper proposes a novel method for detecting indicators of image forgery by locating grid alignment abnormalities in JPEG compressed image bitmaps. The method evaluates multiple grid positions with respect to a fitting function, and areas of lower contribution are identified as grid discontinuities and possibly tampered areas. An image segmentation step is introduced to differentiate between discontinuities produced by tampering and those that are attributed to image content, making the output maps easier to interpret by suppressing non-relevant activations. Our evaluations, on both synthetically produced datasets and real world tampering cases against seven methods from the literature, highlight the effectiveness of the proposed method in its ability to produce output maps that are clear and readable, and which can achieve successful detections on cases where other algorithms fail.","Image forensics, JPEG artifacts, Forgery localization, Splicing",Chryssanthi Iakovidou and Markos Zampoglou and Symeon Papadopoulos and Yiannis Kompatsiaris,https://www.sciencedirect.com/science/article/pii/S1047320318301068,https://doi.org/10.1016/j.jvcir.2018.05.011,1047-3203,2018,155--170,54,Journal of Visual Communication and Image Representation,Content-aware detection of JPEG grid inconsistencies for intuitive image forensics,article,IAKOVIDOU2018155
"In this paper, we propose a digital image watermarking method for camera-captured images. In our proposed method, an image component of all image pixels is used for embedding an individual watermark bit in order to provide large amount of the embedded watermark. The watermark strength is adjusted in accordance with the modified just noticeable distortion. After the watermarked image is printed and then captured by a digital camera, the reliable watermark extraction is accomplished based on the techniques of reducing distortions introduced from the printing and camera-capturing processes, and predicting original image component from the watermarked image component. In the experiments, various types of pixel value distortions and geometric distortions are considered and explored. With the proposed method, the results show that the watermark can be invisibly embedded, and reliably extracted. We also demonstrate its robustness against various types of distortions from the printing and camera-capturing processes.","Digital image watermarking, Just noticeable distortion, Pixel value distortions, Geometric distortions",Kharittha Thongkor and Thumrongrat Amornraksa and Edward J. Delp,https://www.sciencedirect.com/science/article/pii/S1047320318300531,https://doi.org/10.1016/j.jvcir.2018.03.005,1047-3203,2018,146--160,53,Journal of Visual Communication and Image Representation,Digital watermarking for camera-captured images based on just noticeable distortion and Wiener filtering,article,THONGKOR2018146
"This paper by proposing a novel approach, is one the first works that addresses the highly ill-posed problem of nonparametric blind single image super resolution (SISR) of the synthetic aperture radar (SAR) images. Combination of an adaptive compressive sensing (CS) technique and some effective sparse priors, as a powerful regularizer in the both high resolution (HR) image reconstruction and the point spread function (PSF) estimation domains is the fundamental idea of the proposed method. This task is formulated as a new cost function to be minimized with respect to an intermediate reconstructed HR image patch and a nonparametric PSF kernel, according to the alternative minimization (AM) algorithm. To solve the optimization of cost function, a numerical scheme based on the conjugate gradient least squares (CGLS) method is proposed. Experimental results for the both synthetic and realistic low resolution (LR) SAR images demonstrate that the proposed method achieves the state-of-the-art performance.","Blind super resolution, Synthetic aperture radar, Point spread function estimation, Compressive sensing, Anisotropic total variation, Conjugate gradient least squares",Naser Karimi and Mohammad Reza Taban,https://www.sciencedirect.com/science/article/pii/S1047320318300774,https://doi.org/10.1016/j.jvcir.2018.04.001,1047-3203,2018,853--865,55,Journal of Visual Communication and Image Representation,Nonparametric blind SAR image super resolution based on combination of the compressive sensing and sparse priors,article,KARIMI2018853
"Structured light depth sensing with a single-shot pattern is widely employed to capture depth maps for dynamic scenes. For conventional structured light techniques, the projected pattern has to be coded delicately in regards to color, shape, and intensity, in order to assign each pixel with a unique label. However, using such a complicated pattern is a double-edged sword, as although it is effective in labelling pixels, it is also sensitive to environmental noise such as: ambient illumination, textures, uneven albedos, or colors of objects in a scene. In contrast, a coding-free pattern is simply constructed and also insensitive to various environmental noise. Therefore, the coding-free pattern method is capable of robustly sensing the depth for complex scenes. The main challenge in coding-free depth sensing is the âcorrespondence retrievalâ between the projected and captured pattern (i.e. matching pixels between the projected and captured pattern). In this study, we focused on evaluating the correspondence retrieval in a coding-free binary grid pattern. A graph based topological labelling (GBTL) algorithm is proposed to determine the topological coordinates of the intersections of the grid. Then we retrieved the correspondence by using the topology of the grid and the epipolar constraint. We also demonstrated the upper bounds of depth variance by employing the proposed method. The proposed technique alleviates many of the limitations faced with traditional correspondence retrieval. Experimental results showed that the proposed technique performed better (i.e. in terms of precision) than the popular RGB-D cameras Kinect v1 and Kinect v2. Compared with the traditional single-shot techniques, which require complicated patterns, the proposed technique significantly improved the robustness and ease of work, while achieving comparable precision. Additionally, this proposed technique could also be used for both the binary coding-free and the traditional chromatic grid patterns.","Coding-free, Depth sensing, Structured light, Topological constraint, Dynamic programming",Guangming Shi and Ruodai Li and Fu Li and Yi Niu and Lili Yang,https://www.sciencedirect.com/science/article/pii/S1047320318301342,https://doi.org/10.1016/j.jvcir.2018.06.009,1047-3203,2018,229--242,55,Journal of Visual Communication and Image Representation,Depth sensing with coding-free pattern based on topological constraint,article,SHI2018229
"In this paper, we propose a new algorithm to implement the generalized multiple maximum scatter difference (GMMSD). Due to enhanced features of this algorithm over the original GMMSD, we named it GMMSD+. By employing a different projection from both the range of the between-class scatter matrix and the null space of the within-class scatter matrix, GMMSD+ can divide the centroid vector of each class into two components: intrinsic common component (ICC) and discriminant difference component (DCC), and then automatically discards ICC which contains little discriminative information, while keeping DCC which contains the true discriminative power. Next, we introduce a practical implementation of GMMSD+, which can accurately and efficiently update the discriminant vectors with new training samples incrementally, eliminating the complete re-computation of the training process. Our experiments demonstrate that incremental version of GMMSD+(IGMMSD+) eliminates the complete re-computation of the training process when new training samples are presented, leading to significantly reduced computational cost.","Feature extraction, Generalized multiple maximum scatter difference, Incremental GMMSD+",Ning Zheng and Xin Guo and Yun Tie and Nan Dong and Lin Qi and Ling Guan,https://www.sciencedirect.com/science/article/pii/S1047320318300890,https://doi.org/10.1016/j.jvcir.2018.04.012,1047-3203,2018,67--79,55,Journal of Visual Communication and Image Representation,Incremental generalized multiple maximum scatter difference with applications to feature extraction,article,ZHENG201867
"This paper proposes a lossless data hiding method for JPEG images using adaptive embedding. By constructing an optimal mapping between the used and unused Huffman codes in each category, we take full use of the combination of mapping to achieve a high embedding rate. In order to improve the payload, we further use a code reordering based embedding algorithm. Both algorithms are alternatively used during data hiding. After modifying the Huffman Table defined in JPEG header and substituting the codes in entropy-encoded segments, additional messages are embedded into the JPEG bitstream. The proposed method is lossless to the image, i.e., the decoded content of a marked JPEG bitstream is identical to the original JPEG image. Meanwhile, the file size can be well preserved after data hiding. Experimental results show that the proposed method has a better performance than state-of-the-art works.","Information hiding, Lossless data hiding, Reversible data hiding",Yingqiang Qiu and Han He and Zhenxing Qian and Sheng Li and Xinpeng Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318300373,https://doi.org/10.1016/j.jvcir.2018.02.005,1047-3203,2018,86--91,52,Journal of Visual Communication and Image Representation,Lossless data hiding in JPEG bitstream using alternative embedding,article,QIU201886
"In Region based Image Retrieval (RBIR) methods, region matching mainly focuses on region-to-region and image-to-image methods. The former may cause loss of image information and the latter may lead to similar regions being matched repeatedly. To solve these problems, we propose a new image retrieval method based on merged regions, and feature extraction and matching are processed at the category level. Merged regions in an image belong to the same category to some extent, and are obtained by a statistical region merging and affinity propagation (SRM-AP) algorithm. For feature extraction, regional convolution mapping feature (RCMF) based on the convolutional neural networks (CNN) are extracted. RCMF is further combined with the number and distribution of regions to represent the characteristics of merged regions. Moreover, to match the merged regions according to their significance in images, an integrated category matching (ICM) method is designed. Experimental results on Corel-1000 and Caltech-256 show that the proposed method is more effective than some existing RBIR methods.","Image retrieval (IR), Regional convolution mapping feature (RCMF), Convolution neural networks (CNN), Integrated category matching (ICM)",Fanjie Meng and Dalong Shan and Ruixia Shi and Yang Song and Baolong Guo and Weidong Cai,https://www.sciencedirect.com/science/article/pii/S1047320318301627,https://doi.org/10.1016/j.jvcir.2018.07.003,1047-3203,2018,572--585,55,Journal of Visual Communication and Image Representation,Merged region based image retrieval,article,MENG2018572
"Multi-view data with each view corresponding to a type of feature generally provides more comprehensive information. Learning from multi-view data is a challenging research topic in pattern recognition. For recognition task, most multi-view learning methods separately learn multi-view dimensionality reduction (MvDR) and classification models. Thus, the connection between the two models has not been well studied. In this paper, we propose a novel multi-view dimensionality reduction and recognition framework, which can establish the connection between MvDR and classification. Specifically, a multi-view dimensionality reduction method, termed as sparse representation regularized multiset canonical correlation analysis (SR2MCC) is first proposed. SR2MCC considers both correlation and sparse discrimination among multiple views. In accord with SR2MCC, a classifier, called multi-view sparse representation based classifier (MvSRC) is further developed. MvSRC performs classification by comparing the reconstruction residuals of different classes among all views. An efficient iterative algorithm is proposed to solve the proposed model. Extensive experiments on the AR, CMU PIE, FERET, and FRGC datasets demonstrate that the proposed framework can achieve superior recognition performance than several state-of-the-art methods.","Multi-view learning, Canonical correlations, Dimensionality reduction, Face recognition",Xiaobo Shen and Yun-Hao Yuan and Fumin Shen and Yang Xu and Quan-Sen Sun,https://www.sciencedirect.com/science/article/pii/S104732031830052X,https://doi.org/10.1016/j.jvcir.2018.03.004,1047-3203,2018,161--170,53,Journal of Visual Communication and Image Representation,A novel multi-view dimensionality reduction and recognition framework with applications to face recognition,article,SHEN2018161
"As one classic technique of computer vision, image retrieval could retrieve the target images from hundreds of thousands of images effectively. Furthermore, with the rapid development of deep learning, the quality of retrieval is increased obviously. However, under normal conditions, the high-quality retrieval is supported by a large number of learning instances. The large number of learning instances not only need much human source in the process of selection, but also need much computing source in the process of computation. More importantly, for some special categories, itâs difficult to obtain a large number of learning instances. Aiming at the problem above, we proposed one joint entropy based learning model which could reduce the number of learning instances through optimizing the distribution of learning instances. Firstly, the learning instances are pre-selected using improved watershed segmentation method. Then, joint entropy model is used for reducing the possibility of double, useless even mistaken instances existence. After that, a database using a large number of images is built up. Sufficient experiments based on the database show the modelâs superiority that our model not only could reduce the number of learning instances but also could keep the accuracy of retrieval.","joint entropy, learning instance, image retrieval, watershed segmentation, precise-recall curve, AP value, AUC value",Hao Wu and Yueli Li and Xiaohan Bi and Linna Zhang and Rongfang Bie and Yingzhuo Wang,https://www.sciencedirect.com/science/article/pii/S1047320318301469,https://doi.org/10.1016/j.jvcir.2018.06.021,1047-3203,2018,415--423,55,Journal of Visual Communication and Image Representation,Joint entropy based learning model for image retrieval,article,WU2018415
"Block compressive sensing FOCal Underdetermined System Solver (BCS-FOCUSS) and distributed BCS-FOCUSS (DBCS-FOCUSS) are iterative algorithms for individual and joint recovery of correlated images. The performance of both these algorithms was noticed to be best within BCS framework. However, both these algorithms suffer from high computational complexity and recovery time. This is caused by the need for an explicit computation of matrix inverse in each iteration and a slow convergence from a poor starting point. In this paper, we propose a methodology to obtain fast and good initial solution using the augmented Lagrangian method to improve the convergence rate of both algorithms. We also propose to incorporate the minimum residual method to avoid matrix inversion to reduce the computational cost. Simulation studies with the proposed modified BCS-FOCUSS and DBCS-FOCUSS demonstrate a significant reduction in the computational cost and recovery time while improving reconstruction quality for both individual and joint reconstruction algorithms.","Block compressive sensing, BCS-FOCUSS, DBCS-FOCUSS, BCS-augmented Lagrangian method, Minimum residual method",Amit Satish Unde and P.P. Deepthi,https://www.sciencedirect.com/science/article/pii/S1047320318300415,https://doi.org/10.1016/j.jvcir.2018.02.009,1047-3203,2018,92--100,52,Journal of Visual Communication and Image Representation,Fast BCS-FOCUSS and DBCS-FOCUSS with augmented Lagrangian and minimum residual methods,article,UNDE201892
"3D hand pose estimation is an important and challenging problem for human-computer interaction. Recently convolutional networks (ConvNet) with sophisticated design have been employed to address it, but the improvement is not so significant. To exploit good practice and promote the performance for hand pose estimation, we propose a Region Ensemble Network (REN) for directly 3D coordinate regression. It first partitions the last convolutional outputs of ConvNet into several grid regions. Results from separate fully-connected (FC) regressors on each regions are integrated by another FC layer to perform estimation. By exploitation of several training strategies including data augmentation and smooth L1 loss, REN significantly improves the performance of ConvNet for hand pose estimation. Experiments demonstrate that our approach achieves strong performance on par or better than state-of-the-art algorithms on three public hand pose datasets. We also experiment our methods on fingertip detection and human pose datasets and obtain state-of-the-art accuracy.","Convolutional network, Hand pose estimation, Human pose estimation, Fingertip detection, Ensemble learning, Depth imaging",Guijin Wang and Xinghao Chen and Hengkai Guo and Cairong Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318300816,https://doi.org/10.1016/j.jvcir.2018.04.005,1047-3203,2018,404--414,55,Journal of Visual Communication and Image Representation,Region ensemble network: Towards good practices for deep 3D hand pose estimation,article,WANG2018404
"The popularity of multi-view panoramic video has been considerably increased for producing Virtual Reality (VR) content, due to its immersive visual experience. We argue in this paper that PSNR is less effective in assessing objective visual quality of compressed panoramic video than Sphere-based PSNR (S-PNSR), in which sphere-to-plain mapping of panoramic videos is considered. We also argue that S-PSNR is less effective in assessing perceptual visual quality compared with Perceptual PSNR (P-PSNR), which considers the front-center-bias prior of human viewing direction. The conventional Rate Control (RC) schemes of 2-Dimensional (2D) video coding are optimized on PSNR, and thus they are not suitable for panoramic video coding. To optimize S-PSNR and P-PSNR, two novel RC schemes are proposed for panoramic video coding. In particular, we develop objective and perceptual RC formulations, corresponding to optimization on S-PSNR and P-PSNR, respectively. Then, solutions to these two formulations are provided, such that bits can be allocated to each coding block for achieving optimal S-PSNR or P-PSNR in panoramic video coding. Finally, the experiment results validate the effectiveness of the proposed RC schemes in improving S-PSNR and P-PSNR of panoramic video coding.","Virtual reality, Panoramic video, Rate control, Video coding",Yufan Liu and Li Yang and Mai Xu and Zulin Wang,https://www.sciencedirect.com/science/article/pii/S104732031830049X,https://doi.org/10.1016/j.jvcir.2018.03.001,1047-3203,2018,76--85,53,Journal of Visual Communication and Image Representation,Rate control schemes for panoramic video coding,article,LIU201876
"Set-to-set face recognition has drawn much attention thanks to its rich set information. We propose a robust and efficient Set-to-Set Nearest Neighbor Classification (S2S-NNC) approach for face recognition by using the maximum weighted correlation between sets in low-dimensional projection subspaces. A pair of face sets is represented as two sets of Mutual Typical Samples (MTS) based on their maximum weighted correlation, and the S2S distance is equivalent to that between two sets of MTS. For the variation of objects within a set, the faces are partitioned into patches and projected onto a correlation subspace to find the MTS between two sets. Furthermore, we develop a S2S-NNC approach for image set-based face recognition. Compared with existing approaches, the S2S-NNC unifies the image-to-image, image-to-set and set-to-set recognition problems into one model. Experimental results show the S2S-NNC approach significantly outperforms the state-of-art approaches on large video samples and small occluded samples.","Face recognition, Set-to-set, Robust analysis, Weighted correlation analysis",Ling Wang and Hong Cheng and Zicheng Liu,https://www.sciencedirect.com/science/article/pii/S1047320318300300,https://doi.org/10.1016/j.jvcir.2018.02.004,1047-3203,2018,13--19,53,Journal of Visual Communication and Image Representation,A set-to-set nearest neighbor approach for robust and efficient face recognition with image sets,article,WANG201813
"In this paper, a robust watermarking technique is proposed using integer discrete cosine transform, non-linear chaotic map and dynamic stochastic resonance (DSR). Firstly, the host image is transformed into integer DCT domain where the coefficients are partitioned into non-over-lapping blocks. A circulant matrix is then constructed from the selected blocks. Block selection is done using a non-linear chaotic map. This circulant matrix is used for embedding the watermark by computing the singular values. The extraction of the watermark is done by producing the dynamic stochastic resonance (DSR) phenomena and casting a verification step. This verification step essentially solves the false positive detection problem that arises in SVD based watermarking. The experimental results demonstrate that the proposed scheme is imperceptible and robust against a variety of intentional or unintentional attacks.","Digital watermarking, Integer discrete cosine transform, Chaotic map, Stochastic resonance",Satendra Pal Singh and Gaurav Bhatnagar,https://www.sciencedirect.com/science/article/pii/S1047320318300543,https://doi.org/10.1016/j.jvcir.2018.03.006,1047-3203,2018,86--101,53,Journal of Visual Communication and Image Representation,A new robust watermarking system in integer DCT domain,article,SINGH201886
"This paper aims to present histogram-based local descriptors applied to Facial Expression Recognition (FER) from static images and provide a systematic review and analysis of them. First, we describe the main steps in encoding binary patterns in a local patch, which are required in every histogram-based local descriptor. Then, we list the existing local descriptors, while analysing their strengths and weaknesses. Finally, we present the experimental results of all these descriptors on commonly used facial expression databases, with varying resolution, noise, occlusion, and number of sub-regions, as well as comparing them with the results obtained by the state-of-the-art deep learning methods. This paper aims to bring together different studies of the visual features for FER by evaluating their performances under the same experimental setup, and critically reviewing various classifiers making use of the local descriptors.","Feature extraction, Local descriptors, Facial expression recognition",Cigdem Turan and Kin-Man Lam,https://www.sciencedirect.com/science/article/pii/S1047320318301196,https://doi.org/10.1016/j.jvcir.2018.05.024,1047-3203,2018,331--341,55,Journal of Visual Communication and Image Representation,Histogram-based local descriptors for facial expression recognition (FER): A comprehensive study,article,TURAN2018331
"We study the problem of 3D object detection from RGB-D images so as to achieve localization (i.e., producing a bounding box around the object) and classification (i.e., determining the object category) simultaneously. Its challenges arise from high intra-class variability, illumination change, background clutter and occlusion. To solve this problem, we propose a novel solution that integrates the 2D information (RGB images), the 3D information (RGB-D images) and the object/scene context information together, and call it the Context-Assisted 3D (C3D) method. In the proposed C3D method, we first use a convolutional neural network (CNN) to jointly detect a 3D object in a scene and its scene category. Then, we improve the detection result furthermore with a Conditional Random Field (CRF) model that incorporates the object potential, the scene potential, the scene/object context, the object/object context, and the room geometry. Extensive experiments are conducted to demonstrate that the proposed C3D method achieves the state-of-the-art performance for 3D object detection against the SUN RGB-D benchmark dataset.","3D Object Detection, Indoor Scene Understanding, Convolutional Neural Network, Conditional Random Field",Yuzhuo Ren and Chen Chen and Shangwen Li and C.-C. Jay Kuo,https://www.sciencedirect.com/science/article/pii/S1047320318301111,https://doi.org/10.1016/j.jvcir.2018.05.019,1047-3203,2018,131--141,55,Journal of Visual Communication and Image Representation,Context-Assisted 3D (C3D) Object Detection from RGB-D Images,article,REN2018131
"A (t, s, k, n) essential secret image sharing scheme (ESIS) shares a secret image between s essential and n-s non-essential shadows, where t essential shadows and total k shadows are needed to recover the secret image. This paper presents a three-layered structure for ESIS by generating equal-sized essential and non-essential shadows for security to prevent the discovery of essential shadows using only the size difference. In the first layer, two criteria of t essential shadows and totally k shadows are needed. The second and third layers generate essential and non-essential shadows to fit the requirement of the (t, s, k, n) thresholds. The proposed scheme is proved to fit ESIS and equal-sized requirements with properly chosen parameters. Theoretical analysis and experimental results show that the proposed scheme is superior to other schemes on the smallest shadow sizes among equal-sized ESIS schemes.","Secret image sharing, Essential, Non-essential, Three-layered structure",Chien-Chang Chen,https://www.sciencedirect.com/science/article/pii/S1047320318300385,https://doi.org/10.1016/j.jvcir.2018.02.006,1047-3203,2018,143--150,52,Journal of Visual Communication and Image Representation,Essential secret image sharing scheme with equal-sized shadows generation,article,CHEN2018143
"A keyframe summary of a video must be concise, comprehensive and diverse. Current video summarisation methods may not be able to enforce diversity of the summary if the events have highly similar visual content, as is the case of egocentric videos. We cast the problem of selecting a keyframe summary as a problem of prototype (instance) selection for the nearest neighbour classifier (1-nn). Assuming that the video is already segmented into events of interest (classes), and represented as a dataset in some feature space, we propose a Greedy Tabu Selector algorithm (GTS) which picks one frame to represent each class. An experiment with the UT (Egocentric) video database and seven feature representations illustrates the proposed keyframe summarisation method. GTS leads to improved match to the user ground truth compared to the closest-to-centroid baseline summarisation method. Best results were obtained with feature spaces obtained from a convolutional neural network (CNN).","Keyframe summary, Nearest neighbour classifier, Instance selection, Egocentric video, Feature representations",Ludmila I. Kuncheva and Paria Yousefi and Jurandy Almeida,https://www.sciencedirect.com/science/article/pii/S1047320318300427,https://doi.org/10.1016/j.jvcir.2018.02.010,1047-3203,2018,118--130,52,Journal of Visual Communication and Image Representation,Edited nearest neighbour for selecting keyframe summaries of egocentric videos,article,KUNCHEVA2018118
"Reversible visual transformation reversibly transforms a secret image to a freely-selected target image and gets a camouflage image similar to the target image, which has been proved to be very useful in such two applications: privacy protection of images and reversible data hiding in encrypted images. Now, a new reversible transformation technique for color images is proposed by exploring and utilizing the correlations among three color channels and inside each color channel. The amount of the accessorial information for recording transformation parameters is largely reduced. Therefore, the visual quality of the created camouflage image is much improved by dividing the secret image and the target image into even smaller blocks for transformation.","Reversible visual transformation, Image camouflage, Image encryption, Reversible data hiding",Dongdong Hou and Chuan Qin and Nenghai Yu and Weiming Zhang,https://www.sciencedirect.com/science/article/pii/S1047320317302237,https://doi.org/10.1016/j.jvcir.2017.11.014,1047-3203,2018,134--145,53,Journal of Visual Communication and Image Representation,Reversible visual transformation via exploring the correlations within color images,article,HOU2018134
"Digital forensics is rapidly evolving as a direct consequence of the adoption of machine-learning methods allied with ever-growing amounts of data. Despite the fact that these methods yield more consistent and accurate results, they may face adoption hindrances in practice if their produced results are absent in a human-interpretable form. In this paper, we exemplify how human-interpretable (a.k.a., accountable) extensions can enhance existing algorithms to aid human experts, by introducing a new method for the source printer attribution problem. We leverage the recently proposed Convolutional Texture Gradient Filter (CTGF) algorithmâs ability to capture local printing imperfections to introduce a new method that maps and highlights important attribution features directly onto the investigated printed document. Supported by Random Forest classifiers, we isolate and rank features that are pivotal for differentiating a printer from others, and back-project those features onto the investigated document, giving analysts further evidence about the attribution process.","Accountable machine learning, Digital forensics, Source printer attribution, Feature back-projection, Feature mapping, Feature importance",Luiz C. Navarro and Alexandre K.W. Navarro and Anderson Rocha and Ricardo Dahab,https://www.sciencedirect.com/science/article/pii/S1047320318300786,https://doi.org/10.1016/j.jvcir.2018.04.002,1047-3203,2018,257--272,53,Journal of Visual Communication and Image Representation,Connecting the dots: Toward accountable machine-learning printer attribution methods,article,NAVARRO2018257
"In this paper, we propose a novel mid-level feature representation for the recognition of actions in videos. This descriptor proves to posses relevant discriminative power when used in a generic action recognition pipeline. It is well known that mid-level feature descriptors learnt using class-oriented information are potentially more distinctive than the low-level features extracted in a bottom-up unsupervised fashion. In this regard, we introduce the notion of concepts, a mid-level feature representation capable of tracking the dynamics of motion salient regions over consecutive frames in a video sequence. Our feature representation is based on the idea of region correspondence over consecutive frames and we make use of an unsupervised iterative bipartite graph matching algorithm to extract representative visual concepts from action videos. The progression of such salient regions, which are also consistent in appearance, are henceforth represented as chain graphs. Finally, we adopt an intuitive time-series pooling strategy to extract discriminant features from the chains, which are then used in a dictionary learning based classification framework. Given the high variability of the movements of different human body parts in separate actions, the extracted conceptual descriptors are proved to capture the different dynamic characteristics by exclusively encoding the interaction parts associated to the chains. Further, we use such descriptors in a semi-supervised, clustering-based zero-shot action recognition setting, showing good performance and without resorting to costly attribute annotation. We validate the proposed framework on four public datasets namely KTH, UCF-101, HOHA and HMDB-51, reporting increased (and comparable in some cases) classification accuracies with respect to the state of the art.","Action recognition, Mid level feature, Zero shot learning",Abhinaba Roy and Biplab Banerjee and Vittorio Murino,https://www.sciencedirect.com/science/article/pii/S1047320318301512,https://doi.org/10.1016/j.jvcir.2018.06.026,1047-3203,2018,829--840,55,Journal of Visual Communication and Image Representation,Discriminative body part interaction mining for mid-level action representation and classification,article,ROY2018829
"Image completion is a technique to fill missing regions in a damaged or redacted image. A patch-based approach is one of major approaches, which solves an optimization problem that involves pixel values in missing regions and similar image patch search. One major problem of this approach is that it sometimes duplicates implausible texture in the image or overly smooths down a missing region when the algorithm cannot find better patches. As a practical remedy, the user may provide an interaction to identify such regions and re-apply image completion iteratively until she/he gets a desirable result. In this work, inspired by this idea, we propose a framework of human-in-the-loop style image completion with automatic failure detection using a deep neural network instead of human interaction. Our neural network takes small patches extracted from multiple feature maps obtained from the completion process as input for the automated interaction process, which is iterated several times. We experimentally show that our neural network outperforms a conventional linear support vector machine. Our subjective evaluation demonstrates that our method drastically improves the visual quality of resulting images compared to non-iterative application.","Image completion, Image inpainting, Convolutional neural network, Failure detection",Takahiro Tanaka and Norihiko Kawai and Yuta Nakashima and Tomokazu Sato and Naokazu Yokoya,https://www.sciencedirect.com/science/article/pii/S104732031830110X,https://doi.org/10.1016/j.jvcir.2018.05.015,1047-3203,2018,56--66,55,Journal of Visual Communication and Image Representation,Iterative applications of image completion with CNN-based failure detection,article,TANAKA201856
"Hyperspectral pansharpening aims to integrate the panchromatic (PAN) and hyperspectral (HS) images into a single HS image with high spatial and high spectral resolution. This paper proposes a novel hyperspectral pansharpening method based on guided filter and gaussian filter. Most guided filter based researches extract the spatial details from the PAN image or the single band HS intensity component, and incorrect generation of the intensity component causes the spectral distortion. Different from the traditional guided filter based methods, the structure of the HS image is fully considered by the proposed method. We first use the high frequency layer of each band of the HS image as the guidance image of the guided filter. Then, the total spatial details are extracted from both the PAN image and the HS image. The total spatial details are finally injected into each band of the HS image low frequency layer to generate the fused image. Experiments demonstrate that the proposed method outperforms some state-of-the-art methods in terms of objective quality assessment and subjective visual effect.","Hyperspectral image, Panchromatic image, Image fusion, Guided filter",Wenqian Dong and Song Xiao and Yongxu Li,https://www.sciencedirect.com/science/article/pii/S1047320318300701,https://doi.org/10.1016/j.jvcir.2018.03.014,1047-3203,2018,171--179,53,Journal of Visual Communication and Image Representation,Hyperspectral pansharpening based on guided filter and Gaussian filter,article,DONG2018171
"Graph-based methods have shown their potentialities for saliency detection. In this paper, a graph-based framework is proposed for saliency detection, which incorporates perceptual cues into the framework and uses the background-excluded seeds to propagate saliency. Firstly, a graph is constructed by two perceptual cues, including proximity and similarity. Secondly, probable background nodes are generated by a novel background probability measure and used to pick out reliable seeds. Then a label propagation model is developed to diffuse saliency based on these reliable seeds. Lastly, another perceptual cue called rareness is integrated into a cost function to optimize the propagation result. Results on four datasets demonstrate that the proposed method achieves superior performance against fifteen state-of-the-art methods in terms of different evaluation metrics.","Saliency detection, Perceptual cue, Graph-based framework, Label propagation, Seed, Background probability",Xiyin Wu and Zhong Jin and Jingbo Zhou and Xiaodi Ma,https://www.sciencedirect.com/science/article/pii/S1047320318300828,https://doi.org/10.1016/j.jvcir.2018.04.006,1047-3203,2018,51--62,54,Journal of Visual Communication and Image Representation,Saliency propagation with perceptual cues and background-excluded seeds,article,WU201851
"Measurements arose from strong reflections combined with occlusions significantly degrade accuracy of multi-target tracking. Few methods have addressed this problem, and thus this paper proposes a robust multi-target tracker for mixed images with occlusions. For multi-cue integration using co-inference tracking, moving object detection significantly enhances motion cue based correction in the presence of reflections. Target templates are represented by sets of color and spatiality histograms. Joint likelihoods referring to both the target motion trajectory and appearance model of the co-inference fused state are computed. Thus each optimized particle weight with the criteria of maximum joint likelihood is more reliable in the face of reflections and inter-object occlusions. State estimation is achieved with the sample-based data association probability and occlusion confidence indicator. Experimental results show that the proposed tracker outperforms the-state-of-the-art multi-target trackers on images with strong reflections and inter-object occlusions.","Multi-target tracking, Reflections, Occlusions, Multi-cue integration, Data association",Ting-hao Zhang and Chih-Wei Tang,https://www.sciencedirect.com/science/article/pii/S1047320318300270,https://doi.org/10.1016/j.jvcir.2018.02.001,1047-3203,2018,45--57,52,Journal of Visual Communication and Image Representation,Multiple-target tracking on mixed images with reflections and occlusions,article,ZHANG201845
"Following the rapid developments of network multimedia, video copyright protection online has become a hot topic in recent researches. However, video copy detection is still a challenging task in the domain of video analysis and computer vision, due to the large variations in scale and illumination of the copied contents. In this paper, we propose a novel deep learning based approach, in which we jointly use the Convolution Neural Network (CNN) and Recurrent Neural Network (RNN) to solve the specific problem of detecting copied segments in videos. We first utilize a Residual Convolutional Neural Network(ResNet) to extract content features of frame-levels, and then employ a SiameseLSTM architecture for spatial-temporal fusion and sequence matching. Finally, the copied segments are detected by a graph based temporal network. We evaluate the performance of the proposed CNN-RNN based approach on a public large scale video copy dataset called VCDB, and the experiment results demonstrate the effectiveness and high robustness of our method which achieves the significant performance improvements compared to the state of the art.","Video copyright, CNN, Sequence matching, SiamesLSTM",Yaocong Hu and Xiaobo Lu,https://www.sciencedirect.com/science/article/pii/S1047320318301081,https://doi.org/10.1016/j.jvcir.2018.05.013,1047-3203,2018,21--29,55,Journal of Visual Communication and Image Representation,Learning spatial-temporal features for video copy detection by the combination of CNN and RNN,article,HU201821
"Currently, high-speed cameras has been a very common equipment in many important application fields. How to effectively and automatically extract the ROI (region of interest) for the slow-motion video has been a novel interesting challenge. In recent research work, we designed a ROI extraction framework for the video frames produced by high-speed cameras. The entire framework includes two parts: a novel but simple color similarity measure model is improved to distinguish different pixels; a skeleton feature points based serialized segmentation tactics is proposed to generate seed points. By using the multithreading patterns of parallelizing computations in the extraction process, the ROI in the serialized color slow-motion video frames can be marked automatically and accurately. Comparing with the common methods, this method has advantage in segmentation effect and computational efficiency. It can establish the technical basis for the pertinent subsequent studies.","Slow-motion video, Serialized extraction, Color frame image, Automatic seed point generation",Bin Liu and Xiaohui Zhang and Mingzhe Wang and Fengqi Li,https://www.sciencedirect.com/science/article/pii/S1047320318301366,https://doi.org/10.1016/j.jvcir.2018.06.013,1047-3203,2018,270--318,55,Journal of Visual Communication and Image Representation,An automatic and serialized ROI extraction framework for the slow-motion video frames,article,LIU2018270
"The just noticeable distortion (JND) has been considered a suitable solution for controlling the watermark strength and generating robust watermarking schemes with distortions that are below the sensitivity threshold. However, JND assumes the same attention level for all image regions, which does not reflect the behavior of an observer. Recently, several models have utilized the modulatory effect of visual attention over JND to improve the efficiency of watermarking schemes. However, most of them have focused on still images. In this paper, we propose a saliency-modulated JND profile for improving video watermarking schemes. Our method aims to adapt the watermark strength to obtain the most robust possible scheme with an imperceptible watermark. Moreover, it has the advantage of fully exploiting the spatiotemporal properties of video to minimize its perceptual redundancies and achieve low computational complexity. Experimental results show the effectiveness of our proposed method and its contributions to video watermarking process.","Video watermarking, Visual attention, Saliency regions, Just-Noticeable-Distortion (JND)",Antonio Cedillo-Hernandez and Manuel Cedillo-Hernandez and Mariko {Nakano Miyatake} and Hector {Perez Meana},https://www.sciencedirect.com/science/article/pii/S1047320318300397,https://doi.org/10.1016/j.jvcir.2018.02.007,1047-3203,2018,106--117,52,Journal of Visual Communication and Image Representation,A spatiotemporal saliency-modulated JND profile applied to video watermarking,article,CEDILLOHERNANDEZ2018106
"Image set based collaborative representation and classification(ISCRC) has been proposed and achieved state-of-the-art performance. Though ISCRC works well for Image set based face recognition(ISFR), the classification mechanism of ISCRC is still unclear. Besides, another challenge that ISCRC encountered is to deal with the high-dimensional data. In this paper, we first propose a novel Probabilistic Collaborative Representation based Classifier for Image Set (ProCRCIS), which is interpreted from a probabilistic viewpoint. Then, according to the reconstruction residual-based classification rule of ProCRCIS, we propose a novel dimensionality reduction method, called Probabilistic Collaborative Representation based Orthogonal Discriminative Projection for Image Set(ProCR-ODP-IS). The goal of ProCR-ODP-IS is to find a projection space such that the between-class reconstruction residual is maximized and the within-class reconstruction residual is minimized simultaneously. Hence, this projected space can fit ProCRCIS very well. Extensive experimental results on different datasets demonstrate the superiority of the proposed method compared to the state-of-the-arts.","Image set, Face recognition, Probabilistic collaborative representation, Orthogonal discriminative projection",Quan Zhang and Huaijiang Sun,https://www.sciencedirect.com/science/article/pii/S1047320318301123,https://doi.org/10.1016/j.jvcir.2018.05.016,1047-3203,2018,106--114,55,Journal of Visual Communication and Image Representation,Probabilistic collaborative representation based orthogonal discriminative projection for image set classification,article,ZHANG2018106
"Gamut mapping transforms the color gamut of an image to that of a target device. Two cases are usually considered: gamut reduction (target gamut smaller than source gamut), and gamut extension (target gamut larger than the source gamut). Less attention is devoted to the more general case, when neither gamut is fully included in the other. In this work we unify and expand two recent methods for gamut extension and reduction, so as to simultaneously perform both forms of gamut mapping in different regions of the same image without introducing color artifacts or halos. We demonstrate the usefulness of this approach for the traditional gamut mapping problem, and also how the proposed method can be used to adapt the color palette of an image so that it is closer to that of a given reference image. Results are compared with the state-of-the-art and validated through user tests and objective metrics.","Gamut mapping, Gamut extension, Gamut reduction, Color coherence",Javier Vazquez-Corral and Marcelo BertalmÃ­o,https://www.sciencedirect.com/science/article/pii/S104732031830107X,https://doi.org/10.1016/j.jvcir.2018.05.012,1047-3203,2018,204--212,54,Journal of Visual Communication and Image Representation,Spatial gamut mapping among non-inclusive gamuts,article,VAZQUEZCORRAL2018204
"We propose a method for the automatic positioning of pre-defined landmarks on 3-D models of anatomical structures. We exploit a group of atlases consisting of multiple triangular meshes for which the defined landmarks have been placed by experts. We compute an initial coarse global registration of the patient mesh with an expert mesh by using a curvature-enhanced Iterative Closest Point (ICP) algorithm. Adaptive local rigid registrations refine the fit for the projection of reference landmarks onto the surface of the patient structure. An automatic selection based on a fit criterion computes a final position for each landmark. Our positioning method improves the efficiency of the positioning task, being completely unsupervised and yielding results competitive with those of the manual positioning. We provide comparisons with previous works of the literature. The automatic positioning for each target structure is completely reproducible as opposed to manual positioning, affected by intra-operator variability.","Orthopaedics, Knee, Landmarks, Atlas, Registration, Positioning",Hector Jacinto and SÃ©bastien Valette and RÃ©my Prost,https://www.sciencedirect.com/science/article/pii/S1047320317302249,https://doi.org/10.1016/j.jvcir.2017.11.015,1047-3203,2018,167--177,50,Journal of Visual Communication and Image Representation,Multi-atlas automatic positioning of anatomical landmarks,article,JACINTO2018167
"Among various traditional art forms, brush stroke drawing is one of the widely used styles in modern computer graphic tools such as GIMP, Photoshop and Painter. In this paper, we develop an AI-aided art authoring (A4) system of non-photorealistic rendering that allows users to automatically generate brush stroke paintings in a specific artistâs style. Within the reinforcement learning framework of brush stroke generation proposed by Xie et al. (2012), the first contribution in this paper is the application of regularized policy gradient method, which is more suitable for the stroke generation task; the other contribution is to learn artistsâ drawing styles from video-captured stroke data by inverse reinforcement learning. Through experiments, we demonstrate that our system can successfully learn artistsâ styles and render pictures with consistent and smooth brush strokes.","Stroke-based stylization, Reinforcement learning, Inverse reinforcement learning",Ning Xie and Yang Yang and Heng Tao Shen and Ting Ting Zhao,https://www.sciencedirect.com/science/article/pii/S1047320317302444,https://doi.org/10.1016/j.jvcir.2017.12.012,1047-3203,2018,29--39,51,Journal of Visual Communication and Image Representation,Stroke-based stylization by learning sequential drawing examples,article,XIE201829
"Many studies have been conducted on multi-server HTTP Adaptive Streaming (HAS). The existing schemes can highly utilize the available bandwidth by aggregating the multipath bandwidth and improve the video quality by switching servers when network congestion occurs. However, these existing schemes have problems that adversely affect the Quality of Experience (QoE) in a multi-server environment. To cope with these problems, we analyze the existing HAS schemes in multi-server environments. Through simulation-based performance analysis, we prove that these existing schemes lead to playback stalling and frequent quality changes. Based on the analysis, we propose a new HAS scheme for multi-server environments. The proposed scheme improves the QoE by alleviating the problems of the existing schemes. Through the simulation results, we prove that the proposed scheme alleviates the shortcomings of the existing schemes and improves QoE metrics compared with the existing multi-server schemes.","HTTP adaptive streaming, Multi-server environment, Quality of experience",Dooyeol Yun and Woonjoo Park and Sun-Joong Kim and Kwangsue Chung,https://www.sciencedirect.com/science/article/pii/S1047320318300038,https://doi.org/10.1016/j.jvcir.2018.01.003,1047-3203,2018,14--22,51,Journal of Visual Communication and Image Representation,HTTP adaptive streaming scheme for improving the quality of experience in multi-server environments,article,YUN201814
"Adaptive quantization (AQ) proves to be an effective coding tool to improve the performance of video coding. This paper presents a perceptually temporal AQ method to improve the subjective coding performance for High Efficiency Video Coding (HEVC). We first put forward a perceptual quality oriented motion estimation algorithm, which is conducted with a spatial-temporal just noticeable distortion (JND) model. Then one perceptual feature in temporal domain is proposed to develop our AQ method, which can generate different quantization parameter (QP) offsets for each coding unit (CU). The proposed method fully utilizes the temporal and perceptual characteristics of each CU, which can produce more visual-friendly QP offsets distribution. Experiments are conducted on HM16.0 (HEVC reference software), and with SSIM (Structure Similarity Index Metric) as the distortion metric, more than 8.08% and 7.95% rate savings can be obtained for Low-Delay-P (LDP) and Low-Delay-B (LDB) configurations on average, respectively. The subjective quality evaluation demonstrates that the proposed AQ method can achieve comparable visual quality as the HM16.0 while the proposed method can yield remarkable bitrate reductions.","Inter-frame dependency, AQ, QP offset, JND, SSIM, Subjective quality",Guoqing Xiang and Huizhu Jia and Mingyuan Yang and Xinfeng Zhang and Xiaofeng Huang and Jie Liu and Xiaodong Xie,https://www.sciencedirect.com/science/article/pii/S1047320317302201,https://doi.org/10.1016/j.jvcir.2017.11.011,1047-3203,2018,280--289,50,Journal of Visual Communication and Image Representation,A perceptually temporal adaptive quantization algorithm for HEVC,article,XIANG2018280
"Facial biometric systems are vulnerable to fraudulent access attempts by presenting photographs or videos of a valid user in front of the sensor also known as âspoofing attacksâ. Multiple protection measures have been proposed but limited attention has been dedicated to exclusive motion-based countermeasures since the arrival of video and mask attacks. A novel motion-based countermeasure which exploits natural and unnatural motion cues is presented. The proposed method takes advantage of the Conditional Local Neural Fields (CLNF) face tracking algorithm to extract rigid and non-rigid face motions. Similarly to the bag-of-words feature encoding, a vocabulary of motion sequences is constructed to derive discriminant mid-level motion features using the Fisher vector framework. Extensive experiments are conducted on ReplayAttack-DB, CASIA-FASD and MSU-MFSD databases. Complementary experiments on rigid mask attacks from the 3DMAD public database are also conducted and generalization issues are investigated via cross-database evaluation in particular.","Motion cues, Face anti-spoofing, Fisher kernel, Replay attacks",Taiamiti Edmunds and Alice Caplier,https://www.sciencedirect.com/science/article/pii/S1047320317302365,https://doi.org/10.1016/j.jvcir.2017.12.004,1047-3203,2018,314--332,50,Journal of Visual Communication and Image Representation,Motion-based countermeasure against photo and video spoofing attacks in face recognition,article,EDMUNDS2018314
"The fusion of hyperspectral and panchromatic images aims to generate a fused image with high spatial and high spectral resolutions. This paper proposes a novel hyperspectral pansharpening method using an average filter and a guided filter. Based on the traditional component substitution methods, we propose a new and simple method to extract the spatial information of the HS image by average filtering at first. Then to solve the significant spectral distortion, a guided filter is utilized to obtain more detailed spatial information from the PAN image which has been sharpened. The appropriate injection gains matrix is generated by selecting the optimal value of the tradeoff coefficient. The spatial detail is finally injected into each band of the interpolated HS image to achieve the fused image. Experimental results demonstrate that the proposed method can obtain more spatial information and preserve more spectral information in both subjective and objective evaluations.","Hyperspectral (HS) image, Panchromatic (PAN) image, Guided filter, Average filter, Component substitution (CS)",Jiahui Qu and Yunsong Li and Wenqian Dong,https://www.sciencedirect.com/science/article/pii/S1047320318300117,https://doi.org/10.1016/j.jvcir.2018.01.006,1047-3203,2018,151--158,52,Journal of Visual Communication and Image Representation,Fusion of hyperspectral and panchromatic images using an average filter and a guided filter,article,QU2018151
"In this paper, a novel reversible data hiding scheme for Two-stage VQ (Vector quantization) compressed images based on SOC (Search-order coding) scheme is proposed. Two-stage VQ improves VQ by obtaining better reconstructed image and generating indices with higher correlation. The difference image, as the input of Two-stage VQ, is produced by employing the first codeword in state codebook and the current image block. The main idea of SOC method is exploiting the correlation of indices to derive better compression performance. The combination of SOC and data hiding can achieve both high compression rate and high embedding capacity. Since Two-stage VQ can achieve indices with higher correlation, this advantage is applied to enhance the data hiding performance. Moreover, the cover image can be reconstructed by the receiver without using any side information. To show the superiority of our proposed scheme, several state-of-the-art schemes designed for compression domain are cited for comparison.","Reversible data hiding (RDH), Vector quantization (VQ), Side-match vector quantization (SMVQ), SOC (Search-order coding), Two-stage VQ, Difference image",Zhibin Pan and Lingfei Wang,https://www.sciencedirect.com/science/article/pii/S1047320317302286,https://doi.org/10.1016/j.jvcir.2017.11.020,1047-3203,2018,186--198,50,Journal of Visual Communication and Image Representation,Novel reversible data hiding scheme for Two-stage VQ compressed images based on search-order coding,article,PAN2018186
"Recently, saliency detection has become an active research topic in learning from labeled image, where various supervised methods were designed. Many existing methods usually cast saliency detection as a binary classification or regression problem, in which saliency detection performance relies heavily on the expensive pixel-wise annotations of salient objects. This paper addresses the issue by developing a novel learning-to-rank model with a limited number of training data, which combines the strength of cost-sensitive label ranking methods with the power of low-rank matrix recovery theories. Rather than using a binary decision for each saliency value, our approach ranks saliency values in a descending order with the estimated relevance to the given saliency. Additionally, we also aggregate the prediction models for different saliency labels into a matrix, and solve saliency ranking via a low-rank matrix recovery problem. Extensive experiments over challenging benchmarks clearly validate advantage of our method.","Saliency detection, Label ranking, Matrix recovery",Zun Li and Congyan Lang and Songhe Feng and Tao Wang,https://www.sciencedirect.com/science/article/pii/S1047320317302043,https://doi.org/10.1016/j.jvcir.2017.11.004,1047-3203,2018,16--26,50,Journal of Visual Communication and Image Representation,Saliency ranker: A new salient object detection method,article,LI201816
"How to improve the compression efficiency of encrypted signals remains a challenging problem. To alleviate this problem, this paper develops a new compression scheme on encrypted gray images by exploiting the Cauchy distribution and the weighted rate-distortion optimization (wRDO). In the scheme, the low-frequency and wavelet subbands generated through lifting wavelet transform are encrypted by stream and permutation ciphers, respectively. They are then compressed in lossless and lossy ways, respectively. Inverse operations are finally conducted at the receiver to reconstruct the original image. The lossy compression is formulated as a problem of wRDO and further solved by incorporating the Cauchy distribution that is demonstrated via extensive simulations to well characterize statistical distributions of wavelet subbands. Experimental results show that the proposed scheme is significantly better than other permutation-based prior arts and achieves comparable or even better performance in comparison to the conventional JPEG algorithm with original unencrypted images as input.","Compression of encrypted signals, Statistical model, Rate-distortion optimization, Lifting wavelet",Chuntao Wang and Deqing Xiao and Hongxing Peng and Rongyue Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318300166,https://doi.org/10.1016/j.jvcir.2018.01.007,1047-3203,2018,122--130,51,Journal of Visual Communication and Image Representation,A lossy compression scheme for encrypted images exploiting Cauchy distribution and weighted rate distortion optimization,article,WANG2018122
"Egocentric vision consists in acquiring images along the day from a first person point-of-view using wearable cameras. The automatic analysis of this information allows to discover daily patterns for improving the quality of life of the user. A natural topic that arises in egocentric vision is storytelling, that is, how to understand and tell the story relying behind the pictures. In this paper, we tackle storytelling as an egocentric sequences description problem. We propose a novel methodology that exploits information from temporally neighboring events, matching precisely the nature of egocentric sequences. Furthermore, we present a new method for multimodal data fusion consisting on a multi-input attention recurrent network. We also release the EDUB-SegDesc dataset. This is the first dataset for egocentric image sequences description, consisting of 1339 events with 3991 descriptions, from 55â¯days acquired by 11 people. Finally, we prove that our proposal outperforms classical attentional encoder-decoder methods for video description.","Egocentric vision, Video description, Deep learning, Multi-modal learning",Marc BolaÃ±os and Ãlvaro Peris and Francisco Casacuberta and Sergi Soler and Petia Radeva,https://www.sciencedirect.com/science/article/pii/S1047320317302316,https://doi.org/10.1016/j.jvcir.2017.11.022,1047-3203,2018,205--216,50,Journal of Visual Communication and Image Representation,Egocentric video description based on temporally-linked sequences,article,BOLANOS2018205
"With the rapid development of Internet, the views of the online video have increased dramatically. Meanwhile, the corresponding online video advertising market showed a momentum of rapid and sustained development. In order to attract more potential purchasers and reduce the interference on the ordinary video browsers, many researchers and enterprises have conducted the research of video online advertising. At present, the insertion methods of most video advertising are always position-fixed, mandatory timing, quantitative, and the relevance of advertisement content and the video content is usually ignored. These methods will inevitably reduce the advertising effect because of browsersâ dissatisfaction and resistance. In order to overcome the shortages of the existing methods of video advertisement insertion, this paper proposed an effective content-targeted method for online video advertising. The insertion of advertisement is determined by comparing the content of the videos and the advertisements. At the same time, the characteristics of the scene switching in the video are taken into account to select the appropriate position of the advertisement insertion. Experimental results show that our method can provide a better user experience than existing methods, and its attractiveness and comfortableness is greatly improved.","Advertising insertion, Content-targeted, Key frame extraction, Scene boundary detection, Similarity measurement",Guanyao Wang and Li Zhuo and Jiafeng Li and Dongyue Ren and Jing Zhang,https://www.sciencedirect.com/science/article/pii/S1047320317302018,https://doi.org/10.1016/j.jvcir.2017.11.001,1047-3203,2018,40--48,50,Journal of Visual Communication and Image Representation,An efficient method of content-targeted online video advertising,article,WANG201840
"Image denoising algorithms are evaluated using images corrupted by artificial noise, which may lead to incorrect conclusions about their performances on real noise. In this paper we introduce a dataset of color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes. We also introduce a method for estimating the true noise level in our images, since even the low noise images contain small amounts of noise. We evaluate the accuracy of our noise estimation method on real and artificial noise, and investigate the Poisson-Gaussian noise model. Finally, we use our dataset to evaluate six denoising algorithms: Active Random Field, BM3D, Bilevel-MRF, Multi-Layer Perceptron, and two versions of NL-means. We show that while the Multi-Layer Perceptron, Bilevel-MRF, and NL-means with soft threshold outperform BM3D on gray images with synthetic noise, they lag behind on our dataset.","Image denoising, Denoising dataset, Low light noise, Poisson-Gaussian noise model",Josue Anaya and Adrian Barbu,https://www.sciencedirect.com/science/article/pii/S1047320318300208,https://doi.org/10.1016/j.jvcir.2018.01.012,1047-3203,2018,144--154,51,Journal of Visual Communication and Image Representation,RENOIR â A dataset for real low-light image noise reduction,article,ANAYA2018144
"In this paper, we propose a design method for linear phase (LP) nonsubsampled nonuniform directional filter bank (NUDFB) with arbitrary number of subbands and arbitrary directional partitioning. The proposed NUDFB is simply designed by windowing the analytical expressions of wedge-shaped filters in space domain. The direction and angular bandwidth of the filters are determined by only two angular parameters. It can extract directional information according to the directional distribution of images, making it efficient in the directional representation of images. In addition, the perfect reconstruction conditions are derived. Numerical experiments on image directional information extraction and image denoising are given to illustrate the performance of our NUDFB. The results show that our NUDFB outperforms various directional decomposition methods while possesses LP property and more flexibility.","Nonsubsampled nonuniform directional filter banks, Arbitrary directional partitioning, Multiresolution decomposition",Li Fang and Long Ye and Yun Tie and Wei Zhong and Qin Zhang,https://www.sciencedirect.com/science/article/pii/S1047320317302456,https://doi.org/10.1016/j.jvcir.2017.12.013,1047-3203,2018,23--28,51,Journal of Visual Communication and Image Representation,Design of linear-phase nonsubsampled nonuniform directional filter bank with arbitrary directional partitioning,article,FANG201823
"The traditional (k,n) threshold secret image sharing (SIS) schemes dealt with the full secret image neglecting the possible situation that only part of the secret image needs protection. However, in some applications, only target part of the secret image may need to be protected while other parts may be not in a full image. In this paper, we consider the partial secret image sharing (PSIS) issue as well as propose a PSIS scheme for (k,n) threshold based on image inpainting and linear congruence (LC)-based SIS. The full secret image including the secret target part and other parts will be recovered by collecting any k or more shadow images, which can be further reconstructed losslessly by adding all the inpainted meaningful shadow images. Furthermore, the proposed scheme can share irregular target in a progressive way. Experiments are conducted to evaluate the efficiency of the proposed scheme.","Secret image sharing, Partial secret image sharing, Image inpainting, Linear congruence, Color image, Lossless recovery",Xuehu Yan and Yuliang Lu and Lintao Liu and Shen Wang,https://www.sciencedirect.com/science/article/pii/S1047320317302213,https://doi.org/10.1016/j.jvcir.2017.11.012,1047-3203,2018,135--144,50,Journal of Visual Communication and Image Representation,"Partial secret image sharing for (k,n) threshold based on image inpainting",article,YAN2018135
"Over the past two decades, the nature of child pornography in terms of generation, distribution and possession of images drastically changed, evolving from basically covert and offline exchanges of content to a massive network of contacts and data sharing. Nowadays, the internet has become not only a transmission channel but, probably, a child pornography enabling factor by itself. As a consequence, most countries worldwide consider a crime to take, or permit to be taken, to store or to distribute images or videos depicting any child pornography grammar. But before action can even be taken, we must detect the very existence or presence of sexually exploitative imagery of children when gleaning over vast troves of data. With this backdrop, veering away from virtually all off-the-shelf solutions and existing methods in the literature, in this work, we leverage cutting-edge data-driven concepts and deep convolutional neural networks (CNNs) to harness enough characterization aspects from a wide range of images and point out the presence of child pornography content in an image. We explore different transfer-learning strategies for CNN modeling. CNNs are first trained with problems for which we can gather more training examples and upon which there are no serious concerns regarding collection and storage and then fine-tuned with data from the target problem of interest. The learned networks outperform different existing solutions and seem to represent an important step forward when dealing with child pornography content detection. The proposed solutions are encapsulated in a sandbox virtual machine ready for deployment by experts and practitioners. Experimental results with tens of thousands of real cases show the effectiveness of the proposed methods.","Child pornography, SEIC content, Deep learning, Transfer learning, Fine tuning",Paulo Vitorino and Sandra Avila and Mauricio Perez and Anderson Rocha,https://www.sciencedirect.com/science/article/pii/S1047320317302377,https://doi.org/10.1016/j.jvcir.2017.12.005,1047-3203,2018,303--313,50,Journal of Visual Communication and Image Representation,Leveraging deep neural networks to fight child pornography in the age of social media,article,VITORINO2018303
"In this paper, an automatic facial expression recognition method is proposed to extract feature from video sequences. First, we modified the Marr-Hildreth detector with Wiener filtering, adaptive sub-layer compensation (ASLC) and hysteresis to alleviate the negative effects of traditional one and then, the deformable elastic body spline (EBS) model is extended by using different Poissonâs rate to model the facial muscle fiber, which accommodate the fact that different muscle fiber has a different way of deformation. The ASLC feature and the improved EBS feature are fused together to form the facial feature vector. Further, we utilize the Discriminative Isomap (D-Isomap) approach to embed the facial feature into a low dimensional space. The final decision is made by computing the nearest class center of the feature space. RML Emotion database and Cohn-Kanade (CK) database are both used for the experiment and the results demonstrate the effectiveness of the proposed method.","Marr-Hildreth detector, Adaptive sub-layer compensation, Elastic body spline, Discriminative Isomap",Xin Guo and Yun Tie and Long Ye and Jinyao Yan,https://www.sciencedirect.com/science/article/pii/S104732031730216X,https://doi.org/10.1016/j.jvcir.2017.11.007,1047-3203,2018,65--73,50,Journal of Visual Communication and Image Representation,Identifying facial expression using adaptive sub-layer compensation based feature extraction,article,GUO201865
"Improving denoising algorithms based on nonlocal self-similarity (NSS) to cope with increasing noise levels has become difficult. This is primarily because of difficulty in accurately grouping similar image patches solely on original spatial-domain of noisy images. To solve this problem, we propose to group similar patches on transform-domain learned from clean natural images. In this paper, we introduce a denoising algorithm comprising principal component dictionary (PCD)-based patch grouping and a low-rank approximation process. In the proposed algorithm, PCD learns from clean natural images and uses the knowledge gained to guide similar patches grouping results in noisy images. Patch grouping is directly implemented on PCD-based transform-domain. And, external knowledge and internal NSS prior are used jointly for image denoising. The results of experiments conducted indicate that the proposed denoising algorithm outperforms several state-of-the-art denoising algorithms, especially in heavy noise conditions.","Nonlocal self-similarity, Transform-domain, External knowledge, Image denoising",Shoukui Yao and Yi Chang and Xiaojuan Qin and Yaozong Zhang and Tianxu Zhang,https://www.sciencedirect.com/science/article/pii/S1047320317302298,https://doi.org/10.1016/j.jvcir.2017.11.019,1047-3203,2018,111--122,50,Journal of Visual Communication and Image Representation,Principal component dictionary-based patch grouping for image denoising,article,YAO2018111
"Saliency detection is a popular topic in computer vision, especially propagation-based method. This paper proposes a novel and effective coarse-to-fine saliency detection framework. In the coarse map stage, color spatial distribution map based on hue cue and central compactness rule is proposed, and integrated with texture boundary contrast information and background information to construct multi-prior coarse saliency map. In the refining stage, saliency values are updated under the comprehensive guidance of local structure propagation which is a novel algorithm to preserve local structural integrity during saliency propagation. With the global and local information, the detection procedure enhances the correctness of salient object gradually. Demonstrated in the extensive experiments on the public benchmark datasets, the performance of the proposed framework is superior to the state-of-the-art methods.","Saliency detection, Coarse-to-fine, Local structure propagation, Color distribution map, Global and local information, Multi-prior",Ming Zhang and Yu Pang and Yunhe Wu and Yue Du and Hui Sun and Ke Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318300129,https://doi.org/10.1016/j.jvcir.2018.01.004,1047-3203,2018,131--142,52,Journal of Visual Communication and Image Representation,Saliency detection via local structure propagation,article,ZHANG2018131
"Outdoor images are often degraded by haze, resulting in a distinctive gray or bluish hue which diminishes visibility. Of the existing haze removal methods, the ones that are effective are computationally complex and memory intensive. In this paper, we propose a simple haze removal technique, whose computational complexity is that of a simple convolution. To this purpose, a center surround filter is employed to improve speed and memory requirements of the transmission estimation in image dehazing. This can be useful for real time applications such as driver assistance, runway hazard detection and surveillance. The proposed technique relies on deriving an alternative transmission estimate by filtering the input image in three different color spaces, namely RGB, Lab and HSV. The effectiveness of the proposed method is compared with that of other state of the art methods using a subjective quality assessment method and a number of objective quality assessment methods.","Atmospheric light, Color space, Dark channel prior, Dehazing, No reference quality assessment, Transmission map",Deepa Nair and Praveen Sankaran,https://www.sciencedirect.com/science/article/pii/S1047320317302055,https://doi.org/10.1016/j.jvcir.2017.11.005,1047-3203,2018,9--15,50,Journal of Visual Communication and Image Representation,Color image dehazing using surround filter and dark channel prior,article,NAIR20189
"Proper recognition of complex-shaped handwritten compound characters is still a big challenge for Bangla OCR systems. In this paper, we propose a novel shape decomposition-based segmentation technique to decompose the compound characters into prominent shape components. This shape decomposition reduces the classification complexity in terms of less number of classes to recognize, and at the same time improves the recognition accuracy. The decomposition is done at the segmentation area where the two basic shapes are joined to form a compound character. We use chain code histogram feature set with multi-layer perceptron (MLP) based classifier with backpropagation learning for classification. On experimentation, the proposed method is observed to provide good recognition accuracy comparing with other existing methods.","Bangla OCR, Chain code histogram, Handwritten compound character, MLP, Segmentation, Shape decomposition",Rahul Pramanik and Soumen Bag,https://www.sciencedirect.com/science/article/pii/S1047320317302250,https://doi.org/10.1016/j.jvcir.2017.11.016,1047-3203,2018,123--134,50,Journal of Visual Communication and Image Representation,Shape decomposition-based handwritten compound character recognition for Bangla OCR,article,PRAMANIK2018123
"The correlation filter based trackers have drawn much attention due to their encouraging performance on precision, robustness and speed. In this paper, we introduce the spatial regularization component into the ridge regression model used by classical kernelized correlation filter (KCF) to improve its performance. It overcomes the fact that the traditional KCF does not consider the prior spatial constraint of the feature distribution of the target. We found that, after adding the spatial regularized function, we can solve the ridge regression formula efficiently with the property of circulant matrices. In this way, we can simultaneously keep the realtime and improve the tracking performance. Finally, we evaluate the proposed SRKCF tracker on the OTB-2013 and OTB-2015 comparing with 36 trackers and our tracker achieves state-of-art. Comparing with the SRDCF which applies the spatial regularized function, our algorithm achieves comparable performance with the obvious advantages in speed.","Visual tracking, Correlation filter, Spatial regularization, Kernel method",Long Gao and Yunsong Li and Jifeng Ning,https://www.sciencedirect.com/science/article/pii/S1047320317302171,https://doi.org/10.1016/j.jvcir.2017.11.008,1047-3203,2018,74--82,50,Journal of Visual Communication and Image Representation,Improved kernelized correlation filter tracking by using spatial regularization,article,GAO201874
"In this work, we propose a technique that utilizes a fully convolutional network (FCN) to localize image splicing attacks. We first evaluated a single-task FCN (SFCN) trained only on the surface label. Although the SFCN is shown to provide superior performance over existing methods, it still provides a coarse localization output in certain cases. Therefore, we propose the use of a multi-task FCN (MFCN) that utilizes two output branches for multi-task learning. One branch is used to learn the surface label, while the other branch is used to learn the edge or boundary of the spliced region. We trained the networks using the CASIA v2.0 dataset, and tested the trained models on the CASIA v1.0, Columbia Uncompressed, Carvalho, and the DARPA/NIST Nimble Challenge 2016 SCI datasets. Experiments show that the SFCN and MFCN outperform existing splicing localization algorithms, and that the MFCN can achieve finer localization than the SFCN.","Image splicing, Image forensics, Convolutional neural network (CNN), Fully convolutional network (FCN), Multi-task network",Ronald Salloum and Yuzhuo Ren and C.-C. {Jay Kuo},https://www.sciencedirect.com/science/article/pii/S1047320318300178,https://doi.org/10.1016/j.jvcir.2018.01.010,1047-3203,2018,201--209,51,Journal of Visual Communication and Image Representation,Image Splicing Localization using a Multi-task Fully Convolutional Network (MFCN),article,SALLOUM2018201
"Aiming to counterstrike face spoofing attacks such as photo attacks and video attacks, a face spoofing detection scheme based on color texture Markov feature (CTMF) and support vector machine recursive feature elimination (SVM-RFE) is proposed. In this paper, the adjacent facial pixels discrepancy between the real and the fake face is analyzed, and texture information between the color channels is fully considered. Firstly, the directional difference filter is used to capture the facial texture difference between the real and the fake face, which can be regarded as low-level features of CTMF. Then, the facial texture difference is modeled by the Markov process to form a high-level representation of the low-level features. Meanwhile, the mutual information of facial texture between the color channels, which is ignored in the previous literature, is investigated. In addition, SVM-RFE is utilized to reduce the feature dimension and makes it suitable for real-time detection. Experiments on four public benchmark databases indicate that the proposed scheme can effectively resist photo and video spoofing attacks in face recognition.","Face anti-spoofing, Color texture Markov feature, Adjacent facial pixels discrepancy, SVM-RFE",Le-Bing Zhang and Fei Peng and Le Qin and Min Long,https://www.sciencedirect.com/science/article/pii/S1047320318300014,https://doi.org/10.1016/j.jvcir.2018.01.001,1047-3203,2018,56--69,51,Journal of Visual Communication and Image Representation,Face spoofing detection based on color texture Markov feature and support vector machine recursive feature elimination,article,ZHANG201856
"Convolutional neural networks (CNN) have been successfully applied to visible image super-resolution (SR) methods. In this study, we propose a CNN-based SR algorithm for up-scaling near-infrared (NIR) images under low-light conditions, using corresponding visible images. Our algorithm first extracts high-frequency (HF) components from the up-scaled low-resolution (LR) NIR image and its corresponding high-resolution (HR) visible image, and then takes them as multiple inputs of the CNN. Next, the CNN outputs the HR HF component of the input NIR image. Finally, an HR NIR image is synthesized by adding the HR HF component to the up-scaled LR NIR image. The simulation results show that the proposed algorithm outperforms the state-of-the-art methods, in terms of both qualitative and quantitative aspects.","Near-infrared and visible images, Super-resolution, Convolutional neural networks, Low-light images",Tae Young Han and Dae Ha Kim and Seung Hyun Lee and Byung Cheol Song,https://www.sciencedirect.com/science/article/pii/S1047320318300257,https://doi.org/10.1016/j.jvcir.2018.01.018,1047-3203,2018,191--200,51,Journal of Visual Communication and Image Representation,Infrared image super-resolution using auxiliary convolutional neural network and visible image under low-light conditions,article,HAN2018191
"Fast intra mode decision strategies are proposed to overcome the brute force mode decision for the coding unit (CU) in High Efficiency Video Coding (HEVC). The proposed work improves the rough-mode-decision (RMD) by initializing the candidate intra mode list using the fusion of the Hadamard-cost and the statistical-inference formed using spatial/ temporal correlations. Then an early termination is predicted using optimal stopping theory that addresses early decision for a generic class of decision problems. Subsequently, a novel RD-cost prediction model is developed for early termination that is based on the RD-cost variation in the neighboring CUs with-respect-to their co-located CUs. Experimental results demonstrate that the RMD module of HEVC and the state-of-the-art fast intra mode prediction published method are outperformed by saving up to 0.61% and 0.91% BjÃ¸ntegaard delta bit rate (BDBR) on average, respectively.","Early mode decision, HEVC, Intra mode, RMD, RDOQ, Sum of absolute difference, Video coding, Video coding",Junaid Tariq and Sam Kwong,https://www.sciencedirect.com/science/article/pii/S1047320317302407,https://doi.org/10.1016/j.jvcir.2017.12.008,1047-3203,2018,1--13,51,Journal of Visual Communication and Image Representation,Adaptive stopping strategies for fast intra mode decision in HEVC,article,TARIQ20181
"Retinal images are frequently corrupted by unwanted variations in the brightness that occur due to over-all imperfections in the image acquisition process. This inhomogeneous illumination across the retina can limit the pathological information that can be gained from the image; and can lead to serious difficulties when performing image processing tasks that requires qualitative as well as quantitative analysis of feature presence on the image. On that perspective we have proposed a novel two-step approach for non-uniform and/or poor illumination correction in the context of retinal imaging. A subjective experiment was conducted to ensure that the proposed method did not create visually noticeable false color or artifacts on the images, especially on the areas that did not suffer non-uniform/poor illumination prior to correction. An objective experiment on 25,872 retinal images was performed to justify the significance of the proposed method for automated pathology detection/classification.","Color fundus image, Color correction, Illumination correction, Automated analysis of retinal image",Sajib Saha and Alexander Fletcher and Di Xiao and Yogesan Kanagasingam,https://www.sciencedirect.com/science/article/pii/S1047320318300130,https://doi.org/10.1016/j.jvcir.2018.01.005,1047-3203,2018,95--103,51,Journal of Visual Communication and Image Representation,A novel method for automated correction of non-uniform/poor illumination of retinal images without creating false artifacts,article,SAHA201895
"Being motivated by the multilayer RECOS (REctified-COrrelations on a Sphere) transform, we develop a data-driven Saak (Subspace approximation with augmented kernels) transform in this work. The Saak transform consists of three steps: (1) building the optimal linear subspace approximation with orthonormal bases using the second-order statistics of input vectors, (2) augmenting each transform kernel with its negative, (3) applying the rectified linear unit (ReLU) to the transform output. The Karhunen-LoÃ©ve transform (KLT) is used in the first step. The integration of Steps 2 and 3 is powerful since they resolve the sign confusion problem, remove the rectification loss and allow a straightforward implementation of the inverse Saak transform at the same time. Multiple Saak transforms are cascaded to transform images of a larger size. All Saak transform kernels are derived from the second-order statistics of input random vectors in a one-pass feedforward manner. Neither data labels nor backpropagation is used in kernel determination. Multi-stage Saak transforms offer a family of joint spatial-spectral representations between two extremes; namely, the full spatial-domain representation and the full spectral-domain representation. We select Saak coefficients of higher discriminant power to form a feature vector for pattern recognition, and use the MNIST dataset classification problem as an illustrative example.","Data-driven transform, RECOS transform, Saak transform, The Karhunen-LoÃ©ve transform (KLT), Linear subspace approximation, Principal component analysis",C.-C. {Jay Kuo} and Yueru Chen,https://www.sciencedirect.com/science/article/pii/S1047320317302328,https://doi.org/10.1016/j.jvcir.2017.11.023,1047-3203,2018,237--246,50,Journal of Visual Communication and Image Representation,On data-driven Saak transform,article,JAYKUO2018237
"This paper proposes a fragile and blind dual watermarking method for tamper detection and self-recovery. This method generates two image digests from the host image, based on the lifting wavelet and the halftoning technique. Therefore, for each 2Ã2 non-overlapping blocks, two chances for recovering tampered blocks is provided. Then, the authentication bit is obtained by using the image digests. Totally, eight bits are embedded in two LSBs for each block of image. To enhance the quality of the digest, a new LSBRounding technique is proposed. Additionally, to determine the mapping blocks and shuffling LSBs, the Arnold Cat Map is utilized. To improve the recovery rate, a Shift-aside operation is proposed. For preventing copy-move, vector-quantization attacks, and any manipulation in LSBs, the information embedded in each block depends on the key which is assigned to it. Experimental results show the efficiency of TRLH compared to the state of the art methods.","Data hiding, Watermarking, Tamper detection and self-recovery, Image authentication, Lifting wavelet transform, Halftoning technique",Behrouz {Bolourian Haghighi} and Amir Hossein Taherinia and Ahad Harati,https://www.sciencedirect.com/science/article/pii/S1047320317301876,https://doi.org/10.1016/j.jvcir.2017.09.017,1047-3203,2018,49--64,50,Journal of Visual Communication and Image Representation,TRLH: Fragile and blind dual watermarking for image tamper detection and self-recovery based on lifting wavelet transform and halftoning technique,article,BOLOURIANHAGHIGHI201849
"Semantic gap is an important challenging problem in content-based image retrieval (CBIR) up to now. Bag-of-words (BOW) framework is a popular approach that tries to reduce the semantic gap in CBIR. In this paper, an approach integrating visual saliency model with BOW is proposed for semantic image retrieval. Images are firstly segmented into background regions and foreground objects by a visual saliency-based segmentation method. And then multi-features including Scale Invariant Feature Transform (SIFT) features packed in BOW are extracted from regions and objects respectively and fused considering different characteristics of background regions and foreground objects. Finally, a fusion of z-score normalized Chi-Square distance is adopted as the similarity measurement. This proposal has been implemented on two widely used benchmark databases and the results evaluated in terms of mean Average Precision (mAP) show that our proposal outperforms the referred state-of-the-art approaches.","Semantic gap, Bag of words, Visual saliency, Semantic image retrieval",Cong Bai and Jia-nan Chen and Ling Huang and Kidiyo Kpalma and Shengyong Chen,https://www.sciencedirect.com/science/article/pii/S1047320317302304,https://doi.org/10.1016/j.jvcir.2017.11.021,1047-3203,2018,199--204,50,Journal of Visual Communication and Image Representation,Saliency-based multi-feature modeling for semantic image retrieval,article,BAI2018199
"Intensity restoration of pixels corrupted by impulse-noise contributes greatly to the quality of decision based filters (DBF). In this paper, we present an efficient structural post-processing method, which is based on directional-correlation, linear-regression-analysis, and inverse-distance-weighted-mean techniques. The proposed method is adopted as a complementary part after DBFs to enhance the quality of the final restored image. We assume that by adopting the preliminary DBF, noisy-pixels are detected by noise-detection unit and afterwards their intensities are estimated by the noise-restoration unit. In our method for each detected noisy-pixel, the intensity variation of adjacent pixels of restored image on different directions are analyzed in the corresponding local window and based on this structural information, the intensity of the previously-restored noisy-pixel is modified more accurately. Since the structures in images are more recognizable for low-density impulse-noise, our method is more effective in this case however a gradual improvement is achieved for high-density impulse-noise.","Impulse-noise, Image denoising, Image restoration, Decision based filters, Edge and detail preserving",Payam Sanaee and Payman Moallem and Farbod Razzazi,https://www.sciencedirect.com/science/article/pii/S1047320317302468,https://doi.org/10.1016/j.jvcir.2017.12.014,1047-3203,2018,40--55,51,Journal of Visual Communication and Image Representation,A structural post-processing method for enhancing intensity restoration of low-density impulse-noise for decision based filters,article,SANAEE201840
"Patch-based low-rank minimization for image processing attracts much attention in recent years. The minimization of the matrix rank coupled with the Frobenius norm data fidelity can be solved by the hard thresholding filter with principle component analysis (PCA) or singular value decomposition (SVD). Based on this idea, we propose a patch-based low-rank minimization method for image denoising. The main denoising process is stated in three equivalent way: PCA, SVD and low-rank minimization. Compared to recent patch-based sparse representation methods, experiments demonstrate that the proposed method is rather rapid, and it is effective for a variety of natural grayscale images and color images, especially for texture parts in images. Further improvements of this method are also given. In addition, due to the simplicity of this method, we could provide an explanation of the choice of the threshold parameter, estimation of PSNR values, and give other insights into this method.","Image denoising, Patch-based method, Low-rank minimization, Principal component analysis, Singular value decomposition, Hard thresholding",Haijuan Hu and Jacques Froment and Quansheng Liu,https://www.sciencedirect.com/science/article/pii/S1047320317302225,https://doi.org/10.1016/j.jvcir.2017.11.013,1047-3203,2018,100--110,50,Journal of Visual Communication and Image Representation,A note on patch-based low-rank minimization for fast image denoising,article,HU2018100
"A novel saliency detection method via spectral graph (SG) weighted low rank matrix recovery (LR) is presented in this paper. The location, color, and boundary priors are exploited in many LR-based saliency detection methods. However, these priors do not work well when the salient objects are far away from image center, especially when the background is complicated and has low contrast with objects. Because spectral graph contains rich image contrast, it is used as an efficient weight to obtain a much reasonable high-level prior in the proposed LR-based saliency model. Compared with previous LR-based methods, low rank matrix and sparse matrix rather than only sparse matrix are used to calculate the final saliency by an integration function and an activation function. The numerical and visual results on four challenging salient object datasets show that our method performs competitively for salient object detection task against some recent state-of-the-art algorithms.","Saliency detection, Spectral graph, Low rank matrix recovery, Sparse decomposition, Feature matrix",Jiazhong Chen and Jie Chen and Hefei Ling and Hua Cao and Weiping Sun and Yebin Fan and Weimin Wu,https://www.sciencedirect.com/science/article/pii/S1047320317302389,https://doi.org/10.1016/j.jvcir.2017.12.006,1047-3203,2018,270--279,50,Journal of Visual Communication and Image Representation,Salient object detection via spectral graph weighted low rank matrix recovery,article,CHEN2018270
"Sketch face recognition (SFR) has been widely and successfully applied in law enforcement, which attracts a growing number of researchers. In this paper, a face sketch-photo synthesis and recognition method is proposed. Our method has two parts: Firstly, according to the different synthesis results for different scales, a cascade sketch-photo synthesis method via dual-scale Markov Network is utilized for image synthesis; Secondly, structural information and feature information-based data fusion method has been presented for face recognition. It is inspired by the Face Recognition Cognitive Theory, which applies both structural information and feature information for recognition. The experimental results on different databases based on the proposed method, demonstrate the outperformance of our method compared with state-of-the-art methods both in synthesis and recognition processes.","Sketch face synthesis and recognition, Dual-scale Markov Network, Structural information, Feature information, Fusion",Zhenxue Chen and Saisai Yao and Yunyi Jia and Chengyun Liu,https://www.sciencedirect.com/science/article/pii/S1047320317302420,https://doi.org/10.1016/j.jvcir.2017.12.010,1047-3203,2018,112--121,51,Journal of Visual Communication and Image Representation,Face sketch-photo synthesis and recognition: Dual-scale Markov Network and multi-information fusion,article,CHEN2018112
"This paper presents a new stereo image (SI) retrieval method based on a statistical model of complex wavelet coefficients subbands. In this context, a Gaussian copula-based multivariate model is used to capture the dependence between complex wavelet coefficients of both left and right images, and a non-Gaussian univariate model is used to characterize the statistical behavior of the disparity map. Thanks to its flexibility, the copula tool allows us to choose several marginal densities while keeping the multivariate properties. Features are extracted by estimating parameters for both multivariate and univariate models. Finally, a weighted Jeffrey divergence (JD) is used as a similarity measurement between the underlying models. Experimental results on a stereo image database demonstrate the performance of the proposed method in terms of the retrieval rates as well as the computational time.","Content-based stereo image retrieval, Gaussian copula, Non-Gaussian distribution, Jeffrey divergence, Complex wavelet transform, Feature extraction",Ayoub Karine and Ahmed Drissi {El Maliani} and Mohammed {El Hassouni},https://www.sciencedirect.com/science/article/pii/S1047320317302067,https://doi.org/10.1016/j.jvcir.2017.11.006,1047-3203,2018,27--39,50,Journal of Visual Communication and Image Representation,A novel statistical model for content-based stereo image retrieval in the complex wavelet domain,article,KARINE201827
"This paper proposes a saliency integration approach via the use of similar images to elevate saliency detection performance. Given the input image, a group of similar images are first retrieved, and meanwhile, the corresponding multiple saliency maps of the input image are generated by using existing saliency models. Then, the saliency fusion map is generated by using an adaptive fusion method to integrate such saliency maps, for which the fusion weights are measured by the corresponding similarity between each similar image and the input image. Next, an inter-image graph, for each pair of input image and similar image, is constructed to propagate the confident saliency values from the similar image to the input image, yielding the saliency propagation map. Finally, the saliency fusion map and the saliency propagation map are integrated to obtain the final saliency map. Experimental results on two public datasets demonstrate that the proposed approach achieves the better saliency detection performance compared to the existing saliency models and other saliency integration approaches.","Saliency integration, Saliency propagation, Similar image, Saliency model",Jingru Ren and Zhi Liu and Xiaofei Zhou and Guangling Sun and Cong Bai,https://www.sciencedirect.com/science/article/pii/S1047320317302341,https://doi.org/10.1016/j.jvcir.2017.12.002,1047-3203,2018,227--236,50,Journal of Visual Communication and Image Representation,Saliency integration driven by similar images,article,REN2018227
"This paper presents a fast blind image sharpness/blurriness assessment model (BISHARP) which operates in spatial and transform domain. The proposed model generates local contrast image maps by computing the root-mean-squared values for each image pixel within a defined size of local neighborhood. The resulting local contrast maps are then transformed into the wavelet domain where the reduction of high frequency content is evaluated in the presence of varying blur strengths. It was found that percentile values computed from sorted, level-shifted, high-frequency wavelet coefficients can serve as reliable image sharpness/blurriness estimators. Furthermore, it was found that higher dynamic range of contrast maps significantly improves model performance. The results of validation performed on seven image databases showed a very high correlation with perceptual scores. Due to low computational requirements the proposed model can be easily utilized in real-world image processing applications.","No-reference, Image quality assessment, Contrast, Percentile, Dynamic range, Wavelet",Goran Gvozden and Sonja Grgic and Mislav Grgic,https://www.sciencedirect.com/science/article/pii/S1047320317302262,https://doi.org/10.1016/j.jvcir.2017.11.017,1047-3203,2018,145--158,50,Journal of Visual Communication and Image Representation,Blind image sharpness assessment based on local contrast map statistics,article,GVOZDEN2018145
"In recent years, image sharpening detection has become one of the main topics in the field of image forensics. It is, however, still a challenge to detect the images sharpened with weak sharpening strength. To address this challenge, we propose an efficient method for image sharpening detection. In the proposed method, a ternary coding strategy with adaptive threshold is introduced to reveal the overshoot artifacts caused by weak sharpening. Extensive experiments are conducted to illustrate the superiority of the proposed method. The experimental results show that the proposed method can achieve a considerable improvement in sharpening detection, especially for slightly sharpened images.","Image forensics, Sharpening detection, Overshoot artifact, Edge perpendicular binary coding, Ternary coding",Feng Ding and Guopu Zhu and Weiqiang Dong and Yun-Qing Shi,https://www.sciencedirect.com/science/article/pii/S1047320317302183,https://doi.org/10.1016/j.jvcir.2017.11.009,1047-3203,2018,93--99,50,Journal of Visual Communication and Image Representation,An efficient weak sharpening detection method for image forensics,article,DING201893
"Visual tracking is a fundamental component for high-level video understanding problems such as motion analysis, event detection and action recognition. Recently, Discriminative Correlation Filters (DCF) have achieved enormous popularity in the tracking community due to high computational efficiency and fair robustness. However, the underlying boundary effect of DCF leads to a very restricted target search region at the detection step. Generally, a larger search area is adopted to overcome this disadvantage. Such an expansion of search area usually includes substantial amount of background information which will contaminate the tracking model in realist tracking scenarios. To alleviate this major drawback, we propose a generic DCF tracking framework which suppresses background information and highlights the foreground object with an object likelihood map computed from the color histograms. This object likelihood map is merged with the cosine window and then integrated into the DCF formulation. Therefore, DCF are less burdened in the training step by focusing more on pixels with higher object likelihood probability. Extensive experiments on the OTB50 and OTB100 benchmarks demonstrate that our adaptively windowed tracking framework can be combined with many DCF trackers and achieves significant performance improvement.","Correlation filter, Target likelihood, Window adaptation",Yangliu Kuai and Gongjian Wen and Dongdong Li,https://www.sciencedirect.com/science/article/pii/S1047320318300142,https://doi.org/10.1016/j.jvcir.2018.01.008,1047-3203,2018,104--111,51,Journal of Visual Communication and Image Representation,Learning adaptively windowed correlation filters for robust tracking,article,KUAI2018104
"The task of spatiotemporal saliency detection is to distinguish the salient objects from background across all the frames in the video. Although many spatiotemporal models have been designed from various aspects, it is still a very challenging task for handing the unconstrained videos with complicated motions and complex scenes. Therefore, in this paper we propose a novel spatiotemporal saliency model to estimate salient objects in unconstrained videos. Specifically, a bagging-based saliency prediction model, i.e. an ensembling regressor, which is the combination of random forest regressors learned from undersampled training sets, is first used to perform saliency prediction for each current frame. Then, both forward and backward propagation within a local temporal window are deployed on each current frame to make a complement to the predicted saliency map and yield the temporal saliency map, in which the backward propagation is constructed based on the temporary saliency estimation of the following frames. Finally, by building the appearance and motion based graphs in a parallel way, spatial propagation is employed over the temporal saliency map to generate the final spatiotemporal saliency map. Through experiments on two challenging datasets, the proposed model consistently outperforms the state-of-the-art models for popping out salient objects in unconstrained videos.","Spatiotemporal saliency, Unconstrained video, Bagging, Prediction, Propagation",Xiaofei Zhou and Zhi Liu and Kai Li and Guangling Sun,https://www.sciencedirect.com/science/article/pii/S104732031830021X,https://doi.org/10.1016/j.jvcir.2018.01.014,1047-3203,2018,131--143,51,Journal of Visual Communication and Image Representation,Video saliency detection via bagging-based prediction and spatiotemporal propagation,article,ZHOU2018131
"Crowd saliency prediction refers to predicting where people look at in crowd scene. Humans have remarkable ability to rapidly direct their gaze to select visual information of interest when looking at a visual scene. Until now, research efforts are still focused on what type of feature is representative for crowd saliency, and which type of learning model is robust for crowd saliency prediction. In this paper, we propose a Random Forest (RF) based crowd saliency prediction approach with optimal feature combination, i.e., the Feature Combination Selection for Crowd Saliency (FCSCS) framework. More specifically, we first define three representative crowd saliency features, namely, FaceSizeDiff, FacePoseDiff and FaceWhrDiff. Next, we adopt the Random Forest (RF) algorithm to construct our saliency learning model. Then, we evaluate the performance of FCSCS framework with different feature combinations (fifteen combinations in our experiments). Those selected features include low-level features (i.e., color, intensity, orientation), four crowd features (i.e., face size, face density, frontal face, profile face) and three new defined features (i.e., FaceSizeDiff, FacePoseDiff and FaceWhrDiff). We use FCSCS framework to obtain the optimal feature combination that is most suitable for crowd saliency prediction and further train the saliency model based on the optimal feature combination. After that, we evaluate the performance of the crowd saliency prediction classifiers. Finally, we conduct extensive experiments and empirical evaluation to demonstrate the satisfactory performance of our approach.","Crowd, Saliency, Random forest, Visual attention, Face detection",Guangyu Gao and Cen Han and Kun Ma and Chi Harold Liu and Gangyi Ding and Erwu Liu,https://www.sciencedirect.com/science/article/pii/S104732031730202X,https://doi.org/10.1016/j.jvcir.2017.11.002,1047-3203,2018,1--8,50,Journal of Visual Communication and Image Representation,Optimal feature combination analysis for crowd saliency prediction,article,GAO20181
"Multimodal medical image fusion has become a powerful tool in clinical applications. The main aim is to fuse different multimodal medical images, obtained from different imaging modalities, into a single fused image that is extensively used by the physicians for explicit diagnosis and treatment of diseases. In this paper, an improved multimodal medical image fusion algorithm based on fuzzy transform (FTR) is proposed. The core idea behind the proposed algorithm is to improve the performance of multimodal medical image fusion algorithm by taking into consideration the error images obtained using FTR pair. Subjective as well as objective evaluations demonstrate that the fusion quality in terms of edge strength, standard deviation, feature mutual information, fusion factor, feature similarity and structural similarity has significantly improved in the proposed algorithm as compared to other state-of-art multimodal medical image fusion algorithms.","Multimodal medical image fusion, Fuzzy transform, Fusion performance measures",Meenu Manchanda and Rajiv Sharma,https://www.sciencedirect.com/science/article/pii/S1047320317302432,https://doi.org/10.1016/j.jvcir.2017.12.011,1047-3203,2018,76--94,51,Journal of Visual Communication and Image Representation,An improved multimodal medical image fusion algorithm based on fuzzy transform,article,MANCHANDA201876
"Temporal video segmentation is useful to exploit and organize long egocentric videos. Previous work has focused on general purpose methods designed to deal with data acquired by different users. In contrast, egocentric video tends to be very personal and meaningful for the specific user who acquires it. We propose a method to segment egocentric video according to the personal locations visited by the user. The method aims at providing a personalized output and allows the user to specify which locations he wants to keep track of. To account for negative locations (i.e., locations not specified by the user), we propose a negative rejection method which does not require any negative sample at training time. For the experiments, we collected a dataset of egocentric videos in 10 different personal locations, plus various negative ones. Results show that the method is accurate and compares favorably with the state of the art.","Egocentric Vision, Lifelogging, Personal locations, Temporal segmentation",Antonino Furnari and Sebastiano Battiato and Giovanni Maria Farinella,https://www.sciencedirect.com/science/article/pii/S1047320318300269,https://doi.org/10.1016/j.jvcir.2018.01.019,1047-3203,2018,1--12,52,Journal of Visual Communication and Image Representation,Personal-location-based temporal segmentation of egocentric videos for lifelogging applications,article,FURNARI20181
"Traditional multi-exposure based high dynamic range fringe projection profilometry (FPP) technique is an effective method to obtain the 3D profiles of objects with drastic surface reflectivity variations. However, in this technique different exposure times often need to be selected empirically, making this method rather complicated. In this paper a completely automatic multi-exposure based FPP technique is proposed. No human intervention is required while applying the proposed method, which greatly simplify the whole reconstruction process. It is mathematically proved that once a pixelâs modulation is larger than a threshold, the phase quality of this pixel can be considered satisfactory. This threshold can be used to guide the calculation of the needed exposure times. The software then automatically adjusts the cameraâs exposure time and captures the needed fringe images. Experiments show that with these captured images, the final reconstruction with a high dynamic range can be readily obtained.","3D measurement, Fringe projection, Random noises, High dynamic range, Multiple exposures",Li Rao and Feipeng Da,https://www.sciencedirect.com/science/article/pii/S1047320317302353,https://doi.org/10.1016/j.jvcir.2017.12.003,1047-3203,2018,217--226,50,Journal of Visual Communication and Image Representation,High dynamic range 3D shape determination based on automatic exposure selection,article,RAO2018217
"The 3D extension of High Efficiency Video Coding (3D-HEVC) is a new international video coding standard developed by the Joint Collaborative Team on 3D Video Coding Extensions (JCT-3V) in order to support coding of multiple views and its associated depth data. 3D-HEVC aims at improving the coding efficiency of 3D and multi-view videos by introducing new coding tools to utilize the correlations between views and between texture and depth components. In this paper, an inter-view motion prediction (inter-view merge candidate) and an inter-component motion prediction (texture merge candidate) are proposed to explore the inter-view and the inter-component redundancies for texture and depth components, respectively. Moreover, a new coding mode termed as single depth mode which simply reconstructs a coding block with a single depth value based on block merging scheme under the HEVC quad-tree based block partitioning is also introduced. All the proposed schemes are adopted in 3D-HEVC. The experimental results evaluated under the common test conditions (CTC) for developing 3D-HEVC show that the proposed inter-view merge candidate, texture merge candidate, and single depth mode achieve significant BD-rate reductions of 19.5% for dependent texture views and 8.3% for the synthesized texture views.","3D-HEVC, Inter-view motion prediction, Disparity derivation, Depth coding",Jian-Liang Lin and Yi-Wen Chen and Yu-Lin Chang and Jicheng An and Kai Zhang and Yu-Wen Huang and Shawmin Lei,https://www.sciencedirect.com/science/article/pii/S1047320317302031,https://doi.org/10.1016/j.jvcir.2017.11.003,1047-3203,2018,83--92,50,Journal of Visual Communication and Image Representation,Advanced texture and depth coding in 3D-HEVC,article,LIN201883
"Cubic-spline interpolation (CSI) scheme is known to be designed to resample the discrete image data based on the least-square method in conjunction with the cubic convolution interpolation (CCI) function. It is superior in performance and can be used together with the discrete cosine transform (DCT)-based image or video codec to improve the coding performance for a variety of high compression ratios. In this paper, we firstly make some comments on the direct computation algorithm for CSI scheme developed by Lin et al. Moreover, a low-complexity direct computation algorithm for CSI scheme is developed to further improve the computational efficiency. The mathematical derivations and simulation results indicate that such simplified CSI scheme using the proposed low-complexity direct computation algorithm can achieve almost the same objective and subjective performance with much fewer arithmetic operations in comparison with the CSI scheme using the direct computation algorithm.","Cubic-spline interpolation, Direct computation algorithm, Fast Fourier transform, Low-complexity direct computation algorithm",Shao-Hua Hong and Lin Wang and Trieu-Kien Truong,https://www.sciencedirect.com/science/article/pii/S1047320317302195,https://doi.org/10.1016/j.jvcir.2017.11.010,1047-3203,2018,159--166,50,Journal of Visual Communication and Image Representation,Low-complexity direct computation algorithm for cubic-spline interpolation scheme,article,HONG2018159
"Motivated by the observation that most methods for accelerating the generalized Lloyd algorithm (GLA) normally lack the capability to improve the quality of its end result and that most methods for improving the quality of the end result of GLA usually lack the capability to speed it up, an efficient and effective method is presented in this paper to enhance the performance of GLA and its variants, in terms of both the computation time and the quality of the end result, by leveraging the strengths of several reduction methods and the multiple stage mechanism. Simulation results show that the proposed method can efficiently and effectively reduce the computation time of GLA by up to about 93% while at the same time improving its quality by up to about 1â¯dB in terms of the peak-signal-to-noise-ratio in most cases.","Generalized Lloyd algorithm, Reduction method, Multiple stage vector quantization",Kai-Cheng Hu and Chun-Wei Tsai and Ming-Chao Chiang,https://www.sciencedirect.com/science/article/pii/S1047320317302390,https://doi.org/10.1016/j.jvcir.2017.12.007,1047-3203,2018,290--302,50,Journal of Visual Communication and Image Representation,A highly efficient method for improving the performance of GLA-based algorithms,article,HU2018290
"Existing median filtering detection methods are no longer effective for small size or highly compressed images. To deal with this problem, a new median filtering detection method based on CNN is proposed in this paper. Specifically, a new network structure called MFNet is constructed. First, for preprocessing, the nearest neighbor interpolation method is utilized to up-sample the small-size images. The property of median filtering can be well preserved by the up-sampling operation and enlarged difference between the original image and its median filtered version can be obtained. Then, the well-known mlpconv structure is employed in the first and second layers of MFNet. With mlpconv layers, the nonlinear classification ability of the proposed method can be enhanced. After that, three conventional convolutional layers are utilized to finally derive the feature maps. The experimental results show that the proposed method achieves significant improved detection performance. Moreover, the proposed method performs well for highly compressed image of size as small as 16â¯Ãâ¯16.","Median filtering forensics, CNN, Nearest neighbor interpolation",Hongshen Tang and Rongrong Ni and Yao Zhao and Xiaolong Li,https://www.sciencedirect.com/science/article/pii/S104732031830018X,https://doi.org/10.1016/j.jvcir.2018.01.011,1047-3203,2018,162--168,51,Journal of Visual Communication and Image Representation,Median filtering detection of small-size image based on CNN,article,TANG2018162
"In this paper, we propose an efficient and straightforward approach, video you only look once (VideoYOLO), to capture the overall temporal dynamics from an entire video in a single process for action recognition. It remains an open question for action recognition on how to deal with the temporal dimension in videos. Existing methods subdivide a whole video into either individual frames or short clips and consequently have to process these fractions multiple times. A post process is then used to aggregate the partial dynamic cues to implicitly infer the whole temporal information. On the contrary, in VideoYOLO, we first generate a proxy video by selecting a subset of frames to roughly reserve the overall temporal dynamics presented in the original video. A 3D convolutional neural network (3D-CNN) is employed to learn the overall temporal characteristics from the proxy video and predict action category in a single process. Our proposed method is extremely fast. VideoYOLO-32 is able to process 36 videos per second that is 10 times and 7 times faster than prior 2D-CNN (Two-stream (Simonyan and Zisserman, 2014)) and 3D-CNN (C3D (Tran et al., 2015)) based models, respectively, while still achieves superior or comparable classification accuracies on the benchmark datasets, UCF101 and HMDB51.","Video understanding, Video classification, Action recognition, Convolutional neural network",Longlong Jing and Xiaodong Yang and Yingli Tian,https://www.sciencedirect.com/science/article/pii/S1047320318300233,https://doi.org/10.1016/j.jvcir.2018.01.016,1047-3203,2018,58--65,52,Journal of Visual Communication and Image Representation,Video you only look once: Overall temporal convolutions for action recognition,article,JING201858
"We describe a framework for lifelogging monitoring in the scope of dementia care, based on activity recognition from egocentric vision and semantic context-enrichment. As pure vision-based approaches appear to be already saturating in terms of recognition accuracy, we propose their enhancement with wearable bracelet accelerometer information. For that purpose, we design and study appropriate early and late fusion schemes to increase accuracy. The incorporation of mechanical variables, such as jerk, improves the recognition accuracy of activities that require fine motion. In addition, we describe a framework for semantic activity representation and interpretation, using Semantic Web technologies for building interoperable activity graphs. The system is personalized, as deployment-specific activity models are authored, while problems related to the disease are detected by rules. Complemented by lifelogging applications, the system is able to support interventions by clinicians, and endorse a feeling of safety and inclusion for end-users and their carers.","Instrumental activity recognition, Egocentric camera, Mechanical measurements, Visual cues, Ontologies, Semantic knowledge graphs",Georgios Meditskos and Pierre-Marie Plans and Thanos G. Stavropoulos and Jenny Benois-Pineau and Vincent Buso and Ioannis Kompatsiaris,https://www.sciencedirect.com/science/article/pii/S1047320318300154,https://doi.org/10.1016/j.jvcir.2018.01.009,1047-3203,2018,169--190,51,Journal of Visual Communication and Image Representation,"Multi-modal activity recognition from egocentric vision, semantic enrichment and lifelogging applications for the care of dementia",article,MEDITSKOS2018169
"A no reference stereo video quality assessment method based on motion features extracted in tensor decomposition domain is proposed. Tensor decomposition is used to reduce dimension of color, view and time of stereo video, and motion information maps containing time-varying information of inter-views and intra-views are obtained. Statistical features such as generalized Gaussian distribution (GGD), asymmetric GGD, spatial entropy, spectral entropy associated with two views, and spectral entropy related to depth perception of stereo video, are extracted. Random forest is utilized to establish relationship between stereo video quality and the extracted features. Experimental results on NAMA3DS1-COSPAD1 database demonstrate that the proposed method achieves good performance on JP2K, resolution reduction, sharpening and their combination distortions, Pearson linear correlation coefficient (PLCC) values of these types of distortions are higher than 0.97, while for H.264 distortion the PLCC value is 0.8850, which means that the proposed metric is consistent with human visual perception.","No reference stereo video quality assessment, Tensor decomposition, Motion feature, Entropy, Random forest",Gangyi Jiang and Shanshan Liu and Mei Yu and Feng Shao and Zongju Peng and Fen Chen,https://www.sciencedirect.com/science/article/pii/S104732031730233X,https://doi.org/10.1016/j.jvcir.2017.12.001,1047-3203,2018,247--262,50,Journal of Visual Communication and Image Representation,No reference stereo video quality assessment based on motion feature in tensor decomposition domain,article,JIANG2018247
"In order to solve the problem of Thangka image inpainting quality assessment (IIQA) and existing quality evaluation methods are not suitable for inpainting Thangka image, this paper proposes a new non-reference quality evaluation method which can effectively solve this problem. Firstly, due to lack of original Thangka image, the proposed method using symmetry of Thangka images to predicted an undamaged image. Secondly, we extract harries corner in Thangka inpainting images to show the structural feature of total graph. Thirdly, demonstrate subjective evaluation score of inpainting Thangka image by caparison difference of structural feature between inpainting and predicted original area. Finally, in order to compensate the lack of Thangka images in existing database, we add Generative Adversarial Nets (GANs) to generate large number of available image. Experiments show that our proposed method generates a state-of-the-art index for Thangka image inpainting quality which correlated with human vision.","Image inpainting quality assessment, Structural symmetry, Thangka image, Harris corner",Wenjin Hu and Yuqi Ye and Fuliang Zeng and Jiahao Meng,https://www.sciencedirect.com/science/article/pii/S1047320318303705,https://doi.org/10.1016/j.jvcir.2018.12.045,1047-3203,2019,292--299,59,Journal of Visual Communication and Image Representation,A new method of Thangka image inpainting quality assessment,article,HU2019292
"Deep learning based human activity recognition approach combines spatial and temporal information to complete the recognition task. The temporal information is extracted by optical flow, which is always compensated by the warping method in order to achieve better performance. However, these methods usually take the global feature as the starting point, only consider global information of video frames, and ignore local information that reflects the changes of human behavior, causing the algorithm to be sensitive to the external environment such as occlusion, illumination change. In view of the above problems, this paper fuses the local spatial features of video frames, global spatial features and temporal features to recognize different actions, and further extracts the visual attention weight to make constraint on the global spatial features. Experiments show that the algorithm proposed in this paper has better accuracy compared with the existing methods.","Activity recognition, Multimodal, Visual attention",Suguo Zhu and Zhenying Fang and Yi Wang and Jun Yu and Junping Du,https://www.sciencedirect.com/science/article/pii/S1047320318303547,https://doi.org/10.1016/j.jvcir.2018.12.026,1047-3203,2019,38--43,60,Journal of Visual Communication and Image Representation,Multimodal activity recognition with local block CNN and attention-based spatial weighted CNN,article,ZHU201938
"The advancement in video surveillance has raised significant concerns about privacy protection. The existing methods focus on identifying the sensitive region and preserving the behavior of the target, however, they ignore the recoverability of private content. In this paper, we propose a novel and efficient privacy protection scheme for data security in video surveillance, which jointly addresses several key challenges, including de-identification, behavior preservation, recoverability, and compressibility in one unified system. Our method constructs a public stream and a private residual error stream by blurring the private sensitive region. With our scheme, ordinary users could recognize the behaviors in the public identity-protected video stream for surveillance purpose, while authorized users are able to access the recovered private content (e.g., for law investigations). Moreover, the compressed privacy protected region and residual error could be able to save the costs associated with transmission and storage. The extensive experiments on two standard surveillance datasets and a user study demonstrate the effectiveness of our privacy protection system.","Privacy protection, Data security, Video surveillance",Ling Du and Wei Zhang and Huazhu Fu and Wenqi Ren and Xinpeng Zhang,https://www.sciencedirect.com/science/article/pii/S1047320319300367,https://doi.org/10.1016/j.jvcir.2019.01.027,1047-3203,2019,347--362,59,Journal of Visual Communication and Image Representation,An efficient privacy protection scheme for data security in video surveillance,article,DU2019347
"To protect the privacy of users, tables generally must be anonymized before publication. All existing anonymous methods have deficiencies. They do not consider the differences in attributes, or the optimization of information loss and time efficiency. his paper proposes a new method called KACM to realize k-anonymity. This method is mainly used for hybrid tables. The calculation of the distance between records considers the connection between quasi-identifier attributes and sensitive attributes, their effect on the sensitive privacy, and the information loss during the anonymity process. In the clustering process, the records with the minimum distance are always selected to add, and the clustering is individually controlled according to k to realize the equalization division of the equivalence class and reduce the total amount of distance calculation. Finally, the validity and practicability of the method are proved using theory and experiment.","Clustering algorithm, Analytic hierarchy process",Kun Wang and Wei Zhao and Junjie Cui and Yanpeng Cui and Jianwei Hu,https://www.sciencedirect.com/science/article/pii/S1047320318303791,https://doi.org/10.1016/j.jvcir.2018.12.052,1047-3203,2019,76--83,59,Journal of Visual Communication and Image Representation,A K-anonymous clustering algorithm based on the analytic hierarchy process,article,WANG201976
"To date, surveillance based person search has focused on locating a person of interest from an image query, distinct from the law enforcement task of locating a person from a description. In this paper, we introduce a novel probabilistic framework that combines multiple traits whilst incorporating their uncertainty to tackle the emerging challenge: locating a person from a semantic query. In addressing this, we improve clothing texture recognition by leveraging Dempster-Shafer theory against an ensemble of support vector machines; achieving state-of-the-art performance for high and low resolution clothing textures. Our proposed person search framework combines information from clothing texture and colour in the torso and leg regions to produce a probabilistic match between unknown subjects and the designated target query. Results are presented on a newly created 520 subject surveillance dataset which is made available to researchers. This multi-modal person search technique achieves promising results for locating target subjects, without the requirement of pre-search target enrollment.","Soft biometrics, Dempster-Shafer theory, Surveillance, Semantic person search",Michael A. Halstead and Simon Denman and Sridha Sridharan and YingLi Tian and Clinton Fookes,https://www.sciencedirect.com/science/article/pii/S1047320318303274,https://doi.org/10.1016/j.jvcir.2018.12.001,1047-3203,2019,439--452,58,Journal of Visual Communication and Image Representation,Multimodal clothing recognition for semantic search in unconstrained surveillance imagery,article,HALSTEAD2019439
"This paper takes into account both unlabeled data and their local neighbors to learn their sparse representations, and proposes a face recognition approach named Weighted Locality Collaborative Representation Classifier based on sparse subspace (WLCRC). WLCRC firstly learns a subset of the original training data to build a much correlated dictionary, and then combines linear regression techniques together with weighted collaborative representation techniques to optimize the linear reconstruction of unlabeled data. It uses the newly built dictionary to learn the reconstruction coefficients for each unlabeled datum while considering the influence of its local neighbors. Classifications are performed according to the reconstruction residuals, and experimental results on benchmark datasets demonstrate that WLCRC is effective.","Collaborative representation, Sparse subspace, Linear representation, Face recognition",Xiao Dong and Huaxiang Zhang and Lei Zhu and Wenbo Wan and Zhenhua Wang and Qiang Wang and Peilian Guo and Hui Ji and Jiande Sun,https://www.sciencedirect.com/science/article/pii/S1047320318303031,https://doi.org/10.1016/j.jvcir.2018.11.030,1047-3203,2019,187--194,58,Journal of Visual Communication and Image Representation,Weighted locality collaborative representation based on sparse subspace,article,DONG2019187
"In the continuous development of Internet technology and geological information technology, problems such as illegal possession, copying, modification and dissemination of unauthorized digital products have occurred in the field of geological information technology. In the field of geological information technology, not only traditional digital products such as text, image, audio and video are produced, but also 3D model digital products unique to geological information technology, which are geological bodies, geological phenomena, geological structures, geological processes and geology. Regular 3D visualization analysis and the foundation and platform for comprehensive decision making of 3D visualization, the 3D model of geological body plays an increasingly important role in the field of geological information technology. Of course, it also faces the problems faced by traditional digital products and copyright protection. In this paper, the advantages and disadvantages of digital watermarking technology in copyright protection and the characteristics of geological body 3D model itself, and the current mature 3D model digital watermarking algorithm are introduced. The research idea of 3D geological body digital watermarking algorithm based on point cloud model is proposed. According to the spatial characteristics of the point cloud model of geological body, the spatial characteristic variables of the point cloud model are obtained by using the spatial characteristic analysis algorithm. The spatial domain information represented by the spatial characteristic variables is transformed into the frequency domain information, and then the frequency domain information is analyzed by means of mathematical statistics to extract the digital watermarking information of the whole geological body point cloud model. The feasibility of the algorithm is analyzed and verified by the experimental analysis. The digital watermark information before and after the attack is extracted by geometric attacks such as affine transformation and shearing. The extracted digital watermark information is correlated with the coefficient analysis and the robustness of the algorithm is obtained. A better robustness effect can effectively protect the copyright of the owner of the 3D model of the geological body.","Digital watermarking, Geological body, Point cloud model, Copyright protection",Xialin Zhang and Zaiquan Shen,https://www.sciencedirect.com/science/article/pii/S1047320318303390,https://doi.org/10.1016/j.jvcir.2018.12.013,1047-3203,2019,334--346,59,Journal of Visual Communication and Image Representation,Copyright protection method for 3D model of geological body based on digital watermarking technology,article,ZHANG2019334
"The conventional methods of urban impervious surfaces extraction mainly use the shallow-layer machine learning algorithms based on the medium- or low-resolution remote sensing images, and always provide low accuracy and poor automation level because the potential of multi-source remote sensing data are not fully utilized and the low-level features are not effectively organized. In order to address this problem, a novel method (AEIDLMRS) is proposed to automatically extract impervious surfaces based on deep learning and multi-source remote sensing data. First, the multi-source remote sensing data consisting of LIDAR points cloud data, Landsat8 images and PlÃ©iades-1A images are pre-processed, re-sampled and registered, and then the combined features of spectral, elevation and intensity from the multi-source data are denoised using the minimum noise fraction (MNF) method to generate some representative MNF features. A small number of reliable labelled samples are automatically extracted using the fuzzy C-means (FCM) clustering method based on the MNF features. Secondly, the convolutional neural network (CNN) is used to extract the representative features of the neighborhood windows of each pixel in the fused PlÃ©iades-1A image through multi-layer convolution and pooling operations. Finally, the combined features of MNF features and CNN features are pre-learned via the deep belief network (DBN). The DBN parameters are globally optimized jointly using the Extreme Learning Machine (ELM) classifier on the top level and the small set of labelled samples extracted via FCM, and the urban impervious surfaces are distinguished from others based on the trained ELM classifier and morphological operations. Experiments are performed to compare the proposed method with other three related methods in three different experimental regions respectively. Experimental results demonstrate that AEIDLMRS has better accuracy and automation level than the others under relatively good efficiency, and it is more suitable for the extraction of complex urban impervious surfaces.","Multi-source remote sensing data, Deep learning, Extraction of urban impervious surface, ELM classifier, Fuzzy C means clustering",Fenghua Huang and Ying Yu and Tinghao Feng,https://www.sciencedirect.com/science/article/pii/S104732031830378X,https://doi.org/10.1016/j.jvcir.2018.12.051,1047-3203,2019,16--27,60,Journal of Visual Communication and Image Representation,Automatic extraction of urban impervious surfaces based on deep learning and multi-source remote sensing data,article,HUANG201916
"Conventional discriminant locality preserving projections (DLPP) is sensitive to outliers because the formulation of its objective function is based on L2-norm. Not only that, the learned features by DLPP are linear combinations of all the original features. So, it is hard to know which features play an important role in feature extraction, and the learned features often contain irrelevant information. In this paper, we propose a robust version of DLPP based on L2,p-norm with 0<p<1, termed DLPP-L2,p, for image feature extraction and recognition. DLPP-L2,p learns an optimal projection matrix by maximizing the L2,p-norm-based locality preserving between-class dispersion and minimizing the L2,p-norm-based locality preserving within-class dispersion simultaneously. Furthermore, by imposing an L2,p-norm penalty on the projection matrix to achieve row-sparsity, DLPP-L2,p can discard irrelevant features and transform relevant features simultaneously. Experimental results on several image datasets demonstrate the effectiveness and robustness of DLPP-L2,p.","Discriminant locality preserving projections, -norm, Subspace learning, Feature extraction, Image recognition",Haishun Du and Guodong Li and Sheng Wang and Fan Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318303158,https://doi.org/10.1016/j.jvcir.2018.11.037,1047-3203,2019,166--177,58,Journal of Visual Communication and Image Representation,"Discriminant locality preserving projections based on L2,p-norm for image feature extraction and recognition",article,DU2019166
"Current zero shot learning methods mostly focus on applying the knowledge learnt by seen images to the unseen images. However, there is a big distribution difference between seen and unseen data, also called source and target domain. Thus, there are many irrelevant seen samples for unseen samples. We want to partially transfer the seen samples to target domain by selecting relevant seen samples. In this paper, we propose a method, zero shot learning by partial transfer from source domain with L2,1 norm constraint, called ZSLPT which embeds visual similarity and semantic similarity to transfer partial source samples. The relevant source samples are selected, while the irrelevant are eliminated. Whatâs more, we train source classification model used for transferring to target domain with the selected source samples, making the transferred target model more accurate. We have experimented on the state-of-the-art zero shot learning datasets, demonstrating that ZSLPT has good performance.","Zero shot learning, Partial transfer, Visual similarity, Semantic similarity,  norm",Xiao Li and Min Fang and Dazheng Feng and Haikun Li and Jinqiao Wu,https://www.sciencedirect.com/science/article/pii/S1047320318303675,https://doi.org/10.1016/j.jvcir.2018.12.041,1047-3203,2019,701--711,58,Journal of Visual Communication and Image Representation,"Zero shot learning by partial transfer from source domain with L2,1 norm constraint",article,LI2019701
"This paper proposes a visibility forecast method based on hierarchical sparse representations. Firstly, it selects meteorological factors from the data of 138 ground stations located in Beijing, Tianjin and Hebei during the months (Oct.-to-Dec. and January) of years 2002â2016. Then, it uses fuzzy C means algorithm (FCM) to construct historical databases containing 5000 samples. Finally, it takes the meteorological factors corresponding to visibility as the sample of historical databases, and uses a hierarchical sparse representation to predict the visibility of new inputs. Experiment, conducted with the data of European Centre for Medium-Range Weather Forecasts (ECMWF), indicates a better performance of the hierarchical sparse representation in contrast to a sparse representation. And the visibility forecast based on hierarchical sparse representation is better than Beijing Regional Environmental Meteorology Prediction System (BREMPS) and BP neural network. The hierarchical sparse representation is simple and easy to expand, which improves the accuracy and reduce the absolute error, which is convenience for other meteorological analysis.","FCM, Sparse representation, Predict, Visibility",Zhenyu Lu and Bingjian Lu and Hengde Zhang and You Fu and Yunan Qiu and Tianming Zhan,https://www.sciencedirect.com/science/article/pii/S104732031830302X,https://doi.org/10.1016/j.jvcir.2018.11.029,1047-3203,2019,160--165,58,Journal of Visual Communication and Image Representation,A method of visibility forecast based on hierarchical sparse representation,article,LU2019160
"Researches have shown that holistic examination of an image provides better understanding of the image compared to separate processes each devoted to a single task like annotation, classification or segmentation. During the past decades, there have been several efforts for simultaneous image classification and annotation using probabilistic or neural network based topic models. Despite their relative success, most of these models suffer from the poor visual word representation and the imbalance between the number of visual and annotation words in the training data. This paper proposes a novel model for simultaneous image classification and annotation model based on SupDocNADE, a neural network based topic model for image classification and annotation. The proposed model, named wSupDocNADE, addresses the above shortcomings by using a new coding and introducing a weighting mechanism for the SupDocNADE model. In the coding step of the model, several patches extracted from the input image are first fed to a deep convolutional neural network and the feature vectors obtained from this network are coded using the LLC coding. These vectors are then aggregated in a final descriptor through sum pooling. To overcome the imbalance between the visual and annotation words, a weighting factor is considered for each visual or annotation word. The weights of the visual words are set based on their frequencies obtained from the pooling method and the weights of the annotation words are learned from the training data. The experimental results on three benchmark datasets show the superiority of the proposed model in both image classification and annotation tasks over state-of-the-art models.","Image classification and annotation, Topic models, Probabilistic model, Deep learning, Convolutional neural network, LLC",Seyed Navid Mohammadi Foumani and Ahmad Nickabadi,https://www.sciencedirect.com/science/article/pii/S1047320319300094,https://doi.org/10.1016/j.jvcir.2019.01.009,1047-3203,2019,195--203,59,Journal of Visual Communication and Image Representation,A probabilistic topic model using deep visual word representation for simultaneous image classification and annotation,article,FOUMANI2019195
"Financial paper is a note without reason debt or consideration acceptance, issued for obtaining money financing. Financial paper identification system is a hot issue of the current file analysis and identification system, it covers paper classification, image processing, character segmentation and identification, file image compression and other series of processes. A research on multiple aspects of financial paper identification system is made in this paper. On which basis, a financial paper identification system with applied value is established. Through substantive experimental test and practical application, the method has better classification performance and higher processing efficiency, and has been used in bank bill identification processing system in a large scale.","Finance bill, Mode identification, Image processing, Binary decision tree",Man-Wen Tian and Shu-Rong Yan and Xiao-Xiao Tian and Jing-Ai Liu,https://www.sciencedirect.com/science/article/pii/S1047320318303432,https://doi.org/10.1016/j.jvcir.2018.12.016,1047-3203,2019,123--128,60,Journal of Visual Communication and Image Representation,Research on image recognition method of bank financing bill based on binary tree decision,article,TIAN2019123
"With the development of information technology, image has become the mainstream of information transmission. Compared with character, image contains more information, but because image and character need more storage capacity, it will occupy more bandwidth in network transmission. In order to transmit image information more quickly, image compression is a good choice. This paper is based on an eye of image compression. The method of image compression in this paper is that firstly, the image is filtered by wavelet transform to remove the redundant information in the image, and then the Huffman method is used to encode the image. The simulation results of JPEG format image show that the size of the image can be reduced in the same image effect.","Image compression, Wavelet transform, Huffman coding, JPEG picture",Shuyun Yuan and Jianbo Hu,https://www.sciencedirect.com/science/article/pii/S1047320318303717,https://doi.org/10.1016/j.jvcir.2018.12.043,1047-3203,2019,33--38,59,Journal of Visual Communication and Image Representation,Research on image compression technology based on Huffman coding,article,YUAN201933
"Though blind image quality assessment (BIQA) is highly desired in perceptual-oriented image processing systems, it is extremely difficult to design a reliable BIQA method. With the help of the prior knowledge, the human visual system (HVS) hierarchically perceives the quality degradation during the visual recognition. Inspired by this, we suggest different levels of distortion generate individual degradations on hierarchical features, and propose to consider the degradations on both low and high level features for quality prediction. By mimicking the orientation selectivity (OS) mechanism in the primary visual cortex, an OS based local structure is designed for low-level visual information representation. At the meantime, the deep residual network, which possesses multiple levels for feature integration, is employed to extract the deep semantics for high-level visual content representation. By fusing the local structure and the deep semantics, a hierarchical feature set is acquired. Next, the correlations between the degradations of image qualities and their corresponding hierarchical feature sets are analyzed, and a novel hierarchical feature degradation (HFD) based BIQA (HFD-BIQA) method is built. Experimental results on the legacy and wild image quality assessment databases demonstrate the prediction accuracy of the proposed HFD-BIQA method, and verify that the HFD-BIQA performs highly consistent with the subjective perception.","Blind image quality assessment, Hierarchical feature degradation, Local structure, Deep semantics",Jinjian Wu and Jichen Zeng and Weisheng Dong and Guangming Shi and Weisi Lin,https://www.sciencedirect.com/science/article/pii/S1047320318303316,https://doi.org/10.1016/j.jvcir.2018.12.005,1047-3203,2019,353--362,58,Journal of Visual Communication and Image Representation,Blind image quality assessment with hierarchy: Degradation from local structure to deep semantics,article,WU2019353
"With the development of vision and optimization techniques, visual-inertial odometry (VIO) has shown the capability of motion estimating in the GNSS-denied condition. The VIO can provide absolute pitch and roll angles estimating value, but no the absolute azimuth. In the paper, we proposed a VIO aided by compass, which can obtain the azimuth with respect to the north direction in the geographic frame. Moreover, aided by compass, the yaw angle estimating error was reduced to a greater degree, due to the measurement of azimuth. Furthermore, the consistency of the VIO backend estimator is improved as well, while the accuracy of the estimated pose states was also wholly improved. The aiding approach is a tightly-couple information fusion system of camera, IMU and magnetoresistive sensors. The optimization method is based on the pre-integration and bundle adjustment. In the paper, we derived the compass residual model based on the pre-integration model, and then its Jacobian and covariance formation were deduced to solve the nonlinear equations. The compass aided VIO software was implemented based on the Nvidia Jetson Tx2. The system was fully tested based on hardware-in-the-loop simulation and vehicle test in the real physical environment. The pose errors of VIOs with and without compass aiding were compared in the above tests. The simulation results showed that the position was and yaw errors were improved obviously; the compass aided VIO was still consistent, but the pure VIO was consistent not. The consistency character is evaluated by average NEES by Monte-Carlo in simulation. The vehicle test showed that the position error was reduced by 23%; the yaw error was reduced by 21%. As a result, the compass aided VIO not only improved the pose estimated accuracy, especially position and yaw, but also improved the consistency of VIO system.","Visual-inertial odometry (VIO), Compass, Sliding window estimator, Inconsistency, Pre-integration, Minimum cost function",Yandong Wang and Tao Zhang and Yuanchao Wang and Jingwei Ma and Yanhui Li and Jingzhuang Han,https://www.sciencedirect.com/science/article/pii/S1047320318303559,https://doi.org/10.1016/j.jvcir.2018.12.029,1047-3203,2019,101--115,60,Journal of Visual Communication and Image Representation,Compass aided visual-inertial odometry,article,WANG2019101
"With the development of computer technology, the related achievements of image processing have been applied. Among them, the results of automatic target detection and recognition are widely used in the fields of reconnaissance, early warning and traffic control with the application of UAV. But now, the research of automatic target detection and tracking is becoming smaller and smaller. The original automatic target detection and recognition algorithm seems to be inadequate. The bottleneck of low-level feature design and optimization makes the accuracy and efficiency of automatic target detection inefficient. Therefore, based on in-depth learning, this paper establishes a method to automatically learn effective image features from images to achieve automatic target detection. Through the simulation of target detection in VEDAI database. The results show that the recognition rate of the proposed model is more than 95%. The results show that the proposed method can realize the automatic detection and recognition of targets very well.","Image processing, Target detection, Target recognition, In-depth learning",Jia Wang and Chen Liu and Tian Fu and Lili Zheng,https://www.sciencedirect.com/science/article/pii/S1047320319300240,https://doi.org/10.1016/j.jvcir.2019.01.017,1047-3203,2019,44--50,60,Journal of Visual Communication and Image Representation,Research on automatic target detection and recognition based on deep learning,article,WANG201944
"Wearable electrocardiogram (ECG) measurement systems have been widely used in patients with CVD (Cardiovascular Disease) which can be worn in daily lives. However, currently the main problem is motion artifact interference, and reducing motion artifacts (MA) is one of the most challenging problems encountered in the filtering and processing of physiological signals. In this paper, by analyzing the spectral energy changes during the input process of motion artifacts, a cosine transform LMS adaptive cancellation algorithm (DCT-LMS) implementation is proposed aiming to remove the motion artifacts from the ECG. In order to study the performance of the algorithm and effectively remove the motion artifacts in the ECG signal, this thesis collects ECG signals of people's daily activities from fabric-based chest straps with dry electrodes. It verifies the classic LMS adaptive elimination algorithm and the normalized one. Besides, two LMS adaptive cancellation algorithms based on sine and cosine transform are compared. The simulation and experimental results show that the cosine-based adaptive algorithm is superior to the classical LMS algorithm in eliminating high-amplitude motion artifact noise of ECG.","Adaptive cancellation algorithm, Motion artifacts, Auxiliary dry electrode, Cosine transform",Fan Xiong and Dongyi Chen and Zhenghao Chen and Shumei Dai,https://www.sciencedirect.com/science/article/pii/S1047320318303560,https://doi.org/10.1016/j.jvcir.2018.12.030,1047-3203,2019,606--618,58,Journal of Visual Communication and Image Representation,Cancellation of motion artifacts in ambulatory ECG signals using TD-LMS adaptive filtering techniques,article,XIONG2019606
"360-degree video is one of the key components of Virtual Reality (VR) applications. 360-degree videos viewed on Head Mounted Displays can offer impressive viewing experiences to users. Yet, streaming of 360-degree videos over the Internet is a very challenging task since it requires extremely high bandwidth. To reduce the bandwidth requirement while still providing good experiences, viewport adaptive streaming has been introduced. In this paper, we propose a client-based adaptation framework for viewport adaptive streaming of 360 videos, which can support different application scenarios. The key components and important issues are presented with a general problem formulation for tile version selection. Especially, we introduce for the first time the use of bitrate and quality estimation in viewport adaptive streaming of 360 videos. Experiments show that the proposed framework can significantly improve video quality for users. In addition, the impacts of buffering delay and projection formats in VR context are investigated.","Virtual reality, 360-degree videos, Framework, Adaptive streaming",D.V. Nguyen and Huyen T.T. Tran and Truong Cong Thang,https://www.sciencedirect.com/science/article/pii/S1047320319300124,https://doi.org/10.1016/j.jvcir.2019.01.012,1047-3203,2019,231--243,59,Journal of Visual Communication and Image Representation,A client-based adaptation framework for 360-degree video streaming,article,NGUYEN2019231
"This paper proposes a Tone Mapping (TM) approach converting a High Dynamic Range (HDR) image into a Low Dynamic Range (LDR) image while preserving as much information of the HDR image as possible to ensure a good LDR image visual quality. This approach is based on a separable near optimal lifting scheme using an adaptive powerful prediction step. The latter relies on a linear weighted combination depending on the neighboring coefficients extracting then the relevant finest details in the HDR image at each resolution level. Moreover the approximation and detail coefficients are modified according to the entropy of each subband. The pixelâs distribution of the coarse reconstructed LDR image is then adjusted according to a perceptual quantizer with respect to the human visual system using a piecewise linear function. Simulation results provide good results, both in terms of visual quality and TMQI metric, compared to existing competitive TM approaches.","High dynamic range, Separable multiresolution, Lifting scheme, Optimized prediction, Cell-average interpolation, Tone mapping operators",Ba Chien Thai and Anissa Mokraoui and Basarab Matei,https://www.sciencedirect.com/science/article/pii/S1047320318303444,https://doi.org/10.1016/j.jvcir.2018.12.024,1047-3203,2019,589--599,58,Journal of Visual Communication and Image Representation,Contrast enhancement and details preservation of tone mapped high dynamic range images,article,THAI2019589
"With the increasingly busy urban traffic and the development of modern communication technologies, traffic conditions need to be transmitted from major intersections to command and dispatch centers for analysis and processing, which raises a large number of problems of storing and transmitting static images of traffic conditions. Research on image compression of traffic conditions has also become a hot issue that people pay more and more attention to. In the process of traffic image research, due to the lack of essential attributes of the image, especially, the selection and use of compression methods has greater blindness. However, an overall analysis of the image prior to traffic image processing is a difficult task. This article selects the road traffic data, public transit data and orbital data first to compress the image. Then the streaming Media transmission System of DASH is introduced. In the specific application, the code of traffic data flow in this paper is converted into Real Media format through SDK. With the help of Helix Server, all traffic data flow files can be integrated into the synchronous Media integration language, based on the Internet of TCP/IP which is released in a stream through Real System. The experimental results show that the traffic conditions such as vehicle queuing, congestion and signal lights are directly mastered, the signal timing is timely adjusted or other means are adopted to ease the traffic, the distribution of traffic flow is changed, and ordinary terminal users are enabled to master the distribution of traffic flow through wireless network and choose the travel path actively.","Image compression, DASH, Streaming media, Wavelet transform, Noise-signal ratio",Ge Zhang and Jianlin Wang and Chaokun Yan and Sheng Wang,https://www.sciencedirect.com/science/article/pii/S1047320318303687,https://doi.org/10.1016/j.jvcir.2018.12.042,1047-3203,2019,168--175,59,Journal of Visual Communication and Image Representation,Application research of image compression and wireless network traffic video streaming,article,ZHANG2019168
"Object memorability prediction is a task of estimating the probability that a human recognises the recurrence of an object after a single view. Initial research on object memorability showed that it is possible to predict the object memorability scores from the intrinsic features of an object. Though the existing works proposed some of the features for object memorability prediction task, the influence of Spatial-location and Spatial-size of an object to its memorability have not been explored yet. In this work, the importance of these two characteristics in determining object memorability prediction is investigated and the same is demonstrated by building a baseline model. Further, a deep learning model is devised for automatic feature learning on these two object characteristics. Experimental results highlight that the Spatial-location and Spatial-size of an object play a significant role in object memorability prediction and the proposed models outperformed the existing methods.","Object Memorability, Deep Learning, Transfer Learning",Sathisha Basavaraju and Sibaji Gaj and Arijit Sur,https://www.sciencedirect.com/science/article/pii/S1047320319300082,https://doi.org/10.1016/j.jvcir.2019.01.008,1047-3203,2019,117--127,59,Journal of Visual Communication and Image Representation,Object Memorability Prediction using Deep Learning: Location and Size Bias,article,BASAVARAJU2019117
"In this paper we propose a novel approach for detecting and tracking objects in videos captured by moving cameras without any additional sensor. In such a video both the background and foreground change in each frame of the image sequence; making the separation of actual moving object from the background a challenging task. In this work, moving objects are detected as clusters of spatio-temporal blobs generated by spatio-temporal analysis of the image sequence using a three-dimensional Gabor filter and merged using Minimum Spanning Tree. Problem of data association during tracking is solved by Linear Assignment Problem and occlusion is handled by the application of Kalman filter. The major advantage of the proposed method is that, it does not require initialization or training on sample data to perform. Our algorithm demonstrated very satisfactory state-of-the-art result on benchmark videos. The performance of the algorithm is equivalent or superior to some benchmark algorithms.","Variable background, Object detection, Gabor filter, Spatio-temporal analysis, Minimum Spanning Tree (MST), Object tracking, Linear Assignment Problem (LAP), Kalman filter, Occlusion",Kumar S. Ray and Soma Chakraborty,https://www.sciencedirect.com/science/article/pii/S1047320318303286,https://doi.org/10.1016/j.jvcir.2018.12.002,1047-3203,2019,662--674,58,Journal of Visual Communication and Image Representation,Object detection by spatio-temporal analysis and tracking of the detected objects in a video with variable background,article,RAY2019662
"Due to the complexity of urban surface, the differences in impervious surface materials, the mutual interference between the spectra of ground objects and the huge impact of ground object shadows in high-resolution remote sensing (HRRS) images, it is improper to directly use shallow machine learning algorithms and conventional object-oriented segmentation methods to extract urban impervious surfaces from HRRS images. Therefore, a method for automatic extraction of impervious surfaces from HRRS images based on deep learning (AEISHIDL) is proposed to address this problem. Firstly, the original HRRS images are pre-processed and the Gram-Schmidt algorithm is employed for the fusion of panchromatic and multi-spectral bands in HRRS images. In addition, an enhanced bilateral filtering method considering edge characteristics (EBFCEC) is designed and adopted to remove noises and enhance edges of man-made objects in original HRRS images. Secondly, the EBFCEC filtered images are partitioned into multi-layer object sets by using improved marker watershed based on LAB color space (IMWLCS), and the related objects in different sets are re-segmented to have the same edges through edge integration, after which we extract spectral feature averages and shape feature values of all objects while the convolutional neural network (CNN) is used to calculate the CNN feature averages of all pixel neighborhoods in each object. Finally, the fuzzy c-means clustering (FCM) algorithm is employed jointly considering the spectrum, shape and CNN features of the segmented objects in HRRS images to judge whether the objects belong to impervious surfaces, thereby effectively increasing the accuracy of automatically extracting impervious surfaces. Two different experimental regions are selected from two different types of HRRS images (WorldView 2 and PlÃ©iades-1A) respectively (4 experimental regions in all). The experimental results show that AEISHIDL has higher accuracy and automation level compared with other four representative methods in urban impervious surfaces extraction from HRRS images.","High-resolution remote sensing images, Impervious surfaces extraction, Bilateral filtering, Convolutional neural network, Improved watershed algorithm",Fenghua Huang and Ying Yu and Tinghao Feng,https://www.sciencedirect.com/science/article/pii/S1047320318303201,https://doi.org/10.1016/j.jvcir.2018.11.041,1047-3203,2019,453--461,58,Journal of Visual Communication and Image Representation,Automatic extraction of impervious surfaces from high resolution remote sensing images based on deep learning,article,HUANG2019453
"In this paper, we propose a no-reference (NR) quality assessment method for stereoscopic images by deep convolutional neural network (DCNN). Inspired by the internal generative mechanism (IGM) in the human brain, which shows that the brain first analyzes the perceptual information and then extract effective visual information. Meanwhile, in order to simulate the inner interaction process in the human visual system (HVS) when perceiving the visual quality of stereoscopic images, we construct a two-channel DCNN to evaluate the visual quality of stereoscopic images. First, we design a Siamese Network to extract high-level semantic features of left- and right-view images for simulating the process of information extraction in the brain. Second, to imitate the information interaction process in the HVS, we combine the high-level features of left- and right-view images by convolutional operations. Finally, the information after interactive processing is used to estimate the visual quality of stereoscopic image. Experimental results show that the proposed method can estimate the visual quality of stereoscopic images accurately, which also demonstrate the effectiveness of the proposed two-channel convolutional neural network in simulating the perception mechanism in the HVS.","Image quality assessment, Stereoscopic images, No reference, Convolutional neural network",Yuming Fang and Jiebin Yan and Xuelin Liu and Jiheng Wang,https://www.sciencedirect.com/science/article/pii/S1047320318303328,https://doi.org/10.1016/j.jvcir.2018.12.006,1047-3203,2019,400--406,58,Journal of Visual Communication and Image Representation,Stereoscopic image quality assessment by deep convolutional neural network,article,FANG2019400
"Along with the development of mobile Internet and social network, peopleâs lifestyles are also changing, and many social websites (e.g. Facebook, YouTube, and WeChat) have sprung up, which leads to the emergence of a large number of multimedia data (e.g. text, image, and video) of various events. The goal of this paper is to mine event topics efficiently from massive and unordered social media data, which is beneficial to search, browse and monitor significant social events for users or governments. In order to achieve this goal, this paper proposes a novel Knowledge-Based Multi-Modal Weighted Topic Model (KBMMWTM) for social event analysis. The proposed KBMMWTM has following advantages: (1) The proposed KBMMWTM can effectively take advantage of the multi-modality of social events jointly. (2) The proposed KBMMWTM exploits word correlation in dataset as prior knowledge to improve the performance of event topic mining. We evaluate our KBMMWTM model on a real dataset and full experiments show that our model outperforms state-of-the-art models.","Social media, Multi-modal, Topic mining, Knowledge-based",Feng Xue and Jian Sun and Xueliang Liu and Tianpeng Liu and Qiang Lu,https://www.sciencedirect.com/science/article/pii/S1047320318303602,https://doi.org/10.1016/j.jvcir.2018.12.033,1047-3203,2019,1--8,59,Journal of Visual Communication and Image Representation,Social multi-modal event analysis via knowledge-based weighted topic model,article,XUE20191
"With the wide application of big data technology and the continuous innovation of the financial industry, the development of Internet finance has become irreversible. The new financial model has improved the efficiency of financial services and formed new ways of payment and transaction. The traditional credit information system has been unable to meet the needs of the development of the financial industry. The research methods adopted in this paper mainly include literature research, case analysis and comparative analysis. The development process, current situation and characteristics of China's personal credit system, as well as the development process and current situation of China's personal credit system under Internet finance are analyzed. It also points out the main problems faced by China's personal credit reporting system under the Internet finance. Then, aiming at the specific problems faced by China's personal credit reporting system under Internet finance, this paper makes case studies of representative market-oriented credit reporting institutions such as Beijing, Tianjin and Hebei, and uses convolutional neural network to analyze the specific practices of these individual credit reporting institutions and achieves results. At the same time, it compares how different credit agencies make up for the shortcomings of personal credit reporting system. Finally, it draws the conclusion of perfecting the personal credit system of our country under the Internet finance, points out the new problems that may be brought by the superposition of various solutions, and puts forward the countermeasures and suggestions of perfecting the personal credit system of our country under the Internet finance.","SVM, Information asymmetry, Convolution neural network, Decision tree, Credit investigation",Cheng-yong Liu and Ling-Jan Chiou and Cheng-chung Li and Xiu-Wen Ye,https://www.sciencedirect.com/science/article/pii/S1047320319300227,https://doi.org/10.1016/j.jvcir.2019.01.018,1047-3203,2019,300--308,59,Journal of Visual Communication and Image Representation,Analysis of Beijing Tianjin Hebei regional credit system from the perspective of big data credit reporting,article,LIU2019300
"Nowadays, more and more places need authentication. Face recognition is a mature technology for identity verification research. Recognition accuracy is an important indicator for evaluating authentication algorithms. In order to improve the accuracy of identity verification, advanced face is used. Feature recognition algorithm is an effective way, but it is also an effective algorithm to study the factors affecting facial features. Therefore, many researchers study the recognition results based on the poses of the face, light and other factors. This paper is also a study on the factors affecting face recognition, mainly by studying the influence of the age and gender factors on the identity verification results, and using the deep learning method to classify facial features. The simulation results show that the average recognition rate reaches 83.73%. At the same time, this paper analyzes the effect of age and gender on the classification results. The results show that the recognition effect of middle-aged men in male subjects is lower than that of youth and the elderly. Women have little difference in recognition effect with age. Males have higher recognition rates than women.","Face recognition, Deep learning, Gender, Age",Shifeng wu and Dahu Wang,https://www.sciencedirect.com/science/article/pii/S1047320319300197,https://doi.org/10.1016/j.jvcir.2019.01.013,1047-3203,2019,116--122,60,Journal of Visual Communication and Image Representation,Effect of subject's age and gender on face recognition results,article,WU2019116
"Trademark images are usually used to distinguish goods due to their uniqueness, and the amount becomes too huge to search these images accurately and fast. Most existing methods utilize conventional dense features to search visually-similar images, however, the performance and search time are not satisfactory. In this paper, we propose a unified deep hashing framework to learn the binary codes for trademark images, resulting in good performance with less search time. The unified framework integrates two types of deep convolutional networks (i.e., spatial transformer network and recurrent convolutional network) for obtaining transformation-invariant features. These features are discriminative for describing trademark images and robust to different types of transformations. The two-stream networks are followed by the hashing layer. Network parameters are learned by minimizing a sample-weighted loss, which can leverage the hard-searched images. We conduct experiments on two benchmark image sets, i.e., NPU-TM and METU, and verify the effectiveness and efficiency of our proposed approach over state-of-the-art.","Trademark image retrieval, Deep hashing, Transformation-invariant feature, Spatial transformer network, Recurrent convolutional network, Sample-weighted loss",Zhaoqiang Xia and Jie Lin and Xiaoyi Feng,https://www.sciencedirect.com/science/article/pii/S1047320319300112,https://doi.org/10.1016/j.jvcir.2019.01.011,1047-3203,2019,108--116,59,Journal of Visual Communication and Image Representation,Trademark image retrieval via transformation-invariant deep hashing,article,XIA2019108
"Depth difference, as a popularly used feature for characterizing pairwise pixels of range images, fails to precisely capture skeleton joints when human body possesses a wild and complicated articulation. As the geodesic distance of pairwise pixels is able to present a global connected property and adjacent pixels often belong to the same body component, we propose an effective and efficient framework for pose estimation from range images. Firstly, all the pixels of a range image are grouped into superpixels using an improved Simple Linear Iterative Clustering algorithm. Secondly, those superpixels are labelled as the components of a human body using the hybrid feature. Thirdly, componentwise cluster feature extraction is undertaken on skeleton joints of body components with K-means clustering algorithm. Finally, the feature points of each component are then stacked as a compact representation of human poses and mapped to the skeleton joints of a human body. Experimental results demonstrate that the proposed framework outperforms several state-of-the-art pose estimation methods.","Human pose estimation, Superpixel, Geodesic distance, Random decision forest, Sparse representation",Wenhui Zhang and Dehui Kong and Shaofan Wang and Zhiyong Wang,https://www.sciencedirect.com/science/article/pii/S1047320319300355,https://doi.org/10.1016/j.jvcir.2019.01.028,1047-3203,2019,272--282,59,Journal of Visual Communication and Image Representation,3D human pose estimation from range images with depth difference and geodesic distance,article,ZHANG2019272
"No-reference (NR) image quality assessment (IQA) aims to evaluate the quality of an image without reference image, which is greatly desired in the automatic visual signal processing system. Distortions degrade the visual contents and affect the semantics acquisition during the process of human perception. Although the existing methods evaluate the quality of images based on the structure, texture, or statistical characteristics, and deliver high quality prediction accuracy, they do not take the spatial semantics into account. From the perspective of human perception, distortions decrease the structural semantics that represent the structural information, and disturb the spatial semantics that describe the contents of images. Therefore, we attempt to measure the image quality by its degradation of semantics in an image. To extract the semantics of an image, a semantic network is proposed. The network contains convolutional neural networks (CNN) and Long Short-Term Memory (LSTM) that correspond to structural semantics and spatial semantics, respectively. CNN can be regarded as a coarse imitation of human visual mechanism to obtain the structural information, and LSTM can express the contents of an image. Then, by measuring the degradations of different semantics on images, a novel NR IQA is introduced. The proposed approach is evaluated on the databases of LIVE, CSIQ, TID2013, and LIVE multiply distorted database as well as LIVE in the wild image quality challenge database, and the results show superior performance to other state-of-the-art NR IQA methods. Furthermore, we explore the generalization capability of the proposed approach, and the experimental results indicate the proposed approach has a high robustness.","No-reference image quality assessment, Human perception, Semantic network, Structural semantics, Spatial semantics",Weiping Ji and Jinjian Wu and Guangming Shi and Wenfei Wan and Xuemei Xie,https://www.sciencedirect.com/science/article/pii/S1047320318303171,https://doi.org/10.1016/j.jvcir.2018.11.038,1047-3203,2019,195--204,58,Journal of Visual Communication and Image Representation,Blind image quality assessment with semantic information,article,JI2019195
"In view of the problem of laser dizzy interference of medium-wave infrared seeker detector in the field of photoelectric confrontation, there is a lack of equidistant distance test to verify the problem. In this paper, a medium-wave infrared seeker for short-range precision guided missile CCD detector was used as the subject, and the 3.8â¯Î¼m wavelength medium wave laser was used as the laser source to study the calibration of the laser. In the experiment, two locations with a viewing distance of 14.5â¯km were selected to place the laser and missile seeker detectors respectively. The atmospheric transmittance was estimated using the MODTRAN software based on the meteorological parameters provided by the simple meteorological monitoring tool. Under the different target power density conditions, the simulation of the missile seeker head of the interference of the dizzy effect. By analyzing the interference images in the tracking state, the corresponding qualitative and quantitative dizziness interference results are obtained. After the calculation and analysis, it is considered that the laser energy output from the mid-wave infrared laser is 0.08â¯mW/m2 when the power density of the target surface of the detector is 0.08â¯mW/m2, respectively, under medium meteorological conditions with a visibility of 0.625. Box cannot lock the original target, there cannot effectively extract the target dizzy effect. The experimental results show that the anti-jamming ability of the anti-jamming capability of the medium-wave infrared precision guidance probe is verified by the photoelectric countermeasure, and it is considered that the dodge power density is smaller when the interference laser wavelength matches the wavelength of the seeker detector. The experimental data can provide theoretical guidance for the inversion and determination of parameters in the study of the medium wave infrared photoelectric countermeasure.","Image processing, Laser dizziness interference, Medium wave infrared detector, Photoelectric interference",Zhaobing Chen and Kui Shi and Ning Chen and Long Shi and Xinyu Zhuang and Jiaqi Zhou and Yushuai Zhang and Hongqi Wang and Xingyang Liu and Guannan Li,https://www.sciencedirect.com/science/article/pii/S1047320318303699,https://doi.org/10.1016/j.jvcir.2018.12.044,1047-3203,2019,401--406,59,Journal of Visual Communication and Image Representation,The experimental study about laser-induced dizziness effect of medium-wave infrared seeker which based on image processing,article,CHEN2019401
"Information security is one of the most challenging issues. Cryptography and Steganography techniques are two popular methods for protecting data privacy. In this study, an information hiding method with dynamic embedding capacity based on vector quantization is proposed for protecting confidential data. To improve embedding capacity and image quality at the same time, dynamic-length secret bits are embedded into each pixel. Compared with previous approaches, the proposed method preforms better regarding the embedding capacity and image quality.","Vector quantization, Dynamic embedding algorithm, Steganography",Cheng-Ta Huang and Li-Chiun Lin and Cheng-Hsing Yang and Shiuh-Jeng Wang,https://www.sciencedirect.com/science/article/pii/S1047320318303456,https://doi.org/10.1016/j.jvcir.2018.12.018,1047-3203,2019,14--32,59,Journal of Visual Communication and Image Representation,Dynamic embedding strategy of VQ-based information hiding approach,article,HUANG201914
"In this paper, we propose a Multimodal Deep Belief Network (MDBN) for learning a generative model of 2D and 3D skeletal data. The MDBM learns the cross-modal relationship between these data in the form of a joint probability distribution over the space of multimodal inputs. It can regenerate any missing modality by sampling from the conditional distribution over the given data modality. The skeletal sequences are converted into the motion images which help us utilize the impressive power of the generative deep networks in the image processing. Furthermore, we use the variation of information (VI) as the training criterion, instead of the conventional maximum likelihood. It is proven that VI is efficient in cross-modal learning where some data modalities are missing. Our experimental results have shown that the model has an outstanding performance on the over-complete MHAD and CMU Mocap datasets in data-driven motion regeneration on a full-body 2D and 3D skeleton structures.","Multimodal deep learning, Multimodal Deep Belief Network, 2D/3D recovery, Cross-modal motion regeneration, Visual correspondence",Muhamad Javad Heydari and Saeed {Shiry Ghidary},https://www.sciencedirect.com/science/article/pii/S1047320318303213,https://doi.org/10.1016/j.jvcir.2018.11.042,1047-3203,2019,245--260,58,Journal of Visual Communication and Image Representation,Cross-modal motion regeneration using Multimodal Deep Belief Network,article,HEYDARI2019245
"In this paper we present a new adaptive boosting technique for total generalized variation (TGV) based image denoising and inpainting. Instead of the strengthening and substracting steps in existing boosting techniques, the proposed technique is iteratively operated by two steps: the first step is to take average of restored image with observed image, and updated parameter; the second step is to operate the TGV restoration algorithm with the average and dynamic parameter. For each iteration, as the input contains more correct information, the restoration algorithm can produce signals with more details. We have solved our boosting TGV model by primal-dual method, and applied the boosting TGV technique for gray/color image denoising and inpainting. Our algorithms have been discussed about influence of parameters, computational cost and compared with several typical existing methods. Plenty of experimental results show that our method can produce images with more structures and prevent staircase artifacts effectively.","Total generalized variation, Boosting technique, Image denoising, Image inpainting, Primal-dual method",Samad Wali and Huayan Zhang and Huibin Chang and Chunlin Wu,https://www.sciencedirect.com/science/article/pii/S1047320318303730,https://doi.org/10.1016/j.jvcir.2018.12.047,1047-3203,2019,39--51,59,Journal of Visual Communication and Image Representation,A new adaptive boosting total generalized variation (TGV) technique for image denoising and inpainting,article,WALI201939
"Person re-identification aims at the maintenance of a global identity as a person moves among non-overlapping surveillance cameras. It is a hard task due to different illumination conditions, viewpoints and the small number of annotated individuals from each pair of cameras (small-sample-size problem). Common subspace learning methods have been proposed to handle the camera transition problems. However, after learning the low-dimensional representation, these methods usually compute distances using a simple cosine or Mahalanobis distance. Therefore, an still open question is how to better match probe and gallery images in the learned common subspace considering reduced number of training samples and the nonlinear behavior of the data. Collaborative Representation based Classification (CRC) has been employed successfully to address the small-sample-size problem in computer vision. However, the original CRC formulation is not well-suited for person re-identification since it does not consider that probe and gallery samples are from different cameras. Furthermore, it is a linear model, while appearance changes caused by different camera conditions indicate a strong nonlinear transition between cameras. To overcome such limitations, we propose the Kernel Cross-View Collaborative Representation based Classification (Kernel X-CRC), method that represents probe and gallery images by balancing representativeness and similarity nonlinearly. According to experimental results, we achieve state-of-the-art for rank-1 matching rates in three person re-identification datasets (CUHK03, PRID450S and GRID) and the second best results on VIPeR and CUHK01 datasets. Furthermore, we present outperforming results on Market-1501 dataset demonstrating that the Kernel X-CRC is suitable to a large-scale and multiple cameras scenario.","Person re-identification, Kernel collaborative representation based classification",Raphael Prates and William Robson Schwartz,https://www.sciencedirect.com/science/article/pii/S1047320318303298,https://doi.org/10.1016/j.jvcir.2018.12.003,1047-3203,2019,304--315,58,Journal of Visual Communication and Image Representation,Kernel cross-view collaborative representation based classification for person re-identification,article,PRATES2019304
"During the peak hours, the concentration of passenger flow is relatively high for some busy subway lines, if the measures canât be taken in time, more serious accidents may happen, which will influence the social image of the subway. At present, the passenger flow of the key stations is judged mainly by the experience of the staffs, and then the corresponding measures are taken, the errors may be large, and the relevant technical research is urgently needed. First, a data collection device called âthe elf of passenger flow-collectingâ, which integrates high definition camera image acquisition equipment and WIFI probe technology was set up. It can be used to collect the original passenger flow data of congestion points of subway stations. Second, a convolution neural network passenger flow identification algorithm based on deep learning is designed, which is used to estimate the P0 of stations. Third, because of the error in the video image recognition algorithm, the WIFI probe data acquisition scheme is designed, and the SQL preprocessing assembly for WIFI data processing is established. The noise of WIFI probe is preprocessed, and the flow rate of P5 based on WIFI probe is obtained. The difference between P0 and P5 is defined, and the degree of the difference between P0 and P5 is calculated, so the final passenger flow P6 can be obtained. Finally, the Songjiang University Hall Station of Shanghai Metro line 9 was taken as an experimental analysis object, the high definition camera and WIFI probe are set up on the spot, the passenger flow video data and the WIFI data are collected synchronously, so the real-time passenger flow in the station's internal position is estimated, and the accuracy is corrected, meanwhile the passenger flow early warning of the station position is obtained. An emergency response plan based on passenger flow early warning level is proposed, and the flow chart of passenger flow density inside Songjiang University hall station is drawn. The construction of the equipment platform and the identification and correction methods of passenger flow are of good practical guiding significance for the Metro to run safely.","Rail transit, Safety of stations, Passenger flow identification, Passengersâ limiter of station, Emergency warning",Xiaobing Ding and Zhigang Liu and Haibo Xu,https://www.sciencedirect.com/science/article/pii/S1047320318303055,https://doi.org/10.1016/j.jvcir.2018.11.033,1047-3203,2019,119--129,58,Journal of Visual Communication and Image Representation,The passenger flow status identification based on image and WiFi detection for urban rail transit stations,article,DING2019119
"Water levels in lakes can reflect changes in such bodies. Therefore, there is value in identifying the variations in water levels using observations from altimetry satellites and analyzing the possible causes. In this work, the water-level changes of Hulun Lake in Inner Mongolia during the period from 2002 to 2015 are monitored by the use of Jason satellite data, the results of which are compared with historical data. Landsat TM/ETM/OLI_TRIS remote sensing images are analyzed, and the surface area of the lake extracted from them and converted to the corresponding water level to verify the values obtained from the Jason observations. The results show a downward trend after 2000 (â0.98â¯mm/year) and a sharp increase after 2012 (3.07â¯mm/year). The root mean square error (RMSE) between the two methods was 0.2369â¯m, and the correlation coefficient was 0.986. By analyzing the various influencing factors, we draw the conclusion that the water level of Hulun Lake is affected by both natural factors (e.g., rainfall, runoff, evapotranspiration etc.) and anthropogenic influences (e.g., water consumption in coal mining, overgrazing, etc.). These are the main causes of the decrease in the area of Hulun Lake and other lakes in the Inner Mongolia Autonomous Region. By comparing the lake storage anomalies of Hulun Lake with the terrestrial Total Water Storage anomalies (TWSA) inverted from GRACE satellite data and the Surface Water Storage anomalies (SWSA) from WaterGAP Global Hydrology Model (WGHM) within the Hulun Basin, we find that not only do Hulun Lake and basin interact with each other, but also that Hulun Lake has an important function with regards to the changes within the basin as a whole. This work therefore provides a method for monitoring the dynamic changes of lake water levels, while analyzing the influencing factors based on multi-scale data. Such a method shows potential for being applied to efforts to ensure environmental protection.","Jason satellite, Hulun Lake, Water level, Landsat, GRACE",Shi Li and Jianping Chen and Jie Xiang and Yun Pan and Zhiyong Huang and Yongliang Wu,https://www.sciencedirect.com/science/article/pii/S1047320318303572,https://doi.org/10.1016/j.jvcir.2018.12.031,1047-3203,2019,565--575,58,Journal of Visual Communication and Image Representation,Water level changes of Hulun Lake in Inner Mongolia derived from Jason satellite data,article,LI2019565
"Effective features are important for visual tracking, and efficiency also needs to be considered especially for multi-object tracking. Thanks to the simplicity, we think compressive sensing features are suitable for this task. In this paper, we use compressive sensing features to improve the Markov decision process (MDP) multi-object tracking framework. First, we design a single object tracker which uses the compressive tracking to correct the optical flow tracking and apply this tracker into the MDP tracking framework. The appearance model constructed during compressive tracking also helps for data association. In order to validate our method, we firstly test the designed single object tracker with a common dataset. Then, we test our multi-object tracking method for vehicle tracking. Finally, we analyze and test our approach in the multi-object tracking (MOT) benchmark for pedestrian tracking. The results show our approach performs superiorly against several state-of-the-art online multi-object trackers.","Multi-object tracking, Markov decision process, Tracking-learning-detection, Compressive sensing features",Tao Yang and Cindy Cappelle and Yassine Ruichek and Mohammed {El Bagdouri},https://www.sciencedirect.com/science/article/pii/S104732031830316X,https://doi.org/10.1016/j.jvcir.2018.11.034,1047-3203,2019,178--186,58,Journal of Visual Communication and Image Representation,Online multi-object tracking combining optical flow and compressive tracking in Markov decision process,article,YANG2019178
"Eye tracking is widely used in modern intelligent applications, such as HCI, somatosensory game and fatigue driving. Traditional eye tracking system based on Haar-like features or external hardware, which is loss of accuracy and complicated. It is obviously that human gaze point is related to head pose. However, the label of head pose in most dataset is ambiguous. So in this paper, we propose a crowdsourced system which can collect large-scale dataset for eye tracking. For better performance, we leverage head guidance point and random dot instead of fixed dot as the concern when capture frames from camera. And different illumination, poses and persons also considered for robust performance. And we propose a two-phase CNN training strategy for combining head pose and eye angles. The proposed CNN architecture can reduce the overfitting when we train eye tracking models with head pose directly. The experimental results show that our proposed method can perform well in eye tracking.","Eye tracking, Gaze shifting path, Large-scale dataset, CNN",Honghe Huang and Yi Xu and Xiao Hua and Weixiong Yan and Yanjie Huang,https://www.sciencedirect.com/science/article/pii/S1047320319300070,https://doi.org/10.1016/j.jvcir.2019.01.007,1047-3203,2019,28--32,60,Journal of Visual Communication and Image Representation,A crowdsourced system for robust eye tracking,article,HUANG201928
"The collection of digital images is growing at ever-increasing rate which rises the interest of mining the embedded information. The appropriate representation of an image is inconceivable by a single feature. Thus, the research addresses that point for content based image retrieval (CBIR) by fusing parametric color and shape features with nonparametric texture feature. The color moments, and moment invariants which are parametric methods and applied to describe color distribution and shapes of an image. The nonparametric ranklet transformation is performed to narrate the texture features. Experimentally these parametric and nonparametric features are integrated to propose a robust and effective algorithm. The proposed work is compared with seven existing techniques by determining statistical metrics across five image databases. Finally, a hypothesis test is carried out to establish the significance of the proposed work which, infers evaluated precision and recall values are true and accepted for the all image database.","CBIR, Color moments, Ranklet transform, Nonparametric statistics, Moment invariants, Hypothesis test",Soumya Prakash Rana and Maitreyee Dey and Patrick Siarry,https://www.sciencedirect.com/science/article/pii/S1047320318302888,https://doi.org/10.1016/j.jvcir.2018.11.015,1047-3203,2019,205--219,58,Journal of Visual Communication and Image Representation,Boosting content based image retrieval performance through integration of parametric & nonparametric approaches,article,RANA2019205
"Predicting the downside risk of a hedge fund is the foundation of risk measurement. These predictions also provide conditions that can be used for designing and implementing risk prevention measures. Hence, this paper proposes a big data hedge fund downside risk evaluation model based on a multi-objective neural network. First, two evaluation indexes are defined. Then, local search is applied to merge parent and descendant populations. Only those individuals from the Pareto front are optimized. Experimental results suggest that this model and method is feasible and valid. Specifically, the VaR model is unable to estimate the possible extreme risk of a hedge fund. In contrast, the CVaR model can accurately measure the risks under extreme market conditions. However, a combination of VaR and CVaR can help a fund manager avoid extreme risks to a hedge fund.","Downside risk evaluation, Big data hedge fund, Multi-objective neural network",Zhaoquan Cai and Guangcai Chen and Lining Xing and Jinghui Yang and Xu Tan,https://www.sciencedirect.com/science/article/pii/S104732031830275X,https://doi.org/10.1016/j.jvcir.2018.11.002,1047-3203,2019,433--438,59,Journal of Visual Communication and Image Representation,Evaluating hedge fund downside risk using a multi-objective neural network,article,CAI2019433
"China's environmental problems are not only related to the fundamental interests of the broad masses of the people, but also to China's national security and international image. At present, China's environmental protection work is facing a complex situation. Pollution sources can be divided into natural pollution sources and man-made pollution sources. Natural sources of pollution refer to places where nature releases harmful substances or causes harmful effects to the environment, such as active volcanoes. Man-made pollution source refers to the pollution source formed by human activities, and is also the main object of environmental protection research and control. Among the man-made pollution sources, air pollution sources, water pollution sources and soil pollution sources can be classified according to the main objects of pollution. Among them, air pollution sources and water pollution sources have the greatest impact on human life. Therefore, it has become an important subject worthy of in-depth discussion to take automatic and electronic measures for potential environmental pollution incidents, discover environmental pollution problems in time, reduce the probability of environmental pollution incidents, and even put some major environmental pollution incidents in their infancy. In this paper, deep learning method is used to classify the existing key pollution source video. Water pollution experiments show that the accuracy of video counting reaches 93.1%, which is better than other video processing schemes. The operation time of the system reaches acceptable range, and a solution to meet the real-time requirement is put forward.","Pollution sources, Deep learning, Surveillance video classification, Convolution neural network",Kunrong Zhao and Tingting He and Shuang Wu and Songling Wang and Bilan Dai and Qifan Yang and Yutao Lei,https://www.sciencedirect.com/science/article/pii/S1047320319300215,https://doi.org/10.1016/j.jvcir.2019.01.015,1047-3203,2019,283--291,59,Journal of Visual Communication and Image Representation,Research on video classification method of key pollution sources based on deep learning,article,ZHAO2019283
"Immersive learning in Virtual Reality (VR) environments is the developing trend for future education systems including remote physical training. This paper presents âImmerTaiâ, a system that is designed for effective remote motion training, particularly for Chinese Taichi, in an immersive way. With ImmerTai, the Taichi expertâs motion is captured and delivered to remote students in CAVE, HMD and PC environments for learning. The studentsâ motions are also captured for motion quality assessment and a group of students can form a virtual collaborative learning scenario. We built up a Taichi motion dataset with ground truth of motion quality, and based on this, we developed and evaluated several motion quality assessment methods. Then, user tests were designed and carried out to measure and compare the learning outcomes (learning time, quality and overall efficiency) of students in Cave Automatic Virtual Environment (CAVE), Head Mounted Display (HMD) and Personal Computer (PC) environments. Meanwhile, the connections between studentsâ learning outcomes and their VR experience were investigated and discussed too. Our results show that ImmerTai can accelerate the learning process of students noticeably (up to 17%) compared to non-immersive learning with the conventional PC setup. However, we observed a substantial difference in the quality of the learnt motion between CAVE (26% gain) and HMD (23% drop) compared to PC (baseline). While strong VR presence can enhance the learning experience of students, their learning outcomes are not fully consistent to their experience. Overall, ImmerTai with CAVE demonstrated a significantly higher learning efficiency than other tested environments.","Immersive education, Motion training, VR education",Xiaoming Chen and Zhibo Chen and Ye Li and Tianyu He and Junhui Hou and Sen Liu and Ying He,https://www.sciencedirect.com/science/article/pii/S1047320318303183,https://doi.org/10.1016/j.jvcir.2018.11.039,1047-3203,2019,416--427,58,Journal of Visual Communication and Image Representation,ImmerTai: Immersive Motion Learning in VR Environments,article,CHEN2019416
"We propose a new framework that extends the standard Probability Hypothesis Density (PHD) filter for multiple targets having Nâ©¾2 different types based on Random Finite Set theory, taking into account not only background clutter, but also confusions among detections of different target types, which are in general different in character from background clutter. Under Gaussianity and linearity assumptions, our framework extends the existing Gaussian mixture (GM) implementation of the standard PHD filter to create a N-type GM-PHD filter. The methodology is applied to real video sequences by integrating object detectorsâ information into this filter for two scenarios. For both cases, Munkresâs variant of the Hungarian assignment algorithm is used to associate tracked target identities between frames. This approach is evaluated and compared to both raw detection and independent GM-PHD filters using the Optimal Sub-pattern Assignment metric and discrimination rate. This shows the improved performance of our strategy on real video sequences.","Visual tracking, Random finite sets, FISST, Multiple target filtering, PHD filter, N-type GM-PHD filter, Gaussian mixture, OSPA metric",Nathanael L. Baisa and Andrew Wallace,https://www.sciencedirect.com/science/article/pii/S1047320319300343,https://doi.org/10.1016/j.jvcir.2019.01.026,1047-3203,2019,257--271,59,Journal of Visual Communication and Image Representation,"Development of a N-type GM-PHD filter for multiple target, multiple type visual tracking",article,BAISA2019257
"In robotic systems, the fisheye camera can provide a large field of view (FOV). Usually, the traditional restoring algorithms are needed, which are computational heavy and will introduce noise into original data, since the fisheye images are distorted. In this paper, we propose a framework to detect objects from the raw fisheye images without restoration, then locate objects in the real world coordinate by fusing attitude information. A deep neural network architecture based on the MobileNet and feature pyramid structure is designed to detect targets directly on the fisheye raw images. Then, the target can be located based on the fisheye visual model and the attitude of the camera. Compared to traditional approaches, this approach has advantages in computational efficiency and accuracy. This approach is validated by experiments with a fisheye camera and an onboard computer on a micro-aerial vehicle (MAV).","Object detection, Deep learning, Data fusion, Fisheye camera, Micro aerial vehicle, Localization",Jun Zhu and Jiangcheng Zhu and Xudong Wan and Chao Wu and Chao Xu,https://www.sciencedirect.com/science/article/pii/S1047320319300069,https://doi.org/10.1016/j.jvcir.2019.01.005,1047-3203,2019,128--139,59,Journal of Visual Communication and Image Representation,Object detection and localization in 3D environment by fusing raw fisheye image and attitude data,article,ZHU2019128
"A new method of fracturing hydraulic fracturing is proposed for the characteristics of poor coal permeability and soft coal in coal mine area in China. This method can make the hydraulic pressure concentrated on the coal at one point, and the water pressure is reduced, Effective water pressure increases, the smaller the flow that can get a better fracture effect. Through the numerical simulation, the law of fracture distribution and permeability evolution during segmental hydraulic fracturing is analyzed. Complete sets of anti-reflection equipment for segmented hydraulic fracturing have been developed, which is mainly composed of mobile high-pressure pump station, high-pressure rubber hose, water injector, putters and corresponding connecting parts. It solves the problem of tightness of hydraulic fracturing device, and breaks through the problem of miniaturization of high pressure pumping station, so that the high pressure pumping station can achieve larger working pressure at smaller flow rate. The whole process is simulated and validated by assistant image simulation technology, and applied to Pansidong Coal Mine of Huainan Mining Group. The industrial test of hydraulic directional fracturing of Pan coal mine in Huainan mining group shows that the loose influence range of coal can reach 10â¯m, and the pressure of underground movable high pressure hydraulic pumping station reaches 25â¯MPa, and the flow rate is 180â¯L/min. Compared with non fracturing drilling, drilling, the average gas concentration after fracturing, the average daily average flow rate of mixed gas drainage quantity is improved significantly, the application effect is good, and the equipment is simple, easy to use, can be repeatedly used, and has better economic benefit and application prospect.","Gas extraction, Coal seam permeability enhancement, Sublevel hydraulic fracturing, Fracturing equipment, Auxiliary image technology",Jiangtao Li and Baoshan Jia and Chunhua Zhang and Wei Wang,https://www.sciencedirect.com/science/article/pii/S104732031930029X,https://doi.org/10.1016/j.jvcir.2019.01.019,1047-3203,2019,244--252,59,Journal of Visual Communication and Image Representation,Seepage mechanism technical practice of hydraulic fracturing of coal seam and auxiliary image simulation technology,article,LI2019244
"Many current successful Person Re-Identification (ReID) methods train a model with the softmax loss function to classify images of different persons and obtain the feature vectors at the same time. However, the underlying feature embedding space is ignored. In this paper, we use a modified softmax function, termed Sphere Softmax, to solve the classification problem and learn a hypersphere manifold embedding simultaneously. A balanced sampling strategy is also introduced. Finally, we propose a convolutional neural network called SphereReID adopting Sphere Softmax and training a single model end-to-end with a new warming-up learning rate schedule on four challenging datasets including Market-1501, DukeMTMC-reID, CHHK-03, and CUHK-SYSU. Experimental results demonstrate that this single model outperforms the state-of-the-art methods on all four datasets without fine-tuning or re-ranking. For example, it achieves 94.4% rank-1 accuracy on Market-1501 and 83.9% rank-1 accuracy on DukeMTMC-reID. The code and trained weights of our model will be released.","Person re-identification, Classification, Feature embedding, CNN, Hypersphere",Xing Fan and Wei Jiang and Hao Luo and Mengjuan Fei,https://www.sciencedirect.com/science/article/pii/S1047320319300100,https://doi.org/10.1016/j.jvcir.2019.01.010,1047-3203,2019,51--58,60,Journal of Visual Communication and Image Representation,SphereReID: Deep hypersphere manifold embedding for person re-identification,article,FAN201951
"The traditional FCM algorithm is developed on the basis of classical fuzzy theory, though the classical fuzzy theory has its own limitations. The lack of expressive ability of uncertain information makes it hard for FCM algorithm to handle clustered boundary pixels and outliers. This paper proposes a Neutrosophic C-means Clustering with local information and noise distance-based kernel metric for image segmentation (NKWNLICM). At first, noisy distance and fuzzy spatial information are introduced to NCM model to improve the robustness of noise image segmentation. Then, the kernel function is used to measure the distance between pixels. By mapping low-dimensional data into high-dimensional data, the classification performance is further improved. At last, the fuzzy factor is redefined based on the distance between the center pixel and its neighborhood. The new fuzzy factor can excellently reflect the influence of neighborhood pixels on central pixels and improve the classification accuracy much better. The experimental results on Berkeley Segmentation Database demonstrates the excellent performance of the proposed method for noisy image segmentation.","Image segmentation, Noise clustering, Fuzzy clustering, Nutrosophic clustering",Zhenyu Lu and Yunan Qiu and Tianming Zhan,https://www.sciencedirect.com/science/article/pii/S1047320318303225,https://doi.org/10.1016/j.jvcir.2018.11.045,1047-3203,2019,269--276,58,Journal of Visual Communication and Image Representation,Neutrosophic C-means clustering with local information and noise distance-based kernel metric image segmentation,article,LU2019269
"Recently, defocus blur detection has been an extensive study, but it is still full of challenges in the blur estimation without having any prior knowledge of test image such as blur kernel, degree, or camera parameters. Inspired by the observation that the degree of defocus blur depth could be distinguished by different frequencies, a novel blur metric based on Multiscale SVD fusion (M-SVD) is proposed. The blur metric fuses different sub-bands of the selected singular values (SVs) in multiscale image windows, which could drastically reduce the chances of false positives for blur detection and overcome the difficulty that the sharp region is misjudged for a blur region because of its smooth texture. Finally, a blur map is applied on the test image combined with post-processing operation meanshift cluster to segment the blur region. Experimental results demonstrate that the proposed method can detect the defocus blur regions of test images with a satisfactory performance and outperforms the state-of-the-art methods.","Defocus blur detection, Multiscale singular value decomposition, Sub-bands, Meanshift",Huimei Xiao and Wei Lu and Ruipeng Li and Nan Zhong and Yuileong Yeung and Junjia Chen and Fei Xue and Wei Sun,https://www.sciencedirect.com/science/article/pii/S1047320318303742,https://doi.org/10.1016/j.jvcir.2018.12.048,1047-3203,2019,52--61,59,Journal of Visual Communication and Image Representation,Defocus blur detection based on multiscale SVD fusion in gradient domain,article,XIAO201952
"In this paper, we address the challenging problem of spatial and temporal action detection in videos. We first develop an effective approach to localize frame-level action regions through integrating static and kinematic information by the early- and late-fusion detection scheme. With the intention of exploring important temporal connections among the detected action regions, we propose a tracking-by-point-matching algorithm to stitch the discrete action regions into a continuous spatio-temporal action tube. Recurrent 3D convolutional neural network is used to predict action categories and determine temporal boundaries of the generated tubes. We then introduce an action footprint map to refine the candidate tubes based on the action-specific spatial characteristics preserved in the convolutional layers of R3DCNN. In the extensive experiments, our method achieves superior detection results on the three public benchmark datasets: UCFSports, J-HMDB and UCF101.","Spatio-temporal action detection, Deep neural networks",Yuancheng Ye and Xiaodong Yang and YingLi Tian,https://www.sciencedirect.com/science/article/pii/S1047320318303468,https://doi.org/10.1016/j.jvcir.2018.12.019,1047-3203,2019,515--524,58,Journal of Visual Communication and Image Representation,Discovering spatio-temporal action tubes,article,YE2019515
"Generally speaking, the probability of every qualified set is the same and fixed in conventional secret image sharing (SIS) for general access structure (GAS). In this paper, first we introduce generalized GAS (GGAS), which allows the user to assign probability to every qualified set. Then we design a SIS scheme for GGAS by Chinese remainder theorem (CRT). On one hand, for any qualified set, we can decode the secret image at pre-assigned probability. When we collect all the shares, we can losslessly decode the secret image. On the other hand, for any forbidden set, we will decode nothing of the secret image. We only employ modular operation to decode the secret image, which may be suitable for real-time and green computing scenarios. Experimental results and analyses are realized to examine the effectiveness of our scheme.","Secret image sharing, General access structure, Chinese remainder theorem, Lossless recovery, Generalized general access structure",Xuehu Yan and Yuliang Lu,https://www.sciencedirect.com/science/article/pii/S1047320318303067,https://doi.org/10.1016/j.jvcir.2018.11.031,1047-3203,2019,89--101,58,Journal of Visual Communication and Image Representation,Generalized general access structure in secret image sharing,article,YAN201989
"We present a novel local descriptor based on the dependent effects model. Different types of effects are computed in a local region and properly normalized to form a descriptor, which is designed to be robust to rotation, scale changes, and skew. Two specific instances of descriptor are presented and evaluated. The first, named AG, is real-valued, and uses 126 floating-point values to represent 126 effects. The second version, named AGS, is binarized, and uses 60 bytes to represent 240 effects, each with two bits. The first bit represents the sign of an effect value, while the second denotes whether the value is near or far from zero. We experimentally evaluate the proposed descriptors in combination with popular keypoint detectors on standard feature matching datasets. The extensive evaluation shows that AG and AGS achieve high score in several performance measures, and as such, they represent an attractive alternative to popular local descriptors.","Local image descriptor, Dependent effects model, Image matching, Keypoint descriptor evaluation",Rok Mandeljc and Jasna Maver,https://www.sciencedirect.com/science/article/pii/S1047320318303341,https://doi.org/10.1016/j.jvcir.2018.12.008,1047-3203,2019,503--514,58,Journal of Visual Communication and Image Representation,AGs: Local descriptors derived from the dependent effects model,article,MANDELJC2019503
"Affective image classification, which aims to classify images according to their affective characteristics of inducing human emotions, has drawn increasing research attentions in the multimedia community. Although many features have been attempted, the semantic gap between low-level visual features and high-level emotional semantics, however, remains a major challenge. In this paper, we propose an affective image classification algorithm by jointly using the visual features extracted under the guidance of the art theory and semantic image annotations, such as the categories of objects and scenes, generated by a pre-trained deep convolutional neural network. This algorithm has been evaluated against three state-of-the-art approaches on three benchmark image datasets. Our results indicate that combining interpretable aesthetic features and semantic annotations can better characterize the emotional semantics and the proposed algorithm is able to produce more accurate affective image classification than the other three approaches.","Affective image classification, Discrete emotion space, Deep convolutional neural network (DCNN), Feature extraction, Support vector machine (SVM)",Xuan Liu and Na Li and Yong Xia,https://www.sciencedirect.com/science/article/pii/S1047320318303584,https://doi.org/10.1016/j.jvcir.2018.12.032,1047-3203,2019,576--588,58,Journal of Visual Communication and Image Representation,Affective image classification by jointly using interpretable art features and semantic annotations,article,LIU2019576
"Intelligence technology is an important research area. As a very special yet important case of object recognition, hand-held object recognition plays an important role in intelligence technology for its many applications such as visual question-answering and reasoning. In real-world scenarios, the datasets are open-ended and dynamic: new object samples and new object classes increase continuously. This requires the intelligence technology to enable hybrid incremental learning, which supports both data-incremental and class-incremental learning to efficiently learn the new information. However, existing work mainly focuses on one side of incremental learning, either data-incremental or class-incremental learning while do not handle two sides of incremental learning in a unified framework. To solve the problem, we present a Hybrid Incremental Learning (HIL) method based on Support Vector Machine (SVM), which can incrementally improve its recognition ability by learning new object samples and new object concepts during the interaction with humans. In order to integrate data-incremental and class-incremental learning into one unified framework, HIL adds the new classification-planes and adjusts existing classification-planes under the setting of SVM. As a result, our system can simultaneously improve the recognition quality of known concepts by minimizing the prediction error and transfer the previous model to recognize unknown objects. We apply the proposed method into hand-held object recognition and the experimental results demonstrated its advantage of HIL. In addition, we conducted extensive experiments on the subset of ImageNet and the experimental results further validated the effectiveness of the proposed method.","Incremental learning, Object recognition, SVM, Human-machine interaction",Chengpeng Chen and Weiqing Min and Xue Li and Shuqiang Jiang,https://www.sciencedirect.com/science/article/pii/S1047320318302815,https://doi.org/10.1016/j.jvcir.2018.11.009,1047-3203,2019,138--148,58,Journal of Visual Communication and Image Representation,Hybrid incremental learning of new data and new classes for hand-held object recognition,article,CHEN2019138
"The convolutional sparse coding-based super-resolution (CSC-SR) method has shown its good performance in single image super-resolution. It divides the low-resolution (LR) image into low-frequency part and the high-frequency part, and reconstructs their corresponding high-resolution (HR) image with bicubic interpolation and convolutional sparse coding (CSC) method, respectively. This paper is devoted to improve the performance of CSC-SR method. As convolutional neural network (CNN) can reveal the mapping relation between the LR image and the HR image for the low-frequency part better, we replace the bicubic interpolation with CNN to reconstruct the HR image for the low-frequency part. In addition, we propose an adaptive CSC method to reconstruct the HR image for the high-frequency part. We name our proposed super-resolution method as hybrid adaptive convolutional sparse coding-based super-resolution (HACSC-SR) method. Many comparison experiments illustrate that our proposed HACSC-SR method is superior to CSC-SR, CNN as well as several existing super-resolution methods.","Super-resolution, Convolutional sparse coding, Convolutional neural network, Adaptive",Jianwei Zhao and Chen Chen and Zhenghua Zhou and Feilong Cao,https://www.sciencedirect.com/science/article/pii/S1047320318303626,https://doi.org/10.1016/j.jvcir.2018.12.036,1047-3203,2019,651--661,58,Journal of Visual Communication and Image Representation,Single image super-resolution based on adaptive convolutional sparse coding and convolutional neural networks,article,ZHAO2019651
"In this study, a new approach and mathematical framework are proposed for exposing image forgeries by detecting inconsistencies in the geometry of cast shadows. The main difficulty in detecting shadow inconsistencies is the precise establishment of correspondences between object points and their corresponding shadow points. To counter the problem, a mathematical framework is proposed to formulate the geometric transformation between the object points and their corresponding shadow points. We assume a rough correspondence between the object and shadow points and use Expectation-Maximization (EM) algorithm to simultaneously calculate the transformation parameters and categorize rough correspondences as inliers or outliers. To enhance the efficiency of the proposed algorithm, we extend the proposed algorithm to handle the ambiguity in initial correspondence by using the one-to-many correspondence strategy. Experimental results on the provided database comprising forged and authentic images showed the accuracy of 84% and 98% for one-to-one and one-to-many correspondence strategies, respectively.","Image forensics, Forgery detection, Shadow geometry, EM algorithm, Image tampering",Morteza Nasiri and Alireza Behrad,https://www.sciencedirect.com/science/article/pii/S104732031830333X,https://doi.org/10.1016/j.jvcir.2018.12.007,1047-3203,2019,323--333,58,Journal of Visual Communication and Image Representation,Using Expectation-Maximization for exposing image forgeries by revealing inconsistencies in shadow geometry,article,NASIRI2019323
"With the development of information transmission technology and computer technology, information acquisition mode is mainly converted from character to image nowadays. However, in the process of acquiring and transmitting images, image damage and quality decrease due to various factors. Therefore, how to restore image has become a research hotspot in the field of image processing. This paper establishes an image restoration model based on BP neural network. The simulation results show that the proposed method has made a great improvement compared with the traditional image restoration method.","Image restoration, Image processing, Image denoising, BP neural network",Hongzhi Xue and Hongwei Cui,https://www.sciencedirect.com/science/article/pii/S1047320319300203,https://doi.org/10.1016/j.jvcir.2019.01.014,1047-3203,2019,204--209,59,Journal of Visual Communication and Image Representation,Research on image restoration algorithms based on BP neural network,article,XUE2019204
"Super-resolution reconstruction is a method that can transcend the limitations of optical imaging systems through the use of image processing algorithms. Recent techniques of super-resolution for single monochrome images develop rapidly, but for single multi-color images, to efficiently apply the monochrome super-resolution algorithms to all channels is still under exploration. In most of the recent research, the chromatic channels are simply upscaled by interpolation, which leads to the quality of the chromatic channels downgraded. This application may not be noticed by the visual systems of humans, but can affect other algorithms when super-resolution plays roles at image pre-processing. In this paper, we present a novel approach for multi-color super-resolution reconstruction. Using the super-resolution reconstructed luminance channel as the guide image, we adopt guided filters to manage the interpolated chromatic channels. Guided filters retain the sharp edges and fine details from the guided image and carry them to the output images. Meanwhile the whole process is quite computationally economic. Extensive experiments on natural images show that our method achieves better results than the method that is used in most of the algorithm in the literature in both statistic and visual aspects.","Super-resolution, Multi-color image, Guided filter, Chromatic channel",Qinglin Zhang and Bingling Chen and Xuan Lu and Qiaoqiao Xia,https://www.sciencedirect.com/science/article/pii/S1047320318303195,https://doi.org/10.1016/j.jvcir.2018.11.040,1047-3203,2019,277--284,58,Journal of Visual Communication and Image Representation,Super-resolution of single multi-color image with guided filter,article,ZHANG2019277
"We concentrate on modeling the person-person interactions for group activity recognition. In order to solve the complexity and ambiguity problems caused by a large number of human objects, we propose a causality-induced hierarchical Bayesian model to tackle the interaction activity video, referring to the âwhatâ interaction activities happen, âwhereâ interaction atomic occurs in spatial, and âwhenâ group interaction happens in temporal. In particular, Granger Causality has been characterized with multiple features to encode the interacting relationships between each individual in the group. Furthermore, to detect and identify the concurrent interactive simultaneously, we investigate the Relative Entropy as a metric to measure the reasonable motion dependency between two arbitrary individuals. Filtered by the causality dependency, causality motion features have been cast as the multiplicative probabilistic ingredients in Bayesian factors to formulate the compact learned latent interaction patterns aggregately that enable the power of discrimination. Experiments demonstrate our model outperforms state-of-the-art models.","Group activity detection and recognition, Casual context, Granger casual topic model",Zhao Xie and Tianfu Wu and Xingming Yang and Luming Zhang and Kewei Wu,https://www.sciencedirect.com/science/article/pii/S1047320319300057,https://doi.org/10.1016/j.jvcir.2019.01.006,1047-3203,2019,62--75,59,Journal of Visual Communication and Image Representation,Jointly social grouping and identification in visual dynamics with causality-induced hierarchical Bayesian model,article,XIE201962
"In this paper, a computer vision based framework is proposed that detects falls from surveillance videos. Firstly, we employ background subtraction and rank pooling to model spatial and temporal representations in videos, respectively. We then introduce a novel three-stream Convolutional Neural Networks as an event classifier. Silhouettes and their motion history images serve as input to the first two streams, while dynamic images whose temporal duration is equal to motion history images, are used as input to the third stream. Finally, we apply voting on the results of event classification to perform multi-camera fall detection. The main novelty of our method against the conventional ones is that high-quality spatiotemporal representations in different levels are learned to take full advantage of the appearance and motion information. Extensive experiments have been conducted on two widely used fall datasets. The results have shown to demonstrate the effectiveness of the proposed method.","Fall detection, Human silhouette, Motion history image, Dynamic image, Convolutional Neural Networks, High-quality representation",Yongqiang Kong and Jianhui Huang and Shanshan Huang and Zhengang Wei and Shengke Wang,https://www.sciencedirect.com/science/article/pii/S1047320319300331,https://doi.org/10.1016/j.jvcir.2019.01.024,1047-3203,2019,215--230,59,Journal of Visual Communication and Image Representation,Learning spatiotemporal representations for human fall detection in surveillance video,article,KONG2019215
"Image classification is a hot topic in image processing. Image classification aims to automatically classify large numbers of images. Many methods have been proposed for solving this task. Traditional methods usually leverage low-level features. Clustering is the most commonly used method of image classification. In recent years, convolutional neural networks (CNNs) is widely used in extracting deep features. Many network architectures are proposed for image classification, such as ResNeXt, Cifar10. These deep learning methods aims at fusing features of texture, color and segmentation. In this paper, we discuss the different methods and techniques of image classification, and made a detailed summary of their performance. We believe that our work plays an important role in the field of image classification.","Image classification, Deep learning, Convolutional neural networks",Yafei Wang and Zepeng Wang,https://www.sciencedirect.com/science/article/pii/S1047320318303754,https://doi.org/10.1016/j.jvcir.2018.12.049,1047-3203,2019,210--214,59,Journal of Visual Communication and Image Representation,A survey of recent work on fine-grained image classification techniques,article,WANG2019210
"Learning based hashing have been widely adopted to the approximate nearest neighbour search in large-scale image retrieval. However, how to preserve the semantic information in hashing embedding is still a challenge problem. Moreover, most of the existing methods employ the relaxation strategy to solve discrete constraint problem, which may accumulate binary quantization error as the coding length increases. In this paper, we propose a graph-based supervised hashing framework to address these problems, where the semantic information is preserved from two aspects. On one hand, we employ a supervised learning model to keep the semantic consistency. On the other hand, the intrinsic manifold structure is captured by a graph-based model. In addition, to reduce the quantization error, we adopt a discrete optimization strategy to replace the relaxation one. Experiments conducted on three benchmark datasets to demonstrate the effectiveness of the proposed method.","Discrete optimization, Graph regularization, Image retrieval, Supervised discrete hashing",Jian Guan and Yifan Li and Jianguo Sun and Xuan Wang and Hainan Zhao and Jiajia Zhang and Zechao Liu and Shuhan Qi,https://www.sciencedirect.com/science/article/pii/S1047320318303511,https://doi.org/10.1016/j.jvcir.2018.12.025,1047-3203,2019,675--687,58,Journal of Visual Communication and Image Representation,Graph-based supervised discrete image hashing,article,GUAN2019675
"Emotional human facial animation has become an indispensable technique in a series of multimedia systems. The technique first generates the phoneme and emotion sequences. Then, the viseme/expression sequences are calculated accordingly, which are further converted into a coherent facial animation video. In this work, a completely automatic system is designed by selecting acoustic features discriminative to both emotion and phoneme tags. More specifically, acoustic features highly representative to both emotion and phoneme tags are selected under a multi-task learning framework. Based on this, speech phoneme and emotion sequences are effectively calculated. Then, an active learning algorithm is developed to discover the key facial frames representative to both the phoneme and emotion tags. Finally, we associate each phonemeâ¯+â¯emotion tuple with a key facial frame. And a popular morphing algorithm is employed to fit them into a coherent animation video. Experimental results have demonstrated that our generated facial animation video is natural, coherent, and highly synchronized with the input speech.","Multimodal feature refinement, Emotional speech animation",Jinqing Shen and Yunzhong Yu and Chongbiao Zhang and Yongming Xu and Feiwei Li and Yiyang Yao,https://www.sciencedirect.com/science/article/pii/S1047320318303237,https://doi.org/10.1016/j.jvcir.2018.11.043,1047-3203,2019,712--716,58,Journal of Visual Communication and Image Representation,Multi-task multimodal feature refinement for emotional speech animation,article,SHEN2019712
"In this paper, we present a Deep Generative Directed-Network (DGDN), which estimates the occlusion relationship of boundaries. Specially, we use a low-level segmentater to partition the image into regions, then estimate their occlusion relationship by two perceptual depth cues. We decompose our DGDN model into three sub-modules to extract regional appearance cue, edgel orientation cue and to further infer global occlusion relationship with these cues, respectively. Firstly, we predict regional scene depth by a upsampling deep dense network (DenseNet). Secondly, we simultaneously estimate edgel occlusion with logistic regression. However, the occlusion relationship always suffers from unexpected conflicts due to noisy regional and edgel cues. Therefore, we finally infer occlusion relationship in a Hidden Markov Field (HMF), which tackles conflicts with bi-direction inference and the HMF parameters are exploited by iterative EM-like procedure. Ablation experiments on NYUv2 and Make3D database prove that our DGDN model outperforms state-of-the-art methods.","Deep generative directed-network, Depth ordering, Hidden Markov field, DenseNet",Kewei Wu and Yang Gao and Hailong Ma and Yongxuan Sun and Tingting Yao and Zhao Xie,https://www.sciencedirect.com/science/article/pii/S1047320318303596,https://doi.org/10.1016/j.jvcir.2018.12.034,1047-3203,2019,554--564,58,Journal of Visual Communication and Image Representation,A deep generative directed network for scene depth ordering,article,WU2019554
"In this automation era, video surveillance becomes an essential component and omnipresent at ATMs, public places, airports, railways, roadways, etc. There are many challenges to store and access such massive data generated by video surveillance. Therefore, a novel technique is required to manage the comprehensive view of the content. In this work, we propose an event summarization technique using Deep learning framework for monocular videos. A spatiotemporal similarity function is developed to construct a similarity matrix based on the visual features. Video frames are represented by the sparse matrix as graph vertices based on an objective function, where Highly Connected Subgraphs (HCS) are constructed as clusters. Finally, events are obtained from such clusters assuming that the centroid of the cluster is a key-frame of the event. Consequently, this approach does not require assumption to determine the number of clusters. Due to this advantage, users can select the number of keyframes without incurring an extra computational cost. Experimental results on two benchmark datasets show that the proposed model outperforms the state-of-the-art models on Precision and F-measure and also cover the major contents of the original video.","Clustering, Deep learning, Event summarization, Highly connected subgraph, Key-frames, Video, Graph",Krishan Kumar,https://www.sciencedirect.com/science/article/pii/S1047320318303353,https://doi.org/10.1016/j.jvcir.2018.12.009,1047-3203,2019,345--352,58,Journal of Visual Communication and Image Representation,EVS-DK: Event video skimming using deep keyframe,article,KUMAR2019345
"Editing a real-world photo through computer software or mobile applications is one of the easiest things one can do today before sharing the doctored image on oneâs social networking sites. Although most people do it for fun, it is suspectable if one concealed an object or changed someoneâs face within the image. Before questioning the intention behind the editing operations, we need to first identify how and which part of the image has been manipulated. It therefore demands automatic tools for identifying the intrinsic difference between authentic images and tampered images. This survey provides an overview on typical image tampering types, released image tampering datasets and recent tampering detection approaches. It presents a distinct perspective to rethink various assumptions of tampering clues behind different detection approaches. And this further encourages the research community to develop general tampering localization methods in the future instead of adhering to single-type tampering detection.","Image tampering detection, Image forgery detection, Image forensics, Image copy-move detection, Image splicing detection",Lilei Zheng and Ying Zhang and Vrizlynn L.L. Thing,https://www.sciencedirect.com/science/article/pii/S104732031830350X,https://doi.org/10.1016/j.jvcir.2018.12.022,1047-3203,2019,380--399,58,Journal of Visual Communication and Image Representation,A survey on image tampering and its detection in real-world photos,article,ZHENG2019380
"Histogram equalisation (HE) is a simple and effective contrast enhancement method. However, it has certain drawbacks, namely, brightness inconsistency, over-enhancement, and noise amplification. In addition, there is structure information loss while processing HE. To overcome those drawbacks simultaneously, we propose a novel edge enhancing bi-histogram equalisation method using guided image filter. In the proposed algorithm, a new adaptive plateau limit and a new edge-enhancing transformation function are proposed. The adaptive plateau limit makes the method robust to various histogram distributions, and the edge-enhancing transformation enhances edges while suppressing noise amplification in the flat region. The performance of the various HE algorithms are evaluated both quantitatively and qualitatively. The qualitative assessment shows that the proposed algorithm avoids over-enhancement and noise amplification, effectively. In addition, the quantitative metrics show that the proposed algorithm outperforms the existing HE algorithms in terms of local contrast, discrete entropy, and perceptual sharpening index.","Histogram equalisation, Contrast enhancement, Brightness preservation, Edge enhancement, Guided image filter",Junwon Mun and Yuneseok Jang and Yoojun Nam and Jaeseok Kim,https://www.sciencedirect.com/science/article/pii/S1047320318303638,https://doi.org/10.1016/j.jvcir.2018.12.037,1047-3203,2019,688--700,58,Journal of Visual Communication and Image Representation,Edge-enhancing bi-histogram equalisation using guided image filter,article,MUN2019688
"Gait as a biometric feature is widely used for human identification, and gait recognition has recently become a significant research problem. According to a small amount of labeled multi-view, multi-walking-condition and multi-clothes-condition human walking videos, we can find an effective model based on capsule network to capture more discriminative features and promote gait recognition performance. This paper works on gait recognition based on capsule network and we consider two different architectures, namely matching local features at the bottom layer based on capsule network and matching mid-level features at the middle layer based on capsule network, input images such as GEI, CGI, and resolution of input image. Empirical evaluations are conducted in the aspect of kinds scenarios, namely cross-walking-condition, cross-view and cross-clothes condition. The approaches are evaluated on the CASIA-B dataset and OU-ISIR Treadmill dataset B. These results show that the methods exceed the previous state-of-the-art outcomes.","Gait recognition, Capsule network, Deep learning",Zhaopeng Xu and Wei Lu and Qin Zhang and Yuileong Yeung and Xin Chen,https://www.sciencedirect.com/science/article/pii/S1047320319300318,https://doi.org/10.1016/j.jvcir.2019.01.023,1047-3203,2019,159--167,59,Journal of Visual Communication and Image Representation,Gait recognition based on capsule network,article,XU2019159
"In response to the increased demand for high-resolution video, the new generation of video standards, High Efficiency Video Coding (HEVC) and its scalable extension (SHVC) have been finalized. The compression of HEVC/SHVC is efficiently improved and supports ultra-high resolution (UHD). Therefore, the coding complexity of HEVC/SHVC is much higher than those of previous standards. The framework of SHVC is based on HEVC and is divided into several types of scalable video. SHVC can be decoded into various video resolutions, frame rates and qualities, and only needs to be encoded once, but with higher complexity than HEVC. Thus, how to reduce the coding complexity of SHVC is the purpose of this paper. Our proposed algorithm accelerates the enhancement layer (EL) prediction by utilizing encoded Coding Unit (CU) sizes, prediction modes, motion vectors and Rate-Distortion Costs (RD-Costs) of the base layer (BL) and encoded CU sizes of the enhancement layer for quality scalability of SHVC. Experimental results show that the proposed algorithm can save lots of time while maintaining good video quality, and the performance is better than those of previous works.","High Efficiency Video Coding Scalable Extension, SHVC, Quality scalability, Fast decision, Inter-layer prediction",Chih-Hsuan Yeh and Jie-Ru Lin and Mei-Juan Chen and Chia-Hung Yeh and Cheng-An Lee and Kuang-Han Tai,https://www.sciencedirect.com/science/article/pii/S1047320318303481,https://doi.org/10.1016/j.jvcir.2018.12.021,1047-3203,2019,462--476,58,Journal of Visual Communication and Image Representation,Fast prediction for quality scalability of High Efficiency Video Coding Scalable Extension,article,YEH2019462
"Nowadays embedded multimedia devices are designed for computationally intensive applications such as image processing in various multimedia systems. Image processing algorithms should be implemented on hardware platforms for improving the performance. Reconfigurable hardware implementation using Field Programmable Gate Arrays (FPGAs) provides low latency with high performance in real time applications. FPGAs offer the reprogrammability of an application specific solution while retaining the performance advantage. In real time applications as image sizes increase rapidly, only hardware systems must be used with low complex software. In this paper, main perspective of developing and implementing skeletonization algorithm as a part of computer vision, pattern recognition application is focused and presented. A simple algorithm to skeletonize the 2-D image using MATLAB is developed. An architecture and implementation of this skeletonization algorithm for 2-D gray scale images is proposed. For analyzing pixel values 3â¯Ãâ¯3 windowing operator is used. The proposed architecture is tested for an image size of 8â¯Ãâ¯8, but the approach presented in this paper can be used for images of any size (Mâ¯Ãâ¯N), if the FPGA memory is sufficiently large. The implementation was carried out on Xilinx Vertex 5 board.","FPGA, Skeleton, Gray scale images, Computer vision, 2-D image",Perumalla {Srinivasa Rao} and Kamatham Yedukondalu,https://www.sciencedirect.com/science/article/pii/S1047320319300045,https://doi.org/10.1016/j.jvcir.2019.01.004,1047-3203,2019,140--149,59,Journal of Visual Communication and Image Representation,Hardware implementation of digital image skeletonization algorithm using FPGA for computer vision applications,article,SRINIVASARAO2019140
"A novel filter design for the restoration of the corrupted digital image is proposed in this paper. The filter design incorporates type II fuzzy system and cuckoo search optimization algorithm (T2FCS) based filter design for the restoration of the noise in the images. The noisy pixels in the images are detected using the proposed circular based searching scheme and the detected corrupt pixels are removed using the cuckoo search algorithm. The enhanced pixels in place of the corrupt pixels are acquired using the proposed type II fuzzy system. The proposed filter adapts to various noisy conditions such as random noise, salt and pepper noise and scratch noise. The experimentation of the proposed filter design is carried out over two images. The performance of the proposed T2FCS filter design is compared over the existing image restoration algorithms using metrics; Peak Signal to Noise ratio (PSNR), Structural Similarity Index (SSIM), Second Derivative Like Measure of Enhancement (SDME). The result obtained favours the performance of the proposed filter in the restoration of the noisy images.","Cuckoo search optimization algorithm, Type II fuzzy system, Image de-noising and restoration, Second derivative like measure of enhancement",Sagenela {Vijaya Kumar} and C. Nagaraju,https://www.sciencedirect.com/science/article/pii/S104732031830347X,https://doi.org/10.1016/j.jvcir.2018.12.020,1047-3203,2019,619--641,58,Journal of Visual Communication and Image Representation,T2FCS filter: Type 2 fuzzy and cuckoo search-based filter design for image restoration,article,VIJAYAKUMAR2019619
"In existing multi-vision tracking methods, a distributed collaborative tracking mode based on homography constraints is often adopted, yet there are significant shortcomings to this approach. For example, visual information complementation is not used to improve the robustness of tracking, and collaborative tracking is limited by homography constraints. In this study, a three-dimensional spatial particle filter tracking method was proposed, and multi-vision joint tracking and collaboration were effectively achieved. This method was based on the existing particle filter framework. A two-dimensional plane particle was taken as the projection of a three-dimensional spatial particle on the imaging plane, and the formula for calculating a spatial particleâs weight was derived based on Bayesian posterior probability recursion. In addition, an approximation method to determine spatial particle weight was given. The resampling of spatial particles was performed by using an epipolar line resampling method, and a collaborative tracking mechanism was established based on the concept of resolution. The results showed that the proposed method had higher tracking precision and anti-occlusion performance than other existing methods. In this method, the robustness of tracking was effectively improved, and unlimited optimization cooperation between visual sensing was achieved.","Visual tracking, Particle filter, Collaboration, Epipolar line, Homography",Long Liu and Zhaohui Xi and Qiang Sun,https://www.sciencedirect.com/science/article/pii/S1047320318303766,https://doi.org/10.1016/j.jvcir.2018.12.050,1047-3203,2019,316--326,59,Journal of Visual Communication and Image Representation,Multi-vision tracking and collaboration based on spatial particle filter,article,LIU2019316
"3D object retrieval has attractive extensive research focus in recent years. Among various schemes, view based 3D object retrieval is regarded as a promising direction. In this paper, we present a novel view-based 3D object retrieval framework, which is deployed over a graph-based collaborative learning scheme to intelligently fuse multiple features. In particular, we introduce a hypergraph based collaborative feature learning scheme to fuse complement descriptors from both the contour and the interior region of 3D object effectively. Then, the view-based 3D object retrieval is done via a greedy bipartite graph matching algorithm, which achieves highly accurate and efficient 3D object matching. With the above bipartite graph matching and feature concatenation, significant performance improvement is achieved in the 3D object retrieval task, on either widely-used benchmark datasets or open competitions like SHREC15 challenge. In both evaluations, the proposed graph-based collaborative feature learning scheme has beaten a serial of existing approaches and state-of-the-art schemes.","3D Object retrieval, Collaborative feature learning, Hypergraph learning, Bipartite graph matching",Feng Chen and Bo Li and Liang Li,https://www.sciencedirect.com/science/article/pii/S1047320318303249,https://doi.org/10.1016/j.jvcir.2018.11.046,1047-3203,2019,261--268,58,Journal of Visual Communication and Image Representation,3D object retrieval with graph-based collaborative feature learning,article,CHEN2019261
"Rich multimedia contents are dominating the current Web. In popular social media platforms such as FaceBook, Twitter, and Instagram, there are over millions of multimedia contents being created by users. In the meantime, multimedia data consists of data in multiple modalities, such as text, images, videos, audio, time series sequences, and so on. Many research efforts have been devoted to multimedia annotation to further improve the performance. However, the prevailing methods are designed for single-media annotation task. In fact, heterogeneous media content describes given labels from respective modality and is complementary to each other, and it becomes critical to explore advanced techniques for heterogeneous data analysis and multimedia annotation. Inspired by this idea, this paper presents a new multimodal correlation learning method for heterogeneous multimedia cooperative annotation, named unified space learning, which projects heterogeneous media data into one unified space. We formulate the multimedia annotation task into a semi-supervised learning framework, in which we learn different projection matrices for different media type. By doing so, different media content is aligned cooperatively, and jointly provides a more complementary profile of given semantic labels. Experimental results on data set with images, audio clips, videos and 3D models show that the proposed approach is more effective.","Multimedia annotation, Cooperative annotation, Multimodal correlation learning",Feng Tian and Quge Wang and Xin Li and Ning Sun,https://www.sciencedirect.com/science/article/pii/S1047320318303523,https://doi.org/10.1016/j.jvcir.2018.12.028,1047-3203,2019,544--553,58,Journal of Visual Communication and Image Representation,Heterogeneous multimedia cooperative annotation based on multimodal correlation learning,article,TIAN2019544
"Many systems have been developed to support remote guidance, where a local worker manipulates objects under guidance of a remote expert helper. These systems typically use speech and visual cues between the local worker and the remote helper, where the visual cues could be pointers, hand gestures, or sketches. However, the effects of combining visual cues together in remote collaboration has not been fully explored. We conducted a user study comparing remote collaboration with an interface that combined hand gestures and sketching (the HandsInTouch interface) to one that only used hand gestures, when solving two tasks; Lego assembly and repairing a laptop. In the user study, we found that (1) adding sketch cues improved the task completion time, only with the repairing task as this had complex object manipulation but (2) using gesture and sketching together created a higher task load for the user.","Hand gestures, Multimodal communication, Remote collaboration, Physical task, Sketches",Weidong Huang and Seungwon Kim and Mark Billinghurst and Leila Alem,https://www.sciencedirect.com/science/article/pii/S1047320318303365,https://doi.org/10.1016/j.jvcir.2018.12.010,1047-3203,2019,428--438,58,Journal of Visual Communication and Image Representation,Sharing hand gesture and sketch cues in remote collaboration,article,HUANG2019428
"Facial expression recognition (FER) plays an important role in the applications of human computer interaction. Given the wide use of convolutional neural networks (CNNs) in automatic video and image classification systems, higher-level features can be automatically learned from hierarchical neural networks with big data. However, learning CNNs require large amount of training data for adequate generalization, while the Scale-invariant feature transform (SIFT) does not need large training samples to generate useful feature. In this paper, we propose a new hybrid feature representation for the recognition of facial expressions from a single image frame that uses a combination of SIFT and deep-learning feature of different level extracted from the CNN model, then adopt the combined features and classify the expression by support vector machines (SVM). The performance of the proposed method has been validated on public CK+ databases. To evaluate the generalization ability of our method, we also performed an experiment on a cross-database environment. Experimental results show that the proposed approach can achieve better classification rates compared with state-of-art CNN methods, which indicate the considerable potential of combining shallow feature with deep-learning feature.","Facial expression recognition, Convolutional neural networks, Scale-invariant feature transform, Deep-learning feature, Support vector machines",Fengyuan Wang and Jianhua Lv and Guode Ying and Shenghui Chen and Chi Zhang,https://www.sciencedirect.com/science/article/pii/S1047320318302839,https://doi.org/10.1016/j.jvcir.2018.11.010,1047-3203,2019,84--88,59,Journal of Visual Communication and Image Representation,Facial expression recognition from image based on hybrid features understanding,article,WANG201984
"Geotechnical mechanical testing machine is an important means to study the characteristics of rock and soil rupture, which is of great significance in shale gas exploitation, nuclear waste disposal and earthquake prediction. For the convenience of research, the complex structure of rock and soil is often neglected, and the geotechnical material is regarded as a macro continuum. On this basis, a new method is used, X-ray CT scale cracks, crack size is larger than the micro-scale cracks, the number of cracks is less, but geotechnical CT images can still show the crack initiation location, propagation path, through the process, cracks and the relationship between aggregate mortar. When CT-scale microcracks can be found, the length of microcracks is equal to the magnitude of aggregate-scale, and can be compared with numerical simulation results. In this paper, four different kinds of soil samples are selected to design relevant tests. The specific effects of CT scanning parameters on CT images of rock and soil samples are studied by direct and indirect methods combined with CT number curves under different scanning conditions. The results show that the scanning voltage and filtering function have great influence on CT images and CT numbers of rock and soil samples. The enhancement or inhibition of the filtering function to the geotechnical CT image depends on the property of the selected filtering function, but has nothing to do with the soil quality of the sample. Finally, the selection principle of the CT scanning parameters is given. With the help of reasonable CT scanning parameters, the quality of the geotechnical CT image can be improved and the relatively accurate geotechnical CT value can be obtained.","Digital image processing, Relative standard deviation, Parameters, Geotechnical CT image",Pengfei Shan and Xingping Lai,https://www.sciencedirect.com/science/article/pii/S1047320318303407,https://doi.org/10.1016/j.jvcir.2018.12.014,1047-3203,2019,642--650,58,Journal of Visual Communication and Image Representation,Influence of CT scanning parameters on rock and soil images,article,SHAN2019642
"Image segmentation plays a fundamental role in image processing. Active contour models have been widely used since they handle topological change easily and provide smooth contours. However, noise presents challenges for edge-based level set methods since it leads contours easily passing through objects or falling into local minima. In this paper, we propose a weighted edge-based level set method based on multi-local statistical information to better segment noisy images. Through analysing the deficiencies of constant length and regional coefficients and traditional edge stop function in noisy image segmentation, weighted length and regional coefficients and modified edge stop function are proposed to overcome their shortcomings, respectively. The weighted edge-based level set method is used to segment synthetic and real images that have added different types and levels of noise. The experiments indicate that our method provides higher segmentation accuracies and more accurate segmentation results, which demonstrate its effectiveness and robustness.","Noisy image segmentation, Active contour model, Level set method, Weighted coefficients, Local statistical information, Edge stop function",Cheng Liu and Weibin Liu and Weiwei Xing,https://www.sciencedirect.com/science/article/pii/S104732031930001X,https://doi.org/10.1016/j.jvcir.2019.01.001,1047-3203,2019,89--107,59,Journal of Visual Communication and Image Representation,A weighted edge-based level set method based on multi-local statistical information for noisy image segmentation,article,LIU201989
"In this paper, a robust re-ranking method based on expanded k-reciprocal neighbors is proposed. Our method assumes that if a gallery image is the probe image of the expanded k-reciprocal nearest neighbors, these images are more likely to be of the same person. Specifically, given a probe image, we replace the probe with its expanded reciprocal nearest neighbor and the final distance is computed by the mean value of the corresponding neighbor set. The proposed method is unsupervised, automatic and applicable to other person re-identification problems. Moreover, our method can perform well even with a simple direct rank list where the Euclidean distance was used to compute the distances between the images. Experiments on many public datasets demonstrate the effectiveness and robustness of our re-ranking method. The proposed method achieves 4.9% improvement in Rank-1 on the CUHK03 dataset and a significant improvement of 18.6% in mAP on the Duke dataset.","Person re-identification, Re-ranking, Expanded k-reciprocal neighbors, Rank list similarity",Ying Chen and Jin Yuan and Zhiyong Li and Yiqiang Wu and Mourad Nouioua and Guoqi Xie,https://www.sciencedirect.com/science/article/pii/S1047320318303250,https://doi.org/10.1016/j.jvcir.2018.11.044,1047-3203,2019,486--494,58,Journal of Visual Communication and Image Representation,Person re-identification based on re-ranking with expanded k-reciprocal nearest neighbors,article,CHEN2019486
"Object tracking is very important in intelligent systems, such as video surveillance, automatic drive and traffic security. Background subtraction algorithm is a mature method for foreground object extraction, but it may be affected by complicated background or the change of object shape. So in this paper, a novel object tracking method is proposed using a triplet model. First, BING feature is used to find some potential object proposals. Then, we construct a triplet model for each potential object. The triplet of the same object between two consecutive frames is considered similar. Finally, object tracking can be achieved by computing feature difference of triplets. Experimental results show that our method can achieve object tracking effectively and in real time.","Object tracking, BING feature, Triplet model",Bing Tu and Wenlan Kuang and Yongheng Shang and Danbing He and Lin Zhao,https://www.sciencedirect.com/science/article/pii/S1047320319300392,https://doi.org/10.1016/j.jvcir.2019.01.032,1047-3203,2019,64--68,60,Journal of Visual Communication and Image Representation,A multi-view object tracking using triplet model,article,TU201964
"In this paper, we propose a novel reversible data hiding method in encrypted images. The proposed method takes full advantage of the spatial correlation in the original images to vacate room for embedding data before image encryption. By jointly using an extended run-length coding and a block-based most significant bit (MSB) plane rearrangement mechanism, the MSB planes of images can be compressed efficiently to generate room for high-capacity embedding. The receiver can extract data directly from encrypted images with only the data hiding key, and the original image or the high-quality plain image that contains secret data can be recovered with only the encryption key. The experimental results prove that the proposed method can reach a high embedding rate and a high PSNR.","Reversible data hiding, Run-length coding, Image encryption",Kaimeng Chen and Chin-Chen Chang,https://www.sciencedirect.com/science/article/pii/S1047320318303493,https://doi.org/10.1016/j.jvcir.2018.12.023,1047-3203,2019,334--344,58,Journal of Visual Communication and Image Representation,High-capacity reversible data hiding in encrypted images based on extended run-length coding and block-based MSB plane rearrangement,article,CHEN2019334
"Active defense technology is very important in intelligent systems and video surveillance. In some important fields, active defense system can effectively find intruders. Many intelligent video surveillance systems were proposed in recent years. They achieved good performance to some extent. Since power station is an important field, it is important to develop an intelligent video surveillance. Considering that detecting the whole surveillance video is time consumption and computation. So in this paper, we propose an active defense system to find intruders automatically. First, a key frame selection algorithm based on adaptive features is presented to select key frames. These key frames can cover the main content of videos and detecting these key frames can also reduce time consumption and computation. Then, a probabilistic model is proposed to learn the training data distribution. Finally, our system can achieve active defense based on probabilistic model. Experimental results show that our active defense system can achieve finding intruders effectively.","Intelligent video surveillance, Active defense technology, Key frames",Jinghui Fang and Weijie Qian and Zhijun Zhao and Yiyang Yao and Zhen Wen,https://www.sciencedirect.com/science/article/pii/S1047320319300033,https://doi.org/10.1016/j.jvcir.2019.01.003,1047-3203,2019,33--37,60,Journal of Visual Communication and Image Representation,Adaptively feature learning for effective power defense,article,FANG201933
"Visual perception is an important way for organisms to perceive the external world. Simulation of visual cognitive process can enhance the cognitive ability of machine vision. Therefore, how to simulate visual perception system and make computer have a high world understanding ability is a hotspot of current neurocomputing. Based on the information processing mechanism of visual perception system, this paper establishes a neural computing model based on visual perception mechanism. The simulation results of standard face database and natural landscape images show that the proposed method can recognize face samples better when other noise samples are added to the face image samples. In landscape contour fitting simulation, the results show that although this method has little advantage for large contour image recognition, but for small contour recognition, this method is obviously superior to other methods.","Neural computing, Visual perception, Contour detection, Face recognition",Hengqing Ge and Haichun Yu,https://www.sciencedirect.com/science/article/pii/S1047320319300276,https://doi.org/10.1016/j.jvcir.2019.01.020,1047-3203,2019,309--315,59,Journal of Visual Communication and Image Representation,The application and design of neural computation in visual perception,article,GE2019309
"Deep multimodal hashing has received increasing research attention in recent years due to its superior performance for large-scale multimedia retrieval. However, limited e orts have been made to explore the complex multilevel semantic structure for deep multimodal hashing. In this paper, we propose a novel deep multimodal hashing method, termed as Deep Hashing with Multilevel Similarity Learning (DHMSL), for learning compact and discriminative hash codes, which explores multilevel semantic similarity correlations of multimedia data. In DHMSL, multilevel similarity correlation is explored to learn the unified binary hash codes by exploiting the local structure and semantic label information simultaneously. Meanwhile, the bit balance and quantization constraints are taken into account to further make the unified hash codes compact. With the unified binary codes learned, two deep neural networks are jointly trained to simultaneously learn feature representations and two sets of nonlinear hash functions. Specifically, the well-designed loss functions are introduced to minimize the prediction errors of the feature representations as well as the errors between the unified binary codes and outputs of the networks. Extensive experiments on two widely-used multimodal datasets demonstrate that the proposed method can achieve the state-of-the-art performance for both image-query-text and text-query-image tasks.","Multimedia retrieval, Deep neural networks, Hashing and multilevel similarity correlation measurement",Qiuli Liu and Lu Jin and Zechao Li and Jinhui Tang,https://www.sciencedirect.com/science/article/pii/S1047320318302840,https://doi.org/10.1016/j.jvcir.2018.11.011,1047-3203,2019,150--158,59,Journal of Visual Communication and Image Representation,Multimedia retrieval by deep hashing with multilevel similarity learning,article,LIU2019150
"While deep neural networks have recently achieved promising results on the image captioning task, they do not explicitly use the structural visual and textual knowledge within an image. In this work, we propose the Scene Graph Captioner (SGC) framework for the image captioning task, which captures the comprehensive structural semantic of visual scene by explicitly modeling objects, attributes of objects, and relationships between objects. Firstly, we develop an approach to generate the scene graph by learning individual modules on the large object, attribute and relationship datasets. Then, SGC incorporates high-level graph information and visual attention information into a deep captioning framework. Specifically, we propose a novel framework to embed a scene graph into the structural representation, which captures the semantic concepts and the graph topology. Further, we develop the scene-graph-driven method to generate the attention graph by exploiting high internal homogeneity and external inhomogeneity among the nodes in the scene graph. Finally, a LSTM-based framework translates these information into text. We evaluate the proposed framework on a held-out MSCOCO dataset.","Image captioning, Scene graph, Structural representation, Attention",Ning Xu and An-An Liu and Jing Liu and Weizhi Nie and Yuting Su,https://www.sciencedirect.com/science/article/pii/S1047320318303535,https://doi.org/10.1016/j.jvcir.2018.12.027,1047-3203,2019,477--485,58,Journal of Visual Communication and Image Representation,Scene graph captioner: Image captioning based on structural visual representation,article,XU2019477
"Most state-of-the-art binary image steganography methods depend on the content of the image to determine where to embed secret messages, which is capacity-limited and indicates that their distortion measurement may be not precise enough. In this paper, we propose a kind of distortion measurement that is not only based on the discrimination effects after flipping the pixels but also depends on the visual effects of flipping corresponding pixels, which is called joint distortion measurement. Instead of selecting suitable position to embed secret messages, we then employ the syndrome-trellis code to minimize the embedding distortion and get messages embedded. And experimental results have demonstrated that the proposed distortion measurement has higher performance and the steganography scheme can achieve stronger statistical security with high capacity and image quality.","Binary image steganography, Distortion measurement, Local texture pattern",Junhong Zhang and Wei Lu and Xiaolin Yin and Wanteng Liu and Yuileong Yeung,https://www.sciencedirect.com/science/article/pii/S1047320318303651,https://doi.org/10.1016/j.jvcir.2018.12.038,1047-3203,2019,600--605,58,Journal of Visual Communication and Image Representation,Binary image steganography based on joint distortion measurement,article,ZHANG2019600
"With the development of the game industry, and the new electronic consumption chain to promote the development of the game industry model continues to strengthen. Therefore, it is more necessary to make good use of good technology to do a good job of games. From the momentum of the development of mobile terminal games in recent years, the game industry has become a new economic growth point in China's economic transformation. With the development of computer and Internet, graphics and image processing technology has entered another unprecedented stage. In recent years, different image processing techniques have been used to process and analyze the game interface. Appropriate image processing methods include image enhancement, image binarization, image edge detection and image feature extraction. In this paper, by changing the mapping function from priority to probability and comparing the single mapping function of the previous algorithm, the mapping function of playback learning with higher probability of the important priority playback unit is found. In the experiment, the intuitive model strategy analysis of the improved algorithm is carried out firstly. Then the choice of CNN network layer structure, cost function analysis, efficiency analysis and game score comparison of each algorithm are carried out. Finally, the test results show that the new algorithm in this paper can make more effective decisions in video games and achieve the goal of winning higher scores and spending less time.","Image processing, Game animation, Visual expression, Human-machine interface",Kun Sui and Won-Hyung Lee,https://www.sciencedirect.com/science/article/pii/S1047320318303377,https://doi.org/10.1016/j.jvcir.2018.12.011,1047-3203,2019,94--100,60,Journal of Visual Communication and Image Representation,Image processing analysis and research based on game animation design,article,SUI201994
"Discriminative Correlation Filters (DCF) have achieved enormous popularity in the tracking community. Generally, DCF based trackers assume that the target can be well shaped by an axis-aligned bounding box. Therefore, in terms of irregularly shaped objects, the learned correlation filter is unavoidably deteriorated by the background pixels inside the bounding box. To tackle this problem, we propose Target-Aware Correlation Filters (TACF) for visual tracking. A target likelihood map is introduced to impose discriminative weight on filter values according to the probability of this location belonging to the foreground target. According to the TACF formulation, we further propose an optimization strategy based on the Preconditioned Conjugate Gradient method for efficient filter learning. With hand-crafted features (HOG), our approach achieves state-of-the-art performance (62.8% AUC) on OTB100 while running in real-time (24 fps) on a single CPU. With shallow convolutional features, our approach achieves 66.7% AUC on OTB100 and the top rank in EAO on the VOT2016 challenge.","Correlation filter, Target likelihood map, Visual tracking",Dongdong Li and Gongjian Wen and Yangliu Kuai and Jingjing Xiao and Fatih Porikli,https://www.sciencedirect.com/science/article/pii/S1047320318303134,https://doi.org/10.1016/j.jvcir.2018.11.036,1047-3203,2019,149--159,58,Journal of Visual Communication and Image Representation,Learning target-aware correlation filters for visual tracking,article,LI2019149
"The plantar pressure image is an important tool for gait analysis. It has important applications in evaluating the recovery of stroke patients after operation and formulating the rehabilitation training program. It is one of the key technologies of gait analysis to extract foot feature parameters from static/dynamic plantar pressure images. This article deals with the noise in the original image through the piecewise linear grayscale transformation, the time domain mean filter and the maximum value filter, then determine the position of the feet in the image by the foot localization algorithm based on the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) and the K-means clustering method. Finally, the plantar pressure feature parameters were extracted according to the positioned images. Based on the above feature parameter extraction algorithm, the plantar pressure feature parameters of 20 healthy subjects and 20â¯S patients with relative recovery period (2â6â¯months after the onset) were compared, showing a statistically significant difference (Pâ¯<â¯0.001). Based on the above data, gait characteristics of stroke patients were further analyzed.","Plantar pressure, Feature extraction, Image denoising, Clustering analysis, Gait analysis",Mo Wang and Xin'an Wang and Zhuochen Fan and Fei Chen and Sixu Zhang and Chen Peng,https://www.sciencedirect.com/science/article/pii/S1047320318303419,https://doi.org/10.1016/j.jvcir.2018.12.017,1047-3203,2019,525--531,58,Journal of Visual Communication and Image Representation,Research on feature extraction algorithm for plantar pressure image and gait analysis in stroke patients,article,WANG2019525
"This paper focuses on the issue of recognition of facial emotion expressions in video sequences and proposes an integrated framework of two networks: a local network, and a global network, which are based on local enhanced motion history image (LEMHI) and CNN-LSTM cascaded networks respectively. In the local network, frames from unrecognized video are aggregated into a single frame by a novel method, LEMHI. This approach improves MHI by using detected human facial landmarks as attention areas to boost local value in difference image calculation, so that the action of crucial facial unit can be captured effectively. Then this single frame will be fed into a CNN network for prediction. On the other hand, an improved CNN-LSTM model is used as a global feature extractor and classifier for video facial emotion recognition in the global network. Finally, a random search weighted summation strategy is conducted as late-fusion fashion to final predication. Our work also offers an insight into networks and visible feature maps from each layer of CNN to decipher which portions of the face influence the networksâ predictions. Experiments on the AFEW, CK+ and MMI datasets using subject-independent validation scheme demonstrate that the integrated framework of two networks achieves a better performance than using individual network separately. Compared with state-of-the-arts methods, the proposed framework demonstrates a superior performance.","Video emotion recognition, Motion history image, LSTM, Facial landmarks",Min Hu and Haowen Wang and Xiaohua Wang and Juan Yang and Ronggui Wang,https://www.sciencedirect.com/science/article/pii/S104732031830364X,https://doi.org/10.1016/j.jvcir.2018.12.039,1047-3203,2019,176--185,59,Journal of Visual Communication and Image Representation,Video facial emotion recognition based on local enhanced motion history image and CNN-CTSLSTM networks,article,HU2019176
"For the construction of image, video and text fusion quality data packet system during the whole life of complex products, a business intelligence-based logic modeling method is proposed in this paper. As the amount of Polymorphic data from multiple distributed sources continues to grow exponentially, automation tools are becoming critical to decision makers. The balanced scorecard method is used as the basis for modeling, and the traditional dimensions are modified slightly to meet the requirements of quality data management in aerospace enterprises. A data warehouse with predefined fact and dimension tables is created, and a technical solution is provided to meet the requirements and scales of enterprises. In terms of model applications, any enterprise quality manager can extract value from data in the quality data package system for improvement. Online analytical processing (OLAP) cubes support major tasks, such as key quality feature source tracing and analysis, quality issue data mining and integrated quality data delivery.","Balanced scorecard, Business intelligence, Data warehouse, Quality data package, Polymorphic data, Complex product",CuiBin Ji and Guijiang Duan and HanYong Ma and Long Zhang and HuanYun Xu,https://www.sciencedirect.com/science/article/pii/S1047320318303778,https://doi.org/10.1016/j.jvcir.2018.12.053,1047-3203,2019,439--447,59,Journal of Visual Communication and Image Representation,"Modeling of image, video and text fusion quality data packet system for aerospace complex products based on business intelligence",article,JI2019439
"Utilizing multi-channel visual features to characterize scenery images is standard for state-of-the-art scene recognition systems. However, how to encode human visual perception for scenery image modeling and how to optimally combine visual features from multiple views remains a tough challenge. In this paper, we propose a perceptual multi-view sparse learning (PMSL) framework to distinguish sceneries from different categories. Specifically, we first project regions from each scenery into the so-called perceptual space, which is established by combining human gaze behavior, color and texture. Afterward, a novel PMSL is developed which fuzes the above three visual cues into a sparse representation. PMSL can support absent channel visual features, which is frequently occurred in practical circumstances. Finally, the sparse representation from each scenery image is incorporated into an image kernel, which is further fed into a kernel SVM for scene categorization. Comprehensive experimental results on popular data sets have demonstrated the superiority of our method over well-known shallow/deep recognition models.",Scene categorization,Weibin Yin and Dongsheng Xu and Zheng Wang and Zhijun Zhao and Chao Chen and Yiyang Yao,https://www.sciencedirect.com/science/article/pii/S1047320319300021,https://doi.org/10.1016/j.jvcir.2019.01.002,1047-3203,2019,59--63,60,Journal of Visual Communication and Image Representation,Perceptually learning multi-view sparse representation for scene categorization,article,YIN201959
"MR technique is prevalent for doctor to diagnose and assess glioblastomas which are the most lethal form of brain tumors. Although Convolutional Neural Networks (CNN) has been applied in automatic brain tumor segmentation and is proved useful and efficient, traditional one-pathway CNN architecture with convolutional layers and max pooling layers has limited receptive fields representing the local context information. Such mindset in traditional CNN may dismiss useful global context information. In this paper, we design a two-pathway model with average and max pooling layers in different paths. Besides, 1â¯Ãâ¯1 kernels are followed input layers to add the non-linearity dimensions of input data. Finally, we combine the CNN architecture with fully connected CRF(FCRF) as a mixture model to introduce the global context information to optimize prediction results. Our experiments proved that the mixture model improved segmentation and labeling accuracy.","MR image segmentation, Convolutional Neural Network, Fully CRF",Jie Chang and Luming Zhang and Naijie Gu and Xiaoci Zhang and Minquan Ye and Rongzhang Yin and Qianqian Meng,https://www.sciencedirect.com/science/article/pii/S1047320318303262,https://doi.org/10.1016/j.jvcir.2018.11.047,1047-3203,2019,316--322,58,Journal of Visual Communication and Image Representation,A mix-pooling CNN architecture with FCRF for brain tumor segmentation,article,CHANG2019316
"The development of computer technology has led to the development of face recognition technology. Nowadays, face recognition technology has been successfully applied in many fields with the help of computer technology and network technology. This paper establishes an effective face recognition model based on principal component analysis, genetic algorithm and support vector machine, in which principal component analysis is used to reduce feature dimension, genetic algorithm is used to optimize search strategy, and support vector machine is used to realize classification. Through the simulation experiment on the face database of the Institute of Technology of Chinese Academy of Sciences in 2003, the results show that the model can achieve face recognition with high efficiency, and the highest accuracy rate is 99%.","Face recognition, Genetic algorithm, Principal component analysis, Support vector machine",Hui Zhi and Sanyang Liu,https://www.sciencedirect.com/science/article/pii/S1047320318303389,https://doi.org/10.1016/j.jvcir.2018.12.012,1047-3203,2019,495--502,58,Journal of Visual Communication and Image Representation,Face recognition based on genetic algorithm,article,ZHI2019495
"Using volume rendering to generate 3D models is associated with the problem of missing features on areas of interest, which are possibly concealed by other information. This article presents a novel focus-and-context medical imaging observation system using gesture-based technique to build a touchless interactive environment. The system offers two types of medical imaging observation tool, namely, 3D section cutting tool and 3-axes cross-section synchronization tool, enabling users to quickly and easily observe tissue sections. Feature classification was achieved using region growing and size-based transfer approaches. Combined with view penetration function (cylinder and cone view penetration functions), the system allows for direct observation of hidden features. The analytical experimental results verified that the proposed system is easy to operate in a touchless environment and creates positive user experience regarding observation and interaction.","Touchless, Volume rendering, Focus and context, Visualization, Medical imaging",Pei-Ying Chiang and Chun-Chi Chen and Chih-Hsien Hsia,https://www.sciencedirect.com/science/article/pii/S1047320318303304,https://doi.org/10.1016/j.jvcir.2018.12.004,1047-3203,2019,363--373,58,Journal of Visual Communication and Image Representation,A touchless interaction interface for observing medical imaging,article,CHIANG2019363
"Deep residual networks have emerged as a leading technique showing the accuracy and excellent convergence performance. However, because of overfitting and the vanishing gradient problems, we cannot achieve better results by increasing the model parameters. In this study, we propose a novel architecture that has a multi-branch residual network. The shortcut branch of residual networks can ease off the vanishing gradient. The random function and the structure of a multi-branch network improve the fitting ability. The dropout function can weaken overfitting. The proposed method also adopts the adaptive cosine learning rate method and variate batch size to improve the test accuracy. Some experimental investigations are set up to explore the impact of the following factors on the performance: the sequence of the layers, random numbers and different batch sizes, etc. We report that the results achieve 2.63% and 14.2% Top-1 error on CIFAR-10 and CIFAR-100.","Image classification, Residual network, Overfitting, Deep leaning, Batch size, Learning rate",Yifeng Xu and Huigang Wang and Xing Liu and Weitao Sun's,https://www.sciencedirect.com/science/article/pii/S1047320319300380,https://doi.org/10.1016/j.jvcir.2019.01.030,1047-3203,2019,363--370,59,Journal of Visual Communication and Image Representation,An improved multi-branch residual network based on random multiplier and adaptive cosine learning rate method,article,XU2019363
"Accurately discriminating complicated sceneries from different categories is a useful technique in multimedia and computer vision. In this work, we propose a novel multi-view non-negative matrix factorization to detect human gaze behavior, which is subsequently integrated into an image kernel machine for scene categorization. More specifically, we first project regions from each scenery into the so-called perceptual space, which is established by combining color, texture, and semantic features. Then, a novel non-negative matrix factorization (NMF) algorithm is developed which decomposes the regionsâ feature matrix into the product of the basis matrix and the sparse codes. The sparse codes indicate the saliency level of different regions which is used to constructed gaze shifting path. Thereby, the path from each scenery is derived and further incorporated into an image kernel for scene categorization. Comprehensive experiments on six scenery data sets have demonstrated the superiority of our method over a series of recognition models.","Non-negative matrix factorization, Scene recognition, Multi-view",Jinjiang Tang and Weijie Qian and Zhijun Zhao and Weiliang Liu and Ping He,https://www.sciencedirect.com/science/article/pii/S1047320318303663,https://doi.org/10.1016/j.jvcir.2018.12.040,1047-3203,2019,9--13,59,Journal of Visual Communication and Image Representation,Multi-view non-negative matrix factorization for scene recognition,article,TANG20199
"Crowd behavior analysis has become one of the new areas of interest in the computer vision community due to the increasing demands from surveillance and security industries. It is important to meticulously understand crowd behavior to prevent any disaster and unwanted incidents such as thief, stampede and riots. For this purpose, crowd features such as density, motion and trajectory are analyzed to detect any abnormality in the crowd. Thus, this review is aimed to provide insight on several detection methods including Gaussian Mixture Model (GMM), Hidden Markov Model (HMM), Optical Flow method and Spatio-Temporal Technique (STT). Providing the latest development, the review presented the studies that are published in journals and conferences over the past 5 years.","Crowd analysis, Abnormal detection, Gaussian Mixture Model (GMM), Hidden Markov Model (HMM), Optical Flow (OF), Spatio-Temporal Technique (STT)",A.A. Afiq and M.A. Zakariya and M.N. Saad and A.A. Nurfarzana and M.H.M. Khir and A.F. Fadzil and A. Jale and W. Gunawan and Z.A.A. Izuddin and M. Faizari,https://www.sciencedirect.com/science/article/pii/S1047320318303146,https://doi.org/10.1016/j.jvcir.2018.11.035,1047-3203,2019,285--303,58,Journal of Visual Communication and Image Representation,A review on classifying abnormal behavior in crowd scene,article,AFIQ2019285
"Soil-rock mixture (S/RM) is a very complex discontinuous medium material, which is a multiphase system consisting of high strength rock blocks (Rocks), relatively soft filling components (Soils) and corresponding pores. Because the mechanical properties of various components of soil-rock mixtures under external loads are very different, and there are extremely complex interactions between them. Therefore, the mechanical properties of this geotechnical material (such as stress transfer, failure mode, crack propagation, bearing capacity, etc.) are quite different from those of homogeneous geotechnical materials, and largely depend on the internal structure characteristics of soil-rock mixtures (such as particle size composition, particle shape, particle distribution and arrangement). Due to the complexity of the model, the simulation of its meso-mechanical properties is mostly confined to the random simulation of regular blocks. In this paper, an automatic generation method of PFCâ¼2D numerical model of soil-rock mixture microstructure based on digital image processing is proposed, and the experimental simulation is carried out with matlab. Thus, the rapid, real and automatic modeling of heterogeneous material microstructure by PFCâ¼2D software is realized. The PFCâ¼2D numerical calculation model of soil-rock mixtures is established. The results show that when the stone content is 80%, the analysis should be caused by the large amount of rock, which leads to the large internal voids, and the sudden unloading between the rock and the rock during compaction and then the structural reorganization.","Soil rock mixture, PFCâ¼2D model, Particle flow simulation, Meso mechanical properties",Pengfei Shan and Xingping Lai,https://www.sciencedirect.com/science/article/pii/S1047320318303420,https://doi.org/10.1016/j.jvcir.2018.12.015,1047-3203,2019,407--415,58,Journal of Visual Communication and Image Representation,Mesoscopic structure PFCâ¼2D model of soil rock mixture based on digital image,article,SHAN2019407
