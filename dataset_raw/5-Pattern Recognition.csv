abstract,keywords,author,url,doi,issn,year,pages,volume,journal,title,ENTRYTYPE,ID,note
"Change detection between heterogeneous images has become an increasingly interesting research topic in remote sensing. The different appearances and statistics of heterogeneous images bring great challenges to this task. In this paper, we propose an unsupervised iterative structure transformation and conditional random field (IST-CRF) based multimodal change detection (MCD) method, combining an imaging modality-invariant based structure transformation method with a random filed framework specifically designed for MCD, to acquire an optimal change map within a global probabilistic model. IST-CRF first constructs graphs to represent the structures of the images, and transforms the heterogeneous images to the same differential domain by using graph based forward and backward structure transformations. Then, the change vectors are calculated to distinguish the changed and unchanged areas. Finally, in order to classify the change vectors and compute the binary change map, a CRF model is designed to fully explore the spectral-spatial information, which incorporates the change information, local spatially-adjacent neighbor information, and global spectrally-similar neighbor information with a random field framework. As the changed samples will influence the structure transformation and reduce the quality of change vectors, we use an iterative framework to propagate the CRF segmentation results back to the structure transformation process that removes the changed samples, and thus improve the accuracy of change detection. Experiments conducted on different real data sets show the effectiveness of IST-CRF. Source code of the proposed method will be made available at https://github.com/yulisun/IST-CRF.","Unsupervised change detection, KNN graph, Image transformation, Multimodal, Conditional random field",Yuli Sun and Lin Lei and Dongdong Guan and Junzheng Wu and Gangyao Kuang,https://www.sciencedirect.com/science/article/pii/S0031320322003260,https://doi.org/10.1016/j.patcog.2022.108845,0031-3203,2022,108845,131,Pattern Recognition,Iterative structure transformation and conditional random field based method for unsupervised multimodal change detection,article,SUN2022108845,
"Visual modality is one of the most dominant modalities for current continuous emotion recognition methods. Compared to which the EEG modality is relatively less sound due to its intrinsic limitation such as subject bias and low spatial resolution. This work attempts to improve the continuous prediction of the EEG modality by using the dark knowledge from the visual modality. The teacher model is built by a cascade convolutional neural network - temporal convolutional network (CNN-TCN) architecture, and the student model is built by TCNs. They are fed by video frames and EEG average band power features, respectively. Two data partitioning schemes are employed, i.e., the trial-level random shuffling (TRS) and the leave-one-subject-out (LOSO). The standalone teacher and student can produce continuous prediction superior to the baseline method, and the employment of the visual-to-EEG cross-modal KD further improves the prediction with statistical significance, i.e., p-value <0.01 for TRS and p-value <0.05 for LOSO partitioning. The saliency maps of the trained student model show that the brain areas associated with the active valence state are not located in precise brain areas. Instead, it results from synchronized activity among various brain areas. And the fast beta and gamma waves, with the frequency of 18â30Hz and 30â45Hz, contribute the most to the human emotion process compared to other bands. The code is available at https://github.com/sucv/Visual_to_EEG_Cross_Modal_KD_for_CER.","Continuous emotion recognition, Knowledge distillation, Cross-modality",Su Zhang and Chuangao Tang and Cuntai Guan,https://www.sciencedirect.com/science/article/pii/S0031320322003144,https://doi.org/10.1016/j.patcog.2022.108833,0031-3203,2022,108833,130,Pattern Recognition,Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition,article,ZHANG2022108833,
"For dimensionality reduction of HSI, many clustering-based unsupervised band selection (UBS) methods have been proposed due to their superiority of reducing the high redundancy between selected bands. However, most of these methods fail to reflect the data structure of HSI, leading to inconsistent results of band selection. To tackle this particular issue, we have proposed a novel hyperbolic clustering-based band hierarchy (HCBH) to fully represent the underlying spectral structure and obtain a more consistent band selection. With the proposed adaptive hyperbolic clustering, the performance can be effectively improved with the aid of geometrical information. By introducing a cluster-centre based ranking metric, the desired band subset can be naturally obtained during the clustering process. Experimental results on three popularly used datasets have validated the superior performance of the proposed approach, which outperforms a few state-of-the-art (SOTA) UBS approaches.","Hyperspectral image, Unsupervised band selection, Hyperbolic space clustering, Hierarchical clustering",He Sun and Lei Zhang and Jinchang Ren and Hua Huang,https://www.sciencedirect.com/science/article/pii/S0031320322002692,https://doi.org/10.1016/j.patcog.2022.108788,0031-3203,2022,108788,130,Pattern Recognition,Novel hyperbolic clustering-based band hierarchy (HCBH) for effective unsupervised band selection of hyperspectral images,article,SUN2022108788,
"Sequence representation, which is aimed at embedding sequentially symbolic data in a real space, is a foundational task in sequence pattern recognition. It is a difficult problem due to the challenges entailed in learning the intrinsic structural features within sequences in small sample size cases, in an unsupervised way. In this paper, we propose to represent each symbolic sequence by its transition probability distribution over discriminating topics, formalized by a set of optimized Hidden Markov Model (HMM) states shared by all sequences. An efficient method, called Markovian state clustering with hierarchical model selection, is proposed to optimize the Markovian states and to adaptively determine the number of topics. The proposed method is experimentally evaluated on human activity recognition and protein recognition, and results obtained demonstrate its effectiveness and efficiency.","Sequence representation, Hidden Markov model, State clustering, Hierarchical model selection, Activity recognition",Lifei Chen and Haiyan Wu and Wenxuan Kang and Shengrui Wang,https://www.sciencedirect.com/science/article/pii/S0031320322003302,https://doi.org/10.1016/j.patcog.2022.108849,0031-3203,2022,108849,131,Pattern Recognition,Symbolic sequence representation with Markovian state optimization,article,CHEN2022108849,
"We propose a first-order stochastic optimization algorithm incorporating adaptive regularization for pattern recognition problems in deep learning framework. The adaptive regularization is imposed by stochastic process in determining batch size for each model parameter at each optimization iteration. The stochastic batch size is determined by the update probability of each parameter following a distribution of gradient norms in consideration of their local and global properties in the neural network architecture where the range of gradient norms may vary within and across layers. We empirically demonstrate the effectiveness of our algorithm using an image classification task based on conventional network models applied to commonly used benchmark datasets. The quantitative evaluation indicates that our algorithm outperforms the state-of-the-art optimization algorithms in generalization while providing less sensitivity to the selection of batch size which often plays a critical role in optimization, thus achieving more robustness to the selection of regularity.","Deep network optimization, Adaptive regularization, Stochastic gradient descent, Adaptive mini-batch size",Kensuke Nakamura and Stefano Soatto and Byung-Woo Hong,https://www.sciencedirect.com/science/article/pii/S0031320322002576,https://doi.org/10.1016/j.patcog.2022.108776,0031-3203,2022,108776,129,Pattern Recognition,Stochastic batch size for adaptive regularization in deep network optimization,article,NAKAMURA2022108776,
"Data sources for medical image segmentation can be quite extensive, and models trained with data from a source domain may perform poorly on data from the target domain owing to domain shift issues. To overcome the impact of domain shift, we propose a novel meta-learning-based multi-source domain adaptation framework for medical image segmentation. Specifically, we designed a domain discriminator module to produce category prediction over the latent features, and an image reconstruction module to reconstruct the foreground and background of the target domain image separately. Furthermore, we constructed a large-scale multi-modal prostate dataset, which contained 495,902 magnetic resonance images of 419 cases, with prostate and lesion masks, as well as diagnostic descriptions for each patient. We evaluated our proposed method through extensive experiments using the proposed and the benchmark datasets. Experimental results show that our model achieves better segmentation and generalization performance compared to state-of-the-art approaches.","Latent space regularization, Meta learning, Domain generalization, Domain discriminator, Multi-source domain adaptation",Bo Zhang and Yunpeng Tan and Hui Wang and Zheng Zhang and Xiuzhuang Zhou and Jingyun Wu and Yue Mi and Haiwen Huang and Wendong Wang,https://www.sciencedirect.com/science/article/pii/S0031320322003028,https://doi.org/10.1016/j.patcog.2022.108821,0031-3203,2022,108821,130,Pattern Recognition,LSRML: A latent space regularization based meta-learning framework for MR image segmentation,article,ZHANG2022108821,
"Few-shot fine-grained recognition (FS-FGR) aims to distinguish several highly similar objects from different sub-categories with limited supervision. However, traditional few-shot learning solutions typically exploit image-level features and are committed to capturing global silhouettes while accidentally ignore to exploring local details, resulting in an inevitable problem of inconspicuous but distinguishable information loss. Thus, how to effectively address the fine-grained recognition issue given limited samples still remains a major challenging. In this article, we tend to propose an effective bidirectional pyramid architecture to enhance internal representations of features to cater to fine-grained image recognition task in the few-shot learning scenario. Specifically, we deploy a multi-scale feature pyramid and a multi-level attention pyramid on the backbone network, and progressively aggregated features from different granular spaces via both of them. We then further present an attention-guided refinement strategy in collaboration with a multi-level attention pyramid to reduce the uncertainty brought by backgrounds conditioned by limited samples. In addition, the proposed method is trained with the meta-learning framework in an end-to-end fashion without any extra supervision. Extensive experimental results on four challenging and widely-used fine-grained benchmarks show that the proposed method performs favorably against state-of-the-arts, especially in the one-shot scenarios.","Few-shot learning, Fine-grained recognition, Weakly-supervised learning",Hao Tang and Chengcheng Yuan and Zechao Li and Jinhui Tang,https://www.sciencedirect.com/science/article/pii/S0031320322002734,https://doi.org/10.1016/j.patcog.2022.108792,0031-3203,2022,108792,130,Pattern Recognition,Learning attention-guided pyramidal features for few-shot fine-grained recognition,article,TANG2022108792,
"Unsupervised retrieval of image features is vital for many computer vision tasks where the annotation is missing or scarce. In this work, we propose a new unsupervised approach to detect the landmarks in images, validating it on the popular task of human face key-points extraction. The method is based on the idea of auto-encoding the wanted landmarks in the latent space while discarding the non-essential information (and effectively preserving the interpretability). The interpretable latent space representation (the bottleneck containing nothing but the wanted key-points) is achieved by a new two-step regularization approach. The first regularization step evaluates transport distance from a given set of landmarks to some average value (the barycenter by Wasserstein distance). The second regularization step controls deviations from the barycenter by applying random geometric deformations synchronously to the initial image and to the encoded landmarks. We demonstrate the effectiveness of the approach both in unsupervised and semi-supervised training scenarios using 300-W, CelebA, and MAFL datasets. The proposed regularization paradigm is shown to prevent overfitting, and the detection quality is shown to improve beyond the state-of-the-art face models.",,Iaroslav Bespalov and Nazar Buzun and Dmitry V. Dylov,https://www.sciencedirect.com/science/article/pii/S0031320322002977,https://doi.org/10.1016/j.patcog.2022.108816,0031-3203,2022,108816,131,Pattern Recognition,BRULÃ: Barycenter-Regularized Unsupervised Landmark Extraction,article,BESPALOV2022108816,
"Clustering methods are based on the computations of both the distances between every pair of the n observations in a multivariate dataset as well as the distances between every pair of clusters in the dataset. The clusters can have different locations and varying elliptical shapes and directions. Numerous methods have been proposed in the literature for computing both of these two types of distances. The contributions of this paper are two folds. First, we propose a new elliptical distance between pairs of clusters in a dataset with different cluster centers and elliptical shapes and directions, Second, we proved analytically that the Ward distance and the Euclidean distance are equivalent. We propose a new classical method for computing the distance between a pair of clusters in the dataset. It is the only distance that does not assume spherical clusters. The proposed classical distances could also be made robust by replacing estimates of location and scale by their respective robust estimators. The proposed distance has a number of advantages including simplicity, interpretability, computational efficiency as well as the ability to accurately capture both the variability of the cluster centers as well as the variability of shapes and directions of their respective covariance matrices. The method is also illustrated by several motivating examples that demonstrate the need of the new proposed distance. The superiority of the proposed method is also demonstrated by application to real-life as well as challenging synthetic data.","Clustering methods, Complete linkage, Elliptical distance, Euclidean distance, Hamming distance, Hierarchical clustering, Iris data, -Means clustering, Manhattan distance, Single linkage, Robust estimation, Ward method",Ali S. Hadi,https://www.sciencedirect.com/science/article/pii/S0031320322002618,https://doi.org/10.1016/j.patcog.2022.108780,0031-3203,2022,108780,129,Pattern Recognition,"A new distance between multivariate clusters of varying locations, elliptical shapes, and directions",article,HADI2022108780,
"High-dimension and low-sample-size (HDLSS) data sets have posed great challenges to many machine learning methods. To deal with practical HDLSS problems, development of new classification techniques is highly desired. After the cause of the over-fitting phenomenon is identified, a new classification criterion for HDLSS data sets, termed tolerance similarity, is proposed to emphasize maximization of within-class variance on the premise of class separability. Leveraging on this criterion, a novel linear binary classifier, termed No-separated Data Maximum Dispersion classifier (NPDMD), is designed. The main idea of the NPDMD is to spread samples of two classes in a large interval in the respective positive or negative space along the projecting direction when the distance between the projection means for two classes is large enough. The salient features of the proposed NPDMD are: (1) The NPDMD operates well on HDLSS data sets; (2) The NPDMD solves the objective function in the entire feature space to avoid the data-piling phenomenon. (3) The NPDMD leverages on the low-rank property of the covariance matrix for HDLSS data sets to accelerate the computation speed. (4) The NPDMD is suitable for different real-word applications. (5) The NPDMD can be implemented readily using Quadratic Programming. Not only theoretical properties of the NPDMD have been derived, but also a series of evaluations have been conducted on one simulated and six real-world benchmark data sets, including face classification and mRNA classification. Experimental results and comprehensive studies demonstrate the superiority of the NPDMD in terms of correct classification rate, mean within-group correct classification rate and the area under the ROC curve.","Binary linear classifier, Quadratic programming, Data piling, Covariance matrix",Liran Shen and Meng Joo Er and Qingbo Yin,https://www.sciencedirect.com/science/article/pii/S0031320322003090,https://doi.org/10.1016/j.patcog.2022.108828,0031-3203,2022,108828,130,Pattern Recognition,Classification for high-dimension low-sample size data,article,SHEN2022108828,
"In the era of User Generated Content (UGC), authors (IDs) of texts widely exist and play a key role in determining the topic categories of texts. Existing text clustering efforts are mainly attributed to utilizing textual information, but the effect of authors on text clustering remains largely underexplored. To mitigate this issue, we propose a novel Contrastive Author-aware Text clustering approach, dubbed as CAT. CAT injects author information not only in characterizing texts through representations but also in pushing or pulling text representations of different authors through contrastive learning, which is rarely adopted by text clustering. Specifically, the developed contrastive learning method conducts both cluster-instance contrast by the text representation augmentation and instance-instance contrast by the multi-view representations. We perform comprehensive experiments on three public datasets, demonstrating that CAT largely outperforms strong competitive text clustering baselines and validating the effectiveness of the CATâs main components.","Text clustering, Contrastive learning, Representation learning",Xudong Tang and Chao Dong and Wei Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322002680,https://doi.org/10.1016/j.patcog.2022.108787,0031-3203,2022,108787,130,Pattern Recognition,Contrastive author-aware text clustering,article,TANG2022108787,
"Face manipulation techniques, especially DeepFake techniques, are causing severe social concerns and security problems. When faced with skewed data distributions such as those found in the real world, existing DeepFake detection methods exhibit significantly degraded performance, especially the AUC score. In this paper, we focus on DeepFake detection in real-world situations. We propose a dual-level collaborative framework to detect frame-level and video-level forgeries simultaneously with a joint loss function to optimize both the AUC score and error rate at the same time. Our experiments indicate that the AUC loss boosts imbalanced learning performance and outperforms focal loss, a state-of-the-art loss function to address imbalanced data. In addition, our multitask structure enables mutual reinforcement of frame-level and video-level detection and achieves outstanding performance in imbalanced learning. Our proposed method is also more robust to video quality variations and shows better generalization ability in cross-dataset evaluations than existing DeepFake detection methods. Our implementation is available online at https://github.com/PWB97/Deepfake-detection.","DeepFake detection, Multitask learning, Imbalanced learning, AUC optimization",Wenbo Pu and Jing Hu and Xin Wang and Yuezun Li and Shu Hu and Bin Zhu and Rui Song and Qi Song and Xi Wu and Siwei Lyu,https://www.sciencedirect.com/science/article/pii/S0031320322003132,https://doi.org/10.1016/j.patcog.2022.108832,0031-3203,2022,108832,130,Pattern Recognition,Learning a deep dual-level network for robust DeepFake detection,article,PU2022108832,
"Few-shot learning (FSL) aims at fast adaptation to novel classes with few training samples. Among FSL methods, meta-learning and transfer learning-based methods are the most powerful ones. However, most of them rely to some extent on cross-entropy loss, which leads to representations that are overly concerned with the classes already seen, and in turn leads to sub-optimal generalization on novel classes. In this study, we are inspired by meta-learning and transfer learning-based methods and believe good feature representations are vital for FSL. To this end, we propose a new multi-granularity episodic contrastive learning method (MGECL) that introduces contrastive learning into the episode training process. In particular, by enforcing our proposed contrastive loss on both class and instance granularities, the model is able to extract category-independent discriminative patterns and learn richer and more transferable feature representations. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance on three popular few-shot benchmarks. Our code is available at https://github.com/z1358/MGECL_PR.","Multi-granularity computing, Episodic contrastive learning, Few-shot learning, Deep learning",Pengfei Zhu and Zhilin Zhu and Yu Wang and Jinglin Zhang and Shuai Zhao,https://www.sciencedirect.com/science/article/pii/S0031320322003016,https://doi.org/10.1016/j.patcog.2022.108820,0031-3203,2022,108820,131,Pattern Recognition,Multi-granularity episodic contrastive learning for few-shot learning,article,ZHU2022108820,
"Feature fusion has been widely used for improving the tracking performance. However, how to effectively analyze the characteristics of different visual features to realize dynamical feature fusion is still a challenging task. In this paper, we propose a spatial-temporal context-based dynamic feature fusion method (STCDFF) with the correlation filters framework for object tracking. The proposed STCDFF method exploits spatial-temporal context to deeply analyze the characteristics of multiple visual features (e.g., HOG, Color-Names and CNN features) to perform feature fusion. On the one hand, spatial context is employed to evaluate the discriminative ability of different features to distinguish the target object from the background. On the other hand, temporal context is utilized to consider the representative ability of different features to capture significant appearance changes of the target object. The weight of a feature is decided by both its discriminative ability and representative ability. By exploring spatial-temporal context for feature fusion, the STCDFF method can fully utilize the strengths of different features to handle complex appearance changes and background clutters to achieve better performance. Extensive experiments on multiple object tracking datasets prove that our STCDFF method performs competitively against several popular tracking methods.","Object tracking, Dynamic feature fusion, Spatial-temporal context, Correlation filters framework",Ke Nai and Zhiyong Li and Haidong Wang,https://www.sciencedirect.com/science/article/pii/S0031320322002564,https://doi.org/10.1016/j.patcog.2022.108775,0031-3203,2022,108775,130,Pattern Recognition,Dynamic feature fusion with spatial-temporal context for robust object tracking,article,NAI2022108775,
"Preserving the intrinsic structure of data is very important for unsupervised dimensionality reduction. For structure preserving, graph embedding technique is widely considered. However, most of the existing unsupervised graph embedding based methods cannot effectively preserve the intrinsic structure of data since these methods either use the constant graph or only explore the geometric structure based on the distance information or representation information. To solve this problem, a novel method, called locality preserving projection with symmetric graph embedding (LPP_SGE), is proposed. LPP_SGE introduces a novel adaptive graph learning model and can obtain the intrinsic graph and projection in a unified framework by fully exploring the representation information and distance information of the original data. Different from the existing works which generally introduce no less than two constraints to capture the representation information and distance information, LPP_SGE can simultaneously capture the above two kinds of structure information in one term. Moreover, LPP_SGE introduces an âl2,1â norm based projection constraint to select the most discriminative features from the complex data for dimensionality reduction, such that the robustness is enhanced. Experimental results on four databases and two kinds of noisy databases show that LPP_SGE performs better than many well-known methods.","Dimensionality reduction, Feature extraction, Graph embedding, Unsupervised learning",Xiaohuan Lu and Jiang Long and Jie Wen and Lunke Fei and Bob Zhang and Yong Xu,https://www.sciencedirect.com/science/article/pii/S0031320322003259,https://doi.org/10.1016/j.patcog.2022.108844,0031-3203,2022,108844,131,Pattern Recognition,Locality preserving projection with symmetric graph embedding for unsupervised dimensionality reduction,article,LU2022108844,
"Multi-view data clustering based on Non-negative Matrix Factorization (NMF) has been commonly used for pattern recognition by grouping multi-view high-dimensional data by projecting it to a lower-order dimensional space. However, the NMF framework fails to learn the accurate lower-order representation of the input data if it exhibits complex and non-linear relationships. This paper proposes a deep non-negative matrix factorization-based framework for effective multi-view data clustering by uncovering both the non-linear relationships and the intrinsic components of the data. Both the consensus and complementary information present in multiple views are sufficiently learned in the proposed framework with the effective use of constraints such as normalized cut-type and orthogonal. The optimal manifold of multi-view data is effectively incorporated in all layers of the framework. Extensive experimental results show the proposed method outperforms state-of-the-art multi-view matrix factorization-based methods.","Multi-view data/clustering, Manifold learning, Non-negative Matrix Factorization (NMF), Deep Matrix Factorization (DMF), Deep Non-negative Matrix Factorization (Deep-NMF)",Khanh Luong and Richi Nayak and Thirunavukarasu Balasubramaniam and Md Abul Bashar,https://www.sciencedirect.com/science/article/pii/S0031320322002965,https://doi.org/10.1016/j.patcog.2022.108815,0031-3203,2022,108815,131,Pattern Recognition,Multi-layer manifold learning for deep non-negative matrix factorization-based multi-view clustering,article,LUONG2022108815,
"Website fingerprinting (WF) attack aims to identify which website a user is visiting from the traffic data patterns. Whilst existing methods assume many training samples, we investigate a more realistic and scalable few-shot WF attack with only a few labeled training samples per website. To solve this problem, we introduce a novel Meta-Bias Learning (MBL) method for few-shot WF learning. Taking the meta-learning strategy, MBLÂ simulates and optimizes the target tasks. Moreover, a new model parameter factorization idea is introduced for facilitating meta-training with superior task adaptation. Expensive experiments show that our MBL outperforms significantly existing hand-crafted feature and deep learning based alternatives in both closed-world and open-world attack scenarios, at the absence and presence of defense.","User privacy, Internet anonymity, Data traffic, Website fingerprinting, Deep learning, Neural network, Few-shot learning, Meta-learning, Parameter factorization",Mantun Chen and Yongjun Wang and Xiatian Zhu,https://www.sciencedirect.com/science/article/pii/S0031320322002205,https://doi.org/10.1016/j.patcog.2022.108739,0031-3203,2022,108739,130,Pattern Recognition,Few-shot Website Fingerprinting attack with Meta-Bias Learning,article,CHEN2022108739,
"Subspace clustering aims to fit each category of data points by learning an underlying subspace and then conduct clustering according to the learned subspace. Ideally, the learned subspace is expected to be block diagonal such that the similarities between clusters are zeros. In this paper, we provide the explicit theoretical connection between spectral clustering and the subspace clustering based on block diagonal representation. We propose Enforced Block Diagonal Subspace Clustering (EBDSC) and show that the spectral clustering with the Radial Basis Function kernel can be regarded as EBDSC. Compared with the exiting subspace clustering methods, an analytical, nonnegative and symmetrical solution can be obtained by EBDSC. An important difference with respect to the existing ones is that our model is a more general case. EBDSC directly uses the obtained solution as the similarity matrix, which can avoid the complex computation of the optimization program. Then the solution obtained by the proposed method can be used for the final clustering. Finally, we provide the experimental analysis to show the efficiency and effectiveness of our method on the synthetic data and several benchmark data sets in terms of different metrics.","Subspace clustering, General form, Analytical, Nonnegative, Symmetrical solution",Yalan Qin and Hanzhou Wu and Jian Zhao and Guorui Feng,https://www.sciencedirect.com/science/article/pii/S0031320322002722,https://doi.org/10.1016/j.patcog.2022.108791,0031-3203,2022,108791,130,Pattern Recognition,Enforced block diagonal subspace clustering with closed form solution,article,QIN2022108791,
"Unsupervised person re-identification (Re-ID) is to retrieve pedestrians from different camera views without supervision information. State-of-the-art methods are usually built upon training a convolution neural network with pseudo labels generated by clustering. Unfortunately, the pseudo labels are highly unbalanced and heavily noisy, carrying ineffective or even erroneous supervision information. To address these deficiencies, we present an effective clustering and reorganization approach, called Cluster Consolidation, which aims to separate a small proportion of unreliable data points from each cluster. This approach benefits to improve the quality of the pseudo labels, but also yields more tiny clusters. Thus, we further propose a Cluster Adaptive Balancing (CAB) loss to effectively train the network with the imbalance pseudo labels, where our CAB loss is able to automatically balance the importance of each cluster. We conduct extensive experiments on widely used person Re-ID benchmark datasets and demonstrate the effectiveness of our proposals.","Unsupervised person re-identification, Cluster consolidation, Cluster adaptive balancing loss, Long-tail problem",Mingkun Li and He Sun and Chaoqun Lin and Chun-Guang Li and Jun Guo,https://www.sciencedirect.com/science/article/pii/S0031320322002448,https://doi.org/10.1016/j.patcog.2022.108763,0031-3203,2022,108763,129,Pattern Recognition,The devil in the tail: Cluster consolidation plus cluster adaptive balancing loss for unsupervised person re-identification,article,LI2022108763,
"Discovering unknown objects from visual information as curiosity is highly demanded for autonomous exploration in underwater environment. In this research, we propose an end-to-end deep neural network for anomaly detection in the highly dynamic unstructured underwater background faced by a moving robot. A novel patch-level autoencoder combined with a context-enhanced autoregressive network is introduced to differentiate abnormal patterns (unknowns) from normal ones (knowns) in fine-scale regions. The autoencoder and autoregressive network share the same encoder to extract latent features. The autoregressive branch learns semantic dependence based on conditional probability to identify anomaly in a latent feature space. The overall anomaly score is weighted by both image reconstruction loss and feature similarity loss. The model outperforms state-of-the-art anomaly detection, demonstrated on the benchmark dataset CIFAR-10. Average discrimination performance AUROC improved 2.18%, and inception distance between normal and anomalous classes improved 9.33% in Z-score. The network has been tested using three underwater datasets from underwater simulation, a real-world undersea video and public SUIM data. The AUROC accuracy improved 6.36%, 32.45% and 40.17% respectively by using the proposed patch learning paradigm. It is the first report on unknown detection as navigation clues for curiosity-driven autonomous underwater exploration.","Anomaly detection, Learning unknown objects, Deep learning autoencoder, Autonomous underwater robotics",Yang Zhou and Baihua Li and Jiangtao Wang and Emanuele Rocco and Qinggang Meng,https://www.sciencedirect.com/science/article/pii/S0031320322003417,https://doi.org/10.1016/j.patcog.2022.108860,0031-3203,2022,108860,131,Pattern Recognition,Discovering unknowns: Context-enhanced anomaly detection for curiosity-driven autonomous underwater exploration,article,ZHOU2022108860,
"Although quantum neural networks (QNNs) have shown promising results in solving simple machine learning tasks recently, the behavior of QNNs in binary pattern classification is still underexplored. In this work, we find that QNNs have an Achillesâ heel in binary pattern classification. To illustrate this point, we provide a theoretical insight into the properties of QNNs by presenting and analyzing a new form of symmetry embedded in a family of QNNs with full entanglement, which we term negational symmetry. Due to negational symmetry, QNNs can not differentiate between a quantum binary signal and its negational counterpart. We empirically evaluate the negational symmetry of QNNs in binary pattern classification tasks using Googleâs quantum computing framework. Both theoretical and experimental results suggest that negational symmetry is a fundamental property of QNNs, which is not shared by classical models. Our findings also imply that negational symmetry is a double-edged sword in practical quantum applications.","Deep learning, Quantum machine learning, Binary pattern classification, Representation learning, Symmetry",Nanqing Dong and Michael Kampffmeyer and Irina Voiculescu and Eric Xing,https://www.sciencedirect.com/science/article/pii/S003132032200231X,https://doi.org/10.1016/j.patcog.2022.108750,0031-3203,2022,108750,129,Pattern Recognition,Negational symmetry of quantum neural networks for binary pattern classification,article,DONG2022108750,
"Proximal support vector machine via generalized eigenvalues (GEPSVM) is one of the most successful methods for classification problems. However, GEPSVM is vulnerable to outliers since it learns classifiers based on the squared L2-norm distance without a specific strategy to deal with the outliers. Motivated by existing studies that improve the robustness of GEPSVM via the L1-norm distance or not-squared L2-norm distance formulation, a novel GEPSVM formulation that minimizes the p-order of L2-norm distance is proposed, namely, L2,p-GEPSVM. This formulation weakens the negative effects of both light and heavy outliers in the data. An iterative algorithm is designed to solve the general L2,p-norm distance minimization problems and rigorously prove its convergence. In addition, we adjust the parameters of L2,p-GEPSVM to balance the accuracy and training time. This is especially useful for larger datasets. Extensive results indicate that the L2,p-GEPSVM improves the classification performance and robustness in various experimental settings.","Classification problem, Distance metric learning, Outliers and noises, Robust L-GEPSVM method, Squared L-norm distance",He Yan and Liyong Fu and Tian'an Zhang and Jun Hu and Qiaolin Ye and Yong Qi and Dong-Jun Yu,https://www.sciencedirect.com/science/article/pii/S0031320322002606,https://doi.org/10.1016/j.patcog.2022.108779,0031-3203,2022,108779,129,Pattern Recognition,Robust distance metric optimization driven GEPSVM classifier for pattern classification,article,YAN2022108779,
"Feature selection is a crucial preprocessing step in data analysis and machine learning. Since causal relationships imply the underlying mechanism of a system, causality-based feature selection methods have gradually attracted great attentions. For a high dimensional system undergoing dynamic transformation, because of the non-stationarity and sample scarcity, modeling the causal structure among these features is difficult. In this paper, we propose a time-varying Granger causal networks to capture the causal relations underlying high dimensional time-varying vector autoregressive models with high order lagged dependence. A kernel reweighted group lasso method is proposed, which overcomes the limitations of sample scarcity and transforms the problem of Granger causal structural learning into a group variable selection problem. The asymptotic consistency of the proposed algorithm is proved. We apply the time-varying Granger causal networks to simulation experiments and real data in the financial market. The study demonstrates that the method provides an efficient tool to detect changes and analysis characters of causal dependency structure in network evolution.","Time-varying Granger causality, Feature selection, Group Lasso, Financial market network",Wei Gao and Haizhong Yang,https://www.sciencedirect.com/science/article/pii/S0031320322002709,https://doi.org/10.1016/j.patcog.2022.108789,0031-3203,2022,108789,130,Pattern Recognition,Time-varying Group Lasso Granger Causality Graph for High Dimensional Dynamic system,article,GAO2022108789,
"Deep neural networks (DNNs) have shown vulnerability to adversarial attacks. By exploiting the transferability of adversarial examples, attackers can fool models under black-box settings without accessing the underlying information. However, they often exhibit weak performance when transferring to defenses, which may give a false sense of security. In this paper, we propose Cyclical Adversarial Attack (CA2), a general and straightforward method to boost the transferability to break defenders. We first revisit the momentum-based methods from the perspective of optimization and find that they usually suffer from the transferability saturation dilemma. To address this, CA2 performs cyclical optimization algorithm to produce adversarial examples. Unlike the standard momentum policy that accumulates the velocity to continuously update the solution, we divide the generation process into multiple phases and treat the velocity vectors from the previous phase as proper knowledge to guide a new adversarial attack with larger steps. Moreover, CA2 applies a novel and compatible augmentation algorithm at every optimization in a loop manner for enhancing the black-box transferability further, referred to as cyclical augmentation. Extensive experiments conducted on a variety of models not only validate the efficacy of each designed algorithm in CA2, but also illustrate the superiority of our method compared with the state-of-the-art transferable attacks. Our implemental code is publicly available at https://github.com/mesunhlf/CA2.","Adversarial example, Transferability, Black-box attack, Defenses",Lifeng Huang and Shuxin Wei and Chengying Gao and Ning Liu,https://www.sciencedirect.com/science/article/pii/S0031320322003120,https://doi.org/10.1016/j.patcog.2022.108831,0031-3203,2022,108831,131,Pattern Recognition,Cyclical Adversarial Attack Pierces Black-box Deep Neural Networks,article,HUANG2022108831,
"Recently, deep learning-based compressed sensing (CS) algorithms have been reported, which remarkably achieve pleasing reconstruction quality with low computational complexity. However, the sampling process of the common deep learning-based CS methods and the conventional ones cannot sufficiently exploit the structured sparsity within image sequences, especially in preserving finer texture details. In this paper, we propose a novel multilevel wavelet-based hierarchical networks for image compressed sensing (dubbed MWHCS-Net). In particular, MWHCS-Net consists of three modules: a sampling module based on a multilevel wavelet transform, a hierarchical initial reconstruction module and a lightweight deep reconstruction module. Motivated by the fact that a sparser signal is easier to reconstruct accurately, we present the sampling module based on multilevel wavelet transform with hierarchical subspace learning for progressive acquisition of measurements to further optimize sampling efficiency and stability. To enhance the finer texture details, the hierarchical initial reconstruction module is designed as a basic initial reconstruction network plus an enhanced initial reconstruction network, which corresponding to the dominant structure component and the texture detail component of the reconstructed image, respectively. At the same time, we also further explore the impact of the hierarchical initial reconstruction module and prove that the texture detail component branch plays an important role in improving the reconstruction quality. Experimental results demonstrate that the proposed MWHCS-Net achieves the state-of-the-art performance while maintaining an efficient running speed. Furthermore, MWHCS-Net outperforms the existing image CS methods based on deep learning in terms of anti-noise performance in most cases.","Compressed sensing, Hierarchical reconstruction, Sparse signal, Multilevel wavelet transform",Zhu Yin and WuZhen Shi and Zhongcheng Wu and Jun Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322002394,https://doi.org/10.1016/j.patcog.2022.108758,0031-3203,2022,108758,129,Pattern Recognition,Multilevel wavelet-based hierarchical networks for image compressed sensing,article,YIN2022108758,
"In this paper, a multi-scale visual transformer model, referred as GasHis-Transformer, is proposed for Gastric Histopathological Image Detection (GHID), which enables the automatic global detection of gastric cancer images. GasHis-Transformer model consists of two key modules designed to extract global and local information using a position-encoded transformer model and a convolutional neural network with local convolution, respectively. A publicly available hematoxylin and eosin (H&E) stained gastric histopathological image dataset is used in the experiment. Furthermore, a Dropconnect based lightweight network is proposed to reduce the model size and training time of GasHis-Transformer for clinical applications with improved confidence. Moreover, a series of contrast and extended experiments verify the robustness, extensibility and stability of GasHis-Transformer. In conclusion, GasHis-Transformer demonstrates high global detection performance and shows its significant potential in GHID task.","Gastric histropathological image, Multi-scale visual transformer, Image detection",Haoyuan Chen and Chen Li and Ge Wang and Xiaoyan Li and Md {Mamunur Rahaman} and Hongzan Sun and Weiming Hu and Yixin Li and Wanli Liu and Changhao Sun and Shiliang Ai and Marcin Grzegorzek,https://www.sciencedirect.com/science/article/pii/S0031320322003089,https://doi.org/10.1016/j.patcog.2022.108827,0031-3203,2022,108827,130,Pattern Recognition,GasHis-Transformer: A multi-scale visual transformer approach for gastric histopathological image detection,article,CHEN2022108827,
"Multi-view based 3D shape recognition methods have achieved state-of-the-art performance in 3D shape recognition and retrieval. The main focus of multi-view based approaches is determining how to fuse multi-view features into a compact, descriptive, and robust 3D shape descriptor that can then be utilized for 3D shape recognition and retrieval. This paper proposes a novel multi-view aggregating framework, view-filtering-based multi-view aggregating convolution (VFMVAC) to learn global shape descriptors for 3D shape recognition. The proposed VFMVAC applies a voting-based view filtering strategy to select representative views, also introduces a novel multi-view aggregating module to integrate multi-view features; this substantially improves the descriptiveness of the descriptors, and therefore improves the performance of 3D shape recognition and retrieval. Specifically, all views are fed into a voting-based view filtering module to select the top-k representative views. Subsequently, the features of the top-k views are fed into the multi-view aggregating module, which first conducts cross-view channel shuffle for achieving cross-view information flowing, and the resulted reshaped features are then fed into the aggregating convolution module for feature fusion. Experiments on benchmark datasets demonstrate that the proposed VFMVAC is effective and outperforms several recent techniques with respect to the classification and retrieval performance, robustness and efficiency.","Multi-view, Channel shuffle, Convolution, Recognition, Retrieval",Zehua Liu and Yuhe Zhang and Jian Gao and Shurui Wang,https://www.sciencedirect.com/science/article/pii/S0031320322002552,https://doi.org/10.1016/j.patcog.2022.108774,0031-3203,2022,108774,129,Pattern Recognition,VFMVAC: View-filtering-based multi-view aggregating convolution for 3D shape recognition and retrieval,article,LIU2022108774,
"Local binary pattern (LBP) and its variants have been successfully applied in texture feature extraction. However, it is hard for most LBP-based methods to effectively describe and distinguish the local neighborhoods with similar structures (that is, the calculated feature patterns are identical) but different contrasts or grayscales. To alleviate such problems, we propose a novel global refined local binary pattern (GRLBP) by analyzing the nature of pixel intensity distribution in local neighborhoods. GRLBP consists of two descriptors called magnitude refined local sign binary pattern (MRLBP_S) and center refined local magnitude binary pattern (CRLBP_M). MRLBP_S distinguishes local neighborhoods with contrast differences by using global magnitude anchors to refine local sign patterns. And CRLBP_M identifies local neighborhoods with grayscale differences by employing global central grayscale anchors to refine local magnitude patterns. Finally, frequency histograms of MRLBP_S and CRLBP_M from each image are cascaded to generate the GRLBP. Extensive experimental results on seven benchmark texture databases: Outex, CUReT, KTH-TIPS, UMD, UIUC, KTH-T2b, and DTD demonstrate that the proposed GRLBP can represent the detailed information of texture images. Furthermore, compared with state-of-the-art LBP variants, GRLBP has competitive advantages in classification accuracy, feature dimension, and computational complexity, respectively.","Texture classification, Texture descriptor, Texture representation, Feature pattern refinement, Local binary pattern",Xin Shu and Hui Pan and Jinlong Shi and Xiaoning Song and Xiao-Jun Wu,https://www.sciencedirect.com/science/article/pii/S0031320322003247,https://doi.org/10.1016/j.patcog.2022.108843,0031-3203,2022,108843,131,Pattern Recognition,Using global information to refine local patterns for texture representation and classification,article,SHU2022108843,
"Lifelong learning algorithms aim to enable robots to handle open-set and detrimental conditions, and yet there is a lack of adequate datasets with diverse factors for benchmarking. In this work, we constructed and released a lifelong learning robotic vision dataset, OpenLORIS-Object. This dataset was collected by RGB-D camera capturing dynamic environment in daily life scenarios with diverse factors, including illumination, occlusion, object pixel size and clutter, of quantified difficulty levels. To the best of our knowledge, this is an unique real-world dataset for robotic vision with independent and quantifiable environmental factors, which are currently unaccounted for in other lifelong learning datasets such as CORe50 and NICO. We tested 9 state-of-the-art algorithms with 4 evaluation metrics over the dataset in Domain Incremental Learning, Task Incremental Learning, and Class Incremental Learning scenarios. The results demonstrate that these existing algorithms are insufficient to handle lifelong learning task in dynamic environments. Our dataset and benchmarks are now publicly available at this website.22https://lifelong-robotic-vision.github.io/dataset/object","Robotic vision, Continual learning, Lifelong learning, Object recognition",Chuanlin Lan and Fan Feng and Qi Liu and Qi She and Qihan Yang and Xinyue Hao and Ivan Mashkin and Ka Shun Kei and Dong Qiang and Vincenzo Lomonaco and Xuesong Shi and Zhengwei Wang and Yao Guo and Yimin Zhang and Fei Qiao and Rosa H.M. Chan,https://www.sciencedirect.com/science/article/pii/S0031320322003004,https://doi.org/10.1016/j.patcog.2022.108819,0031-3203,2022,108819,130,Pattern Recognition,Towards lifelong object recognition: A dataset and benchmark,article,LAN2022108819,
"The motivation for this research is to develop an approach that reliably captures the disease dynamics of COVID-19 for an entire population in order to identify the key events driving change in the epidemic through accurate estimation of daily COVID-19 cases. This has been achieved through the new CP-ABM approach which uniquely incorporates Change Point detection into an Agent Based Model taking advantage of genetic algorithms for calibration and an efficient infection centric procedure for computational efficiency. The CP-ABM is applied to the Northern Ireland population where it successfully captures patterns in COVID-19 infection dynamics over both waves of the pandemic and quantifies the significant effects of non-pharmaceutical interventions (NPI) on a national level for lockdowns and mask wearing. To our knowledge, there is no other approach to date that has captured NPI effectiveness and infection spreading dynamics for both waves of the COVID-19 pandemic for an entire country population.","COVID-19, Non pharmaceutical interventions, Change point detection, Agent based model, Genetic algorithm",Aleksandar Novakovic and Adele H. Marshall,https://www.sciencedirect.com/science/article/pii/S0031320322002710,https://doi.org/10.1016/j.patcog.2022.108790,0031-3203,2022,108790,130,Pattern Recognition,The CPâABM approach for modelling COVIDâ19 infection dynamics and quantifying the effects of nonâpharmaceutical interventions,article,NOVAKOVIC2022108790,
"Traditional methods of handwritten character recognition rely on extensive labeled data. However, humans can generalize to unseen handwritten characters by watching a few printed examples in textbooks. To simulate this ability, we propose a cross-modal prototype learning method (CMPL) to realize zero-shot recognition. For each character class, a prototype is generated by mapping the printed character into a deep neural network feature space. For unseen character class, its prototype can be directly produced from a printed character sample, therefore, not requiring any handwritten samples to realize class-incremental learning. Specifically, CMPL considers different modalities simultaneously - online handwritten trajectories, offline handwritten images, and auxiliary printed character images. The joint learning of the above modalities is achieved through sharing printed prototypes between online and offline data. In zero-shot inference, we feed CMPL the printed samples to obtain corresponding class prototypes, and then the unseen handwritten character can be recognized by the nearest prototype. Our experimental results demonstrate that CMPL outperforms the state-of-the-art methods in both online and offline zero-shot handwritten Chinese character recognition. Moreover, we also show the cross-domain generalization of CMPL from two perspectives: cross-language and modern-to-ancient handwritten character recognition, focusing on the transferability between different languages and different styles (i.e., modern and historical handwritings).","Online handwriting, Offline handwriting, Printed character, Zero-shot, Prototype, Cross-modality",Xiang Ao and Xu-Yao Zhang and Cheng-Lin Liu,https://www.sciencedirect.com/science/article/pii/S0031320322003405,https://doi.org/10.1016/j.patcog.2022.108859,0031-3203,2022,108859,131,Pattern Recognition,Cross-modal prototype learning for zero-shot handwritten character recognition,article,AO2022108859,
"Cross-modality fusing complementary information of multispectral remote sensing image pairs can improve the perception ability of detection algorithms, making them more robust and reliable for a wider range of applications, such as nighttime detection. Compared with prior methods, we think different features should be processed specifically, the modality-specific features should be retained and enhanced, while the modality-shared features should be cherry-picked from the RGB and thermal IR modalities. Following this idea, a novel and lightweight multispectral feature fusion approach with joint common-modality and differential-modality attentions are proposed, named Cross-Modality Attentive Feature Fusion (CMAFF). Given the intermediate feature maps of RGB and thermal images, our module parallel infers attention maps from two separate modalities, common- and differential-modality, then the attention maps are multiplied to the input feature map respectively for adaptive feature enhancement or selection. Extensive experiments demonstrate that our proposed approach can achieve the state-of-the-art performance at a low computation cost.","Cross-modality, Attention, Feature fusion, Object detection, Multispectral remote sensing imagery",Fang Qingyun and Wang Zhaokui,https://www.sciencedirect.com/science/article/pii/S0031320322002679,https://doi.org/10.1016/j.patcog.2022.108786,0031-3203,2022,108786,130,Pattern Recognition,Cross-modality attentive feature fusion for object detection in multispectral remote sensing imagery,article,QINGYUN2022108786,
"Estimating the 3D pose of a hand from a 2D image is a well-studied problem and a requirement for several real-life applications such as virtual reality, augmented reality, and hand gesture recognition. Currently, reasonable estimations can be computed from single RGB images, especially when a multi-task learning approach is used to force the system to consider the shape of the hand when its pose is determined. However, depending on the method used to represent the hand, the performance can drop considerably in real-life tasks, suggesting that stable descriptions are required to achieve satisfactory results. In this paper, we present a keypoint-based end-to-end framework for 3D hand and pose estimation and successfully apply it to the task of hand gesture recognition as a study case. Specifically, after a pre-processing step in which the images are normalized, the proposed pipeline uses a multi-task semantic feature extractor generating 2D heatmaps and hand silhouettes from RGB images, a viewpoint encoder to predict the hand and camera view parameters, a stable hand estimator to produce the 3D hand pose and shape, and a loss function to guide all of the components jointly during the learning phase. Tests were performed on a 3D pose and shape estimation benchmark dataset to assess the proposed framework, which obtained state-of-the-art performance. Our system was also evaluated on two hand-gesture recognition benchmark datasets and significantly outperformed other keypoint-based approaches, indicating that it is an effective solution that is able to generate stable 3D estimates for hand pose and shape.","Hand pose estimation, Hand shape estimation, Deep learning, Hand gesture recognition",Danilo Avola and Luigi Cinque and Alessio Fagioli and Gian Luca Foresti and Adriano Fragomeni and Daniele Pannone,https://www.sciencedirect.com/science/article/pii/S0031320322002436,https://doi.org/10.1016/j.patcog.2022.108762,0031-3203,2022,108762,129,Pattern Recognition,3D hand pose and shape estimation from RGB images for keypoint-based hand gesture recognition,article,AVOLA2022108762,
"Object detection is advancing rapidly with the development of deep learning solutions and big data dimensions. This paper takes the challenging recognition task as the core work and proposes a novel and efficient network framework dedicated to unseen congestion detection. To guarantee the accuracy as well as the speed of inference, the detector utilizes the advanced You Only Look Once v4 (YOLOv4) as the backbone and agglutinates the four proposed strategies, called YOLO-Anti. Our model mainly consists of three modules: First, an adaptive context module similar to valve control is proposed to obtain contextual information that balances foreground and background features. Second, to solve the problem that the imbalance between feature levels weakens the detection performance, a balanced prediction layer method is developed. Finally, we propose an anti-congestion network to selectively expand the local domain to achieve finer-grained detection. Besides, in the training procedure, a designed heterogeneous cross-entropy loss is utilized to strengthen the detectorâs discrimination of similar targets in different categories. Extensive experiments were conducted on the PASCAL VOC, COCO, and UA-DETRAC data sets. The state-of-the-art results were achieved on UA-DETRAC and the leading performance on PASCAL VOC and COCO. Also, compared with baseline YOLOv4, the proposed method brings significant accuracy improvement and negligible time consumption.","Deep learning, Congested and occluded objects, Object detection",Kun Wang and Maozhen Liu,https://www.sciencedirect.com/science/article/pii/S0031320322002953,https://doi.org/10.1016/j.patcog.2022.108814,0031-3203,2022,108814,131,Pattern Recognition,YOLO-Anti: YOLO-based counterattack model for unseen congested object detection,article,WANG2022108814,
"The accurate mesoscale eddy identification methods with deep learning framework depend on either single eddy characteristic from altimeter missions or multi-step eddy examination strategies, disregarding those indistinguishable features from multiple eddy data integration. In this article, we first propose a data-attention-based YOLO (DAY) to precisely recognize mesoscale eddies in the South China Sea (SCS), which can hierarchically unite multiple eddy attributes and efficiently predict eddies with one-step strategy involving detection and classification. It consists of two main components: heterogeneous eddy data integration module and dynamic attention detecting module for eddy identification. The data integration component empirically transforms the field of multi-source eddy data and propagates eddy labels through automatic labeling method, which sustains a good supply for our dynamic attention-base detecting network. To thoroughly identify mesoscale eddies based on spatio-temporal patterns, DAY efficiently learns the characteristics of mesoscale eddies with an improved one-step identification YOLO network. The comparative evaluation results demonstrate that DAY achieves 54% performance improvement over the state-of-the-art methods on single gray SLA data and outperforms two-stage detecting technique Faster R-CNN by 51%.","Mesoscale eddy identification, Attention mechanism, Data-attention-based YOLO, One-stage detection",Xinning Wang and Xuegong Wang and Chong Li and Yuben Zhao and Peng Ren,https://www.sciencedirect.com/science/article/pii/S003132032200351X,https://doi.org/10.1016/j.patcog.2022.108870,0031-3203,2022,108870,131,Pattern Recognition,Data-attention-YOLO (DAY): A comprehensive framework for mesoscale eddy identification,article,WANG2022108870,
"Segmentation of cancerous tumors in ultrasound (US) images of human organs is one of the critical problems in medical imaging. The US images are characterized by low contrast, irregular shapes, high levels of speckle-noise and acoustic shadows, making it difficult to segment the tumor. Yet, US imaging is considered one of the most inexpensive and safe imaging tests available to detect cancer in its early stages. However, an automatic segmentation method applicable to all types of US imagery does not exist. This paper proposes a novel segmentation method that combines image fusion, artificial life (AL) and a genetic algorithm (GA). The new algorithm has been applied to US images of breast cancer. The method is based on tracing agents (TA), which are artificial organisms with memory and the ability to communicate. They live inside a fusion image generated from the US and the elastography (EL) images. The TA can recognize the patterns of strong edges and boundary gaps allowing to outline the tumor. The new model has been tested against six types of segmentation models, i.e., machine learning, active contours, level set models, superpixel models, edge linking models and selected hybrid methods. The experiments include 16 state-of-the-art methods, which outperform 69 recent and classical segmentation routines. The tests were run on 395 breast cancer images from http://onlinemedicalimages.com and https://www.ultrasoundcases.info/. TA training employs a GA. The model has been verified on âhardâ cases (complex shapes, boundary leakage, and noisy edge maps). The proposed algorithm produces more accurate results than the reference methods on high complexity images. A video demo of the algorithm is at http://shorturl.at/htBW9.","Artificial life, Fusion image, Medical image segmentation, Genetic algorithm, Ultrasound images, Breast cancer",Nalan Karunanayake and Wanrudee Lohitvisate and Stanislav S. Makhanov,https://www.sciencedirect.com/science/article/pii/S0031320322003193,https://doi.org/10.1016/j.patcog.2022.108838,0031-3203,2022,108838,131,Pattern Recognition,Artificial life for segmentation of fusion ultrasound images of breast abnormalities,article,KARUNANAYAKE2022108838,
"Different categories of visual stimuli evoke distinct activation patterns in the human brain. These patterns can be captured with EEG for utilization in application such as Brain-Computer Interface (BCI). However, accurate classification of these patterns acquired using single-trial data is challenging due to the low signal-to-noise ratio of EEG. Recently, deep learning-based transformer models with multi-head self-attention have shown great potential for analyzing variety of data. This work introduces an EEG-ConvTranformer network that is based on both multi-headed self-attention and temporal convolution. The novel architecture incorporates self-attention modules to capture inter-region interaction patterns and convolutional filters to learn temporal patterns in a single module. Experimental results demonstrate that EEG-ConvTransformer achieves improved classification accuracy over state-of-the-art techniques across five different visual stimulus classification tasks. Finally, quantitative analysis of inter-head diversity also shows low similarity in representational space, emphasizing the implicit diversity of multi-head attention.","EEG, Visual stimulus classification, Deep learning, Transformer, Multi-head attention, Inter-region similarity, Temporal convolution, Inter-head diversity, Head representations",Subhranil Bagchi and Deepti R. Bathula,https://www.sciencedirect.com/science/article/pii/S0031320322002382,https://doi.org/10.1016/j.patcog.2022.108757,0031-3203,2022,108757,129,Pattern Recognition,EEG-ConvTransformer for single-trial EEG-based visual stimulus classification,article,BAGCHI2022108757,
"The devastating outbreak of Coronavirus Disease (COVID-19) cases in early 2020 led the world to face health crises. Subsequently, the exponential reproduction rate of COVID-19 disease can only be reduced by early diagnosis of COVID-19 infection cases correctly. The initial research findings reported that radiological examinations using CT and CXR modality have successfully reduced false negatives by RT-PCR test. This research study aims to develop an explainable diagnosis system for the detection and infection region quantification of COVID-19 disease. The existing research studies successfully explored deep learning approaches with higher performance measures but lacked generalization and interpretability for COVID-19 diagnosis. In this study, we address these issues by the Covid-MANet network, an automated end-to-end multi-task attention network that works for 5 classes in three stages for COVID-19 infection screening. The first stage of the Covid-MANet network localizes attention of the model to the relevant lungs region for disease recognition. The second stage of the Covid-MANet network differentiates COVID-19 cases from bacterial pneumonia, viral pneumonia, normal and tuberculosis cases, respectively. To improve the interpretation and explainability, three experiments have been conducted in exploration of the most coherent and appropriate classification approach. Moreover, the multi-scale attention model MA-DenseNet201 proposed for the classification of COVID-19 cases. The final stage of the Covid-MANet network quantifies the proportion of infection and severity of COVID-19 in the lungs. The COVID-19 cases are graded into more specific severity levels such as mild, moderate, severe, and critical as per the score assigned by the RALE scoring system. The MA-DenseNet201 classification model outperforms eight state-of-the-art CNN models, in terms of sensitivity and interpretation with lung localization network. The COVID-19 infection segmentation by UNet with DenseNet121 encoder achieves dice score of 86.15% outperforming UNet, UNet++, AttentionUNet, R2UNet, with VGG16, ResNet50 and DenseNet201 encoder. The proposed network not only classifies images based on the predicted label but also highlights the infection by segmentation/localization of model-focused regions to support explainable decisions. MA-DenseNet201 model with a segmentation-based cropping approach achieves maximum interpretation of 96% with COVID-19 sensitivity of 97.75%. Finally, based on class-varied sensitivity analysis Covid-MANet ensemble network of MA-DenseNet201, ResNet50 and MobileNet achieve 95.05% accuracy and 98.75% COVID-19 sensitivity. The proposed model is externally validated on an unseen dataset, yields 98.17% COVID-19 sensitivity.","Covid-19, Lung segmentation, Infection segmentation, Chest X-ray, Deep learning, Transfer learning, Explainable AI",Ajay Sharma and Pramod Kumar Mishra,https://www.sciencedirect.com/science/article/pii/S0031320322003077,https://doi.org/10.1016/j.patcog.2022.108826,0031-3203,2022,108826,131,Pattern Recognition,Covid-MANet: Multi-task attention network for explainable diagnosis and severity assessment of COVID-19 from CXR images,article,SHARMA2022108826,
"Deep learning based methods have achieved remarkable progress in action recognition. Existing works mainly focus on designing novel deep architectures to learn video representations for action recognition. Most existing methods treat sampled frames equally and average all the frame-level predictions to generate video-level predictions at the testing stage. However, within a video, discriminative actions may occur sparsely in a few frames whereas most other frames are irrelevant to the ground truth which may even lead to wrong results. As a result, we think that the strategy of selecting relevant frames would be a further important key to enhance the existing deep learning based action recognition. In this paper, we propose an attention-aware sampling method for action recognition, which aims to discard the irrelevant and misleading frames and preserve the most discriminative frames. We formulate the process of mining key frames from videos as a Markov decision process and train the attention agent through deep reinforcement learning without extra labels. The agent takes features and predictions from the baseline model as inputs and generates importance scores for all frames. Moreover, our approach is extensible, which can be applied to different existing deep learning based action recognition models. We achieve very competitive action recognition performance on two widely used action recognition datasets.","Action recognition, Deep learning, Reinforcement learning, Pseudo labels",Wenkai Dong and Zhaoxiang Zhang and Chunfeng Song and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S0031320322002783,https://doi.org/10.1016/j.patcog.2022.108797,0031-3203,2022,108797,130,Pattern Recognition,Identifying the key frames: An attention-aware sampling method for action recognition,article,DONG2022108797,
"In visual tracking tasks, the training data are commonly composed of a large number of video sequences and each frame in the sequences needs to be labeled manually, which is labor-intensive and time-consuming. In addition, considering the similarity among the consecutive frames in the same sequence, there is significant redundancy in the training data. To address these problems, a novel pseudo loss active learning (PLAL) method is developed in this paper. PLAL aims to select the most informative and least redundant data for training to reduce the cost of labeling and maintain competitive tracking results simultaneously. Firstly, the Gaussian distribution based pseudo label is generated for the unlabeled candidates based on the tracking model which is initially trained on a small amount of training data. Then, the pseudo loss based on cross entropy is designed to compute the difference between the pseudo label and the target response map. The pseudo loss measures the uncertainty of the target spatial context which is used as the informativeness criterion of the image frame for selection. Meanwhile, a sampling interval threshold and a temporal penalty are employed for frame selection to avoid drastic variation in target appearance and reduce the redundancy within the consecutive candidate frames. Only the selected frames are labeled by the oracle (human expert) and then added to the training data. Extensive experiments on public benchmarks (OTB2013, OTB2015, VOT2018, UAV123, GOT-10K, TrackingNet, LaSOT, OxUvA and TLP) demonstrate that PLAL method outperforms the baseline and other recent active learning approaches. With only 3% of labeled data from the training dataset, PLAL reaches competitive performance (98-100%) compared to the model trained on the entire training dataset.","Active learning, Visual tracking, Pseudo loss, Pseudo label",Zhiyan Cui and Na Lu and Weifeng Wang,https://www.sciencedirect.com/science/article/pii/S0031320322002540,https://doi.org/10.1016/j.patcog.2022.108773,0031-3203,2022,108773,130,Pattern Recognition,Pseudo loss active learning for deep visual tracking,article,CUI2022108773,
"Multiview pedestrian detection detects pedestrians based on the perception of the same environment from multiple perspectives. This task requires feature extraction in a single view with occlusion and aggregation of multiview information. However, existing research is limited by the local occlusion and the multiview feature stitching method, which cannot perform multiview aggregation efficiently. This paper introduces a network that utilizes key points supervision and grouped feature fusion to address these challenges. It uses key points to regress pedestrians in a single view, and augments the pedestrian consistency information in overlapping views by a grouped feature fusion module. Specifically, the proposed key points supervision effectively alleviates false negatives due to occlusion, and the grouped feature fusion module enhances pedestrian location features by computing the similarity and spatial correlation of overlapping views after single view projection to the ground plane, thereby reducing target ambiguity. Quantitative and qualitative results show that the proposed method can reduce false negatives and false positives in multiview pedestrian detection and achieve efficient multiview feature aggregation. Compared to state-of-the-art methods, the proposed model achieves superior performance, achieving the highest MODA of 92.4 and 93.9 on Wildtrack and MultiviewX datasets, respectively. We believe, to the best of our knowledge, that this approach offers a new optimization idea for multiview aggregation.","Multiview aggregation, Pedestrian detection, Key points, Grouped feature fusion",Xin Gao and Yijin Xiong and Guoying Zhang and Hui Deng and Kangkang Kou,https://www.sciencedirect.com/science/article/pii/S0031320322003478,https://doi.org/10.1016/j.patcog.2022.108866,0031-3203,2022,108866,131,Pattern Recognition,Exploiting key points supervision and grouped feature fusion for multiview pedestrian detection,article,GAO2022108866,
"Interest in image hiding has been continually growing. Recently, deep learning-based image hiding approaches improve the hidden capacity significantly. However, the major challenges of the existing methods are that they are difficult to balance between the errors of the modified cover image and those of the recovered secret image. To solve this problem, in this paper, we develop an image hiding algorithm based on a joint compressive autoencoder framework. Further, we propose a novel strategy to enlarge the hidden capacity, i.e., hiding multi-images in one container image. Specifically, our approach provides an extremely high image hidden capacity coupled with small reconstruction errors of the secret image. More importantly, we tackle the trade-off problem of earlier approaches by mapping the image representations in the latent spaces of the joint compressive autoencoder models, leading to both high visual quality of the container image and low reconstruction error the secret image. In an extensive set of experiments, we confirm our proposed approach to outperform several state-of-the-art image hiding methods, yielding high imperceptibility and steganalysis resistance of the container images with high recovery quality of the secret images, while improving the image hidden capacity significantly (four times higher than full-image hiding capacity).","Image hiding, Neural networks, Deep learning, Compressive autoencoder",Xiyao Liu and Ziping Ma and Zhihong Chen and Fangfang Li and Ming Jiang and Gerald Schaefer and Hui Fang,https://www.sciencedirect.com/science/article/pii/S0031320322003235,https://doi.org/10.1016/j.patcog.2022.108842,0031-3203,2022,108842,131,Pattern Recognition,Hiding multiple images into a single image via joint compressive autoencoders,article,LIU2022108842,
"Child-face aging and rejuvenation have amassed considerable active research interest, owing to their immense impact on a broad range of social and security applications, e.g., digital entertainment, fashion and wellness, and searching for long-lost children using childhood photos. All current face aging approaches based on generative adversarial networks (GANs) focus on adult images or long-term aging. We present a new large-scale longitudinal Indian child (ICD) benchmark dataset to facilitate face age progression and regression, cross-age face recognition, age estimation, gender prediction, and kinship face recognition to alleviate these issues. Furthermore, we propose an automatic child-face age progression and regression model, namely, ChildGAN, that generates visually realistic images for enhanced face-identification accuracy while preserving the identity. Consequently, we have trained state-of-the-art (SOTA) face aging models on ICD for comprehensive qualitative and quantitative evaluations. We also present a multi-racial experiments dataset named Multi-Racial Child Dataset (MRCD) containing 64,965 child face images. The images are selected from publicly available datasets and web crawling. Finally, we investigate the generalization of ChildGAN by experimenting with White, Black, Asian, and Indian races. The experimental results suggest that the proposed ChildGAN and SOTA models can aid in reconnecting young children, who were lost at a young age as victims of child trafficking or abduction, with their families. The model and the MRCD web crawled images are available at https://github.com/praveenkumarchandaliya/ChildGAN_Tamp1/.","Child face aging and rejuvenation, Child datasets, Face recognition, Age estimation, Gender preservation, Child trafficking",Praveen Kumar Chandaliya and Neeta Nain,https://www.sciencedirect.com/science/article/pii/S0031320322002424,https://doi.org/10.1016/j.patcog.2022.108761,0031-3203,2022,108761,129,Pattern Recognition,ChildGAN: Face aging and rejuvenation to find missing children,article,CHANDALIYA2022108761,
"The recent explosive development of knowledge graphs (KGs) in artificial intelligence tasks coupled with incomplete or partial information has triggered considerable research interest in relation prediction. However, many challenges still remain unsolved: (i) the previous relation prediction methods require a significant amount of training instances (i.e., head-tail entity pairs) for every relation, which is infeasible in practical scenarios; and (ii) the representation learning of entities and relations always assumes that all local neighbors and their features contribute equally to the embedding, not sufficiently considering the heterogeneity of the information; and (iii) the state-of-the-art methods usually require a lot of training time, resulting in a high cost in real-world applications. To overcome these challenges, we propose a heterogeneous representation learning and matching approach, Multi-metric Feature Extraction Network (MFEN for short), for few-shot relation prediction in KGs. Our method focuses on knowledge graphs to sufficiently explore the topological structure and node content in graphs. Rather than taking the average of the embeddings of all relational neighbors, a heterogeneity-aware representation learning method is proposed to generate high-expressive embeddings, which capture the heterogenous roles of the relational neighbors of given entity and all of their features via a convolutional encoder. To learn the expressive representations efficiently, a single-layer CNN architecture with multi-scale filters is devised. In addition, multiple heuristic metrics are combined to efficiently improve the accuracy of similarity calculation. The proposed MFEN model is evaluated on two representative benchmark datasets NELL and Wiki. Extensive experiments have demonstrated that our method gets more than 5% accuracy improvement and three times speedup to state-of-the-art models. Code is available on https://github.com/summer-funny/MFEN.","Knowledge graphs, Few-shot learning, Relation prediction, Representation learning, Convolutional network",Tao Wu and Hongyu Ma and Chao Wang and Shaojie Qiao and Liang Zhang and Shui Yu,https://www.sciencedirect.com/science/article/pii/S0031320322003119,https://doi.org/10.1016/j.patcog.2022.108830,0031-3203,2022,108830,131,Pattern Recognition,Heterogeneous representation learning and matching for few-shot relation prediction,article,WU2022108830,
"Recently, many researches have demonstrated that the attention mechanism has great potential in improving the performance of deep convolutional neural networks (CNNs). However, the existing methods either ignore the importance of using channel attention and spatial attention mechanisms simultaneously or bring much additional model complexity. In order to achieve a balance between performance and model complexity, we propose the Hybrid Attention Module (HAM), a really lightweight yet efficient attention module. Given an intermediate feature map as the input feature, HAM firstly produces one channel attention map and one channel refined feature through the channel submodule, and then based on the channel attention map, the spatial submodule divides the channel refined feature into two groups along the channel axis to generate a pair of spatial attention descriptors. By applying saptial attention descriptors, the spatial submodule generates the final refined feature which can adaptively emphasize the important regions. Besides, HAM is a simple and general module, it can be embedded into various mainstream deep CNN architectures seamlessly and can be trained with base CNNs in the end-to-end way. We evaluate HAM through abundant of experiments on CIFAR-10, CIFAR-100 and STL-10 datasets. The experimental results show that HAM-integrated networks achieve accuracy improvements and further reduce the negative impact of less training data on deeper networks performance than its counterparts, which proves the effectiveness of HAM.","Hybrid attention module, Channel attention map, Spatial feature descriptor, HAM-integrated networks",Guoqiang Li and Qi Fang and Linlin Zha and Xin Gao and Nenggan Zheng,https://www.sciencedirect.com/science/article/pii/S0031320322002667,https://doi.org/10.1016/j.patcog.2022.108785,0031-3203,2022,108785,129,Pattern Recognition,HAM: Hybrid attention module in deep convolutional neural networks for image classification,article,LI2022108785,
"Continual learning from streaming data sources becomes more and more popular due to the increasing number of online tools and systems. Dealing with dynamic and everlasting problems poses new challenges for which traditional batch-based offline algorithms turn out to be insufficient in terms of computational time and predictive performance. One of the most crucial limitations is that we cannot assume having an access to a finite and complete data set â we always have to be ready for new data that may complement our model. This poses a critical problem of providing labels for potentially unbounded streams. In real world, we are forced to deal with very strict budget limitations, therefore, we will most likely face the scarcity of annotated instances, which are essential in supervised learning. In our work, we emphasize this problem and propose a novel instance exploitation technique. We show that when: (i) data is characterized by temporary non-stationary concepts, and (ii) there are very few labels spanned across a long time horizon, it is actually better to risk overfitting and adapt models more aggressively by exploiting the only labeled instances we have, instead of sticking to a standard learning mode and suffering from severe underfitting. We present different strategies and configurations for our methods, as well as an ensemble algorithm that attempts to maintain a sweet spot between risky and normal adaptation. Finally, we conduct a complex in-depth comparative analysis of our methods, using state-of-the-art streaming algorithms relevant for the given problem.","Machine learning, Data stream mining, Concept drift, Sparse labeling, Active learning",Åukasz Korycki and Bartosz Krawczyk,https://www.sciencedirect.com/science/article/pii/S0031320322002308,https://doi.org/10.1016/j.patcog.2022.108749,0031-3203,2022,108749,129,Pattern Recognition,Instance exploitation for learning temporary concepts from sparsely labeled drifting data streams,article,KORYCKI2022108749,
"An echo State Network (ESN) is a special structure of a recurrent neural network (RNN) in which the recurrent neurons are randomly connected. ESN models which have achieved a high accuracy on time series prediction tasks can be used as time series prediction models in many domains. Nevertheless, in most ESN models, the input weights are randomly generated and the output weights calculated by the least square method are susceptible to outliers, which cannot guarantee that the ESN models will always be optimal for a given task. In this paper, a novel discriminative and regularized ESN (DR-ESN) combines discriminative feature aggregation (DFA) and outlier-robust weights (ORW) algorithms are proposed for time series classification. DFA is firstly proposed to replace the random input weights of ESN with the constrained weights generated from sample information. In DFA, weight vectors are selected from the vector space spanned by initial input sequence vectors, then the new generated input weights can adequately represent the data features. Secondly, ORW is employed to enhance the robustness of output weights by constraining the weights assigned to samples with large training errors. The weights evaluation and experiments on a massive set of the synthetic time series data, real-world bearing fault data and UCR benchmarks indicate that the proposed DR-ESN can not only considerably improve the original ESN classifier but also effectively suppress the effect of outliers on classification performance.","Echo state network, Recurrent neural networks, Discriminative feature extraction, Time series classification, Outlier-robust weights",Heshan Wang and Yuxi Liu and Dongshu Wang and Yong Luo and Chudong Tong and Zhaomin Lv,https://www.sciencedirect.com/science/article/pii/S0031320322002928,https://doi.org/10.1016/j.patcog.2022.108811,0031-3203,2022,108811,130,Pattern Recognition,Discriminative and regularized echo state network for time series classification,article,WANG2022108811,
"Multisensory integration has attracted intense studies for decades. How to combine visual and auditory information to optimize perception and decision-making is a key question in neuroscience as well as machine learning. Inspired by the mechanisms of multisensory integration in the brain, we propose a multimodal channel-wise attention transformer (MCAT) that performs reliability-weighted integration and revises the weights allocation according to a top-down attention-like mechanism. We apply MCAT on EF-LSTM neural networks for a fine-grained video bird recognition task, and on MulT neural networks for an emotion recognition task. The performance of both models is improved remarkably. Ablation study shows that the attention mechanism is indispensable for effective multisensory integration. Moreover, we found that cross-modal integration models are in accordance with the law of inverse effectiveness of multisensory integration in the brain, which reveals that our model may have mechanisms similar to those in the brain. Taken together, the results demonstrate that the brain-inspired MCAT block is effective for improving multisensory integration, providing useful clues for designing new algorithms and understanding multisensory integration in the brain.","Multisensory integration, Top-down attention, Multimodal transformer, Fine-grained bird recognition, Emotion recognition",Qianqian Shi and Junsong Fan and Zuoren Wang and Zhaoxiang Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322003181,https://doi.org/10.1016/j.patcog.2022.108837,0031-3203,2022,108837,130,Pattern Recognition,Multimodal channel-wise attention transformer inspired by multisensory integration mechanisms of the brain,article,SHI2022108837,
"Recently, deep learning based Computer-Aided Diagnosis methods have been widely utilized due to their highly effective diagnosis of patients. Although Convolutional Neural Networks (CNNs) are capable of extracting the latent structural characteristics of dementia and of capturing the changes of brain anatomy in Magnetic Resonance Imaging (MRI) scans, the high-dimensional input to a deep CNN usually makes the network difficult to train, and affects its diagnostic accuracy. In this paper, a novel method called the hierarchical pseudo-3D convolution neural network based on a kernel attention mechanism with a new global context block, which is abbreviated as âPKG-Netâ, is proposed to accurately predict Alzheimerâs disease even when the input features are complex. Specifically, the proposed network first extracts multi-scale features from pre-processed images. Second, the attention mechanism and global context blocks are applied to combine features from different layers to hierarchically transform the MRI into more compact high-level features. Then, a joint loss function is used to train the proposed network to generate more distinguishing features, which improve the generalization performance of the network. In addition, we combine our method with different architectures. Extensive experiments are conducted to analyze the performance of the PKG-Net with different hyper-parameters and architectures. Finally, in order to verify the effectiveness of our method on Alzheimerâs disease diagnosis, we carry out extensive experiments on the ADNI dataset, and compare the results of our method with that of existing methods in terms of accuracy, recall and precision. Furthermore, our network can fully take advantage of the deep 3D convolutional neural network for automatic feature extraction and representation, and thus can avoid the limitation of low processing efficiency caused by the preprocessing procedure in which a specific area needs to be annotated in advance. Finally, we evaluate our proposed framework using two public datasets, ADNI-1 and ADNI-2, and the experimental results show that our proposed framework can achieve superior performance over state-of-the-art approaches.","Diagnosis of Alzheimerâs disease, Pseudo-3D, Attention mechanism, Multi-scale, Joint loss function",Zhao Pei and Zhiyang Wan and Yanning Zhang and Miao Wang and Chengcai Leng and Yee-Hong Yang,https://www.sciencedirect.com/science/article/pii/S0031320322003065,https://doi.org/10.1016/j.patcog.2022.108825,0031-3203,2022,108825,131,Pattern Recognition,Multi-scale attention-based pseudo-3D convolution neural network for Alzheimerâs disease diagnosis using structural MRI,article,PEI2022108825,
"Knowledge graph (KG) has increasingly been seen as a significant resource in financial applications (e.g., risk control, auditing and anti-fraud). However, there are few prior studies that focus on multi-relational circles, extracting additional information under the completed KG and selecting similarity measures for knowledge representation. In this paper, we introduce multi-relational circles and propose a novel embedding model, which considers entity weights calculated by PageRank algorithm to improve TransE method. In order to extract additional information, we use entity weights to convert embeddings into an on-map mining problem, and propose a model called CNNe based on entity weights and a convolutional neural network with three hidden layers, which converts vectors of entities, entity weights and relationships into matrices to perform link prediction in the same way as image processing. With the help of ten different similarity measures, it is demonstrated that the choice of distance measure greatly effect the results of the translation embedding models. Moreover, we propose two embedding methods, sMFE and tMFE, to enhance the results using matrix factorization. The complete incidence matrix is first applied to knowledge embedding, which contains the most comprehensive topological properties of the graph. Experimental results on standard benchmark datasets demonstrate that the proposed models are effective. In particular, CNNe achieves a mean rank of 166 less than the baseline method and an improvement of 2.1% on the proportion of correct entities ranked in the top ten on YAGO3-10 dataset.","Graph-based finance, Representation learning, Complete incidence matrix, Convolutional neural network, Matrix factorization",Zhengdi Wang and Lvqing Yang and Zhenfeng Lei and Anwar {Ul Haq} and Defu Zhang and Shuangyuan Yang and Akindipe Olusegun Francis,https://www.sciencedirect.com/science/article/pii/S0031320322003223,https://doi.org/10.1016/j.patcog.2022.108841,0031-3203,2022,108841,131,Pattern Recognition,An entity-weights-based convolutional neural network for large-sale complex knowledge embedding,article,WANG2022108841,
"Though much effort has been spent on designing new active learning algorithms, little attention has been paid to the initialization problem of active learning, i.e., how to find a set of labeled samples which contains at least one instance per category. This work identifies the initialization of active learning as a separate and novel research problem, reviews existing methods that can be adapted to be used for this task and, in addition, proposes a new active initialization criterion: the Nearest Neighbor Criterion. Experiments on 16 benchmark datasets verify that the novel method often finds an initialization set with fewer queried samples than other methods do.","active learning, active initialization, nearest neighbor criterion, minimum nearest neighbor distance",Yazhou Yang and Marco Loog,https://www.sciencedirect.com/science/article/pii/S003132032200317X,https://doi.org/10.1016/j.patcog.2022.108836,0031-3203,2022,108836,131,Pattern Recognition,To Actively Initialize Active Learning,article,YANG2022108836,
"Autonomous driving is regarded as one of the most promising remedies to shield human beings from severe crashes. To this end, 3D object detection serves as the core basis of perception stack especially for the sake of path planning, motion prediction, and collision avoidance etc.. Taking a quick glance at the progress we have made, we attribute challenges to visual appearance recovery in the absence of depth information from images, representation learning from partially occluded unstructured point clouds, and semantic alignments over heterogeneous features from cross modalities. Despite existing efforts, 3D object detection for autonomous driving is still in its infancy. Recently, a large body of literature have been investigated to address this 3D vision task. Nevertheless, few investigations have looked into collecting and structuring this growing knowledge. We therefore aim to fill this gap in a comprehensive survey, encompassing all the main concerns including sensors, datasets, performance metrics and the recent state-of-the-art detection methods, together with their pros and cons. Furthermore, we provide quantitative comparisons with the state of the art. A case study on fifteen selected representative methods is presented, involved with runtime analysis, error analysis, and robustness analysis. Finally, we provide concluding remarks after an in-depth analysis of the surveyed works and identify promising directions for future work.","3D object detection, Autonomous driving, Point clouds",Rui Qian and Xin Lai and Xirong Li,https://www.sciencedirect.com/science/article/pii/S0031320322002771,https://doi.org/10.1016/j.patcog.2022.108796,0031-3203,2022,108796,130,Pattern Recognition,3D Object Detection for Autonomous Driving: A Survey,article,QIAN2022108796,
"In this paper, we propose an enhanced deep clustering network (EDCN), which is composed of a Feature Extractor, a Conditional Generator, a Discriminator and a Siamese Network. Specifically, we will utilize two kinds of generated data based on adversarial training, as well as the original data, to train the Feature Extractor for learning effective latent representations. In addition, we adopt the Siamese network to find an embedding space, where a better affinity similarity matrix is obtained as the key to success of spectral clustering in providing reliable pseudo-labels. Particularly, the obtained pseudo-labels will be used to generate realistic data by the Generator. Finally, the discriminator is used to model the real joint distribution of data and corresponding latent representations for Feature Extractor enhancement. To evaluate our proposed EDCN, we conduct extensive experiments on multiple data sets including MNIST, USPS, FRGC, CIFAR-10, STL-10, and Fashion-MNIST by comparing our method with a number of state-of-the-art deep clustering methods, and experimental results demonstrate its effectiveness and superiority.","Deep clustering, Unsupervised learning, Generative adversarial networks, Siamese network",Wenming Cao and Zhongfan Zhang and Cheng Liu and Rui Li and Qianfen Jiao and Zhiwen Yu and Hau-San Wong,https://www.sciencedirect.com/science/article/pii/S0031320322002497,https://doi.org/10.1016/j.patcog.2022.108768,0031-3203,2022,108768,129,Pattern Recognition,Unsupervised discriminative feature learning via finding a clustering-friendly embedding space,article,CAO2022108768,
"The task of pedestrian attribute recognition (PAR) is to distinguish a series of person semantic attributes. Generally, existing methods adopt multi-label classification algorithms to tackle the PAR task by utilizing multiple attribute labels. Despite remarkable progress, this kind of method normally ignores relations between different attributes. In order to be aware of relations between attributes, we propose an inter-attribute aware network via vector-neuron capsule for PAR (IAA-Caps). Our IAA-Caps method replaces traditional one-dimensional scalar neurons with two-dimensional vector-neuron capsules by embedding them in IAA-Caps. Specifically, during IAA-Caps training, one dimension in capsules is used to recognize different attributes, and the other dimension is used to strengthen the relations of different attributes. Through considering inter-attribute relations, compared with previous methods that use a heavyweight backbone (e.g., ResNet50 or BN-Inception), a more lightweight backbone (i.e., OSNet) can be adopted in our proposed IAA-Caps to achieve better performance. Experiments are conducted on several PAR benchmark datasets, including PETA, PA-100K, RAPv1, and RAPv2, demonstrating the effectiveness of the proposed IAA-Caps. In addition, experiments also show that the proposed method can improve the performance of PAR on different backbones, showing its generalization ability.","Pedestrian attribute recognition, Inter-Attribute awareness, Vector-Neuron capsules",Junyi Wu and Yan Huang and Zhipeng Gao and Yating Hong and Jianqiang Zhao and Xinsheng Du,https://www.sciencedirect.com/science/article/pii/S0031320322003466,https://doi.org/10.1016/j.patcog.2022.108865,0031-3203,2022,108865,131,Pattern Recognition,Inter-Attribute awareness for pedestrian attribute recognition,article,WU2022108865,
"In this paper, we construct a novel finite dimensional shape manifold for shape analyses. Elements of the shape manifold are a set of discrete, planar, and closed curves, which stand for object boundaries and are represented by direction function. On this manifold, we use a set of N-dimensional Fourier basis to construct the tangent space of the shape manifold as a finite dimensional space. Furthermore, we construct the shape manifold as a Riemannian manifold, in which the Riemannian metric is interpreted as an l2 metric. Our method improves the performance of bending-only models in the issues of shape analysis including the shape synthesis, comparison, and statistic analysis. We evaluate the performance of the manifold via the following applications: 1)Â shape interpolation and extrapolation between curves, 2)Â shape retrieval on the Flavia leaf database, 3)Â shape synthesis using an estimated probability distribution on the manifold, and 4)Â a novel application named shape arithmetic. All the above experiments clearly demonstrate our approach achieves superior performance to state-of-the-art methods.","Discrete curve model, Non-elastic shape analysis, Shape manifold, Shape synthesis, Shape retrieval, Shape arithmetics",Peng Chen and Xutao Li and Changxing Ding and Jianxing Liu and Ligang Wu,https://www.sciencedirect.com/science/article/pii/S0031320322002412,https://doi.org/10.1016/j.patcog.2022.108760,0031-3203,2022,108760,130,Pattern Recognition,Discrete curve model for non-elastic shape analysis on shape manifold,article,CHEN2022108760,
"Transformation equivariance has been widely investigated in 3D point cloud representation learning for more informative descriptors, which formulates the change of the representation with respect to the transformation of the input point clouds explicitly. In this paper, we extend this property to the task of 3D point cloud registration and propose a rigid transformation equivariance (RTE) for accurate 3D point cloud registration. Specifically, RTE formulates the change of the relative pose explicitly with respect to the rigid transformation of the input point clouds. To exploit RTE, we adopt a Siamese structure network with two shared registration branches. One focuses on the input pair of point clouds, and the other one focuses on the new pair achieved by applying two random rigid transformations to the input point clouds respectively. Since the change of the two output relative poses has been predicted according to RTE, a new additional self-supervised loss is obtained to supervise the training. This general network structure can be integrated with most learning-based point cloud registration frameworks easily to improve the performance. Our method adopts the state-of-the-art virtual point-based pipelines as our shared branches, in which we propose a data-driven matching based on learned cost volume (LCV) rather than traditional hand-crafted matching strategies. Experimental evaluations on both synthetic datasets and real datasets validate the effectiveness of our proposed framework. The source code will be made public.","Point cloud, Rigid transformation equivariance, Learned cost volume",Zhiyuan Zhang and Jiadai Sun and Yuchao Dai and Dingfu Zhou and Xibin Song and Mingyi He,https://www.sciencedirect.com/science/article/pii/S0031320322002655,https://doi.org/10.1016/j.patcog.2022.108784,0031-3203,2022,108784,130,Pattern Recognition,Self-supervised rigid transformation equivariance for accurate 3D point cloud registration,article,ZHANG2022108784,
"Cervical cancer is the seventh most common cancer among all the cancers worldwide and the fourth most common cancer among women. Cervical cytopathology image classification is an important method to diagnose cervical cancer. However, manual inspection is very troublesome, and experts are prone to make mistakes. The emergence of the automatic computer-aided diagnosis system solves this problem. This paper proposes a framework called CVM-Cervix based on deep learning to perform cervical cell classification tasks. It can analyze pap slides quickly and accurately. CVM-Cervix first proposes a Convolutional Neural Network module and a Visual Transformer module for local and global feature extraction respectively, then a Multilayer Perceptron module is designed to fuse the local and global features for the final classification. Experimental results show the effectiveness and potential of the proposed CVM-Cervix in the field of cervical Pap smear image classification. In addition, according to the practical needs of clinical work, we perform a lightweight post-processing to compress the model.","Convolutional neural network, Visual transformer, Multilayer perceptron, Cervical cellÂ classification, Pap smear, Image classification",Wanli Liu and Chen Li and Ning Xu and Tao Jiang and Md Mamunur Rahaman and Hongzan Sun and Xiangchen Wu and Weiming Hu and Haoyuan Chen and Changhao Sun and Yudong Yao and Marcin Grzegorzek,https://www.sciencedirect.com/science/article/pii/S0031320322003107,https://doi.org/10.1016/j.patcog.2022.108829,0031-3203,2022,108829,130,Pattern Recognition,"CVM-Cervix: A hybrid cervical Pap-smear image classification framework using CNN, visual transformer and multilayer perceptron",article,LIU2022108829,
"In applications like stock markets, engineering, medicine, etc., a large amount of time series data has been collected. Interrogating the data for patterns is important for analysis like event prediction and event investigation. A fundamental operation to support such analysis is query processing. In this paper, we aim to efficiently find the optimal match of a query in a timeseries when the match is calculated based on the trend and allows points to be skipped from the middle and ends of the sequences. This problem requires global optimization. The solutions in the literature have prohibitively high time complexities and are not practical for long timeseries. Our method consists of three parts. The first part is an efficiency improvement algorithm called FastOPM which applies the Dijkstra algorithm to get the optimal solution in an efficient manner. The second part derives bounds for optimal solutions. The third part is an algorithm for efficiently searching the target timeseries for the best optimal match of a query. Our experiments show that our method is faster than the baseline method, the bounds are effective, and the search algorithm can identify the best optimal match efficiently. Overall, our algorithm effectively outperforms the state-of-the-art algorithms DTW and MASS in retrieving target segments.","Time series, Query processing, Global optimization, Partial match",Jixue Liu and Jiuyong Li and Lin Liu,https://www.sciencedirect.com/science/article/pii/S0031320322002898,https://doi.org/10.1016/j.patcog.2022.108808,0031-3203,2022,108808,130,Pattern Recognition,FastOPMâA practical method for partial match of time series,article,LIU2022108808,
"A new method is proposed for the solution of the data-driven optimal transport barycenter problem and of the more general distributional barycenter problem that the article introduces. The distributional barycenter problem provides a conceptual and computational toolbox for central problems in pattern recognition, such as the simulation of conditional distributions, the construction of a representative for a family of distributions indexed by a covariate and a new class of data-based generative models. The method proposed improves on previous approaches based on adversarial games, by slaving the discriminator to the generator and minimizing the need for parameterizations. It applies not only to a discrete family of distributions, but to more general distributions conditioned to factors z of any cardinality and type. The methodology is applied to numerical examples, including an analysis of the MNIST data set with a new cost function that penalizes non-isometric maps.","Optimal transport, Barycenter problem, Pattern visualization, Simulation, Generative models",Esteban G. Tabak and Giulio Trigila and Wenjun Zhao,https://www.sciencedirect.com/science/article/pii/S003132032200276X,https://doi.org/10.1016/j.patcog.2022.108795,0031-3203,2022,108795,130,Pattern Recognition,Distributional barycenter problem through data-driven flows,article,TABAK2022108795,
"Aiming at solving the problem of clustering in the multi-view datasets which include samples with information missing in one or more views, incomplete multi-view clustering has received considerable attention. However, most studies can not get satisfying accuracy and efficiency when dealing with datasets in which a considerable number of instances are missing in partial views. To address this problem, a method named Auto-weighted Sample-level Fusion with Anchors for Incomplete Multi-view Clustering (ASA-IC) is proposed in this paper. It designs an auto-weighted sample-level fusion strategy, which realizes the optimized conversion from the individual instance-to-anchor similarity learning to the concensus instance-to-anchor similarity matrix construction. ASA-IC can not only handle incomplete samples and effectively explore the relationship between each instance and anchors, but also deal with various incomplete clustering situations and be applied in large-scale datasets as well. Besides, experiments on 5 complete datasets and 27 incomplete ones illustrate its effectiveness quantitatively and qualitatively.","Incomplete data, Multi-view clustering, Anchor, Auto-weighted, Large-scale",Xiao Yu and Hui Liu and Yuxiu Lin and Yan Wu and Caiming Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322002539,https://doi.org/10.1016/j.patcog.2022.108772,0031-3203,2022,108772,130,Pattern Recognition,Auto-weighted sample-level fusion with anchors for incomplete multi-view clustering,article,YU2022108772,
"Visual relationship detection aims to recognize visual relationships in scenes as triplets ãsubject-predicate-objectã. Previous works have shown remarkable progress by introducing multimodal features, external linguistics, scene context, etc. Due to the loss of informative multimodal hyper-relations (i.e. relations of relationships), the meaningful contexts of relationships are not fully captured yet, which limits the reasoning ability. In this work, we propose a Multimodal Similarity Guided Relationship Interaction Network (MSGRIN) to explicitly model the relations of relationships in graph neural network paradigm. In a visual scene, the MSGRIN takes the visual relationships as nodes to construct an adaptive graph and enhances deep message passing by introducing Entity Appearance Reconstruction, Entity Relevance Filtering and Multimodal Similarity Attention. We have conducted extensive experiments on two datasets: Visual Relationship Detection (VRD) and Visual Genome (VG). The evaluation results demonstrate that the proposed MSGRIN has empirically performed more effectively overall.","Visual relationship detection, Scene graph generation, Relationship context, Multimodal relationship interaction",Zhixuan Liu and Wei-Shi Zheng,https://www.sciencedirect.com/science/article/pii/S0031320322003296,https://doi.org/10.1016/j.patcog.2022.108848,0031-3203,2022,108848,132,Pattern Recognition,Learning multimodal relationship interaction for visual relationship detection,article,LIU2022108848,
"Adversarial examples refer to the malicious inputs that can mislead deep neural networks (DNNs) to falsely classify them. In practice, some adversarial examples are transferable and hence can deceive different target models. In multi-stage ensemble adversarial example attacks, adversaries can generate strongly transferable adversarial examples through iteratively perturbing legitimate examples to attack well-trained source models in a white-box manner. Limited by computational and memory resources (e.g., GPU memory), however, adversaries cannot handle all models and all legitimate examples at a time. This brings an important but never studied research issue: how to optimally schedule source models and appropriately select samples to improve adversarial example transferability and reduce unnecessary computational overheads? To shed light on this problem, we develop a novel multi-stage ensemble adversarial example attack method based on our proposed strategies of model scheduling and sample selection. The first strategy schedules source models to be attacked in every stage, based on the criteria of decision boundary similarity and model diversity. The second selects input samples to be handled by ensemble attacks, according to their sensitivity level for adversarial perturbations. To our knowledge, we are the first to study model scheduling and sample selection for multi-stage ensemble attacks. We conduct extensive experiments on three datasets with a variety of source and target models. Experiments show that our model scheduling based ensemble attack outperforms the all-model ensemble attack and the state-of-the-art ensemble attacks SCES, SMBEA and EnsembleFool in transferability. Moreover, our sample selection strategy improves attack success rate by about 138%.","Adversarial example, Black-box attack, Model scheduling, Sample selection",Zichao Hu and Heng Li and Liheng Yuan and Zhang Cheng and Wei Yuan and Ming Zhu,https://www.sciencedirect.com/science/article/pii/S0031320322003053,https://doi.org/10.1016/j.patcog.2022.108824,0031-3203,2022,108824,130,Pattern Recognition,Model scheduling and sample selection for ensemble adversarial example attacks,article,HU2022108824,
"Network architecture search (NAS), in particular the differentiable architecture search (DARTS) method, has shown a great power to learn excellent model architectures on the specific dataset of interest. In contrast to using a fixed dataset, in this work, we focus on a different but important scenario for NAS: how to refine a deployed networkâs model architecture to enhance its robustness with the guidance of a few collected and misclassified examples that are degraded by some real-world unknown corruptions having a specific pattern (e.g., noise, blur, etc..). To this end, we first conduct an empirical study to validate that the model architectures can be definitely related to the corruption patterns. Surprisingly, by just adding a few corrupted and misclassified examples (e.g., 103 examples) to the clean training dataset (e.g., 5.0Ã104 examples), we can refine the model architecture and enhance the robustness significantly. To make it more practical, the key problem, i.e., how to select the proper failure examples for the effective NAS guidance, should be carefully investigated. Then, we propose a novel core-failure-set guided DARTS that embeds a K-center-greedy algorithm for DARTS to select suitable corrupted failure examples to refine the model architecture. We use our method for DARTS-refined DNNs on the clean as well as 15 corruptions with the guidance of four specific real-world corruptions. Compared with the state-of-the-art NAS as well as data-augmentation-based enhancement methods, our final method can achieve higher accuracy on both corrupted datasets and the original clean dataset. On some of the corruption patterns, we can achieve as high as over 45% absolute accuracy improvements.","Network architecture search, Core-failure-set selection, Robustness enhancement, Differentiable architecture search",Xuhong Ren and Jianlang Chen and Felix Juefei-Xu and Wanli Xue and Qing Guo and Lei Ma and Jianjun Zhao and Shengyong Chen,https://www.sciencedirect.com/science/article/pii/S0031320322003454,https://doi.org/10.1016/j.patcog.2022.108864,0031-3203,2022,108864,131,Pattern Recognition,DARTSRepair: Core-failure-set guided DARTS for network robustness to common corruptions,article,REN2022108864,
"In this paper, we tackle the problem called learning from label proportions (LLP), where the training data is arranged into various bags, with only the proportions of different categories in each bag available. Existing efforts mainly focus on training a model with only the limited proportion information in a weakly supervised manner, thus result in apparent performance gap to supervised learning, as well as computational inefficiency. In this work, we propose a multi-task pipeline called SELF-LLP to make full use of the information contained in the data and model themselves. Specifically, to intensively learn representation from the data, we leverage the self-supervised learning as a plug-in auxiliary task to learn better transferable visual representation. The main insight is to benefit from the self-supervised representation learning with deep model, as well as improving classification performance by a large margin. Meanwhile, in order to better leverage the implicit benefits from the model itself, we incorporate the self-ensemble strategy to guide the training process with an auxiliary supervision information, which is constructed by aggregating multiple previous network predictions. Furthermore, a ramp-up mechanism is further employed to stabilize the training process. In the extensive experiments, our method demonstrates compelling advantages in both accuracy and efficiency over several state-of-the-art LLP approaches.","Learning from label proportion, Self-supervised learning, Self-ensemble strategy, Multi-task learning",Jiabin Liu and Zhiquan Qi and Bo Wang and YingJie Tian and Yong Shi,https://www.sciencedirect.com/science/article/pii/S0031320322002485,https://doi.org/10.1016/j.patcog.2022.108767,0031-3203,2022,108767,129,Pattern Recognition,SELF-LLP: Self-supervised learning from label proportions with self-ensemble,article,LIU2022108767,
"With the explosive growth of video data, video summarization, which attempts to seek the minimum subset of frames while still conveying the main story, has become one of the hottest topics. Nowadays, substantial achievements have been made by supervised learning techniques, especially after the emergence of deep learning. However, it is extremely expensive and difficult to construct a large-scale video summarization dataset through human annotation. To address this problem, we propose a convolutional attentive adversarial network (CAAN), whose key idea is to build a deep summarizer in an unsupervised way. Upon the generative adversarial network, our overall framework consists of a generator and a discriminator. The former predicts importance scores for all the frames of a video while the latter tries to distinguish the score-weighted frame features from original frame features. To capture the global and local temporal relationship of video frames, the generator employs a fully convolutional sequence network to build global representation of a video, and an attention-based network to predict normalized importance scores. To optimize the parameters, our objective function is composed of three loss functions, which can guide the frame-level importance score prediction collaboratively. To validate this proposed method, we have conducted extensive experiments on two public benchmarks SumMe and TVSum. The results show the superiority of our proposed method against other state-of-the-art unsupervised approaches. Our method even outperforms some published supervised approaches.","Video summarization, Generative adversarial network, Self attention",Guoqiang Liang and Yanbing Lv and Shucheng Li and Shizhou Zhang and Yanning Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322003211,https://doi.org/10.1016/j.patcog.2022.108840,0031-3203,2022,108840,131,Pattern Recognition,Video summarization with a convolutional attentive adversarial network,article,LIANG2022108840,
"Linear discriminant analysis (LDA) is one of the most important dimensionality reduction techniques and applied in many areas. However, traditional LDA algorithms aim to capture the global structure from data and ignore the local information. That may lead to the failure of LDA in some real-world datasets which have a complex geometry distribution. Although there are many previous works that focus on preserving the local information, they are all stuck in the same problem that the neighbor relationships of pairwise data points obtained from the original space may not be reliable, especially in the case of heavy noise. Therefore, we proposed a novel self-weighted learning framework, named Self-Weighted Adaptive Locality Discriminant Analysis (SALDA), for locality-aware based dimensionality reduction. The proposed framework can adaptively learn an intrinsic low-dimensional subspace, so that we can explore the better neighbor relationships for samples under the ideal subspace. In addition, our model can automatically learn to assign the weights to data pairwise points within the same class and takes no extra parameters compared to other classical locality-aware methods. At last, the experimental results on both synthetic and real-world benchmark datasets demonstrate the effectiveness and superiority of the proposed algorithm.","Supervised dimensionality reduction, Linear discriminant analysis, Re-weighted method",Wei Chang and Feiping Nie and Zheng Wang and Rong Wang and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S003132032200259X,https://doi.org/10.1016/j.patcog.2022.108778,0031-3203,2022,108778,129,Pattern Recognition,Self-weighted learning framework for adaptive locality discriminant analysis,article,CHANG2022108778,
"We consider the kernel completion problem with the presence of multiple views in the data. In this context the data samples can be fully missing in some views, creating missing columns and rows to the kernel matrices that are calculated individually for each view. We propose to solve the problem of completing the kernel matrices with Cross-View Kernel Transfer (CVKT) procedure, in which the features of the other views are transformed to represent the view under consideration. The transformations are learned with kernel alignment to the known part of the kernel matrix, allowing for finding generalizable structures in the kernel matrix under completion. Its missing values can then be predicted with the data available in other views. We illustrate the benefits of our approach with simulated data, multivariate digits dataset and multi-view dataset on gesture classification, as well as with real biological datasets from studies of pattern formation in early Drosophila melanogaster embryogenesis.","Multi-view learning, Cross-view transfer, Kernel completion, Kernel learning",Riikka Huusari and CÃ©cile Capponi and Paul Villoutreix and Hachem Kadri,https://www.sciencedirect.com/science/article/pii/S0031320322002400,https://doi.org/10.1016/j.patcog.2022.108759,0031-3203,2022,108759,129,Pattern Recognition,Cross-View kernel transfer,article,HUUSARI2022108759,
"Many internet search engines have been developed, however, the retrieval of video clips remains a challenge. This paper considers the retrieval of incident videos, which may contain more spatial and temporal semantics. We propose an encoder-decoder ConvLSTM model that explores multiple embeddings of a video to facilitate comparison of similarity between a pair of videos. The model is able to encode a video into an embedding that integrates both its spatial information and temporal semantics. Multiple video embeddings are then generated from coarse- and fine-grained features of a video to capture high- and low-level meanings. Subsequently, a learning-based comparative model is proposed to compare the similarity of two videos based on their embeddings. Extensive evaluations are presented and show that our model outperforms state-of-the-art methods for several video retrieval tasks on the FIVR-200K, CC_WEB_VIDEO, and EVVE datasets.","Artificial intelligence, Computer vision, Deep metric learning, Incident video retrieval",Ting-Hui Chiang and Yi-Chun Tseng and Yu-Chee Tseng,https://www.sciencedirect.com/science/article/pii/S0031320322002886,https://doi.org/10.1016/j.patcog.2022.108807,0031-3203,2022,108807,130,Pattern Recognition,A multi-embedding neural model for incident video retrieval,article,CHIANG2022108807,
"Face recognition is one of the most successful applications of image analysis. Since 1960s, automatic face recognition research has been carried out, but the problem is still unresolved. Therefore, in this manuscript, a novel Partial face reconstruction (PFR) algorithm called Self- motivated feature mapping (SMFM) combining a Fully Convolutional Network (FCN) and Deep Stacked Denoising Sparse Autoencoders (DS-DSA) algorithm is proposed to overcome the challenges. The proposed approach focuses on the generation of feature maps from the Fully Convolutional Network and it is used Deep Stacked Denoising Sparse Autoencoders to perform the partial face reconstruction. The spatial maps are generated by extracting the features from Fully Convolutional Network and it is supplied as the input for partial reconstruction and re-identification to the Deep Stacked Denoising Sparse Autoencoders network. The main aim of the proposed work is âto enhance the accuracy during facial reconstructionâ. The proposed approach is implemented in MATLAB platform. The performance of the proposed approach attains 23.45% and 20.41% accuracy,25.93`% and 19.43% sensitivity, 22.21% and 24.41% precision and20.21% and 23.41% Specificity greater than the existing approaches, like Partial Face Reconstruction using generative adversarial networks (GANs), Partial Face Reconstruction using DeepÂ RecurrentÂ neural network (DRNN).","Partial face recognition, Deep learning algorithm, Fully convolutional network, Autoencoder",P.S. Dinesh and M. Manikandan,https://www.sciencedirect.com/science/article/pii/S0031320322002643,https://doi.org/10.1016/j.patcog.2022.108783,0031-3203,2022,108783,130,Pattern Recognition,Fully convolutional Deep Stacked Denoising Sparse Auto encoder network for partial face reconstruction,article,DINESH2022108783,
"Partial multi-label learning refers to the problem that each instance is associated with a candidate label set involving both relevant and noisy labels. Existing solutions mainly focus on label disambiguation, while ignoring the negative effect of the inconsistency between feature information and label information. Specifically, the existence of completely unlabeled instances makes the estimation of label co-occurrence difficult. To tackle these problems, we propose a novel framework for partial multi-label learning in semi-supervised scenarios by solving the inconsistency between features and labels. In the first stage, the label-level correlation matrix on both labeled and unlabeled instances is derived via Hilbert-Schmidt Independence Criterion (HSIC). The correlation matrix can characterize the label correlation of labeled instances and can propagate the label correlation of unlabeled instances. In the second stage, the proposed framework achieves the training of feature mapping, the recovery of ground-truth labels, and the alleviation of noisy labels in a mutually beneficial manner, and develops an alternative optimization procedure to optimize them. In addition, a nonlinear version is extended by using kernel trick. Experimental studies demonstrate that the proposed methods can achieve competitive superiority against existing well-established methods.","Semi-supervised partial multi-label learning, Label correlation, HSIC",Anhui Tan and Jiye Liang and Wei-Zhi Wu and Jia Zhang,https://www.sciencedirect.com/science/article/pii/S003132032200320X,https://doi.org/10.1016/j.patcog.2022.108839,0031-3203,2022,108839,131,Pattern Recognition,Semi-supervised partial multi-label classification via consistency learning,article,TAN2022108839,
"Recent advances in unsupervised domain adaptation have significantly improved the recognition accuracy of CNNs by alleviating the domain shift between (labeled) source and (unlabeled) target data distributions. While the problem of single-target domain adaptation (STDA) for object detection has recently received much attention, multi-target domain adaptation (MTDA) remains largely unexplored, despite its practical relevance in several real-world applications, such as multi-camera video surveillance. Compared to the STDA problem that may involve large domain shifts between complex source and target distributions, MTDA faces additional challenges, most notably the computational requirements and catastrophic forgetting of previously-learned targets, which can depend on the order of target adaptations. STDA for detection can be applied to MTDA by adapting one model per target, or one common model with a mixture of data from target domains. However, these approaches are either costly or inaccurate. The only state-of-art MTDA method specialized for detection learns targets incrementally, one target at a time, and mitigates the loss of knowledge by using a duplicated detection model for knowledge distillation, which is computationally expensive and does not scale well to many domains. In this paper, we introduce an efficient approach for incremental learning that generalizes well to multiple target domains. Our MTDA approach is more suitable for real-world applications since it allows updating the detection model incrementally, without storing data from previous-learned target domains, nor retraining when a new target domain becomes available. Our approach leverages domain discriminators to train a novel Domain Transfer Module (DTM), which only incurs a modest overhead. The DTM transforms source images according to diverse target domains, allowing the model to access a joint representation of previously-learned target domains, and to effectively limit catastrophic forgetting. Our proposed method â called MTDA with DTM (MTDA-DTM) â is compared against state-of-the-art approaches on several MTDA detection benchmarks and Wildtrack, a benchmark for multi-camera pedestrian detection. Results indicate that MTDA-DTM achieves the highest level of detection accuracy across multiple target domains, yet requires significantly fewer computational resources. Our code is available.11https://github.com/Natlem/M-HTCN.","Deep learning, Convolutional NNs, Object detection, Unsupervised domain adaptation, Multi-Target domain adaptation, Incremental learning",Le Thanh Nguyen-Meidine and Madhu Kiran and Marco Pedersoli and Jose Dolz and Louis-Antoine Blais-Morin and Eric Granger,https://www.sciencedirect.com/science/article/pii/S0031320322002527,https://doi.org/10.1016/j.patcog.2022.108771,0031-3203,2022,108771,129,Pattern Recognition,Incremental multi-target domain adaptation for object detection with efficient domain transfer,article,NGUYENMEIDINE2022108771,
"Cross-modal hashing aims at using modality content to retrieve semantically relevant objects of different modalities, so cross-modal retrieval has attracted much attention. To effectively exploit the discriminative label information and retain more semantic information in the process of hash learning, we propose a novel cross-modal hashing method, named high-level semantic similarity analysis hashing (HSSAH) for cross-modal retrieval. To reduce time complexity and enhance discriminant ability in hash codes, HSSAH constructs an asymmetric high-level semantic similarity learning framework to replace the binary semantic similarity matrix. Moreover, the developed HSSAH is a two-stage approach, and a semantic-enhanced scheme is proposed in the second stage, which fully leverages the label information to gain more powerful hash functions. We conducted comprehensive experiments on three benchmark datasets to evaluate the performance of HSSAH. Experimental results show that HSSAH can achieve significantly better retrieval precision and outperforms several state-of-the-art approaches.","Cross-modal retrieval, Hashing, Similarity search, Supervised, Optimization",Fan Yang and Yufeng Liu and Xiaojian Ding and Fumin Ma and Jie Cao,https://www.sciencedirect.com/science/article/pii/S0031320322003041,https://doi.org/10.1016/j.patcog.2022.108823,0031-3203,2022,108823,130,Pattern Recognition,Asymmetric crossâmodal hashing with highâlevel semantic similarity,article,YANG2022108823,
"This paper proposes a method exploiting temporal context with an attention mechanism for detecting objects in real-time in a live streaming video. Video object detection is challenging and essential in practical applications such as robotics, smartphones, and surveillance cameras. Although methods have been proposed to improve the accuracy or run-time speed by exploiting temporal information, the trade-off between them tends to be ignored. We thus focus on the trade-off between accuracy and speed, and propose a method to improve the accuracy by aggregating the past information from a lightweight feature extractor with an attention mechanism. Evaluations on the UA-DETRAC and ImageNet VID datasets demonstrate our modelâs superior performance to state-of-the-art methods on live streaming real-time object detection.","Video object detection, Video analysis, Object detection",Masato Fujitake and Akihiro Sugimoto,https://www.sciencedirect.com/science/article/pii/S0031320322003284,https://doi.org/10.1016/j.patcog.2022.108847,0031-3203,2022,108847,131,Pattern Recognition,Temporal feature enhancement network with external memory for live-stream video object detection,article,FUJITAKE2022108847,
"The recognition of Chinese characters has always been a challenging task due to their huge variety and complex structures. The current radical-based methods fail to recognize Chinese characters without learning all of their radicals in the training stage. To this end, we propose a novel Hippocampus-heuristic Character Recognition Network (HCRN), which can recognize unseen Chinese characters only by training part of radicals. More specifically, the network architecture of HCRN is a new pseudo-siamese network designed by us, which can learn features from pairs of input samples and use them to predict unseen characters. The experimental results on the recognition of printed and handwritten characters show that HCRN is robust and effective on zero/few-shot learning tasks. For the printed characters, the mean accuracy of HCRN outperforms the state-of-the-art approach by 23.93% on recognizing unseen characters. For the handwritten characters, HCRN improves the mean accuracy by 11.25% on recognizing unseen characters.","Chinese character recognition, Hippocampus thinking, Radical analysis, Zero-shot learning, Label embedding",Guanjie Huang and Xiangyu Luo and Shaowei Wang and Tianlong Gu and Kaile Su,https://www.sciencedirect.com/science/article/pii/S0031320322002990,https://doi.org/10.1016/j.patcog.2022.108818,0031-3203,2022,108818,130,Pattern Recognition,Hippocampus-heuristic character recognition network for zero-shot learning in Chinese character recognition,article,HUANG2022108818,
"We demonstrate an effective framework to achieve a better performance based on Deep Sparse auto-encoder for Multi-task Learning, called DSML for short. To learn the reconstructed and higher-level features on cross-domain instances for multiple tasks, we combine the labeled and unlabeled data from all tasks to reconstruct the feature representations. Furthermore, we propose the model of Stacked Reconstruction Independence Component Analysis (SRICA for short) for the optimization of feature representations with a large amount of unlabeled data, which can effectively address the redundancy of image data. Our proposed SRICA model is developed from RICA and is based on deep sparse auto-encoder. In addition, we adopt a Semi-Supervised Learning method (SSL for short) based on model parameter regularization to build a unified model for multi-task learning. There are several advantages in our proposed framework as follows: 1) The proposed SRICA makes full use of a large amount of unlabeled data from all tasks. It is used to pursue an optimal sparsity feature representation, which can overcome the over-fitting problem effectively. 2) The deep architecture used in our SRICA model is applied for higher-level and better representation learning, which is designed to train on patches for sphering the input data. 3) Training parameters in our proposed framework has lower computational cost compared to other common deep learning methods such as stacked denoising auto-encoders. Extensive experiments on several real image datasets demonstrate our proposed framework outperforms the state-of-the-art methods.","Deep sparse auto-encoder, Multi-task learning, RICA, Labeled and unlabeled data",Yi Zhu and Xindong Wu and Jipeng Qiang and Xuegang Hu and Yuhong Zhang and Peipei Li,https://www.sciencedirect.com/science/article/pii/S0031320322002230,https://doi.org/10.1016/j.patcog.2022.108742,0031-3203,2022,108742,129,Pattern Recognition,Representation learning with deep sparse auto-encoder for multi-task learning,article,ZHU2022108742,
"The advent of recurrent neural networks for handwriting recognition marked an important milestone reaching impressive recognition accuracies despite the great variability that we observe across different writing styles. Sequential architectures are a perfect fit to model text lines, not only because of the inherent temporal aspect of text, but also to learn probability distributions over sequences of characters and words. However, using such recurrent paradigms comes at a cost at training stage, since their sequential pipelines prevent parallelization. In this work, we introduce a novel method that bypasses any recurrence during the training process with the use of transformer models. By using multi-head self-attention layers both at the visual and textual stages, we are able to tackle character recognition as well as to learn language-related dependencies of the character sequences to be decoded. Our model is unconstrained to any predefined vocabulary, being able to recognize out-of-vocabulary words, i.e. words that do not appear in the training vocabulary. We significantly advance over prior art and demonstrate that satisfactory recognition accuracies are yielded even in few-shot learning scenarios.","Handwriting text recognition, Transformers, Self-Attention, Implicit language model",Lei Kang and Pau Riba and MarÃ§al RusiÃ±ol and Alicia FornÃ©s and Mauricio Villegas,https://www.sciencedirect.com/science/article/pii/S0031320322002473,https://doi.org/10.1016/j.patcog.2022.108766,0031-3203,2022,108766,129,Pattern Recognition,Pay attention to what you read: Non-recurrent handwritten text-Line recognition,article,KANG2022108766,
"This work aims to estimate 6Dof (6D) object pose in background clutter. Considering the strong occlusion and background noise, we propose to utilize the spatial structure for better tackling this challenging task. Observing that the 3D mesh can be naturally abstracted by a graph, we build the graph using 3D points as vertices and mesh connections as edges. We construct the corresponding mapping from 2D image features to 3D points for filling the graph and fusion of the 2D and 3D features. Afterward, a Graph Convolutional Network (GCN) is applied to help the feature exchange among objectsâ points in 3D space. To address the problem of rotation symmetry ambiguity for objects, a spherical convolution is utilized and the spherical features are combined with the convolutional features that are mapped to the graph. Predefined 3D keypoints are voted and the 6DoF pose is obtained via the fitting optimization. Two scenarios of inference, one with the depth information and the other without it are discussed. Tested on the datasets of YCB-Video and LINEMOD, the experiments demonstrate the effectiveness of our proposed method.","6D Pose estimation, Rotation symmetry, Spherical convolution, Graph convolutional network",Jianhan Mei and Xudong Jiang and Henghui Ding,https://www.sciencedirect.com/science/article/pii/S0031320322003168,https://doi.org/10.1016/j.patcog.2022.108835,0031-3203,2022,108835,131,Pattern Recognition,Spatial feature mapping for 6DoF object pose estimation,article,MEI2022108835,
"In multi-target tracking, object interactions and occlusions are two significant factors that affect tracking performance. To settle this, we propose an identity association network (IANet) that integrates the geometry refinement network (GRNet) and the identity verification (IV) module to perform data association and reason the mapping between the detections and tracklets. In our data association process, the object drifts caused by object interactions are suppressed effectively by encoding the direction and velocity of objects to refine the geometric position of tracklets. The tracklets with refined geometric information are further utilized in the IV module to achieve a sufficient encoding of multivariate spatial cues including both appearance and geometry information, which defeats the misleading impacts of interactions and occlusions dramatically in multi-object tracking. The extensive experiments and comparative evaluations have demonstrated that our proposed method can significantly outperform many state-of-the-art methods on benchmarks of 2D MOT2015, MOT16, MOT17, MOT20, and KITTI by using public detection and online settings.","Multi-object tracking, Interactions, Occlusions, Data association, Identity verification",Rui Li and Baopeng Zhang and Zhu Teng and Jianping Fan,https://www.sciencedirect.com/science/article/pii/S0031320322002199,https://doi.org/10.1016/j.patcog.2022.108738,0031-3203,2022,108738,129,Pattern Recognition,An end-to-end identity association network based on geometry refinement for multi-object tracking,article,LI2022108738,
"In real applications, new object classes often emerge after the detection model has been trained on a prepared dataset with fixed classes. Fine-tuning the old model with only new data will lead to a well-known phenomenon of catastrophic forgetting, which severely degrades the performance of modern object detectors. Due to the storage burden, data privacy and time consumption, sometimes it is impractical to train the model from scratch with all data of both old and new classes. In this paper, we propose a novel Multi-View Correlation Distillation (MVCD) based incremental object detection method, which explores the intra-feature correlations in the feature space of the object detector. To better transfer the knowledge learned from the old classes and maintain the ability to learn new classes, we select the sample-specific discriminative features from channel-wise, point-wise and instance-wise views. Meanwhile, the correlation distillation losses on the selective features are designed to regularize the learning of the incremental object detector. A new metric named Stability-Plasticity-mAP (SPmAP) is proposed to evaluate the incremental learning performance as a complementary metric to mAP, which integrates the metrics for the stability on old classes and the plasticity on new classes in incremental object detection. The extensive experiments conducted on VOC2007 and COCO demonstrate that MVCD achieves a better trade-off between stability and plasticity than state-of-the-art first-order distillation-based incremental object detection methods.","Object detection, Incremental learning, Catastrophic forgetting, Knowledge distillation",Dongbao Yang and Yu Zhou and Aoting Zhang and Xurui Sun and Dayan Wu and Weiping Wang and Qixiang Ye,https://www.sciencedirect.com/science/article/pii/S0031320322003442,https://doi.org/10.1016/j.patcog.2022.108863,0031-3203,2022,108863,131,Pattern Recognition,Multi-View correlation distillation for incremental object detection,article,YANG2022108863,
"Current video enhancement approaches have achieved good performance in specific rainy, hazy, foggy, and snowy weather conditions. However, they currently suffer from two important limitations. First, they can only handle degradation caused by single weather. Second, they use large, complex models with 10â50 millions of parameters needing high computing resources. As video enhancement is a pre-processing step for applications like video surveillance, traffic monitoring, autonomous driving, etc., it is necessary to have a lightweight enhancement module. Therefore, we propose a dual-frame spatio-temporal feature modulation architecture to handle the degradation caused by diverse weather conditions. The proposed architecture combines the concept of spatio-temporal multi-resolution feature modulation with a multi-receptive parallel encoders and domain-based feature filtering modules to learn domain-specific features. Further, the architecture provides temporal consistency with recurrent feature merging, achieved by providing feedback of the previous frame output. The indoor (REVIDE, NYUDepth), synthetically generated outdoor weather degraded video de-hazing, and de-raining with veiling effect databases are used for experimentation. Also, the performance of the proposed method is analyzed for night-time de-hazing and de-raining with veiling effect weather conditions. Experimental results show the superior performance of our framework compared to existing state-of-the-art methods used for video de-hazing (indoor/outdoor) and de-raining with veiling effect weather conditions. The code is available at https://github.com/pwp1208/PR2022","Multi-frame features, Spatio-temporal feature modulation, Recurrent feature sharing, Multi-weather video enhancement",Prashant W. Patil and Sunil Gupta and Santu Rana and Svetha Venkatesh,https://www.sciencedirect.com/science/article/pii/S003132032200303X,https://doi.org/10.1016/j.patcog.2022.108822,0031-3203,2022,108822,130,Pattern Recognition,Dual-frame spatio-temporal feature modulation for video enhancement,article,PATIL2022108822,
"Recent deep learning based salient object detection (SOD) methods have achieved impressive performance. However, while fully-supervised methods require a large amount of labeled data, weakly-supervised methods still require a considerable human effort. To address this problem, we propose a novel weakly-supervised method for salient object detection based on only binary image tags, which are much cheaper to collect. Our basic idea is to construct a dataset of images that are labeled as either salient (with salient objects) or non-salient (without salient objects), and leverage such binary labels as supervision to learn a salient object detector based on existing unsupervised methods. In particular, we propose a target saliency map hallucinator, which can synthesize pseudo ground truth saliency maps for the salient images in the training data solely from binary labels. We can then use the pseudo ground truth labels to train a salient object detector. Experimental results show that our method performs comparably to the state-of-the-art weakly-supervised methods, but requires considerably less human supervision.","Weak supervision, Salient object detection, Binary labels",Pengjie Wang and Yuxuan Liu and Ying Cao and Xin Yang and Yu Luo and Huchuan Lu and Zijian Liang and Rynson W.H. Lau,https://www.sciencedirect.com/science/article/pii/S0031320322002631,https://doi.org/10.1016/j.patcog.2022.108782,0031-3203,2022,108782,129,Pattern Recognition,Salient object detection with image-level binary supervision,article,WANG2022108782,
"Supervised learning depth estimation methods can achieve good performance when trained on high-quality ground-truth, like LiDAR data. However, LiDAR can only generate sparse 3D maps which causes losing information. Obtaining high-quality ground-truth depth data per pixel is difficult to acquire. In order to overcome this limitation, we propose a novel approach combining structure information from a promising Plane and Parallax geometry pipeline with depth information into a U-Net supervised learning network, which results in quantitative and qualitative improvement compared to existing popular learning-based methods. In particular, the model is evaluated on two large-scale and challenging datasets: KITTI Vision Benchmark and Cityscapes dataset and achieve the best performance in terms of relative error. Compared with pure depth supervision models, our model has impressive performance on depth prediction of thin objects and edges, and compared to structure prediction baseline, our model performs more robustly.","Monocular depth estimation, Plane and parallax geometry, Structure information, Joint prediction model",Hao Xing and Yifan Cao and Maximilian Biber and Mingchuan Zhou and Darius Burschka,https://www.sciencedirect.com/science/article/pii/S0031320322002874,https://doi.org/10.1016/j.patcog.2022.108806,0031-3203,2022,108806,130,Pattern Recognition,Joint prediction of monocular depth and structure using planar and parallax geometry,article,XING2022108806,
"Variational methods, which have been tremendously successful in image segmentation, work by minimizing a given objective functional. The objective functional usually consists of a fidelity term and a regularization term. Because objective functionals may vary from different types of images, developing an efficient, simple, and general numerical method to minimize them has become increasingly vital. However, many existing methods are model-based, converge relatively slowly, or involve complicated techniques. In this paper, we develop a novel iterative convolutionâthresholding method (ICTM) that is simple, efficient, and applicable to a wide range of variational models for image segmentation. In ICTM, the interface between two different segment domains is implicitly represented by the characteristic functions of domains. The fidelity term is usually written into a linear functional of the characteristic functions, and the regularization term is approximated by a functional of characteristic functions in terms of heat kernel convolution. This allows us to design an iterative convolutionâthresholding method to minimize the approximate energy. The method has the energy-decaying property, and thus the unconditional stability is theoretically guaranteed. Numerical experiments show that the method is simple, easy to implement, robust, and applicable to various image segmentation models.","Convolution, Thresholding, Image segmentation, Heat kernel",Dong Wang and Xiao-Ping Wang,https://www.sciencedirect.com/science/article/pii/S0031320322002758,https://doi.org/10.1016/j.patcog.2022.108794,0031-3203,2022,108794,130,Pattern Recognition,The iterative convolutionâthresholding method (ICTM) for image segmentation,article,WANG2022108794,
"Multiple kernel learning (MKL) is a crucial issue which has been widely researched over the last two decades. Although existing MKL algorithms have achieved satisfactory performance in a broad range of applications, these methods do not adequately consider the adverse effects of unreliable or less reliable instances. To handle this shortcoming, we formulate multiple kernel learning in a bi-level learning paradigm consisting of the kernel combination weight learning (KWL) stage and the self-paced learning (SPL) stage, which alternatively negotiate with each other. The KWL stage dynamically absorbs reliable instances into model learning to accurately capture neighborhood relationships and obtains kernel coefficients via maximizing both global and local kernel alignment in a common schema. The SPL stage automatically evaluates the reliability of training samples via self-paced training. The extensive experiments indicate the robustness and superiority of the presented approach in comparison with existing MKL methods.","Multiple kernel learning, Self-paced learning, Bi-level optimization, Local kernel alignment, Global kernel alignment",Fatemeh Alavi and Sattar Hashemi,https://www.sciencedirect.com/science/article/pii/S0031320322002515,https://doi.org/10.1016/j.patcog.2022.108770,0031-3203,2022,108770,129,Pattern Recognition,A bi-level formulation for multiple kernel learning via self-paced training,article,ALAVI2022108770,
"As most object detectors rely on dense candidate samples to cover objects, they have always suffered from the extreme imbalance between very few foreground samples and numerous background samples during training, i.e., the foreground-background imbalance. Although several resampling and reweighting schemes (e.g., OHEM, Focal Loss, GHM) have been proposed to alleviate the imbalance, they are usually heuristic with multiple hyper-parameters, which is difficult to generalize on different object detectors and datasets. In this paper, we propose a novel Residual Objectness (ResObj) mechanism that adaptively learns how to address the foreground-background imbalance problem in object detection. Specifically, we first formulate the imbalance problems on all object classes as an imbalance problem on an âobjectnessâ class. Then, we design multiple cascaded objectness estimators with residual connections for that objectness class to progressively distinguish the foreground samples from background samples. With our residual objectness mechanism, object detectors can learn how to address the foreground-background problem in an end-to-end way, rather than rely on hand-crafted resampling or reweighting schemes. Extensive experiments on the COCO benchmark demonstrate the effectiveness and compatibility of our method for various object detectors: the RetinaNet-ResObj, YOLOv3-ResObj and FasterRCNN-ResObj achieve relative 3%â¼4% Average Precision (AP) improvements compared with their vanilla models, respectively.","Object detection, Class imbalance, Residual objectness",Joya Chen and Dong Liu and Bin Luo and Xuezheng Peng and Tong Xu and Enhong Chen,https://www.sciencedirect.com/science/article/pii/S003132032200262X,https://doi.org/10.1016/j.patcog.2022.108781,0031-3203,2022,108781,130,Pattern Recognition,Residual objectness for imbalance reduction,article,CHEN2022108781,
"Document image enhancement and binarization methods are often used to improve the accuracy and efficiency of document image analysis tasks such as text recognition. Traditional non-machine-learning methods are constructed on low-level features in an unsupervised manner but have difficulty with binarization on documents with severely degraded backgrounds. Convolutional neural network (CNN)based methods focus only on grayscale images and on local textual features. In this paper, we propose a two-stage color document image enhancement and binarization method using generative adversarial neural networks. In the first stage, four color-independent adversarial networks are trained to extract color foreground information from an input image for document image enhancement. In the second stage, two independent adversarial networks with global and local features are trained for image binarization of documents of variable size. For the adversarial neural networks, we formulate loss functions between a discriminator and generators having an encoderâdecoder structure. Experimental results show that the proposed method achieves better performance than many classical and state-of-the-art algorithms over the Document Image Binarization Contest (DIBCO) datasets, the LRDE Document Binarization Dataset (LRDE DBD), and our shipping label image dataset. We plan to release the shipping label dataset as well as our implementation code at github.com/opensuh/DocumentBinarization/.","Document image binarization, Generative adversarial networks, Optical character recognition, Color document image enhancement",Sungho Suh and Jihun Kim and Paul Lukowicz and Yong Oh Lee,https://www.sciencedirect.com/science/article/pii/S0031320322002916,https://doi.org/10.1016/j.patcog.2022.108810,0031-3203,2022,108810,130,Pattern Recognition,Two-stage generative adversarial networks for binarization of color document images,article,SUH2022108810,
"The increasing amount of data available and the rate at which it is collected leads to rapid developments of systems for intelligent information processing and pattern recognition. Often the underlying data is inherently complex, making it difficult to represent it by linear, vectorial data structures. This is where graphs offer a versatile alternative for formal data representation. Actually, quite an amount of graph-based methods for pattern recognition has been proposed. A considerable part of these methods rely on graph matching. In the present paper, we propose a novel encoding of specific graph matching information. The basic idea is to formalize the stable cores of individual classes of graphs â discovered during intra-class matchings â by means of so called matching-graphs. We evaluate the benefit of these matching-graphs by researching two classification approaches that rely on this novel data structure. The first approach is a distance based classifier focusing on the matching-graphs during dissimilarity computation. For the second approach, we propose to use sets of matching-graphs to embed input graphs into a vector space. The basic idea is to produce hundreds of matching-graphs first, and then represent each graph g as a vector that shows the occurrence of, or the distance to, each matching-graph. In a thorough experimental evaluation on seven real world data sets we empirically confirm that our novel approaches are able to improve the classification accuracy of systems that rely on comparable information as well as state-of-the-art methods.","Graph matching, Matching-graphs, Graph edit distance, Structural pattern recognition",Mathias Fuchs and Kaspar Riesen,https://www.sciencedirect.com/science/article/pii/S0031320322003272,https://doi.org/10.1016/j.patcog.2022.108846,0031-3203,2022,108846,131,Pattern Recognition,A novel way to formalize stable graph cores by using matching-graphs,article,FUCHS2022108846,
"In the field of support vector machines, online random feature map algorithms are very important methods for large-scale nonlinear classification problems. At present, the existing methods have the following shortcomings: (1) If only the hyperplane vector is updated during learning while the random feature components are fixed, there is no guarantee that these online methods can adapt to the change of data distribution shape when the data is coming one by one. (2) When the kernel is selected improperly, the samples mapped to an inappropriate space may not be well classified. In order to overcome these shortcomings, considering the fact that iteratively updating random feature components can make data better fit in the current space and lead to the flexible adjustment of the kernel function, random features based online adaptive kernel learning (RF-OAK) is proposed for large-scale nonlinear classification problems. Theoretical analysis of the proposed algorithm is also provided. The experimental results and the Wilcoxon signed-ranks test show that in terms of test accuracy, the proposed method is significantly better than the state-of-the-art online feature mapping classification methods. Compared with the deep learning algorithms, the training time of RF-OAK is shorter. In terms of test accuracy, RF-OAK is better than online algorithm and comparable with offline algorithms.","Large-scale, Nonlinear classification, Online learning, Random feature map",Yingying Chen and Xiaowei Yang,https://www.sciencedirect.com/science/article/pii/S0031320322003430,https://doi.org/10.1016/j.patcog.2022.108862,0031-3203,2022,108862,131,Pattern Recognition,Online Adaptive Kernel Learning with Random Features for Large-scale Nonlinear Classification,article,CHEN2022108862,
"Gait data captured by inertial sensors of smartphone have demonstrated promising results on user authentication. However, most existing models stored the enrolled gait pattern in plaintext for matching with the pattern being validated, thus, posed critical security and privacy issues. In this study, we present a gait cryptosystem that generates from gait data captured by smartphone sensors the random keys for user authentication, meanwhile, secures the gait pattern. First, we propose a revocable and random binary string extraction method using deep neural network followed by feature-wise binarization. A novel loss function for network optimization is also designed, to tackle not only the intra-user stability but also the inter-user randomness. Second, we propose a new biometric key generation scheme, namely Irreversible Error Correct and Obfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme, to securely generate from the binary string a random and irreversible key. The model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. The evaluation showed that our model could generate the key of 139 bits from 5-second data sequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR) smaller than 5.441%. In addition, the security and user privacy analyses showed that our model was secure against existing attacks on biometric template protection, and fulfilled the irreversibility and unlinkability requirements.","Gait authentication, Biometric template protection, Biometric cryptosystems, Gait recognition, Key binding scheme, Biometric key generation",Lam Tran and Thuc Nguyen and Hyunil Kim and Deokjai Choi,https://www.sciencedirect.com/science/article/pii/S0031320322002461,https://doi.org/10.1016/j.patcog.2022.108765,0031-3203,2022,108765,129,Pattern Recognition,Security and privacy enhanced smartphone-based gait authentication with random representation learning and digital lockers,article,TRAN2022108765,
"Facial expression recognition focuses on extracting expression-related features on a face. In this paper, a novel method is proposed for facial expression modeling based on the following two aspects: seeking expression-related regions more accurately, and enhancing expression features more discriminating. To this end, we design a model containing three submodules: the Expression Feature Extractor (EFE), the Expression Mask Refiner (EMR), and the Expression Pattern-Map Generator (EPMG). The EFE module is the backbone that extracts expression features and generates a coarse attention mask which roughly indicates expression-related regions. The EMR module refines the mask to be more precise by modeling the relationship among expression-related regions, and generates the masked features. The EPMG module utilizes the masked features to further model the fusion and extraction process which obtains a compact and discriminating expression-salient embedding for recognition, and generates an expression pattern-map. We propose the concept of the expression pattern-map, which provides a unified visualization of expression features and improves the interpretability of facial expression recognition. Our model is evaluated on four public datasets (CK+, Oulu-CASIA, RAF-DB, AffectNet), and achieves the competitive performance compared with the state-of-the-art.","Facial expression recognition, Facial expression visualization, Expression pattern-map generator, Deep neural networks",Jing Zhang and Huimin Yu,https://www.sciencedirect.com/science/article/pii/S0031320322002187,https://doi.org/10.1016/j.patcog.2022.108737,0031-3203,2022,108737,129,Pattern Recognition,Improving the Facial Expression Recognition and Its Interpretability via Generating Expression Pattern-map,article,ZHANG2022108737,
"There is considerable variation in the size, shape and location of tumours, which makes it challenging for radiologists to diagnose breast cancer. Automated diagnosis of breast cancer from Contrast Enhanced Spectral Mammography (CESM) can support clinical decision making. However, existing methods fail to obtain an effective representation of the CESM and ignore the relationships between images. In this paper, we investigated for the first time a novel and flexible multimodal representation learning method, multi-feature deep information bottleneck (MDIB), for breast cancer classification in CESM. Specifically, the method incorporated an information bottleneck (IB)-based module to learn the prominent representation that provide concise input while informative for the classification. In addition, we creatively extended IB theory to multi-feature IB, which facilitates the learning of relevant features for classification between CESM images. To validate our method, experiments were conducted on our private and public datasets. The classification results of our method were also compared with those of state-of-the-art methods. The experiment results proved the effectiveness and the efficiency of the proposed method. We release our code at https://github.com/sjq5263/MDIB-for-CESM-classification.","Contrast enhanced spectral mammography, Classification, Deep learning, Multi-feature, Information bottleneck",Jingqi Song and Yuanjie Zheng and Jing Wang and Muhammad Zakir Ullah and Xuecheng Li and Zhenxing Zou and Guocheng Ding,https://www.sciencedirect.com/science/article/pii/S0031320322003399,https://doi.org/10.1016/j.patcog.2022.108858,0031-3203,2022,108858,131,Pattern Recognition,Multi-feature deep information bottleneck network for breast cancer classification in contrast enhanced spectral mammography,article,SONG2022108858,
"While online video sharing becomes more popular, it also causes unconscious leakage of personal information in the video retrieval systems like deep hashing. A snoop can collect more usersâ private information from the video database by querying similar videos. This paper focuses on bypassing the deep video hashing based retrieval to prevent information from being maliciously collected. We propose universal adversarial head (UAH), which crafts adversarial query videos by prepending the original videos with a sequence of adversarial frames to perturb the normal hash codes in the Hamming space. This adversarial head can be generated only with a few natural videos, and mislead the retrieval system to return irrelevant videos when it is applied to most query videos. Furthermore, to obey the principle of information protection, we expand the proposed method to a data-free paradigm to generate the UAH, without access to usersâ original videos. Extensive experiments demonstrate the effectiveness of our method in misleading deep video hashing under both white-box and black-box settings.","Privacy protection, Video retrieval, Deep hashing, Adversarial attack",Jiawang Bai and Bin Chen and Kuofeng Gao and Xuan Wang and Shu-Tao Xia,https://www.sciencedirect.com/science/article/pii/S0031320322003156,https://doi.org/10.1016/j.patcog.2022.108834,0031-3203,2022,108834,131,Pattern Recognition,Practical protection against video data leakage via universal adversarial head,article,BAI2022108834,
"The problem of âfinding best lines passing through a set of straight linesâ has appeared in applications such as archaeological pottery analysis, precision manufacturing, and 3D modelling. In these applications, an instance of this problem is finding the symmetry axis of a symmetrical object from a set of its surface normal lines. We show that the mentioned instance of the problem may have two meaningful local minima, one of which is the symmetry axis, a fact that has been neglected in the literature. A multiple-solutions RANSAC algorithm is proposed for finding initial estimates of both local minima in the presence of outliers. Then, a coordinate-descent algorithm is presented that starts from these initial estimates and finds the local minima of the problem. The proposed coordinate-descent method does not involve any line search procedure, and its convergence is guaranteed. We also provide a proof for the rate of the convergence.","Symmetry axis, Multiple-solutions RANSAC, 3D Reconstruction",Seyed-Mahdi Nasiri and Reshad Hosseini and Hadi Moradi,https://www.sciencedirect.com/science/article/pii/S0031320322002862,https://doi.org/10.1016/j.patcog.2022.108805,0031-3203,2022,108805,131,Pattern Recognition,Multiple-solutions RANSAC for finding axes of symmetry in fragments of objects,article,NASIRI2022108805,
"Recent semi-supervised learning methods use pseudo supervision as core idea, especially self-training methods that generate pseudo labels. However, pseudo labels are unreliable. Self-training methods usually rely on single model prediction confidence to filter low-confidence pseudo labels, thus remaining high-confidence errors and wasting many low-confidence correct labels. In this paper, we point out it is difficult for a model to counter its own errors. Instead, leveraging inter-model disagreement between different models is a key to locate pseudo label errors. With this new viewpoint, we propose mutual training between two different models by a dynamically re-weighted loss function, called Dynamic Mutual Training (DMT). We quantify inter-model disagreement by comparing predictions from two different models to dynamically re-weight loss in training, where a larger disagreement indicates a possible error and corresponds to a lower loss value. Extensive experiments show that DMT achieves state-of-the-art performance in both image classification and semantic segmentation. Our codes are released at https://github.com/voldemortX/DST-CBC.","Dynamic mutual training, Inter-model disagreement, Noisy pseudo label, Semi-supervised learning",Zhengyang Feng and Qianyu Zhou and Qiqi Gu and Xin Tan and Guangliang Cheng and Xuequan Lu and Jianping Shi and Lizhuang Ma,https://www.sciencedirect.com/science/article/pii/S0031320322002588,https://doi.org/10.1016/j.patcog.2022.108777,0031-3203,2022,108777,130,Pattern Recognition,DMT: Dynamic mutual training for semi-supervised learning,article,FENG2022108777,
"Action segmentation aims to split videos into segments of different actions. Recent work focuses on dealing with long-range dependencies of long, untrimmed videos, but still suffers from over-segmentation and performance saturation due to increased model complexity. This paper addresses the aforementioned issues through a divide-and-conquer strategy that first maximizes the frame-wise classification accuracy of the model and then reduces the over-segmentation errors. This strategy is implemented with the Dilation Passing and Reconstruction Network, composed of the Dilation Passing Network, which primarily aims to increase accuracy by propagating information of different dilations, and the Temporal Reconstruction Network, which reduces over-segmentation errors by temporally encoding and decoding the output features from the Dilation Passing Network. We also propose a weighted temporal mean squared error loss that further reduces over-segmentation. Through evaluations on the 50Salads, GTEA, and Breakfast datasets, we show that our model achieves significant results compared to existing state-of-the-art models.","Action segmentation, Temporal segmentation, Video understanding",Junyong Park and Daekyum Kim and Sejoon Huh and Sungho Jo,https://www.sciencedirect.com/science/article/pii/S003132032200245X,https://doi.org/10.1016/j.patcog.2022.108764,0031-3203,2022,108764,129,Pattern Recognition,Maximization and restoration: Action segmentation through dilation passing and temporal reconstruction,article,PARK2022108764,
"Leaf image patterns have been actively researched for plant species recognition. However, as a very challenging fine-grained pattern identification issue, cultivar recognition in which the leaf image patterns usually have very subtle difference among cultivars has not yet received considerable attention in computer vision and pattern recognition community. In this paper, a novel symmetric geometric configuration, named Symmetric Binary Tree (SBT) which has multiple symmetric branch pairs and can change in size, is designed to mine the multiple scale co-occurrence texture patterns. The resulting SBT descriptors encode both shape and texture features which make them more informative than the existing individual descriptors and co-occurrence features. A novel feature fusion scheme, named K-NN Based Handcrafted and Deep Features Fusion (KNN-HDFF) that encodes the neighbouring information of distance measure, is proposed for further boosting the retrieval performance. Extensive experiments conducted on the challenging soybean cultivar leaf image dataset and peanut cultivar leaf image dataset consistently indicate the superiority of the proposed method over the state-of-the-art methods on fine-grained leaf image retrieval. We also conduct extensive experiments of feature fusions using the proposed KNN-HDFF on the benchmark datasets and the experimental results prove its potential for improving the performance of cultivar identification which also indicates that fusing handcrafted and deep features may be the direction to address the challenging fine-grained image recognition problem.","Leaf image pattern, Species recognition, Fine-grained image recognition, Feature fusion, Image retrieval",Xin Chen and Bin Wang and Yongsheng Gao,https://www.sciencedirect.com/science/article/pii/S0031320322002503,https://doi.org/10.1016/j.patcog.2022.108769,0031-3203,2022,108769,129,Pattern Recognition,Symmetric Binary Tree Based Co-occurrence Texture Pattern Mining for Fine-grained Plant Leaf Image Retrieval,article,CHEN2022108769,
"Multi-view learning aims to explore a global common structure shared by different views collected from multiple individual sources. The nascent field of federated learning tries to learn a global model over distributed networks of devices. This paper shows that multi-view learning is naturally suited to address the feature heterogeneity of the federated setting. We propose a novel model, namely robust federated multi-view learning (FedMVL), which is considered in the following formulation: given a dataset with M views, it is required to train machine learning models while the M views are distributed across M devices or nodes. Considering the unique challenges like stragglers and fault tolerance in federated setting, we derive an iterative federated optimization algorithm that allows each node with the flexibility to approximately address its subproblem. To the best of our knowledge, our model for the first time considers the issues including high communication cost, fault tolerance, and stragglers for distributed multi-view learning. The proposed model also achieves encouraging performance on clustering task compared to closely related methods, as we illustrate through simulations on several real-world datasets.","Federated learning, Multi-view learning, Matrix factorization, Clustering",Shudong Huang and Wei Shi and Zenglin Xu and Ivor W. Tsang and Jiancheng Lv,https://www.sciencedirect.com/science/article/pii/S0031320322002989,https://doi.org/10.1016/j.patcog.2022.108817,0031-3203,2022,108817,131,Pattern Recognition,Efficient federated multi-view learning,article,HUANG2022108817,
"Online Action Detection (OAD) in videos addresses the problem of real-time analysis for streaming videos, i.e., only the observed historical video frames are available at prediction time. Considering the future frames observable only at the training stage as a form of privileged information, this paper adopts the Learning Using Privileged Information (LUPI) paradigm. Knowledge distillation (KD) is employed to transfer the privileged information from the offline teacher to the online student. Note that this setting is different from conventional KD because the difference between the teacher and student models mostly lies in the input data rather than the network architecture. To relieves the input information gap for the LUPI, we propose a simple but effective Privileged Knowledge Distillation (PKD) method that enforce KD loss to partial hidden features of the student model. Moreover, we also schedules a curriculum learning procedure to gradually distill the privileged information. This approach is named as Progressive Privileged Knowledge Distillation (PPKD). Compared to some OAD methods that explicitly predict future frames or feature, our approach avoids predicting stage and achieves state-of-the-art accuracy on two popular OAD benchmarks, TVSeries and THUMOS14.","Online action detection, Knowledge distillation, Privileged information, Curriculum learning",Peisen Zhao and Lingxi Xie and Jiajie Wang and Ya Zhang and Qi Tian,https://www.sciencedirect.com/science/article/pii/S0031320322002229,https://doi.org/10.1016/j.patcog.2022.108741,0031-3203,2022,108741,129,Pattern Recognition,Progressive privileged knowledge distillation for online action detection,article,ZHAO2022108741,
"Multiple object tracking (MOT) generally employs the paradigm of tracking-by-detection, where object detection and object tracking are executed conventionally using separate systems. Current progress in MOT has focused on detecting and tracking objects by harnessing the representational power of deep learning. Since existing methods always combine two submodules in the same network, it is particularly important that they must be trained effectively together. Therefore, the development of a suitable network architecture for the end-to-end joint training of detection and tracking submodules remains a challenging issue. The present work addresses this issue by proposing a novel architecture denoted as YOLOTracker that performs online MOT by exploiting a joint detection and embedding network. First, an efficient and powerful joint detection and tracking model is constructed to accomplish instance-level embedded training, which can ensure that the proposed tracker achieves highly accurate MOT results with high efficiency. Then, the Path Aggregation Network is employed to combine low-resolution and high-resolution features for integrating textural features and semantic information and mitigating the misalignment of the re-identification features. Experiments are conducted on three challenging and publicly available benchmark datasets and results demonstrate the proposed tracker outperforms other state-of-the-art MOT trackers in terms of accuracy and efficiency.","One-shot MOT, Joint detection and tracking, YOLO tracker",Sixian Chan and Yangwei Jia and Xiaolong Zhou and Cong Bai and Shengyong Chen and Xiaoqin Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322002746,https://doi.org/10.1016/j.patcog.2022.108793,0031-3203,2022,108793,130,Pattern Recognition,Online multiple object tracking using joint detection and embedding network,article,CHAN2022108793,
"Graph-based multi-view clustering, which aims to uncover clusters from multi-view data with graph clustering technique, is one of the most important multi-view clustering methods. Such methods usually perform eigen-decomposition first to solve the relaxed problem and then obtain the final cluster indicator matrix from eigenvectors by k-means or spectral rotation. However, such a two-step process may result in undesired clustering result since the two steps aim to solve different problems. In this paper, we propose a k-way normalized cut method for multi-view data, named as the Multi-view Discrete Normalized Cut (MDNC). The new method learns a set of implicit weights for each view to identify its quality, and a novel iterative algorithm is proposed to directly solve the new model without relaxation and post-processing. Moreover, we propose a new method to adjust the distribution of the implicit view weights to obtain better clustering result. Extensive experimental results show that the performance of our approach is superior to the state-of-the-art methods.","Clustering, Graph cut, Multi-view",Chen Wang and Xiaojun Chen and Feiping Nie and Joshua Zhexue Huang,https://www.sciencedirect.com/science/article/pii/S0031320322002904,https://doi.org/10.1016/j.patcog.2022.108809,0031-3203,2022,108809,130,Pattern Recognition,Directly solving normalized cut for multi-view data,article,WANG2022108809,
"Deep learning based object detection methods have achieved promising performance in controlled environments. However, these methods lack sufficient capabilities to handle underwater object detection due to these challenges: (1) images in the underwater datasets and real applications are blurry whilst accompanying severe noise that confuses the detectors and (2) objects in real applications are usually small. In this paper, we propose a Sample-WeIghted hyPEr Network (SWIPENET), and a novel training paradigm named Curriculum Multi-Class Adaboost (CMA), to address these two problems at the same time. Firstly, the backbone of SWIPENET produces multiple high resolution and semantic-rich Hyper Feature Maps, which significantly improve small object detection. Secondly, inspired by the human education process that drives the learning from easy to hard concepts, we propose the noise-robust CMA training paradigm that learns the clean data first and then move on to learns the diverse noisy data. Experiments on four underwater object detection datasets show that the proposed SWIPENET+CMA framework achieves better or competitive accuracy in object detection against several state-of-the-art approaches.","Underwater object detection, Curriculum Multi-Class Adaboost, Sample-weighted detection loss, Noisy data",Long Chen and Feixiang Zhou and Shengke Wang and Junyu Dong and Ning Li and Haiping Ma and Xin Wang and Huiyu Zhou,https://www.sciencedirect.com/science/article/pii/S0031320322004071,https://doi.org/10.1016/j.patcog.2022.108926,0031-3203,2022,108926,132,Pattern Recognition,SWIPENET: Object detection in noisy underwater scenes,article,CHEN2022108926,
"3D object detection plays a pivotal role in driver assistance systems and has practical requirements for small storage and fast inference. Monocular 3D detection alternatives abandon the complexity of LiDAR setup and pursues the effectiveness and efficiency of the vision scheme. In this work, we propose a set of anchor-free monocular 3D detectors called MonoPoly based on the keypoint paradigm. Specifically, we design a polynomial feature aggregation sampling module to extract multi-scale context features for auxiliary training and alleviate classification and localization misalignment through an attention-aware loss. Extensive experiments show that the proposed MonoPoly series achieves an excellent trade-off between performance and model size while maintaining real-time efficiency on KITTI and nuScenes datasets.","Object detection, Monocular 3D, Real-time, Light-weight",He Guan and Chunfeng Song and Zhaoxiang Zhang and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S0031320322004472,https://doi.org/10.1016/j.patcog.2022.108967,0031-3203,2022,108967,132,Pattern Recognition,MonoPoly: A practical monocular 3D object detector,article,GUAN2022108967,
"In this paper, we aim to tackle semi-and-weakly supervised semantic segmentation (SWSSS), where many image-level classification labels and a few pixel-level annotations are available. We believe the most crucial point for solving SWSSS is to produce high-quality pseudo labels, and our method deals with it from two perspectives. Firstly, we introduce a class-aware cross entropy (CCE) loss for network training. Compared to conventional cross entropy loss, CCE loss encourages the model to distinguish concurrent classes only and simplifies the learning target of pseudo label generation. Secondly, we propose a progressive cross training (PCT) method to build cross supervision between two networks with a dynamic evaluation mechanism, which progressively introduces high-quality predictions as additional supervision for network training. Our method significantly improves the quality of generated pseudo labels in the regime with extremely limited annotations. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods significantly. The code is released for public access11https://github.com/YudeWang/Learning-Pseudo-Label.","Semi-supervised, Weakly supervised, Semi-and-weakly supervised, Semantic segmentation, Pseudo label, Self-training",Yude Wang and Jie Zhang and Meina Kan and Shiguang Shan,https://www.sciencedirect.com/science/article/pii/S003132032200406X,https://doi.org/10.1016/j.patcog.2022.108925,0031-3203,2022,108925,132,Pattern Recognition,Learning pseudo labels for semi-and-weakly supervised semantic segmentation,article,WANG2022108925,
"Deep Learning approaches have brought solutions, with impressive performance, to general classification problems where wealthy of annotated data are provided for training. In contrast, less progress has been made in continual learning of a set of non-stationary classes, mainly when applied to unsupervised problems with streaming data. Here, we propose a novel incremental learning approach which combines a deep features encoder with an Open-Set Dynamic Ensembles of SVM, to tackle the problem of identifying individuals of interest (IoI) from streaming face data. From a simple weak classifier trained on a few video-frames, our method can use unsupervised operational data to enhance recognition. Our approach adapts to new patterns avoiding catastrophic forgetting and partially heals itself from miss-adaptation. Besides, to better comply with real world conditions, the system was designed to operate in an open-set setting. Results show a benefit of up to 15% F1-score increase respect to non-adaptive state-of-the-art methods.","Open-set face recognition, Incremental Learning, Self-updating, Adaptive biometrics, Video-surveillance",Eric Lopez-Lopez and Xose M. Pardo and Carlos V. Regueiro,https://www.sciencedirect.com/science/article/pii/S0031320322003661,https://doi.org/10.1016/j.patcog.2022.108885,0031-3203,2022,108885,131,Pattern Recognition,Incremental Learning from Low-labelled Stream Data in Open-Set Video Face Recognition,article,LOPEZLOPEZ2022108885,
"Deep learning-based multi-source unsupervised domain adaptation (MUDA) has been actively studied in recent years. Compared with single-source unsupervised domain adaptation (SUDA), domain shift in MUDA exists not only between the source and target domains but also among multiple source domains. Most existing MUDA algorithms focus on extracting domain-invariant representations among all domains whereas the task-specific decision boundaries among classes are largely neglected. In this paper, we propose an end-to-end trainable network that exploits domain Consistency Regularization for unsupervised Multi-source domain Adaptive classification (CRMA). CRMA aligns not only the distributions of each pair of source and target domains but also that of all domains. For each pair of source and target domains, we employ an intra-domain consistency to regularize a pair of domain-specific classifiers to achieve intra-domain alignment. In addition, we design an inter-domain consistency that targets joint inter-domain alignment among all domains. To address different similarities between multiple source domains and the target domain, we design an authorization strategy that assigns different authorities to domain-specific classifiers adaptively for optimal pseudo label prediction and self-training. Extensive experiments show that CRMA tackles unsupervised domain adaptation effectively under a multi-source setup and achieves superior adaptation consistently across multiple MUDA datasets.","Domain adaptation, Transfer learning, Adversarial learning, Feature alignment",Zhipeng Luo and Xiaobing Zhang and Shijian Lu and Shuai Yi,https://www.sciencedirect.com/science/article/pii/S0031320322004356,https://doi.org/10.1016/j.patcog.2022.108955,0031-3203,2022,108955,132,Pattern Recognition,Domain consistency regularization for unsupervised multi-source domain adaptive classification,article,LUO2022108955,
"The restoration of motion-blurred images has always been a complex problem in image restoration. The current single blurred image algorithm cannot very well solve the estimation error of motion blur parameters. A comprehensive motion-blurred image restoration framework is proposed, which includes motion-blurred data generation, blur parameter estimation, and image quality assessment of restored images. First, we designed and used four image data sets with different degrees of blurring. We innovatively propose a blur parameter estimation algorithm based on the particle swarm optimization (B-PSO) algorithm. The Naturalness Image Quality Evaluator (NIQE) is used as the fitness function of the PSO algorithm. The framework also introduces a polynomial-based radial basis function neural network (P-RBFNN) as a new image quality assessment (IQA) method, with good image classification performance. Test results from public datasets show that the proposed framework can accurately estimate blur parameters. The peak signal-to-noise ratio (PSNR) reaches 29.976Â dB, the structural similarity (SSIM) reaches 0.9044, and the classification rate is 96%. The proposed restoration framework produces the best image restoration results.","Motion-blurred image restoration framework, Point spread function, Blur parameter estimation based on the particle swarm optimization, Polynomial-based radial basis function neural network, Image Quality Assessment",Shengmin Zhao and Sung-Kwun Oh and Jin-Yul Kim and Zunwei Fu and Witold Pedrycz,https://www.sciencedirect.com/science/article/pii/S0031320322004630,https://doi.org/10.1016/j.patcog.2022.108983,0031-3203,2022,108983,132,Pattern Recognition,Motion-blurred image restoration framework based on parameter estimation and fuzzy radial basis function neural networks,article,ZHAO2022108983,
"The support vector data description (SVDD) approach serves as a de facto standard for one-class classification where the learning task entails inferring the smallest hyper-sphere to enclose target objects while linearly penalising the errors/slacks via an â1-norm penalty term. In this study, we generalise this modelling formalism to a general âp-norm (pâ¥1) penalty function on slacks. By virtue of an âp-norm function, in the primal space, the proposed approach enables formulating a non-linear cost for slacks. From a dual problem perspective, the proposed method introduces a dual norm into the objective function, thus, proving a controlling mechanism to tune into the intrinsic sparsity/uniformity of the problem for enhanced descriptive capability. A theoretical analysis based on Rademacher complexities characterises the generalisation performance of the proposed approach while the experimental results on several datasets confirm the merits of the proposed method compared to other alternatives.","One-class classification, Kernel methods, Support vector data description, -norm penalty",Shervin {Rahimzadeh Arashloo},https://www.sciencedirect.com/science/article/pii/S0031320322004113,https://doi.org/10.1016/j.patcog.2022.108930,0031-3203,2022,108930,132,Pattern Recognition,âp-Norm Support Vector Data Description,article,RAHIMZADEHARASHLOO2022108930,
"Anomaly detection (AD) has been receiving great attention as it plays a crucial role in many areas of basic research and industrial applications. However, most existing AD methods not only rely on training on normal data, but also ignore the multi-cluster nature of normal and abnormal patterns. To overcome these limitations, this paper proposes a novel method called Adaptive Aggregation-Distillation AutoEncoder (AADAE) for unsupervised anomaly detection. AADAE is built upon the density-based landmark selection in respect to representing diverse normal patterns. During training, AADAE adaptively updates the location and quantity of landmarks. Then, an aggregation-distillation mechanism is constructed: Firstly, it aggregates the latent representations of normal and anomalous to different landmark-guided regions within the convex polygon with landmarks as vertices, which minimizes the intra-class variation and promotes the separability of normal and abnormal samples. Secondly, the distillation mechanism is applied to obtain reliable detection results when there are anomalies in the training set. The aggregation process motivates AADAE to learn the distribution of multi-cluster normal samples with the help of landmarks, which in turn facilitates the distillation process to differentiate normal from anomalies for training. Extensive empirical studies on ten datasets from different application domains demonstrate the efficiency and generalization ability of the method.","Anomaly detection, Aggregation-distillation mechanism, Autoencoders, Unsupervised learning",Jiaqi Zhu and Fang Deng and Jiachen Zhao and Jie Chen,https://www.sciencedirect.com/science/article/pii/S0031320322003788,https://doi.org/10.1016/j.patcog.2022.108897,0031-3203,2022,108897,131,Pattern Recognition,Adaptive aggregation-distillation autoencoder for unsupervised anomaly detection,article,ZHU2022108897,
"Recent advances in deep neural networks (DNNs) have mainly focused on innovations in network architecture and loss function. In this paper, we introduce a flexible high-order coverage function (HCF) neuron model to replace the fully-connected (FC) layers. The approximation theorem and proof for the HCF are also presented to demonstrate its fitting ability. Unlike the FC layers, which cannot handle high-dimensional data well, the HCF utilizes weight coefficients and hyper-parameters to mine underlying geometries with arbitrary shapes in an n-dimensional space. To explore the power and potential of our HCF neuron model, a high-order coverage function neural network (HCFNN) is proposed, which incorporates the HCF neuron as the building block. Moreover, a novel adaptive optimization method for weights and hyper-parameters is designed to achieve effective network learning. Comprehensive experiments on nine datasets in several domains validate the effectiveness and generalizability of the HCF and HCFNN. The proposed method provides a new perspective for further developments in DNNs and ensures wide application in the field of image classification. The source code is available at https://github.com/Tough2011/HCFNet.git","DNNs, Neuron modeling, Heuristic algorithm, Back propagation, Computer vision",Xin Ning and Weijuan Tian and Zaiyang Yu and Weijun Li and Xiao Bai and Yuebao Wang,https://www.sciencedirect.com/science/article/pii/S0031320322003545,https://doi.org/10.1016/j.patcog.2022.108873,0031-3203,2022,108873,131,Pattern Recognition,HCFNN: High-order coverage function neural network for image classification,article,NING2022108873,
"Despite the significant progress of conditional image generation, it remains difficult to synthesize a ground-view panorama image from a top-view aerial image. Among the core challenges are the vast differences in image appearance and resolution between aerial images and panorama images, and the limited aside information available for top-to-ground viewpoint transformation. To address these challenges, we propose a new Progressive Attention Generative Adversarial Network (PAGAN) with two novel components: a multistage progressive generation framework and a cross-stage attention module. In the first stage, an aerial image is fed into a U-Net-like network to generate one local region of the panorama image and its corresponding segmentation map. Then, the synthetic panorama image region is extended and refined through the following generation stages with our proposed cross-stage attention module that passes semantic information forward stage-by-stage. In each of the successive generation stages, the synthetic panorama image and segmentation map are separately fed into an image discriminator and a segmentation discriminator to compute both later real and fake, as well as feature alignment score maps for discrimination. The model is trained with a novel orientation-aware data augmentation strategy based on the geometric relation between aerial and panorama images. Extensive experimental results on two cross-view datasets show that PAGAN generates high-quality panorama images with more convincing details than state-of-the-art methods.","Progressive attention GANs, Cross-view panorama image synthesis, Cross-stage attention, Orientation-aware data augmentation, Multi-stage image generation",Songsong Wu and Hao Tang and Xiao-Yuan Jing and Jianjun Qian and Nicu Sebe and Yan Yan and Qinghua Zhang,https://www.sciencedirect.com/science/article/pii/S003132032200365X,https://doi.org/10.1016/j.patcog.2022.108884,0031-3203,2022,108884,131,Pattern Recognition,Cross-view panorama image synthesis with progressive attention GANs,article,WU2022108884,
"Loss functions engineering and the assessment of prediction performances are two crucial and intertwined aspects of supervised machine learning. This paper focuses on binary classification to introduce a class of loss functions that are defined on probabilistic confusion matrices and that allow an automatic and a priori maximization of the skill scores. These loss functions are tested in various classification experiments, which show that the probability distribution function associated with the confusion matrices significantly impacts the outcome of the score maximization process, and that the proposed functions are competitive with other state-of-the-art probabilistic losses.","Supervised machine learning, Binary classification, Loss functions, Skill scores",F. Marchetti and S. Guastavino and M. Piana and C. Campi,https://www.sciencedirect.com/science/article/pii/S0031320322003946,https://doi.org/10.1016/j.patcog.2022.108913,0031-3203,2022,108913,132,Pattern Recognition,Score-Oriented Loss (SOL) functions,article,MARCHETTI2022108913,
"In this paper, to compute the firing strength values of type-2 fuzzy models, a soft version of minimum is presented, which endows the fuzzy model with the ability to solve large dimensional problems. In addition, a conjugate gradient method is borrowed to train the designed interval type-2 Takagi-Sugeno fuzzy model. Compared with the existing gradient-based learning strategy, this scheme can efficiently enhance the fuzzy model performance. Last but not least, convergence analysis for this modified interval type-2 Takagi-Sugeno fuzzy neural network (MIT2TSFNN) is conducted in detail, which proves that the gradient of the error function tends to zero with the iteration increasing (weak convergence) and the sequence of model parameters (weights) convergences to a fixed point (strong convergence). To validate the effectiveness of the proposed MIT2TSFNN and its theoretical results, simulation results of six regression and six classification problems are presented.","IT2 fuzzy model, Fuzzy neural network, Takagi-Sugeno, Conjugate gradient, Convergence",Tao Gao and Xiao Bai and Chen Wang and Liang Zhang and Jin Zheng and Jian Wang,https://www.sciencedirect.com/science/article/pii/S0031320322003429,https://doi.org/10.1016/j.patcog.2022.108861,0031-3203,2022,108861,131,Pattern Recognition,A modified interval type-2 Takagi-Sugeno fuzzy neural network and its convergence analysis,article,GAO2022108861,
"Deep neural networks have achieved remarkable success in machine learning, computer vision, and pattern recognition in the last few decades. Recent studies, however, show that neural networks (both shallow and deep) may be easily fooled by certain imperceptibly perturbed input samples called adversarial examples. Such security vulnerability has resulted in a large body of research in recent years because real-world threats could be introduced due to the vast applications of neural networks. To address the robustness issue to adversarial examples particularly in pattern recognition, robust adversarial training has become one mainstream. Various ideas, methods, and applications have boomed in the field. Yet, a deep understanding of adversarial training including characteristics, interpretations, theories, and connections among different models has remained elusive. This paper presents a comprehensive survey trying to offer a systematic and structured investigation on robust adversarial training in pattern recognition. We start with fundamentals including definition, notations, and properties of adversarial examples. We then introduce a general theoretical framework with gradient regularization for defending against adversarial samples - robust adversarial training with visualizations and interpretations on why adversarial training can lead to model robustness. Connections will also be established between adversarial training and other traditional learning theories. After that, we summarize, review, and discuss various methodologies with defense/training algorithms in a structured way. Finally, we present analysis, outlook, and remarks on adversarial training.","Adversarial examples, Adversarial training, Robust learning",Zhuang Qian and Kaizhu Huang and Qiu-Feng Wang and Xu-Yao Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322003703,https://doi.org/10.1016/j.patcog.2022.108889,0031-3203,2022,108889,131,Pattern Recognition,"A survey of robust adversarial training in pattern recognition: Fundamental, theory, and methodologies",article,QIAN2022108889,
"Modern unsupervised feature selection methods predominantly obtain the cluster structure and pseudo-labels information through spectral clustering. However, the pseudo-labels obtained by spectral clustering are usually mixed between positive and negative. Moreover, the Laplacian matrix in spectral clustering typically affects feature selection. Additionally, spectral clustering does not consider the interconnection information between data. To address these problems, this paper proposes uncorrelated feature selection via sparse latent representation and extended orthogonal least square discriminant analysis (OLSDA), which we term SLREO). Firstly, SLREO retains the interconnection between data by latent representation learning, and preserves the internal information between the data. In order to remove redundant interconnection information, an l2,1-norm constraint is applied to the residual matrix of potential representation learning. Secondly, SLREO obtains non-negative pseudo-labels through orthogonal least square discriminant analysis (OLSDA) of embedded non-negative manifold structure. It not only avoids the appearance of negative pseudo-labels, but also eliminates the effect of the Laplacian matrix on feature selection. The manifold information of the data is also preserved. Furthermore, the matrix of the learned latent representation and OLSDA is used as pseudo-labels information. It not only ensures that the generated pseudo-labels are non-negative, but also makes the pseudo-labels closer to the true class labels. Finally, in order to avoid trivial solutions, an uncorrelated constraint and l2,1-norm constraint are imposed on the feature transformation matrix. These constraints ensure row sparsity of the feature transformation matrix, select low-redundant and discriminative features, and improve the effect of feature selection. Experimental results show that the Clustering Accuracy (ACC) and Normalized Mutual Information (NMI) of SLREO are significantly improved, as compared with six other published algorithms, tested on 11 benchmark datasets.","Unsupervised feature selection, Sparse latent representation, OLSDA, Pseudo-labels, Uncorrelated constraints",Ronghua Shang and Jiarui Kong and Weitong Zhang and Jie Feng and Licheng Jiao and Rustam Stolkin,https://www.sciencedirect.com/science/article/pii/S0031320322004460,https://doi.org/10.1016/j.patcog.2022.108966,0031-3203,2022,108966,132,Pattern Recognition,Uncorrelated feature selection via sparse latent representation and extended OLSDA,article,SHANG2022108966,
"Anchor-free detectors basically formulate object detection as dense classification and regression. For popular anchor-free detectors, it is common to introduce an individual prediction branch to estimate the quality of localization. The following inconsistencies are observed when we delve into the practices of classification and quality estimation. Firstly, for some adjacent samples which are assigned completely different labels, the trained model would produce similar classification scores. This violates the training objective and leads to performance degradation. Secondly, it is found that detected bounding boxes with higher confidences contrarily have smaller overlaps with the corresponding ground-truth. Accurately localized bounding boxes would be suppressed by less accurate ones in the Non-Maximum Suppression (NMS) procedure. To address the inconsistency problems, the Dynamic Smooth Label Assignment (DSLA) method is proposed. Based on the concept of centerness originally developed in FCOS, a smooth assignment strategy is proposed. The label is smoothed to a continuous value in [0,1] to make a steady transition between positive and negative samples. Intersection-of-Union (IoU) is predicted dynamically during training and is coupled with the smoothed label. The dynamic smooth label is assigned to supervise the classification branch. Under such supervision, quality estimation branch is naturally merged into the classification branch, which simplifies the architecture of anchor-free detector. Comprehensive experiments are conducted on the MS COCO benchmark. It is demonstrated that, DSLA can significantly boost the detection accuracy by alleviating the above inconsistencies for anchor-free detectors. Our codes are released at https://github.com/YonghaoHe/DSLA.","Convolutional neural network, Object detection, Centerness score, Intersection-of-union",Hu Su and Yonghao He and Rui Jiang and Jiabin Zhang and Wei Zou and Bin Fan,https://www.sciencedirect.com/science/article/pii/S0031320322003491,https://doi.org/10.1016/j.patcog.2022.108868,0031-3203,2022,108868,131,Pattern Recognition,DSLA: Dynamic smooth label assignment for efficient anchor-free object detection,article,SU2022108868,
"Image fusion plays a pivotal role in numerous high-level computer vision tasks. Existing deep learning-based image fusion methods usually leverage an implicit manner to achieve feature extraction, which would cause some characteristics of source images, e.g., contrast and structural information, are unable to be fully extracted and integrated into the fused images. In this work, we propose an infrared and visible image fusion method via parallel scene and texture learning. Our key objective is to deploy two branches of deep neural networks, namely the content branch and detail branch, to synchronously extract different characteristics from source images and then reconstruct the fused image. The content branch focuses primarily on coarse-grained information and is deployed to estimate the global content of source images. The detail branch primarily pays attention to fine-grained information, and we design an omni-directional spatially variant recurrent neural networks in this branch to model the internal structure of source images more accurately and extract texture-related features in an explicit manner. Extensive experiments show that our approach achieves significant improvements over state-of-the-arts on qualitative and quantitative evaluations with comparatively less running time consumption. Meanwhile, we also demonstrate the superiority of our fused results in the object detection task. Our code is available at: https://github.com/Melon-Xu/PSTLFusion.","Image fusion, Infrared, Scene and texture learning, Recurrent neural network",Meilong Xu and Linfeng Tang and Hao Zhang and Jiayi Ma,https://www.sciencedirect.com/science/article/pii/S0031320322004101,https://doi.org/10.1016/j.patcog.2022.108929,0031-3203,2022,108929,132,Pattern Recognition,Infrared and visible image fusion via parallel scene and texture learning,article,XU2022108929,
"Contact tracking plays an important role in the epidemiological investigation of COVID-19, which can effectively reduce the spread of the epidemic. As an excellent alternative method for contact tracking, mobile phone location-based methods are widely used for locating and tracking contacts. However, current inaccurate positioning algorithms that are widely used in contact tracking lead to the inaccurate follow-up of contacts. Aiming to achieve accurate contact tracking for the COVID-19 contact group, we extend the analysis of the GPS data to combine GPS data with video surveillance data and address a novel task named group activity trajectory recovery. Meanwhile, a new dataset called GATR-GPS is constructed to simulate a realistic scenario of COVID-19 contact tracking, and a coordinated optimization algorithm with a spatio-temporal constraint table is further proposed to realize efficient trajectory recovery of pedestrian trajectories. Extensive experiments on the novel collected dataset and commonly used two existing person re-identification datasets are performed, and the results evidently demonstrate that our method achieves competitive results compared to the state-of-the-art methods.","Contact tracking, COVID-19, Group activity, Trajectory recovery",Chao Wang and XiaoChen Wang and Zhongyuan Wang and WenQian Zhu and Ruimin Hu,https://www.sciencedirect.com/science/article/pii/S0031320322003892,https://doi.org/10.1016/j.patcog.2022.108908,0031-3203,2022,108908,132,Pattern Recognition,COVID-19 contact tracking by group activity trajectory recovery over camera networks,article,WANG2022108908,
"In this paper, we present a novel 3D point cloud harvesting method, which can harvest 3D points from an estimated surface distribution in an unsupervised manner (i.e., an input is a prior distribution). Our method outputs the surface distribution of a 3D object and samples 3D points from the distribution based on the proposed progressive random sampling strategy. The progressive sampling regards a prior distribution itself as a network input and uses a progressively increasing number of latent variables for training, which can diversify the coordinates of 3D points with fast convergence. Subsequently, our stochastic instance normalization transforms the implicit distribution into other distributions, which enables diverse shapes of 3D objects. Experimental results show that our method is competitive with other state-of-the-art methods. Our method can harvest an arbitrary number of 3D points, wherein the 3D object is represented in detail with highly dense 3D points or a part of it is described with partial sampling.","3D point cloud harvesting, Progressive sampling, Stochastic instance normalization",Dong Wook Shu and Sung Woo Park and Junseok Kwon,https://www.sciencedirect.com/science/article/pii/S0031320322004587,https://doi.org/10.1016/j.patcog.2022.108978,0031-3203,2022,108978,132,Pattern Recognition,Wasserstein distributional harvesting for highly dense 3D point clouds,article,SHU2022108978,
"Generalizable person re-identification (re-ID) has attracted growing attention due to its powerful adaptation capability in the unseen data domain. However, existing solutions often neglect either crossing cameras (e.g., illumination and resolution differences) or pedestrian misalignments (e.g., viewpoint and pose discrepancies), which easily leads to poor generalization capability when adapted to the new domain. In this paper, we formulate these difficulties as: 1) Camera-Camera (CC) problem, which denotes the various human appearance changes caused by different cameras; 2) Camera-Person (CP) problem, which indicates the pedestrian misalignments caused by the same identity person under different camera viewpoints or changing pose. To solve the above issues, we propose a Bi-stream Generative Model (BGM) to learn the fine-grained representations fused with camera-invariant global feature and pedestrian-aligned local feature, which contains an encoding network and two stream decoding sub-network. Guided by original pedestrian images, one stream is employed to learn a camera-invariant global feature for the CC problem via filtering cross-camera interference factors. For the CP problem, another stream learns a pedestrian-aligned local feature for pedestrian alignment using information-complete densely semantically aligned part maps. Moreover, a part-weighted loss function is presented to reduce the influence of missing parts on pedestrian alignment. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on the large-scale generalizable re-ID benchmarks, involving domain generalization setting and cross-domain setting.","Person re-identification, Generalizable re-ID, Camera-Camera problem, Camera-Person problem",Xin Xu and Wei Liu and Zheng Wang and Ruimin Hu and Qi Tian,https://www.sciencedirect.com/science/article/pii/S0031320322004344,https://doi.org/10.1016/j.patcog.2022.108954,0031-3203,2022,108954,132,Pattern Recognition,Towards generalizable person re-identification with a bi-stream generative model,article,XU2022108954,
"The overestimation in Deep Deterministic Policy GradientsÂ (DDPG) caused by value approximation error may result in unstable policy training. Twin Delayed Deep Deterministic Policy GradientÂ (TD3)Â addresses the overestimation but suffers from the underestimation. In this paper, we propose a Co-Regularization based Deep Deterministic (CoD2) policy gradient method to mitigate the estimation bias. Two learners characterized by overestimated and underestimated biases are trained with Co-regularization to achieve this goal. The overestimated and underestimated values are updated conservatively in CoD2 for policy evaluation. Experimental results show that our method achieves comparable performance compared with other methods.","Reinforcement learning, Overestimation, Underestimation, Co-training, Deterministic policy gradient",Yao Li and YuHui Wang and YaoZhong Gan and XiaoYang Tan,https://www.sciencedirect.com/science/article/pii/S0031320322003533,https://doi.org/10.1016/j.patcog.2022.108872,0031-3203,2022,108872,131,Pattern Recognition,Alleviating the estimation bias of deep deterministic policy gradient via co-regularization,article,LI2022108872,
"The non-linearity between human perception and image brightness levels results in different definitions of NORMAL-light. Thus, most existing low-light image enhancement methods which produce one-to-one mapping can not meet the aesthetic demand. Other pioneers enhance low-light images guided by a given value. However, the inherent problem of non-linearity will cause poor usability. To this end, we propose a user-friendly neural network for multi-level low-light image enhancement. Inspired by style transfer, our method decomposes an image into content component feature and luminance component feature in the latent space. Then we enhance the image brightness to different levels by concatenating the content components from low-light images and the luminance components from reference images. The network meets various user requirements by selecting different brightness references. Moreover, information except for brightness is preserved to alleviate color distortion. Extensive experiments demonstrate the superiority of our network against existing methods.","Low-light image enhancement, Multi-level mapping, Arbitrary references, Codec network, Decomposition, Concatenation",Yaânan Wang and Zhuqing Jiang and Chang Liu and Kai Li and Aidong Men and Haiying Wang and Xiaobo Chen,https://www.sciencedirect.com/science/article/pii/S003132032200348X,https://doi.org/10.1016/j.patcog.2022.108867,0031-3203,2022,108867,131,Pattern Recognition,Shedding light on images: Multi-level image brightness enhancement guided by arbitrary references,article,WANG2022108867,
"With the development of IoT and mobile devices, cross-device palmprint recognition is becoming an emerging research topic in multimedia for its great application potential. Due to the diverse characteristics of different devices, e.g.resolution or artifacts caused by post-processing, cross-device palmprint recognition remains a challenging problem. In this paper, we make efforts to improve cross-device palmprint recognition in two aspects: (1) we put forward a novel distribution-based loss to narrow the representation gap across devices, and (2) we establish a new cross-device benchmark based on existing palmprint recognition datasets. Different from many recent studies that only utilize instance-level or pairwise-level information between devices, the proposed progressive target distribution loss (PTD loss) uses the distributional information. Moreover, we establish a progressive target mechanism that will be dynamically updated during training, making the optimization easier and smoother. The newly established benchmark contains more samples and more types of IoT devices than previous benchmarks, which can facilitate cross-device palmprint research. Extensive comparisons on several benchmarks reveal that: (1) our method outperforms other cross-device biometric recognition approaches significantly; (2) our method presents superior performance compared to SOTA competitors on several general palmprint recognition benchmarks; Code and data are openly available at https://kaizhao.net/palmprint.","Palmprint recognition, Deep learning, Loss function, Biometric recognition, Person Reidentification",Lei Shen and Yingyi Zhang and Kai Zhao and Ruixin Zhang and Wei Shen,https://www.sciencedirect.com/science/article/pii/S0031320322004228,https://doi.org/10.1016/j.patcog.2022.108942,0031-3203,2022,108942,132,Pattern Recognition,Distribution alignment for cross-device palmprint recognition,article,SHEN2022108942,
"The variance-ratio binary multi-layer classifier (VRBMLC) has been recently proposed and shown to outperform conventional binary decision trees (BDTs). Though effective with better interpretability, the VRBMLC generates deep layers of tree nodes as it employs a one-feature-at-a-time binary split at each layer. To further condense the tree depth and enhance the classification performance, this research proposes a multivariate multi-layer classifier that applies a variance-ratio criterion to enable ternary splits of each tree node and that integrates the oblique discriminant hyperplane in the tree node. We benchmark 16 state-of-the-art univariate and multivariate classifiers on 43 publicly available datasets. The results show that the proposed methods greatly simplify the tree structure and yield a significantly higher average accuracy.","Classification, Classifiers, Multivariate decision tree, Machine learning, Tree construction",Huanze Zeng and Argon Chen,https://www.sciencedirect.com/science/article/pii/S0031320322003776,https://doi.org/10.1016/j.patcog.2022.108896,0031-3203,2022,108896,131,Pattern Recognition,Multivariate multi-layer classifier,article,ZENG2022108896,
"The intrusion detection system (IDS) has gained a rapid increase of interest due to its widely recognized potential in various security fields, however, it suffers from several challenges. Different network datasets have several redundant and irrelevant features that affect the decision of the IDS classifier. Therefore, it is essential to decrease these features to improve the system performance. In this paper, an efficient wrapper feature selection method is proposed for improving the performance and decreasing the processing time of the IDS. The proposed approach employs a differential evaluation algorithm to select the useful features whilst the extreme learning machine classifier is applied after feature selection to evaluate the selected features. Many experiments are performed using the full NSL-KDD dataset to evaluate the performance of the proposed method. The results prove that the proposed approach can efficiently reduce the features, increase the accuracy, reduce the false alarm rates, and improve the processing time of the IDS in comparison to other recent related works.","Intrusion detection system (IDS), Feature selection, Differential evolution (DE), Extreme learning machine (ELM), NSL-KDD",Wathiq Laftah Al-Yaseen and Ali Kadhum Idrees and Faezah Hamad Almasoudy,https://www.sciencedirect.com/science/article/pii/S0031320322003934,https://doi.org/10.1016/j.patcog.2022.108912,0031-3203,2022,108912,132,Pattern Recognition,Wrapper feature selection method based differential evolution and extreme learning machine for intrusion detection system,article,ALYASEEN2022108912,
"Existing shape from focus (SFF) techniques cannot preserve depth edges and fine structural details from a sequence of multi-focus images. Moreover, noise in the sequence affects the accuracy of the depth map. In this paper, a novel depth enhancement algorithm for the SFF based on an adaptive weighted guided image filtering (AWGIF) is proposed to address the above issues. The AWGIF is applied to decompose an initial depth map estimated by the traditional SFF into base and detail layers. In order to preserve the edges accurately in the refined depth map, the guidance image is constructed from the sequence, and the coefficient of the AWGIF is utilized to suppress the noise while enhancing the fine depth details. Experiments on real and synthetic objects demonstrate the superiority of our algorithm in terms of anti-noise, and the ability to preserve depth edges and fine structural details w.r.t. existing methods.","Shape from focus, Depth enhancement, Adaptive weighted guided image filtering, Edge-preserving, Robustness",Yuwen Li and Zhengguo Li and Chaobing Zheng and Shiqian Wu,https://www.sciencedirect.com/science/article/pii/S0031320322003818,https://doi.org/10.1016/j.patcog.2022.108900,0031-3203,2022,108900,131,Pattern Recognition,Adaptive weighted guided image filtering for depth enhancement in shape-from-focus,article,LI2022108900,
"The training loss function that enforces certain training sample distribution patterns plays a critical role in building a re-identification (ReID) system. Besides the basic requirement of discrimination, i.e., the features corresponding to different identities should not be mixed, additional intra-class distribution constraints, such as features from the same identities should be close to their centers, have been adopted to construct losses. Despite the advances of various new loss functions, it is still challenging to strike the balance between the need of reducing the intra-class variation and allowing certain distribution freedom. Traditional intra-class losses try to shrink samples of the same class into one point in the feature space and may easily drop their intra-class similarity structure. In this paper, we propose a new loss based on center predictivity, that is, a sample must be positioned in a location of the feature space such that from it we can roughly predict the location of the center of same-class samples. The prediction error is then regarded as a loss called Center Prediction Loss (CPL). Unlike most existing metric learning loss functions, CPL involves learnable parameters, i.e., the center predictor, which brings a remarkable change in the properties of the loss. In particular, it allows higher freedom in intra-class distributions. And the parameters in CPL will be discarded after training. Extensive experiments on various real-world ReID datasets show that the proposed loss can achieve superior performance and can also be complementary to existing losses.","Person re-identification, Loss, Deep metric learning",Lu Yang and Yunlong Wang and Lingqiao Liu and Peng Wang and Yanning Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322004290,https://doi.org/10.1016/j.patcog.2022.108949,0031-3203,2022,108949,132,Pattern Recognition,Center Prediction Loss for Re-identification,article,YANG2022108949,
"Weakly supervised semantic segmentationÂ (WSSS) aims to produce pixel-wise class predictions with only image-level labels for training. To this end, previous methods adopt the common pipeline: they generate pseudo masks from class activation mapsÂ (CAMs) and use such masks to supervise segmentation networks. However, it is challenging to derive comprehensive pseudo masks that cover the whole extent of objects due to the local property of CAMs, i.e., they tend to focus solely on small discriminative object parts. In this paper, we associate the locality of CAMs with the texture-biased property of convolutional neural networks (CNNs). Accordingly, we propose to exploit shape information to supplement the texture-biased CNN features, thereby encouraging mask predictions to be not only comprehensive but also well-aligned with object boundaries. We further refine the predictions in an online fashion with a novel refinement method that takes into account both the class and the color affinities, in order to generate reliable pseudo masks to supervise the model. Importantly, our model is end-to-end trained within a single-stage framework and therefore efficient in terms of the training cost. Through extensive experiments on PASCAL VOC 2012, we validate the effectiveness of our method in producing precise and shape-aligned segmentation results. Specifically, our model surpasses the existing state-of-the-art single-stage approaches by large margins. What is more, it also achieves a new state-of-the-art performance over multi-stage approaches, when adopted in a simple two-stage pipeline without bells and whistles.","Semantic segmentation, Weakly supervised learning, Texture biases, Shape cues",Sungpil Kho and Pilhyeon Lee and Wonyoung Lee and Minsong Ki and Hyeran Byun,https://www.sciencedirect.com/science/article/pii/S0031320322004332,https://doi.org/10.1016/j.patcog.2022.108953,0031-3203,2022,108953,132,Pattern Recognition,Exploiting shape cues for weakly supervised semantic segmentation,article,KHO2022108953,
"Retaining a small subset to replay is a direct and effective way to prevent catastrophic forgetting in continual learning. However, due to data complexity and restricted memory, picking a proper subset for rehearsal is challenging and still being explored. In this work, we present a Multi-criteria Subset Selection approach that can stabilize and advance replay-based continual learning. The method picks rehearsal samples by integrating multiple criteria, including distance to prototype, intra-class cluster variation, and classifier loss. By doing so, it maximizes the comprehensive representation power of the sampled subset by ensuring its representativeness, diversity, and discriminability. We empirically find that singular criteria are likely to fail in particular tasks, while multi-criteria minimizes this risk and stabilizes task training throughout the continual learning process. Moreover, our method improves replay-based methods consistently and achieves state-of-the-art performance on both CIFAR100 and Tiny-Imagenet datasets.","Continual Learning, Multiple Criteria, Rehersal Method, Learning to learn",Chen Zhuang and Shaoli Huang and Gong Cheng and Jifeng Ning,https://www.sciencedirect.com/science/article/pii/S0031320322003880,https://doi.org/10.1016/j.patcog.2022.108907,0031-3203,2022,108907,132,Pattern Recognition,Multi-criteria Selection of Rehearsal Samples for Continual Learning,article,ZHUANG2022108907,
"This paper introduces a novel supervised dimension reduction method for classification and regression problems using reproducing kernel Hilbert spaces. The proposed approach takes advantage of the modeling power of kernel exponential families to extract nonlinear summary statistics of the data that are sufficient to preserve information about the target response. For the special case of finite dimensional exponential family distributions, the proposed method is shown to simplify the known solutions for sufficient dimension reduction. A connection with support vector machines is shown and exploited to obtain efficient estimation procedures. Experiments with simulated and real data illustrate the potential of the proposed approach.","Discriminant analysis, Sufficient dimension reduction, Reproducing kernel Hilbert spaces, Support vector machine",IsaÃ­as IbaÃ±ez and Liliana Forzani and Diego Tomassi,https://www.sciencedirect.com/science/article/pii/S0031320322004058,https://doi.org/10.1016/j.patcog.2022.108933,0031-3203,2022,108933,132,Pattern Recognition,Generalized discriminant analysis via kernel exponential families,article,IBANEZ2022108933,
"The existing learning-based dynamic scene deblurring methods have made good progress to some extent. However, these methods are usually based on multiscale strategy, which has the following shortcomings: (1) The bilinear downsampling operation will cause some loss of important high-frequency information, e.g., strong edges, which also further affects the network learning a better deblurring mapping. (2) Existing methods only use a single activation function, which limits the ability of the network model to fit data and causes the network performance to be easily saturated. Therefore, we propose an end-to-end progressive downsampling and adaptive guidance network called PDAG-Net for solving above problems. The proposed PDAG-Net can retain more strong edges and other high-frequency information of a blurry image so as to make the network learn a more effective deblurring mapping between the input and label images. In the proposed PDAG-Net, we implement a multiscale blended activation residual block called MSBA-ResBlock for learning the nonlinear characteristics of dynamic scene blur, which can also alleviate the performance saturation problem caused by a single activation function and improve multiscale feature extraction ability. Finally, we propose a multisupervision strategy for obtaining more robust and effective features and making the network possess more stable trainging and faster convergence. Extensive experimental results on a public dataset indicate that the proposed network outperforms the state-of-the-art image deblurring methods.","Progressive downsampling, Adaptive guidance, Blended activation, Multisupervision, Dynamic scene deblurring",Jinkai Cui and Weihong Li and Wei Guo and Weiguo Gong,https://www.sciencedirect.com/science/article/pii/S003132032200468X,https://doi.org/10.1016/j.patcog.2022.108988,0031-3203,2022,108988,132,Pattern Recognition,Progressive downsampling and adaptive guidance networks for dynamic scene deblurring,article,CUI2022108988,
"This paper presents a novel feature selection method based on the conditional mutual information (CMI). The proposed High Order Conditional Mutual Information Maximization (HOCMIM) method incorporates high order dependencies into the feature selection procedure and has a straightforward interpretation due to its bottom-up derivation. The HOCMIM is derived from the CMIâs chain expansion and expressed as a maximization optimization problem. The maximization problem is solved using a greedy search procedure, which speeds up the entire feature selection process. The experiments are run on a set of benchmark datasets (20 in total). The HOCMIM is compared with eighteen state-of-the-art feature selection algorithms, from the results of two supervised learning classifiers (Support Vector Machine and K-Nearest Neighbor). The HOCMIM achieves the best results in terms of accuracy and shows to be faster than high order feature selection counterparts.","Feature selection, Mutual information, Information theory, Pattern recognition",Francisco Souza and Cristiano Premebida and Rui AraÃºjo,https://www.sciencedirect.com/science/article/pii/S0031320322003764,https://doi.org/10.1016/j.patcog.2022.108895,0031-3203,2022,108895,131,Pattern Recognition,High-order conditional mutual information maximization for dealing with high-order dependencies in feature selection,article,SOUZA2022108895,
"Existing online video processing methods such as online action detection focus on a frame-level understanding for high responsiveness. However, it has a fundamental limitation in that it lacks instance-level understanding of videos, making it difficult to be applied to higher-level vision tasks. The instance-level action detection, known as Temporal Action Localization (TAL), have limitations when applying to the online settings. In this work, we introduce a new task that aims to detect action instances of videos in an online setting, named Online Temporal Action Localization (OnTAL). To tackle this problem, we propose a 2-Pass End/Start detection Network (2PESNet) that detects action instances by effectively finding the start and end of an action instance. Additionally, we propose a two-stage action end detection method to further improve the performance. Extensive experiments on THUMOSâ14 and ActivityNet v1.3 demonstrate that our model is able to take both accuracy and responsiveness when predicting action instances from streaming videos.","Online video understanding, Temporal action localization",Young Hwi Kim and Seonghyeon Nam and Seon Joo Kim,https://www.sciencedirect.com/science/article/pii/S0031320322003521,https://doi.org/10.1016/j.patcog.2022.108871,0031-3203,2022,108871,131,Pattern Recognition,2PESNet: Towards online processing of temporal action localization,article,KIM2022108871,
"Estimating 3D human poses from a single image is an important task in computer graphics. Most model-based estimation methods represent the labeled/detected 2D poses and the projection of approximated 3D poses using vector representations of body joints. However, such lower-dimensional vector representations fail to maintain the spatial relations of original body joints, because the representations do not consider the inherent structure of body joints. In this paper, we propose JSL3d, a novel joint subspace learning approach with implicit structure supervision based on Sparse Representation (SR) model, capturing the latent spatial relations of 2D body joints by an end-to-end autoencoder network. JSL3djointly combines the learned latent spatial relations and 2D joints as inputs for the standard SR inference frame. The optimization is simultaneously processed via geometric priors in both latent and original feature spaces. We have evaluated JSL3dusing four large-scale and well-recognized benchmarks, including Human3.6M, HumanEva-I, CMU MoCap and MPII. The experiment results demonstrate the effectiveness of JSL3d.",", , ,",Mengxi Jiang and Shihao Zhou and Cuihua Li and Yunqi Lei,https://www.sciencedirect.com/science/article/pii/S0031320322004459,https://doi.org/10.1016/j.patcog.2022.108965,0031-3203,2022,108965,132,Pattern Recognition,JSL3d: Joint subspace learning with implicit structure supervision for 3D pose estimation,article,JIANG2022108965,
"Data distribution alignment and clustering-based self-training are two feasible solutions to tackle unsupervised domain adaptation (UDA) on person re-identification (re-ID). Most existing alignment-based methods solely learn the source domain decision boundaries and align the data distribution of the target domain to the source domain, thus the re-ID performance on the target domain completely depends on the shared decision boundaries and how well the alignment is performed. However, two domains can hardly be precisely aligned because of the label space discrepancy of two domains, resulting in poor target domain re-ID performance. Although clustering-based self-training approaches could learn independent decision boundaries on the pseudo-labelled target domain data, they ignore both the accurate ID-related information of the labelled source domain data and the underlying relations between two domains. To fully exploit the source domain data to learn discriminative target domain ID-related features, in this paper, we propose a novel cross-domain alignment method in the homogeneous distance space, which is constructed by the newly designed stair-stepping alignment (SSA) matcher. Such alignment method can be integrated into both alignment-based framework and clustering-based framework. Extensive experiments validate the effectiveness of our proposed alignment method in these two frameworks. We achieve superior performance when the proposed alignment module is integrated into the clustering-based framework. Codes will be available at: http://github.com/Dingyuan-Zheng/HDS.","Person re-identification, Unsupervised domain adaptation, Distribution alignment, Clustering, Pseudo label",Dingyuan Zheng and Jimin Xiao and Yunchao Wei and Qiufeng Wang and Kaizhu Huang and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S0031320322004216,https://doi.org/10.1016/j.patcog.2022.108941,0031-3203,2022,108941,132,Pattern Recognition,Unsupervised domain adaptation in homogeneous distance space for person re-identification,article,ZHENG2022108941,
"In this paper, we aim to tackle the problem of unsupervised domain adaptation (UDA) of semantic segmentation and improve the UDA performance with a novel conception of learning intra-domain style-invariant representation. Previous UDA methods focused on reducing the inter-domain inconsistency between the source domain and the target domain. However, due to the different data distributions of the two domains, reducing the inter-domain inconsistency cannot ensure the generalization ability of the trained model in the target domain. Therefore, to improve the UDA performance, we take into consideration the intra-domain diversity of the target domain for the first time in studies on UDA and aim to train the model to generalize well to the diverse intra-domain styles. To achieve this, we propose a self-ensembling method to learn the intra-domain style-invariant representation and we introduce a semantic-aware multimodal image-to-image translation model to obtain images with diversified intra-domain styles. Our method achieves state-of-the-art performance on two synthetic-to-real adaptation benchmarks, and we demonstrate the effectiveness of our method by conducting extensive experiments.","Style-invariant representation, Self-ensembling, Domain adaptation",Zongyao Li and Ren Togo and Takahiro Ogawa and Miki Haseyama,https://www.sciencedirect.com/science/article/pii/S0031320322003922,https://doi.org/10.1016/j.patcog.2022.108911,0031-3203,2022,108911,132,Pattern Recognition,Learning intra-domain style-invariant representation for unsupervised domain adaptation of semantic segmentation,article,LI2022108911,
"Generalizing beyond the experiences has a significant role in developing robust and practical machine learning systems. It has been shown that current Visual Question Answering (VQA) models are over-dependent on the language-priors (spurious correlations between question-types and their most frequent answers) from the train set and pose poor performance on Out-of-Distribution (OOD) test sets. This conduct negatively affects the robustness of VQA models and restricts them from being utilized in real-world situations. This paper shows that the sequence model architecture used in the question-encoder has a significant role in the OOD performance of VQA models. To demonstrate this, we performed a detailed analysis of various existing RNN-based and Transformer-based question-encoders, and along, we proposed a novel Graph attention network (GAT)-based question-encoder. Our study found that a better choice of sequence model in the question-encoder reduces the over-fit to language biases and improves OOD performance in VQA even without using any additional relatively complex bias-mitigation approaches.","Visual question answering, Out-of-distribution performance, Gated recurrent unit, Transformer, Graph attention network",Gouthaman KV and Anurag Mittal,https://www.sciencedirect.com/science/article/pii/S0031320322003648,https://doi.org/10.1016/j.patcog.2022.108883,0031-3203,2022,108883,131,Pattern Recognition,On the role of question encoder sequence model in robust visual question answering,article,KV2022108883,
"Recently, attention mechanisms have shown great potential in improving the performance of mobile networks. Typically, they involve 2D symmetric convolution operations or generate 2D attention maps. However, such manners usually introduce high computational cost and large memory consumption, increasing the computational burden of mobile networks. To address this problem, we propose a novel lightweight attention mechanism, called Dimension-Aware Attention (DAA) block, by modeling the intra-dependencies of each dimension of the input feature map. Specifically, we factorize the channel and spatial attention by three parallel feature vector encoding branches, where stacked 1D asymmetric convolution operations can be naturally leveraged to capture large receptive fields. In this way, channel-aware, horizontal-aware, and vertical-aware attention vectors are extracted to effectively encode multi-dimensional information and greatly reduce the computational complexity of mobile networks. Experiments on multiple vision tasks demonstrate that our DAA block achieves better accuracy against state-of-the-art attention mechanisms with much lower computational operations. Our code is available at https://github.com/rymo96/DAANet.","Efficient mobile networks, Attention mechanism, Feature enhancement, Multi-branch factorization, Multi-dimensional information",Rongyun Mo and Shenqi Lai and Yan Yan and Zhenhua Chai and Xiaolin Wei,https://www.sciencedirect.com/science/article/pii/S0031320322003806,https://doi.org/10.1016/j.patcog.2022.108899,0031-3203,2022,108899,131,Pattern Recognition,Dimension-aware attention for efficient mobile networks,article,MO2022108899,
"Recently, block-based design methods have shown effectiveness in image restoration tasks, which are usually designed in a handcrafted manner and have computation and memory consumption challenges in practice. In this paper, we propose a joint operation and attention block search algorithm for image restoration, which focuses on searching for optimal combinations of operation blocks and attention blocks. Specifically, we first construct two search spaces: operation block search space and attention block search space. The former is used to explore the suitable operation of each layer and aims to construct a lightweight and effective operation search module (OSM). The latter is applied to discover the optimal connection of various attention mechanisms and aims to enhance the feature expression. The searched structure is called the attention search module (ASM). Then we combine OSM and ASM to construct a joint search module (JSM), which serves as the basic module to build the final network. Moreover, we propose a cross-scale fusion module (CSFM) to effectively integrate multiple hierarchical features from JSMs, which helps to mine feature corrections of intermediate layers. Extensive experiments on image super-resolution, gray image denoising, and JPEG image deblocking tasks demonstrate that our proposed network can achieve competitive performance. The source code is available on https://github.com/it-hao/JSNet.","Image restoration, Neural architecture search, Attention mechanism",Hao Shen and Zhong-Qiu Zhao and Wenrui Liao and Weidong Tian and De-Shuang Huang,https://www.sciencedirect.com/science/article/pii/S0031320322003909,https://doi.org/10.1016/j.patcog.2022.108909,0031-3203,2022,108909,132,Pattern Recognition,Joint operation and attention block search for lightweight image restoration,article,SHEN2022108909,
"Convolutional neural networks (CNNs) have been widely applied to medical images. However, medical images are vulnerable to adversarial attacks by perturbations that are undetectable to human experts. This poses significant security risks and challenges to CNN-based applications in clinic practice. In this work, we quantify the scale of adversarial perturbation imperceptible to clinical practitioners and investigate the cause of the vulnerability in CNNs. Specifically, we discover that noise (i.e., irrelevant or corrupted discriminative information) in medical images might be a key contributor to performance deterioration of CNNs against adversarial perturbations, as noisy features are learned unconsciously by CNNs in feature representations and magnified by adversarial perturbations. In response, we propose a novel defense method by embedding sparsity denoising operators in CNNs for improved robustness. Tested with various state-of-the-art attacking methods on two distinct medical image modalities, we demonstrate that the proposed method can successfully defend against those unnoticeable adversarial attacks by retaining as much as over 90% of its original performance. We believe our findings are critical for improving and deploying CNN-based medical applications in real-world scenarios.","CNNs, Adversarial examples, Sparsity denoising",Xiaoshuang Shi and Yifan Peng and Qingyu Chen and Tiarnan Keenan and Alisa T. Thavikulwat and Sungwon Lee and Yuxing Tang and Emily Y. Chew and Ronald M. Summers and Zhiyong Lu,https://www.sciencedirect.com/science/article/pii/S0031320322004046,https://doi.org/10.1016/j.patcog.2022.108923,0031-3203,2022,108923,132,Pattern Recognition,Robust convolutional neural networks against adversarial attacks on medical images,article,SHI2022108923,
"Non-rigid point set registration is a crucial task and an unsolved problem in the field of computer vision. One commonly used method for solving the problem is based on the Gaussian mixture model (GMM). In this method, the point set registration is formalized as a probability density estimation problem. Most GMM-based methods achieve registration by maintaining global and local structures of points. However, the previous methods did not filter the neighborhood information in the local structure, and the quality of local neighborhood information directly affects the accuracy of registration. Therefore, extracting effective local neighborhood information is still a challenge. We propose a novel point set registration method based on GMM by extracting local neighborhood information. The two point sets X and Y are regarded as the centroids of GMM and data points produced by GMM, respectively. Our method computes initial correspondences by comparing the feature descriptors of point sets, and the initial correspondences are updated by considering the neighborhood information. Our method then uses the ExpectationâMaximization method to solve the GMM. In the experimental results, the efficiency and advantages of our method relative to the current methods are verified by applying five commonly used datasets.","Non-rigid point set registration, Gaussian mixture model, ExpectationâMaximization method, Local neighborhood information",Chuanju Liu and Dongmei Niu and Peng Wang and Xiuyang Zhao and Bo Yang and Caiming Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322004320,https://doi.org/10.1016/j.patcog.2022.108952,0031-3203,2022,108952,132,Pattern Recognition,Non-rigid point set registration based on local neighborhood information support,article,LIU2022108952,
"The goal of Visual Question Answering (VQA) is to answer questions based on an image. In the VQA task, reasoning plays an important role in dealing with relations because this task has a high requirement for modeling complex features. In most existing models, the features are only extracted and integrated between adjacent layers. This pattern arguably affects the integrity of information interaction during reasoning. In this paper, we propose a comprehensive-perception dynamic reasoning (CPDR) model to utilize the cross-layer object features for multi-step compound reasoning. It calculates the interactions among the object features from all previous layers and integrates these interactions to generate new object features, iteratively. Finally, the object features of all layers will be used for the final prediction. Empirical results show that our model achieves superior performance among VQA models which are not VLP-based, and incorporating the CPDR module into the VLP models brings considerable performance improvements.","Cross-modal information fusion, Visual question answering, Comprehensive perception, Relational reasoning",Kai Shuang and Jinyu Guo and Zihan Wang,https://www.sciencedirect.com/science/article/pii/S0031320322003594,https://doi.org/10.1016/j.patcog.2022.108878,0031-3203,2022,108878,131,Pattern Recognition,Comprehensive-perception dynamic reasoning for visual question answering,article,SHUANG2022108878,
"Non-functional requirements are property that software products must have in order to meet the userâs business requirements, and are additional constraints on the quality and characteristics of software systems. They are generally written by software designers and documented in various parts of requirements documentation. When developing systems, developers need to classify non-functional requirements from requirements documents, and classifying these non-functional requirements requires professional skills, experience, and domain knowledge, which is challenging and time-consuming for developers. It would be beneficial to implement automatic classification of non-functional requirements from requirements documents, which could reduce the manual, time, and mental fatigue involved in identifying specific non-functional requirements from a large number of requirements. In this paper, a deep neural network model called NFRNet is designed to automatically classify non-functional requirements from software requirement documents. The network consists of two parts. One is an improved BERT word embedding model based on N-gram masking for learning context representation of the requirement descriptions, and the other is a Bi-LSTM classification network for capture context information of the requirement descriptions. We use a Softmax classifier in the end to classify the requirement descriptions. At the same time, in order to accelerate the training and improve the generalization ability of the model, the network uses multi-sample dropout regularization technology. This new regularization technology can reduce the number of iterations needed for training, accelerate the training of deep neural networks, and the networks trained achieved lower error rates. In addition, we expanded the original non-functional requirements dataset (PROMISE dataset) and designed a new dataset called SOFTWARE NFR. The new dataset far exceeds the original dataset in terms of the number of requirement description sentences and the number of non-functional requirements categories. It can be taken as a new testbed for non-functional requirements classification. Through cross-validation on the new dataset, the experimental results show that the network designed in this paper is significantly better than the other 17 classification methods in terms of Precision, Recall, and F1-score. At the same time, for the training set and the validation set, using the multi-sample dropout regularization technology can accelerate the training speed, reduce the number of iterations, and achieve lower error rates and loss.","Non-functional requirements, Non-functional requirements classification, BERT, N-gram, Bi-LSTM, Multi-sample dropout",Bing Li and Xiuwen Nong,https://www.sciencedirect.com/science/article/pii/S0031320322004289,https://doi.org/10.1016/j.patcog.2022.108948,0031-3203,2022,108948,132,Pattern Recognition,Automatically classifying non-functional requirements using deep neural network,article,LI2022108948,
"Unsupervised Domain Adaptation (UDA) aims to classify unlabeled target domain by transferring knowledge from labeled source domain with domain shift. Most of the existing UDA methods try to mitigate the adverse impact induced by the shift via reducing domain discrepancy. However, such approaches easily suffer a notorious mode collapse issue due to the lack of labels in target domain. Naturally, one of the effective ways to mitigate this issue is to reliably estimate the pseudo labels for target domain, which itself is hard. To overcome this, we propose a novel UDA method named Progressive Adaptation of Subspaces approach (PAS) in which we utilize such an intuition that appears much reasonable to gradually obtain reliable pseudo labels. Specifically, we progressively and steadily refine the shared subspaces as bridge of knowledge transfer by adaptively anchoring/selecting and leveraging those target samples with reliable pseudo labels. Subsequently, the refined subspaces can in turn provide more reliable pseudo-labels of the target domain, making the mode collapse highly mitigated. Our thorough evaluation demonstrates that PAS is not only effective for common UDA, but also outperforms the state-of-the arts for more challenging Partial Domain Adaptation (PDA) situation, where the source label set subsumes the target one.","Unsupervised domain adaptation, Partial domain adaptation, Subspace learning, Pseudo label",Weikai Li and Songcan Chen,https://www.sciencedirect.com/science/article/pii/S0031320322003995,https://doi.org/10.1016/j.patcog.2022.108918,0031-3203,2022,108918,132,Pattern Recognition,Unsupervised domain adaptation with progressive adaptation of subspaces,article,LI2022108918,
"Motion in-betweening (MIB) is a process of generating intermediate skeletal movement between the given start and target poses while preserving the naturalness of the motion, such as periodic footstep motion while walking. Although state-of-the-art MIB methods are capable of producing plausible motions given sparse key-poses, they often lack the controllability to generate motions satisfying the semantic contexts required in practical applications. We focus on the method that can handle pose or semantic conditioned MIB tasks using a unified model. We also present a motion augmentation method to improve the quality of pose-conditioned motion generation via defining a distribution over smooth trajectories. Our proposed method outperforms the existing state-of-the-art MIB method in pose prediction errors while providing additional controllability. Our code and results are available on our project web page: https://jihoonerd.github.io/Conditional-Motion-In-Betweening.","Motion in-betweening, Conditional motion generation, Generative model, Motion data augmentation",Jihoon Kim and Taehyun Byun and Seungyoun Shin and Jungdam Won and Sungjoon Choi,https://www.sciencedirect.com/science/article/pii/S0031320322003752,https://doi.org/10.1016/j.patcog.2022.108894,0031-3203,2022,108894,132,Pattern Recognition,Conditional motion in-betweening,article,KIM2022108894,
"Multilabel classification (MLC) is a challenging task in real-world applications, such as project document classification which led us to conduct this research. In the past decade, deep neural networks (DNNs) have been explored in MLC due to their flexibility in dealing with annotated data. However, DNN-based MLC still suffers many problems. Two critical problems are data imbalance and label correlation. These two problems will become more prominent when a training dataset is limited and with a large label set. In this study, special neural network configurations were developed to enhance the performance of DNN-based MLC based on data imbalance and label correlation. The classification accuracy of minority labels and users-preferred labels was increased using customized label groups. The proposed method was evaluated using river restoration project documents and other fifteen datasets. The results show that the proposed method generally increases f1-score for minority labels up to 10%. Adding label dependence into label groups improves the f1-score of user-preferred majority labels up to 5%. The accuracy increase varies in different datasets.","Multilabel classification, data imbalance, label correlation, neural network",Ling Chen and Yuhong Wang and Hao Li,https://www.sciencedirect.com/science/article/pii/S0031320322004447,https://doi.org/10.1016/j.patcog.2022.108964,0031-3203,2022,108964,132,Pattern Recognition,Enhancement of DNN-based multilabel classification by grouping labels based on data imbalance and label correlation,article,CHEN2022108964,
"Despite the progress of interactive image segmentation methods, high-quality pixel-level annotation is still time-consuming and laborious â a bottleneck for several deep learning applications. We take a step back to propose interactive and simultaneous segment annotation from multiple images guided by feature space projection. This strategy is in stark contrast to existing interactive segmentation methodologies, which perform annotation in the image domain. We show that feature space annotation achieves competitive results with state-of-the-art methods in foreground segmentation datasets: iCoSeg, DAVIS, and Rooftop. Moreover, in the semantic segmentation context, it achieves 91.5% accuracy in the Cityscapes dataset, being 74.75 times faster than the original annotation procedure. Further, our contribution sheds light on a novel direction for interactive image annotation that can be integrated with existing methodologies. The supplementary material presents video demonstrations. Code available at https://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation.","Interactive image segmentation, Data annotation, Interactive machine learning, Feature space annotation",JordÃ£o Bragantini and Alexandre X. FalcÃ£o and Laurent Najman,https://www.sciencedirect.com/science/article/pii/S0031320322003636,https://doi.org/10.1016/j.patcog.2022.108882,0031-3203,2022,108882,131,Pattern Recognition,Rethinking interactive image segmentation: Feature space annotation,article,BRAGANTINI2022108882,
"Recently, thanks to the successful application of the attention-based encoder-decoder framework, handwritten mathematical expression recognition (HMER) has achieved significant improvement. However, HMER is still a challenging task in the handwriting recognition area, which suffers from the ambiguity of handwritten symbols, the two-dimensional structure of mathematical expressions, and the lack of labeled data. In this paper, we attempt to improve the recognition performance and generalization ability of the existing state-of-the-art method from two perspectives: data augmentation and model design. We first propose a tree-based multi-level (including symbol level, sub-expression level, and image level) data augmentation strategy, which can generate many synthetic images. Then, we present a novel encoder-decoder hybrid model via tree-based mutual learning to fully utilize the complementarity between tree decoder and string decoder. Benefitting from our data augmentation strategy, we achieve 58.47%/57.82%/62.67% and 74.45% expression recognition accuracy respectively on the CROHME14/16/19 competition datasets and the OffRaSHME20 competition dataset. Moreover, tree-based data augmentation is a key technology to our champion system for the OffRaSHME20 competition. Our tree-based mutual learning method further improves the recognition accuracy to 61.63%/59.81%/64.38% and 75.68% on these datasets. Further quantitative and qualitative analyses also demonstrate the effectiveness and robustness of our proposed methods.","Tree-based data augmentation, Tree-based mutual learning, Encoder-decoder, Offline handwritten mathematical expression recognition",Chen Yang and Jun Du and Jianshu Zhang and Changjie Wu and Mingjun Chen and JiaJia Wu,https://www.sciencedirect.com/science/article/pii/S0031320322003910,https://doi.org/10.1016/j.patcog.2022.108910,0031-3203,2022,108910,132,Pattern Recognition,Tree-based data augmentation and mutual learning for offline handwritten mathematical expression recognition,article,YANG2022108910,
"Understanding multimodal information is the key to visual question answering (VQA) tasks. Most existing approaches use attention mechanisms to acquire fine-grained information understanding. However, these approaches with merely attention mechanisms do not solve the potential understanding bias problem. Hence, this paper introduces contextual information into VQA for the first time and presents a context-aware attention network (CAAN) to tackle the case. By improving the modular co-attention network (MCAN) framework, CAANâs main work includes: designing a novel absolute position calculation method based on the coordinates of each image region in the image and the imageâs actual size, the position information of all image regions are integrated as contextual information to enhance the visual representation; based on the question itself, several internal contextual information representations are introduced to participate in the modeling of the question words, solving the understanding bias caused by the similarity of the question. Additionally, we also designed two models of different scales, namely CAAN-base and CAAN-large, to explore the effect of the field of view on interaction. Finally, extensive experimental results show that CAAN significantly outperforms MCAN and achieves comparable or even better performance than other state-of-the-art approaches, proving our method can tackle the understanding bias.","Visual question answering, Attention mechanism, Understanding bias, Absolute position, Contextual information",Chongqing Chen and Dezhi Han and Chin-Chen Chang,https://www.sciencedirect.com/science/article/pii/S0031320322004605,https://doi.org/10.1016/j.patcog.2022.108980,0031-3203,2022,108980,132,Pattern Recognition,CAAN: Context-Aware attention network for visual question answering,article,CHEN2022108980,
"This paper introduces a Feature Nonlinear Transformation Non-Negative Matrix Factorization with Kullback-Leibler Divergence (FNTNMF-KLD) for extracting the nonlinear features of a matrix in standard NMF. This method uses a nonlinear transformation to act on the feature matrix for constructing a NMF model based on the objective function of Kullback-Leibler Divergence, and the Taylor series expansion and the Newton iteration formula of solving root are used to obtain the iterative update rules of the basis matrix and the feature matrix. Experimental results show that the proposed method obtains the nonlinear features of data matrix in a more efficient way. In object recognition and clustering tasks, better accuracy can be achieved over some typical NMF methods.","Non-negative matrix factorization, Nonlinear transformation, Feature extraction, Object recognition, Clustering, Kullback-Leibler divergence",Lirui Hu and Ning Wu and Xiao Li,https://www.sciencedirect.com/science/article/pii/S0031320322003879,https://doi.org/10.1016/j.patcog.2022.108906,0031-3203,2022,108906,132,Pattern Recognition,Feature Nonlinear Transformation Non-Negative Matrix Factorization with Kullback-Leibler Divergence,article,HU2022108906,
"Video Question Answering (VideoQA) has gained increasing attention as an important task in understanding the rich spatio-temporal contents, i.e., the appearance and motion in the video. However, existing approaches mainly use the question to learn attentions over all the sampled appearance and motion features separately, which neglect two properties of VideoQA: (1) the answer to the question is often reflected on a few frames and video clips, and most video contents are superfluous; (2) appearance and motion features are usually concomitant and complementary to each other in time series. In this paper, we propose a novel VideoQA model, i.e., Dynamic Self-Attention with Vision Synchronization Networks (DSAVS), to address these problems. Specifically, a gated token selection mechanism is proposed to dynamically select the important tokens from appearance and motion sequences. These chosen tokens are fed into a self-attention mechanism to model the internal dependencies for more effective representation learning. To capture the correlation between the appearance and motion features, a vision synchronization block is proposed to synchronize the two types of vision features at the time slice level. Then, the visual objects can be correlated with their corresponding activities and the performance is further improved. Extensive experiments conducted on three public VideoQA data sets confirm the effectivity and superiority of our model compared with state-of-the-art methods.","Video question answering, Dynamic self-attention, Vision synchronization",Yun Liu and Xiaoming Zhang and Feiran Huang and Shixun Shen and Peng Tian and Lang Li and Zhoujun Li,https://www.sciencedirect.com/science/article/pii/S0031320322004393,https://doi.org/10.1016/j.patcog.2022.108959,0031-3203,2022,108959,132,Pattern Recognition,Dynamic self-attention with vision synchronization networks for video question answering,article,LIU2022108959,
"Convolutional neural networks (CNNs)-based classifiers, trained with the softmax cross-entropy loss, have achieved remarkable success in learning embeddings for pattern recognition. The cosine similarity-based softmax variants further improve the performance by focusing on optimizing the angles between embeddings and class weights. However, embeddings learned by these variants still have significant intra-class variances since these methods only optimize the relative differences between intra- and inter-class cosine similarities. To simultaneously optimize intra- and inter-class cosine similarities, this paper proposes a cosine Similarity Optimization-based softmax (SO-softmax) loss, which is based on a generalized softmax loss formulation that combines both similarities. The proposed loss constrains the intra-class (positive) and inter-class (negative) cosine similarity by quadratic transformations, thus making the embedding representation more compact within classes and more distinguishable between classes. It is verified theoretically that SO-softmax loss can optimize both the similarities simultaneously. Thorough experiments are conducted on typical audio classification, image classification, face verification, image retrieval, and person re-identification tasks, and the results show that SO-softmax loss outperforms the state-of-the-art loss functions in CNNs-based frameworks.","Convolutional neural networks, Cosine similarity, Cross entropy loss, Quadratic transformation, Embedding learning, Softmax",Qiang Zhang and Jibin Yang and Xiongwei Zhang and Tieyong Cao,https://www.sciencedirect.com/science/article/pii/S0031320322003582,https://doi.org/10.1016/j.patcog.2022.108877,0031-3203,2022,108877,131,Pattern Recognition,SO-softmax loss for discriminable embedding learning in CNNs,article,ZHANG2022108877,
"One key challenging problem in data mining and decision-making is to establish a decision support system based on unbalanced datasets. In this study, we propose a novel algorithm to handle unbalanced learning problems that integrates the advantages of Siamese convolutional neural networks (SCNN) and the online reweighted example (ORE) algorithm into a unified method. First, the SCNN model is established for learning and extracting deep feature features at different levels. Second, the ORE algorithm is used to address the problem of data with a class-imbalanced distribution. Compared with baseline approaches, the experimental results show that our proposed method substantially enhances the performance of both within-project defect prediction and cross-project defect prediction.","Few-shot learning, Reweighted example learning, Data mining, Imbalanced learning",Linchang Zhao and Zhaowei Shang and Jin Tan and Mingliang Zhou and Mu Zhang and Dagang Gu and Taiping Zhang and Yuan Yan Tang,https://www.sciencedirect.com/science/article/pii/S0031320322004277,https://doi.org/10.1016/j.patcog.2022.108947,0031-3203,2022,108947,132,Pattern Recognition,Siamese networks with an online reweighted example for imbalanced data learning,article,ZHAO2022108947,
"As a leading graph clustering technique, spectral clustering is one of the most widely used clustering methods that captures complex clusters in data. However, some of its deficiencies, such as the high computational complexity in eigen decomposition and the guidance without supervised information, limit its real applications. To get rid of the deficiencies, we propose a self-supervised spectral clustering algorithm. In this algorithm, we define an exemplar constraint which reflects the relations between objects and exemplars. We provide the related analysis to show that it is more suitable for unsupervised learning. Based on the exemplar constraint, we build an optimization model for self-supervised spectral clustering so that we can simultaneously learn clustering results and exemplar constraints. Furthermore, we propose an iterative method to solve the new optimization problem. Compared to other existing versions of spectral clustering algorithms, the new algorithm can use the low computational costs to discover a high-quality cluster structure of a data set without prior information. Furthermore, we did a number of experiments of algorithm comparison and parameter analysis on benchmark data sets to illustrate that the proposed algorithm is very effective and efficient.","Spectral clustering, Self-supervised algorithm, Exemplar constraint, Optimization model",Liang Bai and Yunxiao Zhao and Jiye Liang,https://www.sciencedirect.com/science/article/pii/S0031320322004551,https://doi.org/10.1016/j.patcog.2022.108975,0031-3203,2022,108975,132,Pattern Recognition,Self-supervised spectral clustering with exemplar constraints,article,BAI2022108975,
"Current adoption of machine learning in industrial, societal and economical activities has raised concerns about the fairness, equity and ethics of automated decisions. Predictive models are often developed using biased datasets and thus retain or even exacerbate biases in their decisions and recommendations. Removing the sensitive covariates, such as gender or race, is insufficient to remedy this issue since the biases may be retained due to other related covariates. We present a regularization approach to this problem that trades off predictive accuracy of the learned models (with respect to biased labels) for the fairness in terms of statistical parity, i.e. independence of the decisions from the sensitive covariates. In particular, we consider a general framework of regularized empirical risk minimization over reproducing kernel Hilbert spaces and impose an additional regularizer of dependence between predictors and sensitive covariates using kernel-based measures of dependence, namely the Hilbert-Schmidt Independence Criterion (HSIC) and its normalized version. This approach leads to a closed-form solution in the case of squared loss, i.e. ridge regression. We also provide statistical consistency results for both risk and fairness bound for our approach. Moreover, we show that the dependence regularizer has an interpretation as modifying the corresponding Gaussian process (GP) prior. As a consequence, a GP model with a prior that encourages fairness to sensitive variables can be derived, allowing principled hyperparameter selection and studying of the relative relevance of covariates under fairness constraints. Experimental results in synthetic examples and in real problems of income and crime prediction illustrate the potential of the approach to improve fairness of automated decisions.","Fairness, Kernel methods, Gaussian processes, Regularization, Hilbert-Schmidt independence criterion",Zhu Li and AdriÃ¡n PÃ©rez-Suay and Gustau Camps-Valls and Dino Sejdinovic,https://www.sciencedirect.com/science/article/pii/S0031320322004034,https://doi.org/10.1016/j.patcog.2022.108922,0031-3203,2022,108922,132,Pattern Recognition,Kernel dependence regularizers and Gaussian processes with applications to algorithmic fairness,article,LI2022108922,
"Deep Neural Networks (DNNs) are generated by sequentially performing linear and non-linear processes. The combination of linear and non-linear procedures is critical for generating a sufficiently deep feature space. Most non-linear operators are derivations of activation functions or pooling functions. Mathematical morphology is a branch of mathematics that provides non-linear operators for various image processing problems. This paper investigates the utility of integrating these operations into an end-to-end deep learning framework. DNNs are designed to acquire a realistic representation for a particular job. Morphological operators give topological descriptors that convey salient information about the shapes of objects depicted in images. We propose a method based on meta-learning to incorporate morphological operators into DNNs. The learned architecture demonstrates how our novel morphological operations significantly increase DNN performance on various tasks, including picture classification, edge detection, and semantic segmentation. Our codes are available at https://nao-morpho.github.io/.","Mathematical morphology, Deep learning, Architecture search, Edge detection, Semantic segmentation",Yufei Hu and Nacim Belkhir and Jesus Angulo and Angela Yao and Gianni Franchi,https://www.sciencedirect.com/science/article/pii/S0031320322003740,https://doi.org/10.1016/j.patcog.2022.108893,0031-3203,2022,108893,131,Pattern Recognition,Learning deep morphological networks with neural architecture search,article,HU2022108893,
"In early 2020, the global spread of the COVID-19 has presented the world with a serious health crisis. Due to the large number of infected patients, automatic segmentation of lung infections using computed tomography (CT) images has great potential to enhance traditional medical strategies. However, the segmentation of infected regions in CT slices still faces many challenges. Specially, the most core problem is the high variability of infection characteristics and the low contrast between the infected and the normal regions. This problem leads to fuzzy regions in lung CT segmentation. To address this problem, we have designed a novel global feature network(GFNet) for COVID-19 lung infections: VGG16 as backbone, we design a Edge-guidance module(Eg) that fuses the features of each layer. First, features are extracted by reverse attention module and Eg is combined with it. This series of steps enables each layer to fully extract boundary details that are difficult to be noticed by previous models, thus solving the fuzzy problem of infected regions. The multi-layer output features are fused into the final output to finally achieve automatic and accurate segmentation of infected areas. We compared the traditional medical segmentation networks, UNet, UNet++, the latest model Inf-Net, and methods of few shot learning field. Experiments show that our model is superior to the above models in Dice, Sensitivity, Specificity and other evaluation metrics, and our segmentation results are clear and accurate from the visual effect, which proves the effectiveness of GFNet. In addition, we verify the generalization ability of GFNet on another ânever seenâ dataset, and the results prove that our model still has better generalization ability than the above model. Our code has been shared at https://github.com/zengzhenhuan/GFNet.","Image segmentation, COVID-19, Edge-guidance, Convolutional neural network, CT image",Chaodong Fan and Zhenhuan Zeng and Leyi Xiao and Xilong Qu,https://www.sciencedirect.com/science/article/pii/S0031320322004435,https://doi.org/10.1016/j.patcog.2022.108963,0031-3203,2022,108963,132,Pattern Recognition,GFNet: Automatic segmentation of COVID-19 lung infection regions using CT images based on boundary features,article,FAN2022108963,
"In this paper, we propose a novel Bit-Slicing Context Attention Network (BSCA-Net), an end-to-end network, to improve the extraction ability of boundary information for polyp segmentation. The core of BSCA-Net is a new Bit Slice Context Attention (BSCA) module, which exploits the bit-plane slicing information to effectively extract the boundary information between polyps and the surrounding tissue. In addition, we design a novel Split-Squeeze-Bottleneck-Union (SSBU) module, to exploit the geometrical information from different aspects. Also, based on SSBU, we propose an multipath concat attention decoder (MCAD) and an multipath attention concat encoder (MACE), to further improve the network performance for polyp segmentation. Finally, by combining BSCA, SSBU, MCAD and MACE, the proposed BSCA-Net is able to effectively suppress noises in feature maps, and simultaneously improve the ability of feature expression in different levels, for polyp segmentation. Empirical experiments on five benchmark datasets (Kvasir, CVC-ClinicDB, ETIS, CVC-ColonDB and CVC-300) demonstrate the superior of the proposed BSCA-Net over existing cutting-edge methods.","Medical image segmentation, Polyp segmentation, Colonoscopy, Attention mechanism",Yi Lin and Jichun Wu and Guobao Xiao and Junwen Guo and Geng Chen and Jiayi Ma,https://www.sciencedirect.com/science/article/pii/S0031320322003983,https://doi.org/10.1016/j.patcog.2022.108917,0031-3203,2022,108917,132,Pattern Recognition,BSCA-Net: Bit Slicing Context Attention network for polyp segmentation,article,LIN2022108917,
"In this study, we present novel visual tracking methods based on the Wasserstein approximate Bayesian computation (ABC). For visual tracking, the proposed Wasserstein ABC (WABC) method approximates the likelihood within the Wasserstein space more accurately than the conventional ABC methods by directly measuring the discrepancy between the likelihood distributions. To encode the temporal dependency among time-series likelihood distributions, we extend the WABC method to the time-series WABC (TWABC) method. Subsequently, the proposed Hilbert TWABC (HTWABC) method reduces the computational costs caused by the TWABC method while substituting the original Wasserstein distance with the Hilbert distance. Experimental results demonstrate that the proposed visual trackers outperform other state-of-the-art visual tracking methods quantitatively. Moreover, ablation studies verify the effectiveness of individual components consisting of the proposed method (e.g., the Wasserstein distance, curve matching, and Hilbert metric).",,Jinhee Park and Junseok Kwon,https://www.sciencedirect.com/science/article/pii/S0031320322003867,https://doi.org/10.1016/j.patcog.2022.108905,0031-3203,2022,108905,131,Pattern Recognition,Wasserstein approximate bayesian computation for visual tracking,article,PARK2022108905,
"RGB-T semantic segmentation has attracted growing attention because it makes a model robust towards challenging illumination. Most existing methods fuse RGB and thermal information in an equal manner along spatial dimensions, which results in feature redundancy and affects the discriminability of cross-modal features. In this paper, we propose a Complementarity-aware Cross-modal Feature Fusion Network (CCFFNet) including a Complementarity-Aware Encoder (CAE) and a Three-Path Fusion and Supervision (TPFS). The CAE, which consists of cascaded cross-modal fusion modules, can select complementary information from RGB and thermal features via a novel gate and fuse them by a channel-wise weighting mechanism. TPFS not only iteratively performs Three-Path Fusion (TPF) to further enhance cross-modal features, but also supervise the training of CCFFNet along three branches by Three-Supervision (TS). Extensive experiments are carried out and the results demonstrate that our model outperforms the state-of-the-art models by at least 1.6% mIoU on MFNet dataset and 2.9% mIoU on PST900 dataset, respectively. And a single-modality-based model can be easily applied to multi-modal semantic segmentation when plugging our CAE.","RGB-T, Cross-modal fusion, Multi-supervision, Semantic segmentation",Wei Wu and Tao Chu and Qiong Liu,https://www.sciencedirect.com/science/article/pii/S0031320322003624,https://doi.org/10.1016/j.patcog.2022.108881,0031-3203,2022,108881,131,Pattern Recognition,Complementarity-aware cross-modal feature fusion network for RGB-T semantic segmentation,article,WU2022108881,
"The Graph Convolutional Networks (GCN) proposed by Kipf and Welling is an effective model to improve semi-supervised learning of pattern recognition, but faces the obstacle of over-smoothing, which will weaken the representation ability of GCN. Recently some works are proposed to tackle above limitation by randomly perturbing graph topology or feature matrix to generate data augmentations as input for training. However, these operations inevitably do damage to the integrity of information structures and have to sacrifice the smoothness of feature manifold. In this paper, we first introduce a novel graph entropy definition as a measure to quantitatively evaluate the smoothness of a data manifold and then point out that this graph entropy is controlled by triangle motif-based information structures. Considering the preservation of graph entropy, we propose an effective strategy to generate randomly perturbed training data but maintain both graph topology and graph entropy. Extensive experiments have been conducted on real-world datasets and the results verify the effectiveness of our proposed method in improving semi-supervised node classification accuracy compared with a surge of baselines. Beyond that, our proposed approach could significantly enhance the robustness of training process for GCN.","Graph representation, Graph convolutional networks, Information theory, Graph entropy",Xue Liu and Dan Sun and Wei Wei,https://www.sciencedirect.com/science/article/pii/S0031320322004319,https://doi.org/10.1016/j.patcog.2022.108951,0031-3203,2022,108951,132,Pattern Recognition,Alleviating the over-smoothing of graph neural computing by a data augmentation strategy with entropy preservation,article,LIU2022108951,
"In this paper, we propose a novel part-level feature extraction method to enhance the discriminative ability of deep convolutional features for the task of fine-grained vehicle recognition. Generally, the challenges for fine-grained vehicle recognition are mainly caused by the subtle visual differences between part regions of vehicles. Therefore, it is essential to extract discriminative features from part regions. Many existing methods, especially deep convolutional neural networks (D-CNNs), tend to detect the discriminative part regions explicitly or learn the part information implicitly through network restructuring and neglect the abundant part-level information contained in the high-level features generated by CNNs. In light of this, we propose a simple and effective part-level feature extraction method to enhance the representation of part-level features within the global features of target object generated by the backbone networks. The proposed method is built on the deep convolutional layers from which the discriminative part features could be integrated and extracted accordingly. More specifically, a basic feature grouping module is adopted to integrate the feature maps of deep convolutional layers into groups in each of which the related discriminative parts are assembled. The feature grouping process is performed in a multi-stage manner to ensure the integration process. Then a fusion module follows to model the coarse-to-fine relationship of the part features and further ensure the integrity and effectiveness of the part features. We conduct comparison experiments on public datasets, and the results show that the proposed method achieves comparable performance with state-of-the-art algorithms.","Fine-grained recognition, Part-level feature extraction, Feature grouping, Feature fusion",Lei Lu and Ping Wang and Yijie Cao,https://www.sciencedirect.com/science/article/pii/S0031320322003508,https://doi.org/10.1016/j.patcog.2022.108869,0031-3203,2022,108869,131,Pattern Recognition,A novel part-level feature extraction method for fine-grained vehicle recognition,article,LU2022108869,
"To understand the black-box characteristics of deep networks, counterfactual explanation that deduces not only the important features of an input space but also how those features should be modified to classify input as a target class has gained an increasing interest. The patterns that deep networks have learned from a training dataset can be grasped by observing the feature variation among various classes. However, current approaches perform the feature modification to increase the classification probability for the target class irrespective of the internal characteristics of deep networks. This often leads to unclear explanations that deviate from real-world data distributions. To address this problem, we propose a counterfactual explanation method that exploits the statistics learned from a training dataset. Especially, we gradually construct an explanation by iterating over masking and composition steps. The masking step aims to select an important feature from the input data to be classified as a target class. Meanwhile, the composition step aims to optimize the previously selected feature by ensuring that its output score is close to the logit space of the training data that are classified as the target class. Experimental results show that our method produces human-friendly interpretations on various classification datasets and verify that such interpretations can be achieved with fewer feature modification.","Explainable AI, Counterfactual explanation, Interpretability, Model-agnostics, Generative model",Hong-Gyu Jung and Sin-Han Kang and Hee-Dong Kim and Dong-Ok Won and Seong-Whan Lee,https://www.sciencedirect.com/science/article/pii/S0031320322004381,https://doi.org/10.1016/j.patcog.2022.108958,0031-3203,2022,108958,132,Pattern Recognition,Counterfactual explanation based on gradual construction for deep networks,article,JUNG2022108958,
"Aesthetic attributes are crucial for aesthetics because they explicitly present some photo quality cues that a human expert might use to evaluate a photoâs aesthetic quality. However, the aesthetic attributes have not been largely and sufficiently exploited for photo aesthetic assessment. In this paper, we propose a novel approach to photo aesthetic assessment with the help of aesthetic attributes. The aesthetic attributes are used as privileged information (PI), which is often available during training phase but unavailable in prediction phase due to the high collection expense. The proposed framework consists of a deep multi-task network as generator and a fully connected network as discriminator. Deep multi-task network learns the aesthetic attributes and score simultaneously to capture their dependencies and extract better feature representations. Specifically, we use ranking constraint in the label space, similarity constraint and prior probabilities loss in the privileged information space to make the output of multi-task network converge to that of ground truth. Adversarial loss is used to identify and distinguish the predicted privileged information of a deep multi-task network from the ground truth PI distribution. Experimental results on two benchmark databases demonstrate the superiority of the proposed method to state-of-the-art.","Aesthetic assessment, Privileged information, Multi-task learning",Yangyang Shu and Qian Li and Lingqiao Liu and Guandong Xu,https://www.sciencedirect.com/science/article/pii/S0031320322004022,https://doi.org/10.1016/j.patcog.2022.108921,0031-3203,2022,108921,132,Pattern Recognition,Privileged multi-task learning for attribute-aware aesthetic assessment,article,SHU2022108921,
"Visual classification for medical images has been dominated by convolutional neural networks (CNNs) for years. Though they have shown great performance on accuracy, some of them provide decisions that are hard to explain while others encode information from irrelevant or noisy regions. In this work, we try to close this gap by proposing an explainable framework which consists of a predictor and an explainable tool, so as to provide accurate diagnoses with intuitive visualization maps and prediction basis. Specifically, the predictor is designed by applying attention mechanisms to multi-scale features so as to learn and discover class discriminative latent representations that are close to each brain volumeâs label. Meanwhile, to explain our predictor, we propose the novel explainable tool which includes a high-resolution visualization method and a prediction-basis creation and retrieval module. The former effectively integrates the feature maps of intermediate layers as well as the last convolutional layer, which surpasses state-of-the-art visualization approaches in producing high-resolution representations with more accurate localization of discriminative areas. While the latter provides prediction basis evidence via retrieved volumes with similar latent representations which are accessible to neurologists. Extensive experiments show that the proposed framework achieves higher level of accuracy and explainability over other state-of-the-art solutions. More importantly, it localizes crucial brain areas with clearer boundaries, less noises, which matches background knowledge in the neuroscience literature.","Explainable neural networks, XAI, High-resolution heatmap, MRI",Lu Yu and Wei Xiang and Juan Fang and Yi-Ping {Phoebe Chen} and Ruifeng Zhu,https://www.sciencedirect.com/science/article/pii/S0031320322003570,https://doi.org/10.1016/j.patcog.2022.108876,0031-3203,2022,108876,131,Pattern Recognition,A novel explainable neural network for Alzheimerâs disease diagnosis,article,YU2022108876,
"The recognition of two-dimensional structure of tables and forms from document images is a challenge due to the complexity of document structures and the diversity of layouts. In this paper, we propose a graph neural network (GNN) based unified framework named Table Structure Recognition Network (TSRNet) to jointly detect and recognize the structures of various tables and forms. First, a multi-task fully convolutional network (FCN) is used to segment primitive regions such as text segments and ruling lines from document images, then a GNN is used to classify and group these primitive regions into page objects such as tables and cells. At last, the relationships between neighboring page objects are analyzed using another GNN based parsing module. The parameters of all the modules in the system can be trained end-to-end to optimize the overall performance. Experiments of table detection and structure recognition for modern documents on the POD 2017, cTDaR 2019 and PubTabNet datasets and template-free form parsing for historical documents on the NAF dataset show that the proposed method can handle various table/form structures and achieve superior performance.","Table detection, Table structure recognition, Template-free form parsing, Graph neural network, End-to-end training",Xiao-Hui Li and Fei Yin and He-Sen Dai and Cheng-Lin Liu,https://www.sciencedirect.com/science/article/pii/S0031320322004265,https://doi.org/10.1016/j.patcog.2022.108946,0031-3203,2022,108946,132,Pattern Recognition,Table Structure Recognition and Form Parsing by End-to-End Object Detection and Relation Parsing,article,LI2022108946,
"Face anti-spoofing, a biometric authentication method, is a central part of automatic face recognition. Recently, two sets of approaches have performed particularly well against presentation attacks: 1) pixel-wise supervision-based methods, which intend to provide fine-grained pixel information to learn specific auxiliary maps; and 2) anomaly detection-based methods, which regard face anti-spoofing as an open-set training task and learn spoof detectors using only bona fide data, where the detectors are shown to generalize well to unknown attacks. However, these approaches depend on handcrafted prior information to control the generation of intermediate difference maps and easily fall into local optima. In this paper, we propose a novel frame-level face anti-spoofing method, Covered Style Mining-GAN (CSM-GAN), which converts face anti-spoofing detection into a style transfer process without any prior information. Specifically, CSM-GAN has four main components: the Covered Style Encoder (CSE), responsible for mining the difference map containing the photography style and discriminative clues; the Auxiliary Style Classifier (ASC), consisting of several stacked Difference Capture Blocks (DCB) responsible for distinguishing bona fide faces from spoofing faces; and the Style Transfer Generator (STG) and Style Adversarial Discriminator (SAD), which form generative adversarial networks to achieve style transfer. Comprehensive experiments on several benchmark datasets show that the proposed method not only outperforms current state-of-the-art but also produces better visual diversity in difference maps.","Face anti-spoofing, Generative adversarial networks, Deep learning",Yiqiang Wu and Dapeng Tao and Yong Luo and Jun Cheng and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S003132032200437X,https://doi.org/10.1016/j.patcog.2022.108957,0031-3203,2022,108957,132,Pattern Recognition,Covered Style Mining via Generative Adversarial Networks for Face Anti-spoofing,article,WU2022108957,
"Person identification with a single feature (e.g., face recognition, speaker verification, person re-identification, etc.) has been studied extensively for many years, while few works focus on multi-feature person identification. Though promising performance has been achieved by only using the information of facial images, voice, or pedestrian appearance, it is still challenging to recognize a person with only a single feature in some situations (e.g., a person at a distance or occluded by other objects, and a partial person out of view). In this paper, we present a multi-feature sparse similar representation (MFSSR) method to effectively fuse face features, body features, and global image features for the task of person identification. In MFSSR, we designed a reconstructed deep spatial feature for representing the appearance of human body by using the spatial correlation coding of partial deep spatial features. Then we presented a multi-feature sparse similar representation model for jointly using different features, e.g., face, body, and the global image. Besides, considering that the coding coefficients associated with good samples but not outliers should be more similar among different features, we jointly represent different features by imposing a weighted â1-norm distance regularization, instead of the conventional â2-norm regularization, on the coefficients. Experimental results on several multi-feature person identification databases have clearly shown the superior performance of the proposed model.","Multi-feature, Person identification, Sparse representation",Meng Yang and Lei Liao and Kangyin Ke and Guangwei Gao,https://www.sciencedirect.com/science/article/pii/S0031320322003971,https://doi.org/10.1016/j.patcog.2022.108916,0031-3203,2022,108916,132,Pattern Recognition,Multi-feature sparse similar representation for person identification,article,YANG2022108916,
"In monocular image scenes, 3D human pose estimation exhibits inherent ambiguity due to the loss of depth information and occlusions. Simply regressing body joints with high uncertainties will lead to model overfitting and poor generalization. In this paper, we propose an uncertainty-based framework to jointly learn 3D human poses and the uncertainty of each joint. Our proposed joint estimation framework aims to mitigate the adverse effects of training samples with high uncertainties and facilitate the training procedure. To be specific, we model each body joint as a Laplace distribution for uncertainty representation. Since visual joints often exhibit low uncertainties while occluded ones have high uncertainties, we develop an adaptive scaling factor, named the uncertainty-aware scaling factor, to ease the network optimization in accordance with the joint uncertainties. By doing so, our network is able to converge faster and significantly reduce the adverse effects caused by those ambiguous joints. Furthermore, we present an uncertainty-aware graph convolutional network by exploiting the learned joint uncertainties and the relationships among joints to refine the initial joint localization. Extensive experiments on single-person (Human3.6M) and multi-person (MuCo-3DHP & MuPoTS-3D) 3D human pose estimation datasets demonstrate the effectiveness of our method.","Uncertainty, 3D pose estimation, Graph convolutional network",Chuchu Han and Xin Yu and Changxin Gao and Nong Sang and Yi Yang,https://www.sciencedirect.com/science/article/pii/S0031320322004149,https://doi.org/10.1016/j.patcog.2022.108934,0031-3203,2022,108934,132,Pattern Recognition,Single image based 3D human pose estimation via uncertainty learning,article,HAN2022108934,
"For neural architecture search, NSGA-Net has searched a representative neural architecture set of Pareto-optimal solutions to consider both accuracy and computation complexity simultaneously. However, some decision-makers only concentrate on such neural architectures in the subpart regions of Pareto-optimal Frontier that they have interests in. Under the above circumstances, certain uninterested neural architectures may cost many computing resources. In order to consider the preference of decision-makers, we propose the reference point based NSGA-Net (RNSGA-Net) for neural architecture search. The core of RNSGA-Net adopts the reference point approach to guarantee the Pareto-optimal region close to the reference points and also combines the advantage of NSGAII with the fast nondominated sorting approach to split the Pareto front. Moreover, we augment an extra bit value of the original encoding to represent two types of residual block and one type of dense block for residual connection and dense connection in the RNSGA-Net. In order to satisfy the decision-maker preference, the multi-objective is measured to search competitive neural architecture by minimizing an error metric and FLOPs of computational complexity. Experiment results on the CIFAR-10 dataset demonstrate that RNSGA-Net can improve NSGA-Net in terms of the more structured representation space and the preference of decision-makers.","Neural architecture search, Multi-objective evolutionary algorithm, The image classification",Lyuyang Tong and Bo Du,https://www.sciencedirect.com/science/article/pii/S0031320322004423,https://doi.org/10.1016/j.patcog.2022.108962,0031-3203,2022,108962,132,Pattern Recognition,Neural architecture search via reference point based multiâobjective evolutionary algorithm,article,TONG2022108962,
"Incremental and decremental problems are challenging tasks in semi-supervised learning. The incremental semi-supervised discriminant analysis (ISSDA) method proposed by Dhamecha etÂ al. is an efficient method for incremental semi-supervised learning. However, one deficiency of the ISSDA method is that the total scatter matrix remains unchanged during incremental learning, which is impractical in practice. On the other hand, there may be a series of incorrectly artificial labeling in the public data set, and it is interesting to consider the decremental problem in semi-supervised learning. To the best of our knowledge, however, there are few decremental algorithms for semi-supervised discriminant analysis. The contributions of this work are as follows. First, a new incremental semi-supervised discriminant analysis method is proposed, in which we consider updating the total scatter matrix and the between-class scatter matrix simultaneously when new samples are added. Second, we show how to solve the large eigenproblem of the updated total scatter matrix efficiently. Third, we propose two decremental algorithms for semi-supervised discriminant analysis. Numerical experiments demonstrate the superiority of the proposed algorithms over many state-of-the-art algorithms for semi-supervised discriminant analysis.","Dimensionality reduction, Semi-supervised discriminant analysis, Incremental learning, Decremental learning, Modified total scatter matrix",Wenrao Pang and Gang Wu,https://www.sciencedirect.com/science/article/pii/S0031320322003697,https://doi.org/10.1016/j.patcog.2022.108888,0031-3203,2022,108888,131,Pattern Recognition,Fast algorithms for incremental and decremental semi-supervised discriminant analysis,article,PANG2022108888,
"Few-shot classification aims to identify novel categories using only a few labeled samples. Generally, the metric-based few-shot classification methods compare the feature embedding of Query samples (unlabeled samples) with Support samples (labeled samples) in a metric algorithm to predict which category the Query sample belongs to. Obtaining a good feature embedding for each sample in the feature extraction stage can improve the classification accuracy in the metric stage. Based on this, we design the Self-Guided Information Convolution (SGI-Conv), an improved convolution structure, which utilizes the high-level features to guide the network to extract the required discriminative features. To effectively utilize the feature embeddings of samples, we divide the metric network into multiple blocks and build a multi-layer graph convolutional network by sharing adjacent matrices. The multi-layer structure enhances the aggregation ability of graph convolution. Extensive experiments on multiple benchmark datasets demonstrate that our method has achieved competitive results on the few-shot classification tasks.","Few-shot classification, Graph convolution network, Self-guided information",Zhineng Zhao and Qifan Liu and Wenming Cao and Deliang Lian and Zhihai He,https://www.sciencedirect.com/science/article/pii/S0031320322003612,https://doi.org/10.1016/j.patcog.2022.108880,0031-3203,2022,108880,131,Pattern Recognition,Self-guided information for few-shot classification,article,ZHAO2022108880,
"With the development of deep learning, salient object detection methods have made great progress. However, there are still two challenges: 1) The lack of rich features extracted from multiple perspectives at different encoder levels results in the omission of salient objects with varying scales. 2) The ineffective fusion of multi-level features during decoding dilutes the saliency features, which destroys the purity of the predicted maps. In this paper, we design a Condensing-and-Filtering Network (CFNet), in which a saliency pyramid condensing module (SPCM) and a saliency filtering module (SFM) are proposed to solve the above two problems respectively. Specifically, SPCM introduces pyramid convolution as the basic unit to condense full-scale features from global and local perspectives at each level of the encoder. SFM is equipped with an ingenious âfunnelâ structure to effectively filter multi-level features and supplement details, which makes the fusion of features more robust. The two modules complement each other, so that the full-scale features can be used effectively to predict salient objects. Extensive experimental results on five benchmark datasets demonstrate that our method performs favourably against the state-of-the-art approaches, and also shows superiority in terms of speed (16.18ms) and FLOPs (21.19G). Meanwhile, we extend our CFNet to the task of RGB-D salient object detection and achieve better results, which further demonstrate its effectiveness. The code will be made available.","Salient object detection, Neural networks, Full-scale feature extraction, Multi-level feature fusion",Xinyu Yan and Meijun Sun and Yahong Han and Zheng Wang and Qi Tian,https://www.sciencedirect.com/science/article/pii/S0031320322003855,https://doi.org/10.1016/j.patcog.2022.108904,0031-3203,2022,108904,131,Pattern Recognition,Effective full-scale detection for salient object based on condensing-and-filtering network,article,YAN2022108904,
"It is a fundamental and vital task to enhance the perception capability of the point cloud learning network in 3D machine vision applications. Most existing methods utilize feature fusion and geometric transformation to improve point cloud learning without paying enough attention to mining further intrinsic information across multiple network layers. Motivated to improve consistency between hierarchical features and strengthen the perception capability of the point cloud network, we propose exploring whether maximizing the mutual information (MI) across shallow and deep layers is beneficial to improve representation learning on point clouds. A novel design of Maximizing Mutual Information (MMI) Module is proposed, which assists the training process of the main network to capture discriminative features of the input point clouds. Specifically, the MMI-based loss function is employed to constrain the differences of semantic information in two hierarchical features extracted from the shallow and deep layers of the network. Extensive experiments show that our method is generally applicable to point cloud tasks, including classification, shape retrieval, indoor scene segmentation, 3D object detection, and completion, and illustrate the efficacy of our proposed method and its advantages over existing ones. Our source code is available at https://github.com/wendydidi/MMI.git.","Deep learning, 3D vision, Point clouds, Mutual information",Di Wang and Lulu Tang and Xu Wang and Luqing Luo and Zhi-Xin Yang,https://www.sciencedirect.com/science/article/pii/S0031320322003739,https://doi.org/10.1016/j.patcog.2022.108892,0031-3203,2022,108892,131,Pattern Recognition,Improving deep learning on point cloud by maximizing mutual information across layers,article,WANG2022108892,
"Constraint selection is an effective means to alleviate the problem of a massive amount of constraints in metric learning. However, it is difficult to find and deal with all association constraints with the same hard-to-classify instance (i.e., an instance surrounded by dissimilar instances), negatively affecting metric learning algorithms. To address this problem, we propose a new metric learning algorithm from the perspective of selecting instances, Metric Learning via Perturbing of Hard-to-classify Instances (ML-PHI), which directly perturbs the hard-to-classify instances to reduce over-fitting for the hard-to-classify instances. ML-PHI perturbs hard-to-classify instances to be closer to similar instances while keeping the positions of the remaining instances as constant as possible. As a result, the negative impacts of hard-to-classify instances are effectively reduced. We have conducted extensive experiments on real data sets, and the results show that ML-PHI is effective and outperforms state-of-the-art methods.","Metric learning, Hard-to-classify instances, Instance perturbation, Alternating minimization",Xinyao Guo and Wei Wei and Jianqing Liang and Chuangyin Dang and Jiye Liang,https://www.sciencedirect.com/science/article/pii/S0031320322004095,https://doi.org/10.1016/j.patcog.2022.108928,0031-3203,2022,108928,132,Pattern Recognition,Metric learning via perturbing hard-to-classify instances,article,GUO2022108928,
"Anomaly detection in time series is a complex task that has been widely studied. In recent years, the ability of unsupervised anomaly detection algorithms has received much attention. This trend has led researchers to compare only learning-based methods in their articles, abandoning some more conventional approaches. As a result, the community in this field has been encouraged to propose increasingly complex learning-based models mainly based on deep neural networks. To our knowledge, there are no comparative studies between conventional, machine learning-based and, deep neural network methods for the detection of anomalies in multivariate time series. In this work, we study the anomaly detection performance of sixteen conventional, machine learning-based and, deep neural network approaches on five real-world open datasets. By analyzing and comparing the performance of each of the sixteen methods, we show that no family of methods outperforms the others. Therefore, we encourage the community to reincorporate the three categories of methods in the anomaly detection in multivariate time series benchmarks.","Anomaly detection, Multivariate time series, Neural networks",Julien Audibert and Pietro Michiardi and FrÃ©dÃ©ric Guyard and SÃ©bastien Marti and Maria A. Zuluaga,https://www.sciencedirect.com/science/article/pii/S0031320322004253,https://doi.org/10.1016/j.patcog.2022.108945,0031-3203,2022,108945,132,Pattern Recognition,Do deep neural networks contribute to multivariate time series anomaly detection?,article,AUDIBERT2022108945,
"For supervised classification tasks that involve a large number of instances, we propose and study a new efficient tool, namely the Local-to-Global Support Vector Machine (LGSVM) method. Its background somehow lies in the framework of approximation theory and of local kernel-based models, such as the Partition of Unity (PU) method. Indeed, even if the latter needs to be accurately tailored for classification tasks, such as allowing the use of the cosine semi-metric for defining the patches, the LGSVM is a global method constructed by gluing together the local SVM contributions via compactly supported weights. When the number of instances grows, such a construction of a global classifier enables us to significantly reduce the usually high complexity cost of SVMs. This claim is supported by a theoretical analysis of the LGSVM and of its complexity as well as by extensive numerical experiments carried out by considering benchmark datasets.","Local-to-global support vector machines, Partition of unity, Supervised classification, Kernel models",F. Marchetti and E. Perracchione,https://www.sciencedirect.com/science/article/pii/S0031320322004010,https://doi.org/10.1016/j.patcog.2022.108920,0031-3203,2022,108920,132,Pattern Recognition,Local-to-Global Support Vector Machines (LGSVMs),article,MARCHETTI2022108920,
"Due to its strong feature representation ability, the deep learning (DL)-based method is preferable for the unsupervised band selection task of hyperspectral image (HSI). However, the current DL-based UBS methods have not further investigated the nonlinear relationship between spectral bands, a more robust DL model with effective loss function is desired. To solve the above problem, a novel stochastic gate-based autoencoder (SGAE) has been proposed for the UBS task. With the proposed stochastic gate layer, the desired band subset with learnable parameters can be directly obtained. For obtaining better UBS results, a nonlinear regularization term is added with the loss function to supervise the training process of SGAE. Furthermore, an early stopping criteria with a regularization term-based threshold is developed. Experimental results on four publicly available remote sensing datasets prove the effectiveness of our SGAE.","Hyperspectral data, Unsupervised band selection, Autoencoder, Stochastic gate",He Sun and Lei Zhang and Lizhi Wang and Hua Huang,https://www.sciencedirect.com/science/article/pii/S0031320322004496,https://doi.org/10.1016/j.patcog.2022.108969,0031-3203,2022,108969,132,Pattern Recognition,Stochastic gate-based autoencoder for unsupervised hyperspectral band selection,article,SUN2022108969,
"With the deepening of deep neural network research, deep metric learning has been further developed and achieved good results in many computer vision tasks. Deep metric learning trains the deep neural network by designing appropriate loss functions, and the deep neural network projects the training samples into an embedding space, where similar samples are very close, while dissimilar samples are far away. In the past two years, the proxy-based loss achieves remarkable improvements, boosts the speed of convergence and is robust against noisy labels and outliers due to the introduction of proxies. In the previous proxy-based losses, fixed margins were used to achieve the goal of metric learning, but the intra-class variance of fine-grained images were not fully considered. In this paper, a new proxy-based loss is proposed, which aims to set a learnable margin for each class, so that the intra-class variance can be better maintained in the final embedding space. Moreover, we also add a loss between proxies, so as to improve the discrimination between classes and further maintain the intra-class distribution. Our method is evaluated on fine-grained image retrieval, person re-identification and remote sensing image retrieval common benchmarks. The standard network trained by our loss achieves state-of-the-art performance. Thus, the possibility of extending our method to different fields of pattern recognition is confirmed.","Deep metric learning, Proxy-based loss, Adaptive margin, Image retrieval, Fine-grained images",Yifan Wang and Pingping Liu and Yijun Lang and Qiuzhan Zhou and Xue Shan,https://www.sciencedirect.com/science/article/pii/S0031320322004411,https://doi.org/10.1016/j.patcog.2022.108961,0031-3203,2022,108961,132,Pattern Recognition,Learnable dynamic margin in deep metric learning,article,WANG2022108961,
"Vehicle re-identification (re-ID) aims to discover and match the target vehicles from a gallery image set taken by different cameras on a wide range of road networks. It is crucial for lots of applications such as security surveillance and traffic management. The remarkably similar appearances of distinct vehicles and the significant changes in viewpoints and illumination conditions pose grand challenges to vehicle re-ID. Conventional solutions focus on designing global visual appearances without sufficient consideration of vehiclesâ spatio-temporal relationships in different images. This paper proposes a discriminative feature representation with spatio-temporal clues (DFR-ST) for vehicle re-ID. It is capable of building robust features in the embedding space by involving appearance and spatio-temporal information. The proposed DFR-ST constructs an appearance model for a multi-grained visual representation by a two-stream architecture and a spatio-temporal metric to provide complementary information based on this multi-modal information. Experimental results on four public datasets demonstrate DFR-ST outperforms the state-of-the-art methods, which validates the effectiveness of the proposed method.","Vehicle re-identification, Computer vision, Deep learning, Attention mechanism, Video surveillance",Jingzheng Tu and Cailian Chen and Xiaolin Huang and Jianping He and Xinping Guan,https://www.sciencedirect.com/science/article/pii/S0031320322003685,https://doi.org/10.1016/j.patcog.2022.108887,0031-3203,2022,108887,131,Pattern Recognition,DFR-ST: Discriminative feature representation with spatio-temporal cues for vehicle re-identification,article,TU2022108887,
"Many existing works represent signals by covariance matrices and then develop learning methods on the Riemannian symmetric positive-definite (SPD) manifold to deal with such data. However, they summarize each instance with a single covariance matrix, omitting some potential important information, such as the time evolution of the correlation in signals. In this paper, we represent each instance by a sequence of covariance matrices and develop a novel dynamic generalized learning Riemannian space quantization (DGLRSQ) method to deal with such data representations. The proposed DGLRSQ method incorporates short-term memory mechanism in generalized learning Riemannian space quantization (GLRSQ), which is an extension of Euclidean generalized learning vector quantization to deal with SPD matrix-valued data. The proposed method can capture the temporal evolution of the correlation in signals and thus provides better performance to its the counterpart â GLRSQ, which treats each instance as a signal covariance matrix. Empirical investigations on synthetic data and motor imagery EEG data show the superior performance of the proposed method.","Learning vector quantization, Dynamic learning vector quantization, Riemannian manifold, Short-term memory",MengLing Fan and Fengzhen Tang and Yinan Guo and Xingang Zhao,https://www.sciencedirect.com/science/article/pii/S0031320322004137,https://doi.org/10.1016/j.patcog.2022.108932,0031-3203,2022,108932,132,Pattern Recognition,Riemannian dynamic generalized space quantization learning,article,FAN2022108932,
"Deep learning-based 6D object pose estimation methods from a single RGBD image have recently received increasing attention because of their powerful representation learning capabilities. These methods, however, cannot handle severe occlusion and truncation. In this paper, we present a novel 6D object pose estimation method based on multiple geometry representations. Specifically, we introduce a network to fuse the appearance and geometry features extracted from input color and depth images. Then, we utilize these per-point fusion features to estimate keypoint offsets, edge vectors, and dense symmetry correspondences in the canonical coordinate system. Finally, a two-stage pose regression module is applied to compute the 6D pose of an object. Relative to the unitary 3D keypoint-based strategy, such combination of multiple geometry representations provides sufficient and diverse information, especially for occluded or truncated scenes. To show the robustness to occlusion and truncation of the proposed method, we conduct comparative experiments on the Occlusion LineMOD, Truncation LineMOD, and T-LESS datasets. Results reveal that the proposed method outperforms state-of-the-art techniques by a large margin.","Neural network, Pose estimation, Keypoints, Edge vectors, Symmetry correspondences",Jichun Wang and Lemiao Qiu and Guodong Yi and Shuyou Zhang and Yang Wang,https://www.sciencedirect.com/science/article/pii/S0031320322003843,https://doi.org/10.1016/j.patcog.2022.108903,0031-3203,2022,108903,132,Pattern Recognition,Multiple geometry representations for 6D object pose estimation in occluded or truncated scenes,article,WANG2022108903,
"In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to provide supervision for attention. An observation we make is that visual explanations as obtained through class activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the attention maps. A generalization of the work is also provided by extending our approach to the task of âVisual Dialogâ where the attention is more contextual. Thorough evaluation for this task is also provided. Visualization of the results confirms our hypothesis that attention maps improve using the proposed form of supervision.","CNN, LSTM, Explanation, Attention, Grad-CAM, MMD, CORAL, GAN, VQA, Visual Dialog, Deep learning",Badri N. Patro and  Anupriy and Vinay P. Namboodiri,https://www.sciencedirect.com/science/article/pii/S003132032200379X,https://doi.org/10.1016/j.patcog.2022.108898,0031-3203,2022,108898,132,Pattern Recognition,Explanation vs. attention: A two-player game to obtain attention for VQA and visual dialog,article,PATRO2022108898,
"In this paper, we first integrate normalization to the Ensemble Deep Random Vector Functional Link network (edRVFL). This re-normalization step can help the network avoid divergence of the hidden features. Then, we propose novel variants of the edRVFL network. Weighted edRVFL (WedRVFL) uses weighting methods to give training samples different weights in different layers according to how the samples were classified confidently in the previous layer thereby increasing the ensembleâs diversity and accuracy. Furthermore, a pruning-based edRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based on their importance for classification before generating the next hidden layer. Through this method, we ensure that the randomly generated inferior features will not propagate to deeper layers. Subsequently, the combination of weighting and pruning, called Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network (WPedRVFL), is proposed. We compare their performances with other state-of-the-art classification methods on 24 tabular UCI classification datasets. The experimental results illustrate the superior performance of our proposed methods.","Ensemble deep random vector functional link (edRVFL), Weighting methods, Pruning, UCI classification datasets",Qiushi Shi and Minghui Hu and Ponnuthurai Nagaratnam Suganthan and Rakesh Katuwal,https://www.sciencedirect.com/science/article/pii/S0031320322003600,https://doi.org/10.1016/j.patcog.2022.108879,0031-3203,2022,108879,132,Pattern Recognition,Weighting and pruning based ensemble deep random vector functional link network for tabular data classification,article,SHI2022108879,
"Reinforcement learning is a useful tool for training an agent to effectively achieve the desired goal in the sequential decision-making problem. It trains the agent to make decision by exploiting the experience in the transitions resulting from the different decisions. To exploit this experience, most reinforcement learning methods replay the explored transitions by uniform sampling. But in this way, it is easy to ignore the last explored transitions. Another way to exploit this experience defines the priority of each transition by the estimation error in training and then replays the transitions according to their priorities. But it only updates the priorities of the transitions replayed at the current training time step, thus the transitions with low priorities will be ignored. In this paper, we propose a clustering experience replay, called CER, to effectively exploit the experience hidden in all explored transitions in the current training. CER clusters and replays the transitions by a divide-and-conquer framework based on time division as follows. Firstly, it divides the whole training process into several periods. Secondly, at the end of each period, it uses k-means to cluster the transitions explored in this period. Finally, it constructs a conditional probability density function to ensure that all kinds of transitions will be sufficiently replayed in the current training. We construct a new method, TD3_CER, to implement our clustering experience replay on TD3. Through the theoretical analysis and experiments, we illustrate that our TD3_CER is more effective than the existing reinforcement learning methods. The source code can be downloaded from https://github.com/grcai/CER-Master.","Reinforcement learning, Clustering, Experience replay, Exploitation efficiency, Time division",Min Li and Tianyi Huang and William Zhu,https://www.sciencedirect.com/science/article/pii/S0031320322003569,https://doi.org/10.1016/j.patcog.2022.108875,0031-3203,2022,108875,131,Pattern Recognition,Clustering experience replay for the effective exploitation in reinforcement learning,article,LI2022108875,
"Change detection is a crucial but extremely challenging task in remote sensing image analysis, and much progress has been made with the rapid development of deep learning. However, most existing deep learning-based change detection methods try to elaborately design complicated neural networks with powerful feature representations. However, they ignore the universal domain shift induced by time-varying land cover changes, including luminance fluctuations and seasonal changes between pre-event and post-event images, thereby producing suboptimal results. In this paper, we propose an end-to-end supervised domain adaptation framework for cross-domain change detection named SDACD, to effectively alleviate the domain shift between bi-temporal images for better change predictions. Specifically, our SDACD presents collaborative adaptations from both image and feature perspectives with supervised learning. Image adaptation exploits generative adversarial learning with cycle-consistency constraints to perform cross-domain style transformation, which effectively narrows the domain gap in a two-side generation fashion. As for feature adaptation, we extract domain-invariant features to align different feature distributions in the feature space, which could further reduce the domain gap of cross-domain images. To further improve the performance, we combine three types of bi-temporal images for the final change prediction, including the initial input bi-temporal images and two generated bi-temporal images from the pre-event and post-event domains. Extensive experiments and analyses conducted on two benchmarks demonstrate the effectiveness and generalizability of our proposed framework. Notably, our framework pushes several representative baseline models up to new State-Of-The-Art records, achieving 97.34% and 92.36% on the CDD and WHU building datasets, respectively. The source code and models are publicly available at https://github.com/Perfect-You/SDACD.","Change Detection, Supervised Domain Adaptation, Image Adaptation, Feature Adaptation",Jia Liu and Wenjie Xuan and Yuhang Gan and Yibing Zhan and Juhua Liu and Bo Du,https://www.sciencedirect.com/science/article/pii/S003132032200440X,https://doi.org/10.1016/j.patcog.2022.108960,0031-3203,2022,108960,132,Pattern Recognition,An End-to-end Supervised Domain Adaptation Framework for Cross-Domain Change Detection,article,LIU2022108960,
"Learning dynamics of collectively moving agents such as fish or humans is an essential task in research. Due to phenomena such as occlusion or change of illumination, the multi-object methods tracking such dynamics may lose the tracks of the agents which may result in fragmentations of trajectories. Here, we present an extended deep autoencoder (DA) that we train only on the fully observed segments of the trajectories by defining its loss function as the Hadamard product of a binary indicator matrix with the absolute difference between the outputs and the labels. The trajectory matrix of the agents practicing collective motion is low-rank due to mutual interactions and dependencies between the agents that we utilize as the underlying pattern that our Hadamard deep autoencoder (HDA) codes during its training. The performance of this HDA is compared with that of a low-rank matrix completion scheme in the context of fragmented trajectory reconstruction.","Multi-object tracking, Collective motion, Deep autoencoders, Hadamard product, Self-propelled particles",Kelum Gajamannage and Yonggi Park and Randy Paffenroth and Anura P. Jayasumana,https://www.sciencedirect.com/science/article/pii/S0031320322003727,https://doi.org/10.1016/j.patcog.2022.108891,0031-3203,2022,108891,131,Pattern Recognition,Reconstruction of fragmented trajectories of collective motion using Hadamard deep autoencoders,article,GAJAMANNAGE2022108891,
"The prevalence of employing attention mechanisms has brought along concerns about the interpretability of attention distributions. Although it provides insights into how a model is operating, utilizing attention as the explanation of model predictions is still highly dubious. The community is still seeking more interpretable strategies for better identifying local active regions that contribute the most to the final decision. To improve the interpretability of existing attention models, we propose a novel Bilinear Representative Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant human-interpretable information. The target model is first distilled to have higher-resolution intermediate feature maps. From which, representative features are then grouped based on local pairwise feature similarity, to produce finer-grained, more precise attention maps highlighting task-relevant parts of the input. The obtained attention maps are ranked according to the activity level of the compound feature, which provides information regarding the important level of the highlighted regions. The proposed model can be easily adapted in a wide variety of modern deep models, where classification is involved. Extensive quantitative and qualitative experiments showcase more comprehensive and accurate visual explanations compared to state-of-the-art attention models and visualization methods across multiple tasks including fine-grained image classification, few-shot classification, and person re-identification, without compromising the classification accuracy. The proposed visualization model sheds imperative light on how neural networks âpay their attentionâ differently in different tasks.","Deep learning, Interpretability, Spatial attention, Resolution, Non-parametric",Tristan Gomez and Suiyi Ling and Thomas FrÃ©our and Harold MouchÃ¨re,https://www.sciencedirect.com/science/article/pii/S0031320322004083,https://doi.org/10.1016/j.patcog.2022.108927,0031-3203,2022,108927,132,Pattern Recognition,BR-NPA: A non-parametric high-resolution attention model to improve the interpretability of attention,article,GOMEZ2022108927,
"Gaze tracking estimates and tracks the userâs gaze by analyzing facial or eye features, it is an important way to realize automated vision-based interaction. This paper introduces the visual information used in gaze tracking, and discusses the commonly used gaze estimation methods and their research dynamics, including: 2D mapping-based methods, 3D model-based methods, and appearance-based methods. In this way, some key issues that need to be solved in these methods are considered, and their research trends are discussed. Their characteristics in system configuration, personal calibration, head motion, gaze accuracy and robustness are also compared. Finally, the applications of gaze tracking techniques are analyzed from various application factors and fields. This paper reviews the latest development of gaze tracking, focuses more on various gaze tracking algorithms and their existing challenges. The development trends of gaze tracking are prospected, which provides ideas for future theoretical research and practical applications.","Gaze estimation, eye features, appearance-based, personal calibration, head motion",Jiahui Liu and Jiannan Chi and Huijie Yang and Xucheng Yin,https://www.sciencedirect.com/science/article/pii/S0031320322004241,https://doi.org/10.1016/j.patcog.2022.108944,0031-3203,2022,108944,132,Pattern Recognition,In the eye of the beholder: A survey of gaze tracking techniques,article,LIU2022108944,
"Deep non-negative matrix factorization is committed to using multi-layer structure to extract underlying parts-based representation. However, the basis images obtained by continuous depth factorization is too sparse, resulting in too fragmented parts reflected by the basis image. This makes the number of factorization layers limited and the underlying local feature representation is inaccurate. Therefore, we propose a novel progressive deep non-negative matrix factorization (PDNMF) architecture that adds a basis image reconstruction step to the successive basis image factorization steps. This helps the basis image in depth factorization to maintain better robustness of feature representation. In the reconstruction step, the attribute similarity graph (ASG) is constructed to describe the semantic expression ability of each basis image. With the help of the ASG, the basis image enhances its own semantic integrity through graph convolution without drastically destroying its representation. The evaluation in image recognition shows that the recognition accuracy of the proposed PDNMF improves with the increase of layers. Our method outperforms the state-of-the-art deep factorization methods in image recognition.","Deep non-negative matrix factorization, Graph convolution, Basis image reconstruction, Basis image factorization, Face recognition",Yang Zhao and Furong Deng and Jihong Pei and Xuan Yang,https://www.sciencedirect.com/science/article/pii/S0031320322004642,https://doi.org/10.1016/j.patcog.2022.108984,0031-3203,2022,108984,132,Pattern Recognition,Progressive Deep Non-Negative Matrix Factorization Architecture with Graph Convolution-based Basis Image Reorganization,article,ZHAO2022108984,
"In many object recognition applications, the set of possible categories is an open set, and the deployed recognition system will encounter novel objects belonging to categories unseen during training. Detecting such ânovel categoryâ objects is usually formulated as an anomaly detection problem. Anomaly detection algorithms for feature-vector data identify anomalies as outliers, but outlier detection has not worked well in deep learning. Instead, methods based on the computed logits of visual object classifiers give state-of-the-art performance. This paper proposes the Familiarity Hypothesis that these methods succeed because they are detecting the absence of familiar learned features rather than the presence of novelty. This distinction is important, because familiarity-based detection will fail in many situations where novelty is present. For example when an image contains both a novel object and a familiar one, the familiarity score will be high, so the novel object will not be noticed. The paper reviews evidence from the literature and presents additional evidence from our own experiments that provide strong support for this hypothesis. The paper concludes with a discussion of whether familiarity-based detection is an inevitable consequence of representation learning.","Anomaly detection, Open set learning, Computer vision, Object recognition, Novel category detection, Representation learning, Deep learning",Thomas G. Dietterich and Alex Guyer,https://www.sciencedirect.com/science/article/pii/S0031320322004125,https://doi.org/10.1016/j.patcog.2022.108931,0031-3203,2022,108931,132,Pattern Recognition,The familiarity hypothesis: Explaining the behavior of deep open set methods,article,DIETTERICH2022108931,
"Cancellable biometrics (CB) is one of the major approaches for biometric template protection. However, almost all the prior arts are designed to work under verification (one-to-one matching). This paper proposes a deep learning-based cancellable biometric scheme for face identification (one-to-many matching). Our scheme comprises two key ingredients: a deep rank hashing (DRH) network and a cancellable identification scheme. The DRH network transforms a raw face image into discriminative yet compact face hash codes based upon the nonlinear subspace ranking notion. The network is designed to be trained for both identification and hashing goals with their respective rich identity-related and rank hashing relevant loss functions. A modified softmax function is utilized to alleviate the hashing quantization error, and a regularization term is designed to encourage hash code balance. The hash code is binarized, compressed, and secured with the randomized lookup table function. Unlike prior CB schemes that require two input factors for verification, the proposed scheme demands no additional input except face images during identification, yet the face template is replaceable whenever needed based upon a one-time XOR cipher notion. The proposed scheme is evaluated on five public unconstrained face datasets in terms of verification, closed-set and open-set identification performance accuracy, computation cost, template protection criteria, and security.","Cancellable biometrics, Deep learning, Face biometrics, Hashing, Identification",Xingbo Dong and Sangrae Cho and Youngsam Kim and Soohyung Kim and Andrew Beng Jin Teoh,https://www.sciencedirect.com/science/article/pii/S0031320322003673,https://doi.org/10.1016/j.patcog.2022.108886,0031-3203,2022,108886,131,Pattern Recognition,Deep rank hashing network for cancellable face identification,article,DONG2022108886,
"With the continuous increase of the amount of information, people urgently need to identify the information in the image in more detail in order to obtain richer information from the image. This work explores the dynamic complex image segmentation of self-driving vehicle under Digital Twins (DTs) based on Memory-augmented Neural Networks (MANNs), so as to further improve the performance of self-driving in intelligent transportation. In view of the complexity of the environment and the dynamic changes of the scene in intelligent transportation, this work constructs a segmentation model for dynamic complex image of self-driving vehicle under DTs based on MANNs by optimizing the Deep Learning algorithm and further combining with the DTs technology, so as to recognize the information in the environment image during the self-driving. Finally, the performance of the constructed model is analyzed by experimenting with different image datasets (PASCALVOC 2012, NYUDv2, PASCAL CONTEXT, and real self-driving complex traffic image data). The results show that compared with other classical algorithms, the established MANN-based model has an accuracy of about 85.80%, the training time is shortened to 107.00 s, the test time is 0.70 s, and the speedup ratio is high. In addition, the average algorithm parameter of the given energy function Î±=0.06 reaches the maximum value. Therefore, it is found that the proposed model shows high accuracy and short training time, which can provide experimental reference for future image visual computing and intelligent information processing.","Deep learning, Image segmentation, Memory-augmented neural networks, LSTM, Self-driving, Digital twins",Zhihan Lv and Liang Qiao and Shuo Yang and Jinhua Li and Haibin Lv and Francesco Piccialli,https://www.sciencedirect.com/science/article/pii/S0031320322004368,https://doi.org/10.1016/j.patcog.2022.108956,0031-3203,2022,108956,132,Pattern Recognition,Memoryâaugmented neural networks based dynamic complex image segmentation in digital twins for selfâdriving vehicle,article,LV2022108956,
"Developing machine learning models that can detect and localize the unexpected or anomalous structures within images is very important for numerous computer vision tasks, such as the defect inspection of manufactured products. However, it is challenging especially when there are few or even no anomalous image samples available. In this paper, we propose an unsupervised mechanism, i.e. deep feature correspondence (DFC), which can be effectively leveraged to detect and segment out the anomalies in images solely with the prior knowledge from anomaly-free samples. We develop our DFC in an asymmetric dual network framework that consists of a generic feature extraction network and an elaborated feature estimation network, and detect the possible anomalies within images by modeling and evaluating the associated deep feature correspondence between the two dual network branches. Furthermore, to improve the robustness of the DFC and further boost the detection performance, we specifically propose a self-feature enhancement (SFE) strategy and a multi-context residual learning (MCRL) network module. Extensive experiments have been carried out to validate the effectiveness of our DFC and the proposed SFE and MCRL. Our approach is very effective for detecting and segmenting the anomalies that appear in confined local regions of images, especially the industrial anomalies. It advances the state-of-the-art performances on the benchmark dataset â MVTec AD. Besides, when applied to a real industrial inspection scene, it outperforms the comparatives by a large margin.","Anomaly detection, Anomaly segmentation, Feature correspondence, Dual network",Jie Yang and Yong Shi and Zhiquan Qi,https://www.sciencedirect.com/science/article/pii/S0031320322003557,https://doi.org/10.1016/j.patcog.2022.108874,0031-3203,2022,108874,132,Pattern Recognition,Learning deep feature correspondence for unsupervised anomaly detection and segmentation,article,YANG2022108874,
"Object detectors are typically learned on fully-annotated training data with fixed predefined categories. However, categories are often required to be increased progressively. Usually, only the original training set annotated with old classes and some new training data labeled with new classes are available in such scenarios. Based on the limited datasets, a unified detector that can handle all categories is strongly needed. We propose a practical scheme to achieve it in this work. A conflict-free loss is designed to avoid label ambiguity, leading to an acceptable detector in one training round. To further improve performance, we propose a retraining phase in which Monte Carlo Dropout is employed to calculate the localization confidence to mine more accurate bounding boxes, and an overlap-weighted method is proposed for making better use of pseudo annotations during retraining. Extensive experiments demonstrate the effectiveness of our method.","Object detector, Category-extended, Limited data, Multi-dataset",Bowen Zhao and Chen Chen and Xi Xiao and Shutao Xia,https://www.sciencedirect.com/science/article/pii/S003132032200423X,https://doi.org/10.1016/j.patcog.2022.108943,0031-3203,2022,108943,132,Pattern Recognition,Towards a category-extended object detector with limited data,article,ZHAO2022108943,
"It is very challenging to accurately segment smoke images because smoke has some adverse properties, such as semi-transparency and blurry boundary. Aiming at solving these problems, we first fuse convolutional results along different axes to equivalently produce a cubic-cross convolutional kernel, which enlarges receptive fields at affordable computational costs for capturing long-range dependency of smoke pixels, and then we propose a Cubic-cross Convolutional Attention (CCA). To embed global category information, we propose a count prior structure to model and supervise the count of smoke pixels. To ensure the network can correctly extract a count prior map, we impose a regression loss on the count prior map and corresponding ideal count map directly calculated from its ground truth. Then we multiply the reshaped input by the count prior map to produce a Count Prior Attention (CPA) map, which is upsampled to generate the final output. A cross entropy loss is used to supervise the final segmentation. Finally, we use ResNet50 for feature encoding, and stack CCA and CPA together to propose a Cubic-cross convolutional attention and Count prior Embedding Network (CCENet) for smoke segmentation. Experiments on both synthetic and real smoke datasets show that our method outperforms existing state-of-the-art methods.","Smoke segmentation, Information embedding, Cubic-cross convolutional attention, Count prior attention",Feiniu Yuan and Zeshu Dong and Lin Zhang and Xue Xia and Jinting Shi,https://www.sciencedirect.com/science/article/pii/S0031320322003831,https://doi.org/10.1016/j.patcog.2022.108902,0031-3203,2022,108902,131,Pattern Recognition,Cubic-cross convolutional attention and count prior embedding for smoke segmentation,article,YUAN2022108902,
"Nowadays, deep learning methods have achieved state-of-the-art results in human action recognition. These methods process a full video sequence to recognize an action, which is unnecessary because many frames are similar. Recently, keyframe-based methods are proposed to overcome this issue. Though keyframe based methods have shown competitive performance in action recognition, both methods still process all the required frames of a video clip and average the results of individual clips/frames to recognize the action of the video. We argue that by simply using the average of the results of the video clips, deep models are not using the motion information of the video and thus leads to an inaccurate recognition of the action. To cope with the aforementioned issue, we propose a new online temporal classification model (OTCM) that classifies an action from a video in an online fashion and addresses the issue of averaging by making decision of each frame of a video sequence. As well, we propose a new action inference graph (AIG) that enables early recognition. Hence, the proposed model can recognize an action early before using all the keyframes or the whole video sequence and thus, requires less computation for recognizing human actions. Moreover, our OTCM can perform online action detection. To the best of our knowledge, this is the first time that the OTCM model along with the AIG is proposed. The experimental results of the benchmark datasets show that the proposed OTCM model has achieved and set a new record of the SOTA results, in particular, without using full video sequences.","Online temporal classification, Action recognition, Action detection, Action inference graph",G M Mashrur {E Elahi} and Yee-Hong Yang,https://www.sciencedirect.com/science/article/pii/S0031320322004526,https://doi.org/10.1016/j.patcog.2022.108972,0031-3203,2022,108972,132,Pattern Recognition,Online temporal classification of human action using action inference graph,article,EELAHI2022108972,
"Information in digital mammogram images has been shown to be associated with the risk of developing breast cancer. Longitudinal breast cancer screening mammogram examinations may carry spatiotemporal information that can enhance breast cancer risk prediction. No deep learning models have been designed to capture such spatiotemporal information over multiple examinations to predict the risk. In this study, we propose a novel deep learning structure, LRP-NET, to capture the spatiotemporal changes of breast tissue over multiple negative/benign screening mammogram examinations to predict near-term breast cancer risk in a case-control setting. Specifically, LRP-NET is designed based on clinical knowledge to capture the imaging changes of bilateral breast tissue over four sequential mammogram examinations. We evaluate our proposed model with two ablation studies and compare it to three models/settings, including 1) a âlooseâ model without explicitly capturing the spatiotemporal changes over longitudinal examinations, 2) LRP-NET but using a varying number (i.e., 1 and 3) of sequential examinations, and 3) a previous model that uses only a single mammogram examination. On a case-control cohort of 200 patients, each with four examinations, our experiments on a total of 3200 images show that the LRP-NET model outperforms the compared models/settings.","Breast cancer, Risk prediction, Deep learning, Digital mammogram, Longitudinal data",Saba Dadsetan and Dooman Arefan and Wendie A. Berg and Margarita L. Zuley and Jules H. Sumkin and Shandong Wu,https://www.sciencedirect.com/science/article/pii/S0031320322004009,https://doi.org/10.1016/j.patcog.2022.108919,0031-3203,2022,108919,132,Pattern Recognition,Deep learning of longitudinal mammogram examinations for breast cancer risk prediction,article,DADSETAN2022108919,
"The purpose of this work is to develop a method for accurate and robust prostate segmentation in transrectal ultrasound (TRUS) images. These images are difficult to segment due to missing/ambiguous boundary between the prostate and neighboring structures, the presence of shadow artifacts, as well as the large variability in prostate shapes. This paper develops a novel hybrid method for TRUS prostate segmentation by combining an improved principal curve-based method with an evolutionary neural network; the former for achieving the data sequences while and the latter for improving the smoothness of the prostate contour. Both qualitative and quantitative experimental results showed that our proposed method achieved superior segmentation accuracy and robustness as compared to state-of-the-art methods. The average Dice similarity coefficient (DSC), Jaccard similarity coefficient (Î©), and accuracy (ACC) of prostate contours against ground-truths were 96.8%, 95.7%, and 96.4%, and the DSC of around 92% and 95% for other deep learning and hybrid methods, respectively.","Accurate prostate segmentation, Transrectal ultrasound, Principal curve, Optimized closed polygonal segment method, Evolutionary neural network, Interpretable mathematical model",Tao Peng and Jing Zhao and Yidong Gu and Caishan Wang and Yiyun Wu and Xiuxiu Cheng and Jing Cai,https://www.sciencedirect.com/science/article/pii/S0031320322003715,https://doi.org/10.1016/j.patcog.2022.108890,0031-3203,2022,108890,131,Pattern Recognition,H-ProMed: Ultrasound image segmentation based on the evolutionary neural network and an improved principal curve,article,PENG2022108890,
"In this study, we develop a fast and accurate computational method for a weighted three-dimensional (3D) volume reconstruction from a series of slice data using a phase-field model. The proposed method is based on a modified AllenâCahn (AC) equationÂ with a fidelity term. The algorithm automatically generates the necessary slices between the given slices by solving the governing equation. To reconstruct a 3D volume, we first set a source slice and target slice. Next, we set the source slice as the initial condition and the target slice as the fidelity function. Finally, we retain the numerical solutions during an evolution as intermediate slices between the source and target slices. There are two criteria for choosing the intermediate slice: One is based on the area of the symmetric difference between the phase-field solution and the target and the other is based on the change of the phase-field solution relative to the area of the target. We use the weighted average of the two criteria. To validate the efficiency and accuracy of the proposed numerical algorithm, several computational experiments are conducted. Computational test results confirm the superior performance of the proposed algorithm.","Shape transformation, 3D volume reconstruction, AllenâCahn equation",Yibao Li and Xin Song and Soobin Kwak and Junseok Kim,https://www.sciencedirect.com/science/article/pii/S0031320322003958,https://doi.org/10.1016/j.patcog.2022.108914,0031-3203,2022,108914,132,Pattern Recognition,Weighted 3D volume reconstruction from series of slice data using a modified AllenâCahn equation,article,LI2022108914,
"As one of the most successful local feature descriptors, the local binary pattern (LBP) estimates the texture distribution rule of an image based on the signs of differences between neighboring pixels to obtain intensity- and rotation- invariance. In this paper, we propose a novel image descriptor to address scale transformation and noise interference simultaneously. We name it scale-selective and noise-robust extended LBP (SNELBP). First, each image in training sets is transformed into different scale spaces by a Gaussian filter. Second, noise-robust pattern histograms are obtained from each scale space by using our previously proposed median robust extended LBP (MRELBP). Then, scale-invariant histograms are determined by selecting the maximum among all scale levels for a certain image. Finally, the most informative patterns are selected from the dictionary pretrained by the two-stage compact dominant feature selection method (CDFS), maintaining the descriptor more lightweight with sufficiently low time cost. Extensive experiments on five public databases (Outex_TC_00011, TC_00012, KTH-TIPS, UMD and NEU) and one fresh texture database (JoJo) under two kinds of interferences (Gaussian and salt pepper) indicate that our SNELBP yields more competitive results than thirty classical LPB variants as well as eight typical deep learning methods.","Local binary pattern (LBP), Texture descriptor, Feature extraction, Texture classification",Qiwu Luo and Jiaojiao Su and Chunhua Yang and Olli Silven and Li Liu,https://www.sciencedirect.com/science/article/pii/S003132032200382X,https://doi.org/10.1016/j.patcog.2022.108901,0031-3203,2022,108901,132,Pattern Recognition,Scale-selective and noise-robust extended local binary pattern for texture classification,article,LUO2022108901,
"Graph embedding is an effective method for deriving low-dimensional representations of graph data. The power of graph deep learning methods to characterize electroencephalogram (EEG) graph embedding is still in question. We designed a novel graph variational auto-encoder (GVAE) method to extract nodal features of brain functional connections. A new decoder model for the GVAEs network is proposed, which considers the node neighborhood of the reconstructed adjacency matrix. The GVAE is applied and tested on 3 biometric databases which contain 64 to 9 channelsâ EEG recordings. For all datasets, promising results with more than 95% accuracy and considerably low computational cost are achieved compared to state-of-the-art user identification methods. The proposed GVAE is robust to a limited number of nodes and stable to usersâ task performance. Moreover, we developed a traditional variational auto-encoder to demonstrate that more accurate features can be obtained when observing EEG-based brain connectivity from a graph perspective.","Biometrics, Functional connectivity, Electroencephalogram (EEG), Graph variational auto encoder (GVAE), Graph deep learning",Tina Behrouzi and Dimitrios Hatzinakos,https://www.sciencedirect.com/science/article/pii/S0031320321003848,https://doi.org/10.1016/j.patcog.2021.108202,0031-3203,2022,108202,121,Pattern Recognition,Graph variational auto-encoder for deriving EEG-based graph embedding,article,BEHROUZI2022108202,
"Neuro-fuzzy models have been applied in various domains, in which the issue of long time-consumption for optimizing parameters and less innovation in fuzzy method for feature extraction remains to be solved. Here, we present a novel cycle reinforce hierarchical model (CRHM) for effective and efficient recognition. The innovative strategies of CRHM consist of the hierarchical structure, the groups of fuzzy subsystems and the cycle mechanism. The hierarchical structure is innovatively built to extract features and transform the low-level features into advanced ones semantically, in which we adopt the groups of fuzzy subsystems as feature extraction units in each hidden layer, which ensures the diversity of features, avoids the fuzzy rules explosion, and reduces the time for clustering. The cycle mechanism is first proposed to connect the hierarchical structure and the output layer directly, transferring the tuned parameters again and again, to reinforce features gradually. To demonstrate the performance of CRHM, we have conducted extensive comparison with several state-of-the-art algorithms on benchmark 1D and 2D datasets. The experimental results show that the recognition rate of CRHM is higher than convolutional neural network (CNN), while the training time is only 5% of CNN's, which confirms that our approach provides a novel model for recognition, which can simultaneously improve the effectiveness and efficiency without the need of advanced equipment. In addition, the analysis results about the contribution of the core strategies to CRHM performance indicates that the contribution of the hierarchical structure is greater than that of the groups of fuzzy subsystems, which is superior than that of the cycle mechanism.","Pattern recognition, Hierarchical structure, Fuzzy system, Cycle mechanism",Hongjun Li and Ze Zhou and Chaobo Li and Ching Y. Suen,https://www.sciencedirect.com/science/article/pii/S0031320321003605,https://doi.org/10.1016/j.patcog.2021.108173,0031-3203,2022,108173,122,Pattern Recognition,A near effective and efficient model in recognition,article,LI2022108173,
"Pedestrian Attribute Recognition (PAR) is an important task in computer vision community and plays an important role in practical video surveillance. The goal of this paper is to review existing works using traditional methods or based on deep learning networks. Firstly, we introduce the background of pedestrian attribute recognition, including the fundamental concepts and formulation of pedestrian attributes and corresponding challenges. Secondly, we analyze popular solutions for this task from eight perspectives. Thirdly, we discuss the specific attribute recognition, then, give a comparison between deep learning and traditional algorithm based PAR methods. After that, we show the connections between PAR and other computer vision tasks. Fourthly, we introduce the benchmark datasets, evaluation metrics in this community, and give a brief performance comparison. Finally, we summarize this paper and give several possible research directions for PAR. The project page of this paper can be found at: https://sites.google.com/view/ahu-pedestrianattributes/.","Pedestrian attribute recognition, Multi-label learning, Multi-task learning, Deep learning, CNN-RNN",Xiao Wang and Shaofei Zheng and Rui Yang and Aihua Zheng and Zhe Chen and Jin Tang and Bin Luo,https://www.sciencedirect.com/science/article/pii/S0031320321004015,https://doi.org/10.1016/j.patcog.2021.108220,0031-3203,2022,108220,121,Pattern Recognition,Pedestrian attribute recognition: A survey,article,WANG2022108220,
"Nowadays, multi-label learning is ubiquitous in practical applications, in which multi-label data is always confronted with the curse of high-dimensional features. Feature selection has been shown to effectively improve learning performance by selecting discriminative features. Conventional multi-label feature selection only focuses on associating input features with corresponding labels while neglecting the potential structural information, i.e., instance correlations and label correlations. To tackle this problem, we propose manifold learning with structured subspace for multi-label feature selection. Specifically, we first uncover a latent subspace for a more compact and accurate data representation, and take advantage of the subspace to explore the correlations among instances. Then, we explore label correlations in manifold learning to guarantee the global and local structural consistency of labels. Besides, l2,1-norm is introduced into loss function and sparse regularization to facilitate feature selection process. A detail optimization algorithm is presented to solve the objective function of the proposed method. Extensive experiments on real-world data show the superiority of the proposed method under various metrics.","Multi-label learning, Feature selection, Manifold learning, Structured subspace, Instance correlations, Label correlations",Yuling Fan and Jinghua Liu and Peizhong Liu and Yongzhao Du and Weiyao Lan and Shunxiang Wu,https://www.sciencedirect.com/science/article/pii/S0031320321003563,https://doi.org/10.1016/j.patcog.2021.108169,0031-3203,2021,108169,120,Pattern Recognition,Manifold learning with structured subspace for multi-label feature selection,article,FAN2021108169,
"Recently, graph representation learning based on autoencoders has received much attention. However, these methods suffer from two limitations. First, most graph autoencoders ignore the reconstruction of either the graph structure or the node attributes, which often leads to a poor latent representation of the graph-structured data. Second, for existing graph autoencoders models, the encoder and decoder are mainly composed of an initial graph convolutional network (GCN) or its variants. These traditional GCN-based graph autoencoders more or less encounter the problem of incomplete filtering, which causes these models to be unstable in practical applications. To address the above issues, this paper proposes the Graph convolutional Autoencoders with co-learning of graph Structure and Node attributes (GASN) based on variational autoencoders. Specifically, the proposed GASN encodes and decodes the node attributes and graph structure comprehensively in the graph-structured data. Furthermore, we design a completely low-pass graph encoder and a high-pass graph decoder. The experimental results on real-world datasets demonstrate that the proposed GASN achieves state-of-the-art performance on node clustering, link prediction, and visualization tasks.","Graph representation learning, Graph convolutional autoencoders, Graph filter",Jie Wang and Jiye Liang and Kaixuan Yao and Jianqing Liang and Dianhui Wang,https://www.sciencedirect.com/science/article/pii/S0031320321003964,https://doi.org/10.1016/j.patcog.2021.108215,0031-3203,2022,108215,121,Pattern Recognition,Graph convolutional autoencoders with co-learning of graph structure and node attributes,article,WANG2022108215,
"Individual diversity poses a cross-user performance variance challenge that stumbles the practicality, especially for the wireless gesture tracking systems. Since the difficulty of annotating low-semantic wireless data limits constructing a big dataset, the recognizer should quickly adjust to different individuals via small datasets. To this end, we present TransTrack, an accurate wireless indoor gesture tracking system that can adjust to different users quickly. The key insight is that each unlabeled gesture contains learnable individual features that can help the gesture tracking model learning how to adapt to different users. Specifically, TransTrack uses recursive Otsu segmentation to separate gesture-induced signals with the background noise inspired by image segmentation. It then augments training data to learn the transferable features by leveraging the redundant information. A datum-based alignment method is proposed to unlock the limitation of classifier selection without distortion. Finally, TransTrack proposes an online meta-transfer learning method that collects unlabeled data transparently to train the tracking model for different tasks. Extensive experiments show that TransTrack can quickly adapt to different users and conditions.","Individual diversity, Meta-transfer learning, Gesture tracking, Channel state information, Data alignment, Online learning",Jiang Xiao and Huichuwu Li and Hai Jin,https://www.sciencedirect.com/science/article/pii/S0031320321003447,https://doi.org/10.1016/j.patcog.2021.108157,0031-3203,2022,108157,121,Pattern Recognition,Transtrack: Online meta-transfer learning and Otsu segmentation enabled wireless gesture tracking,article,XIAO2022108157,
"Salient object detection (SOD) is viewed as a pixel-wise saliency modeling task by traditional deep learning-based methods. A limitation of current SOD models is insufficient utilization of inter-pixel information, which usually results in imperfect segmentation near edge regions and low spatial coherence. As we demonstrate, using a saliency mask as the only label is suboptimal. To address this limitation, we propose a connectivity-based approach called bilateral connectivity network (BiconNet), which uses connectivity masks together with saliency masks as labels for effective modeling of inter-pixel relationships and object saliency. Moreover, we propose a bilateral voting module to enhance the output connectivity map, and a novel edge feature enhancement method that efficiently utilizes edge-specific features. Through comprehensive experiments on five benchmark datasets, we demonstrate that our proposed method can be plugged into any existing state-of-the-art saliency-based SOD framework to improve its performance with negligible parameter increase.","Salient object detection, Visual saliency, Connectivity modeling, Deep learning, Edge modeling",Ziyun Yang and Somayyeh Soltanian-Zadeh and Sina Farsiu,https://www.sciencedirect.com/science/article/pii/S003132032100412X,https://doi.org/10.1016/j.patcog.2021.108231,0031-3203,2022,108231,121,Pattern Recognition,BiconNet: An edge-preserved connectivity-based approach for salient object detection,article,YANG2022108231,
"We propose a novel solution that classifies very similar images (fine-grained classification) of variants of retail products displayed on the racks of supermarkets. The proposed scheme simultaneously captures object-level and part-level cues of the product images. The object-level cues of the product images are captured with our novel reconstruction-classification network (RC-Net). For annotation-free modeling of part-level cues, the discriminatory parts of the product images are identified around the keypoints. The ordered sequences of these discriminatory parts, encoded using convolutional LSTM, describe the products uniquely. Finally, the part-level and object-level models jointly determine the products explicitly explaining coarse to finer descriptions of the products. This bi-level architecture is embedded in R-CNN for recognizing variants of retail products on the rack. We perform extensive experiments on one In-house and three benchmark datasets. The proposed scheme outperforms competing methods in almost all the evaluations.","Fine-grained classification, Reconstruction-classification network, Supervised convolutional autoencoder, Retail product detection",Bikash Santra and Avishek Kumar Shaw and Dipti Prasad Mukherjee,https://www.sciencedirect.com/science/article/pii/S0031320321004374,https://doi.org/10.1016/j.patcog.2021.108257,0031-3203,2022,108257,121,Pattern Recognition,Part-based annotation-free fine-grained classification of images of retail products,article,SANTRA2022108257,
"Video anomaly detection aims to detect abnormal segments in a video sequence, which is a key problem in video surveillance. Based on deep prediction methods, we propose a spatiotemporal consistency-enhanced network to generate spatiotemporal consistency predictions. A 3D CNN-based encoder and 2D CNN-based decoder constitute the main part of our model. A resampling strategy is applied to the latent space vector when the model is trained by the normal data, yet this can cause the model to perform poorly if the data include abnormal data. Moreover, we combine an input clip with a generated frame into a reformed video clip, which is then fed into a discriminator that is constructed by the 3D CNN to evaluate the consistency of the input clip. Owing to the adversarial training between the generator and discriminator, the spatiotemporal consistency of the generated results is enhanced. During the testing stage, the abnormal data generates a different appearance and motion changes, which affect the ability of our model to predict spatiotemporal consistency in future images. Then, the prediction quality gap between normal and anomalous contents is used to infer whether anomalies occur. Extensive experiments confirm that the proposed method achieves state-of-the-art performance on three benchmark datasets, including ShanghaiTech, CUHK Avenue, and UCSD Ped2.","Anomaly detection, Unsupervised learning, Spatiotemporal consistency",Yi Hao and Jie Li and Nannan Wang and Xiaoyu Wang and Xinbo Gao,https://www.sciencedirect.com/science/article/pii/S0031320321004131,https://doi.org/10.1016/j.patcog.2021.108232,0031-3203,2022,108232,121,Pattern Recognition,Spatiotemporal consistency-enhanced network for video anomaly detection,article,HAO2022108232,
"Some multi-task trackers adopt an inaccurate shrink strategy to treat different rank components equally. Thus, their flexibility is vulnerable to some tracking challenges. To resolve this problem, we propose a spatial-aware reliable multi-subtask tracker via weighted Schatten p-norm regularization (SLRT-W), which dynamically chooses the suitable and reliable subset of the whole subtasks for tracking. Its major merits not only assign the flexible weights to different subtask rank components depending on their tracking contribution, but also preserve consistent spatial layout structure and correspondence of layered multi-subtask. Specifically, multiple layered subtasks correspond to different target subregions, they are cooperative and complement. A weighted Schatten p-norm is introduced to adaptively shrink different multi-subtask rank components, and emphasize important components as reliable ones. Then, a structured hyper-graph regularized term simultaneously exploits the intrinsic geometry correspondence among multiple layers of subtasks, and spatial layout structure inside each layer. We devise an alternatively generalized iterated shrinkage method to optimize the multi-subtask Schatten p-norm minimization. Finally, a robust decision-evaluation strategy is developed to choose the reliable multi-subtask tracking combination. Encouraging results on some challenging benchmarks demonstrate the proposed tracker performs favorably in robustness and accuracy, against some state-of-the-art trackers.","Reliable multi-subtask tracking, Weighted schatten -norm, Hyper-graph regularization, Decision-evaluation strategy",Baojie Fan and Yang Cong and Jiandong Tian and Yandong Tang,https://www.sciencedirect.com/science/article/pii/S0031320321003162,https://doi.org/10.1016/j.patcog.2021.108129,0031-3203,2021,108129,120,Pattern Recognition,Dynamic and reliable subtask tracker with general schatten p-norm regularization,article,FAN2021108129,
"Spatial information is often used to enhance the robustness of traditional fuzzy c-means (FCM) clustering algorithms. Although some recently emerged improvements are remarkable, the computational complexity of these algorithms is high, which may lead to lack of practicability. To address this problem, an efficient variant named the fuzzy clustering algorithm with variable multi-pixel fitting spatial information (FCM-VMF) is presented. First, a fuzzy clustering algorithm with multi-pixel fitting spatial information (FCM-MF) is developed. Specifically, by dividing the input image into several filter windows, the spatial information of all pixels in each filter window can be obtained simultaneously by fitting the pixels in its corresponding neighbourhood window, which enormously reduces the computational complexity. However, the FCM-MF may result in the loss of edge information. Therefore, the FCM-VMF integrates a variable window strategy with FCM-MF. In this strategy, to preserve more edge information, the sizes of the filter window and generalized neighbourhood window are adaptively reduced. The experimental results show that FCM-VMF is as effective as some recent algorithms. Notably, the FCM-VMF has extremely high efficiency, which means it has a better prospect of application.","Fuzzy clustering, Image segmentation, Spatial information, Variable filter window, Variable generalized neighbourhood window",Hang Zhang and Haili Li and Ning Chen and Shengfeng Chen and Jian Liu,https://www.sciencedirect.com/science/article/pii/S0031320321003836,https://doi.org/10.1016/j.patcog.2021.108201,0031-3203,2022,108201,121,Pattern Recognition,Novel fuzzy clustering algorithm with variable multi-pixel fitting spatial information for image segmentation,article,ZHANG2022108201,
"Time series forecasting involves collecting and analyzing past observations to develop a model to extrapolate such observations into the future. Forecasting of future events is important in many fields to support decision making as it contributes to reducing the future uncertainty. We propose explainable boosted linear regression (EBLR) algorithm for time series forecasting, which is an iterative method that starts with a base model, and explains the modelâs errors through regression trees. At each iteration, the path leading to highest error is added as a new variable to the base model. In this regard, our approach can be considered as an improvement over general time series models since it enables incorporating nonlinear features by residual explanation. More importantly, use of the single rule that contributes to the error most enables access to interpretable results. The proposed approach extends to probabilistic forecasting through generating prediction intervals based on the empirical error distribution. We conduct a detailed numerical study with EBLR and compare against various other approaches. We observe that EBLR substantially improves the base model performance through extracted features, and provide a comparable performance to other well established approaches. The interpretability of the model predictions and high predictive accuracy of EBLR makes it a promising method for time series forecasting.","Time series regression, Probabilistic forecasting, Decision trees, Linear regression, ARIMA",Igor Ilic and Berk GÃ¶rgÃ¼lÃ¼ and Mucahit Cevik and Mustafa GÃ¶kÃ§e BaydoÄan,https://www.sciencedirect.com/science/article/pii/S0031320321003319,https://doi.org/10.1016/j.patcog.2021.108144,0031-3203,2021,108144,120,Pattern Recognition,Explainable boosted linear regression for time series forecasting,article,ILIC2021108144,
"Recently, graph convolutional networks (GCNs) have been employed for graph matching problem. It can integrate graph node feature embedding, node-wise affinity learning and matching optimization together in a unified end-to-end model. However, first, the matching graphs feeding to existing graph matching networks are generally fixed and independent of graph matching task, which thus are not guaranteed to be optimal for the graph matching task. Second, existing methods generally employ smoothing-based graph convolution to generate graph node embeddings, in which extensive smoothing convolution operation may dilute the desired discriminatory information of graph nodes. To overcome these issues, we propose a novel Graph Learning-Matching Network (GLMNet) for graph matching problem. GLMNet has three main aspects. (1) It integrates graph learning into graph matching which thus adaptively learns a pair of optimal graphs for graph matching task. (2) It further employs a Laplacian sharpening graph convolution to generate more discriminative node embeddings for graph matching. (3) A new constraint regularized loss is designed for GLMNet training which can encode the desired one-to-one matching constraints in matching optimization. Experiments demonstrate the effectiveness of GLMNet.","Graph matching, Graph learning, Graph convolutional network, Laplacian sharpening",Bo Jiang and Pengfei Sun and Bin Luo,https://www.sciencedirect.com/science/article/pii/S003132032100354X,https://doi.org/10.1016/j.patcog.2021.108167,0031-3203,2022,108167,121,Pattern Recognition,GLMNet: Graph learning-matching convolutional networks for feature matching,article,JIANG2022108167,
"Learning representation carrying rich local information is essential for recognizing fine-grained objects. Existing methods to this task resort to multi-stage frameworks to capture fine-grained information. However, they usually require multiple forward passes of the backbone network, resulting in efficiency deterioration. In this paper, we propose Sequentially Diversified Networks (SDNs) that enrich representation by promoting their diversity while maintaining the extraction efficiency. Specifically, we construct multiple lightweight sub-networks to model mutually different scales of discriminative patterns. The design of these sub-networks follows the sequentially diversified constraint, encouraging them to be varied in spatial attention. By inserting these sub-networks into a single backbone network, SDNs enable information interaction among local regions of the fine-grained image. In this way, SDNs jointly promote diversity in terms of scale and spatial attention in the one-stage pipeline, thereby facilitating the learning of diversified representation efficiently. We evaluate our proposed method on three challenging datasets, namely CUB-200-2011, Stanford-Cars, and FGVC-Aircraft. Experiments demonstrate its effectiveness in learning diversified information. Moreover, our method achieves state-of-the-art performance, only requiring a single forward pass of the backbone network, which reduces inference time noticeably.","Fine-grained visual categorization, Convolutional neural networks, Diversity learning, Object recognition",Lianbo Zhang and Shaoli Huang and Wei Liu,https://www.sciencedirect.com/science/article/pii/S0031320321004003,https://doi.org/10.1016/j.patcog.2021.108219,0031-3203,2022,108219,121,Pattern Recognition,Learning sequentially diversified representations for fine-grained categorization,article,ZHANG2022108219,
"The current ML approaches do not fully focus to answer a still unresolved and topical challenge, namely the prediction of priorities of COVID-19 vaccine administration. Thus, our task includes some additional methodological challenges mainly related to avoiding unwanted bias while handling categorical and ordinal data with a highly imbalanced nature. Hence, the main contribution of this study is to propose a machine learning algorithm, namely Hierarchical Priority Classification eXtreme Gradient Boosting for priority classification for COVID-19 vaccine administration using the Italian Federation of General Practitioners dataset that contains Electronic Health Record data of 17k patients. We measured the effectiveness of the proposed methodology for classifying all the priority classes while demonstrating a significant improvement with respect to the state of the art. The proposed ML approach, which is integrated into a clinical decision support system, is currently supporting General Pracitioners in assigning COVID-19 vaccine administration priorities to their assistants.","COVID-19, Vaccination, Machine learning, XGBoost, Clinical decision support system, Model interpretability",Luca Romeo and Emanuele Frontoni,https://www.sciencedirect.com/science/article/pii/S0031320321003794,https://doi.org/10.1016/j.patcog.2021.108197,0031-3203,2022,108197,121,Pattern Recognition,A Unified Hierarchical XGBoost model for classifying priorities for COVID-19 vaccination campaign,article,ROMEO2022108197,
"One of the successful approaches in semi-supervised learning is based on the consistency regularization. Typically, a student model is trained to be consistent with teacher prediction for the inputs under different perturbations. To be successful, the prediction targets given by teacher should have good quality, otherwise the student can be misled by teacher. Unfortunately, existing methods do not assess the quality of the teacher targets. In this paper, we propose a novel Certainty-driven Consistency Loss (CCL) that exploits the predictive uncertainty in the consistency loss to let the student dynamically learn from reliable targets. Specifically, we propose two approaches, i.e. Filtering CCL and Temperature CCL to either filter out uncertain predictions or pay less attention on them in the consistency regularization. We further introduce a novel decoupled framework to encourage model difference. Experimental results on SVHN, CIFAR-10, and CIFAR-100 demonstrate the advantages of our method over a few existing methods.","Semi-supervised learning, Certainty-driven consistency loss, Uncertainty estimation, Decoupled student-teacher, Reliable targets, Noisy labels",Lu Liu and Robby T. Tan,https://www.sciencedirect.com/science/article/pii/S0031320321003277,https://doi.org/10.1016/j.patcog.2021.108140,0031-3203,2021,108140,120,Pattern Recognition,Certainty driven consistency loss on multi-teacher networks for semi-supervised learning,article,LIU2021108140,
"In this paper, a progressive global perception and local polishing (PCPLP) network is proposed to automatically segment the COVID-19-caused pneumonia infections in computed tomography (CT) images. The proposed PCPLP follows an encoder-decoder architecture. Particularly, the encoder is implemented as a computationally efficient fully convolutional network (FCN). In this study, a multi-scale multi-level feature recursive aggregation (mmFRA) network is used to integrate multi-scale features (viz. global guidance features and local refinement features) with multi-level features (viz. high-level semantic features, middle-level comprehensive features, and low-level detailed features). Because of this innovative aggregation of features, an edge-preserving segmentation map can be produced in a boundary-aware multiple supervision (BMS) way. Furthermore, both global perception and local perception are devised. On the one hand, a global perception module (GPM) providing a holistic estimation of potential lung infection regions is employed to capture more complementary coarse-structure information from different pyramid levels by enlarging the receptive fields without substantially increasing the computational burden. On the other hand, a local polishing module (LPM), which provides a fine prediction of the segmentation regions, is applied to explicitly heighten the fine-detail information and reduce the dilution effect of boundary knowledge. Comprehensive experimental evaluations demonstrate the effectiveness of the proposed PCPLP in boosting the learning ability to identify the lung infected regions with clear contours accurately. Our model is superior remarkably to the state-of-the-art segmentation models both quantitatively and qualitatively on a real CT dataset of COVID-19.","Coronavirus disease 2019 (COVID-19), Global perception, Local polishing, Feature recursive aggregation, Multiple supervision",Nan Mu and Hongyu Wang and Yu Zhang and Jingfeng Jiang and Jinshan Tang,https://www.sciencedirect.com/science/article/pii/S0031320321003551,https://doi.org/10.1016/j.patcog.2021.108168,0031-3203,2021,108168,120,Pattern Recognition,Progressive global perception and local polishing network for lung infection segmentation of COVID-19 CT images,article,MU2021108168,
"This paper develops Bayesian Compression for Dynamically Expandable Network (BCDEN), which can learn a compact model structure with preserving the accuracy in a continual learning scenarios. Dynamically Expandable Network (DEN) is efficiently trained by performing selective retraining, dynamically expands network capacity with only the necessary number of units, and effectively prevents semantic drift by duplicating and timestamping units in an online manner. Overcoming conventional DEN only giving point estimates, we providing the Bayesian inference under the principle framework. We validate our BCDEN on multiple public datasets under continual learning setting, on which it can outperform existing continual learning methods on a variety of tasks, and with the state-of-the-art compression results, while still maintaining comparable performance.","Bayesian compression, DEN, Continual learning, Selective retraining, Dynamically expands network, Semantic drift",Yang Yang and Bo Chen and Hongwei Liu,https://www.sciencedirect.com/science/article/pii/S0031320321004404,https://doi.org/10.1016/j.patcog.2021.108260,0031-3203,2022,108260,122,Pattern Recognition,Bayesian compression for dynamically expandable networks,article,YANG2022108260,
"Visual question answering (VQA) is a well-known problem in computer vision. Recently, Text-based VQA tasks are getting more and more attention because text information is very important for image understanding. The key to this task is to make good use of text information in the image. In this work, we propose an attention-based encoder-decoder network that combines the multimodal information of visual, linguistic, and location features together. By using the attention mechanism to focus on key features to the question, our multimodal feature fusion can provide more accurate information to improve the performance. Furthermore, we present a decoder with attention map loss, which can not only predict complex answers but also deal with a dynamic vocabulary to reduce the decoding space. Compared with softmax-based cross entropy loss which can only handle a fixed-length vocabulary, the attention map loss significantly improves the accuracy and efficiency. Our method achieved the first place of all three tasks in the ICDAR2019 robust reading challenge on scene text visual question answering (ST-VQA).","Dynamic vocabulary, Attention map, Multimodal fusion, ST-VQA",Jiajia Wu and Jun Du and Fengren Wang and Chen Yang and Xinzhe Jiang and Jinshui Hu and Bing Yin and Jianshu Zhang and Lirong Dai,https://www.sciencedirect.com/science/article/pii/S0031320321003952,https://doi.org/10.1016/j.patcog.2021.108214,0031-3203,2022,108214,122,Pattern Recognition,A multimodal attention fusion network with a dynamic vocabulary for TextVQA,article,WU2022108214,
"Dimensionality reduction (DR) is a key preprocessing stage in high-dimensional data classification. Traditional linear DR algorithms, e.g., Linear Discriminant Analysis, transform the original data into a low-dimensional subspace with a linear transformation matrix. However, these methods cannot handle complex nonlinearly separable data. Although some nonlinear DR methods, e.g., Locally Linear Embedding, are proposed to solve this problem, most of them are unsupervised, which only focus on the data structure hidden in the original high-dimensional space, rather than maximizing the inter-class separability of the transformed data, thus reducing the classification accuracy. To tackle this challenge, a novel supervised nonlinear DR algorithm, distance metric restricted mixture factor analysis (DMR-MFA), is proposed for high-dimensional data classification. In DMR-MFA, the original data is divided into several clusters, and the generation of original data in each cluster is described via a factor analysis model. Meanwhile, the distance metric constraint (DMC) is used for maximizing the separability of transformed low-dimensional data from different classes. Moreover, the optimal model parameters are learned via the joint optimization of log-likelihood function and DMC loss function, which makes the DMR-MFA possible to obtain the more separable low-dimensional embeddings while accurately describing the original data. Experimental results on synthetic data, benchmark datasets and high-resolution range profile data demonstrate that our method can handle nonlinearly separable data and improves the classification accuracy of data with high dimensionality.","Dimensionality reduction, Mixture factor analysis, Distance metric constraint, Classification",Jian Chen and Leiyao Liao and Wei Zhang and Lan Du,https://www.sciencedirect.com/science/article/pii/S0031320321003435,https://doi.org/10.1016/j.patcog.2021.108156,0031-3203,2022,108156,121,Pattern Recognition,Mixture factor analysis with distance metric constraint for dimensionality reduction,article,CHEN2022108156,
"In most applications, anomaly detection operates in an unsupervised mode by looking for outliers hoping that they are anomalies. Unfortunately, most anomaly detectors do not come with explanations about which features make a detected outlier point anomalous. Therefore, it requires human analysts to manually browse through each detected outlier pointâs feature space to obtain the subset of features that will help them determine whether they are genuinely anomalous or not. This paper introduces sequential explanation (SE) methods that sequentially explain to the analyst which features make the detected outlier anomalous. We present two methods for computing SEs called the outlier and sample-based SE that will work alongside any anomaly detector. The outlier-based SE methods use an anomaly detectorâs outlier scoring measure guided by a search algorithm to compute the SEs. Meanwhile, the sample-based SE methods employ sampling to turn the problem into a classical feature selection problem. In our experiments, we compare the performances of the different outlier- and sample-based SEs. Our results show that both the outlier and sample-based methods compute SEs that perform well and outperform sequential feature explanations.","Outlier explanation, Sequential feature explanation, Sequential explanation, Anomaly validation, Explainable AI",Tshepiso Mokoena and Turgay Celik and Vukosi Marivate,https://www.sciencedirect.com/science/article/pii/S0031320321004088,https://doi.org/10.1016/j.patcog.2021.108227,0031-3203,2022,108227,121,Pattern Recognition,Why is this an anomaly? Explaining anomalies using sequential explanations,article,MOKOENA2022108227,
"Head gesture videos recorded of a person bear rich information about the individual. Automatically understanding these videos can empower many useful human-centered applications in areas such as smart health, education, work safety and security. To understand a videoâs content, low-level head gesture signals carried in the video that capture characteristics of both human postures and motions need to be translated into high-level semantic labels. To meet this aim, we propose a hierarchical model for learning to understand head gesture videos. Given a head gesture video of an arbitrary length, the model first segments the full-length video into multiple short clips for clip-based feature extraction. Multiple base feature extraction procedures are then independently tuned via a set of peripheral learning tasks without consuming any labels of the goal task. These independently derived base features are subsequently aggregated through a multi-task learning framework, coupled with a feature dimensionality reduction module, to optimally learn to accomplish the end video understanding task in an weakly supervised manner, utilizing the limited amount of video labels available of the goal task. Experimental results show that the hierarchical model is superior to multiple state-of-the-art peer methods in tackling versatile video understanding tasks.","Video understanding, Head gesture videos, Stacked BLSTM, Multi-task learning, Transfer learning",Jiachen Li and Songhua Xu and Xueying Qin,https://www.sciencedirect.com/science/article/pii/S0031320321004362,https://doi.org/10.1016/j.patcog.2021.108256,0031-3203,2022,108256,121,Pattern Recognition,A hierarchical model for learning to understand head gesture videos,article,LI2022108256,
"This paper makes a major step towards addressing a long-standing challenge in cluster analysis, known as the userâs dilemma, which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this challenge relies on the identification of succinct, user-friendly properties that capture formal differences amongst clustering techniques. While helpful for gaining insight into the nature of clustering paradigms, there is a theory-practice gap that has so far limited the utility of this approach: Formal properties typically highlight advantages of classical linkage-based algorithms, while practical experience shows that center-based methods are preferable for many applications. We present simple new properties that delineate core differences between common clustering paradigms and overcome this theory-practice gap. The properties we present give a formal understanding of the advantages of center-based approaches for some applications and insight into when different clustering paradigms should be used. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight. To complement extensive formal analysis, we discuss how these properties can be applied in practice.","Clustering, Theory, Properties",Margareta Ackerman and Shai Ben-David and Simina BrÃ¢nzei and David Loker,https://www.sciencedirect.com/science/article/pii/S0031320321003393,https://doi.org/10.1016/j.patcog.2021.108152,0031-3203,2021,108152,120,Pattern Recognition,Weighted clustering: Towards solving the userâs dilemma,article,ACKERMAN2021108152,
"Head-mounted device-based human-computer interaction often requires egocentric recognition of hand gestures and fingertips detection. In this paper, a unified approach of egocentric hand gesture recognition and fingertip detection is introduced. The proposed algorithm uses a single convolutional neural network to predict the probabilities of finger class and positions of fingertips in one forward propagation. Instead of directly regressing the positions of fingertips from the fully connected layer, the ensemble of the position of fingertips is regressed from the fully convolutional network. Subsequently, the ensemble average is taken to regress the final position of fingertips. Since the whole pipeline uses a single network, it is significantly fast in computation. Experimental results show that the proposed method outperforms the existing fingertip detection approaches including the Direct Regression and the Heatmap-based framework. The effectiveness of the proposed method is also shown in-the-wild scenario as well as in a use-case of virtual reality.","Convolutional neural network, Fingertip detection, Gesture recognition, Human-computer interaction, Unified detection",Mohammad Mahmudul Alam and Mohammad Tariqul Islam and S.M. Mahbubur Rahman,https://www.sciencedirect.com/science/article/pii/S0031320321003824,https://doi.org/10.1016/j.patcog.2021.108200,0031-3203,2022,108200,121,Pattern Recognition,Unified learning approach for egocentric hand gesture recognition and fingertip detection,article,ALAM2022108200,
"Four-dimensional computed tomography (4D-CT) has been widely used in preoperative evaluation and radiotherapy planning of lung tumors. To reduce the damage to healthy tissue, it is a better way to limit the scan time and the number of CT slices. Yet, it leads to the reduction of CT image resolution in the superior-inferior direction. To improve the resolution of the 4D-CT image, we propose a super-resolution (SR) algorithm based on tensor product and nuclear norm optimization. The proposed cost function includes a tensor fidelity term and a nuclear norm regularization term. The tensor fidelity term consists of low-resolution (LR) and high-resolution (HR) image tensors, as well as SR operators. The nuclear norm regularization term is used to preserve the operatorsâ low-rank. The optimization problem can be effectively solved by an alternative direction method of the multipliers (ADMM) technique. The SR operators can extract useful information from each dimension of LR image tensors to enhance the equality of 4D-CT SR reconstruction. Experimental results show that the proposed method can preserve the edge details of the 4D-CT image. Moreover, quantitative comparisons show that the proposed method increases peak signal-to-noise ratio from 1.5 dB to 5.5 dB, structural similarity index from 2% to 11%, visual information fidelity from 6% to 20%, edge model-based blur metric from 5% to 15%, and decreases the spatial-spectral entropy-based quality index from 1% to 5%, compared with conventional 4D-CT SR algorithms.","4D-CT, Super-resolution, Tensor product, Optimization, Nuclear norm",Shu Zhang and Youshen Xia,https://www.sciencedirect.com/science/article/pii/S003132032100337X,https://doi.org/10.1016/j.patcog.2021.108150,0031-3203,2022,108150,121,Pattern Recognition,4D computed tomography super-resolution reconstruction based on tensor product and nuclear norm optimization,article,ZHANG2022108150,
"The year 2020 was characterized by the COVID-19 pandemic that has caused, by the end of March 2021, more than 2.5 million deaths worldwide. Since the beginning, besides the laboratory test, used as the gold standard, many applications have been applying deep learning algorithms to chest X-ray images to recognize COVID-19 infected patients. In this context, we found out that convolutional neural networks perform well on a single dataset but struggle to generalize to other data sources. To overcome this limitation, we propose a late fusion approach where we combine the outputs of several state-of-the-art CNNs, introducing a novel method that allows us to construct an optimum ensemble determining which and how many base learners should be aggregated. This choice is driven by a two-objective function that maximizes, on a validation set, the accuracy and the diversity of the ensemble itself. A wide set of experiments on several publicly available datasets, accounting for more than 92,000 images, shows that the proposed approach provides average recognition rates up to 93.54% when tested on external datasets.","COVID-19, X-ray, Deep-learning, Multi-expert systems, Optimization, Convolutional neural networks",Valerio Guarrasi and Natascha Claudia DâAmico and Rosa Sicilia and Ermanno Cordelli and Paolo Soda,https://www.sciencedirect.com/science/article/pii/S0031320321004234,https://doi.org/10.1016/j.patcog.2021.108242,0031-3203,2022,108242,121,Pattern Recognition,Pareto optimization of deep networks for COVID-19 diagnosis from chest X-rays,article,GUARRASI2022108242,
"Multi-view subspace clustering algorithms have recently been developed to process multi-view dataset clustering by accurately depicting the essential characteristics of multi-view data. Most existing methods focus on conduct self-representation property using a consistent representation and a set of specific representations with well-designed regularization to learn the common and specific knowledge among different views. However, specific representations only contain the unique information of each individual view, which limits their ability to fully excavate the diversity of multi-view data to enhance the complementarity among different views. Moreover, when conducting multi-view subspace clustering, the learned subspace self-representation and clustering are sequential and independent, which lacks consideration of the interaction between representation learning and the final clustering calculation. In this paper, a novel method termed consistent and diverse multi-view subspace clustering with structure constraint (CDMSC2) is proposed to overcome the above-described deficiencies. (1) An exclusivity constraint term is employed to enhance the diversity of specific representations among different views for modeling consistency and diversity in a unified framework. (2) A clustering structure constraint is imposed on the subspace self-representation by factorizing the learned subspace self-representation into the cluster centroids and the cluster assignments with the goal of obtaining a clustering-oriented subspace self-representation. In addition, we carefully designed an efficient optimization algorithm to solve the objective function through relaxation and alternating minimization. Extensive experiments on five benchmark datasets in terms of six evaluation metrics demonstrate that our method outperforms the state-of-the-art methods.","Subspace self-representation, Multi-view clustering, Consistency, Diversity, Clustering structure",Xiaomeng Si and Qiyue Yin and Xiaojie Zhao and Li Yao,https://www.sciencedirect.com/science/article/pii/S0031320321003782,https://doi.org/10.1016/j.patcog.2021.108196,0031-3203,2022,108196,121,Pattern Recognition,Consistent and diverse multi-View subspace clustering with structure constraint,article,SI2022108196,
"The bimodal ultrasound, namely B-mode ultrasound (BUS) and elastography ultrasound (EUS), provide complementary information to improve the diagnostic accuracy of breast cancers. However, in clinical practice, it is easier to acquire the labeled BUS images than the paired bimodal ultrasound data with shared labels due to the lack of EUS devices, especially in many rural hospitals. Thus, the single-modal BUS-based computer-aided diagnosis (CAD) generally has wide applications. Transfer learning (TL) can promote a BUS-based CAD model by transferring additional knowledge from EUS modality. To make full use of labeled paired bimodal data and the additional single-modal BUS images for knowledge transfer, a novel doubly supervised parameter transfer classifier (DSPTC) is proposed to well handle the TL between imbalanced modalities with the guidance of label information. Specifically, the proposed DSPTC consists of two loss functions corresponding to the paired bimodal ultrasound data with shared labels and the unpaired images with different labels, respectively. The former uses the loss function in the specially designed TL paradigm of support vector machine plus, while the latter adopts the Hilbert-Schmidt Independence Criterion (HSIC) for knowledge transfer between the unpaired images, which consist of the single-modal BUS images and the EUS images from the paired bimodal data. Consequently, the doubly supervised knowledge transfer is implemented by way of parameter transfer in a unified optimization framework. Two experiments are designed to evaluate the proposed DSPTC for the ultrasound-based diagnosis of breast cancers. The experimental results indicate that DSPTC outperforms all the compared algorithms, suggesting its wide potential applications.","Doubly supervised parameter transfer classifier, Support vector machine plus, Hilbert-Schmidt independence criterion, B-mode ultrasound, Breast cancer",Xiaoyan Fei and Shichong Zhou and Xiangmin Han and Jun Wang and Shihui Ying and Cai Chang and Weijun Zhou and Jun Shi,https://www.sciencedirect.com/science/article/pii/S0031320321003265,https://doi.org/10.1016/j.patcog.2021.108139,0031-3203,2021,108139,120,Pattern Recognition,Doubly supervised parameter transfer classifier for diagnosis of breast cancer with imbalanced ultrasound imaging modalities,article,FEI2021108139,
"Node clustering aims to partition the vertices in a graph into multiple groups or communities. Existing studies have mostly focused on developing deep learning approaches to learn a latent representation of nodes, based on which simple clustering methods like k-means are applied. These two-step frameworks for node clustering are difficult to manipulate and usually lead to suboptimal performance, mainly because the graph embedding is not goal-directed, i.e., designed for the specific clustering task. In this paper, we propose a clustering-directed deep learning approach, Deep Neighbor-aware Embedded Node Clustering (DNENC for short) for clustering graph data. Our method focuses on attributed graphs to sufficiently explore the two sides of information in graphs. It encodes the topological structure and node content in a graph into a compact representation via a neighbor-aware graph autoencoder, which progressively absorbs information from neighbors via a convolutional or attentional encoder. Multiple neighbor-aware encoders are stacked to build a deep architecture followed by an inner-product decoder for reconstructing the graph structure. Furthermore, soft labels are generated to supervise a self-training process, which iteratively refines the node clustering results. The self-training process is jointly learned and optimized with the graph embedding in a unified framework, to benefit both components mutually. Experimental results compared with state-of-the-art algorithms demonstrate the good performance of our framework.","Attributed graph, Node clustering, Graph attention network, Graph convolutional network, Network representation",Chun Wang and Shirui Pan and Celina P. Yu and Ruiqi Hu and Guodong Long and Chengqi Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321004118,https://doi.org/10.1016/j.patcog.2021.108230,0031-3203,2022,108230,122,Pattern Recognition,Deep neighbor-aware embedding for node clustering in attributed graphs,article,WANG2022108230,
"Anomaly detection in videos remains a challenging task due to the ambiguous definition of anomaly and the complexity of visual scenes from real video data. Different from the previous work which utilizes reconstruction or prediction as an auxiliary task to learn the temporal regularity, in this work, we explore a novel convolution autoencoder architecture that can dissociate the spatio-temporal representation to separately capture the spatial and the temporal information, since abnormal events are usually different from the normality in appearance and/or motion behavior. Specifically, the spatial autoencoder models the normality on the appearance feature space by learning to reconstruct the input of the first individual frame (FIF), while the temporal part takes the first four consecutive frames as the input and the RGB difference as the output to simulate the motion of optical flow in an efficient way. The abnormal events, which are irregular in appearance or in motion behavior, lead to a large reconstruction error. To improve detection performance on fast moving outliers, we exploit a variance-based attention module and insert it into the motion autoencoder to highlight large movement areas. In addition, we propose a deep K-means cluster strategy to force the spatial and the motion encoder to extract a compact representation. Extensive experiments on some publicly available datasets have demonstrated the effectiveness of our method which achieves the state-of-the-art performance. The code is publicly released at the link11https://github.com/ChangYunPeng/VideoAnomalyDetection.","Video anomaly detection, Spatio-temporal dissociation, Simulate motion of optical flow, Deep K-means cluster",Yunpeng Chang and Zhigang Tu and Wei Xie and Bin Luo and Shifu Zhang and Haigang Sui and Junsong Yuan,https://www.sciencedirect.com/science/article/pii/S0031320321003940,https://doi.org/10.1016/j.patcog.2021.108213,0031-3203,2022,108213,122,Pattern Recognition,Video anomaly detection with spatio-temporal dissociation,article,CHANG2022108213,
"Recently, part-based deep models have achieved promising performance in person re-identification (Re-ID), yet these models ignore the inter-local relationship of the corresponding parts among pedestrian images and the intra-local relationship between adjacent parts in one pedestrian image. As a result, the feature representations are hard to learn the information from the same parts of other pedestrian images and are lack of the contextual information of pedestrian. In this paper, we propose a novel deep graph model named Part-Guided Graph Convolution Network (PGCN) for person Re-ID, which could simultaneously learn the inter-local relationship and the intra-local relationship for feature representations. Specifically, we construct the inter-local graph using the local features extracted from the same parts of pedestrian images and build the adjacency matrix using the similarity so as to mine the inter-local relationship. Meanwhile, we construct the intra-local graph using the local features extracted from different body parts in one pedestrian image, and propose the fractional dynamic mechanism (FDM) to accurately describe the correlations between adjacent parts in the optimization process. Finally, after the graph convolutional operation, the inter-local relationship and the intra-local relationship are injected into the feature representations of pedestrian images. Extensive experiments are conducted on Market-1501, CUHK03, DukeMTMC-reID and MSMT17, and the experimental results show the proposed PGCN exceeds state-of-the-art methods by an overwhelming margin.","Person re-identification, Graph convolution network",Zhong Zhang and Haijia Zhang and Shuang Liu and Yuan Xie and Tariq S. Durrani,https://www.sciencedirect.com/science/article/pii/S0031320321003423,https://doi.org/10.1016/j.patcog.2021.108155,0031-3203,2021,108155,120,Pattern Recognition,Part-guided graph convolution networks for person re-identification,article,ZHANG2021108155,
"Deep neural network models owe their representational power and high performance in classification tasks to the high number of learnable parameters. Running deep neural network models in limited-resource environments is a problematic task. Models employing conditional computing aim to reduce the computational burden while retaining model performance on par with more complex neural network models. This paper, proposes a new model, Conditional Information Gain Networks as Sparse Mixture of Experts (sMoE-CIGNs). A CIGN model is a neural tree that allows conditionally skipping parts of the tree based on routing mechanisms inserted into the architecture. These routing mechanisms are based on differentiable Information Gain objectives. CIGN groups semantically similar samples in the leaves, enabling simpler classifiers to focus on differentiating between similar classes. This lets the CIGN model attain high classification performances with lighter models. We further improve the basic CIGN model by proposing a sparse mixture of experts model for difficult to classify samples that may get routed to suboptimal branches. If a sample has routing confidence higher than a specific threshold, the sample may be routed towards multiple child nodes. The classification decision can then be taken as a mixture of these expert decisions. We learn the optimal routing thresholds by Bayesian Optimization over a validation set by minimizing a weighted loss, including the classification accuracy and the number of multiplication and accumulations (MAC). We show the effectiveness of the CIGN models enhanced with the Sparse Mixture of Experts approach with extensive tests on MNIST, Fashion MNIST, CIFAR 100 and UCI-USPS datasets, as well as comparisons with methods from the literature. sMoE-CIGN models can retain high generalization performance, on par with a thick unconditional model while keeping the operation burden at the same level with a much thinner model.11This article is an extended version of reference âBicici U.C., Keskin C., Akarun L., Conditional information gain networksâ, which appeared in Proceedings of the 24th International Conference on Pattern Recognition (ICPR) (2018).","Machine learning, Deep learning, Conditional deep learning",Ufuk Can Bicici and Lale Akarun,https://www.sciencedirect.com/science/article/pii/S0031320321003381,https://doi.org/10.1016/j.patcog.2021.108151,0031-3203,2021,108151,120,Pattern Recognition,Conditional information gain networks as sparse mixture of experts,article,BICICI2021108151,
"Unsupervised domain adaptation (DA) enables intelligent models to learn transferable knowledge from a labeled source domain and adapt to a similar but unlabeled target domain. Studies showed that knowledge could be transferred from one source domain to another unknown target domain, called Universal DA (UDA). However, there is often more than one source domain in the real-world application to be exploited for DA. In this paper, we formally propose a more general domain adaptation setting for image classification, universal multi-source DA (UMDA), where the label sets of multiple source domains can be different, and the label set of the target domain is completely unknown. The main challenge in UMDA is to identify the common label set among each source and target domain and keep the model scalable as the number of source domains increases. In the face of this challenge, we propose a universal multi-source adaptation network (UMAN) to solve the DA problem without increasing the complexity of the model in various UMDA settings. In UMAN, the reliability of each known class belonging to the common label set is estimated via a novel pseudo-margin vector and its weighted form, which helps adversarial training better align the distributions of multiple source domains and target domain. Moreover, the theoretical guarantee for UMAN is also provided. Massive experimental results show that existing UDA and multi-source DA (MDA) methods cannot be directly deployed to UMDA, and the proposed UMAN achieves the state-of-the-art performance in various UMDA settings.","Universal domain adaptation, Multi-source domain adaptation, Universal multi-source domain adaptation, Universal multi-source adaptation network, Pseudo-margin vector",Yueming Yin and Zhen Yang and Haifeng Hu and Xiaofu Wu,https://www.sciencedirect.com/science/article/pii/S0031320321004192,https://doi.org/10.1016/j.patcog.2021.108238,0031-3203,2022,108238,121,Pattern Recognition,Universal multi-Source domain adaptation for image classification,article,YIN2022108238,
"Deep learning techniques have recently achieved impressive results and raised expectations in the domains of medical diagnosis and physiological signal processing. The widely adopted methods include convolutional neural networks (CNNs) and recurrent neural networks (RNNs). However, the existing models possess static connection weights between layers, which might limit the generalization capability and the classification performance of the models as the weights of different layers are fixed after training. Furthermore, to deal with a large amount of data, a neural network with a sufficiently large size is required. This paper proposes the variable weight convolutional neural networks (VWCNNs), which are a type of network structure employing dynamic weights instead of static weights in their convolutional layers and fully-connected layers. VWCNNs are able to adapt to different characteristics of input data and can be viewed as an infinite number of traditional, fixed-weight CNNs. We will show that the proposed VWCNN structure outperforms the conventional CNN in terms of the classification accuracy, generalization capability, and robustness when the inputs are contaminated by noise. In this paper, VWCNNs are applied to the classification of three seizure phases (seizure-free, pre-seizure and seizure) based on measured electroencephalography (EEG) data. VWCNNs achieve 100% test accuracy and show strong robustness in the classification of the three seizure phases, and thus show the potential to be a useful classification tool for medical diagnosis. Furthermore, the classification of seven types of seizures is investigated in this paper using the worldâs largest open source database of seizure recordings, TUH EEG seizure corpus. Comparisons with conventional CNNs, RNN, MobileNet, ResNet, DenseNet and traditional machine learning methods including random forest, decision tree, support vector machine, K-nearest neighbours, standard neural networks, and NaÃ¯ve Bayes are being conducted using realistic test data sets. The results demonstrate that VWCNNs have advantages over other classifiers in terms of classification accuracy and robustness.","Variable weight convolutional neural networks, Machine learning, Seizure phase classification, Seizure type classification",Guangyu Jia and Hak-Keung Lam and Kaspar Althoefer,https://www.sciencedirect.com/science/article/pii/S0031320321004076,https://doi.org/10.1016/j.patcog.2021.108226,0031-3203,2022,108226,121,Pattern Recognition,Variable weight algorithm for convolutional neural networks and its applications to classification of seizure phases and types,article,JIA2022108226,
"In this paper, a knowledge base graph embedding module is constructed to extend the versatility of knowledge-based VQA (Visual Question Answering) models. The knowledge base graph embedding module constructed in this paper extracts core entities from images and text, and maps them as knowledge base entities, then extracts the sub-graphs closely related to the core entities, and converts the sub-graphs into low-dimensional vectors to realize sub-graph embedding. In order to achieve good subgraph embedding, we first extracted two experimental knowledge bases with rich semantics from DBpedia: DBV and DBA. Based on these two knowledge bases, this paper selects several excellent models in knowledge base embedding as test models, including SE (structured embedding),SME(semantic matching energy function), and TransE model to produce link prediction. The results show that there is a clear correspondence between the entities of the DBV, which can achieve excellent node embedding. And the TransE model can achieve a good knowledge base embedding, so we built the knowledge base graph embedding module based on TransE. And then we construct a VQA model (KBSN) based on the knowledge base graph embedding. Experimental results on VQA2.0 and KB-VQA data sets prove that the knowledge base graph embedding module improves the accuracy.","Faster R-CNN, DBpedia spotlight, knowledge base, VQA",Wenfeng Zheng and Lirong Yin and Xiaobing Chen and Zhiyang Ma and Shan Liu and Bo Yang,https://www.sciencedirect.com/science/article/pii/S003132032100340X,https://doi.org/10.1016/j.patcog.2021.108153,0031-3203,2021,108153,120,Pattern Recognition,Knowledge base graph embedding module design for Visual question answering model,article,ZHENG2021108153,
"State-of-the-art object detection approaches are often composed of two stages, namely, proposing a number of regions on an image and classifying each of them into one class. Both stages share a network backbone which builds visual features in a bottom-up manner. In this paper, we advocate the importance of equipping two-stage detectors with top-down signals, in order to which provides high-level contextual cues to complement low-level features. In practice, this is implemented by adding a side path in the detection head to predict all object classes in the image, which is co-supervised by image-level semantics and requires little extra overheads. Our approach is easily applied to two popular object detection algorithms, and achieves consistent performance gain in the MS-COCO dataset.","Object detection, Co-supervised, Contextual cues",Junran Peng and Haoquan Wang and Shaolong Yue and Zhaoxiang Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321003812,https://doi.org/10.1016/j.patcog.2021.108199,0031-3203,2022,108199,121,Pattern Recognition,Context-aware co-supervision for accurate object detection,article,PENG2022108199,
"Fully convolutional networks have shown outstanding performance in the salient object detection (SOD) field. The state-of-the-art (SOTA) methods have a tendency to become deeper and more complex, which easily homogenize their learned deep features, resulting in a clear performance bottleneck. In sharp contrast to the conventional âdeeperâ schemes, this paper proposes a âwiderâ network architecture which consists of parallel sub-networks with totally different network architectures. In this way, those deep features obtained via these two sub-networks will exhibit large diversity, which will have large potential to be able to complement with each other. However, a large diversity may easily lead to the feature conflictions, thus we use the dense short-connections to enable a recursively interaction between the parallel sub-networks, pursuing an optimal complementary status between multi-model deep features. Finally, all these complementary multi-model deep features will be selectively fused to make high-performance salient object detections. Extensive experiments on several famous benchmarks clearly demonstrate the superior performance, good generalization, and powerful learning ability of the proposed wider framework.","Salient object detection, Deep learning, Multi-model fusion",Zhenyu Wu and Shuai Li and Chenglizhao Chen and Aimin Hao and Hong Qin,https://www.sciencedirect.com/science/article/pii/S0031320321003939,https://doi.org/10.1016/j.patcog.2021.108212,0031-3203,2022,108212,121,Pattern Recognition,Recursive multi-model complementary deep fusion for robust salient object detection via parallel sub-networks,article,WU2022108212,
"Many embedded feature selection methods ignore the correlation among the important features. To reduce correlation, some models introduce constraints to impose sparsity on features, some try to exploit the similarity and group features without changing the objective function. In this paper, we propose diverse feature selection (DFS), which simultaneously performs feature clustering and selection. Given a dataset with known class labels, we separate the features into a set of feature clusters where the features in the same cluster have a higher correlation with each other than with the features in different clusters. A diverse regularization (DR) is proposed to reduce the linear and nonlinear correlations among important features. Using this regularization, DFS can select features that are both informative and diverse. The experimental results on seven image datasets, five gene datasets as well as four other datasets demonstrate the superior performance of DFS.","Feature selection, Supervised feature selection, Diverse feature, Regularization",Weichan Zhong and Xiaojun Chen and Qingyao Wu and Min Yang and Joshua Zhexue Huang,https://www.sciencedirect.com/science/article/pii/S0031320321003411,https://doi.org/10.1016/j.patcog.2021.108154,0031-3203,2021,108154,120,Pattern Recognition,Selection of diverse features with a diverse regularization,article,ZHONG2021108154,
"The paper presents numerical experiments and some theoretical developments in prediction with expert advice (PEA). One experiment deals with predicting electricity consumption depending on temperature and uses real data. As the pattern of dependence can change with season and time of the day, the domain naturally admits PEA formulation with experts having different âareas of expertiseâ. We consider the case where several competing methods produce online predictions in the form of probability distribution functions. The dissimilarity between a probability forecast and an outcome is measured by a loss function (scoring rule). A popular example of scoring rule for continuous outcomes is Continuous Ranked Probability Score (CRPS). In this paper the problem of combining probabilistic forecasts is considered in the PEA framework. We show that CRPS is a mixable loss function and then the time-independent upper bound for the regret of the Vovk aggregating algorithm using CRPS as a loss function can be obtained. Also, we incorporate a âsmoothâ version of the method of specialized experts in this scheme which allows us to combine the probabilistic predictions of the specialized experts with overlapping domains of their competence.","On-line learning, Prediction with expert advice, Aggregating algorithm, Probabilistic prediction, Continuous ranked probability score (), Smooth confidence levels for experts",Vladimir Vâyugin and Vladimir Trunov,https://www.sciencedirect.com/science/article/pii/S0031320321003496,https://doi.org/10.1016/j.patcog.2021.108193,0031-3203,2022,108193,121,Pattern Recognition,Online aggregation of probability forecasts with confidence,article,VYUGIN2022108193,
"With the rapid development of cloud computing, the importance of dynamic virtual machine scheduling is increasing. Existing works formulate the VM scheduling as a bin-packing problem and design greedy methods to solve it. However, cloud service providers widely adopt multi-NUMA architecture servers in recent years, and existing methods do not consider the architecture. This paper formulates the multi-NUMA VM scheduling into a novel structured combinatorial optimization and transforms it into a reinforcement learning problem. We propose a reinforcement learning algorithm called SchedRL with a delta reward scheme and an episodic guided sampling strategy to solve the problem efficiently. Evaluating on a public dataset of Azure under two different scenarios, our SchedRL outperforms FirstFit and BestFit on the fulfill number and allocation rate.","Dynamic virtual machine scheduling, Multi-NUMA, Reinforcement learning, Cloud computing",Junjie Sheng and Yiqiu Hu and Wenli Zhou and Lei Zhu and Bo Jin and Jun Wang and Xiangfeng Wang,https://www.sciencedirect.com/science/article/pii/S0031320321004349,https://doi.org/10.1016/j.patcog.2021.108254,0031-3203,2022,108254,121,Pattern Recognition,Learning to schedule multi-NUMA virtual machines via reinforcement learning,article,SHENG2022108254,
"Region-level representation learning plays a key role in providing discriminative information for person retrieval. Current methods rely on heuristically coarse-grained region strips or directly borrow pixel-level annotations from pretrained human parsing models for region representation learning. How to learn a discriminative region representation within fine-grained segments while avoiding expensive pixel-level annotations is rarely discussed. To that end, we introduce a novel identity-guided human region segmentation (HRS) method for person retrieval. Via learning a set of distinct region bases that are consistent across a given dataset, HRS can predict informative region segments by grouping intermediate feature vectors based on their similarity to these bases. The predicted segments are iteratively refined for discriminative region representation learning. HRS enjoys two advantages: (1) HRS learns region segmentation using only identity labels, making it a much more practical solution to person retrieval. (2) By jointly learning global appearance and local granularity cues, HRS enables a comprehensive feature representation learning. We verify the effectiveness of the proposed HRS on four challenging benchmark datasets of Market1501, DukeMTMC-reID, CUHK03, and Occluded-DukeMTMC. Extensive experiments demonstrate superior performance over the state-of-the-art region-based methods. For instance, on the CUHK03-labeled dataset, the performance increases from 74.1% mAP and 76.5% rank-1 accuracy to 81.5% (+7.4%) mAP and 83.2% (+6.7%) rank-1 accuracy.","Person retrieval, Region representation",Yang Zhao and Xiaohan Yu and Yongsheng Gao and Chunhua Shen,https://www.sciencedirect.com/science/article/pii/S0031320321004106,https://doi.org/10.1016/j.patcog.2021.108229,0031-3203,2022,108229,121,Pattern Recognition,Learning discriminative region representation for person retrieval,article,ZHAO2022108229,
"SAR images have the advantages of being less susceptible to clouds and light, while optical images conform to the human vision system. Both of them are widely applied in the field of scene classification, natural environment monitoring, disaster warning, etc. However, due to the speckle noise caused by the SAR imaging principle, it is difficult for people to distinguish the ground objects from complex background without professional knowledge. One commonly used solution is to exploit Generative Adversarial Networks (GAN) to translate SAR images to optical images which is able to clearly present ground objects with rich color information, i.e., SAR-to-optical image translation. Traditional GAN-based translation methods are apt to cause blurring of contour, disappearance of texture and inconsistency of color. To this end, we propose an improved conditional GAN (ICGAN) method. Compared with the basic CGAN model, the translation ability of our method is improved in the following three aspects. (1) Contour sharpness. We utilize the parallel branches to combine low-level and high-level features, and thus the image contour information is improved without the influence of noise. (2) Texture fine-grainedness. We discriminate the image using multi-scale receptive fields to enrich the local and global texture features of the image. (3) Color fidelity. We use the chromatic aberration loss which is based on Gaussian blur convolution to reduce the color gap between the generated image and the real optical image. Our method considers both the visual layer and the conceptual layer of the image to complete the SAR-to-optical image translation task. The model is able to preserve the contours and textures of the SAR image, while more closely approximates the colors of the ground truth. The experimental results show that the generated image not only has preferable results in visual effects and favorable evaluation metrics (subjective and objective), but also achieves outstanding classification accuracy, which proves the superiority of our method over the state-of-the-arts in the SAR-to-optical image translation task.","SAR-to-optical image translation, Chromatic aberration loss, ICGAN",Xi Yang and Jingyi Zhao and Ziyu Wei and Nannan Wang and Xinbo Gao,https://www.sciencedirect.com/science/article/pii/S0031320321003897,https://doi.org/10.1016/j.patcog.2021.108208,0031-3203,2022,108208,121,Pattern Recognition,SAR-to-optical image translation based on improved CGAN,article,YANG2022108208,
"Change detection (CD) is an important vision task for autonomous landing of unmanned aerial vehicles (UAV) on water. High-density photoreceptors and lateral inhibition mechanisms have inspired a novel biologic computational method based on structure and properties in eagle eyes as proposed for change detection. We call this method âSTabCD,â which ensures spatiotemporal distribution consistency to achieve foreground acquisition, noise reduction, and background adaptability. Therefore, our proposed model responds strongly to object information and suppresses noise and wave textures. Then, we present a cloning method to simulate water scenes and collect a new synthetic dataset (called âSynthetic Boat Sequenceâ) for UAV vision research. Besides, we utilize synthetic datasets and corresponding real datasets to conduct change detection experiments. The experimental results indicate that: 1) the STabCD model achieves the best results in real or synthetic water landing scenes; and 2) change detection models for UAV can be quantitatively analyzed and tested under challenging synthetic scenarios.","Change detection, Eagle eye, Synthetic boat sequence, Synthetic dataset, Unmanned aerial vehicle",Xuan Li and Haibin Duan and Jingchun Li and Yimin Deng and Fei-Yue Wang,https://www.sciencedirect.com/science/article/pii/S0031320321003654,https://doi.org/10.1016/j.patcog.2021.108203,0031-3203,2022,108203,122,Pattern Recognition,Biological eagle eye-based method for change detection in water scenes,article,LI2022108203,
"Twin support vector machine (TWSVM) is an efficient algorithm for binary classification. However, the lack of the structural risk minimization principle restrains the generalization of TWSVM and the guarantee of convex optimization constraints TWSVM to only use positive semi-definite kernels (PSD). In this paper, we propose a novel TWSVM for indefinite kernel called indefinite twin support vector machine with difference of convex functions programming (ITWSVM-DC). The indefinite TWSVM (ITWSVM) leverages a maximum margin regularization term to improve the generalization of TWSVM and a smooth quadratic hinge loss function to make the model continuously differentiable. The representer theorem is applied to the ITWSVM and the convexity of the ITWSVM is analyzed. In order to address the non-convex optimization problem when the kernel is indefinite, a difference of convex functions (DC) is used to decompose the non-convex objective function into the subtraction of two convex functions and a line search method is applied in the DC algorithm to accelerate the convergence rate. A theoretical analysis illustrates that ITWSVM-DC can converge to a local optimum and extensive experiments on indefinite and positive semi-definite kernels show the superiority of ITWSVM-DC.","SVM, TWSVM, Indefinite kernel, DC Programming, Structural risk minimization principle",Yuexuan An and Hui Xue,https://www.sciencedirect.com/science/article/pii/S0031320321003770,https://doi.org/10.1016/j.patcog.2021.108195,0031-3203,2022,108195,121,Pattern Recognition,Indefinite twin support vector machine with DC functions programming,article,AN2022108195,
"Static environment is a prerequisite for most of visual simultaneous localization and mapping systems. Such a strong assumption limits the practical application of most existing SLAM systems. When moving objects enter the cameraâs view field, dynamic matching points will directly interrupt the camera localization, and the noise blocks formed by moving objects will contaminate the constructed map. In this paper, a semantic SLAM system working in indoor dynamic environments named Blitz-SLAM is proposed. The noise blocks in the local point cloud are removed by combining the advantages of semantic and geometric information of mask, RGB and depth images. The global point cloud map can be obtained by merging the local point clouds. We evaluate Blitz-SLAM on the TUM RGB-D dataset and in the real-world environment. The experimental results demonstrate that Blitz-SLAM can work robustly in dynamic environments and generate a clean and accurate global point cloud map simultaneously.","Semantic SLAM, Dynamic environments, Noise block, Local point cloud, Global point cloud map",Yingchun Fan and Qichi Zhang and Yuliang Tang and Shaofen Liu and Hong Han,https://www.sciencedirect.com/science/article/pii/S0031320321004064,https://doi.org/10.1016/j.patcog.2021.108225,0031-3203,2022,108225,121,Pattern Recognition,Blitz-SLAM: A semantic SLAM in dynamic environments,article,FAN2022108225,
"Existing embedding zero-shot learning models usually learn a projection function from the visual feature space to the semantic embedding space, e.g. attribute space or word vector space. However, the projection learned based on seen samples may not generalize well to unseen classes, which is known as the projection domain shift problem in ZSL. To address this issue, we propose a method named Low-rank Semantic Autoencoder (LSA) to consider the low-rank structure of seen samples to maintain the sparse feature of reconstruction error, which can further improve zero-shot learning capability. Moreover, to obtain a more robust projection for unseen classes, we propose a Specific Rank-controlled Semantic Autoencoder (SRSA) to accurately control of the projectionâs rank. Extensive experiments on six benchmarks demonstrate the superiority of the proposed models over most existing embedding ZSL models under the standard zero-shot setting and the more realistic generalized zero-shot setting.","Zero-shot learning, Rank, Domain shift, Autoencoder",Yang Liu and Xinbo Gao and Jungong Han and Li Liu and Ling Shao,https://www.sciencedirect.com/science/article/pii/S0031320321004180,https://doi.org/10.1016/j.patcog.2021.108237,0031-3203,2022,108237,122,Pattern Recognition,Zero-shot learning via a specific rank-controlled semantic autoencoder,article,LIU2022108237,
"Explanation methods shed light on the decision process of black-box classifiers such as deep neural networks. But their usefulness can be compromised because they are susceptible to manipulations. With this work, we aim to enhance the resilience of explanations. We develop a unified theoretical framework for deriving bounds on the maximal manipulability of a model. Based on these theoretical insights, we present three different techniques to boost robustness against manipulation: training with weight decay, smoothing activation functions, and minimizing the Hessian of the network. Our experimental results confirm the effectiveness of these approaches.","Explanation method, Saliency map, Adversarial attacks, Manipulation, Neural networks,",Ann-Kathrin Dombrowski and Christopher J. Anders and Klaus-Robert MÃ¼ller and Pan Kessel,https://www.sciencedirect.com/science/article/pii/S0031320321003769,https://doi.org/10.1016/j.patcog.2021.108194,0031-3203,2022,108194,121,Pattern Recognition,Towards robust explanations for deep neural networks,article,DOMBROWSKI2022108194,
"With the outbreak and wide spread of novel coronavirus (COVID-19), contactless fingerprint recognition has attracted more attention for personal recognition because it can provide significantly higher user convenience and hygiene than the traditional contact-based fingerprint recognition. However, it is still challenging to achieve a highly accurate recognition due to the low ridge-valley contrast and pose variances of contactless fingerprints. Minutiae points are a kind of ridge flow discontinuities, and robust and accurate extraction is an important step for most automatic fingerprint recognition algorithms. Most of existing methods are based on two stages which locate the minutiae points first and then compute their directions. The two-stage method cannot make full use of location and direction information. In this paper, we propose a multi-task fully deep convolutional neural network for jointly learning the minutiae location detection and its corresponding direction computation which operates directly on the whole gray scale contactless fingerprints. The proposed method consists of offline training and online testing stages. In the training stage, a fully deep convolutional neural network is built for the tasks of minutiae detection and its direction regression, with an attention mechanism to make the direction regression branch concentrate on the minutiae points. A new loss function is proposed to jointly learn the tasks of minutiae detection and its direction regression from the whole fingerprints. In the testing stage, the trained network is applied on the whole contactless fingerprint to generate the minutiae location and direction maps. The proposed multi-task leaning method performs better than the individual single task and it operates directly on the raw gray-scale contactless fingerprints without preprocessing. The results on three contactless fingerprint datasets show the proposed algorithm performs better than other minutiae extraction algorithms and the commercial software.","Contactless fingerprint, Minutiae extraction, Deep convolutional neural network, Multi-task learning",Zhao Zhang and Shuxin Liu and Manhua Liu,https://www.sciencedirect.com/science/article/pii/S0031320321003526,https://doi.org/10.1016/j.patcog.2021.108189,0031-3203,2021,108189,120,Pattern Recognition,A multi-task fully deep convolutional neural network for contactless fingerprint minutiae extraction,article,ZHANG2021108189,
"Recently, deep neural networks have expanded the state-of-art in various scientific fields and provided solutions to long standing problems across multiple application domains. Nevertheless, they also suffer from weaknesses since their optimal performance depends on massive amounts of training data and the tuning of an extended number of parameters. As a countermeasure, some deep-forest methods have been recently proposed, as efficient and low-scale solutions. Despite that, these approaches simply employ label classification probabilities as induced features and primarily focus on traditional classification and regression tasks, leaving multi-output prediction under-explored. Moreover, recent work has demonstrated that tree-embeddings are highly representative, especially in structured output prediction. In this direction, we propose a novel deep tree-ensemble (DTE) model, where every layer enriches the original feature set with a representation learning component based on tree-embeddings. In this paper, we specifically focus on two structured output prediction tasks, namely multi-label classification and multi-target regression. We conducted experiments using multiple benchmark datasets and the obtained results confirm that our method provides superior results to state-of-the-art methods in both tasks.","Ensemble learning, Deep-forest, Multi-output prediction, Multi-target regression, Multi-label classification",Felipe Kenji Nakano and Konstantinos Pliakos and Celine Vens,https://www.sciencedirect.com/science/article/pii/S0031320321003927,https://doi.org/10.1016/j.patcog.2021.108211,0031-3203,2022,108211,121,Pattern Recognition,Deep tree-ensembles for multi-output prediction,article,NAKANO2022108211,
"Clustering objects with heterogeneous attributes captured from different dimensions remains challenging in integrating the multiple dimensional information. Most of the current multi-dimensional clustering models pin on direct sample-wised similarity and fail to exploit hidden mutual affinity among different sampling spaces. Thus, it is hard to capture a legible cluster structure. To tackle this issue, we propose a High-order multi-dimensional Spectral Clustering methodÂ (HSC). The proposed HSC aims to learn a high-order similarity to characterize the intrinsic relationship among different dimensional spaces instead of the ordinary similarity. It then performs a clustering task within a latent space by jointly learning the high-order similarity and ordinary similarity. Extensive experiments over synthetic and real-world data sets show that the proposed HSC outperforms benchmark multi-dimensional methods in most scenarios and is capable of revealing a reliable structure concealed across multi-dimensional spaces.","High-order similarity, Low-rank, Multi-dimensional clustering, Spectral clustering",Hong Peng and Haiyan Wang and Yu Hu and Weiwei Zhou and Hongmin Cai,https://www.sciencedirect.com/science/article/pii/S0031320321002958,https://doi.org/10.1016/j.patcog.2021.108108,0031-3203,2022,108108,121,Pattern Recognition,Multi-dimensional clustering through fusion of high-order similarities,article,PENG2022108108,
"Watershed clustering utilizes the concept of watershed algorithm to process clustering or cluster analyzes. The most attractive characteristic of this method is the capability to determine automatically the number of clusters from the data sets. However, in terms of the literature, the purposes of the original watershed clustering algorithm and the improved version are the detection of the clusters within two-dimensional linear data sets. In order to enable watershed clustering to deal with the dataset with multiple dimensions and nonlinear structures, we introduce k-nearest neighbor graph (KNNG), the shared nearest neighbor method and Pauta Criterion into watershed clustering to present a new watershed graph clustering with noise detection, WC-KNNG-PC. This approach first calculates a KNNG for the data sets, and then compute catchment basins (subclusters), basin immersions (connectivity between basins) and outliers. To prevent the merger of illegal subclusters, a maximum normalization stability factor, based on t-nearest neighbors and angle, MNSF, is proposed to detect the invalid basin immersions. Finally, a basin level similarity using median criterion is presented to merge the catchment basins to obtain the final clustering. Experiments on complex synthetic datasets and multidimensional real-world datasets have successfully demonstrated that the performance of the WC-KNNG-PC in clustering some various dimensional and complex datasets with heterogeneous density and diverse shapes.","Watershed clustering, -nearest neighbor graph (KNNG), Pauta criterion, Shared nearest neighbor (SNN)",Jianhua Xia and Jinbing Zhang and Yang Wang and Lixin Han and Hong Yan,https://www.sciencedirect.com/science/article/pii/S0031320321003642,https://doi.org/10.1016/j.patcog.2021.108177,0031-3203,2022,108177,121,Pattern Recognition,WC-KNNG-PC: Watershed clustering based on k-nearest-neighbor graph and Pauta Criterion,article,XIA2022108177,
"Financial time series analysis plays a central role in hedging market risks and optimizing investment decisions. This is a challenging task as the problems are always accompanied by multi-modality streams and lead-lag effects. For example, the price movements of stock are reflections of complicated market states in different diffusion speeds, including historical price series, media news, associated events, etc. Furthermore, the financial industry requires forecasting models to be interpretable and compliant. Therefore, in this paper, we propose a multi-modality graph neural network (MAGNN) to learn from these multimodal inputs for financial time series prediction. The heterogeneous graph network is constructed by the sources as nodes and relations in our financial knowledge graph as edges. To ensure the model interpretability, we leverage a two-phase attention mechanism for joint optimization, allowing end-users to investigate the importance of inner-modality and inter-modality sources. Extensive experiments on real-world datasets demonstrate the superior performance of MAGNN in financial market prediction. Our method provides investors with a profitable as well as interpretable option and enables them to make informed investment decisions.","Graph neural network, Graph attention, Deep learning, Quantitative investment",Dawei Cheng and Fangzhou Yang and Sheng Xiang and Jin Liu,https://www.sciencedirect.com/science/article/pii/S003132032100399X,https://doi.org/10.1016/j.patcog.2021.108218,0031-3203,2022,108218,121,Pattern Recognition,Financial time series forecasting with multi-modality graph neural network,article,CHENG2022108218,
"Noise negatively affects the complexity and performance of models built in classification problems. The most common approach to mitigate its consequences is the usage of preprocessing techniques, known as noise filters, which are designed to remove noisy samples from the training data. Nevertheless, they are specifically oriented to deal with errors affecting class labels. Their employment may not always result in an improvement when noise affects attribute values. In these cases, correcting the errors is an interesting alternative to traditional noise filtering that has not been enough studied so far in the specialized literature. This research proposes an attribute noise correction method with the final aim of increasing the performance of the classification algorithms used later. The identification of noisy data is based on an error score assigned to each one of the attribute values in the dataset, which are then passed through an optimization process to correct their potential noise. The validity of the proposed method is studied in an exhaustive experimental study, in which it is compared to several well-known preprocessing methods to deal with noisy datasets. The results obtained show the suitability of attribute noise correction with respect to the other alternatives when data suffer from attribute noise.","Attribute noise, Noise correction, Noise filtering, Noisy data, Classification",JosÃ© A. SÃ¡ez and Emilio Corchado,https://www.sciencedirect.com/science/article/pii/S0031320321003800,https://doi.org/10.1016/j.patcog.2021.108198,0031-3203,2022,108198,121,Pattern Recognition,ANCES: A novel method to repair attribute noise in classification problems,article,SAEZ2022108198,
"The COVID-19 pandemic is threatening billions of people's life all over the world. As of March 6, 2021, covid-19 has confirmed in 115,653,459 people worldwide. It has also a devastating effect on businesses and social activities. Since there is still no definite cure for this disease, extensive testing is the most critical issue to determine the trend of illness, appropriate medical treatment, and make social distancing policies. Besides, testing more people in a shorter time helps to contain the contagion. The PCR-based methods are the most popular tests which take about an hour to make the output result. Obviously, it makes the number of tests highly limited and consequently, hurts the efficiency of pandemic control. In this paper, we propose a new approach to identify affected individuals with a considerably reduced No. of tests. Intuitively, saving time and resources is the main advantage of our approach. We use contextual information to make a graph-based model to be used in model-based compressive sensing (CS). Our proposed model makes the testing with fewer tests required compared to traditional testing methods and even group testing. We embed contextual information such as age, underlying disease, symptoms (i.e. cough, fever, fatigue, loss of consciousness), and social contacts into a graph-based model. This model is used in model-based CS to minimize the required test. We take advantage of Discrete Graph Signal Processing on Graph (DSPG) to generate the model. Our contextual model makes CS more efficient in both the number of samples and the recovery quality. Moreover, it can be applied in the case that group testing is not applicable due to its severe dependency on sparsity. Experimental results show that the overall testing speed (individuals per test ratio) increases more than 15 times compared to the individual testing with the error of less than 5% which is dramatically lower than that of traditional compressive sensing.","COVID-19, Graph signal model, Group testing, Model-based compressive sensing",Mehdi Hasaninasab and Mohammad Khansari,https://www.sciencedirect.com/science/article/pii/S0031320321004337,https://doi.org/10.1016/j.patcog.2021.108253,0031-3203,2022,108253,122,Pattern Recognition,Efficient COVID-19 testing via contextual model based compressive sensing,article,HASANINASAB2022108253,
"Object recognition from live video streams comes with numerous challenges such as the variation in illumination conditions and poses. Convolutional neural networks (CNNs) have been widely used to perform intelligent visual object recognition. Yet, CNNs still suffer from severe accuracy degradation, particularly on illumination-variant datasets. To address this problem, we propose a new CNN method based on orientation fusion for visual object recognition. The proposed cloud-based video analytics system pioneers the use of bi-dimensional empirical mode decomposition to split a video frame into intrinsic mode functions (IMFs). We further propose these IMFs to endure Reisz transform to produce monogenic object components, which are in turn used for the training of CNNs. Past works have demonstrated how the object orientation component may be used to pursue accuracy levels as high as 93%. Herein we demonstrate how a feature-fusion strategy of the orientation components leads to further improving visual recognition accuracy to 97%. We also assess the scalability of our method, looking at both the number and the size of the video streams under scrutiny. We carry out extensive experimentation on the publicly available Yale dataset, including also a self generated video datasets, finding significant improvements (both in accuracy and scale), in comparison to AlexNet, LeNet and SE-ResNeXt, which are three most commonly used deep learning models for visual object recognition and classification.","Scalable video anaytics, Feature fusion, Object orientation, Object recognition, Convolutional neural networks, Cloud-based video analytics",Muhammad {Usman Yaseen} and Ashiq Anjum and Giancarlo Fortino and Antonio Liotta and Amir Hussain,https://www.sciencedirect.com/science/article/pii/S0031320321003885,https://doi.org/10.1016/j.patcog.2021.108207,0031-3203,2022,108207,121,Pattern Recognition,Cloud based scalable object recognition from video streams using orientation fusion and convolutional neural networks,article,USMANYASEEN2022108207,
"Feature selection is able to select more discriminative features for classification and plays an important role in multi-label learning to alleviate the effect of the curse of dimensionality. Recently, the multi-label feature selection methods based on the sparse regression model have received increasing attentions. However, most of these methods directly project original data space to label space in the regression model, which is inappropriate because the linear assumption between data space and label space doesn't hold in most cases. In the paper, we propose a feature selection method named multi-label feature selection via manifold regularization and dependence maximization (MRDM). In the regression model of MRDM, the original data space is projected to a low-dimensional manifold space, which not only has the same topological structure with the original data, but also has a strong dependence with the class labels. Then, an objective function involving l2,1-norm regularization is formulated, and an alternating optimization-based iterative algorithm is designed to obtain the sparse coefficients for multi-label feature selection. Extensive experiments on various multi-label data sets demonstrate the superiority of the proposed method compared with some state-of-the-art multi-label feature selection methods.","Multi-label learning, Feature selection, Sparse regression, Manifold regularization, Dependence maximization",Rui Huang and Zhejun Wu,https://www.sciencedirect.com/science/article/pii/S0031320321003368,https://doi.org/10.1016/j.patcog.2021.108149,0031-3203,2021,108149,120,Pattern Recognition,Multi-label feature selection via manifold regularization and dependence maximization,article,HUANG2021108149,
"The classification of hyperspectral image is a challenging task due to the high dimensional space, with large number of spectral bands, and low number of labeled training samples. To overcome these challenges, we propose a novel methodology for hyperspectral image classification based on multi-view deep neural networks which fuses both spectral and spatial features by using only a small number of labeled samples. Firstly, we process the initial hyperspectral image in order to extract a set of spectral and spatial features. Each spectral vector is the spectral signature of each pixel of the image. The spatial features are extracted using a simple deep autoencoder, which seeks to reduce the high dimensionality of data taking into account the neighborhood region for each pixel. Secondly, we propose a multi-view deep autoencoder model which allows fusing the spectral and spatial features extracted from the hyperspectral image into a joint latent representation space. Finally, a semi-supervised graph convolutional network is trained based on thee fused latent representation space to perform the hyperspectral image classification. The main advantage of the proposed approach is to allow the automatic extraction of relevant information while preserving the spatial and spectral features of data, and improve the classification of hyperspectral images even when the number of labeled samples is low. Experiments are conducted on three real hyperspectral images respectively Indian Pines, Salinas, and Pavia University datasets. Results show that the proposed approach is competitive in classification performances compared to state-of-the-art.","Deep learning, Representation learning, Hyperspectral image classification, Feature extraction",Akrem Sellami and Salvatore Tabbone,https://www.sciencedirect.com/science/article/pii/S0031320321004052,https://doi.org/10.1016/j.patcog.2021.108224,0031-3203,2022,108224,121,Pattern Recognition,Deep neural networks-based relevant latent representation learning for hyperspectral image classification,article,SELLAMI2022108224,
"Panoptic segmentation has attracted increasing attention as a joint task of semantic and instance segmentation. However, previous works have not noticed that the different requirements for semantic and instance segmentation can lead to conflict of feature discriminability. Instance segmentation mainly focuses on the central area of each instance in things regions, while semantic segmentation focuses on the whole region of a specific class. To resolve it, we propose: 1) a Dual-FPN framework which separates the shared Feature Pyramid Network (FPN) in previous works to reduce the conflict of receptive field and meet different requirements of the two tasks; 2) a Region Refinement Module which leverages the prediction of semantic segmentation to refine the result of instance segmentation and resolves the conflict between the things regions and the stuff regions. Experimental results on Cityscapes dataset and Mapillary Vistas dataset show that our proposed method can improve the result of both things and stuff and obtain state-of-the-art performance.","Panoptic segmentation, Feature discriminability, Region refinement",Tao Chu and Wenjie Cai and Qiong Liu,https://www.sciencedirect.com/science/article/pii/S0031320321004210,https://doi.org/10.1016/j.patcog.2021.108240,0031-3203,2022,108240,122,Pattern Recognition,Learning panoptic segmentation through feature discriminability,article,CHU2022108240,
"The lack of interpretability is an inevitable problem when using neural network models in real applications. In this paper, an explainable neural network based on generalized additive models with structured interactions (GAMI-Net) is proposed to pursue a good balance between prediction accuracy and model interpretability. GAMI-Net is a disentangled feedforward network with multiple additive subnetworks; each subnetwork consists of multiple hidden layers and is designed for capturing one main effect or one pairwise interaction. Three interpretability aspects are further considered, including a) sparsity, to select the most significant effects for parsimonious representations; b) heredity, a pairwise interaction could only be included when at least one of its parent main effects exists; and c) marginal clarity, to make main effects and pairwise interactions mutually distinguishable. An adaptive training algorithm is developed, where main effects are first trained and then pairwise interactions are fitted to the residuals. Numerical experiments on both synthetic functions and real-world datasets show that the proposed model enjoys superior interpretability and it maintains competitive prediction accuracy in comparison to the explainable boosting machine and other classic machine learning models.","Explainable neural network, Generalized additive model, Pairwise interaction, Interpretability constraints",Zebin Yang and Aijun Zhang and Agus Sudjianto,https://www.sciencedirect.com/science/article/pii/S0031320321003484,https://doi.org/10.1016/j.patcog.2021.108192,0031-3203,2021,108192,120,Pattern Recognition,GAMI-Net: An explainable neural network based on generalized additive models with structured interactions,article,YANG2021108192,
"Contemporary person re-identification (re-id) methods mostly compute independentlya feature representation of each person image in the query set and the gallery set. This strategy fails to consider any ranking context information of each probe image in the query set represented implicitly by the whole gallery set. Some recent re-ranking re-id methods therefore propose to take a post-processing strategy to exploit such contextual information for improving re-id matching performance. However, post-processing is independent of model training without jointly optimising the re-id feature and the ranking context information for better compatibility. In this work, for the first time, we show that the appearance feature and the ranking context information can be jointly optimised for learning more discriminative representations and achieving superior matching accuracy. Specifically, we propose to learn a hybrid ranking representation for person re-id with a two-stream architecture: (1) In the external stream, we use the ranking list of each probe image to learn plausible visual variations among the top ranks from the gallery as the external ranking information; (2) In the internal stream, we employ the part-based fine-grained feature as the internal ranking information, which mitigates the harm of incorrect matches in the ranking list. Assembling these two streams generates a hybrid ranking representation for person matching. Extensive experiments demonstrate the superiority of our method over the state-of-the-art methods on four large-scale re-id benchmarks (Market-1501, DukeMTMC-ReID, CUHK03 and MSMT17), under both supervised and unsupervised settings.","Person re-identification, Ranking representation, Ranking ensemble",Guile Wu and Xiatian Zhu and Shaogang Gong,https://www.sciencedirect.com/science/article/pii/S0031320321004209,https://doi.org/10.1016/j.patcog.2021.108239,0031-3203,2022,108239,121,Pattern Recognition,Learning hybrid ranking representation for person re-identification,article,WU2022108239,
"Human action recognition in visual data is one of the most fundamental challenges in computer vision. Existing approaches for this primary goal have been based on video data, often incorporating both color and dynamic flow information. Nevertheless, the majority of the visual data constitute still images, and for this reason, being able to recognize actions in still image is an ultimate objective of visual understanding with an extended list of applications. In this paper, we present a novel method that transfers the knowledge learned from action videos onto images to allow recognition of the principal action depicted in still image. Our intuition is that a generative model for knowledge transfer can be learned by taking advantage of the available action videos in the training stage to bridge images to videos. Based on this, we propose two complementary knowledge-transfer models utilizing fully connected networks to deliver the knowledge extracted from color and motion flow sequences to still images. We introduce a weighted reconstruction and classification loss to steer the generation procedure of the networks. In addition, we describe and analyze the influence of different data augmentation techniques, initialization strategies, and weighting coefficients for improving the performance. We observe that: both the transferred knowledge from color sequences and motion flow sequences can improve the performance of still image based human action recognition; the latter one which provides complementary dynamic information improves the performance a lot. We evaluate our models on two publicly available video based human action recognition datasets: UCF101 and HMDB51. To further validate the generalization ability of the proposed solution, we test the learned models from UCF101 dataset on two still image based human action recognition benchmarks: Willow7 Actions and the Sports. Our results demonstrate that the proposed method outperforms the baseline approaches with more than 2% accuracy, 3% accuracy, 3% accuracy and 5% mAP on UCF101, HMDB51, Sports and Willow 7 Actions datasets, respectively.","Action recognition, Deep learning, Knowledge-transfer",Jian Dong and Wankou Yang and Yazhou Yao and Fatih Porikli,https://www.sciencedirect.com/science/article/pii/S0031320321003757,https://doi.org/10.1016/j.patcog.2021.108188,0031-3203,2021,108188,120,Pattern Recognition,Knowledge memorization and generation for action recognition in still images,article,DONG2021108188,
"In this paper, we propose head pose estimation using deep neural networks and 3D point cloud. Unlike existing methods that either take 2D RGB image or 2D depth image as input, we adopt 3D point cloud data generated from depth to estimate 3D head poses. To further improve robustness and accuracy of head pose estimation, we classify 3D angles of head poses into 36 classes with 5 â interval and predict the probability of each angle in a class based on multi-layer perceptron (MLP). While traditional iterative methods for head model construction require high computation and memory costs, the proposed method is lightweight and computationally efficient by utilizing a sampled 3D point cloud as input combined with a graph convolutional neural network (GCNN). Experimental results on Biwi Kinect Head Pose dataset show that the proposed method achieves outstanding performance in head pose estimation and outperforms state-of-the-art ones in terms of accuracy.","Head pose estimation, Deep learning, Graph convolution, Multi-layer perceptron, Point cloud",Yuanquan Xu and Cheolkon Jung and Yakun Chang,https://www.sciencedirect.com/science/article/pii/S0031320321003915,https://doi.org/10.1016/j.patcog.2021.108210,0031-3203,2022,108210,121,Pattern Recognition,Head pose estimation using deep neural networks and 3D point clouds,article,XU2022108210,
"Most existing approaches for single image super-resolution (SISR) resort to quality low-high resolution (LR-HR) pairs and available degradation kernels to train networks for a specific task in hand in a fully supervised manner. Labeled data used for training are, however, usually limited in terms of the quantity and the diversity degradation kernels. The learned SR networks with one degradation kernel (e.g., bicubic) do not generalize well and their performance sharply deteriorates on other kernels (e.g., blurred or noise). In this paper, we address the critical challenge for SISR: limited labeled LR images and degradation kernels. We propose a novel Semi-supervised Student-Teacher Super-Resolution approach called S2TSR that super-resolves both labelled and unlabeled LR images via adversarial learning. To better exploit the information from labeled LR images, we propose a student-teacher framework (S-T) via knowledge transfer from supervised learning (T) to unsupervised learning (S). Specifically, the S-T knowledge transfer is based on a shared SR network, partial weight sharing of dual discriminators, and a pair matching network which also plays as a âlatent discriminatorâ. Lastly, to learn better features from the limited labeled LR images, we propose a new SR network via non-local and attention mechanisms. Experiments demonstrate that our approach substantially improves unsupervised methods and performs favorably over fully supervised methods.","Semi-supervised learning, Image super-resolution, Student-teacher model, Adversarial learning",Lin Wang and Kuk-Jin Yoon,https://www.sciencedirect.com/science/article/pii/S0031320321003873,https://doi.org/10.1016/j.patcog.2021.108206,0031-3203,2022,108206,121,Pattern Recognition,Semi-supervised student-teacher learning for single image super-resolution,article,WANG2022108206,
"Forecasting models that are trained across sets of many time series, known as Global Forecasting Models (GFM), have shown recently promising results in forecasting competitions and real-world applications, outperforming many state-of-the-art univariate forecasting techniques. In most cases, GFMs are implemented using deep neural networks, and in particular Recurrent Neural Networks (RNN), which require a sufficient amount of time series to estimate their numerous model parameters. However, many time series databases have only a limited number of time series. In this study, we propose a novel, data augmentation based forecasting framework that is capable of improving the baseline accuracy of the GFM models in less data-abundant settings. We use three time series augmentation techniques: GRATIS, moving block bootstrap (MBB), and dynamic time warping barycentric averaging (DBA) to synthetically generate a collection of time series. The knowledge acquired from these augmented time series is then transferred to the original dataset using two different approaches: the pooled approach and the transfer learning approach. When building GFMs, in the pooled approach, we train a model on the augmented time series alongside the original time series dataset, whereas in the transfer learning approach, we adapt a pre-trained model to the new dataset. In our evaluation on competition and real-world time series datasets, our proposed variants can significantly improve the baseline accuracy of GFM models and outperform state-of-the-art univariate forecasting methods.","Time series forecasting, Global forecasting models, Data augmentation, Transfer learning, RNN",Kasun Bandara and Hansika Hewamalage and Yuan-Hao Liu and Yanfei Kang and Christoph Bergmeir,https://www.sciencedirect.com/science/article/pii/S0031320321003356,https://doi.org/10.1016/j.patcog.2021.108148,0031-3203,2021,108148,120,Pattern Recognition,Improving the accuracy of global forecasting models using time series data augmentation,article,BANDARA2021108148,
"Lifelong machine learning can learn a sequence of consecutive robotic perception tasks via transferring previous experiences. However, 1) most existing lifelong learning based perception methods only take advantage of visual information for robotic tasks, while neglecting another important tactile sensing modality to capture discriminative material properties; 2) Meanwhile, they cannot explore the intrinsic relationships across different modalities and the common characterization among different tasks of each modality, due to the distinct divergence between heterogeneous feature distributions. To address above challenges, we propose a new Lifelong Visual-Tactile Learning (LVTL) model for continuous robotic visual-tactile perception tasks, which fully explores the latent correlations in both intra-modality and cross-modality aspects. Specifically, a modality-specific knowledge library is developed for each modality to explore common intra-modality representations across different tasks, while narrowing intra-modality mapping divergence between semantic and feature spaces via an auto-encoder mechanism. Moreover, a sparse constraint based modality-invariant space is constructed to capture underlying cross-modality correlations and identify the contributions of each modality for new coming visual-tactile tasks. We further propose a modality consistencyÂ regularizer to efficiently align the heterogeneous visual and tactile samples, which ensures the semantic consistency between different modality-specific knowledge libraries. After deriving an efficient model optimization strategy, we conduct extensive experiments on several representative datasets to demonstrate the superiority of our LVTL model. Evaluation experiments show that our proposed model significantly outperforms existing state-of-the-art methods with about 1.16%â¼15.36% improvement under different lifelong visual-tactile perception scenarios.","Lifelong machine learning, Robotics, Visual-tactile perception, Cross-modality learning, Multi-task learning",Jiahua Dong and Yang Cong and Gan Sun and Tao Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321003630,https://doi.org/10.1016/j.patcog.2021.108176,0031-3203,2022,108176,121,Pattern Recognition,Lifelong robotic visual-tactile perception learning,article,DONG2022108176,
"The inherent properties of the graph structure of the financial market and the correlation attributes that actually exist in the system inspire us to introduce the concept of the graph to solve the problem of prediction and recommendation in the financial sector. In this paper, we are adhering to the idea of recommending high return ratio stocks and put forward an attributed graph attention network model based on the correlation information, with encoded timing characteristics derived from time series module and global information originating from the stacked graph neural network(GNN) based models, which we called Relation-aware Dynamic Attributed Graph Attention Network (RA-AGAT). On this basis, we have verified the practicality and applicability of the application of graph models in finance. Our innovative structure first captures the local correlation topology information and then introduce a stacked graph neural network structure to recommend Top-N return ratio of stock items. Experiments on the real China A-share market demonstrate that the RA-AGAT architecture is capable of surpassing the previously applicable methods in the prediction and recommendation of stock return ratio.","Financial market, Attributed graph attention network, Correlation coefficient, Chinese stock recommendation",Shibo Feng and Chen Xu and Yu Zuo and Guo Chen and Fan Lin and Jianbing XiaHou,https://www.sciencedirect.com/science/article/pii/S003132032100306X,https://doi.org/10.1016/j.patcog.2021.108119,0031-3203,2022,108119,121,Pattern Recognition,Relation-aware dynamic attributed graph attention network for stocks recommendation,article,FENG2022108119,
"Generative Adversarial Networks (GANs) have become the most used networks towards solving the problem of image generation. Self-supervised GANs are later proposed to avoid the catastrophic forgetting of the discriminator and to improve the image generation quality without needing the class labels. However, the generalizability of the self-supervision tasks on different GAN architectures is not studied before. To that end, we extensively analyze the contribution of a previously proposed self-supervision task, deshuffling of the DeshuffleGANs in the generalizability context. We assign the deshuffling task to two different GAN discriminators and study the effects of the task on both architectures. We extend the evaluations compared to the previously proposed DeshuffleGANs on various datasets. We show that the DeshuffleGAN obtains the best FID results for several datasets compared to the other self-supervised GANs. Furthermore, we compare the deshuffling with the rotation prediction that is firstly deployed to the GAN training and demonstrate that its contribution exceeds the rotation prediction. We design the conditional DeshuffleGAN called cDeshuffleGAN to evaluate the quality of the learnt representations. Lastly, we show the contribution of the self-supervision tasks to the GAN training on the loss landscape and present that the effects of these tasks may not be cooperative to the adversarial training in some settings. Our code can be found at https://github.com/gulcinbaykal/DeshuffleGAN.","Self-Supervised generative adversarial networks, Generative adversarial networks, Self-supervised learning, DeshuffleGANs, Deshuffling",Gulcin Baykal and Furkan Ozcelik and Gozde Unal,https://www.sciencedirect.com/science/article/pii/S0031320321004167,https://doi.org/10.1016/j.patcog.2021.108244,0031-3203,2022,108244,122,Pattern Recognition,Exploring DeshuffleGANs in Self-Supervised Generative Adversarial Networks,article,BAYKAL2022108244,
"Outcomes with a natural order commonly occur in prediction problems and often the available input data are a mixture of complex data like images and tabular predictors. Deep Learning (DL) models are state-of-the-art for image classification tasks but frequently treat ordinal outcomes as unordered and lack interpretability. In contrast, classical ordinal regression models consider the outcomeâs order and yield interpretable predictor effects but are limited to tabular data. We present ordinal neural network transformation models (ontrams), which unite DL with classical ordinal regression approaches. ontrams are a special case of transformation models and trade off flexibility and interpretability by additively decomposing the transformation function into terms for image and tabular data using jointly trained neural networks. The performance of the most flexible ontram is by definition equivalent to a standard multi-class DL model trained with cross-entropy while being faster in training when facing ordinal outcomes. Lastly, we discuss how to interpret model components for both tabular and image data on two publicly available datasets.","Deep learning, Interpretability, Distributional regression, Ordinal regression, Transformation models",Lucas Kook and Lisa Herzog and Torsten Hothorn and Oliver DÃ¼rr and Beate Sick,https://www.sciencedirect.com/science/article/pii/S003132032100443X,https://doi.org/10.1016/j.patcog.2021.108263,0031-3203,2022,108263,122,Pattern Recognition,Deep and interpretable regression models for ordinal outcomes,article,KOOK2022108263,
"Weakly-supervised object detection attempts to limit the amount of supervision by dispensing the need for bounding boxes, but still assumes image-level labels on the entire training set. In this work, we study the problem of training an object detector from one or few images with image-level labels and a larger set of completely unlabeled images. This is an extreme case of semi-supervised learning where the labeled data are not enough to bootstrap the learning of a detector. Our solution is to train a weakly-supervised student detector model from image-level pseudo-labels generated on the unlabeled set by a teacher classifier model, bootstrapped by region-level similarities to labeled images. Building upon the recent representative weakly-supervised pipeline PCLÂ [1], our method can use more unlabeled images to achieve performance competitive or superior to many recent weakly-supervised detection solutions. Code will be made available at https://github.com/zhaohui-yang/NSOD.","Object detection, Weakly-supervised learning, Semi-supervised learning, Unlabelled set",Zhaohui Yang and Miaojing Shi and Chao Xu and Vittorio Ferrari and Yannis Avrithis,https://www.sciencedirect.com/science/article/pii/S0031320321003514,https://doi.org/10.1016/j.patcog.2021.108164,0031-3203,2021,108164,120,Pattern Recognition,Training object detectors from few weakly-labeled and many unlabeled images,article,YANG2021108164,
"Predicting the future trajectories of pedestrians is a challenging problem that has a range of application, from crowd surveillance to autonomous driving. In literature, methods to approach pedestrian trajectory prediction have evolved, transitioning from physics-based models to data-driven models based on recurrent neural networks. In this work, we propose a new approach to pedestrian trajectory prediction, with the introduction of a novel 2D convolutional model. This new model outperforms recurrent models, and it achieves state-of-the-art results on the ETH and TrajNet datasets. We also present an effective system to represent pedestrian positions and powerful data augmentation techniques, such as the addition of Gaussian noise and the use of random rotations, which can be applied to any model. As an additional exploratory analysis, we present experimental results on the inclusion of occupancy methods to model social information, which empirically show that these methods are ineffective in capturing social interaction.","Trajectory prediction, Pedestrian prediction, Convolutional neural networks",Simone Zamboni and Zekarias Tilahun Kefato and Sarunas Girdzijauskas and Christoffer NorÃ©n and Laura {Dal Col},https://www.sciencedirect.com/science/article/pii/S0031320321004325,https://doi.org/10.1016/j.patcog.2021.108252,0031-3203,2022,108252,121,Pattern Recognition,Pedestrian trajectory prediction with convolutional neural networks,article,ZAMBONI2022108252,
"Temporal feature extraction is an essential technique in video-based action recognition. Key points have been utilized in skeleton-based action recognition methods but they require costly key point annotation. In this paper, we propose a novel temporal feature extraction module, named Key Point Shifts Embedding Module (KPSEM), to adaptively extract channel-wise key point shifts across video frames without key point annotation. Key points are adaptively extracted as feature points with maximum feature values at split regions and key point shifts are the spatial displacements of corresponding key points. The key point shifts are encoded as the overall temporal features via linear embedding layers in a multi-set manner. Our method achieves competitive performance through embedding key point shifts with trivial computational cost, achieving the state-of-the-art performance of 78.81% on Mini-Kinetics and competitive performance on UCF101, Something-Something-v1 and HMDB51 datasets.","Action recognition, Temporal feature, Key point shifts",Haozhi Cao and Yuecong Xu and Jianfei Yang and Kezhi Mao and Jianxiong Yin and Simon See,https://www.sciencedirect.com/science/article/pii/S0031320321003599,https://doi.org/10.1016/j.patcog.2021.108172,0031-3203,2021,108172,120,Pattern Recognition,Effective action recognition with embedded key point shifts,article,CAO2021108172,
"In this paper, a user modeling task is examined by processing mobile device gallery of photos and videos. We propose a novel engine for preferences prediction based on scene recognition, object detection and facial analysis. At first, all faces in a gallery are clustered, and all private photos and videos with faces from large clusters are processed on the embedded system in offline mode. Other photos may be sent to the remote server to be analyzed by very deep sophisticated neural networks. The visual features of each photo are obtained from scene recognition and object detection models. These features are aggregated into a single descriptor in the neural attention unit. The proposed pipeline is implemented in mobile Android application. Experimental results for the Photo Event Collection, Web Image Dataset for Event Recognition and Amazon Fashion data demonstrate the possibility to efficiently process images without significant accuracy degradation.","Scene recognition, Event recognition, Object detection, Recognition of a set of images, CNN (Convolutional neural network), Mobile device",A.V. Savchenko and K.V. Demochkin and I.S. Grechikhin,https://www.sciencedirect.com/science/article/pii/S0031320321004283,https://doi.org/10.1016/j.patcog.2021.108248,0031-3203,2022,108248,121,Pattern Recognition,Preference prediction based on a photo gallery analysis with scene recognition and object detection,article,SAVCHENKO2022108248,
"Clustering validity indices are typically used as tools to find the correct number of clusters in a data set and/or to evaluate the quality of the clusters formed by clustering algorithms. Clustering validity indices measure separation and compactness of clusters. Typically, when applying a clustering algorithm, the input includes the number of clusters. After applying the algorithm with several different numbers of clusters, we determine the number of clusters to be the one with the best validity index. There are two types of clustering validity indices: external indices that are supervised, and internal indices that are unsupervised. The focus of this paper is on internal validity indices. Some existing internal validity indices capture the properties of the clusters by using representative statistics such as mean, variance, diameter, etc., however, these do not perform well when clusters have arbitrary shapes. One approach to overcome this issue is to use the density of the data objects in each cluster. That provides the advantage of capturing the full characteristics of the cluster which is most beneficial when there are clusters with arbitrary shapes. In the literature, a few density-based clustering validity indices have been proposed. However, some of them show poor performance when the clusters are not perfectly separated. Some others perform poorly because they use only representative objects from each cluster instead of all objects. The contribution of this paper is an internal validity index named the object-based clustering validity index with densities (OCVD). OCVD is a single number that averages the density-based contribution of individual data objects to both separation and compactness of clusters. The methodology behind calculating the density-based contributions of the objects is kernel density estimation. We show through several experiments that OCVD performs well in detecting the correct number of clusters in data sets with different cluster shapes including arbitrary shapes.","Clustering, Clustering validity index, Internal index, Density-based cluster validation, Unsupervised",Behnam Tavakkol and Jeongsub Choi and Myong Kee Jeong and Susan L. Albin,https://www.sciencedirect.com/science/article/pii/S0031320321004040,https://doi.org/10.1016/j.patcog.2021.108223,0031-3203,2022,108223,121,Pattern Recognition,Object-based cluster validation with densities,article,TAVAKKOL2022108223,
"Combinatorial optimization (CO) has been a hot research topic because of its theoretic and practical importance. As a classic CO problem, deep hashing aims to find an optimal code for each data from finite discrete possibilities, while the discrete nature brings a big challenge to the optimization process. Previous methods usually mitigate this challenge by binary approximation, substituting binary codes for real-values via activation functions or regularizations. However, such approximation leads to uncertainty between real-values and binary ones, degrading retrieval performance. In this paper, we propose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly estimates the uncertainty during training and leverages the uncertainty information to guide the approximation process. Specifically, we model bit-level uncertainty via measuring the discrepancy between the output of a hashing network and that of a momentum-updated network. The discrepancy of each bit indicates the uncertainty of the hashing network to the approximate output of that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can be regarded as image-level uncertainty. It embodies the uncertainty of the hashing network to the corresponding input image. The hashing bit and image with higher uncertainty are paid more attention during optimization. To the best of our knowledge, this is the first work to study the uncertainty in hashing bits. Extensive experiments are conducted on four datasets to verify the superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a million-scale dataset Clothing1M. Our method achieves the best performance on all of the datasets and surpasses existing state-of-the-art methods by a large margin.","Combinatorial optimization, Deep hashing, Uncertainty",Chaoyou Fu and Guoli Wang and Xiang Wu and Qian Zhang and Ran He,https://www.sciencedirect.com/science/article/pii/S0031320321004441,https://doi.org/10.1016/j.patcog.2021.108264,0031-3203,2022,108264,122,Pattern Recognition,Deep momentum uncertainty hashing,article,FU2022108264,
"This paper reports a new method (simplified as AE-ELM-SynMin) to create the Synthetic Minority class samples for imbalanced classification based on AutoEncoder Extreme Learning Machine (AE-ELM). AE-ELM-SynMin first trains an AE-ELM which is a special ELM with the same input and output, i.e., the original minority class samples. Second, the crossover, mutation and filtration operations are conducted on the hidden-layer output of AE-ELM and then the synthetic hidden-layer output is obtained. Third, the synthetic minority class samples are created by decoding the synthetic hidden-layer output with output-layer weights of AE-ELM. AE-ELM-SynMin guarantees that the synthetic minority class has the higher information amount than original minority class and meanwhile keeps the consistent probability distribution with the original minority class. The experimental results demonstrate the better imbalanced classification performances of AE-ELM-SynMin in comparison with the regular synthetic minority over-sampling technique (Regular-SMOTE) and its variants, e.g., Borderline-SMOTE, Random-SMOTE, and SMOTE-IPF.","Imbalanced classification, Minority class, Majority class, Synthetic samples creation, SMOTE",Yu-Lin He and Sheng-Sheng Xu and Joshua Zhexue Huang,https://www.sciencedirect.com/science/article/pii/S0031320321003472,https://doi.org/10.1016/j.patcog.2021.108191,0031-3203,2022,108191,121,Pattern Recognition,Creating synthetic minority class samples based on autoencoder extreme learning machine,article,HE2022108191,
"In this paper, we have implemented a high-frequency quantitative system that can obtain stable returns for the Chinese A-share market, which has been running for more than 3 months (from March 27, 2020 to June 30, 2020) with the expected results. A number of rules and barriers exist in the Chinese A-share market such as trading restrictions and high fees, as well as scarce and expensive hedging tools. It is difficult to achieve stable absolute returns in such a market. Stock correlation analysis and price prediction play an important role to achieve any profitable trading. The portfolio management and subsequent trading decisions highly depend on the results of stock correlation analysis and price prediction. However, it is nontrivial to analyze and predict any stocks, being time-varying and affected by unlimited factors in a given market. Traditional methods only take some certain factors into consideration but ignore others that may be changed dynamically. In this paper, we propose a novel machine learning model named Graph Attention Long Short-Term Memory (GALSTM) to learn the correlations between stocks and predict their future prices automatically. First, a multi-Hawkes Process is used to initial a correlation graph between stocks. This procedure provides a good training start as the multi-Hawkes Processes will be studied on the most saint feature fluctuations with any correlations being statistically significant. Then an attention-based LSTM is built to learn the weighting matrix underlying the dynamic graph. In addition, we also build matching data process plus portfolio management modules to form a complete system. The proposed GALSTM enables us to expand the scope of stock selection under the premise of controlling risks with limited hedging tools in the A-share market, thereby effectively increasing high-frequency excess returns. We then construct a long and short positions combination, select long positions in the A shares of the entire market, and use stock index futures to short. With GALSTM model, the products managed by our fully automatic quantitative trading system achieved an absolute annual return rate of 44.71% and the standard deviation of daily returns is only 0.42% in three months of operation. Only 1 week loss in 13 weeks of running time.","High-frequency trading, Graph attention long short-term memory (GALSTM), Hawkes processes, Portfolio management",Tao Yin and Chenzhengyi Liu and Fangyu Ding and Ziming Feng and Bo Yuan and Ning Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321003903,https://doi.org/10.1016/j.patcog.2021.108209,0031-3203,2022,108209,122,Pattern Recognition,Graph-based stock correlation and prediction for high-frequency trading systems,article,YIN2022108209,
"In the online innovation market, the fund-raising performance of the start-up project is a concerning issue for creators, investors and platforms. Unfortunately, existing studies always focus on modeling the fund-raising process after the publishment of a project but the predicting of a project attraction in the market before setting up is largely unexploited. Usually, this prediction is always with great challenges to making a comprehensive understanding of both the start-up project and market environment. To that end, in this paper, we present a focused study on this important problem from a market graph perspective. Specifically, we propose a Graph-based Market Environment (GME) model for predicting the fund-raising performance of the unpublished project by exploiting the market environment. In addition, we discriminatively model the project competitiveness and market preferences by designing two graph-based neural network architectures and incorporating them into a joint optimization stage. Furthermore, to explore the information propagation problem with dynamic environment in a large-scale market graph, we extend the GME model with parallelizing competitiveness quantification and hierarchical propagation algorithm. Finally, we conduct extensive experiments on real-world data. The experimental results clearly demonstrate the effectiveness of our proposed model.","Crowdfunding, Market environment modeling, Graph neural network",Likang Wu and Zhi Li and Hongke Zhao and Qi Liu and Enhong Chen,https://www.sciencedirect.com/science/article/pii/S003132032100385X,https://doi.org/10.1016/j.patcog.2021.108204,0031-3203,2022,108204,121,Pattern Recognition,Estimating fund-raising performance for start-up projects from a market graph perspective,article,WU2022108204,
"Cross-modal retrieval has become a hot research topic in both computer vision and natural language processing areas. Learning intermediate common space for features of different modalities has become one of mainstream methods. In this paper, we propose a novel multi-task framework based on feature separation and reconstruction (mFSR) for cross-modal retrieval based on common space learning methods, which introduces feature separation module to deal with information asymmetry between different modalities, and introduces image and text reconstruction module to improve the quality of feature separation module. Extensive experiments on MS-COCO and Flickr30K datasets demonstrate that feature separation and specific information reconstruction can significantly improve the baseline performance of cross-modal image-caption retrieval.","Cross-modal retrieval, Feature separation, Image reconstruction, Text reconstruction",Li Zhang and Xiangqian Wu,https://www.sciencedirect.com/science/article/pii/S0031320321003988,https://doi.org/10.1016/j.patcog.2021.108217,0031-3203,2022,108217,122,Pattern Recognition,Multi-task framework based on feature separation and reconstruction for cross-modal retrieval,article,ZHANG2022108217,
"In this paper we extend the setting of the online prediction with expert advice to function-valued forecasts. At each step of the online game several experts predict a function, and the learner has to efficiently aggregate these functional forecasts into a single forecast. We adapt basic mixable (and exponentially concave) loss functions to compare functional predictions and prove that these adaptations are also mixable (exp-concave). We call this phenomenon mixability (exp-concavity) of integral loss functions. As an application of our main result, we prove that various loss functions used for probabilistic forecasting are mixable (exp-concave). The considered losses include Sliced Continuous Ranked Probability Score, Energy-Based Distance, Optimal Transport Costs & Sliced Wasserstein-2 distance, Beta-2 & Kullback-Leibler divergences, Characteristic function and Maximum Mean Discrepancies.","Integral loss functions, Mixability, Exponential concavity, Prediction with expert advice, Functional forecasting, Probabilistic forecasting",Alexander Korotin and Vladimir Vâyugin and Evgeny Burnaev,https://www.sciencedirect.com/science/article/pii/S0031320321003629,https://doi.org/10.1016/j.patcog.2021.108175,0031-3203,2021,108175,120,Pattern Recognition,Mixability of integral losses: A key to efficient online aggregation of functional and probabilistic forecasts,article,KOROTIN2021108175,
"Point cloud data can be produced by many depth sensors, such as Light Detection and Ranging (LIDAR) and RGB-D cameras, and they are widely used in broad applications of robotic navigation and remote-sensing for the understanding of environment. Hence, new techniques for object representation and classification based on 3D point cloud are becoming increasingly in high demand. Due to the irregularity of the object shape, the point cloud-based object recognition is a very challenging task, especially the pose variances of a point cloud will impose many difficulties. In this paper, we tackle the challenge of pose variances in object classification based on point cloud by developing a novel end-to-end pose robust graph convolutional network. Technically, we first represent the point cloud using the spherical system instead of the traditional Cartesian system for simplicity of computation and representation. Then a pose auxiliary network is constructed with an aim to estimate the pose changes in terms of rotation angles. Finally, a graph convolutional network is constructed for object classification against the pose variations of point cloud. The experimental results show the new model outperforms the existing approaches (such as PointNet and PointNet++) on the classification task when conducting experiments on both the ModelNet40 and the ShapeNetCore dataset with a series of random rotations of a 3D point cloud. Specifically, we obtain 73.02% accuracy for classification task on the ModelNet40 with delaunay triangulation algorithm, which is much better than the state of the art algorithms, such as PointNet and PointCNN.","Point cloud, Pose robust, Graph convolutional network, Classification",Huafeng Wang and Yaming Zhang and Wanquan Liu and Xianfeng Gu and Xin Jing and Zicheng Liu,https://www.sciencedirect.com/science/article/pii/S0031320321004313,https://doi.org/10.1016/j.patcog.2021.108251,0031-3203,2022,108251,121,Pattern Recognition,A novel GCN-based point cloud classification model robust to pose variances,article,WANG2022108251,
"In the field of robotics, due to the complexity of real environments, place recognition using the 3D LiDAR is always a challenging problem. The spatial relations of internal structures underlying the LiDAR data from different places are distinguishable, which can be used to describe the environment. In this paper, we utilize the spatial relations of internal structures and propose a two-level framework for 3D LiDAR place recognition based on the spatial relation graph (SRG). At first, the proposed framework segments the point cloud into multiple clusters, then the features of the clusters and the spatial relation descriptors (SRDs) between the clusters are extracted, and the point cloud is represented by the SRG, which uses the clusters as the nodes and their spatial relations as the edges. After that, we propose a two-level matching model in which two different models are fused for accurately and efficiently matching the SRGs, including the upper-level searching model (U-LSM) and lower-level matching model (L-LMM). In the U-LSM, an incremental bag-of-words model is used to search for candidate SRGs through the distribution of the SRDs in the SRG. In the L-LMM, we utilize the improved spectral method to calculate similarities between the current SRG and the candidates. The experimental results demonstrate that our framework achieves good precision, recall and viewpoint robustness on both public benchmarks and self-built campus dataset.","Place recognition, 3D LiDAR, Spatial relation graph, Two-level framework",Yansong Gong and Fengchi Sun and Jing Yuan and Wenbin Zhu and Qinxuan Sun,https://www.sciencedirect.com/science/article/pii/S0031320321003587,https://doi.org/10.1016/j.patcog.2021.108171,0031-3203,2021,108171,120,Pattern Recognition,A two-level framework for place recognition with 3D LiDAR based on spatial relation graph,article,GONG2021108171,
"Embedding learning (EL) and feature synthesizing (FS) are two of the popular categories of fine-grained GZSL methods. EL or FS using global features cannot discriminate fine details in the absence of local features. On the other hand, EL or FS methods exploiting local features either neglect direct attribute guidance or global information. Consequently, neither method performs well. In this paper, we propose to explore global and direct attribute-supervised local visual features for both EL and FS categories in an integrated manner for fine-grained GZSL. The proposed integrated network has an EL sub-network and a FS sub-network. Consequently, the proposed integrated network can be tested in two ways. We propose a novel two-step dense attention mechanism to discover attribute-guided local visual features. We introduce new mutual learning between the sub-networks to exploit mutually beneficial information for optimization. Moreover, we propose to compute source-target class similarity based on mutual information and transfer-learn the target classes to reduce bias towards the source domain during testing. We demonstrate that our proposed method outperforms contemporary methods on benchmark datasets.","Generalized zero-shot learning, Fine-grained classification, Dense attention mechanism",Tasfia Shermin and Shyh Wei Teng and Ferdous Sohel and Manzur Murshed and Guojun Lu,https://www.sciencedirect.com/science/article/pii/S003132032100426X,https://doi.org/10.1016/j.patcog.2021.108246,0031-3203,2022,108246,122,Pattern Recognition,Integrated generalized zero-shot learning for fine-grained classification,article,SHERMIN2022108246,
"Most video analytics applications rely on object detectors to localize objects in frames. However, when real-time is a requirement, running the detector at all the frames is usually not possible. This is somewhat circumvented by instantiating visual object trackers between detector calls, but this does not scale with the number of objects. To tackle this problem, we present SiamMT, a new deep learning multiple visual object tracking solution that applies single-object tracking principles to multiple arbitrary objects in real-time. To achieve this, SiamMT reuses feature computations, implements a novel crop-and-resize operator, and defines a new and efficient pairwise similarity operator. SiamMT naturally scales up to several dozens of targets, reaching 25 fps with 122 simultaneous objects for VGA videos, or up to 100 simultaneous objects in HD720 video. SiamMT has been validated on five large real-time benchmarks, achieving leading performance against current state-of-the-art trackers.","Multiple visual object tracking, Motion estimation, Deep learning, Siamese networks",Lorenzo Vaquero and VÃ­ctor M. Brea and Manuel Mucientes,https://www.sciencedirect.com/science/article/pii/S0031320321003861,https://doi.org/10.1016/j.patcog.2021.108205,0031-3203,2022,108205,121,Pattern Recognition,Tracking more than 100 arbitrary objects at 25 FPS through deep learning,article,VAQUERO2022108205,
"One of the most important problems in regression-based error model is modeling the complex representation error caused by various corruptions and environment changes in images. For example, in robust face recognition, images are often affected by varying types and levels of corruptions, such as random pixel corruptions, block occlusions, or disguises. However, existing works are not robust enough to solve this problem due to they cannot model the complex corrupted errors very well. In this paper, we address this problem by a unified sparse weight learning and low-rank approximation regression model, which enables the random noises and contiguous occlusions in images to be treated simultaneously. For the random noise, we define a generalized correntropy (GC) function to match the error distribution. For the structured error caused by occlusions or disguises, we propose a GC function based rank approximation to measure the rank of error matrices. Since the proposed objective function is non-convex, an effective iterative optimization algorithm is developed to achieve the optimal weight learning and low-rank approximation. Extensive experimental results on three public face databases show that the proposed model can fit the error distribution and structure very well, thus obtain better recognition accuracies in comparison with the existing methods.","Regression, Weight learning, Low-rank approximation, Generalized correntropy, Robust learning",Miaohua Zhang and Yongsheng Gao and Jun Zhou,https://www.sciencedirect.com/science/article/pii/S0031320321003344,https://doi.org/10.1016/j.patcog.2021.108147,0031-3203,2021,108147,120,Pattern Recognition,A unified weight learning and low-rank regression model for robust complex error modeling,article,ZHANG2021108147,
"In multi-label classification, many existing works only pay attention to the label-specific features and label correlation while they ignore the common features and instance correlation, which are also essential for building a competitive classifier. Besides, existing works usually depend on the assumption that they tend to have the similar label-specific features if two labels are correlated. However, this assumption cannot always hold in some cases. Therefore, in this paper, we propose a new approach of learning common and label-specific features for multi-label classification using the correlation information from labels and instances. First, we introduce l2,1-norm and l1-norm regularizers to learn common and label-specific features simultaneously. Second, we use a regularizer to constrain label correlations on label outputs instead of coefficient matrix. Finally, instance correlations are also considered through the k-nearest neighbor mechanism. Comprehensive experiments manifest the superiority of our proposed approach against other well-established multi-label learning algorithms for label-specific features.","Multi-label classification, Label-specific features, Common features, Instance correlation",Junlong Li and Peipei Li and Xuegang Hu and Kui Yu,https://www.sciencedirect.com/science/article/pii/S0031320321004398,https://doi.org/10.1016/j.patcog.2021.108259,0031-3203,2022,108259,121,Pattern Recognition,Learning common and label-specific features for multi-Label classification with correlation information,article,LI2022108259,
"Research on Coronavirus Disease 2019 (COVID-19) detection methods has increased in the last months as more accurate automated toolkits are required. Recent studies show that CT scan images contain useful information to detect the COVID-19 disease. However, the scarcity of large and well balanced datasets limits the possibility of using detection approaches in real diagnostic contexts as they are unable to generalize. Indeed, the performance of these models quickly becomes inadequate when applied to samples captured in different contexts (e.g., different equipment or populations) from those used in the training phase. In this paper, a novel ensemble-based approach for more accurate COVID-19 disease detection using CT scan images is proposed. This work exploits transfer learning using pre-trained deep networks (e.g., VGG, Xception, and ResNet) evolved with a genetic algorithm, combined into an ensemble architecture for the classification of clustered images of lung lobes. The study is validated on a new dataset obtained as an integration of existing ones. The results of the experimental evaluation show that the ensemble classifier ensures effective performance, also exhibiting better generalization capabilities.","Deep learning, CT Scan images, COVID-19, Coronavirus",Lerina Aversano and Mario Luca Bernardi and Marta Cimitile and Riccardo Pecori,https://www.sciencedirect.com/science/article/pii/S0031320321003228,https://doi.org/10.1016/j.patcog.2021.108135,0031-3203,2021,108135,120,Pattern Recognition,Deep neural networks ensemble to detect COVID-19 from CT scans,article,AVERSANO2021108135,
"In this paper, we propose convex cone-based frameworks for image-set classification. Image-set classification aims to classify a set of images, usually obtained from video frames or multi-view cameras, into a target object. To accurately and stably classify a set, it is essential to accurately represent structural information of the set. There are various image features, such as histogram-based features and convolutional neural network features. We should note that most of them have non-negativity and thus can be effectively represented by a convex cone. This leads us to introduce the convex cone representation to image-set classification. To establish a convex cone-based framework, we mathematically define multiple angles between two convex cones, and then use the angles to define the geometric similarity between them. Moreover, to enhance the framework, we introduce two discriminant spaces. We first propose a discriminant space that maximizes gaps between cones and minimizes the within-class variance. We then extend it to a weighted discriminant space by introducing weights on the gaps to deal with complicated data distribution. In addition, to reduce the computational cost of the proposed methods, we develop a novel strategy for fast implementation. The effectiveness of the proposed methods is demonstrated experimentally by using five databases.","Image-set based method, Convex cone representation, Multiple angles",Naoya Sogi and Rui Zhu and Jing-Hao Xue and Kazuhiro Fukui,https://www.sciencedirect.com/science/article/pii/S0031320321003502,https://doi.org/10.1016/j.patcog.2021.108190,0031-3203,2022,108190,121,Pattern Recognition,Constrained mutual convex cone method for image set based recognition,article,SOGI2022108190,
"GAN is a generative modelling framework which has been proven as able to minimise various types of divergence measures under an optimal discriminator. However, there is a gap between the loss function of GAN used in theory and in practice. In theory, the proof of the Jensen divergence minimisation involves the min-max criterion, but in practice the non-saturating criterion is instead used to avoid gradient vanishing. We argue that the formulation of divergence minimization via GAN is biased and may yield a poor convergence of the algorithm. In this paper, we propose the Residual Generator for GAN (Rg-GAN), which is inspired by the closed-loop control theory, to bridge the gap between theory and practice. Rg-GAN minimizes the residual between the loss of the generated data to be real and the loss of the generated data to be fake from the perspective of the discriminator. In this setting, the loss terms of the generator depend only on the generated data and therefore contribute to the optimisation of the model. We formulate the residual generator for standard GAN and least-squares GAN and show that they are equivalent to the minimisation of reverse-KL divergence and a novel instance of f-divergence, respectively. Furthermore, we prove that Rg-GAN can be reduced to Integral Probability Metrics (IPMs) GANs (e.g., Wasserstein GAN) and bridge the gap between IPMs and f-divergence. Additionally, we further improve on Rg-GAN by proposing a loss function for the discriminator that has a better discrimination ability. Experiments on synthetic and natural images data sets show that Rg-GAN is robust to mode collapse, and improves the generation quality of GAN in terms of FID and IS scores.","Generative adversarial networks, Image synthesis, Deep learning",Aurele Tohokantche Gnanha and Wenming Cao and Xudong Mao and Si Wu and Hau-San Wong and Qing Li,https://www.sciencedirect.com/science/article/pii/S0031320321004039,https://doi.org/10.1016/j.patcog.2021.108222,0031-3203,2022,108222,121,Pattern Recognition,The residual generator: An improved divergence minimization framework for GAN,article,GNANHA2022108222,
"Touchless biometrics has become significant in the wake of novel coronavirus 2019 (COVID-19). Due to the convenience, user-friendly, and high-accuracy, touchless palmprint recognition shows great potential when the hygiene issues are considered during COVID-19. However, previous palmprint recognition methods are mainly focused on close-set scenario. In this paper, a novel Weight-based Meta Metric Learning (W2ML) method is proposed for accurate open-set touchless palmprint recognition, where only a part of categories is seen during training. Deep metric learning-based feature extractor is learned in a meta way to improve the generalization ability. Multiple sets are sampled randomly to define support and query sets, which are further combined into meta sets to constrain the set-based distances. Particularly, hard sample mining and weighting are adopted to select informative meta sets to improve the efficiency. Finally, embeddings with obvious inter-class and intra-class differences are obtained as features for palmprint identification and verification. Experiments are conducted on four palmprint benchmarks including fourteen constrained and unconstrained palmprint datasets. The results show that our W2ML method is more robust and efficient in dealing with open-set palmprint recognition issue as compared to the state-of-the-arts, where the accuracy is increased by up to 9.11% and the Equal Error Rate (EER) is decreased by up to 2.97%.","Biometrics, Palmprint recognition, Meta learning, Metric learning",Huikai Shao and Dexing Zhong,https://www.sciencedirect.com/science/article/pii/S0031320321004271,https://doi.org/10.1016/j.patcog.2021.108247,0031-3203,2022,108247,121,Pattern Recognition,Towards open-set touchless palmprint recognition via weight-based meta metric learning,article,SHAO2022108247,
"Deep anomaly detection, which utilizes neural networks to discover anomalies, is a vital research topic in pattern recognition. With the burgeoning of inference mechanism, inference-based methods show the promising performance. However, inference-based methods have two limitations: (1) they use an adversarial training way to learn data features. Such training way fails to learn task-specific features which can be conducive to capture the difference between normal and anomaly data. (2) The structure of detection network cannot capture the marginal distributions of normal data and corresponding features, which influences on the performance of anomaly detection. To overcome these limitations, this paper proposes a deep adversarial anomaly detection (DAAD) method. Specifically, an auxiliary task with self-supervised learning is first designed to learn task-specific features. Then a deep adversarial training (DAT) model is constructed to capture marginal distributions of normal data in different spaces. In addition, a majority voting strategy is applied to obtain reliable detection results. Experimental results on image and sequence datasets show that proposed method performs significantly better than many strong baselines.","Deep anomaly detection, Self-supervised learning, Adversarial training",Xianchao Zhang and Jie Mu and Xiaotong Zhang and Han Liu and Linlin Zong and Yuangang Li,https://www.sciencedirect.com/science/article/pii/S0031320321004155,https://doi.org/10.1016/j.patcog.2021.108234,0031-3203,2022,108234,121,Pattern Recognition,Deep anomaly detection with self-supervised learning and adversarial training,article,ZHANG2022108234,
"Squeeze-and-Excitation (SE) blocks have demonstrated significant accuracy gains for state-of-the-art deep architectures by re-weighting channel-wise feature responses. The SE block is an architecture unit that integrates two operations: a squeeze operation that employs global average pooling to aggregate spatial convolutional features into a channel feature, and an excitation operation that learns instance-specific channel weights from the squeezed feature to re-weight each channel. In this paper, we revisit the squeeze operation in SE blocks, and shed lights on why and how to embed rich (both global and local) information into the excitation module at minimal extra costs. In particular, we introduce a simple but effective two-stage spatial pooling process: rich descriptor extraction and information fusion. The rich descriptor extraction step aims to obtain a set of diverse (i.e., global and especially local) deep descriptors that contain more informative cues than global average-pooling. While, absorbing more information delivered by these descriptors via a fusion step can aid the excitation operation to return more accurate re-weight scores in a data-driven manner. We validate the effectiveness of our method by extensive experiments on ImageNet for image classification and on MS-COCO for object detection and instance segmentation. For these experiments, our method achieves consistent improvements over the SENets on all tasks, in some cases, by a large margin.","Convolutional neural networks, Squeeze-and-excitation, Spatial pooling, Base model",Xin Jin and Yanping Xie and Xiu-Shen Wei and Bo-Rui Zhao and Zhao-Min Chen and Xiaoyang Tan,https://www.sciencedirect.com/science/article/pii/S0031320321003460,https://doi.org/10.1016/j.patcog.2021.108159,0031-3203,2022,108159,121,Pattern Recognition,Delving deep into spatial pooling for squeeze-and-excitation networks,article,JIN2022108159,
"We present a novel learning-based approach to graph representations of road networks employing state-of-the-art graph convolutional neural networks. Our approach is applied to realistic road networks of 17 cities from Open Street Map. While edge features are crucial to generate descriptive graph representations of road networks, graph convolutional networks usually rely on node features only. We show that the highly representative edge features can still be integrated into such networks by applying a line graph transformation. We also propose a method for neighborhood sampling based on a topological neighborhood composed of both local and global neighbors. We compare the performance of learning representations using different types of neighborhood aggregation functions in transductive and inductive tasks and in supervised and unsupervised learning. Furthermore, we propose a novel aggregation approach, Graph Attention Isomorphism Network, GAIN1. Our results show that GAIN outperforms state-of-the-art methods on the road type classification problem.","Road network graphs, Graph representation learning, Line graph transformation, Neighborhood aggregation, Topological neighborhood",Zahra Gharaee and Shreyas Kowshik and Oliver Stromann and Michael Felsberg,https://www.sciencedirect.com/science/article/pii/S0031320321003617,https://doi.org/10.1016/j.patcog.2021.108174,0031-3203,2021,108174,120,Pattern Recognition,Graph representation learning for road type classification,article,GHARAEE2021108174,
"With the prevalence of multimedia content on the Web which usually continuously comes in a stream fashion, online cross-modal hashing methods have attracted extensive interest in recent years. However, most online hashing methods adopt a relaxation strategy or real-valued auxiliary variable strategy to avoid complex optimization of hash codes, leading to large quantization errors. In this paper, based on Discrete Latent Factor model-based cross-modal Hashing (DLFH), we propose a novel cross-modal online hashing method, i.e., Discrete Online Cross-modal Hashing (DOCH). To generate uniform high-quality hash codes of different modal, DOCH not only directly exploits the similarity between newly coming data and old existing data in the Hamming space, but also utilizes the fine-grained semantic information by label embedding. Moreover, DOCH can discretely learn hash codes by an efficient optimization algorithm. Extensive experiments conducted on two real-world datasets demonstrate the superiority of DOCH.","Cross-modal retrieval, Discrete optimization, Online hashing, Learning to hash",Yu-Wei Zhan and Yongxin Wang and Yu Sun and Xiao-Ming Wu and Xin Luo and Xin-Shun Xu,https://www.sciencedirect.com/science/article/pii/S0031320321004428,https://doi.org/10.1016/j.patcog.2021.108262,0031-3203,2022,108262,122,Pattern Recognition,Discrete online cross-modal hashing,article,ZHAN2022108262,
"To recover relative camera motion accurately and robustly, establishing a set of point-to-point correspondences in the pixel space is an essential yet challenging task in computer vision. Even though multi-scale design philosophy has been used with significant success in computer vision tasks, such as object detection and semantic segmentation, learning-based image matching has not been fully exploited. In this work, we explore a scale awareness learning approach in finding pixel-level correspondences based on the intuition that keypoints need to be extracted and described on an appropriate scale. With that insight, we propose a novel scale-aware network and then develop a new fusion scheme that derives high-consistency response maps and high-precision descriptions. We also revise the Second Order Similarity Regularization (SOSR) to make it more effective for the end-to-end image matching network, which leads to significant improvement in local feature descriptions. Experimental results run on multiple datasets demonstrate that our approach performs better than state-of-the-art methods under multiple criteria.","Keypoint detection, Keypoint description, Image matching, Structure from motion, 3D reconstruction",Xuelun Shen and Cheng Wang and Xin Li and Yifan Peng and Zijian He and Chenglu Wen and Ming Cheng,https://www.sciencedirect.com/science/article/pii/S0031320321004027,https://doi.org/10.1016/j.patcog.2021.108221,0031-3203,2022,108221,121,Pattern Recognition,Learning scale awareness in keypoint extraction and description,article,SHEN2022108221,
"In this paper, we propose a novel method named JSPNet, to segment 3D point cloud in semantic and instance simultaneously. First, we analyze the problem in addressing joint semantic and instance segmentation, including the common ground of cooperation of two tasks, conflict of two tasks, quadruplet relation between semantic and instance distributions, and ignorance of existing works. Then we introduce our method to reinforce mutual cooperation and alleviate the essential conflict. Our method has a shared encoder and two decoders to address two tasks. Specifically, to maintain discriminative features and characterize inconspicuous content, a similarity-based feature fusion module is designed to locate the inconspicuous area in the feature of current branch and then select related features from the other branch to compensate for the unclear content. Furthermore, given the salient semantic feature and the salient instance feature, a cross-task probability-based feature fusion module is developed to establish the probabilistic correlation between semantic and instance features. This module could transform features from one branch and further fuse them with the other branch by multiplying probabilistic matrix. Experimental results on a large-scale 3D indoor point cloud dataset S3DIS and a part-segmentation dataset ShapeNet have demonstrated the superiority of our method over existing state-of-the-arts in both semantic and instance segmentation. The proposed method outperforms PointNet with 12% and 26% improvements and outperforms ASIS with 2.7% and 4.3% improvements in terms of mIoU and mPre. Code of this work has been made available at https://github.com/Chenfeng1271/JSPNet.","Instance & semantic segmentation, Point could processing, Multi-task learning",Feng Chen and Fei Wu and Guangwei Gao and Yimu Ji and Jing Xu and Guo-Ping Jiang and Xiao-Yuan Jing,https://www.sciencedirect.com/science/article/pii/S0031320321004301,https://doi.org/10.1016/j.patcog.2021.108250,0031-3203,2022,108250,122,Pattern Recognition,JSPNet: Learning joint semantic & instance segmentation of point clouds via feature self-similarity and cross-task probability,article,CHEN2022108250,
"Pose-based action recognition has drawn considerable attention recently. Existing methods exploit the joint position to extract body-part features from the activation maps of the backbone CNN to assist human action recognition. However, there are two limitations: (1) the body-part features are independently used or simply concatenated to obtain a representation, where the prior knowledge about the structured correlations between body parts are not fully exploited; (2) the backbone CNN, from which the body-part features are extracted, is âlazyâ. It always contents itself with identifying patterns from the most discriminative areas of the input, which causes no information on the features extracted from other areas. This consequently hampers the performance of the followed aggregation process and makes the model easy to be misled by the training data bias. To address these problems, we encode the body-part features into a human-based spatiotemporal graph and employ a light-weight graph convolutional module to explicitly model the dependencies between body parts. Besides, we introduce a novel intermediate dense supervision to promote the backbone CNN to treat all regions equally, which is simple and effective, without extra parameters and computations. The proposed approach, namely, the pose-based graph convolutional network (PGCN), is evaluated on three popular benchmarks, where our approach significantly outperforms the state-of-the-art methods.","Action recognition, Skeleton",Lei Shi and Yifan Zhang and Jian Cheng and Hanqing Lu,https://www.sciencedirect.com/science/article/pii/S0031320321003575,https://doi.org/10.1016/j.patcog.2021.108170,0031-3203,2022,108170,121,Pattern Recognition,Action recognition via pose-based graph convolutional networks with intermediate dense supervision,article,SHI2022108170,
"In the Internet of Things enabled intelligent transportation systems, a huge amount of vehicle video data has been generated and real-time and accurate video analysis are very important and challenging work, especially in situations with complex street scenes. Therefore, we propose edge computing based video pre-processing to eliminate the redundant frames, so that we migrate the partial or all the video processing task to the edge, thereby diminishing the computing, storage and network bandwidth requirements of the cloud center, and enhancing the effectiveness of video analyzes. To eliminate the redundancy of the traffic video, the magnitude of motion detection based on spatio-temporal interest points (STIP) and the multi-modal linear features combination are presented which splits a video into super frame segments of interests. After that, we select the key frames from these interesting segments of the long videos with the design and detection of the prominent region. Finally, the extensive numerical experimental verification results show our methods are superior to the previous algorithms for different stages of the redundancy elimination, video segmentation, key frame selection and vehicle detection.","Video segmentation, Key frames extraction, Edge computing, YOLOv3",Shaohua Wan and Songtao Ding and Chen Chen,https://www.sciencedirect.com/science/article/pii/S0031320321003332,https://doi.org/10.1016/j.patcog.2021.108146,0031-3203,2022,108146,121,Pattern Recognition,Edge computing enabled video segmentation for real-time traffic monitoring in internet of vehicles,article,WAN2022108146,
"Networks derived from various disciplinary of sociality and nature are dynamic and incomplete, and temporal link prediction has wide applications in recommendation system and data mining system, etc. The current algorithms first obtain features by exploiting the topological or latent structure of networks, and then predict temporal links based on the obtained features. These algorithms are criticized by the separation of feature extraction and link prediction, which fails to fully characterize the dynamics of networks, resulting in undesirable performance. To overcome this problem, we propose a novel algorithm by joint multi-label learning and feature extraction (called MLjFE), where temporal link prediction and feature extraction are integrated into an overall objective function. The main advantage of MLjFE is that the features and parameter matrix for temporal link prediction are simultaneously learned during optimization procedure, which is more precise to capture dynamics of networks, improving the performance of algorithms. The experimental results on a number of artificial and real-world temporal networks demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods, showing joint learning with feature extraction and temporal link prediction is promising.","Temporal link prediction, Non-negative matrix factorization, Multi-label learning, Dynamic networks",Xiaoke Ma and Shiyin Tan and Xianghua Xie and Xiaoxiong Zhong and Jingjing Deng,https://www.sciencedirect.com/science/article/pii/S0031320321003976,https://doi.org/10.1016/j.patcog.2021.108216,0031-3203,2022,108216,121,Pattern Recognition,Joint multi-label learning and feature extraction for temporal link prediction,article,MA2022108216,
"Accurate segmentation of corneal layers depicted on optical coherence tomography (OCT) images is very helpful for quantitatively assessing and diagnosing corneal diseases (e.g., keratoconus and dry eye). In this study, we presented a novel boundary-guided convolutional neural network (CNN) architecture (BG-CNN) to simultaneously extract different corneal layers and delineate their boundaries. The developed BG-CNN architecture used three convolutional blocks to construct two network modules on the basis of the classical U-Net network. We trained and validated the network on a dataset consisting of 1,712 OCT images acquired on 121 subjects using a 10-fold cross-validation method. Our experiments showed an average dice similarity coefficient (DSC) of 0.9691, an intersection over union (IOU) of 0.9411, and a Hausdorff distance (HD) of 7.4423 pixels. Compared with several other classical networks, namely U-Net, Attention U-Net, Asymmetric U-Net, BiO-Net, CE-Net, CPFnte, M-Net, and Deeplabv3, on the same dataset, the developed network demonstrated a promising performance, suggesting its unique strength in segmenting corneal layers depicted on OCT images.","Corneal layers, OCT images, Segmentation, Convolutional neural networks",Lei Wang and Meixiao Shen and Qian Chang and Ce Shi and Yang Chen and Yuheng Zhou and Yanchun Zhang and Jiantao Pu and Hao Chen,https://www.sciencedirect.com/science/article/pii/S0031320321003459,https://doi.org/10.1016/j.patcog.2021.108158,0031-3203,2021,108158,120,Pattern Recognition,Automated delineation of corneal layers on OCT images using a boundary-guided CNN,article,WANG2021108158,
"The search cost of neural architecture search (NAS) has been largely reduced by differentiable architecture search and weight-sharing methods. Such methods optimize a super-network with all possible edges and operations, and determine the optimal sub-network by discretization, i.e., pruning off operations/edges of small weights. However, the discretization process performed on either operations or edges incurs significant inaccuracy and thus the quality of the architecture is not guaranteed. In this paper, we propose discretization-aware architecture search (DA2S), and target at pushing the super-network towards the configuration of desired topology. DA2S is implemented with an entropy-based loss term, which can be regularized to differentiable architecture search in a plug-and-play fashion. The regularization is controlled by elaborated continuation functions, so that discretization is adaptive to the dynamic change of edges and operations. Experiments on standard image classification benchmarks demonstrate the effectiveness of our approach, in particular, under imbalanced network configurations that were not studied before. Code is available at github.com/sunsmarterjie/DAAS.","Neural architecture search, Weight-sharing, Discretization-aware, Imbalanced network configuration",Yunjie Tian and Chang Liu and Lingxi Xie and Jianbin jiao and Qixiang Ye,https://www.sciencedirect.com/science/article/pii/S0031320321003733,https://doi.org/10.1016/j.patcog.2021.108186,0031-3203,2021,108186,120,Pattern Recognition,Discretization-aware architecture search,article,TIAN2021108186,
"Gait can be used to recognize people in an uncooperative and noninvasive manner and it is hard to imitate or counterfeit, which makes it suitable for video surveillance. The current solutions for gait recognition are still not robust to handle the conditions when the view angles of the gallery and query are different. We improve the performance of cross-view gait recognition from the perspective of metric learning. Specifically, we propose to use angular softmax loss to impose an angular margin for extracting separable features. At the same time, we use triplet loss to make the extracted features more discriminative. Additionally, we add a batch-normalization layer after extracting gait features to effectively optimize two different losses. We evaluate our approach on two widely-used gait dataset: CASIA-B dataset and TUM GAID dataset. The experiment results show that our approach outperforms the prior state-of-the-art approaches, which shows the effectiveness of our approach.","Biometrics, Gait recognition, Computer vision, Metric learning, Angular softmax loss function, Triplet loss function",Feng Han and Xuejian Li and Jian Zhao and Furao Shen,https://www.sciencedirect.com/science/article/pii/S0031320321006956,https://doi.org/10.1016/j.patcog.2021.108519,0031-3203,2022,108519,125,Pattern Recognition,A unified perspective of classification-based loss and distance-based loss for cross-view gait recognition,article,HAN2022108519,
"The recent years have witnessed a resurgence on neural network. Many functional layers are stacked hierarchically to learn the high-level representations. Yet the large album of radar image with label information are scarce. The fitting power of deep architectures are therefore limited. Additionally, the coherent imaging mechanism inevitably produce many speckles. They are with the statistical specificity of multiplicative noise, and hence make the image interpretation difficult. To solve the problems, this paper presents a new hierarchical receptive neural network. A signal-wise receptive module is first built by a family of delicate convolutional filters, with which the empirical features and knowledge are encoded. The receptive features are further refined in a patch-wise receptive unit, where some convolutional blocks are configured sequentially. The refined representations are finally used to make the inference. Multiple comparative studies are performed to demonstrate the advantage of proposed strategy.","Target recognition, Deep learning, Knowledge, Hierarchical receptive, SAR Image",Ganggang Dong and Hongwei Liu,https://www.sciencedirect.com/science/article/pii/S0031320322000395,https://doi.org/10.1016/j.patcog.2022.108558,0031-3203,2022,108558,126,Pattern Recognition,A hierarchical receptive network oriented to target recognition in SAR images,article,DONG2022108558,
"Recently, structured proximity matrix learning, which aims to learn a structured proximity matrix with explicit clustering structures from the first-order proximity matrix, has become the mainstream of graph-based clustering. However, the first-order proximity matrix always lacks several must-links compared to the groundtruth in real-world data, which results in a mismatched problem and affects the clustering performance. To alleviate this problem, this work introduces the high-order proximity to structured proximity matrix learning, and explores a novel framework named Adaptive-Order Proximity Learning (AOPL) to learn a consensus structured proximity matrix from the proximities of multiple orders. To be specific, AOPL selects the appropriate orders first, then assigns weights to these selected orders adaptively. In this way, a consensus structured proximity matrix is learned from the proximity matrices of appropriate orders. Based on AOPL framework, two practical models with different properties are derived, namely AOPL-Root and AOPL-Log. Besides, AOPL and the derived models are regarded as the same optimization problem subjected to some slightly different constraints. An efficient algorithm is proposed to solve them and the corresponding theoretical analyses are provided. Extensive experiments on several real-world datasets demonstrate superb performance of our model.","Graph-based clustering, Structured proximity matrix learning, High-order proximity, Adaptive learning",Danyang Wu and Wei Chang and Jitao Lu and Feiping Nie and Rong Wang and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320322000310,https://doi.org/10.1016/j.patcog.2022.108550,0031-3203,2022,108550,126,Pattern Recognition,Adaptive-order proximity learning for graph-based clustering,article,WU2022108550,
"In the context of pandemic, COVID-19, recognition of masked face images is a challenging problem, as most of the facial components become invisible. By utilizing prior information that mask-occlusion is located in the lower half of the face, we propose a dual-branch training strategy to guide the model to focus on the upper half of the face to extract robust features for Masked face recognition (MFR). During training, the features learned at the intermediate layers of the global branch are fed to our proposed attention module, named Upper Patch Attention (UPA), which acts as a local branch. Both branches are jointly optimized to enhance the feature extraction from non-occluded regions. We also propose a self-attention module, which integrates into the backbone network to enhance the interaction between the channels and spatial locations in the learning process. Extensive experiments on synthetic and real-masked face datasets demonstrate the effectiveness of our method.","Masked face recognition, Mask-occlusion, Attention module, Dual-branch training strategy",Yuxuan Zhang and Xin Wang and M. Saad Shakeel and Hao Wan and Wenxiong Kang,https://www.sciencedirect.com/science/article/pii/S0031320322000036,https://doi.org/10.1016/j.patcog.2022.108522,0031-3203,2022,108522,126,Pattern Recognition,Learning upper patch attention using dual-branch training strategy for masked face recognition,article,ZHANG2022108522,
"Detecting local features, such as corners, segments or blobs, is the first step in the pipeline of many Computer Vision applications. Its speed is crucial for real-time applications. In this paper we present ELSED, the fastest line segment detector in the literature. The key for its efficiency is a local segment growing algorithm that connects gradient-aligned pixels in presence of small discontinuities. The proposed algorithm not only runs in devices with very low end hardware, but may also be parametrized to foster the detection of short or longer segments, depending on the task at hand. We also introduce new metrics to evaluate the accuracy and repeatability of segment detectors. In our experiments with different public benchmarks we prove that our method accounts the highest repeatability and it is the most efficient in the literature.11Source code: https://github.com/iago-suarez/ELSED In the experiments we quantify the accuracy traded for such gain.","Image edge detection, Efficient line segment detection, Line segment detection evaluation",Iago SuÃ¡rez and JosÃ© M. Buenaposada and Luis Baumela,https://www.sciencedirect.com/science/article/pii/S0031320322001005,https://doi.org/10.1016/j.patcog.2022.108619,0031-3203,2022,108619,127,Pattern Recognition,ELSED: Enhanced line SEgment drawing,article,SUAREZ2022108619,
"Recently, baggage re-identification (ReID) has become an attractive topic in computer vision because it plays an important role in intelligent surveillance. However, the wide variations in different views of baggage items degrade baggage ReID performance. In this paper, a novel QuadNet is proposed to solve the multi-view problem in baggage ReID at three levels. At the sample level, we propose a multi-view sampling strategy which samples hard examples from multiple identities in multiple views. The sampled baggage items are used to construct quadruplets. At the feature level, view-aware attentional local features are extracted from discriminative regions in each view. These local features are fused with global features to obtain better representations of the quadruplets. At the loss level, a multi-view quadruplet loss operating on the representations of quadruplets is proposed to reduce the intra-class distances caused by view variations and increase the inter-class distances of baggage images captured in the same view. A random local blur data augmentation is proposed to handle the motion blur which is often found in baggage images. The multi-task learning of materials is introduced to obtain discriminative features based on the materials of baggage surfaces. Extensive experiments on three ReID datasets, MVB, Market-1501 and VeRi-776, indicate the remarkable effectiveness and good generalization of the QuadNet model. It has achieved the state-of-the-art performance on the three datasets.","Baggage re-identification, Multi-view learning, Quadruplet loss, View-aware features",Hao Yang and Xiuxiu Chu and Li Zhang and Yunda Sun and Dong Li and Stephen J. Maybank,https://www.sciencedirect.com/science/article/pii/S0031320322000279,https://doi.org/10.1016/j.patcog.2022.108546,0031-3203,2022,108546,126,Pattern Recognition,QuadNet: Quadruplet loss for multi-view learning in baggage re-identification,article,YANG2022108546,
"How to integrate various heterogeneous features for better recognition performance is increasingly critical for automatic target recognition. Existing integration methods present the following drawbacks: (1) most feature integration methods ignore the information, both common and discriminate knowledge, among different types of features; (2) most decision integration methods ignore the fact that different knowledge contributes differently; (3) the feature weights of integration model learned in the source domain cannot perform well in the target domain. To tackle these problems, we propose a deep Knowledge Integration framework by combining heterogeneous features for Domain Adaptive synthetic aperture radar (SAR) target recognition (KIDA). In the training phase, we implement deep knowledge integration at both feature and decision levels. At the feature level, to exploit the common and discriminative knowledge, multiple heterogeneous features are projected from the feature space into a unified label space by exploring the shared and specific structures simultaneously. The shared structure integrates common information in different features, while the specific structure reserves discriminative information of each type of feature. At the decision level, to reveal the relative importance of different knowledge, a decision integration strategy with feature weights is adopted in the label space. In the online testing phase, to improve the generalization of the model in dynamical environments, we employ online learning with sequential target domain knowledge to update the feature weights, thus achieving domain adaptation. Extensive experiments on different datasets validate the effectiveness and advantages of the proposed KIDA, especially in noisy environments.","Heterogeneous features, Knowledge integration, Domain adaptation, Online learning, Synthetic aperture radar",Yukun Zhang and Xiansheng Guo and Lin Li and Nirwan Ansari,https://www.sciencedirect.com/science/article/pii/S0031320322000711,https://doi.org/10.1016/j.patcog.2022.108590,0031-3203,2022,108590,126,Pattern Recognition,Deep knowledge integration of heterogeneous features for domain adaptive SAR target recognition,article,ZHANG2022108590,
"Unmanned aerial vehicle (UAV)-based tracking finds its applications in agriculture, aviation, navigation, transportation and public security, etc and develops rapidly recently. However, due to limitations of computing resources, battery capacity, requirement of low power and maximum load of UAV, the deployment of deep learning-based tracking algorithms in UAV is currently not feasible and therefore discriminative correlation filters (DCF)-based trackers have stood out in UAV tracking community for their high efficiency and appealing robustness on a single CPU. But confronted with difficult challenges the efficiency and accuracy of existing DCF-based approaches is still not satisfying. Inspired by the good optimization properties associated with residue representation, in this paper we exploit the residue nature inherent to videos and propose residue-aware correlation filters which demonstrate better convergence properties in filter learning. In addition, we propose a scale refinement strategy to improve the wildly adopted discriminative scale estimation in DCF-based trackers, which, in fact, greatly impacts the precision and accuracy of the trackers since accumulated scale error degrades the appearance model as online updating goes on. Extensive experiments are conducted on four UAV benchmarks, namely, UAV123@10fps, DTB70, UAVDT and Vistrone2018 (VisDrone2018-test-dev). The results show that our method achieves state-of-the-art performance in UAV tracking.","Residue-aware correlation filters, Discriminative scale estimation, GrabCut, Unmanned aerial vehicle (UAV) tracking",Shuiwang Li and Yuting Liu and Qijun Zhao and Ziliang Feng,https://www.sciencedirect.com/science/article/pii/S0031320322000954,https://doi.org/10.1016/j.patcog.2022.108614,0031-3203,2022,108614,127,Pattern Recognition,Learning residue-aware correlation filters and refining scale for real-time UAV tracking,article,LI2022108614,
"Dimensionality reduction plays a crucial role in classification, object detection, and pattern recognition tasks. Its main objective is to decrease the dimension of the original data while retaining the most distinctive information. With the emergence of deep learning, an autoencoder has become a state-of-the-art non-linear dimensionality-reduction method. Nonetheless, as the existing autoencoder models are devised to follow the data distribution and employ similarity techniques, preserving distinctive information can be problematic. To tackle this issue, we propose super-encoder (SE) networks trained in a supervised and cooperative manner. The SE consists of an encoder, separator, and decoder networks. The encoder combined with separator networks are dedicated to generating separable latent representation based on the label, and the decoder network should be able to reconstruct it to the original data simultaneously. Herein, we introduce a novel cooperative learning mechanism with a new loss function; therefore, the encoder, separator, and decoder networks can cooperate to achieve these objectives. Extensive experiments using benchmark datasets were conducted. The results indicated that the SE is more effective in extracting separable latent code than the existing supervised and unsupervised dimensionality-reduction models. Furthermore, as a generator, it can obtain highly competitive realistic images.","Autoencoder, Dimensionality reduction, Feature extraction, Pattern recognition, Cooperative neural networks",Imam Mustafa Kamal and Hyerim Bae,https://www.sciencedirect.com/science/article/pii/S0031320322000437,https://doi.org/10.1016/j.patcog.2022.108562,0031-3203,2022,108562,126,Pattern Recognition,Super-encoder with cooperative autoencoder networks,article,KAMAL2022108562,
"Label Distribution Learning (LDL) is a popular scenario for solving label ambiguity problems by learning the relative importance of each label to a particular instance. Nevertheless, the label is often incomplete due to the difficulty in annotating label distribution. In this mixing label case with complete and incomplete labels, it is often expected that the learning method can achieve better performance than the baseline method merely utilizing complete labeled data. However, the usage of incomplete labeled data may degrade the performance in real applications. Therefore, it is vital to design a safe incomplete LDL method, which will not deteriorate the performance when exploiting incomplete labeled data. To tackle this important but rarely studied problem, we propose a Safe Incomplete LDL method (SILDL), which learns a classifier that can prevent incomplete labeled instances from worsening the performance. Concretely, we learn predictions from multiple incomplete supervised learners and design an efficient solving algorithm by formulating it as a convex quadratic program. Theoretically, we prove that SILDL can obtain the maximal performance gain against the best one of the multiple baseline methods with mild conditions. Extensive experimental results validate the safeness of the proposed approach and show improvements in performance.","Label distribution learning, Safeness, Incomplete supervised learning",Jing Zhang and Hong Tao and Tingjin Luo and Chenping Hou,https://www.sciencedirect.com/science/article/pii/S0031320321006944,https://doi.org/10.1016/j.patcog.2021.108518,0031-3203,2022,108518,125,Pattern Recognition,Safe incomplete label distribution learning,article,ZHANG2022108518,
"In this paper, prediction with expert advice is surveyed focusing on Vovkâs Aggregating Algorithm. The established theory as well as extensions developed in the recent decade are considered. The paper is aimed at practitioners and covers important application scenarios.","Online learning, Prediction, Model selection",Yuri Kalnishkan,https://www.sciencedirect.com/science/article/pii/S0031320322000383,https://doi.org/10.1016/j.patcog.2022.108557,0031-3203,2022,108557,126,Pattern Recognition,Prediction with expert advice for a finite number of experts: A practical introduction,article,KALNISHKAN2022108557,
"Point cloud analysis is a fundamental task in 3D computer vision. Most previous works have conducted experiments on synthetic datasets with well-aligned data; while real-world point clouds are often not pre-aligned. How to achieve rotation invariance remains an open problem in point cloud analysis. To meet this challenge, we propose an approach toward achieving rotation-invariant (RI) representations by combining local geometry with global topology. In our local-global-representation (LGR)-Net, we have designed a two-branch network where one stream encodes local geometric RI features and the other encodes global topology-preserving RI features. Motivated by the observation that local geometry and global topology have different yet complementary RI responses in varying regions, two-branch RI features are fused by an innovative multi-layer perceptron (MLP) based attention module. To the best of our knowledge, this work is the first principled approach toward adaptively combining global and local information under the context of RI point cloud analysis. Extensive experiments have demonstrated that our LGR-Net achieves the state-of-the-art performance on various rotation-augmented versions of ModelNet40, ShapeNet, ScanObjectNN, and S3DIS.","Point cloud analysis, Rotation invariance, Deep learning, Classification, Segmentation",Chen Zhao and Jiaqi Yang and Xin Xiong and Angfan Zhu and Zhiguo Cao and Xin Li,https://www.sciencedirect.com/science/article/pii/S0031320322001078,https://doi.org/10.1016/j.patcog.2022.108626,0031-3203,2022,108626,127,Pattern Recognition,Rotation invariant point cloud analysis: Where local geometry meets global topology,article,ZHAO2022108626,
"Although unsupervised person re-identification (Re-ID) has drawn increasing research attention recently, it remains challenging to learn discriminative features without annotations across disjoint camera views. In this paper, we address the unsupervised person Re-ID with a conceptually novel yet simple framework, termed as Multi-label Learning guided self-paced Clustering (MLC). MLC mainly learns discriminative features with three crucial modules, namely a multi-scale network, a multi-label learning module, and a self-paced clustering module. Specifically, the multi-scale network generates multi-granularity person features in both global and local views. The multi-label learning module leverages a memory feature bank and assigns each image with a multi-label vector based on the similarities between the image and feature bank. After multi-label training for several epochs, the self-paced clustering joins in training and assigns a pseudo label for each image. The benefits of our MLC come from three aspects: i) the multi-scale person features for better similarity measurement, ii) the multi-label assignment based on the whole dataset ensures that every image can be trained, and iii) the self-paced clustering removes some noisy samples for better feature learning. Extensive experiments on three popular large-scale Re-ID benchmarks demonstrate that our MLC outperforms previous state-of-the-art methods and significantly improves the performance of unsupervised person Re-ID.","MLC, Multi-scale network, Multi-label learning, Self-paced clustering, Unsupervised person Re-ID",Qing Li and Xiaojiang Peng and Yu Qiao and Qi Hao,https://www.sciencedirect.com/science/article/pii/S0031320322000024,https://doi.org/10.1016/j.patcog.2022.108521,0031-3203,2022,108521,125,Pattern Recognition,Unsupervised person re-identification with multi-label learning guided self-paced clustering,article,LI2022108521,
"In this paper, we propose a new topic, Human-Centric Captioning, to mainly describe the human behavior in an image. Human activities and relationships are the primary objectives of visual understanding in daily applications. However, existing image captioning systems cannot differently treat humans and other objects, which limits the ability to understand and describe diverse human activities. As the first explorer of this new task, we build a novel Human-Centric COCO dataset concentrating on humans. Accordingly, we propose a novel Human-Centric Captioning Model (HCCM) that focuses on human-centric feature hierarchization and sentence generation. Specifically, our model first utilizes human body part-level knowledge to hierarchize the image features and then applies a novel three-branch captioning model to process these hierarchical features independently to calibrate the descriptions of human actions. Comprehensive experiments demonstrate that our HCCM achieves the state-of-the-art performance with BLEU-4, CIDEr and SPICE scores of 41.5, 127.3, 23.5 respectively. Dataset and code are publicly available at https://github.com/JohnDreamer/HCCM/.","Human-centric, Image captioning, Feature hierarchization",Zuopeng Yang and Pengbo Wang and Tianshu Chu and Jie Yang,https://www.sciencedirect.com/science/article/pii/S0031320322000267,https://doi.org/10.1016/j.patcog.2022.108545,0031-3203,2022,108545,126,Pattern Recognition,Human-Centric Image Captioning,article,YANG2022108545,
"Most deep learning-based super-resolution (SR) methods are not image-specific: 1) They are trained on samples synthesized by predefined degradations (e.g.bicubic downsampling), regardless of the domain gap between training and testing data. 2) During testing, they super-resolve all images by the same set of model weights, ignoring the degradation variety. As a result, most previous methods may suffer a performance drop when the degradations of test images are unknown and various (i.e.the case of blind SR). To address these issues, we propose an online SR (ONSR) method. It does not rely on predefined degradations and allows the model weights to be updated according to the degradation of the test image. Specifically, ONSR consists of two branches, namely internal branch (IB) and external branch (EB). IB could learn the specific degradation of the given test LR image, and EB could learn to super resolve images degraded by the learned degradation. In this way, ONSR could customize a specific model for each test image, and thus get more robust to various degradations. Extensive experiments on both synthesized and real-world images show that ONSR can generate more visually favorable SR results and achieve state-of-the-art performance in blind SR.","Blind super-resolution, Online updating, Internal learning, External learning",Shang Li and Guixuan Zhang and Zhengxiong Luo and Jie Liu and Zhi Zeng and Shuwu Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322000942,https://doi.org/10.1016/j.patcog.2022.108613,0031-3203,2022,108613,127,Pattern Recognition,From general to specific: Online updating for blind super-resolution,article,LI2022108613,
"An effective approach for the task of face recognition is proposed in this paper, which formulates the problem as an enhanced nuclear norm based matrix regression model and explores the low-rank property of the reconstructed image. Previous works have already leveraged the nuclear norm to obtain a low-rank representation of the error image and get a promising recognition rate. Motivated by the low-rank property of the reconstructed image through theoretical observation, our model imposes the nuclear norm constraints not only on the representation residual but also on the reconstructed image. The proposed method preserves the 2D structural information of the error images and reconstructs images, which is significant for the face recognition tasks. To further improve the performance of the proposed model, we explore the impact of different regularization terms under various scenarios. Extensive experiments on several benchmark datasets show the efficacy of the proposed model especially in terms of robustness against contiguous occlusion and illumination changes, which achieves superior performance over the most competitive methods.","Face recognition, Occluded image, Nuclear norm, Low-Rank",Qin Li and Huihui He and Hong Lai and Tie Cai and Qianqian Wang and QuanXue Gao,https://www.sciencedirect.com/science/article/pii/S0031320322000668,https://doi.org/10.1016/j.patcog.2022.108585,0031-3203,2022,108585,126,Pattern Recognition,Enhanced nuclear norm based matrix regression for occluded face recognition,article,LI2022108585,
"Due to the social nature of human beings, group activities have become an integral part of daily life. This creates the need for an in-depth study of the group-recommendation task: recommending items to a group of users. Unlike individual decision-making, which relies primarily on personal preferences, group decision-making is a process of negotiation and agreement among group members, in which social characteristics are a critical factor in achieving positive recommendation results. Therefore, in this paper, we propose a new model to solve the group recommendation problem from both global and local social networks. In a global network, a userâs social influence spreads through social connections and affects the preferences of others. In a local network, group members may contribute differently to the final decision, forming a dynamic negotiation and consensus process. We propose to model global and local networks with two components: 1) an attentive graph convolutional network based global network diffusion (GND) module to simulate the spread of social influence and capture the social gate of each user, and 2) a multi-channel attention-based local network fusion (LNF) module to learn the complex decision-making process among group members and integrate them into a final representation of the group. Finally, two separate neural collaborative filtering (NCF) modules are presented to model group-item and user-item interactions, respectively, to enhance each other. Extensive experimental results from two real-world datasets show the effectiveness of our proposed model.","Group recommendation, Recommendation systems, Graph neural network, Social network analysis, Graph-based method",Youfang Leng and Li Yu,https://www.sciencedirect.com/science/article/pii/S0031320322000826,https://doi.org/10.1016/j.patcog.2022.108601,0031-3203,2022,108601,127,Pattern Recognition,Incorporating global and local social networks for group recommendations,article,LENG2022108601,
"Extracting road maps from high-resolution optical remote sensing images has received much attention recently, especially with the rapid development of deep learning methods. However, most of these CNN based approaches simply focused on multi-scale encoder architectures or multiple branches in neural networks, and ignored some inherent characteristics of the road surface. In this paper, we design a novel network for road extraction based on spatial enhanced and densely connected UNet, called SDUNet. SDUNet aggregates both the multi-level features and global prior information of road networks by combining the strengths of spatial CNN-based segmentation and densely connected blocks. To enhance the feature learning about prior information of road surface, a structure preserving model is designed to explore the continuous clues in the spatial level. Experimental results on two benchmark datasets show that the proposed method achieves the state-of-the-art performance, compared with previous approaches for road extraction. Code will be made available on https://github.com/MrStrangerYang/SDUNet.","Road extraction, Image segmentation, Remote sensing imagery, Spatial topology",Mengxing Yang and Yuan Yuan and Ganchao Liu,https://www.sciencedirect.com/science/article/pii/S0031320322000309,https://doi.org/10.1016/j.patcog.2022.108549,0031-3203,2022,108549,126,Pattern Recognition,SDUNet: Road extraction via spatial enhanced and densely connected UNet,article,YANG2022108549,
"The deep clustering algorithm can learn the latent features of the embedded subspace, and further realize the clustering of samples in the feature space. The existing deep clustering algorithms mostly integrate neural networks and traditional clustering algorithms. However, for sample sets with many noise points, the effect of the clustering remains unsatisfactory. To address this issue, we propose an improved deep convolutional embedded clustering algorithm using reliable samples (IDCEC) in this paper. The algorithm first uses the convolutional autoencoder to extract features and cluster the samples. Then we select reliable samples with pseudo-labels and pass them to the convolutional neural network for training to get a better clustering model. We construct a new loss function for backpropagation training and implement an unsupervised deep clustering method. To verify the performance of the method proposed in this paper, we conducted experimental tests on standard data sets such as MNIST and USPS. Experimental results show that our method has better performance compared to traditional clustering algorithms and the state-of-the-art deep clustering algorithm under four clustering metrics.","Unsupervised clustering, Deep embedded clustering, Autoencoder, Reliable samples",Hu Lu and Chao Chen and Hui Wei and Zhongchen Ma and Ke Jiang and Yingquan Wang,https://www.sciencedirect.com/science/article/pii/S0031320322000929,https://doi.org/10.1016/j.patcog.2022.108611,0031-3203,2022,108611,127,Pattern Recognition,Improved deep convolutional embedded clustering with re-selectable sample training,article,LU2022108611,
"Instance segmentation is one of the most challenging tasks in computer vision, which requires separating each instance in pixels. To date, a low-resolution binary mask is the dominant paradigm for representation of instance mask. For example, the size of the predicted mask in Mask R-CNN is usually 28Ã28. Generally, a low-resolution mask can not capture the object details well, while a high-resolution mask dramatically increases the training complexity. In this work, we propose a flexible and effective approach to encode the high-resolution structured mask to the compact representation which shares the advantages of high-quality and low-complexity. The proposed mask representation can be easily integrated into two-stage pipelines such as Mask R-CNN, improving mask AP by 0.9% on the COCO dataset, 1.4% on the LVIS dataset, and 2.1% on the Cityscapes dataset. Moreover, a novel single shot instance segmentation framework can be constructed by extending the existing one-stage detector with a mask branch for this instance representation. Our model shows its superiority over the explicit contour-based pipelines in accuracy with similar computational complexity. We also evaluate our method for video instance segmentation, achieving promising results on YouTube-VIS dataset. Code is available at: https://git.io/AdelaiDet","Mask encoding, Instance segmentation, Video instance segmentation",Rufeng Zhang and Tao Kong and Xinlong Wang and Mingyu You,https://www.sciencedirect.com/science/article/pii/S0031320321006816,https://doi.org/10.1016/j.patcog.2021.108505,0031-3203,2022,108505,124,Pattern Recognition,Mask encoding: A general instance mask representation for object segmentation,article,ZHANG2022108505,
"Graph-based clustering has been considered as an effective kind of method in unsupervised manner to partition various items into several groups, such as Spectral Clustering (SC). However, there are three species of drawbacks in SC: (1) The effects of clustering is sensitive to the affinity matrix that is fixed by original data. (2) The input affinity matrix is simply based on distance measurement, which lacks of clear physical meaning under probabilistic prediction. (3) Additional discretization procedures still need to be operated. To cope with these issues, we propose a new clustering model, which refers to Entropy Regularization for unsupervised Clustering with Adaptive Neighbors (ERCAN), to dynamically and simultaneously update affinity matrix and clustering results. Firstly, the maximized entropy regularization term is introduced in probability model to avoid trivial similarity distributions. Additionally, we newly introduce the Laplacian rank constraint with â0-norm to construct adaptive neighbors for sparsity and strength segmentation ability without extra discretization process. Finally, we present a novel monotonic function optimization method, which reveals the consistence between graph sparsity and neighbor assignment, to address the â0-norm constraint in alternative optimization process. Comprehensive experiments show the superiority of our method with promising results.","Unsupervised clustering, Similarity matrix, Entropy regularization, Trivial similarity distribution, Laplacian rank constraint, Adaptive neighbors",Jingyu Wang and Zhenyu Ma and Feiping Nie and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320321006932,https://doi.org/10.1016/j.patcog.2021.108517,0031-3203,2022,108517,125,Pattern Recognition,Entropy regularization for unsupervised clustering with adaptive neighbors,article,WANG2022108517,
"Although deep convolutional neural networks (DCNNs) have been proposed for prostate MR image segmentation, the effectiveness of these methods is often limited by inadequate semantic discrimination and spatial context modeling. To address these issues, we propose a Multi-scale Synergic Discriminative Network (MSD-Net), which includes a shared encoder, a segmentation decoder, and a boundary detection decoder. We further design the cascaded pyramid convolutional block and residual refinement block, and incorporate them and the channel attention block into MSD-Net to exploit the multi-scale spatial contextual information and semantically consistent features of the gland. We also fuse the features from two decoders to boost the segmentation performance, and introduce the synergic multi-task loss to impose the consistence constraint on the joint segmentation and boundary detection. We evaluated MSD-Net against several prostate segmentation methods on three public datasets and achieved an improved accuracy. Our results indicate that the proposed MSD-Net outperforms existing methods with setting the new state-of-the-art for prostate segmentation in magnetic resonance images.","Prostate segmentation, Intra-class consistency, Inter-class discrimination, Synergic multi-task loss",Haozhe Jia and Weidong Cai and Heng Huang and Yong Xia,https://www.sciencedirect.com/science/article/pii/S0031320322000371,https://doi.org/10.1016/j.patcog.2022.108556,0031-3203,2022,108556,126,Pattern Recognition,Learning multi-scale synergic discriminative features for prostate image segmentation,article,JIA2022108556,
"Zero-shot sketch-based image retrieval (ZS-SBIR) has recently attracted the attention of the computer vision community due to its real-world applications, and the more realistic and challenging setting that it presents over SBIR. ZS-SBIR inherits the main challenges of multiple computer vision problems including content-based Image Retrieval (CBIR), zero-shot learning and domain adaptation. The majority of previous studies using deep neural networks have achieved improved results by either projecting sketch and images into a common low-dimensional space, or transferring knowledge from seen to unseen classes. However, those approaches are trained with complex frameworks composed of multiple deep convolutional neural networks (CNNs) and are dependent on category-level word labels. This increases the requirements for training resources and datasets. In comparison, we propose a simple and efficient framework that does not require high computational training resources, and learns the semantic embedding space from a vision model rather than a language model, as is done by related studies. Furthermore, at training and inference stages our method only uses a single CNN. In this work, a pre-trained ImageNet CNN (i.e., ResNet50) is fine-tuned with three proposed learning objects: domain-balanced quadruplet loss, semantic classification loss, and semantic knowledge preservation loss. The domain-balanced quadruplet and semantic classification losses are introduced to learn discriminative, semantic and domain invariant features by considering ZS-SBIR as an object detection and verification problem. To preserve semantic knowledge learned with ImageNet and exploit it for unseen categories, the semantic knowledge preservation loss is proposed. To reduce computational cost and increase the accuracy of the semantic knowledge distillation process, ground-truth semantic knowledge is prepared in a class-oriented fashion prior to training. Extensive experiments are conducted on three challenging ZS-SBIR datasets: Sketchy Extended, TU-Berlin Extended and QuickDraw Extended. The proposed method achieves state-of-the-art results, and outperforms the majority of related works by a substantial margin.","Sketch-based image retrieval, Zero-shot learning, Knowledge distillation, Similarity learning",Osman Tursun and Simon Denman and Sridha Sridharan and Ethan Goan and Clinton Fookes,https://www.sciencedirect.com/science/article/pii/S0031320322000097,https://doi.org/10.1016/j.patcog.2022.108528,0031-3203,2022,108528,126,Pattern Recognition,An efficient framework for zero-shot sketch-based image retrieval,article,TURSUN2022108528,
"Gait recognition, as an attractive task in biometrics, remains challenging due to significant intra-class changes of clothing and pose variations across different cameras. Recent approaches mainly focus on silhouette-based gait mode, which is easy to model in Convolutional Neural Networks (CNNs). Compared with silhouettes, the dynamics of skeletons essentially convey more robust information, which is invariant to view and clothing changes. Conventional approaches for modeling skeletons usually rely on hand-crafted features or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we address the skeleton-based gait recognition task with a novel Symmetry-Driven Hyper Feature Graph Convolutional Network (SDHF-GCN), which goes beyond the limitations of previous approaches by automatically learning multiple dynamic patterns and hierarchical semantic features in a unified Graph Convolutional Network (GCN). This model involves three dynamic patterns: natural connection, temporal correlation and symmetric interaction, which enriches the description of dynamic patterns by exploiting symmetry perceptual principles. Furthermore, a hyper feature network is proposed to aggregate the hierarchical semantic features, including dynamic features at the high level, structured features at the intermediate level, and static features at the low level, which complement each other to enhance the discriminative ability. By integrating different patterns in the hierarchical structure, the model is able to generate versatile and discriminative representations, thus improving the recognition rate. On the CASIA-B and OUMVLP-Pose datasets, the proposed SDHF-GCN renders substantial improvements over mainstream methods, especially in the coat-wearing scenario, with superior robustness to covariate factors.","Dynamics of skeleton, Gait recognition, Graph convolutional networks, Symmetric interaction pattern, Hyper feature",Xiaokai Liu and Zhaoyang You and Yuxiang He and Sheng Bi and Jie Wang,https://www.sciencedirect.com/science/article/pii/S0031320322000012,https://doi.org/10.1016/j.patcog.2022.108520,0031-3203,2022,108520,125,Pattern Recognition,Symmetry-Driven hyper feature GCN for skeleton-based gait recognition,article,LIU2022108520,
"In this paper, we present two new algorithms for the fast and stable computation of high-order discrete orthogonal dual Hahn polynomials (DHPs). These algorithms are essentially based on the proposed computation method of the initial values of DHPs following the order n and the variable s. For both algorithms, a single stable value is computed, fully independent of the gamma function that is the source of the numerical overflow, and then the rest of DHPs values are computed recursively via the proposed recurrence scheme. By analyzing the DHPs matrix, we propose a new method, which allows ensuring the numerical stability of high-order DHPs and dual Hahn moments (DHMs) until the last order. This method is based on the use of appropriate stability conditions. The results of simulations and comparisons carried out show on one hand that the second algorithm with the stability condition allows to compute DHPs up to the order nÂ =Â 17,603 without propagation of numerical error. On the other hand, the performance of analyzing large-size signals and images by high-order DHMs computed by the proposed method significantly exceeds the existing methods in terms of numerical stability, accuracy of reconstruction and in terms of maximum size of the analyzed signals and images. After the acceptance of this paper, the proposed algorithms for high-order DHPs computation will be made publically available at https://github.com/AchrafDaoui/On-Computational-Aspects-of-High-Order-Dual-Hahn-Moments.","High-order dual Hahn polynomials, Orthogonal moments, Numerical stability, Signal and image reconstruction, High-order moments",Achraf Daoui and Hicham Karmouni and Mohamed Yamni and Mhamed Sayyouri and Hassan Qjidaa,https://www.sciencedirect.com/science/article/pii/S0031320322000772,https://doi.org/10.1016/j.patcog.2022.108596,0031-3203,2022,108596,127,Pattern Recognition,On computational aspects of high-order dual Hahn moments,article,DAOUI2022108596,
"Object detection methods draw increasing attention in deep learning based visual tracking algorithms due to their robust discrimination and powerful regression ability. To further explore the potential of object detection methods in the visual tracking task, there are two gaps that need to be bridged. The first is the difference in object definition. Object detection is class-specific while visual tracking is class-agnostic. Moreover, visual tracking needs to differentiate the target from intra-class distractors. The second is the difference in temporal dimension. Different from object detection which processes still-image, visual tracking concentrates on objects which vary continuously with time. In this paper, we propose a Detection to Tracking (D2T) framework to address the above issues and effectively transfer existing advanced detection methods to visual tracking task. Specifically, to bridge the gap of object definition, we propose a general-to-specific network that separates learning general object features and instance-level features. To make full use of the contextual information while adapting to the appearance variation of targets, we propose a temporal strategy combining short-term constraint and long-term updating. To the best of our knowledge, our D2T framework is the first universal framework which directly transfers deep learning based object detectors to visual tracking task. It provides a novel solution to visual object tracking, and it achieves superior performance in several public datasets.","Object tracking, Object detection, Transferring detection to tracking",Huai Qin and Changqian Yu and Changxin Gao and Nong Sang,https://www.sciencedirect.com/science/article/pii/S0031320322000255,https://doi.org/10.1016/j.patcog.2022.108544,0031-3203,2022,108544,126,Pattern Recognition,D2T: A Framework For transferring detection to tracking,article,QIN2022108544,
"Extracting meaningful representation is a key challenge for person re-identification (re-ID) task, especially in the absence of ground truth labels. However, existing unsupervised approaches simply utilize pseudo labels generated from clustering to supervise re-ID model and thus have not yet fully explored the semantic information existing in data itself. This also limits the representation capabilities of learned models. To address the above problem, we propose mask prediction (MaskPre) as a pretext task for unsupervised re-ID, such that the clustering network can capture more semantic information and separate the images into semantic clusters automatically. Specifically, MaskPre masks region-level features with dynamic dropblock layer to generate differently masked views of a single image. To predict the masked regions and bridge the domain gap across views, we design mask prediction head and moving-average model to learn visual consistency from still image and temporal consistency during training process. Meanwhile, we optimize the model by grouping the two masked views into the same cluster, thus enhancing the consistency across views. Experimental results on three public benchmark datasets show that our proposed method outperforms the existing state-of-the-art approaches.","Person re-identification, Domain adaptation, Unsupervised clustering, Mask prediction, Semantic cluster",Junhui Yin and Siqing Zhang and Jiyang Xie and Zhanyu Ma and Jun Guo,https://www.sciencedirect.com/science/article/pii/S0031320322000498,https://doi.org/10.1016/j.patcog.2022.108568,0031-3203,2022,108568,126,Pattern Recognition,Unsupervised person re-identification via simultaneous clustering and mask prediction,article,YIN2022108568,
"Providing computers with the ability to process handwriting is both important and challenging, since many difficulties (e.g., different writing styles, alphabets, languages, etc.) need to be overcome for addressing a variety of problems (text recognition, signature verification, writer identification, word spotting, etc.). This paper reviews the growing literature on off-line handwritten document analysis over the last thirty years. A sample of 5389 articles is examined using bibliometric techniques. Using bibliometric techniques, this paper identifies (i) the most influential articles in the area, (ii) the most productive authors and their collaboration networks, (iii) the countries and institutions that have led research on the topic, (iv) the journals and conferences that have published most papers, and (v) the most relevant research topics (and their related tasks and methodologies) and their evolution over the years.","Automatic document analysis, Off-line handwriting recognition, Writer identification, Signature verification, Bibliometrics, Science mapping",Victoria Ruiz-Parrado and Ruben Heradio and Ernesto Aranda-Escolastico and Ãngel SÃ¡nchez and JosÃ© F. VÃ©lez,https://www.sciencedirect.com/science/article/pii/S0031320321006890,https://doi.org/10.1016/j.patcog.2021.108513,0031-3203,2022,108513,125,Pattern Recognition,A bibliometric analysis of off-line handwritten document analysis literature (1990â2020),article,RUIZPARRADO2022108513,
"In big data applications, hierarchical time series prediction is an important element of decision-making and concerns the inherent aggregation consistency, which is maintained by reconciliation methods. The paper proposes a novel multiple alternative clustering time series analysis based hierarchical electricity time series prediction method. Instead of adhering the aggregation consistency passively, we first exploit time series mining to construct a hierarchy, and then apply an optimal reconciliation method to improve the prediction accuracy. In particular, k-means clustering method is employed to cluster time series for many times with different k so as to make a large number of time series clusters (patterns), and then the clusters (patterns) based hierarchies are constructed respectively. With the large number of clusters hierarchies and the original geographical hierarchy, an optimal aggregation consistency reconciliation based prediction approach is proposed. Furthermore, the sparse penalty is adapted in our method for âidealâ clusters selection to improve the prediction performance. Compared with the state-of-the-art methods on real-life datasets, our method achieves the improvement of 11.13% and 24.07% accurate one-step ahead forecasts on electricity load and solar power data respectively.","Hierarchical time series forecasting, Data mining, Machine learning",Yue Pang and Xiangdong Zhou and Junqi Zhang and Quan Sun and Jianbin Zheng,https://www.sciencedirect.com/science/article/pii/S003132032200036X,https://doi.org/10.1016/j.patcog.2022.108555,0031-3203,2022,108555,126,Pattern Recognition,Hierarchical electricity time series prediction with cluster analysis and sparse penalty,article,PANG2022108555,
"Newton-type greedy pursuit methods have been shown to work favorably for cardinality-constrained sparse learning problems. The appealing sparsity recovery performance of the existing Newton-type greedy pursuit methods, however, is typically guaranteed within a local neighborhood around the target solution. To address this limitation, we present in this paper a novel approximate Newton pursuit method for sparse learning with linear models. The computation procedure of our method iterates between constructing an inexact Newton-type quadratic majorization to the global empirical risk and solving the quadratic approximation via iterative hard thresholding. Provable global guarantees on mean squared prediction error, which is less understood for prior methods, are provided for our method. Numerical evidence is provided to show the advantages of our approach over the prior methods.","Sparse learning, Newton-type method, Linear models, Quadratic approximation, Iterative hard thresholding",Fanfan Ji and Hui Shuai and Xiao-Tong Yuan,https://www.sciencedirect.com/science/article/pii/S0031320322000413,https://doi.org/10.1016/j.patcog.2022.108560,0031-3203,2022,108560,126,Pattern Recognition,A globally convergent approximate Newton method for non-convex sparse learning,article,JI2022108560,
"Multi-label classification (MLC) is one of the challenging tasks in computer vision, where it confronts high dimensional problem both in output label and input feature spaces. This paper proposed solving MLC through multi-output residual embedding (MoRE), which learns appropriate distance metric by analyzing the residuals between input and output spaces. Unlike traditional MLC paradigms that learn relationships between label space and feature space, our proposed approach further learns a low-rank structure in residuals between input and output spaces. And it encodes such residual projection to achieve dimension reduction in label space, enhancing the performance of the proposed algorithm in processing high dimensional MLC task. Furthermore, considering the label correlations between instances and its neighbors, multiple residuals of instances neighbors are also incorporated into the proposed model to further learn more appropriate distance metric in the same way. Overall, with residual embedding learning from instances and their neighbors, the obtained metric can learn a more appropriate low-rank structure in label space to handle high dimensional problem in MLC. Experimental results on several data sets, such as Cal500, Corel5k, Bibtex, Delicious, Tmc2007, 20ng, Mirflickr and Rcv1s1, demonstrate the excellent predictive performance of MoRE among STOA methods, such as LMMO-kNN, M3MDC, KRAM, SEEM, CPLST, CSSP, FaIE.","Distance metric, Low-rank structure, Residual embedding",Siyu Liu and Xuehua Song and Zhongchen Ma and Ernest Domanaanmwi Ganaa and XiangJun Shen,https://www.sciencedirect.com/science/article/pii/S0031320322000656,https://doi.org/10.1016/j.patcog.2022.108584,0031-3203,2022,108584,126,Pattern Recognition,MoRE: Multi-output residual embedding for multi-label classification,article,LIU2022108584,
"The existing methods of fine-grained image recognition mainly devote to learning subtle yet discriminative features from the high-resolution input. However, their performance deteriorates significantly when they are used for low quality images because a lot of discriminative details of images are missing. We propose a discriminative information restoration and extraction network, termed as DRE-Net, to address the problem of low-resolution fine-grained image recognition, which has widespread application potential, such as shelf auditing and surveillance scenarios. DRE-Net is the first framework for weakly supervised low-resolution fine-grained image recognition and consists of two sub-networks: (1) fine-grained discriminative information restoration sub-network (FDR) and (2) recognition sub-network with the semantic relation distillation loss (SRD-loss). The first module utilizes the structural characteristic of minimum spanning tree (MST) to establish context information for each pixel by employing the spatial structures between each pixel and other pixels, which can help FDR focus on and restore the critical texture details. The second module employs the SRD-loss to calibrate recognition sub-network by transferring the correct relationships between every two pixels on the feature map. Meanwhile the SRD-loss can further prompt the FDR to recover reliable and accurate fine-grained details and guide the recognition sub-network to perceive the discriminative features from the correct relationships. Extensive experiments on three benchmark datasets and one retail product dataset demonstrate the effectiveness of our proposed framework.","Low-resolution, Fine-grained image recognition, Minimum spanning tree, Semantic relation distillation",Tiantian Yan and Jian Shi and Haojie Li and Zhongxuan Luo and Zhihui Wang,https://www.sciencedirect.com/science/article/pii/S0031320322001108,https://doi.org/10.1016/j.patcog.2022.108629,0031-3203,2022,108629,127,Pattern Recognition,Discriminative information restoration and extraction for weakly supervised low-resolution fine-grained image recognition,article,YAN2022108629,
"Arbitrarily shaped scene text detection has witnessed great development in recent years, and text detection using segmentation has been proven to an effective approach. However, problems caused by the diverse attributes of text instances, such as shapes, scales, and presentation styles (dense or sparse), persist. In this paper, we propose a novel text detector, termed DText, which can effectively formulate an arbitrarily shaped scene text detection task based on dynamic convolution. Our method can dynamically generate independent text-instance-aware convolutional parameters for each text instance from multi-features thus overcoming some intractable limitations of arbitrary text detection, such as the splitting of similar adjacent text, which poses challenges to fixed instance-shared convolutional parameters-based methods. Unlike standard segmentation methods relying on regions-of-interest bounding boxes, DText focuses on enhancing the flexibility of the network to retain details of instances from diverse resolutions while effectively improving prediction accuracy. Moreover, we propose encoding the shape and position information according to the characteristics of the text instance, termed text-shape sensitive position embedding. Thus, it can provide explicit shape and position information to the generator of the dynamic convolution parameters. Experiments on five benchmarks (Total-Text, SCUT-CTW1500, MSRA-TD500, ICDAR2015, and MLT) showed that our method achieves superior detection performance.","Scene text detection, Image segmentation, Arbitrary shape, Dynamic convolution",Ying Cai and Yuliang Liu and Chunhua Shen and Lianwen Jin and Yidong Li and Daji Ergu,https://www.sciencedirect.com/science/article/pii/S0031320322000899,https://doi.org/10.1016/j.patcog.2022.108608,0031-3203,2022,108608,127,Pattern Recognition,Arbitrarily shaped scene text detection with dynamic convolution,article,CAI2022108608,
"The key problem of multi-view clustering is to handle the inconsistency among multiple views. This article proposes an attention-based framework for multi-view clustering on Grassmann manifold (AMCGM). To be specific, the proposed AMCGM framework aims to learn a representative element on Grassmann manifold with the following four highlights: 1) AMCGM framework performs an attention-based weighted-learning scheme to capture the difference of views; 2) The clustering results can be directly generated by the structured graph learned via AMCGM, avoiding the randomness caused by traditional label-generation procedures, such as K-means clustering; 3) AMCGM has high extensibility since it can generate many multi-view clustering models on Grassmann manifold; 4) On Grassmann manifold, the relationship between the projection metric (PM)-based multi-view clustering model and squared projection metric (SPM)-based model is studied. Based on AMCGM framework, we propose some generated models and provide some useful conclusions. Moreover, to solve the optimization problems involved in the proposed AMCGM framework and generated models, we propose an efficiently iterative algorithm and provide rigorous convergence analysis. Extensive experimental results demonstrate the superb performance of our framework.","Multi-view clustering, Grassmann manifold, Principle angles, Attentive weighted-learning scheme",Danyang Wu and Xia Dong and Feiping Nie and Rong Wang and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320322000917,https://doi.org/10.1016/j.patcog.2022.108610,0031-3203,2022,108610,128,Pattern Recognition,An attention-based framework for multi-view clustering on Grassmann manifold,article,WU2022108610,
"Due to the huge commercial interests behind online reviews, a tremendous amount of spammers manufacture spam reviews for product reputation manipulation. To further enhance the influence of spam reviews, spammers often collaboratively post spam reviews within a short period of time, the activities of whom are called collective opinion spam campaign. The goals and members of the spam campaign activities change frequently, and some spammers also imitate normal purchases to conceal the identity, which makes the spammer detection challenging. In this paper, we propose an unsupervised network embedding-based approach to jointly exploiting different types of relations, e.g., direct common behavior relation, and indirect co-reviewed relation to effectively represent the relevances of users for detecting the collective opinion spammers. The average improvements of our method over the state-of-the-art solutions on dataset AmazonCn and YelpHotel are [14.09%,12.04%] and [16.25%,12.78%] in terms of AP and AUC, respectively.","Spam detection, Collective spammer, Network embedding, Signed network",Ziyang Wang and Wei Wei and Xian-Ling Mao and Guibing Guo and Pan Zhou and Sheng Jiang,https://www.sciencedirect.com/science/article/pii/S0031320321006889,https://doi.org/10.1016/j.patcog.2021.108512,0031-3203,2022,108512,125,Pattern Recognition,User-based network embedding for opinion spammer detection,article,WANG2022108512,
"3D point cloud reconstruction is an urgent task in computer vision for environment perception. Nevertheless, the reconstructed scene is inaccurate and incomplete, because the visibility of pixels is not taken into account by existing methods. In this paper, a cascaded network with a multiple cost volume aggregation module named ADR-MVSNet is proposed. Three improvements are presented in ADR-MVSNet. First, to improve the reconstruction accuracy and reduce the time complexity, an adaptive depth reduction module, which adaptively adjusts the depth range of the pixel through the confidence interval, is proposed. Second, to more accurately estimate the depth of occluded pixels in multiview images, a multiple cost volume aggregation module, in which Gini impurity is introduced to measure the confidence of pixel depth prediction, is proposed. Third, a multiscale photometric consistency filter module is proposed, which considers the information in multiple confidence maps at the same time and filters out outliers accurately to remove pixels with low confidence. Therefore, the accuracy of point cloud reconstruction is improved. The experimental results on the DTU and Tanks and Temple datasets demonstrate that ADR-MVSNet achieves highly accurate and highly complete reconstruction compared with state-of-the-art benchmarks.","3D point cloud reconstruction, Multiview stereo, Deep neural network, Cost volume",Ying Li and Zhijie Zhao and Jiahao Fan and Wenyue Li,https://www.sciencedirect.com/science/article/pii/S0031320321006920,https://doi.org/10.1016/j.patcog.2021.108516,0031-3203,2022,108516,125,Pattern Recognition,ADR-MVSNet: A cascade network for 3D point cloud reconstruction with pixel occlusion,article,LI2022108516,
"This work addresses the problem of adversarial robustness in deep neural network classification from an optimal class boundary estimation perspective. It is argued that increased model robustness to adversarial attacks can be achieved when the feature learning process is monitored by geometrically-inspired optimization criteria. To this end, we propose to learn hyperspherical class prototypes in the neural feature embedding space, along with training the network parameters. Three concurrent optimization functions for the intermediate hidden layer training data activations are devised, requiring items of the same class to be enclosed by the corresponding class prototype boundaries, to have minimum distance from their class prototype vector (i.e., hypersphere center) and to have maximum distance from the remainder hypersphere centers. Our experiments show that training standard classification model architectures with the proposed objectives, significantly increases their robustness to white-box adversarial attacks, without adverse (if not beneficial) effects to their classification accuracy.","Adversarial defense, Adversarial robustness, Hypersphere prototype loss, HCP loss",Vasileios Mygdalis and Ioannis Pitas,https://www.sciencedirect.com/science/article/pii/S0031320322000085,https://doi.org/10.1016/j.patcog.2022.108527,0031-3203,2022,108527,125,Pattern Recognition,Hyperspherical class prototypes for adversarial robustness,article,MYGDALIS2022108527,
"Salient object detection aims at highlighting the most visually distinctive objects in the scene. Previous deep learning based works mainly focus on designing different integration strategies of multi-level features to improve the quality of prediction. However, due to the negligence of spatial structure coherence in predicted saliency maps, they fail to produce satisfactory results in complex scenarios. In this work, we present a structure-aware dual pyramid network (SA-DPNet) for salient object detection. By explicitly formulating spatial location information and spatial covariance features into the self-attention mechanism, a structure-aware spatial non-local block is proposed in SA-DPNet to learn the spatial-sensitive global context. With the proposed edge loss and adversarial loss, the edge structure context and patch-based global structure context are introduced to refine the structural coherence of the predicted results. Comprehensive experimental results on six RGB saliency benchmark datasets and three RGB-D saliency benchmark datasets demonstrate the superiority of proposed SA-DPNet over other state-of-the-art methods, both quantitatively and visually.","Saliency detection, Structure coherence, Deep neural network",Xuemiao Xu and Jiaxing Chen and Huaidong Zhang and Guoqiang Han,https://www.sciencedirect.com/science/article/pii/S0031320322001054,https://doi.org/10.1016/j.patcog.2022.108624,0031-3203,2022,108624,127,Pattern Recognition,SA-DPNet: Structure-aware dual pyramid network for salient object detection,article,XU2022108624,
"Efficient neural networks has received ever-increasing attention with the evolution of convolutional neural networks (CNNs), especially involving their deployment on embedded and mobile platforms. One of the biggest problems to obtaining such efficient neural networks is efficiency, even recent differentiable neural architecture search (DNAS) requires to sample a small number of candidate neural architectures for the selection of the optimal neural architecture. To address this computational efficiency issue, we introduce a novel architecture parameterization based on scaled sigmoid function, and propose a general Differentiable Neural Architecture Learning (DNAL) method to obtain efficient neural networks without the need to evaluate candidate neural networks. Specifically, for stochastic supernets as well as conventional CNNs, we build a new channel-wise module layer with the architecture components controlled by a scaled sigmoid function. We train these neural network models from scratch. The network optimization is decoupled into the weight optimization and the architecture optimization, which avoids the interaction between the two types of parameters and alleviates the vanishing gradient problem. We address the non-convex optimization problem of efficient neural networks by the continuous scaled sigmoid method instead of the common softmax method. Extensive experiments demonstrate our DNAL method delivers superior performance in terms of efficiency, and adapts to conventional CNNs (e.g., VGG16 and ResNet50), lightweight CNNs (e.g., MobileNetV2) and stochastic supernets (e.g., ProxylessNAS). The optimal neural networks learned by DNAL surpass those produced by the state-of-the-art methods on the benchmark CIFAR-10 and ImageNet-1K dataset in accuracy, model size and computational complexity. Our source code is available at https://github.com/QingbeiGuo/DNAL.git.","Deep neural network, Convolutional neural network, Neural architecture search, Automated machine learning",Qingbei Guo and Xiao-Jun Wu and Josef Kittler and Zhiquan Feng,https://www.sciencedirect.com/science/article/pii/S0031320321006245,https://doi.org/10.1016/j.patcog.2021.108448,0031-3203,2022,108448,126,Pattern Recognition,Differentiable neural architecture learning for efficient neural networks,article,GUO2022108448,
"Accurate detection of COVID-19 is one of the challenging research topics in today's healthcare sector to control the coronavirus pandemic. Automatic data-powered insights for COVID-19 localization from medical imaging modality like chest CT scan tremendously augment clinical care assistance. In this research, a Contour-aware Attention Decoder CNN has been proposed to precisely segment COVID-19 infected tissues in a very effective way. It introduces a novel attention scheme to extract boundary, shape cues from CT contours and leverage these features in refining the infected areas. For every decoded pixel, the attention module harvests contextual information in its spatial neighborhood from the contour feature maps. As a result of incorporating such rich structural details into decoding via dense attention, the CNN is able to capture even intricate morphological details. The decoder is also augmented with a Cross Context Attention Fusion Upsampling to robustly reconstruct deep semantic features back to high-resolution segmentation map. It employs a novel pixel-precise attention model that draws relevant encoder features to aid in effective upsampling. The proposed CNN was evaluated on 3D scans from MosMedData and Jun Ma benchmarked datasets. It achieved state-of-the-art performance with a high dice similarity coefficient of 85.43% and a recall of 88.10%.","COVID-19, Segmentation, Deep learning, Attention, Decoder, CNN",R. Karthik and R. Menaka and Hariharan M and Daehan Won,https://www.sciencedirect.com/science/article/pii/S003132032200019X,https://doi.org/10.1016/j.patcog.2022.108538,0031-3203,2022,108538,125,Pattern Recognition,Contour-enhanced attention CNN for CT-based COVID-19 segmentation,article,KARTHIK2022108538,
"The fusion of one-class classifiers (OCCs) has been shown to exhibit promising performance in a variety of machine learning applications. The ability to assess the similarity or correlation between the output of various OCCs is an important prerequisite for building of a meaningful OCCs ensemble. However, this aspect of the OCC fusion problem has been mostly ignored so far. In this paper, we propose a new method of constructing a fusion of OCCs with three contributions: (a) As a key contribution, enabling an OCC ensemble design using exclusively non anomalous samples, we propose a novel fitness function to evaluate the competency of OCCs without requiring samples from the anomalous class; (b) As a minor, but impactful contribution, we investigate alternative forms of score normalisation of OCCs, and identify a novel two-sided normalisation method as the best in coping with long tail non anomalous data distributions; (c) In the context of building our proposed OCC fusion system based on the weighted averaging approach, we find that the weights optimised using a particle swarm optimisation algorithm produce the most effective solution. We evaluate the merits of the proposed method on 15 benchmarking datasets from different application domains including medical, anti-spam and face spoofing detection. The comparison of the proposed approach with state-of-the-art methods alongside the statistical analysis confirm the effectiveness of the proposed model.","Anomaly detection, One-class classification, Score normalisation, Face spoofing detection, Convolutional neural network",Soroush Fatemifar and Muhammad Awais and Ali Akbari and Josef Kittler,https://www.sciencedirect.com/science/article/pii/S0031320321006762,https://doi.org/10.1016/j.patcog.2021.108500,0031-3203,2022,108500,124,Pattern Recognition,Developing a generic framework for anomaly detection,article,FATEMIFAR2022108500,
"Domain generalization (DG) and unsupervised domain adaptation (UDA) aim to solve the domain-shift problem that arises when the trained model is tested in the domain with different style distribution from the training data. Style Normalization and Restitution(SNR) has solved this problem to a certain extent and achieved the best performance. However, SNR ignores the discriminative information encoded in the appearance style information, which limits the performance of the model. In this paper, we propose Two-level Style Normalization and Restitution(Tl-SNR) to solve this problem. First, we use group whitening to introduce the appearance style information encoded in the second-order statistic into the SNR, which prepares for restituting the task-relevant discriminative information in the appearance information later. Secondly, we defined dynamic affine parameters, which improves the affine parameters in group whitening. It makes the model adjust adaptively according to the characteristics of the sample, so as to better exploit the capabilities of the model. Finally, we designed a Two-level Style Normalization and Restitution module based on the improved group whitening for domain generalization and unsupervised domain adaptation. Extensive experiments show that our method is effective. And our method outperforms state-of-the-art DG and UDA methods on four benchmarks.","Domain generalization, Unsupervised domain adaptation, Two-level style normalization and restitution, Second-order statistics, Dynamic affine parameter",Hao Wang and Xiaojun Bi,https://www.sciencedirect.com/science/article/pii/S0031320322000760,https://doi.org/10.1016/j.patcog.2022.108595,0031-3203,2022,108595,127,Pattern Recognition,Domain generalization and adaptation based on second-order style information,article,WANG2022108595,
"In this paper, we propose an online model optimization algorithm based on reinforcement learning for quantitative trading. The combination of prediction model and trading policy is the most commonly used framework in practical quantitative trading. Integrated with machine learning methods, this framework brings huge profits to quantified companies. In the framework, the prediction model is used to predict future trading price trend, and the trading policy is used to determine the price and number of orders. Even though, the shortcomings of machine learning models are obvious, mainly are, (1) Slow prediction speed. Huge human-craft features and model computing cost much time, which is ten times of pure trading policy without model. (2) Poor generalization. This kind of models can hardly adapt to market data in each period, because market traders will change time to time at micro level, thus the distribution of market data will change. But current model is trained on a long period dataset, it achieves best effect at average, but can not adapt to different market at each period. To address this problem, we propose a novel online model optimization algorithm. A light model library will be constructed. Each light model in this library corresponds to a different market distribution. By devising the appropriate reward function via inverse reinforcement learning algorithm, the algorithm can accurately estimate the profits of each model. Then the model can be selected automatically in real-time trading, so that the trading policies can automatically adapt to changes in trading market, overcoming previous shortcoming of manually updating model and slow prediction speed. Experimental results show that the proposed algorithm achieves state-of-the-art performance on China Commodity Futures Market Data.","High-frequency trading, Inverse reinforcement learning, Parameter optimization, Multi-armed bandit",Weipeng Zhang and Ning Zhang and Junchi Yan and Guofu Li and Xiaokang Yang,https://www.sciencedirect.com/science/article/pii/S0031320322000243,https://doi.org/10.1016/j.patcog.2022.108543,0031-3203,2022,108543,125,Pattern Recognition,Auto uning of price prediction models for high-frequency trading via reinforcement learning,article,ZHANG2022108543,
"Visual attention has proven to be effective in improving the performance of person re-identification. Most existing methods apply visual attention heuristically by learning an additional attention map to re-weight the feature maps for person re-identification, however, this kind of methods inevitably increase the model complexity and inference time. In this paper, we propose to incorporate the ability of predicting attention maps as additional objectives in a person ReID network without changing the original structure, thus maintain the same inference time and model size. Two kinds of attention maps have been considered to make the learned feature maps being aware of the person and related body parts respectively. Globally, a holistic attention branch (HAB) is proposed to make the feature maps obtained by backbone could focus on persons so as to alleviate the influence of background. Locally, a partial attention branch (PAB) is proposed to make the extracted features can be decoupled into several groups that are separately responsible for different body parts, thus increasing the robustness to pose variation and partial occlusion. These two kinds of attentions are universal and can be incorporated into existing ReID networks. We have tested its performance on two typical networks (TriNetÂ [1] and Bag of TricksÂ [2]) and observed significant performance improvement on five widely used datasets.","Person re-identification, Attention learning, Multi-task learning",Yifan Chen and Han Wang and Xiaolu Sun and Bin Fan and Chu Tang and Hui Zeng,https://www.sciencedirect.com/science/article/pii/S0031320322000486,https://doi.org/10.1016/j.patcog.2022.108567,0031-3203,2022,108567,126,Pattern Recognition,Deep attention aware feature learning for person re-Identification,article,CHEN2022108567,
"In shape from focus (SFF) methods, the quality of depth map is mainly dependent on the accuracy level of image focus volume. Most of the SFF techniques optimize focus volume without incorporating any prior or additional structural information about the scene and thus resultant depth maps are deteriorated. We mitigate this deficiency by proposing to optimize focus volume through energy minimization. The proposed energy function contains smoothness and structural similarity along with data term. Smoothness constraint enforces spatial coherence while structural similarity constraint tries to preserve structures which are consistent with image sequence. This results in an optimized focus volume that imitates the underlying scene accurately. For the implementation of our 3D objective function, we employ an efficient technique that decomposes the problem into a sequence of 1D simple sub-problems. Experiments conducted on synthetic and real image sequences from a variety of datasets demonstrate that the proposed method optimizes the focus volume effectively and thus provides improved depth maps.","Shape from focus, Energy minimization, Focus volume optimization",Usman Ali and Muhammad Tariq Mahmood,https://www.sciencedirect.com/science/article/pii/S0031320322000401,https://doi.org/10.1016/j.patcog.2022.108559,0031-3203,2022,108559,126,Pattern Recognition,Energy minimization for image focus volume in shape from focus,article,ALI2022108559,
"Multi-instance learning (MIL) is able to cope with the weakly supervised problems where the training data is represented by labeled bags consisting of multiple unlabeled instances. Due to its practical significance, MIL has recently drawn increasing attention. Introducing bag representations is an attractive way to learn MIL data. However, it is difficult for the existing MIL methods to utilize both implicit and explicit bag representations simultaneously. In this paper, we propose a bag dissimilarity regularized (BDR) framework that incorporates multiple bag representations regardless of explicitness or implicitness. Here, the implicit bag representations are incorporated into a regularization term that contains the intrinsic geometric information provided by the bag dissimilarities. The regularization term can be added to the objective function of supervised classifiers. An effective method for explicit bag embedding is also proposed, which exploits the Fisher score derived from factor analysis. Finally, we propose two specific BDR methods based on support vector machine and broad learning system. The proposed BDR methods are evaluated on 14 datasets, and have achieved competitive results with limited computation consumption. We also discuss the effectiveness and the characteristics of BDR framework.","Multi-instance learning (MIL), Dissimilarity regularization, Fisher score",Shiluo Huang and Zheng Liu and Wei Jin and Ying Mu,https://www.sciencedirect.com/science/article/pii/S0031320322000644,https://doi.org/10.1016/j.patcog.2022.108583,0031-3203,2022,108583,126,Pattern Recognition,Bag dissimilarity regularized multi-instance learning,article,HUANG2022108583,
"In this paper, we introduce new rotation moment invariants, which are composed of non-separable Appell moments. We prove that Appell polynomials behave under rotation as monomials, which enables easy construction of the invariants. We show by extensive tests that non-separable moments may outperform the separable ones in terms of recognition power and robustness thanks to aÂ better distribution of their zero curves over the image space.","Image recognition, Rotation invariants, Non-separable moments, Appell polynomials, Bi-orthogonality, Recurrent relation",Leonid Bedratyuk and Jan Flusser and TomÃ¡Å¡ Suk and Jitka KostkovÃ¡ and Jaroslav Kautsky,https://www.sciencedirect.com/science/article/pii/S0031320322000887,https://doi.org/10.1016/j.patcog.2022.108607,0031-3203,2022,108607,127,Pattern Recognition,Non-separable rotation moment invariants,article,BEDRATYUK2022108607,
"This article presents SVC-onGoing11https://competitions.codalab.org/competitions/27295., an on-going competition for on-line signature verification where researchers can easily benchmark their systems against the state of the art in an open common platform using large-scale public databases, such as DeepSignDB22https://github.com/BiDAlab/DeepSignDB. and SVC2021_EvalDB33https://github.com/BiDAlab/SVC2021_EvalDB., and standard experimental protocols. SVC-onGoing is based on the ICDAR 2021 Competition on On-Line Signature Verification (SVC 2021), which has been extended to allow participants anytime. The goal of SVC-onGoing is to evaluate the limits of on-line signature verification systems on popular scenarios (office/mobile) and writing inputs (stylus/finger) through large-scale public databases. Three different tasks are considered in the competition, simulating realistic scenarios as both random and skilled forgeries are simultaneously considered on each task. The results obtained in SVC-onGoing prove the high potential of deep learning methods in comparison with traditional methods. In particular, the best signature verification system has obtained Equal Error Rate (EER) values of 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3). Future studies in the field should be oriented to improve the performance of signature verification systems on the challenging mobile scenarios of SVC-onGoing in which several mobile devices and the finger are used during the signature acquisition.","SVC-onGoing, SVC 2021, Biometrics, Handwriting, Signature verification, DeepSignDB, SVC2021_EvalDB",Ruben Tolosana and Ruben Vera-Rodriguez and Carlos Gonzalez-Garcia and Julian Fierrez and Aythami Morales and Javier Ortega-Garcia and Juan {Carlos Ruiz-Garcia} and Sergio Romero-Tapiador and Santiago Rengifo and Miguel Caruana and Jiajia Jiang and Songxuan Lai and Lianwen Jin and Yecheng Zhu and Javier Galbally and Moises Diaz and Miguel {Angel Ferrer} and Marta Gomez-Barrero and Ilya Hodashinsky and Konstantin Sarin and Artem Slezkin and Marina Bardamova and Mikhail Svetlakov and Mohammad Saleem and Cintia {Lia Szcs} and Bence Kovari and Falk Pulsmeyer and Mohamad Wehbi and Dario Zanca and Sumaiya Ahmad and Sarthak Mishra and Suraiya Jabin,https://www.sciencedirect.com/science/article/pii/S0031320322000905,https://doi.org/10.1016/j.patcog.2022.108609,0031-3203,2022,108609,127,Pattern Recognition,SVC-onGoing: Signature verification competition,article,TOLOSANA2022108609,
"Accurately assessing the medical image segmentation quality of the automatically generated predictions is essential for guaranteeing the reliability of the results of computer-assisted diagnosis (CAD). Many researchers have studied segmentation quality estimation without labeled ground truths. Recently, a novel idea is proposed, which transforms segmentation quality assessment (SQA) into the pixel-wise or voxel-wise error map segmentation task. However, the simple application of vanilla segmentation structures in medical domain fails to achieve satisfactory error segmentation results. In this paper, we propose collaborative boundary-aware context encoding networks called EP-Net for error segmentation task. Specifically, we propose a collaborative feature transformation branch for better feature fusion between images and masks, and precise localization of error regions. Further, we propose a context encoding module to utilize the global predictor from the error map to enhance the feature representation and regularize the networks. Extensive experiments on IBSR V2.0 dataset, ACDC dataset and M&Ms dataset demonstrate that EP-Net achieves better error segmentation results compared with the traditional segmentation patterns. Based on error prediction results, we obtain a proxy metric of segmentation quality, which has high Pearson correlation coefficient with the real segmentation accuracy on all datasets.","Segmentation quality assessment, Error map prediction, Medical image segmentation",Zhenxi Zhang and Chunna Tian and Xinbo Gao and Jie Li and Zhicheng Jiao and Cui Wang and Zhusi Zhong,https://www.sciencedirect.com/science/article/pii/S0031320321006919,https://doi.org/10.1016/j.patcog.2021.108515,0031-3203,2022,108515,125,Pattern Recognition,Collaborative boundary-aware context encoding networks for error map prediction,article,ZHANG2022108515,
"The Synthetic Minority Over-sampling Technique (SMOTE) is a well-known resampling strategy that has been successfully used for dealing with the class-imbalance problem, one of the most challenging pattern recognition tasks in the last two decades. In this work, we claim that SMOTE has an important issue when defining the neighborhood in order to create new minority samples: the use of the Euclidean distance may not be suitable in high-dimensional settings. Our hypothesis is that the use of a weighted metric that does not assume that all features are equally important could improve performance in the presence of noisy/redundant variables. In this line, we present a novel SMOTE-like method that uses the weighted Minkowski distance for defining the neighborhood for each example of the minority class. This methodology leads to a better definition of the neighborhood since it prioritizes those features that are more relevant for the classification task. A complementary advantage of the proposal is performing feature selection since attributes can be discarded when their corresponding weights are below a given threshold. Our experiments on 42 class-imbalance datasets show the virtues of the proposed SMOTE variant, achieving the best predictive performance when compared with the traditional SMOTE approach and other recent variants on low- and high-dimensional settings, handling issues such as class overlap and hubness adequately without increasing the complexity of the method.","Data resampling, SMOTE, OWA Operators, Feature selection, Imbalanced data classification",SebastiÃ¡n Maldonado and Carla Vairetti and Alberto Fernandez and Francisco Herrera,https://www.sciencedirect.com/science/article/pii/S0031320321006877,https://doi.org/10.1016/j.patcog.2021.108511,0031-3203,2022,108511,124,Pattern Recognition,FW-SMOTE: A feature-weighted oversampling approach for imbalanced classification,article,MALDONADO2022108511,
"Recipe generation from food images and ingredients is a challenging task, which requires the interpretation of the information from another modality. Different from the image captioning task, where the captions usually have one sentence, cooking instructions contain multiple sentences and have obvious structures. To help the model capture the recipe structure and avoid missing some cooking details, we propose a novel framework: Decomposing Generation Networks (DGN) with structure prediction, to get more structured and complete recipe generation outputs. Specifically, we split each cooking instruction into several phases, and assign different sub-generators to each phase. Our approach includes two novel ideas: (i) learning the recipe structures with the global structure prediction component and (ii) producing recipe phases in the sub-generator output component based on the predicted structure. Extensive experiments on the challenging large-scale Recipe1M dataset validate the effectiveness of our proposed model, which improves the performance over the state-of-the-art results.","Text generation, Vision-and-language",Hao Wang and Guosheng Lin and Steven C.H. Hoi and Chunyan Miao,https://www.sciencedirect.com/science/article/pii/S0031320322000590,https://doi.org/10.1016/j.patcog.2022.108578,0031-3203,2022,108578,126,Pattern Recognition,Decomposing generation networks with structure prediction for recipe generation,article,WANG2022108578,
"Feature matching is used to build correspondences between features in the model and test images. As the extension of graph matching, hypergraph matching is able to encode rich invariance between feature tuples and improve matching accuracy. Different from many existing algorithms based on maximizing the matching score between correspondences, our approach formulates hypergraph matching as a non-cooperative multi-player game and obtains matches by extracting the evolutionary stable strategies (ESS). While this approach generates a high matching accuracy, the number of matches is usually small and it involves a large computation load to obtain more matches. To solve this problem, we extract multiple ESS clusters instead of one single ESS group, thereby transforming hypergraph matching of features to hypergraph clustering of candidate matches. By extracting an appropriate number of clusters, we increase the number of matches efficiently, and improve the matching accuracy by imposing the one-to-one constraint. In experiments with three real datasets, our algorithm is shown to generate a large number of matches efficiently. It also shows significant advantage in matching accuracy in comparison with some other hypergraph matching algorithms.","Feature matching, Hypergraph matching, Game-theoretic, Hypergraph clustering",Jian Hou and Marcello Pelillo and Huaqiang Yuan,https://www.sciencedirect.com/science/article/pii/S0031320322000073,https://doi.org/10.1016/j.patcog.2022.108526,0031-3203,2022,108526,125,Pattern Recognition,Hypergraph matching via game-theoretic hypergraph clustering,article,HOU2022108526,
"Due to the randomness and contingency of wind speed size and direction, it is difficult to predict the wind speed accurately, which seriously affects the stable operation of the power system. To improve the operation stability of power system, the accurate prediction of wind speed is very important. In this paper, a new hybrid model based on gray wolf algorithm (GWO) and support vector machine (SVM) for wind speed prediction is proposed. Firstly, Neo4j(NE) is utilized to identify the data and preprocess the data. Secondly, k-means clustering(KC) is utilized to analyze data and eliminate invalid data. Thirdly, GWO is utilized to optimize the kernel function parameters and penalty factors of SVM to improve the prediction results. Fourthly, The four modules are combined into NE-KC-GWO-SVM model to predict the wind speed accurately. Finally, to verify the effectiveness of the proposed model, the prediction accuracy of the model is experimentally analyzed from two parts. One is to analyze the superiority of the model itself by using the method of single model removed. The results show that the proposed model is the best, and has high accuracy, and can reflect the characteristics of wind speed well and truly. The other one is that models similar to those proposed in the literature are selected for comparative analysis. The experimental results show that compared with the other two models, the proposed model has the best accuracy. At the same time, the proposed model has good prediction stability and acceptable time complexity. Based on all the experimental results, it can be obtained that the proposed model has better prediction effect, which can provide a scientific basis for the macro-control of power system and improve the operation security and stability of power system.","Short-term, Wind speed, Hybrid model, GWO, SVM",Haize Hu and Yunyi Li and Xiangping Zhang and Mengge Fang,https://www.sciencedirect.com/science/article/pii/S0031320322001042,https://doi.org/10.1016/j.patcog.2022.108623,0031-3203,2022,108623,127,Pattern Recognition,A novel hybrid model for short-term prediction of wind speed,article,HU2022108623,
"Large-scale fine-grained image retrieval based hashing learning method has two main problems. First, low dimension feature embedding can fasten the retrieval process but bring accuracy decrease due to much information loss. Second, fine-grained images lead to the same category query hash codes mapping into the different cluster in database hash latent space. To handle these issues, we propose a feature consistency driven attention erasing network (FCAENet) for fine-grained image retrieval. For the first issue, we propose an adaptive augmentation module in FCAENet, which is the selective region erasing module (SREM). SREM makes the network more robust on subtle differences of fine-grained task by adaptively covering some regions of raw images. The feature extractor and hash layer can learn more representative hash codes for fine-grained images by SREM. With regard to the second issue, we fully exploit the pair-wise similarity information and add the enhancing space relation loss (ESRL) in FCAENet to make the vulnerable relation stabler between the query hash code and database hash code. We conduct extensive experiments on five fine-grained benchmark datasets (CUB2011, Aircraft, NABirds, VegFru, Food101) for 12bits, 24bits, 32bits, 48bits hash codes. The results show that FCAENet achieves the state-of-the-art (SOTA) fine-grained image retrieval performance based on the hashing learning method.","Fine-grained image retrieval, Deep hashing learning, Selective region erasing module, Feature consistency",Qi Zhao and Xu Wang and Shuchang Lyu and Binghao Liu and Yifan Yang,https://www.sciencedirect.com/science/article/pii/S0031320322000991,https://doi.org/10.1016/j.patcog.2022.108618,0031-3203,2022,108618,128,Pattern Recognition,A feature consistency driven attention erasing network for fine-grained image retrieval,article,ZHAO2022108618,
"Multimodal classification methods using different modalities have great advantages over traditional single-modality-based ones for the diagnosis of Alzheimer's disease (AD) and its prodromal stage mild cognitive impairment (MCI). With the increasing amount of high-dimensional heterogeneous data to be processed, multi-modality feature selection has become a crucial research direction for AD classification. However, traditional methods usually depict the data structure using pre-defined similarity matrix as a priori, which is difficult to precisely measure the intrinsic relationship across different modalities in high-dimensional space. In this paper, we propose a novel multimodal feature selection method called Adaptive-Similarity-based Multi-modality Feature Selection (ASMFS) which performs adaptive similarity learning and feature selection simultaneously. Specifically, a similarity matrix is learned by jointly considering different modalities and at the same time, an efficient feature selection is conducted by imposing group sparsity-inducing l2,1-norm constraint. Evaluated on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database with baseline MRI and FDG-PET imaging data collected from 51 AD, 43 MCI converters (MCI-C), 56 MCI non-converters (MCI-NC) and 52 normal controls (NC), we demonstrate the effectiveness and superiority of our proposed method against other state-of-the-art approaches for multi-modality classification of AD/MCI.","Multi-modality, Similarity learning, Feature selection, Alzheimer's disease",Yuang Shi and Chen Zu and Mei Hong and Luping Zhou and Lei Wang and Xi Wu and Jiliu Zhou and Daoqiang Zhang and Yan Wang,https://www.sciencedirect.com/science/article/pii/S0031320322000474,https://doi.org/10.1016/j.patcog.2022.108566,0031-3203,2022,108566,126,Pattern Recognition,ASMFS: Adaptive-similarity-based multi-modality feature selection for classification of Alzheimer's disease,article,SHI2022108566,
"In this paper, we address the problem of novelty detection whose goal is to recognize instances from unseen classes during testing. Our key idea is to leverage the inconsistency between class similarity and (latent) attribute similarity. We are motivated by the observation that a novel class may holistically appear like a certain known class (class-level reference) but often exhibits unique properties similar to others (attribute-level references). That is, the related class- and attribute-level references are often inconsistent for a novel class. A new two-stage Class-Attribute Inconsistency Learning network (CAILNet) is proposed to explore class-attribute inconsistency for novelty detection. Stage one aims to learn both class and attribute features based on the class labels and fake attribute labels, and stage two aims to search for the corresponding references and make fine-grained comparisons for final novelty decision. Empirically we conduct comprehensive experiments on three benchmark datasets, and demonstrate state-of-the-art performance.","Novelty detection, Class-attribute inconsistency, Class-level similarity, Attribute-level similarity",Shuaiyuan Du and Chaoyi Hong and Yinpeng Chen and Zhiguo Cao and Ziming Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322000632,https://doi.org/10.1016/j.patcog.2022.108582,0031-3203,2022,108582,126,Pattern Recognition,Class-attribute inconsistency learning for novelty detection,article,DU2022108582,
"Corner detection algorithms based on multi-scale analysis attract more attention due to their promising performance. However, they only consider amplitude information, neglect phase information and partially utilize multi-scale decomposition coefficients to detect corners. This limits their detection accuracy, repeatability and localization ability. This paper describes a new multi-scale analysis based corner detector. To overcome the problems of bilateral margin responses, edge extension and lack of phase information in traditional shearlets, a novel complex shearlet transform is proposed to better localize distributed discontinuities and especially to extract phase information from geometrical features. Moreover, a new rotary phase congruence tensor is proposed to utilize all amplitude and phase information for corner detection. Its tolerances to noise and ability for corner localization are improved further by screening and normalizing the amplitude information. Experimental results demonstrate that the localization ability and detection accuracy of the proposed method are superior to current detectors, and its repeatability is generally higher than current detectors and recent machine learning based interest point detectors.","Complex shearlets, Phase congruence, Rotary structure tensor, Multi-scale analysis, Corner detection",Mingzhe Wang and Changming Sun and Arcot Sowmya,https://www.sciencedirect.com/science/article/pii/S0031320322000875,https://doi.org/10.1016/j.patcog.2022.108606,0031-3203,2022,108606,128,Pattern Recognition,Complex shearlets and rotary phase congruence tensor for corner detection,article,WANG2022108606,
"Tools for understanding and explaining complex predictive models are critical for user acceptance and trust. One such tool is rule extraction, i.e., approximating opaque models with less powerful but interpretable models. Pedagogical (or black-box) rule extraction, where the interpretable model is induced using the original training instances, but with the predictions from the opaque model as targets, has many advantages compared to the decompositional (white-box) approach. Most importantly, pedagogical methods are agnostic to the kind of opaque model used, and any learning algorithm producing interpretable models can be employed for the learning step. The pedagogical approach has, however, one main problem, clearly limiting its utility. Specifically, while the extracted models are trained to mimic the opaque, there are absolutely no guarantees that this will transfer to novel data. This potentially low test set fidelity must be considered a severe drawback, in particular when the extracted models are used for explanation and analysis. In this paper, a novel approach, solving the problem with test set fidelity by utilizing the conformal prediction framework, is suggested for extracting interpretable regression models from opaque models. The extracted models are standard regression trees, but augmented with valid prediction intervals in the leaves. Depending on the exact setup, the use of conformal prediction guarantees that either the test set fidelity or the test set accuracy will be equal to a preset confidence level, in the long run. In the extensive empirical investigation, using 20 publicly available data sets, the validity of the extracted models is demonstrated. In addition, it is shown how normalization can be used to provide individualized prediction intervals, thus providing highly informative extracted models.","Rule extraction, Interpretability, Conformal prediction, Explainable AI, Predictive regression",Ulf Johansson and Cecilia SÃ¶nstrÃ¶d and Tuwe LÃ¶fstrÃ¶m and Henrik BostrÃ¶m,https://www.sciencedirect.com/science/article/pii/S0031320322000358,https://doi.org/10.1016/j.patcog.2022.108554,0031-3203,2022,108554,126,Pattern Recognition,Rule extraction with guarantees from regression models,article,JOHANSSON2022108554,
"With the rapid development of multimedia technologies (e.g. deep learning), Feature Selection (FS) is now playing a critical role in acquiring discriminative features from massive data. Traditional FS methods score feature importance and select the top best features by treating all instances equally; Hence, valuable instances like directional outliers (DOs), which are specific outliers closer to other class centres than to their owns, seldom receive particular attention during feature selection. Based on our observation, DOs derive from âmisclassified instancesâ which lead to misclassification. In this paper, we present a novel supervised feature selection method entitled Feature Selection via Directional Outliers Correcting (FSDOC), for accurate data classification. The proposed FSDOC includes an optimization algorithm to capture DOs, and two correcting algorithms to reasonably capture redundant features by correcting DOs with intraclass deviation minimization and interclass relative distance maximization. We give theoretical guarantees and adequate analysis on all algorithms to show the effectiveness of FSDOC. Extensive experiments on fifteen public datasets, and two case studies of deep features and very-high dimensional Fisher Vector selection, demonstrate the superior performance of FSDOC.","Feature selection, Directional outlier, Redundant features, Deviation, Supervised method",Lixin Yuan and Guoqiang Yang and Qian Xu and Tong Lu,https://www.sciencedirect.com/science/article/pii/S003132032200022X,https://doi.org/10.1016/j.patcog.2022.108541,0031-3203,2022,108541,126,Pattern Recognition,Discriminative feature selection with directional outliers correcting for data classification,article,YUAN2022108541,
"Clothing parsing has made tremendous progress in the domain of computer vision recently. Most state-of-the-art methods are based on the encoder-decoder architecture. However, the existing methods mainly neglect problems of feature uncalibration within blocks and semantics dilution between blocks. In this work, we propose an unabridged adjacent modulation network (UAM-Net) to aggregate multi-level features for clothing parsing. We first build an unabridged channel attention (UCA) mechanism on feature maps within each block for feature recalibration. We further design a top-down adjacent modulation (TAM) for decoder blocks. By deploying TAM, high-level semantic information and visual contexts can be gradually transferred into lower-level layers without loss. The joint implementation of UCA and TAM ensures that the encoder has an enhanced feature representation ability, and the low-level features of the decoders contain abundant semantic contexts. Quantitative and qualitative experimental results on two challenging benchmarks (i.e., colorful fashion parsing and the modified fashion clothing) declare that our proposed UAM-Net can achieve competitive high-accurate performance with the state-of-the-art methods. The source codes are available at:Â https://github.com/ctzuo/UAM-Net.","Encoder-decoder network, Clothing parsing, Attention learning, Features modulation, Self-supervised learning",Dong Zhang and Chengting Zuo and Qianhao Wu and Liyong Fu and Xinguang Xiang,https://www.sciencedirect.com/science/article/pii/S0031320322000759,https://doi.org/10.1016/j.patcog.2022.108594,0031-3203,2022,108594,127,Pattern Recognition,Unabridged adjacent modulation for clothing parsing,article,ZHANG2022108594,
"Most existing person re-identification (re-id) methods assume supervised model training on a separate large set of training samples from the target domain. While performing well in the training domain, such trained models are seldom generalisable to a new independent unsupervised target domain without further labelled training data from the target domain. To solve this scalability limitation, we develop a novel Hierarchical Unsupervised Domain Adaptation (HUDA) method. It can transfer labelled information of an existing dataset (a source domain) to an unlabelled target domain for unsupervised person re-id. Specifically, HUDA is designed to model jointly global distribution alignment and local instance alignment in a two-level hierarchy for discovering transferable source knowledge in unsupervised domain adaptation. Crucially, this approach aims to overcome the under-constrained learning problem of existing unsupervised domain adaptation methods. Extensive evaluations show the superiority of HUDA for unsupervised cross-domain person re-id over a wide variety of state-of-the-art methods on four re-id benchmarks: Market-1501, DukeMTMC, MSMT17 and CUHK03.","Unsupervise person re-identification, Domain adaptation",Xu Lan and Xiatian Zhu and Shaogang Gong,https://www.sciencedirect.com/science/article/pii/S0031320321006907,https://doi.org/10.1016/j.patcog.2021.108514,0031-3203,2022,108514,124,Pattern Recognition,Unsupervised cross-domain person re-identification by instance and distribution alignment,article,LAN2022108514,
"The performance of many state-of-the-art deep face recognition models deteriorates significantly for images captured under low illumination, mainly because the features of dim probe face images cannot match well with those of normal-illumination gallery images. The issue cannot be satisfactorily addressed by enhancing the illumination of face images and performing face recognition on the resulted images alone. We propose a novel deep face recognition framework that consists of a feature restoration network, a feature extraction network, and an embedding matching module. The feature restoration network adopts a two-branch structure based on the convolutional neural network to generate a feature image from the raw image and the illumination-enhanced image. The feature extraction network encodes the feature image into an embedding, which is then used by the embedding matching module for face verification and identification. The overall verification accuracy is improved from 1.1% to 6.7% when tested on the Specs on Faces (SoF) dataset. For face identification, the rank-1 identification accuracy is improved by 2.8%.","Face recognition, Dim image, Rank-1 identification accuracy, Two-branch network, Convolutional neural network",Yu-Hsuan Huang and Homer H. Chen,https://www.sciencedirect.com/science/article/pii/S0031320322000619,https://doi.org/10.1016/j.patcog.2022.108580,0031-3203,2022,108580,126,Pattern Recognition,Deep face recognition for dim images,article,HUANG2022108580,
"A recent study has shown that the real-time anti-noise challenges faced by molecular activity prediction algorithms can be solved by using the part structure features of the molecular graph. However, the sub-structures selected by this method are distributed in a scattered manner such that although they include as many block features as possible, they do not fully consider the connections between these blocks. Therefore, this study was conducted to fully consider the physical interpretation of the betweenness centrality node in the graph, and a sub-structure was obtained by depth-first search (DFS) from this node. This sub-structure not only contains the characteristics of each region but also retains the connections between each region. Then, a cascading multi-layer perception (MLP) model was designed to learn the characteristic representation of the graph from its local structure features. Experiments demonstrated that the performance of our algorithm is superior to that of other algorithms when evaluated on different datasets.","Graph classification, Graph neural network, Betweenness centrality node, Feature fusion, Characteristics representation",Jingyi Ding and Ruohui Cheng and Jian Song and Xiangrong Zhang and Licheng Jiao and Jianshe Wu,https://www.sciencedirect.com/science/article/pii/S0031320322000061,https://doi.org/10.1016/j.patcog.2022.108525,0031-3203,2022,108525,125,Pattern Recognition,Graph label prediction based on local structure characteristics representation,article,DING2022108525,
"To be very specific in this paper, an Attentive Occlusion-adaptive Deep Network, hereafter referred as AODN, is proposed for facial landmark detection, consisting of the geometry-aware module, attention module, and low-rank learning module. Facial Landmark Detection (FLD) is a fundamental pre-processing step of facial related tasks. Occlusion, extreme pose, different expressions and illumination are the main challenges in facial landmark detection related tasks. Convolutional Neural Network (CNN) based FLD methods have attained significant improvement regarding accurate FLD but, to deal with occlusion is still very challenging even for CNN. It is because; probably occlusion misleads CNN on feature representation learning. If faces are partially occluded, the localization accuracy will drop significantly. The role of attention in the human visual system is vital, and researchers proved its significance for the computer vision problem. Taking advantage of geometric relationships among different facial components and attention, we extended our already established Occlusion-adaptive Deep Network (ODN). We introduced the attention module consisting of Channel-wise Attention (CA) and Spatial Attention (SA) to improve its ability to deal with the occlusion and enhance feature representation ability simultaneously. The occlusion probability assists as adaptive weights of high-level features and minimizes the effect of the occlusion and assist in modelling the occlusion. Ablation studies prove the synergistic effect of each module. The summary of our trifold contribution is as follows: i) we introduced attention mechanism in our already established ODN model, to deal with occlusion more precisely, and get the rich feature representation to achieve better performance. ii) As per our best of knowledge, we are the pioneers to introduce CA and SA for FLD to model occlusion. iii) Our proposed methodology reduces the number of entire network parameters, which effectually decreases training time and cost. So, the proposed model is more suitable for scalable data processing. Experimental results prove the better performance of proposed AODN on challenging benchmark datasets.","Facial landmarks detection, Channel-wise attention, Spatial attention, Deep learning, Face alignment",Muhammad Sadiq and Daming Shi,https://www.sciencedirect.com/science/article/pii/S0031320321006865,https://doi.org/10.1016/j.patcog.2021.108510,0031-3203,2022,108510,125,Pattern Recognition,Attentive occlusion-adaptive deep network for facial landmark detection,article,SADIQ2022108510,
"Table structure recognition is an essential part for making machines understand tables. Its main task is to recognize the internal structure of a table. However, due to the complexity and diversity in their structure and style, it is very difficult to parse the tabular data into the structured format which machines can understand, especially for complex tables. In this paper, we introduce Split, Embed and Merge (SEM), an accurate table structure recognizer. SEM is mainly composed of three parts, splitter, embedder and merger. In the first stage, we apply the splitter to predict the potential regions of the table row/column separators, and obtain the fine grid structure of the table. In the second stage, by taking a full consideration of the textual information in the table, we fuse the output features for each table grid from both vision and text modalities. Moreover, we achieve a higher precision in our experiments through providing additional textual features. Finally, we process the merging of these basic table grids in a self-regression manner. The corresponding merging results are learned through the attention mechanism. In our experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR dataset which outperforms other methods by a large margin. We also won the first place of complex tables and third place of all tables in Task-B of ICDAR 2021 Competition on Scientific Literature Parsing. Extensive experiments on other publicly available datasets further demonstrate the effectiveness of our proposed approach.","Table structure recognition, Self-regression, Attention mechanism, Encoder-decoder",Zhenrong Zhang and Jianshu Zhang and Jun Du and Fengren Wang,https://www.sciencedirect.com/science/article/pii/S0031320322000462,https://doi.org/10.1016/j.patcog.2022.108565,0031-3203,2022,108565,126,Pattern Recognition,"Split, Embed and Merge: An accurate table structure recognizer",article,ZHANG2022108565,
"Recovering an unknown object from the magnitude of its Fourier transform is a phase retrieval problem. Here, we consider a much difficult case, where those observed intensity values are incomplete and contaminated by both salt-and-pepper and random-valued impulse noise. To take advantage of the low-rank property within the image of the object, we use a regularization term which penalizes high weighted nuclear norm values of image patch groups. For outliers (impulse noise) in the observation, the â1â2 metric is adopted as the data fidelity term. Then we break down the resulting optimization problem into smaller ones, for example, weighted nuclear norm proximal mapping and â1â2 minimization, because the nonconvex and nonsmooth subproblems have available closed-form solutions. The convergence results are also presented, and numerical experiments are provided to demonstrate the superior reconstruction quality of the proposed method.","Phase retrieval, Partial magnitudes, Nuclear norm minimization, Impulse noise",Zhi Li and Ming Yan and Tieyong Zeng and Guixu Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322000188,https://doi.org/10.1016/j.patcog.2022.108537,0031-3203,2022,108537,125,Pattern Recognition,Phase retrieval from incomplete data via weighted nuclear norm minimization,article,LI2022108537,
"Currently, existing state-of-the-art 3D object detectors are in two-stage paradigm. These methods typically comprise two steps: 1) Utilize a region proposal network to propose a handful of high-quality proposals in a bottom-up fashion. 2) Resize and pool the semantic features from the proposed regions to summarize RoI-wise representations for further refinement. Note that these RoI-wise representations in step 2) are considered individually as uncorrelated entries when fed to following detection headers. Nevertheless, we observe these proposals generated by step 1) offset from ground truth somehow, emerging in local neighborhood densely with an underlying probability. Challenges arise in the case where a proposal largely forsakes its boundary information due to coordinate offset while existing networks lack corresponding information compensation mechanism. In this paper, we propose BADet for 3D object detection from point clouds. Specifically, instead of refining each proposal independently as previous works do, we represent each proposal as a node for graph construction within a given cut-off threshold, associating proposals in the form of local neighborhood graph, with boundary correlations of an object being explicitly exploited. Besides, we devise a lightweight Region Feature Aggregation Module to fully exploit voxel-wise, pixel-wise, and point-wise features with expanding receptive fields for more informative RoI-wise representations. We validate BADet both on widely used KITTI Dataset and highly challenging nuScenes Dataset. As of Apr. 17th, 2021, our BADet achieves on par performance on KITTI 3D detection leaderboard and ranks 1st on Moderate difficulty of Car category on KITTI BEV detection leaderboard. The source code is available at https://github.com/rui-qian/BADet.","3D object detection, autonomous driving, graph neural network, boundary aware, point clouds",Rui Qian and Xin Lai and Xirong Li,https://www.sciencedirect.com/science/article/pii/S003132032200005X,https://doi.org/10.1016/j.patcog.2022.108524,0031-3203,2022,108524,125,Pattern Recognition,BADet: Boundary-Aware 3D Object Detection from Point Clouds,article,QIAN2022108524,
"The goal of molecule optimization is to optimize molecular properties by modifying molecule structures. Conditional generative models provide a promising way to transfer the input molecules to the ones with better property. However, molecular properties are highly sensitive to small changes in molecular structures. This leads to an interesting thought that we can improve the property of molecules with limited modification in structure. In this paper, we propose a structure-aware conditional Variational Auto-Encoder, namely SCVAE, which exploits the topology of molecules as structure condition and optimizes the molecular properties with constrained structural modification. SCVAE leverages graph alignment of two-level molecule structures in an unsupervised manner to bind the structure conditions between two molecules. Then, this structure condition facilitates the molecule optimization with limited structural modification, namely, constrained molecule optimization, under a novel variational auto-encoder framework. Extensive experimental evaluations demonstrate that structure-aware CVAE generates new molecules with high similarity to the original ones and better molecular properties.","Molecule optimization, Conditional generation, Drug discovery",Junchi Yu and Tingyang Xu and Yu Rong and Junzhou Huang and Ran He,https://www.sciencedirect.com/science/article/pii/S0031320322000620,https://doi.org/10.1016/j.patcog.2022.108581,0031-3203,2022,108581,126,Pattern Recognition,Structure-aware conditional variational auto-encoder for constrained molecule optimization,article,YU2022108581,
"Occlusion is a severe problem for pedestrian detection in crowded scenes. Due to the diversity of pedestrian postures and occlusion forms, leading to false detection and missed detection. In this paper, we propose a high quality proposal feature generation pedestrian detection algorithm to improve detection performance. Firstly, Dual-Region Feature Generation (DRFG) is proposed to generate high quality proposal features. Specifically, visible regions with less occlusion are introduced and low-precision proposals are generated for both the full-body and visible regions respectively. Then, proposals are respectively selected from the two kinds of proposals mentioned above to match in pairs, so as to guarantee a strong correspondence in information between the two proposals. Afterwards, the successfully matched proposal features are fused by Selective Kernel Feature Fusion (SKFF) to generate high quality proposal features. Secondly, Paired Multiple Instance Prediction(PMIP) is performed on the fused features to generate multiple prediction branches, and each prediction branch generates full-body and visible prediction box. Finally, Paired Non-Maximum Suppression(PNMS) is applied to the prediction boxes to reduce the false positives. Experiments have been conducted on CrowdHuman [1] and CityPersons [2] datasets. Comparing with baseline, our methods have achieved 5.9% AP and 1.5% MRâ2 improvement on the above two datasets, sufficiently verifying the effectiveness of our methods in crowded pedestrian detection.","Crowded pedestrian, Pedestrian detection, Visible proposal, Feature fusion, Paired prediction",Jing Wang and Cailing Zhao and Zhanqiang Huo and Yingxu Qiao and Haifeng Sima,https://www.sciencedirect.com/science/article/pii/S0031320322000863,https://doi.org/10.1016/j.patcog.2022.108605,0031-3203,2022,108605,128,Pattern Recognition,High quality proposal feature generation for crowded pedestrian detection,article,WANG2022108605,
"Large volumes of training data introduce high computational cost in instance-based classification. Data reduction algorithms select or generate a small (condensing) set of representative training prototypes from the available training data. The Reduction by Space Partitioning algorithm is one of the most well-known prototype generation algorithms that repetitively divides the original training data into subsets. This partitioning process needs to identify the diameter of each subset, i.e., its two farthest instances. This is a costly process since it requires the calculation of all distances between the instances in each subset. The paper introduces two new very fast variations that, instead of computing the actual diameter of a subset, choose a pair of distant-enough instances. The first variation uses instances belonging to an exact 3d convex hull of the subset, while the second one uses instances belonging to the minimum bounding rectangle of the subset. Our experimental study shows that the new variations vastly outperform the original algorithm without a penalty in classification accuracy and reduction rate.","Reduction by space partitioning, RSP3, Classification, Prototype generation, Big training data, Convex hull, Minimum bounding rectangle (MBR)",Thomas Giorginis and Stefanos Ougiaroglou and Georgios Evangelidis and Dimitris A. Dervos,https://www.sciencedirect.com/science/article/pii/S0031320322000346,https://doi.org/10.1016/j.patcog.2022.108553,0031-3203,2022,108553,126,Pattern Recognition,Fast data reduction by space partitioning via convex hull and MBR computation,article,GIORGINIS2022108553,
"Video super-resolution aims to recover the high-resolution (HR) contents from the low-resolution (LR) observations relying on compositing the spatial-temporal information in the LR frames. It is crucial to model the spatial-temporal information jointly since the video sequences are three-dimensional spatial-temporal signals. Compared with explicitly estimating motions between the 2D frames, 3D convolutional neural networks (CNNs) have been shown its efficiency and effectiveness for video super-resolution (SR), as a natural way of spatial-temporal data modelling. Though promising, the performance of 3D CNNs is still far from satisfactory. The high computational and memory requirements limit the development of more advanced designs to extract and fuse the information from a larger spatial and temporal scale. We thus propose a Mixed Spatial-Temporal Convolution (MSTC) block that simultaneously extracts the spatial information and the supplemented temporal dependency among frames by jointly applying 2D and 3D convolution. To further fuse the learned features corresponding to different frames, we propose a novel similarity-based selective features strategy, unlike precious methods directly stacking the learned features. Additionally, an attention-based motion compensation module is applied to alleviate the influence of misalignment between frames. Experiments on three widely used benchmark datasets and real-world dataset show that, relying on superior feature extraction and fusion ability, the proposed network can outperform previous state-of-the-art methods, especially for recovering the confusing details.","Video super-Resolution, Mixed spatial-Temporal convolution, Selective feature fusion",Wei Sun and Dong Gong and Javen Qinfeng Shi and Anton {van den Hengel} and Yanning Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322000589,https://doi.org/10.1016/j.patcog.2022.108577,0031-3203,2022,108577,126,Pattern Recognition,Video super-resolution via mixed spatial-temporal convolution and selective fusion,article,SUN2022108577,
"Unsupervised domain adaptation in person re-identification remains a challenge to learning discriminative representations due to the absence of labels in target domain. Clustering could provide pseudo-labels, but the limitation mainly comes from imperfect clustering and noisy pseudo-labels. To address this drawback, we propose Connective Momentum Clustering (CMC) framework to build a connection estimator via graph convolutional networks to transfer rich connection knowledge from the annotation space of source data to target domain. It estimates connections from context to reveal relationship between unlabeled data and helps to discover more reliable clusters. With momentum mechanism, stable pseudo-labels are updated iteratively with confidence and refined consistently to encourage more discriminative networks. Meanwhile, we notice that the huge domain gap between source and target domains results in severe pollution in BatchNorm layers. To tackle this problem, we normalize the data stream separately to decouple different distribution and further boost the performance in target domain. We adopt our CMC framework on mainstream tasks and achieves 80.2% mAP / 91.3% Rank-1 on DukeâMarket task and 70.4% mAP / 82.4% Rank-1 on MarketâDuke task.","Person re-identification, Unsupervised domain adaptation, Graph convolutional networks, Momentum mechanism, Batch normalization",Yichen Lu and Weihong Deng,https://www.sciencedirect.com/science/article/pii/S0031320322000504,https://doi.org/10.1016/j.patcog.2022.108569,0031-3203,2022,108569,126,Pattern Recognition,Transferring discriminative knowledge via connective momentum clustering on person re-identification,article,LU2022108569,
"In human parsing, graph convolutional networks (GCNs), which naturally model the skeleton of the human body as a fixed graph, have been witnessed to obtain remarkable performance. However, the existing methods perform the fixed graph modeling over all the training samples. This may not be an optimal graph for the diversity of the samples that contain various shapes of human parts, complex body postures, severe occlusions and dense crowd, etc. Focusing on this, we propose a new Multilabel Learning based Adaptive Graph Convolutional Network (ML-AGCN) for human parsing. The ML-AGCN includes three modules: adaptive graph generation module, semantic parts based attention module and label consistency loss. Concretely, to effectively deal with the different sizes and connectivities of the optimal graph for different samples, we first propose an adaptive graph generation module based on multilabel learning that contains graph node adaptation (GNA) and graph connection adaptation (GCA). Then, for a more comprehensive node embedding, we design a semantic parts based attention module to optimally fuse fixed graph embeddings and adaptive graph embeddings. Besides, to further explicitly constraint the consistency between the predicted multilabel and the predicted human parsing results, we propose a label consistency loss that can simultaneously refine the human parsing results and optimize the accuracy of the adaptive graph. Extensive experiments on four challenging datasets, including PASCAL-Person-Part, ATR, LIP and CIHP, well demonstrate the effectiveness of our model, and it outperforms other state-of-the-art methods in human parsing.","Human parsing, Multilabel learning based adaptive graph convolutional network, Adaptive graph",Huaqing Hao and Weibin Liu and Weiwei Xing and Shunli Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322000747,https://doi.org/10.1016/j.patcog.2022.108593,0031-3203,2022,108593,127,Pattern Recognition,Multilabel learning based adaptive graph convolutional network for human parsing,article,HAO2022108593,
"Human pose estimation has achieved significant progress on images with high imaging resolution. However, low-resolution imagery data bring nontrivial challenges which are still under-studied. To fill this gap, we start with investigating existing methods and reveal that the most dominant heatmap-based methods would suffer more severe model performance degradation from low-resolution, and offset learning is an effective strategy. Established on this observation, in this work we propose a novel Confidence-Aware Learning (CAL) method which further addresses two fundamental limitations of existing offset learning methods: inconsistent training and testing, decoupled heatmap and offset learning. Specifically, CAL selectively weighs the learning of heatmap and offset with respect to ground-truth and most confident prediction, whilst capturing the statistical importance of model output in mini-batch learning manner. Extensive experiments conducted on the COCO benchmark show that our method outperforms significantly the state-of-the-art methods for low-resolution human pose estimation.","Human pose estimation, Low resolution image, Heatmap learning, Offset learning, Quantization error",Chen Wang and Feng Zhang and Xiatian Zhu and Shuzhi Sam Ge,https://www.sciencedirect.com/science/article/pii/S0031320322000607,https://doi.org/10.1016/j.patcog.2022.108579,0031-3203,2022,108579,126,Pattern Recognition,Low-resolution human pose estimation,article,WANG2022108579,
"We propose a novel optimization framework that crops a given image based on user description and aesthetics. Unlike existing image cropping methods, where one typically trains a deep network to regress to crop parameters or cropping actions, we propose to directly optimize for the cropping parameters by repurposing pre-trained networks on image captioning and aesthetic tasks, without any fine-tuning, thereby avoiding training a separate network. Specifically, we search for the best crop parameters that minimize a combined loss of the initial objectives of these networks. To make the optimization stable, we propose three strategies: (i) multi-scale bilinear sampling, (ii) annealing the scale of the crop region, therefore effectively reducing the parameter space, (iii) aggregation of multiple optimization results. Through various quantitative and qualitative evaluations, we show that our framework can produce crops that are well-aligned to intended user descriptions and aesthetically pleasing.","Image cropping, Aesthetics, Deep network re-purposing, Image captioning",Nora Horanyi and Kedi Xia and Kwang Moo Yi and Abhishake Kumar Bojja and AleÅ¡ Leonardis and Hyung Jin Chang,https://www.sciencedirect.com/science/article/pii/S0031320321006610,https://doi.org/10.1016/j.patcog.2021.108485,0031-3203,2022,108485,126,Pattern Recognition,Repurposing existing deep networks for caption and aesthetic-guided image cropping,article,HORANYI2022108485,
"The object detection in aerial images is one of the most commonly used tasks in the wide-range of computer vision applications. However, the object detection is more challenging due to the following issues: (a) the pixel occupancy vary among the different scales of objects, (b) the distribution of objects is not uniform in aerial images, (c) the appearance of an object varies with different view-points and illumination conditions, and (d) the number of objects, even though they belong to same type, vary across the images. To address these issues, we propose a novel network for multi-scale object detection in aerial images using hierarchical dilated convolutions, called as mSODANet. In particular, we probe hierarchical dilated network using parallel dilated convolutions to learn the contextual information of different types of objects at multiple scales and multiple field-of-views. The introduced hierarchical dilated network captures the visual information of aerial image more effectively and enhances the detection capability of the model. Further, the extensive experiments conducted on three challenging publicly available datasets, i.e., Visdrone2019, DOTA (OBB & HBB), NWPU VHR-10, demonstrate the effectiveness of the proposed mSODANet and achieve the state-of-the-art performance on all three datasets.","Multi-scale object detection, Contextual features, Dilated convolutions, Aerial images",Vishnu Chalavadi and Prudviraj Jeripothula and Rajeshreddy Datla and Sobhan Babu Ch and Krishna Mohan C,https://www.sciencedirect.com/science/article/pii/S0031320322000292,https://doi.org/10.1016/j.patcog.2022.108548,0031-3203,2022,108548,126,Pattern Recognition,mSODANet: A network for multi-scale object detection in aerial images using hierarchical dilated convolutions,article,CHALAVADI2022108548,
"Image segmentation is a very important topic in the field of computer vision. We present a method for semantic segmentation of selected stuff classes from a superset of classes. We show that in situations where only select stuff classes are required if we group them as per a strategy then it can attain much higher accuracy than the models trained on the original dataset with all classes intact. The COCO-Stuff Dataset is used for demonstrating the aforesaid strategy. For training purposes, the DeepLabv3+ with Mobilenet-v2 architecture is used. We have achieved an 80.2 percent mean Intersection over Union (mIoU) on these selected classes. We also refine the masks using Learning/Computer Vision (CV) methods and hence obtain better visualization results as compared to the existing DeepLabv3+ results.","Image segmentation, Stuff classes, Deeplab",Kunjal Shah and Gururaj Bhat,https://www.sciencedirect.com/science/article/pii/S0031320321006853,https://doi.org/10.1016/j.patcog.2021.108509,0031-3203,2022,108509,124,Pattern Recognition,Exploring semantic segmentation of related subclasses from a superset of classes,article,SHAH2022108509,
"Data imbalance is a significant factor affecting classification performance in computer vision. In particular, data imbalance is harmful to classification learning and representation learning. To address this issue, this paper proposes a geometric deep learning framework combined with Feature Scaling Module (FSM) and Boundary Samples Mining Module (BSMM). Considering the geometric information in sample distributions of training samples, FSM is proposed to scale the features by hypersphere radius of each class, which improves the representation ability of minority classes. Meanwhile, it is noteworthy that the relationships and information between samples are essential for classification. Therefore, BSMM is proposed to mine the boundary samples by Gabriel Graph that takes the relationships into account. Finally, a loss scheduler is designed to adjust the training process of these two modules. With the scheduler, the model first learns representation and then focuses more on minority classes gradually. Extensive experiments on three benchmark datasets demonstrate the advantages of the proposed learning framework over the state-of-the-art models for solving the imbalance problem.","Imbalance problem, Image classification, Geometric information, Boundary samples mining, Feature scaling",Zhe Wang and Qida Dong and Wei Guo and Dongdong Li and Jing Zhang and Wenli Du,https://www.sciencedirect.com/science/article/pii/S0031320322000450,https://doi.org/10.1016/j.patcog.2022.108564,0031-3203,2022,108564,126,Pattern Recognition,Geometric imbalanced deep learning with feature scaling and boundary sample mining,article,WANG2022108564,
"Image matching plays a vital role in many computer vision tasks, and this paper focuses on the mismatch removal problem of feature-based matching. We formulate the problem into a general yet effective optimization framework based on graph matching by combining integer quadratic programming with a compensation term for discouraging matches, termed as Local Graph Structure Consensus (LGSC). Considering the local area similarity of those potential true matches, we design a local graph structure for preserving geometric topology, which contains a local indicator vector and a local affinity vector for each correspondence. The local indicator vector is utilized for edge construction, while the local affinity vector represents the match correctness of the nodes and edges between two graphs. In particular, the ranking shift with scale and rotation invariance is exploited to represent the node affinity. Ultimately, we derive a closed-form solution with linearithmic time and linear space complexity. Moreover, a multi-scale and iterative graph construction strategy is proposed to promote the performance of our method in terms of robustness and effectiveness. Extensive experiments on various real image datasets demonstrate that our LGSC can achieve superior performance over current state-of-the-art approaches.","Image matching, Feature matching, Mismatch removal, Outlier, Image registration",Xingyu Jiang and Yifan Xia and Xiao-Ping Zhang and Jiayi Ma,https://www.sciencedirect.com/science/article/pii/S0031320322000693,https://doi.org/10.1016/j.patcog.2022.108588,0031-3203,2022,108588,126,Pattern Recognition,Robust image matching via local graph structure consensus,article,JIANG2022108588,
"This paper describes probability forecasting systems that are universal, or universally consistent, in the sense of being consistent under any data-generating distribution, assuming that the observations are produced independently in the IID fashion. The notion of universal consistency is asymptotic and does not imply any small-sample guarantees of validity. On the other hand, the method of conformal prediction has been recently adapted to producing predictive distributions that satisfy a natural property of small-sample validity, namely they are automatically probabilistically calibrated. The main result of the paper is the existence of universal conformal predictive systems, which output predictive distributions that are both probabilistically calibrated and universally consistent.","Conformal prediction, Predictive distribution, Probabilistic calibration, Universal consistency",Vladimir Vovk,https://www.sciencedirect.com/science/article/pii/S0031320322000176,https://doi.org/10.1016/j.patcog.2022.108536,0031-3203,2022,108536,126,Pattern Recognition,Universal predictive systems,article,VOVK2022108536,
"Explainable Artificial Intelligence (XAI) has in recent years become a well-suited framework to generate human understandable explanations of âblack- boxâ models. In this paper, a novel XAI visual explanation algorithm known as the Similarity Difference and Uniqueness (SIDU) method that can effectively localize entire object regions responsible for prediction is presented in full detail. The SIDU algorithm robustness and effectiveness is analyzed through various computational and human subject experiments. In particular, the SIDU algorithm is assessed using three different types of evaluations (Application, Human and Functionally-Grounded) to demonstrate its superior performance. The robustness of SIDU is further studied in the presence of adversarial attack on âblack-boxâ models to better understand its performance. Our code is available at: https://github.com/satyamahesh84/SIDU_XAI_CODE.","Explainable AI (XAI), CNN, Adversarial attack, Eye-tracker",Satya M. Muddamsetty and Mohammad N.S. Jahromi and Andreea E. Ciontos and Laura M. Fenoy and Thomas B. Moeslund,https://www.sciencedirect.com/science/article/pii/S0031320322000851,https://doi.org/10.1016/j.patcog.2022.108604,0031-3203,2022,108604,127,Pattern Recognition,Visual explanation of black-box model:Â Similarity Difference and Uniqueness (SIDU) method,article,MUDDAMSETTY2022108604,
"Trajectory prediction aims to predict the movement trend of the agents like pedestrians, bikers, vehicles. It is helpful to analyze and understand human activities in crowded spaces and widely applied in many areas such as surveillance video analysis and autonomous driving systems. Thanks to the success of deep learning, trajectory prediction has made significant progress. The current methods are dedicated to studying the agentsâ future trajectories under the social interaction and the sceneriesâ physical constraints. Moreover, how to deal with these factors still catches researchersâ attention. However, they ignore the Semantic Shift Phenomenon when modeling these interactions in various prediction sceneries. There exist several kinds of semantic deviations inner or between social and physical interactions, which we call the âGapâ. In this paper, we propose a Contextual Semantic Consistency Network (CSCNet) to predict agentsâ future activities with powerful and efficient context constraints. We utilize a well-designed context-aware transfer to obtain the intermediate representations from the scene images and trajectories. Then we eliminate the differences between social and physical interactions by aligning activity semantics and scene semantics to cross the Gap. Experiments demonstrate that CSCNet performs better than most of the current methods quantitatively and qualitatively.","Trajectory prediction, The context-aware transfer, The conditional context loss",Beihao Xia and Conghao Wong and Qinmu Peng and Wei Yuan and Xinge You,https://www.sciencedirect.com/science/article/pii/S0031320322000334,https://doi.org/10.1016/j.patcog.2022.108552,0031-3203,2022,108552,126,Pattern Recognition,CSCNet: Contextual semantic consistency network for trajectory prediction in crowded spaces,article,XIA2022108552,
"Accurate characterization of visual attributes such as spiculation, lobulation, and calcification of lung nodules in computed tomography (CT) images is critical in cancer management. The characterization of these attributes is often subjective, which may lead to high inter- and intra-observer variability. Furthermore, lung nodules are often heterogeneous in the cross-sectional image slices of a 3D volume. Current state-of-the-art methods that score multiple attributes rely on deep learning-based multi-task learning (MTL) schemes. These methods, however, extract shared visual features across attributes and then examine each attribute without explicitly leveraging their inherent intercorrelations. Furthermore, current methods treat each slice with equal importance without considering their relevance or heterogeneity, which limits performance. In this study, we address these challenges with a new convolutional neural network (CNN)-based MTL model that incorporates multiple attention-based learning modules to simultaneously score 9 visual attributes of lung nodules in CT image volumes. Our model processes entire nodule volumes of arbitrary depth and uses a slice attention module to filter out irrelevant slices. We also introduce cross-attribute and attribute specialization attention modules that learn an optimal amalgamation of meaningful representations to leverage relationships between attributes. We demonstrate that our model outperforms previous state-of-the-art methods at scoring attributes using the well-known public LIDC-IDRI dataset of pulmonary nodules from over 1,000 patients. Our model also performs competitively when repurposed for benign-malignant classification. Our attention modules provide easy-to-interpret weights that offer insights into the predictions of the model.","Deep learning, Lung nodule analysis, Multi-task, Computed tomography (CT), Attention",Xiaohang Fu and Lei Bi and Ashnil Kumar and Michael Fulham and Jinman Kim,https://www.sciencedirect.com/science/article/pii/S0031320322000577,https://doi.org/10.1016/j.patcog.2022.108576,0031-3203,2022,108576,126,Pattern Recognition,An attention-enhanced cross-task network to analyse lung nodule attributes in CT images,article,FU2022108576,
"In this paper, we propose a new temporal template approach for action recognition and person identification based on motion sequence information in masked depth video streams obtained from RGB-D data. This new representation creates a membership function that models the change in motion based on the correlation between frames that occur during motion flow. The energy images created with this function emphasize the intervals of motion with more change, while the intervals with less change are suppressed. To understand the distinctive features, the obtained energy images by using the proposed function are given as input to the convolutional neural networks and different handcrafted classifiers. The proposed method was observed on the BodyLogin, NATOPS, and SBU Kinect datasets and compared with the existing temporal templates and recent methods. The results indicate that the proposed method provides both higher performance and better motion representation.","Motion recognition, Human recognition, Correlation coefficients, Deep learning, Behavioral biometrics",Onur Can Kurban and Nurullah Calik and TÃ¼lay Yildirim,https://www.sciencedirect.com/science/article/pii/S0031320322001029,https://doi.org/10.1016/j.patcog.2022.108621,0031-3203,2022,108621,127,Pattern Recognition,Human and action recognition using adaptive energy images,article,KURBAN2022108621,
"We address the data association problem and propose a Bayesian approach based on a mixture of Gaussian Processes (GPs) having two key components, the assignment probabilities and the GPs. In the proposed approach, the two key components are simultaneously updated according to observations through an efficient Expectation-Maximization (EM) algorithm that we develop. The proposed approach is thus more adaptive to the observations than the existing approaches for data association. To validate the performance of the proposed approach, we provide experimental results with real data sets as well as two synthetic data sets. We also provide a theoretical analysis to show the effectiveness of the Bayesian update.","Gaussian processes, Bayesian models, Variational inference, Expectation maximization",Younghwan Jeon and Ganguk Hwang,https://www.sciencedirect.com/science/article/pii/S0031320322000735,https://doi.org/10.1016/j.patcog.2022.108592,0031-3203,2022,108592,127,Pattern Recognition,Bayesian mixture of gaussian processes for data association problem,article,JEON2022108592,
"The visual dialog task requires an AI agent to interact with humans in multi-round dialogs based on a visual environment. As a common linguistic phenomenon, pronouns are often used in dialogs to improve the communication efficiency. As a result, resolving pronouns (i.e., grounding pronouns to the noun phrases they refer to) is an essential step towards understanding dialogs. In this paper, we propose VD-PCR, a novel framework to improve Visual Dialog understanding with Pronoun Coreference Resolution in both implicit and explicit ways. First, to implicitly help models understand pronouns, we design novel methods to perform the joint training of the pronoun coreference resolution and visual dialog tasks. Second, after observing that the coreference relationship of pronouns and their referents indicates the relevance between dialog rounds, we propose to explicitly prune the irrelevant history rounds in visual dialog modelsâ input. With pruned input, the models can focus on relevant dialog history and ignore the distraction in the irrelevant one. With the proposed implicit and explicit methods, VD-PCR achieves state-of-the-art experimental results on the VisDial dataset.","Vision and language, Visual dialog, Pronoun coreference resolution",Xintong Yu and Hongming Zhang and Ruixin Hong and Yangqiu Song and Changshui Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322000218,https://doi.org/10.1016/j.patcog.2022.108540,0031-3203,2022,108540,125,Pattern Recognition,VD-PCR: Improving visual dialog with pronoun coreference resolution,article,YU2022108540,
"Universal Domain Adaptation (UniDA) aims to address a more practical problem compared with traditional Close-Set Domain Adaptation (CSDA). Besides the domain gap in traditional CSDA, the common and private label sets across domains are unknown in UniDA leading to an additional category gap. Without considering the category gap for domain adversarial training to extract domain-relevant features, existing methods may suffer from the feature misalignment problem and result in negative transfer. This paper proposes a Hierarchical Feature Disentangling Network (HFDN) to disentangle domain-relevant features into domain-specific and category-shift features for latent variables caused by domain gap and category gap, respectively. Domain-specific features are trained to distinguish the source domain from the target one by discovering domain-specific attributes (e.g. illumination, style), and adversarially aligned to bridge the domain gap for knowledge transfer. Category-shift features are extracted to distinguish domains by identifying private classes across domains, so that they can be leveraged to assign larger weights for samples from the common label set. Experiments show that the proposed HFDN surpasses state-of-the-art CSDA, partial DA, open-set DA and UniDA models.","Universal domain adaptation, Feature disentanglement, Domain adversarial training, Sample reweighting",Yuan Gao and Peipeng Chen and Yue Gao and Jinpeng Wang and YoungSun Pan and Andy J. Ma,https://www.sciencedirect.com/science/article/pii/S0031320322000978,https://doi.org/10.1016/j.patcog.2022.108616,0031-3203,2022,108616,127,Pattern Recognition,Hierarchical feature disentangling network for universal domain adaptation,article,GAO2022108616,
"Information theoretic-guided feature selection approaches (ITFSs), which exploit the uncertainty of information to measure the correlation of features, aim to select the most informative features. However, most previous approaches suffer from two drawbacks. 1) Complementarity and interaction are not valued, leading to features with potential discriminatory information for learning tasks such as classification not being excavated and affecting the effectiveness of learning. 2) The various correlations that exist between features for the class have not been fully considered, and their differentiation and relationships have not been well reflected. To address the former issue, guided by information theory, the complementarity and interaction between features are studied. For the latter, firstly, some ITFSs are reviewed and analyzed in terms of feature correlation. The analysis reveals that considering feature multi-correlation is absent in the selection process. Motivated by this problem, a feature selection algorithm with class-based relevance, redundancy, complementarity, and interaction (R2CI) is designed for the first time. Moreover, the distinctions and connections among different correlations are also explored. The results of comparisons and hypothesis test against competitive algorithms show that R2CI has significant advantages in most cases.","Feature selection, Information theory, Relevance, Redundancy, Complementarity, Interaction",Jihong Wan and Hongmei Chen and Tianrui Li and Wei Huang and Min Li and Chuan Luo,https://www.sciencedirect.com/science/article/pii/S003132032200084X,https://doi.org/10.1016/j.patcog.2022.108603,0031-3203,2022,108603,127,Pattern Recognition,R2CI: Information theoretic-guided feature selection with multiple correlations,article,WAN2022108603,
"Metric learning aims to learn an appropriate distance metric for a given machine learning task. Despite its impressive performance in the field of image recognition, it may still not be discriminative enough for scene recognition because of the high within-class diversity and high between-class similarity of scene images. In this paper, we propose a novel class-specific discriminative metric learning method (CSDML) to alleviate these problems. More specifically, we learn a distinctive linear transformation for each class (or, equivalently, a Mahalanobis distance metric for each class), which allows to project the samples of that class into a corresponding low-dimensional discriminative space. The overall aim is to simultaneously minimize the Euclidean distances between the projections of samples of the same class (or, equivalently, the Mahalanobis distances between these samples) and maximize the Euclidean distances between the projections of samples of different classes. Additionally, we incorporate least squares regression into the optimization problem, rendering class-specific metric learning more flexible and better suited to tackle scene recognition. Experimental results on four benchmark scene datasets demonstrate that the proposed method outperforms most of the state-of-the-art approaches.","Class-specific distance metrics, Discriminative metric learning, Scene recognition",Chen Wang and Guohua Peng and Bernard {De Baets},https://www.sciencedirect.com/science/article/pii/S003132032200070X,https://doi.org/10.1016/j.patcog.2022.108589,0031-3203,2022,108589,126,Pattern Recognition,Class-specific discriminative metric learning for scene recognition,article,WANG2022108589,
"Conformal prediction is a popular tool for providing valid prediction sets for classification and regression problems, without relying on any distributional assumptions on the data. While the traditional description of conformal prediction starts with a nonconformity score, we provide an alternate (but equivalent) view that starts with a sequence of nested sets and calibrates them to find a valid prediction set. The nested framework subsumes all nonconformity scores, including recent proposals based on quantile regression and density estimation. While these ideas were originally derived based on sample splitting, our framework seamlessly extends them to other aggregation schemes like cross-conformal, jackknife+ and out-of-bag methods. We use the framework to derive a new algorithm (QOOB, pronounced cube) that combines four ideas: quantile regression, cross-conformalization, ensemble methods and out-of-bag predictions. We develop a computationally efficient implementation of cross-conformal, that is also used by QOOB. In a detailed numerical investigation, QOOB performs either the best or close to the best on all simulated and real datasets.","Conformal prediction, Quantile regression, Cross-conformal, Out-of-bag methods, Ensemble methods, Random forests",Chirag Gupta and Arun K. Kuchibhotla and Aaditya Ramdas,https://www.sciencedirect.com/science/article/pii/S0031320321006725,https://doi.org/10.1016/j.patcog.2021.108496,0031-3203,2022,108496,127,Pattern Recognition,Nested conformal prediction and quantile out-of-bag ensemble methods,article,GUPTA2022108496,
"Recently, multi-scale neural networks have shown promising improvements in image inpainting. However, most of them adopt the progressive way, in which the errors on lower scales may be propagated on higher scales. Addressing this issue, we propose a multi-level augmented inpainting network (MLA-Net) to rationally harmonize the inter- and intra-level contexts. Here, a pyramid reconstruction structure (PRS) with three parallel levels is designed to establish the inter-level relationship, which can boost the representation of the features by integrating the texture details into semantics. Then, we propose a novel spatial similarity based attention mechanism (SSA) to ensure the intra-level local continuity between the holes and related available patches. In SSA, in order to focus on the important textures and structures rather than calculating each pixel of the feature equally, a spatial map is utilized to highlight the corresponding spatial locations during the similarity computation. The experiments are evaluated on multiple challenging datasets, which demonstrate that MLA-Net can generate accurate results with better visual quality compared with the state-of-the-art methods. For the 256Ã256 Places2 dataset, PSNR increases 1.02Â dB, while FID decreases 0.075. For the 256Ã256 CelebA-HQ dataset, there are 0.22Â dB and 0.613 improvements in PSNR and FID.","Image inpainting, Spatial information, Spatial similarity, Pyramid reconstruction structure",Jia Qin and Huihui Bai and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S0031320322000280,https://doi.org/10.1016/j.patcog.2022.108547,0031-3203,2022,108547,126,Pattern Recognition,Multi-level augmented inpainting network using spatial similarity,article,QIN2022108547,
"With the popularity of electronic touch-screen and pressure sensing devices, fine-grained sketch based image retrieval (FG-SBIR) has become a research hotspot. In this paper, we stress the core problems of FG-SBIR: a. how to reduce the difference between the non-homogenous of heterogeneous media, and b. how to improve the distinguishability of sketch features. Specifically, a sketch generation model is first proposed to replace the conventional pre-processing of roughly extracting image edges, moreover, this model can alleviate the dilemma of sketch data scarcity. We then construct a novel FG-SBIR model which takes advantage of deformable convolutional neural network while taking into consideration of semantic attributes together. In addition, we build a fine-grained clothing sketch-image dataset, which has rich attribute annotations, for the first time. Extensive experiments exhibit that our proposed model achieves a better performance in improving the retrieval accuracy over the state-of-the-art baselines.","Freehand sketches, FG-SBIR, Semantic attributes, Deformable CNNs, Preprocessing",Xianlin Zhang and Mengling Shen and Xueming Li and Fangxiang Feng,https://www.sciencedirect.com/science/article/pii/S0031320321006841,https://doi.org/10.1016/j.patcog.2021.108508,0031-3203,2022,108508,125,Pattern Recognition,A deformable CNN-based triplet model for fine-grained sketch-based image retrieval,article,ZHANG2022108508,
"Video object detection is a challenging task due to the appearance deterioration in video frames. To enhance feature representation of the deteriorated frames, previous methods usually aggregate features from fixed-density and fixed-length adjacent frames. However, due to the redundancy of videos and irregular object movements over time, temporal information may not be efficiently exploited using the traditional inflexible strategy. Alternatively, we present a temporal-adaptive sparse feature aggregation framework, an accurate and efficient method for video object detection. Instead of adopting a fixed-density and fixed-length window fusion strategy, a temporal-adaptive sparse sampling strategy is proposed using a stride predictor to encode informative frames more efficiently. A collaborative feature aggregation framework, which consists of a pixel-adaptive aggregation module and an object-relational aggregation module, is proposed for feature enhancement. The pixel-adaptive aggregation module enhances pixel-level features on the current frame using corresponding pixel-level features from other frames. Similarly, the object-relational aggregation module further enhances feature representation at proposal level. A graph is constructed to model the relations between different proposals so that the relation features and proposal features are adaptively fused for feature enhancement. Experiments demonstrate that our proposed framework significantly surpasses traditional dense aggregation methods, and comprehensive ablation studies verify the effectiveness of each proposed module in our framework.","Video object detection, Temporal-adaptive sparse sampling, Pixel-adaptive aggregation, Object-relational aggregation",Fei He and Qiaozhe Li and Xin Zhao and Kaiqi Huang,https://www.sciencedirect.com/science/article/pii/S0031320322000681,https://doi.org/10.1016/j.patcog.2022.108587,0031-3203,2022,108587,127,Pattern Recognition,Temporal-adaptive sparse feature aggregation for video object detection,article,HE2022108587,
"In this paper we propose a novel Multi-Task Learning (MTL) framework, Split ânâ Merge Net. We draw the inspiration from the multi-head attention formulation of Transformers and propose a novel, simple and interpretable pathway to process information captured and exploited by multiple tasks. In particular, we propose a novel splitting network design, which is empowered with multi-head attention, and generates dynamic masks to filter task specific information and task agnostic shared factors from the input. To drive this generation, and to avoid the oversharing of information between the tasks, we propose a novel formulation of the mutual information loss which encourages the generated split embeddings to be distinct as possible. A unique merging network is also introduced to fuse the task specific, and shared information and generate an augmented embedding for the individual downstream tasks in the MTL pipeline. We evaluate the proposed Split ânâ Merge Network on two distinct MTL tasks where we achieve state-of-the-art results for both. Our primary, ablation and interpretation evaluations indicate the robustness and flexibility of the propose approach and demonstrates its applicability to numerous, diverse real-world MTL applications.","Multi-task learning, Attention, Cuffless blood pressure measurement, Biomedical signal processing, Deep learning, Emotion recognition",Tharindu Fernando and Sridha Sridharan and Simon Denman and Clinton Fookes,https://www.sciencedirect.com/science/article/pii/S0031320322000322,https://doi.org/10.1016/j.patcog.2022.108551,0031-3203,2022,108551,126,Pattern Recognition,Split ânâ merge net: A dynamic masking network for multi-task attention,article,FERNANDO2022108551,
"Current anchor-free object detectors have obtained detection performances comparable to those of anchor-based object detectors while avoiding the weaknesses of anchor designs. However, two challenges limit the localization performance. First, such anchor-free detectors have one stage that predicts the classification and localization results directly. A large regression space reduces the localization performance of such methods. Second, most of the existing detectors extract features which are ineffective for accurate localization. In this paper, for the first challenge, we propose two-stage networks to predict regression results stage by stage, thereby reducing the scope of the prediction space. For the second challenge, we design two novel modules with the aim of extracting effective features for accurate localization. Experimental results validate that each module in our approach is effective and validate that our approach has better object localization performance than previous related and advanced methods.","Object detection, Accurate localization, Anchor-free, NMS-free, Two-stage",Zhengquan Piao and Junbo Wang and Linbo Tang and Baojun Zhao and Wenzheng Wang,https://www.sciencedirect.com/science/article/pii/S0031320322000048,https://doi.org/10.1016/j.patcog.2022.108523,0031-3203,2022,108523,126,Pattern Recognition,AccLoc: Anchor-Free and two-stage detector for accurate object localization,article,PIAO2022108523,
"A well-performed deep learning model in image segmentation relies on a large number of labeled data. However, it is hard to obtain sufficient high-quality raw data in industrial applications. Meta-learning, one of the most promising research areas, is recognized as a powerful tool for approaching image segmentation. To this end, this paper reviews the state-of-the-art image segmentation methods based on meta-learning. We firstly introduce the background of the image segmentation, including the methods and metrics of image segmentation. Second, we review the timeline of meta-learning and give a more comprehensive definition of meta-learning. The differences between meta-learning and other similar methods are compared comprehensively. Then, we categorize the existing meta-learning methods into model-based, optimization-based, and metric-based. For each categorization, the popular used meta-learning models are discussed in image segmentation. Next, we conduct comprehensive computational experiments to compare these models on two pubic datasets: ISIC-2018 and Covid-19. Finally, the future trends of meta-learning in image segmentation are highlighted.","Deep learning, Image segmentation, Meta-learning, Computer vision",Shuai Luo and Yujie Li and Pengxiang Gao and Yichuan Wang and Seiichi Serikawa,https://www.sciencedirect.com/science/article/pii/S003132032200067X,https://doi.org/10.1016/j.patcog.2022.108586,0031-3203,2022,108586,126,Pattern Recognition,Meta-seg: A survey of meta-learning for image segmentation,article,LUO2022108586,
"Biometric based systems are involved in many areas, from surveillance to user authentication, from autonomous systems to human-robot interactions. Head pose estimation (HPE) is the task to support biometric systems in which any of the biometric traits of the head is involved, as face, ear or iris. This particular biometric branch finds its application in driver attention detection, surveillance for recognition, face frontalization, best frame selection and so on. The goal of HPE is to determine the head pose orientation (yaw, pitch, roll). The implemented methods use different techniques depending on the kind of input. In this survey we present an overview of involved datasets, recent techniques and applications. We evaluate and compare the different approaches with respect to their advantages and practical usage. In addition, we propose a technical comparison between training and training-free techniques for the most popular HPE methods.","Biometrics, Head pose estimation, Face recognition, Frontalization",Andrea F. Abate and Carmen Bisogni and Aniello Castiglione and Michele Nappi,https://www.sciencedirect.com/science/article/pii/S0031320322000723,https://doi.org/10.1016/j.patcog.2022.108591,0031-3203,2022,108591,127,Pattern Recognition,Head pose estimation: An extensive survey on recent techniques and applications,article,ABATE2022108591,
"Establishing reliable correspondences is a fundamental task in computer vision, and it requires rich contextual information. In this paper, we propose a Channel-Spatial Difference Augment Network (CSDA-Net), by selectively aggregating information from spatial and channel aspects, to seek reliable correspondences for feature matching. Specifically, we firstly introduce the spatial and channel attention mechanism to construct a simple yet effective block for discriminately extracting the global context. After that, we design a Overlay Attention block by further exploiting the spatial and channel attention mechanism with different squeeze operations, to gather more comprehensive contextual information. Finally, the proposed CSDA-Net is able to achieve feature maps with a strong representative ability for feature matching due to the integration of the two novel blocks. Extensive experiments on outlier rejection and relative pose estimation have shown better performance improvements of our CSDA-Net over current state-of-the-art methods on both outdoor and indoor datasets.","Feature matching, Deep learning, Outlier rejection, Attention mechanism",Shunxing Chen and Linxin Zheng and Guobao Xiao and Zhen Zhong and Jiayi Ma,https://www.sciencedirect.com/science/article/pii/S0031320322000206,https://doi.org/10.1016/j.patcog.2022.108539,0031-3203,2022,108539,126,Pattern Recognition,CSDA-Net: Seeking reliable correspondences by channel-Spatial difference augment network,article,CHEN2022108539,
"One effective way to tackle unsupervised domain adaptation (UDA) on person re-identification (Re-ID) is to use clustering-based self-training approach, where a model is trained with hard pseudo-labels obtained from a clustering method. Using a hard pseudo-label, a sample is assigned to the cluster with the highest probability, which is sensitive to the incorrect clustering result due to imperfect clustering algorithms. Soft pseudo-labels can mitigate this issue by representing the sample with the full range of class probabilities from all clusters. Specifically, soft pseudo-labels comprise probabilities of full range classes, because they consider both the hard samples and easy samples. This will distract the model from learning more discriminative features in the hard examples. To solve this issue, we propose a coarse-to-fine refinement mechanism to produce robust refined soft pseudo-labels by progressively focusing more on the hard samples while less on the easy samples. The proposed refined soft pseudo-labels can be readily integrated into cross-entropy loss as a strong supervision to guide the model to learn more discriminative features. Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art unsupervised domain adaptation approaches on person Re-ID with a considerable margin. Code will be available at: http://github.com/Dingyuan-Zheng/ctf-UDA.","Person re-identification, Unsupervised domain adaptation, Clustering algorithms, Label noise, Soft pseudo-labels",Dingyuan Zheng and Jimin Xiao and Ke Chen and Xiaowei Huang and Lin Chen and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S0031320322000966,https://doi.org/10.1016/j.patcog.2022.108615,0031-3203,2022,108615,127,Pattern Recognition,Soft pseudo-Label shrinkage for unsupervised domain adaptive person re-identification,article,ZHENG2022108615,
"Zero-shot action recognition can recognize samples of unseen classes that are unavailable in training by exploring common latent semantic representation in samples. However, most methods neglected the connotative relation and extensional relation between the action classes, which leads to the poor generalization ability of the zero-shot learning. Furthermore, the learned classifier inclines to predict the samples of seen class, which leads to poor classification performance. To solve the above problems, we propose a two-stage deep neural network for zero-shot action recognition, which consists of a feature generation sub-network serving as the sampling stage and a graph attention sub-network serving as the classification stage. In the sampling stage, we utilize generative adversarial networks (GAN) trained by action features and word vectors of seen classes to synthesize the action features of unseen classes, which can balance the training sample data of seen classes and unseen classes. In the classification stage, we construct a knowledge graph (KG) based on the relationship between word vectors of action classes and related objects, and propose a graph convolution network (GCN) based on attention mechanism, which dynamically updates the relationship between action classes and objects, and enhances the generalization ability of zero-shot learning. In both stages, we all use word vectors as bridges for feature generation and classifier generalization from seen classes to unseen classes. We compare our method with state-of-the-art methods on UCF101 and HMDB51 datasets. Experimental results show that our proposed method improves the classification performance of the trained classifier and achieves higher accuracy.","Action recognition, Zero-shot learning, Generative adversarial networks, Graph convolution network",Bin Sun and Dehui Kong and Shaofan Wang and Jinghua Li and Baocai Yin and Xiaonan Luo,https://www.sciencedirect.com/science/article/pii/S0031320322000449,https://doi.org/10.1016/j.patcog.2022.108563,0031-3203,2022,108563,126,Pattern Recognition,"GAN for vision, KG for relation: A two-stage network for zero-shot action recognition",article,SUN2022108563,
"Nowadays, most classification networks use one-hot encoding to represent categorical data because of its simplicity. However, one-hot encoding may affect the generalization ability as it neglects inter-class correlations. We observe that, even when a neural network trained with one-hot labels produces incorrect predictions, it still pays attention to the target image region and reveals which classes confuse the network. Inspired by this observation, we propose a confusion-focusing mechanism to address the class-confusion issue. Our confusion-focusing mechanism is implemented by a two-branch network architecture. Its baseline branch generates confusing classes, and its FocusNet branch, whose architecture is flexible, discriminates correct labels from these confusing classes. We also introduce a novel focus-picking loss function to improve classification accuracy by encouraging FocusNet to focus on the most confusing classes. The experimental results validate that our FocusNet is effective for image classification on common datasets, and that our focus-picking loss function can also benefit the current neural networks in improving their classification accuracy.","Image classification, Inter-class correlations, Confusing classes",Xue Zhang and Zehua Sheng and Hui-Liang Shen,https://www.sciencedirect.com/science/article/pii/S003132032200190X,https://doi.org/10.1016/j.patcog.2022.108709,0031-3203,2022,108709,129,Pattern Recognition,FocusNet: Classifying better by focusing on confusing classes,article,ZHANG2022108709,
"Due to the heterogeneity gap, the data representations of different types of media are inconsistent. It is challenging to measure the fine-grained gap between different media. To this end, we propose a self-attention-based hybrid network to learn the common representations of different media data. Specifically, we first utilize a local self-attention layer to learn the common attention space between different media data. Then we propose a similarity concatenation method to understand the content relationship between features. To further improve the robustness of the model, we also learn a local position encoding to capture the spatial relationships between features. Therefore, our proposed approach can effectively reduce the gap between different feature distributions on cross-media retrieval tasks. Extensive experiments and ablation studies demonstrate that our proposed method achieves state-of-the-art performance. The source code and models are publicly available at: https://github.com/NUST-Machine-Intelligence-Laboratory/SAFGCMHN.","Fine-Grained, Cross-Media, Retrieval, Attention",Wei Shan and Dan Huang and Jiangtao Wang and Feng Zou and Suwen Li,https://www.sciencedirect.com/science/article/pii/S0031320322002291,https://doi.org/10.1016/j.patcog.2022.108748,0031-3203,2022,108748,130,Pattern Recognition,Self-Attention based fine-grained cross-media hybrid network,article,SHAN2022108748,
"We present two novel shape signature-based reflection symmetry detection methods with their theoretical underpinning and empirical evaluation. LIP-signature and R-signature share similar beneficial properties allowing to detect reflection symmetry directions in a high-performing manner. For the shape signature of a given shape, its merit profile is constructed to detect candidates of symmetry direction. A verification process is utilized to eliminate the false candidates by addressing Radon projections. The proposed methods can effectively deal with compound shapes which are challenging for traditional contour-based methods. To quantify the symmetric efficiency, a new symmetry measure is proposed over the range [0, 1]. Furthermore, we introduce two symmetry shape datasets with a new evaluation protocol and a lost measure for evaluating symmetry detectors. Experimental results using standard and new datasets suggest that the proposed methods prominently perform compared to state of the art.","Symmetry detection, Reflection symmetry, LIP-signature, -signature, Radon",Thanh Phuong Nguyen and Hung Phuoc Truong and Thanh Tuan Nguyen and Yong-Guk Kim,https://www.sciencedirect.com/science/article/pii/S0031320322001480,https://doi.org/10.1016/j.patcog.2022.108667,0031-3203,2022,108667,128,Pattern Recognition,Reflection symmetry detection of shapes based on shape signatures,article,NGUYEN2022108667,
"In this work, we propose an Augmented Visual Perception Learning (AVPL) method for Person Re-identification (ReID) which is inspired by the two-stream hypothesis theory of Human Visual System (HVS). Deep learning methods dominate ReID and many state-of-the-art performances are achieved from the perspective of optimizing the model of âwhatâ visual pathway. It does not blend âwhatâ and âwhereâ well. Our AVPL method uses the essential mechanism of the ventro-dorsal stream of the âwhereâ visual pathway to expand the perception field of the model, and integrates with the âwhatâ to complete the information of the visually salient regions. A novel Batch Attention (BA), the key component of our Augmented Visual Perception (AVP) module, is proposed to apply fusion and augmentation into all input feature maps within each batch. Through AVP module, the improved attention-based model attaches more importance to enhancement of salient features, therefore acquiring better perceptual ability of salient regions which provide the most distinguishably distinctions for ReID. Extensive experiments have been carried out on four main stream ReID datasets and two recognition datasets. In terms of ReID, our method achieves rank-1 accuracy of 95.2% on CUHK03-NP, 98.7% on Market-1501, 96.0% on DukeMTMC-reID and 88.8% on MSMT17-V1, outperforming the state-of-the-art methods by a large margin. Besides, it has been experimentally proven to be applicable and effective in other recognition tasks including facial expression recognition and action recognition with an improved accuracy.","Person Re-identification, Augmented visual perception learning, Batch attention, Two-stream hypothesis",Yewen Huang and Sicheng Lian and Haifeng Hu,https://www.sciencedirect.com/science/article/pii/S0031320322002175,https://doi.org/10.1016/j.patcog.2022.108736,0031-3203,2022,108736,129,Pattern Recognition,AVPL: Augmented visual perception learning for person Re-identification and beyond,article,HUANG2022108736,
"Concrete is the standard construction material for buildings, bridges, and roads. As safety plays a central role in the design, monitoring, and maintenance of such constructions, it is important to understand the cracking behavior of concrete. Computed tomography captures the microstructure of building materials and allows to study crack initiation and propagation. Manual segmentation of crack surfaces in large 3d images is not feasible. In this paper, automatic crack segmentation methods for 3d images are reviewed and compared. Classical image processing methods (edge detection filters, template matching, minimal path and region growing algorithms) and learning methods (convolutional neural networks, random forests) are considered and tested on semi-synthetic 3d images. Their performance strongly depends on parameter selection which should be adapted to the grayvalue distribution of the images and the geometric properties of the concrete. In general, the learning methods perform best, in particular for thin cracks and low grayvalue contrast.","Computed tomography, Fractional Brownian surface, 3d segmentation, Crack detection, Machine learning, Deep learning",Tin Barisin and Christian Jung and Franziska MÃ¼sebeck and Claudia Redenbach and Katja Schladitz,https://www.sciencedirect.com/science/article/pii/S003132032200228X,https://doi.org/10.1016/j.patcog.2022.108747,0031-3203,2022,108747,129,Pattern Recognition,Methods for segmenting cracks in 3d images of concrete: A comparison based on semi-synthetic images,article,BARISIN2022108747,
"Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. We propose to learn these filters as combinations of preset spectral filters defined by the Discrete Cosine Transform (DCT). Our proposed DCT-based harmonic blocks replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. Using DCT energy compaction properties, we demonstrate how the harmonic networks can be efficiently compressed by truncating high-frequency information in harmonic blocks thanks to the redundancies in the spectral domain. We report extensive experimental validation demonstrating benefits of the introduction of harmonic blocks into state-of-the-art CNN models in image classification, object detection and semantic segmentation applications.","Harmonic network, Convolutional neural network, Discrete cosine transform, Image classification, Object detection, Semantic segmentation",Matej Ulicny and Vladimir A. Krylov and Rozenn Dahyot,https://www.sciencedirect.com/science/article/pii/S0031320322001881,https://doi.org/10.1016/j.patcog.2022.108707,0031-3203,2022,108707,129,Pattern Recognition,Harmonic convolutional networks based on discrete cosine transform,article,ULICNY2022108707,
"Matrix factorization is a popular matrix completion method, however, it is difficult to determine the ranks of the factor matrices. We propose two new sparse matrix factorization methods with l2,1 norm to explicitly force the row sparseness of the factor matrices, where the rank of the factor matrices is adaptively controlled by the regularization coefficient. We further theoretically prove the convergence property of our algorithms. The experimental results on the simulation and the benchmark datasets show that our methods achieve superior performance than its counterparts. Moreover our proposed methods can attain comparable performance with the deep learning-based matrix completion methods.","Matrix Completion, Matrix Factorization,  Norm Regularization, Alternative Optimization, Sparse Property",Xiaobo Jin and Jianyu Miao and Qiufeng Wang and Guanggang Geng and Kaizhu Huang,https://www.sciencedirect.com/science/article/pii/S0031320322001364,https://doi.org/10.1016/j.patcog.2022.108655,0031-3203,2022,108655,127,Pattern Recognition,"Sparse matrix factorization with L2,1 norm for matrix completion",article,JIN2022108655,
"Mobility-on-demand (MoD) systems represent a rapidly developing mode of transportation wherein travel requests are dynamically handled by a coordinated fleet of vehicles. Crucially, the efficiency of an MoD system highly depends on how well supply and demand distributions are aligned in spatio-temporal space (i.e., to satisfy user demand, cars have to be available in the correct place and at the desired time). To do so, we argue that predictive models should aim to explicitly disentangle between temporal and spatial variability in the evolution of urban mobility demand. However, current approaches typically ignore this distinction by either treating both sources of variability jointly, or completely ignoring their presence in the first place. In this paper, we propose recurrent flow networks11Code available at https://www.github.com/DanieleGammelli/recurrent-flow-nets (RFN), where we explore the inclusion of (i) latent random variables in the hidden state of recurrent neural networks to model temporal variability, and (ii) normalizing flows to model the spatial distribution of mobility demand. We demonstrate how predictive models explicitly disentangling between spatial and temporal variability exhibit several desirable properties, and empirically show how this enables the generation of distributions matching potentially complex urban topologies.","Urban mobility, Latent variable models, Normalizing flows, Variational inference",Daniele Gammelli and Filipe Rodrigues,https://www.sciencedirect.com/science/article/pii/S0031320322002333,https://doi.org/10.1016/j.patcog.2022.108752,0031-3203,2022,108752,129,Pattern Recognition,Recurrent flow networks: A recurrent latent variable model for density estimation of urban mobility,article,GAMMELLI2022108752,
"Visual explanations for convolutional neural networks (CNNs) act as the backbone for weakly supervised segmentation with image-level labels. This paper proposes a high-resolution rectified gradient-based class activation mapping with bounding box annotations (bbox) to improve the initial seed for weakly supervised segmentation (WSS) tasks. HRCAM extends Grad-CAM by separating the gradient maps from the class activation maps from the shallow layer for higher resolution. Gradient rectified methods are proposed to improve the visualization and WSS score. Experiments and evaluations are conducted to verify the performance of HRCAM-BB on Pascal VOC 2012 and COCO datasets. On Pascal VOC 2012 set, our method achieves outstanding performance with a mean intersection over union (mIOU) of 71.6 with image-level labels and 78.2 with bbox on WSSS, and increases the WSIS mIOU (AP50) to 52.1 with image-level labels, and 61.9 with bbox. our method surpasses the previous SOTA approach in the same condition.",,Tianyou Zheng and Qiang Wang and Yue Shen and Xiang Ma and Xiaotian Lin,https://www.sciencedirect.com/science/article/pii/S0031320322002059,https://doi.org/10.1016/j.patcog.2022.108724,0031-3203,2022,108724,129,Pattern Recognition,High-resolution rectified gradient-based visual explanations for weakly supervised segmentation,article,ZHENG2022108724,
"This work addresses time series classifier recommendation for the first time in the literature by considering several recommendation forms or meta-targets: classifier accuracies, complete ranking, top-M ranking, best set and best classifier. For this, an ad-hoc set of quick estimators of the accuracies of the candidate classifiers (landmarkers) are designed, which are used as predictors for the recommendation system. The performance of our recommender is compared with the performance of a standard method for non-sequential data and a set of baseline methods, which our method outperforms in 7 of the 9 considered scenarios. Since some meta-targets can be inferred from the predictions of other more fine-grained meta-targets, the last part of the work addresses the hierarchical inference of meta-targets. The experimentation suggests that, in many cases, a single model is sufficient to output many types of meta-targets with competitive results.","Time series classification, Meta-learning, Landmarkers, Hierarchical inference, Meta-targets",A. Abanda and U. Mori and Jose A. Lozano,https://www.sciencedirect.com/science/article/pii/S0031320322001522,https://doi.org/10.1016/j.patcog.2022.108671,0031-3203,2022,108671,128,Pattern Recognition,Time series classifier recommendation by a meta-learning approach,article,ABANDA2022108671,
"This paper addresses the image/surface deformation problem by estimating interpolation functions pixel by pixel(or voxel by voxel) between control point pairs using labeled control points and unlabeled feature points as input. The labeled control points are usually selected by users and labeled through user operations; the unlabeled feature points are extracted from the source image. We formulate the interpolation function estimation at each pixel as a weighted semi-supervised learning problem. Specially, we employ moving least squares to estimate the nonrigid deformation function according to the weights between each pixel and the labeled control points and exploit manifold regularization to preserve the intrinsic geometric information of the unlabeled feature points contained in the object. Moreover, we define the nonrigid deformation function in a reproducing kernel Hilbert space to derive a closed-form solution. To reduce the computational complexity, we also adopt a sparse approximation to realize a fast implementation. It is worth mentioning that our proposed method is a unified framework with two different basis functions. Both basis-function-based methods are applied to 2D image deformation, 3D surface deformation, and medical image registration. Extensive experiments on the data and the resulting mean opinion score (MOS) on the 2D deformation demonstrate that our methods are superior to state-of-the-art ones.","Nonrigid deformation, Manifold regularization, Gaussian kernel, TPS kernel, Medical image registration",Huabing Zhou and Zhichao Xu and Yulu Tian and Zhenghong Yu and Yanduo Zhang and Jiayi Ma,https://www.sciencedirect.com/science/article/pii/S0031320322001765,https://doi.org/10.1016/j.patcog.2022.108695,0031-3203,2022,108695,128,Pattern Recognition,Interpolation-based nonrigid deformation estimation under manifold regularization constraint,article,ZHOU2022108695,
"We first study the suitability of behavioral biometrics to distinguish between computers and humans, commonly named as bot detection. We then present BeCAPTCHA-Mouse, a bot detector based on: i) a neuromotor model of mouse dynamics to obtain a novel feature set for the classification of human and bot samples; and ii) a learning framework involving real and synthetically generated mouse trajectories. We propose two new mouse trajectory synthesis methods for generating realistic data: a) a function-based method based on heuristic functions, and b) a data-driven method based on Generative Adversarial Networks (GANs) in which a Generator synthesizes human-like trajectories from a Gaussian noise input. Experiments are conducted on a new testbed also introduced here and available in GitHub: BeCAPTCHA-Mouse Benchmark; useful for research in bot detection and other mouse-based HCI applications. Our benchmark data consists of 15,000 mouse trajectories including real data from 58 users and bot data with various levels of realism. Our experiments show that BeCAPTCHA-Mouse is able to detect bot trajectories of high realism with 93% of accuracy in average using only one mouse trajectory. When our approach is fused with state-of-the-art mouse dynamic features, the bot detection accuracy increases relatively by more than 36%, proving that mouse-based bot detection is a fast, easy, and reliable tool to complement traditional CAPTCHA systems.","CAPTCHA, Bot detection, Behavior, Biometrics, Mouse, Neuromotor",Alejandro Acien and Aythami Morales and Julian Fierrez and Ruben Vera-Rodriguez,https://www.sciencedirect.com/science/article/pii/S0031320322001248,https://doi.org/10.1016/j.patcog.2022.108643,0031-3203,2022,108643,127,Pattern Recognition,BeCAPTCHA-Mouse: Synthetic mouse trajectories and improved bot detection,article,ACIEN2022108643,
"In this work we present a novel approach for 3D layout recovery of indoor environments using a non-central acquisition system. From a single non-central panorama, full and scaled 3D lines can be independently recovered by geometry reasoning without additional nor scale assumptions. However, their sensitivity to noise and complex geometric modeling has led these panoramas and required algorithms being little investigated. Our new pipeline aims to extract the boundaries of the structural lines of an indoor environment with a neural network and exploit the properties of non-central projection systems in a new geometrical processing to recover scaled 3D layouts. The results of our experiments show that we improve state-of-the-art methods for layout recovery and line extraction in non-central projection systems. We completely solve the problem both in Manhattan and Atlanta environments, handling occlusions and retrieving the metric scale of the room without extra measurements. As far as the authorsâ knowledge goes, our approach is the first work using deep learning on non-central panoramas and recovering scaled layouts from single panoramas.","Omnidirectional vision, 3D vision, Non-central cameras, Layout recovery, Scene understanding",Bruno Berenguel-Baeta and Jesus Bermudez-Cameo and Jose J. Guerrero,https://www.sciencedirect.com/science/article/pii/S0031320322002217,https://doi.org/10.1016/j.patcog.2022.108740,0031-3203,2022,108740,129,Pattern Recognition,Atlanta scaled layouts from non-central panoramas,article,BERENGUELBAETA2022108740,
"There exist uncertain data in the real world due to some factors such as imprecise measurements and noise. Unlike deterministic data, the features of samples in uncertain data are often described by interval numbers or random vectors with probability density functions. In this paper we propose novel twin support vector machines (TSVMs) to handle uncertain data. In the proposed models which are referred to as uncertainty-aware TSVMs, each uncertain sample is modeled as a random vector with Gaussian distributions. To deal with the multi-dimensional integrals in the original models, we derive an interesting and important theorem which helps us transform the original models into the model involving one-dimensional integrals. The simplification of models makes the optimization problem tractable and the simplified models are solved by using the quasi-Newton optimization algorithm. The proposed decision rule allows us to classify uncertain samples with means and covariance matrices. In addition, we extend the proposed models to their kernel versions to capture the nonlinear structure of uncertain data. Experiments on a series of data sets have been performed to demonstrate that the proposed models gain better classification performance than some existing algorithms, especially for representing uncertain cross-plane problems.","Uncertain data, Twin support vector machines, Halfspaces, Kernel functions, Data classification",Zhizheng Liang and Lei Zhang,https://www.sciencedirect.com/science/article/pii/S003132032200187X,https://doi.org/10.1016/j.patcog.2022.108706,0031-3203,2022,108706,129,Pattern Recognition,Uncertainty-aware twin support vector machines,article,LIANG2022108706,
"In human eyes, macula is responsible for sharp central vision with fovea in the center. The location of fovea becomes an important landmark in diagnosing the retinal diseases. As macula doesnât have the clear boundary and obvious shape, deep learning techniques to locate the fovea often fail in complicated lesions and insufficient training samples, and the unsupervised method is incapable for illumination variations. In this paper, a new unsupervised fovea localization method using the retinal raphe and region searching is proposed, and the blood vessel vector (BVV) model is developed. After detecting blood vessels and OD by U-net and probability bubbles, the BVVs are conceived and the retinal raphe is obtained by summating all the BVVs, then the fovea is estimated through the local region searching. Compared with the parabola model, the BVV model does not involve the coordinate transformation and reduces the complexity to the linear time cost O(N). Two other unsupervised techniques the parabola model and intensity searching and five supervised techniques cGAN, U-net, DRNet, MedTnet and EANet are included and compared. The global feature of retinal vessels is utilized which makes the proposed method more robust to the lesions than the other localization methods. The experiments on public datasets Kaggle, MESSIDOR and IDRiD validate the effectiveness of the proposed method by the studentâs t-test, and our method obtains the least average Euclidean distance to the groundtruth on Kaggle and almost least on Base 33 of MESSIDOR.","Blood vessel vector (BVV), Fovea localization, Retinal raphe, Probability bubble, Region search",Yinghua Fu and Ge Zhang and Jiang Li and Dongyan Pan and Yongxiong Wang and Dawei Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322001923,https://doi.org/10.1016/j.patcog.2022.108711,0031-3203,2022,108711,129,Pattern Recognition,Fovea localization by blood vessel vector in abnormal fundus images,article,FU2022108711,
"Molecular machine learning based on graph neural network has a broad prospect in molecular property identification in drug discovery. Molecules contain many types of substructures that may affect their properties. However, conventional methods based on graph neural networks only consider the interaction information between nodes, which may lead to the oversmoothing problem in the multi-hop operations. These methods may not efficiently express the interacting information between molecular substructures. Hence, We develop a Molecular SubStructure Graph ATtention (MSSGAT) network to capture the interacting substructural information, which constructs a composite molecular representation with multi-substructural feature extraction and processes such features effectively with a nested convolution plus readout scheme. We evaluate the performance of our model on 13 benchmark data sets, in which 9 data sets are from the ChEMBL data base and 4 are the SIDER, BBBP, BACE, and HIV data sets. Extensive experimental results show that MSSGAT achieves the best results on most of the data sets compared with other state-of-the-art methods.","Molecular substructure, Graph attention, Molecular property identification",Xian-bin Ye and Quanlong Guan and Weiqi Luo and Liangda Fang and Zhao-Rong Lai and Jun Wang,https://www.sciencedirect.com/science/article/pii/S0031320322001406,https://doi.org/10.1016/j.patcog.2022.108659,0031-3203,2022,108659,128,Pattern Recognition,Molecular substructure graph attention network for molecular property identification in drug discovery,article,YE2022108659,
"Objective To develop and validate a novel convolutional neural network (CNN) termed âSuper U-Netâ for medical image segmentation. Methods Super U-Net integrates a dynamic receptive field module and a fusion upsampling module into the classical U-Net architecture. The model was developed and tested to segment retinal vessels, gastrointestinal (GI) polyps, skin lesions on several image types (i.e., fundus images, endoscopic images, dermoscopic images). We also trained and tested the traditional U-Net architecture, seven U-Net variants, and two non-U-Net segmentation architectures. K-fold cross-validation was used to evaluate performance. The performance metrics included Dice similarity coefficient (DSC), accuracy, positive predictive value (PPV), and sensitivity. Results Super U-Net achieved average DSCs of 0.808Â±0.0210, 0.752Â±0.019, 0.804Â±0.239, and 0.877Â±0.135 for segmenting retinal vessels, pediatric retinal vessels, GI polyps, and skin lesions, respectively. The Super U-net consistently outperformed U-Net, seven U-Net variants, and two non-U-Net segmentation architectures (p < 0.05). Conclusion Dynamic receptive fields and fusion upsampling can significantly improve image segmentation performance.","Image segmentation, U-Net, Dynamic receptive field, Fusion upsampling",Cameron Beeche and Jatin P Singh and Joseph K Leader and Naciye S Gezer and Amechi P Oruwari and Kunal K Dansingani and Jay Chhablani and Jiantao Pu,https://www.sciencedirect.com/science/article/pii/S0031320322001509,https://doi.org/10.1016/j.patcog.2022.108669,0031-3203,2022,108669,128,Pattern Recognition,Super U-Net: A modularized generalizable architecture,article,BEECHE2022108669,
"Unsupervised domain adaptation addresses the problem of classifying data in an unlabeled target domain, given labeled source domain data that share a common label space but follow a different distribution. Most of the recent methods take the approach of explicitly aligning feature distributions between the two domains. Differently, motivated by the fundamental assumption for domain adaptability, we re-cast the domain adaptation problem as discriminative clustering of target data, given strong privileged information provided by the closely related, labeled source data. Technically, we use clustering objectives based on a robust variant of entropy minimization that adaptively filters target data, a soft Fisher-like criterion, and additionally the cluster ordering via centroid classification. To distill discriminative source information for target clustering, we propose to jointly train the network using parallel, supervised learning objectives over labeled source data. We term our method of distilled discriminative clustering for domain adaptation as DisClusterDA. We also give geometric intuition that illustrates how constituent objectives of DisClusterDA help learn class-wisely pure, compact feature distributions. We conduct careful ablation studies and extensive experiments on five popular benchmark datasets, including a multi-source domain adaptation one. Based on commonly used backbone networks, DisClusterDA outperforms existing methods on these benchmarks. It is also interesting to observe that in our DisClusterDA framework, adding an additional loss term that explicitly learns to align class-level feature distributions across domains does harm to the adaptation performance, though more careful studies in different algorithmic frameworks are to be conducted.","Deep learning, Unsupervised domain adaptation, Image classification, Knowledge distillation, Deep discriminative clustering, Implicit domain alignment",Hui Tang and Yaowei Wang and Kui Jia,https://www.sciencedirect.com/science/article/pii/S0031320322001194,https://doi.org/10.1016/j.patcog.2022.108638,0031-3203,2022,108638,127,Pattern Recognition,Unsupervised domain adaptation via distilled discriminative clustering,article,TANG2022108638,
"Person search is an extended task of person re-identification (Re-ID). However, most existing one-step person search works do not study how to employ existing Re-ID models to improve the one-step person search. To address this issue, we propose a Teacher-guided Disentangling Network (TDN) to make the one-step person search enjoy the merits of existing Re-ID research. The proposed TDN can significantly boost person search performance by transferring the advanced person Re-ID knowledge to the person search model. In the proposed TDN, for better knowledge transfer from the Re-ID teacher model to the one-step person search model, we design a new one-step person search base framework by partially disentangling the two subtasks. Besides, we propose a Knowledge Transfer Bridge module to bridge the scale gap caused by different input formats between the Re-ID model and the one-step person search model. Moreover, we also propose a Ranking with Context Persons strategy to exploit the context information in panoramic images for better ranking. Experiments on two public person search datasets demonstrate the favorable performance of the proposed method.","Person search, Person re-identification, Knowledge transfer, Teacher-guided disentangling network, Context ranking",Chuang Liu and Hua Yang and Qin Zhou and Shibao Zheng,https://www.sciencedirect.com/science/article/pii/S0031320322001352,https://doi.org/10.1016/j.patcog.2022.108654,0031-3203,2022,108654,127,Pattern Recognition,Making person search enjoy the merits of person re-identification,article,LIU2022108654,
"When modelling censored observations (i.e. data in which the value of a measurement or observation is un-observable beyond a given threshold), a typical approach in current regression methods is to use a censored-Gaussian (i.e. Tobit) model to describe the conditional output distribution. In this paper, as in the case of missing data, we argue that exploiting correlations between multiple outputs can enable models to better address the bias introduced by censored data. To do so, we introduce a heteroscedastic multi-output Gaussian process model which combines the non-parametric flexibility of GPs with the ability to leverage information from correlated outputs under input-dependent noise conditions. To address the resulting inference intractability, we further devise a variational bound to the marginal log-likelihood suitable for stochastic optimization. We empirically evaluate our model against other generative models for censored data on both synthetic and real world tasks and further show how it can be generalized to deal with arbitrary likelihood functions. Results show how the added flexibility allows our model to better estimate the underlying non-censored (i.e. true) process under potentially complex censoring dynamics.","Censored data, Gaussian processes, Variational inference",Daniele Gammelli and Kasper Pryds Rolsted and Dario Pacino and Filipe Rodrigues,https://www.sciencedirect.com/science/article/pii/S0031320322002321,https://doi.org/10.1016/j.patcog.2022.108751,0031-3203,2022,108751,129,Pattern Recognition,Generalized multi-output Gaussian process censored regression,article,GAMMELLI2022108751,
"Accurate and automatic breast tumor segmentation based on dynamic contrast-enhancement magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer analysis. However, the background parenchymal enhancement and large variations in tumor size, shape or appearance make the task very challenging, and also the segmentation performance is still not satisfactory, especially for non-mass enhancement (NME) and small size tumors (â¤2Â cm). To address these challenges, we propose a novel 3D affinity learning based multi-branch ensemble network for accurate breast tumor segmentation. Specifically, two different types of subnetworks are built to form a multi-branch network. The two subnetworks are equipped with effective operation components, i.e., residual connection and channel-wise attention or making use of dense connectivity patterns, which can process the input images in parallel. Second, we propose an end-to-end trainable 3D affinity learning based refinement module by calculating the similarities between features of voxels, which is useful in discovering more pixels belonging to breast tumors. Third, two local affinity matrices are constructed by 3D affinity learning, which are used to refine the segmentation outputs of two subnetworks, respectively. Finally, a novel ensemble module is proposed to combine the information derived from the subnetworks, which can hierarchically merge the local and global affinity matrices derived from subnetworks. A large-scale breast DCE-MR images dataset with 420 subjects are built for evaluation, and comprehensive experiments have been conducted to demonstrate that our proposed method achieves superior performance over state-of-the-art medical image segmentation methods.","Breast tumor segmentation, Three-dimensional affinity learning based refinement, Multi-branch ensemble network",Lei Zhou and Shuai Wang and Kun Sun and Tao Zhou and Fuhua Yan and Dinggang Shen,https://www.sciencedirect.com/science/article/pii/S0031320322002047,https://doi.org/10.1016/j.patcog.2022.108723,0031-3203,2022,108723,129,Pattern Recognition,Three-dimensional affinity learning based multi-branch ensemble network for breast tumor segmentation in MRI,article,ZHOU2022108723,
"Unsupervised clustering categorizes a sample set into several groups, where the samples in the same group share high-level concepts. As the clustering performances are heavily determined by the metric to assess the similarity between sample pairs, we propose to learn a deep similarity score function and use it to capture the correlations between sample pairs for improved clustering. We formulate the learning procedure in a ranking framework and introduce two new supervisory signals to train our model. Specifically, we train the similarity score function to guarantee 1) a sample should have a higher level of similarity with its nearest neighbors than others in order to achieve correct clustering, and 2) the ordering of the similarity between neighboring sample pairs should be preserved in order to achieve robust clustering. To this end, we not only study the relevance between neighboring sample pairs for local structure learning, but also study the relevance between each sample and the boundary samples for global structure learning. Extensive experiments on seven public available datasets validate the effectiveness of our proposed framework, including face image clustering, object image clustering, and real-world image clustering.","Image clustering, Order preserving, Deep representation learning, Score function learning",Jinghua Wang and Li Wang and Jianmin Jiang,https://www.sciencedirect.com/science/article/pii/S0031320322001510,https://doi.org/10.1016/j.patcog.2022.108670,0031-3203,2022,108670,128,Pattern Recognition,Preserving similarity order for unsupervised clustering,article,WANG2022108670,
"Clustering categorical data is an important task of machine learning, since the type of data widely exists in real world. However, the lack of an inherent order on the domains of categorical features prevents most of classical clustering algorithms from being directly applied for the type of data. Therefore, it is very key issue to learn an appropriate representation of categorical data for the clustering task. In order to address this issue, we develop a categorical data clustering framework based on graph representation. In this framework, a graph-based representation method for categorical data is proposed, which learns the representation of categorical values from their similar graph to provide similar representations for similar categorical values. We compared the proposed framework with other representation methods for categorical data clustering on benchmark data sets. The experiment results illustrate the proposed framework is very effective, compared to other methods.","Cluster analysis, Categorical data clustering, Data representation, Graph embedding",Liang Bai and Jiye Liang,https://www.sciencedirect.com/science/article/pii/S0031320322001753,https://doi.org/10.1016/j.patcog.2022.108694,0031-3203,2022,108694,128,Pattern Recognition,A categorical data clustering framework on graph representation,article,BAI2022108694,
"Kernel discriminant analysis (KDA), the nonlinear extension of linear Discriminant Analysis (LDA), is a popular tool for learning one or multiple categories in nonlinear data sets. However, in most modern pattern recognition applications such as video surveillance, data are collected in flow and require sequential processing. In this context, KDA is faced two critical issues: an original formulation unsuited to the dynamic nature of the data and an increasing memory requirement for the kernel matrix storage. Motivated by the state-of-the-art performance reported by the null KDA, we propose in this paper a new solution to solve the null KDA (NKDA) in the context of data streams. Compared to previous works, our contribution is based on three points: first, we develop an exact incremental scheme which guarantees accurate solutions. Secondly, we develop a compression mechanism based on the following observation: rger the size of the training data set more the distances in the null space contract This property of the null space leads to formulate an indicator of redundancy in the training data set. This criterion is the cornerstone of our incremental KNDA because it authorizes incremental learning on large-scale data sets. Third, the problem of novelty detection in multi-class and one-class scenarios is addressed. More precisely, the fact that distances in the null space change over the training period leads us to define adjustable novelty thresholds. Lastly, numerous experiments based on various publicly available data sets and state-of-the-art classifiers show that the proposed method is effective both for multi-class and one-class real applications.","Incremental kernel discriminant analysis, Null space, Compression mechanism, Multi-class learning, One-class learning, Novelty detection",F. Dufrenois,https://www.sciencedirect.com/science/article/pii/S0031320322001236,https://doi.org/10.1016/j.patcog.2022.108642,0031-3203,2022,108642,127,Pattern Recognition,Incremental and compressible kernel null discriminant analysis,article,DUFRENOIS2022108642,
"One major challenge of Weakly-supervised Temporal Action Localization (WTAL) is to handle diverse backgrounds in videos. To model background frames, most existing methods treat them as an additional action class. However, because background frames usually do not share common semantics, squeezing all the different background frames into a single class hinders network optimization. Moreover, the network would be confused and tends to fail when tested on videos with unseen background frames. To address this problem, we propose an Entropy Guided Attention Network (EGA-Net) to treat background frames as out-of-domain samples. Specifically, we design a two-branch module, where a domain branch detects whether a frame is an action by learning a class-agnostic attention map, and an action branch recognizes the action category of the frame by learning a class-specific attention map. By aggregating the two attention maps to model the joint domain-class distribution of frames, our EGA-Net can handle varying backgrounds. To train the class-agnostic attention map with only the video-level class labels, we propose an Entropy Guided Loss (EGL), which employs entropy as the supervision signal to distinguish action and background. Moreover, we propose a Global Similarity Loss (GSL) to enhance the action-specific attention map via action class center. Extensive experiments on THUMOS14, ActivityNet1.2 and ActivityNet1.3 datasets demonstrate the effectiveness of our EGA-Net.","Temporal action localization, Weakly-supervised learning, Entropy guided loss, Global similarity loss",Yi Cheng and Ying Sun and Hehe Fan and Tao Zhuo and Joo-Hwee Lim and Mohan Kankanhalli,https://www.sciencedirect.com/science/article/pii/S0031320322001996,https://doi.org/10.1016/j.patcog.2022.108718,0031-3203,2022,108718,129,Pattern Recognition,Entropy guided attention network for weakly-supervised action localization,article,CHENG2022108718,
"Recently, RGB-D salient object detection (SOD) has aroused widespread research interest. Existing RGB-D SOD approaches mainly consider the cross-modal information fusion in the decoder. And their multi-modal interaction mainly concentrates on the same level of features between RGB stream and depth stream. They do not deeply explore the coherence of multi-model features at different levels. In this paper, we design a two-stream deep interleaved encoder network to extract RGB and depth information and realize their mixing simultaneously. This network allows us to gradually learn multi-modal representation at different levels from shallow to deep. Moreover, to further fuse multi-modal features in the decoding stage, we propose a cross-modal mutual guidance module and a residual multi-scale aggregation module to implement the global guidance and local refinement of the salient region. Extensive experiments on six benchmark datasets demonstrate that the proposed approach performs favorably against most state-of-the-art methods under different evaluation metrics. During the testing stage, this model can run at a real-time speed of 93 FPS.","RGB-D salient object detection, Deep interleaved encoder, Cross-modal mutual guidance, Residual multi-scale feature aggregation, Real-time",Guang Feng and Jinyu Meng and Lihe Zhang and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320322001479,https://doi.org/10.1016/j.patcog.2022.108666,0031-3203,2022,108666,128,Pattern Recognition,Encoder deep interleaved network with multi-scale aggregation for RGB-D salient object detection,article,FENG2022108666,
"In this paper, we investigate a new representation learning approach, termed as Multiview Predictive Coding with Contrastive Learning (MPCCL), for person re-identification (re-ID). Different from the conventional re-ID approaches that focus on learning representations from semantic label, our approach learns the identification of invariant information via representation reconstruction, which explores more fine-grained semantic information in representation space. Specifically, given a chosen identity, the learned representation of its single view can be reconstructed by those of other views. Therefore, kernel density estimation (KDE) is firstly introduced for the adaptive reconstruction of the representation. Then, contrastive learning is adopted to increase the distance between the representations of the same views with different identities. Finally, representation reconstruction and contrastive learning jointly supervise the representation learning process, thus obtaining fine-grained semantic information and appearance-free representations. Extensive experiments on several re-ID datasets demonstrate that the proposed approach yields state-of-the-art results.","Person re-identification, Kernel density estimation, Representation construction, Contrastive learning",Junhui Yin and Jiyang Xie and Zhanyu Ma and Jun Guo,https://www.sciencedirect.com/science/article/pii/S0031320322001911,https://doi.org/10.1016/j.patcog.2022.108710,0031-3203,2022,108710,129,Pattern Recognition,MPCCL: Multiview predictive coding with contrastive learning for person re-identification,article,YIN2022108710,
"Data augmentation is beneficial for improving robustness of deep meta-learning. However, data augmentation methods for the recent deep meta-learning are still based on photometric or geometric manipulations or combinations of images. This paper proposes a generative adversarial autoaugment network (GA3N) for enlarging the augmentation search space and improving classification accuracy. To achieve, we first extend the search space of image augmentation by using GANs. However, the main challenge is to generate images suitable for the task. For solution, we find the best policy by optimizing a target and GAN losses alternatively. We then use the manipulated and generated samples determined by the policy network as augmented samples for improving the target tasks. To show the effects of our method, we implement classification networks by combining our GA3N and evaluate them on CIFAR-100 and Tiny-ImageNet datasets. As a result, we achieve better accuracy than the recent AutoAugment methods on each dataset.","Data augmentation, AutoAugment, Generative adversarial network, Classification, Deep learning, Adversarial learning",Vanchinbal Chinbat and Seung-Hwan Bae,https://www.sciencedirect.com/science/article/pii/S0031320322001182,https://doi.org/10.1016/j.patcog.2022.108637,0031-3203,2022,108637,127,Pattern Recognition,GA3N: Generative adversarial AutoAugment network,article,CHINBAT2022108637,
"Despite its promising preliminary results, existing cross-modality Visible-Infrared Person Re-IDentification (VI-PReID) models incorporating semantic (person) masks simply use these person masks as selection maps to separate person features from background regions. Such models do not dedicate to extracting more modality-invariant person body features in the VI-PReID network itself, thus leading to suboptimal results in VI-PReID. Differently, we aim to better capture person body information in the VI-PReID network itself for VI-PReID by exploiting the inner relations between person mask prediction and VI-PReID. To this end, a novel multi-task learning model is presented in this paper, where person body features obtained by person mask prediction potentially facilitate the extraction of discriminative modality-shared person body information for VI-PReID. On top of that, considering the task difference between person mask prediction and VI-PReID, we propose a novel task translation sub-network to transfer discriminative person body information, extracted by person mask prediction, into VI-PReID. Doing so enables our model to better exploit discriminative and modality-invariant person body information. Thanks to more discriminative modality-shared features, our method outperforms previous state-of-the-arts by a significant margin on several benchmark datasets. Our intriguing findings validate the effectiveness of extracting discriminative person body features for the VI-PReID task.","Cross-modality person re-identification, Person body information, Multi-task learning",Nianchang Huang and Kunlong Liu and Yang Liu and Qiang Zhang and Jungong Han,https://www.sciencedirect.com/science/article/pii/S0031320322001340,https://doi.org/10.1016/j.patcog.2022.108653,0031-3203,2022,108653,128,Pattern Recognition,Cross-modality person re-identification via multi-task learning,article,HUANG2022108653,
"For segmenting medical images, U-Net has become a popular and effective tool. However, it also has some shortcomings in segmenting fuzzy boundaries and eliminating interferences. Improvements of the original U-Net have been proposed by many authors, resulting in many variants such as MultiResUNet, DoubleU-Net and W-Net. Based on the common characteristics of these structures, we propose in this work a generalized structure by multiplying the folds of a fully convolutional network (FCN) for even more times, and thus name it as â2K-Fold-Netâ. The more folds in this structure provide more freedoms to create cross links between the neighboring folds. The influence of the fold-pair number K on its performance is also studied. The realizations with K up to 6 are compared to three other variants of cascaded U-Nets using the CVC-ClinicDB dataset. Then the special case â4-Fold-Netâ is further empowered with the feature enhancing functionalities recently seen in the attention-aware feature enhancement method. This new net is hence named as âEnhanced-Feature-4-Fold-Netâ, abbreviated as âEF3-Netâ. Finally, 2K-Fold-Net and EF3-Net have been compared with U-Net, SegNet, DoubleU-Net, MultiResUNet and its variants using four challenging medical image datasets. The results have demonstrated that the proposed nets outperform the other variants of U-Net, even with slightly lower amount of parameters. The code is available on: https://github.com/raik7/EF3-Net.","2K-Fold-Net, EF-Net, U-Net, AFE, Image segmentation",Yunchu Zhang and Jianfei Dong,https://www.sciencedirect.com/science/article/pii/S0031320322001066,https://doi.org/10.1016/j.patcog.2022.108625,0031-3203,2022,108625,127,Pattern Recognition,2K-Fold-Net and feature enhanced 4-Fold-Net for medical image segmentation,article,ZHANG2022108625,
"Despite recent improvements in analyzing large-scale 3D point clouds, several problems still exist: (a) segmentation models suffer from intra-class inconsistency and inter-class indistinction; (b) the existing methods ignore the inherent long-tailed class distribution of real-world 3D data. These problems result in unsatisfactory semantic segmentation predictions, especially in object adjacent areas. To handle these problems, this paper proposes a novel Adjacent areas Refinement Network (ARNet). Specifically, an Adjacent areas Refinement (AR) module is designed, which consists of two parallel attention blocks. Besides, our proposed attention blocks can process a large number of points (Nâ¼105) with a slight increase in the computational complexity and time cost. Additionally, to deal with the inherent long-tailed class distribution in real-world 3D data, imbalance adjustment loss and occupancy regression loss are introduced. Based on this, the proposed network can handle the classification of both majority and minority classes, which is essential in distinguishing the ambiguous parts in large-scale 3D scenes. The proposed AR module and the loss functions can be easily integrated into the cutting-edge backbone networks, contributing to better performance in modeling semantic inter-dependencies and significantly improving the accuracy of the state-of-the-art semantic segmentation methods on indoor and outdoor scenes.","Large-scale 3D point clouds, Attention, Long-tailed distribution, Segmentation",Mengtian Li and Yuan Xie and Lizhuang Ma,https://www.sciencedirect.com/science/article/pii/S0031320322002035,https://doi.org/10.1016/j.patcog.2022.108722,0031-3203,2022,108722,129,Pattern Recognition,Paying attention for adjacent areas: Learning discriminative features for large-scale 3D scene segmentation,article,LI2022108722,
"Machine learning methods exploit similarities in usersâ activity patterns to provide recommendations in applications across a wide range of fields including entertainment, dating, and commerce. However, in domains that demand protection of personally sensitive data, such as medicine or banking, how can we learn recommendation models without accessing the sensitive data and without inadvertently leaking private information? Many situations in the medical field prohibit centralizing the data from different hospitals and thus require learning from information kept in separate databases. We propose a new federated approach to learning global and local private models for recommendation without collecting raw data, user statistics, or information about personal preferences. Our method produces a set of locally learned prototypes that allow us to infer global behavioral patterns while providing differential privacy guarantees for users in any database of the system. By requiring only two rounds of communication, we both reduce the communication costs and avoid excessive privacy loss associated with typical federated learning iterative procedures. We test our framework on synthetic data, real federated medical data, and a federated version of Movielens ratings. We show that local adaptation of the global model allows the proposed method to outperform centralized matrix-factorization-based recommender system models, both in terms of the accuracy of matrix reconstruction and in terms of the relevance of recommendations, while maintaining provable privacy guarantees. We also show that our method is more robust and has smaller variance than individual models learned by independent entities.","Recommender systems, Differential Privacy, Federated Learning, Cross-Silo Federated Learning, Matrix Factorization",MÃ³nica Ribero and Jette Henderson and Sinead Williamson and Haris Vikalo,https://www.sciencedirect.com/science/article/pii/S0031320322002278,https://doi.org/10.1016/j.patcog.2022.108746,0031-3203,2022,108746,129,Pattern Recognition,Federating recommendations using differentially private prototypes,article,RIBERO2022108746,
"Covid-19, what a strange, unpredictable mutated virus. It has baffled many scientists, as no firm rule has yet been reached to predict the effect that the virus can inflict on people if they are infected with it. Recently, many researches have been introduced for diagnosing Covid-19; however, none of them pay attention to predict the effect of the virus on the person's body if the infection occurs but before the infection really takes place. Predicting the extent to which people will be affected if they are infected with the virus allows for some drastic precautions to be taken for those who will suffer from serious complications, while allowing some freedom for those who expect not to be affected badly. This paper introduces Covid-19 Prudential Expectation Strategy (CPES) as a new strategy for predicting the behavior of the person's body if he has been infected with Covid-19. The CPES composes of three phases called Outlier Rejection Phase (ORP), Feature Selection Phase (FSP), and Classification Phase (CP). For enhancing the classification accuracy in CP, CPES employs two proposed techniques for outlier rejection in ORP and feature selection in FSP, which are called Hybrid Outlier Rejection (HOR) method and Improved Binary Genetic Algorithm (IBGA) method respectively. In ORP, HOR rejects outliers in the training data using a hybrid method that combines standard division and Binary Gray Wolf Optimization (BGWO) method. On the other hand, in FSP, IBGA as a hybrid method selects the most useful features for the prediction process. IBGA includes Fisher Score (FScore) as a filter method to quickly select the features and BGA as a wrapper method to accurately select the features based on the average accuracy value from several classification models as a fitness function to guarantee the efficiency of the selected subset of features with any classifier. In CP, CPES has the ability to classify people based on their bodiesâ reaction to Covid-19 infection, which is built upon a proposed Statistical NaÃ¯ve Bayes (SNB) classifier after performing the previous two phases. CPES has been compared against recent related strategies in terms of accuracy, error, recall, precision, and run-time using Covid-19 dataset [1]. This dataset contains routine blood tests collected from people before and after their infection with covid-19 through a Web-based form created by us. CPES outperforms the competing methods in experimental results because it provides the best results with values of 0.87, 0.13, 0.84, and 0.79 for accuracy, error, precision, and recall.","Covid-19, Prediction, NaÃ¯ve Bayes, Prudential Expectation",Asmaa H. Rabie and Nehal A. Mansour and Ahmed I. Saleh and Ali E. Takieldeen,https://www.sciencedirect.com/science/article/pii/S0031320322001741,https://doi.org/10.1016/j.patcog.2022.108693,0031-3203,2022,108693,128,Pattern Recognition,Expecting individualsâ body reaction to Covid-19 based on statistical NaÃ¯ve Bayes technique,article,RABIE2022108693,
"Tabular structures in business documents offer a complementary dimension to the raw textual data. For instance, there is information about the relationships among pieces of information. Nowadays, digital mailroom applications have become a key service for workflow automation. Therefore, the detection and interpretation of tables is crucial. With the recent advances in information extraction, table detection and recognition has gained interest in document image analysis, in particular, with the absence of rule lines and unknown information about rows and columns. However, business documents usually contain sensitive contents limiting the amount of public benchmarking datasets. In this paper, we propose a graph-based approach for detecting tables in document images which do not require the raw content of the document. Hence, the sensitive content can be previously removed and, instead of using the raw image or textual content, we propose a purely structural approach to keep sensitive data anonymous. Our framework uses graph neural networks (GNNs) to describe the local repetitive structures that constitute a table. In particular, our main application domain are business documents. We have carefully validated our approach in two invoice datasets and a modern document benchmark. Our experiments demonstrate that tables can be detected by purely structural approaches.","Business document processing, Anonymized document processing, Table detection, Graph neural networks, Node and edge classification",Pau Riba and Lutz Goldmann and Oriol Ramos Terrades and Diede Rusticus and Alicia FornÃ©s and Josep LladÃ³s,https://www.sciencedirect.com/science/article/pii/S0031320322001224,https://doi.org/10.1016/j.patcog.2022.108641,0031-3203,2022,108641,127,Pattern Recognition,Table detection in business document images by message passing networks,article,RIBA2022108641,
"Building change detection (BCD) recently can be handled well under the booming of deep-learning based computer vision techniques. However, segmentation and recognition for objects with sharper boundaries still suffer from the poorly acquired high frequency information, which can result in the deteriorated annotation of building boundaries in BCD. To better obtain the high frequency pattern under the deep learning pipeline, we propose a high frequency attention-guided Siamese network (HFA-Net) in which a novel built-in high frequency attention block (HFAB) is applied. HFA-Net is designed to enhance high frequency information of buildings via HFAB which is composed of two main stages, i.e., the spatial-wise attention (SA) and the high frequency enhancement (HF). The SA firstly guides the model to search and focus on buildings, and HF is employed afterwards to highlight the high frequency information of the input feature maps. With high frequency information of buildings enhanced by HFAB, HFA-Net is able to better detect the edges of changed buildings, so as to improve the performance of BCD. Our proposed method is evaluated on three widely-used public datasets, i.e., WHU-CD, LEVIR-CD, and Google dataset. Remarkable experimental results on these datasets indicate that our proposed method can better detect edges of changed buildings and shows a better performance. The source code will be released at: https://github.com/HaiXing-1998/HFANet.","Building change detection, High frequency enhancement, Spatial-wise attention, Convolutional neural network",Hanhong Zheng and Maoguo Gong and Tongfei Liu and Fenlong Jiang and Tao Zhan and Di Lu and Mingyang Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322001984,https://doi.org/10.1016/j.patcog.2022.108717,0031-3203,2022,108717,129,Pattern Recognition,HFA-Net: High frequency attention siamese network for building change detection in VHR remote sensing images,article,ZHENG2022108717,
"Color image restoration is one of the basic tasks in pattern recognition. Unlike grayscale image, each color image has three channels in the RGB color space. Due to the inner-relationship within the three channels, color image restoration is usually much more difficult than its grayscale counterpart. Indeed, new problems such as color artifacts could emerge when the grayscale image processing methods are extended to color images directly. Note that one of the most effective gray image restoration methods is the weighted nuclear norm minimization (WNNM) approach. However, when applied to color images, the results of WNNM are usually not as promising as that of grayscale images. In order to solve this problem, in this paper, we propose to restore color images with the quaternion-based WNNM method (QWNNM) since the structure of color channels can be well preserved with quaternion representation. The proposed model can be solved efficiently by the alternating direction method of multipliers (ADMM). The theoretical analysis of the optimal solution is also presented. Numerical experiments are carefully conducted with different kinds of degradation to illustrate the superior performance of our proposed QWNNM over the state-of-the-art methods, including a celebrated deep learning approach, in both visual quality and numerical results.","Quaternion representation, Color image restoration, Weighted nuclear norm, Variational method, Low-rank matrix analysis",Chaoyan Huang and Zhi Li and Yubing Liu and Tingting Wu and Tieyong Zeng,https://www.sciencedirect.com/science/article/pii/S0031320322001467,https://doi.org/10.1016/j.patcog.2022.108665,0031-3203,2022,108665,128,Pattern Recognition,Quaternion-based weighted nuclear norm minimization for color image restoration,article,HUANG2022108665,
"Graph Convolutional Network (GCN) has emerged as a new technique for hyperspectral image (HSI) classification. However, in current GCN-based methods, the graphs are usually constructed with manual effort and thus is separate from the classification task, which could limit the representation power of GCN. Moreover, the employed graphs often fail to encode the global contextual information in HSI. Hence, we propose a Multi-level Graph Learning Network (MGLN) for HSI classification, where the graph structural information at both local and global levels can be learned in an end-to-end fashion. First, MGLN employs attention mechanism to adaptively characterize the spatial relevance among image regions. Then localized feature representations can be produced and further used to encode the global contextual information. Finally, prediction can be acquired with the help of both local and global contextual information. Experiments on three real-world hyperspectral datasets reveal the superiority of our MGLN when compared with the state-of-the-art methods.","Graph convolutional network, Graph-based machine learning, Hyperspectral image classification, Remote sensing, Graph structural learning",Sheng Wan and Shirui Pan and Shengwei Zhong and Jie Yang and Jian Yang and Yibing Zhan and Chen Gong,https://www.sciencedirect.com/science/article/pii/S0031320322001868,https://doi.org/10.1016/j.patcog.2022.108705,0031-3203,2022,108705,129,Pattern Recognition,Multi-level graph learning network for hyperspectral image classification,article,WAN2022108705,
"Total Variation and Low-Rank regularizations have shown significant successes in machine learning, data mining, and image processing in past decades. This paper develops the general nonconvex composite regularized model, which contains previous regularizers and motivates novel ones. Although the classical Alternating Direction Methods of Multiplier (ADMM) algorithm is applicable for this model, the nonconvexity of the problem and the complicacy of choosing the parameters increase the difficulty in the use of ADMM. Thus, by the penalty method, we propose the Alternating Minimization (AM) algorithm, whose convergence results are proved under mild assumptions. The proposed model and algorithm are applied to the image restoration problem. Numerical results demonstrate the efficiency of our model and algorithm.","Low-Rank, Total Variation, Nonconvex and nonsmooth minimization, Regularization, image restoration",Tao Sun and Dongsheng Li,https://www.sciencedirect.com/science/article/pii/S003132032200173X,https://doi.org/10.1016/j.patcog.2022.108692,0031-3203,2022,108692,130,Pattern Recognition,"General nonconvex total variation and low-rank regularizations: Model, algorithm and applications",article,SUN2022108692,
"Accurate and automatic segmentation of medical images can greatly assist the clinical diagnosis and analysis. However, it remains a challenging task due to (1) the diversity of scale in the medical image targets and (2) the complex context environments of medical images, including ambiguity of structural boundaries, complexity of shapes, and the heterogeneity of textures. To comprehensively tackle these challenges, we propose a novel and effective iterative edge attention network (EANet) for medical image segmentation with steps as follows. First, we propose a dynamic scale-aware context (DSC) module, which dynamically adjusts the receptive fields to extract multi-scale contextual information efficiently. Second, an edge-attention preservation (EAP) module is employed to effectively remove noise and help the edge stream focus on processing only the boundary-related information. Finally, a multi-level pairwise regression (MPR) module is designed to combine the complementary edge and region information for refining the ambiguous structure. This iterative optimization helps to learn better representations and more accurate saliency maps. Extensive experimental results demonstrate that the proposed network achieves superior segmentation performance to state-of-the-art methods in four different challenging medical segmentation tasks, including lung nodule segmentation, COVID-19 infection segmentation, lung segmentation, and thyroid nodule segmentation. The source code of our method is available at https://github.com/DLWK/EANet","Medical image segmentation, Dynamic scale-aware context, Edge attention preservation, Multi-level pairwise regression, Computer-aided diagnosis (CAD)",Kun Wang and Xiaohong Zhang and Xiangbo Zhang and Yuting Lu and Sheng Huang and Dan Yang,https://www.sciencedirect.com/science/article/pii/S0031320322001170,https://doi.org/10.1016/j.patcog.2022.108636,0031-3203,2022,108636,127,Pattern Recognition,EANet: Iterative edge attention network for medical image segmentation,article,WANG2022108636,
"Due to the strong nonlinear representation capabilities of deep neural networks and the low storage and high efficiency characteristics of hash learning, deep cross-modal hashing has been propelled to the forefront of academics. How to preferably bridge semantic relevance to further bridge the semantic modality gap is the vital bottleneck to improve model performance. Confronting samples with rich semantics, how to comprehensively explore the hidden correlations and establish more precise modality relationships is the primary issue to be solved. In this work, we propose a novel deep hashing method called Multi-Label Semantic Supervised Graph Attention Hashing (MS2GAH), which is an end-to-end framework that integrates graph attention networks (GATs). It constructs graph features through the adjacency of nodes and assigns different weights to adjacent edges to enhance the robustness of the model. Simultaneously, multi-label annotations are utilized to bridge the semantic relevance between modalities in a more fine-grained manner. To make preferable use of rich semantic information, an end-to-end label encoder is designed to mine high-level semantics from multi-label annotations to guide the feature extraction process of specific-modality networks, thereby further narrowing the modality gap. Finally, extensive experiments have been conducted on four datasets, and the results show that MS2GAH is superior to other baselines and one step forward.","Cross-modal retrieval, Deep hashing, Graph attention network",Youxiang Duan and Ning Chen and Peiying Zhang and Neeraj Kumar and Lunjie Chang and Wu Wen,https://www.sciencedirect.com/science/article/pii/S0031320322001571,https://doi.org/10.1016/j.patcog.2022.108676,0031-3203,2022,108676,128,Pattern Recognition,MS2GAH: Multi-label semantic supervised graph attention hashing for robust cross-modal retrieval,article,DUAN2022108676,
"Unsupervised ensemble learning refers to methods devised for a particular task that combine data provided by decision learners taking into account their reliability, which is usually inferred from the data. Here, the variant calling step of the next generation sequencing technologies is formulated as an unsupervised ensemble classification problem. A variant calling algorithm based on the expectation-maximization algorithm is further proposed that estimates the maximum-a-posteriori decision among a number of classes larger than the number of different labels provided by the learners. Experimental results with real human DNA sequencing data show that the proposed algorithm is competitive compared to state-of-the-art variant callers as GATK, HTSLIB, and Platypus.","Expectation maximization algorithm, Variant calling, Genome sequencing, Unsupervised multi-class ensemble classifier, GATK",Alba PagÃ¨s-Zamora and Idoia Ochoa and Gonzalo Ruiz Cavero and Pol Villalvilla-Ornat,https://www.sciencedirect.com/science/article/pii/S0031320322002023,https://doi.org/10.1016/j.patcog.2022.108721,0031-3203,2022,108721,129,Pattern Recognition,Unsupervised ensemble learning for genome sequencing,article,PAGESZAMORA2022108721,
"Recognition of glomeruli lesions is the key for diagnosis and treatment planning in kidney pathology; however, the coexisting glomerular structures such as mesangial regions exacerbate the difficulties of this task. In this paper, we introduce a scheme to recognize fine-grained glomeruli lesions from whole slide images. First, a focal instance structural similarity loss is proposed to drive the model to locate all types of glomeruli precisely. Then an Uncertainty Aided Apportionment Network is designed to carry out the fine-grained visual classification without bounding-box annotations. This double branch-shaped structure extracts common features of the child class from the parent class and produces the uncertainty factor for reconstituting the training dataset. Results of slide-wise evaluation illustrate the effectiveness of the entire scheme, with an 8â22% improvement of the mean Average Precision compared with remarkable detection methods. The comprehensive results clearly demonstrate the effectiveness of the proposed method.","Deep convolutional neural network, Glomerulus segmentation, Fine-grained lesion classification, Uncertainty assessment, Kidney pathology",Yang Nan and Fengyi Li and Peng Tang and Guyue Zhang and Caihong Zeng and Guotong Xie and Zhihong Liu and Guang Yang,https://www.sciencedirect.com/science/article/pii/S0031320322001297,https://doi.org/10.1016/j.patcog.2022.108648,0031-3203,2022,108648,127,Pattern Recognition,Automatic fine-grained glomerular lesion recognition in kidney pathology,article,NAN2022108648,
"Locating the centers before assigning clustering labels is a traditional routine of clustering methods, which also limits the development of new clustering ideas. In this paper, we achieve the clustering task by firstly identifying the boundary points in the feature space, and then we shrink the boundary points to allocate the un-clustered points. Concretely, we propose a Centroid Drift (CD) metric and a Boundary Shrinkage (BS) strategy to detect boundary points in the feature space and allocate labels for un-clustered points, respectively. Both the CD and BS are closely related to the pre-computed k-nearest neighbor matrix, contributing to the decrease of algorithm parameters. Moreover, the common problems of noise points and non-uniform density distribution of data points in clustering task can also be alleviated with our proposed large value suppression and normalization of k-nearest neighbor distance techniques. The experiments on synthetic datasets, real-world face image datasets and hyperspectral images demonstrate the superiorities of our proposed clustering framework.","Clustering, Centroid drift, Boundary detection",Hui Qv and Tao Ma and Xinyi Tong and Xuhui Huang and Zhe Ma and Jiehong Feng,https://www.sciencedirect.com/science/article/pii/S0031320322002266,https://doi.org/10.1016/j.patcog.2022.108745,0031-3203,2022,108745,129,Pattern Recognition,Clustering by centroid drift and boundary shrinkage,article,QV2022108745,
"The biometric authentication system using electrocardiogram (ECG) may protect individualsâ privacy and prevent identity frauds. Researchers have demonstrated that ECG is suitable for biometrics use due to its pervasiveness, immutability, measurability, acceptance, and individuality. However, ECGâs statistical independence for biometric authentication has yet to be substantiated. Thereby, this paper proposes a novel model to evaluate the statistical independence of ECG among individuals using heartbeat morphological features. The signal is qualitatively improved and heartbeat features are extracted using signal processing techniques. Three classes of features such as interval, amplitude, and angle are extracted from each heartbeat. The hypothesis estimating the probability of resemblance of interval, amplitude, and angle classes of features is derived. The accumulated effect of these classes of features measure the statistical independence of ECG. Further, the proposed model of statistical independence of ECG biometrics is validated by comparing the statistical performance with the empirical performance of the ECG verification system. The empirical performance is estimated using three different ECG biometric methods, i.e., traditional intraclass-interclass features, artificial neural network, and convolutional neural network. The false resemblance probabilities of heartbeats among individuals computed for four interval class features, five amplitude class features, and five angle class features are found to be 3.4Ã10â6, 1.0Ã10â7, and 3.9Ã10â8, respectively. The cumulative probability of resemblance computed using fourteen heartbeat features of interval, amplitude, and angle classes is found as 1.3Ã10â20.","Biometric authentication, Electrocardiogram, Statistical independence, Probability of resemblance",Ranjeet Srivastva and Yogendra Narain Singh and Ashutosh Singh,https://www.sciencedirect.com/science/article/pii/S0031320322001212,https://doi.org/10.1016/j.patcog.2022.108640,0031-3203,2022,108640,127,Pattern Recognition,Statistical independence of ECG for biometric authentication,article,SRIVASTVA2022108640,
"Color transfer, which plays a key role in image editing, has attracted noticeable attention recently. It has remained a challenge to date due to various issues such as time-consuming manual adjustments and prior segmentation issues. In this paper, we propose to model color transfer under a probability framework and cast it as a parameter estimation problem. In particular, we relate the transferred image with the example image under the Gaussian Mixture Model (GMM) and regard the transferred image color as the GMM centroids. We employ the Expectation-Maximization (EM) algorithm (E-step and M-step) for optimization. To better preserve gradient information, we introduce a Laplacian based regularization term to the objective function at the M-step which is solved by deriving a gradient descent algorithm. Given the input of a source image and an example image, our method is able to generate multiple color transfer results with increasing EM iterations. Extensive experiments show that our approach generally outperforms other competitive color transfer methods, both visually and quantitatively.","Color transfer, Gaussian mixture model, EM optimization",Chunzhi Gu and Xuequan Lu and Chao Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322001972,https://doi.org/10.1016/j.patcog.2022.108716,0031-3203,2022,108716,129,Pattern Recognition,Example-based color transfer with Gaussian mixture modeling,article,GU2022108716,
"Weakly supervised object localization locates objects based on the localization map generated from the classification network. However, most existing methods utilize the information of the target class to locate objects based on the feature map of a single image, which ignores both the relationships of inter-class and intra-class. In this work, we propose a Gradient-based Refined Class Activation Map (GRCAM) approach to achieve more accurate localization. Two kinds of gradients are applied to reveal the relationships of inter-class and intra-class during the testing stage. First, we exploit the gradients of the classification loss function concerning the feature map to enhance class-specific information. The gradients of classification loss reveal the connection among the predicted probabilities of all classes. Second, we design a regression function that refers to the loss between the pseudo-bounding box coordinates containing category consistency and the predicted coordinates generated from the localization map. The predicted coordinates are revised by the gradients of the regression function. The gradients of the regression function reveal the consistency within a class. Despite the apparent simplicity, we demonstrate the advantages of GRCAM on ILSVRC and CUB-200-2011 in extensive experiments. Especially, on ILSVRC dataset, the proposed GRCAM achieves a new state-of-the-art Top-1 localization error of 42.94%.","Weakly supervised object localization, Gradients of loss function, Class-specific mask, Bounding box revision, Category consistency",Wenjun Hui and Chuangchuang Tan and Guanghua Gu and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S0031320322001455,https://doi.org/10.1016/j.patcog.2022.108664,0031-3203,2022,108664,128,Pattern Recognition,Gradient-based refined class activation map for weakly supervised object localization,article,HUI2022108664,
"We consider a rank regression setting, in which a dataset of N samples with features in Rd is ranked by an oracle via M pairwise comparisons. Specifically, there exists a latent total ordering of the samples; when presented with a pair of samples, a noisy oracle identifies the one ranked higher with respect to the underlying total ordering. A learner observes a dataset of such comparisons and wishes to regress sample ranks from their features. We show that to learn the model parameters with Ïµ>0 accuracy, it suffices to conduct MâÎ©(dNlog3N/Ïµ2) comparisons uniformly at random when N is Î©(d/Ïµ2).","Sample complexity, Rank regression, Pairwise comparisons, Features",Berkan KadÄ±oÄlu and Peng Tian and Jennifer Dy and Deniz ErdoÄmuÅ and Stratis Ioannidis,https://www.sciencedirect.com/science/article/pii/S0031320322001698,https://doi.org/10.1016/j.patcog.2022.108688,0031-3203,2022,108688,130,Pattern Recognition,Sample complexity of rank regression using pairwise comparisons,article,KADIOGLU2022108688,
"Nakagami distribution and related imaging methods are very efficient in diagnostic ultrasonography for visualization and characterization of tissues for years. Abnormalities in tissues are distinguished from surrounding cells by application of the distribution ruled by the Nakagami m-parameter. The potential of discrimination in ultrasonography enables intelligent segmentation of lesions by other diagnostic tools and the imaging technique is very promising in other areas of medicine, like magnetic resonance imaging (MRI) for brain lesion identification, as presented in this paper. Therefore, we propose a novel Nakagami-Fuzzy imaging framework for intelligent and fully automated suspicious region segmentation from axial FLAIR MRI images exhibiting brain tumor characteristics to satisfy ground truth images with different precision levels. The images from MRI data set are processed by applying Nakagami distribution from pre-Rayleigh to post-Rayleigh for adjusting m-parameter. Amorphous and non-homogenous suspicious regions revealed by Nakagami imaging are segmented using customized Fuzzy 2-means to compare with two types of binary ground truths. The framework we propose is an outstanding example of fuzzy-based expert systems providing an average of 92.61% dice score for the main clinical experiment we conducted using the images and two types of ground truths provided by University of Hospital, Hradec Kralove. We also tested our framework by the BraTS 2012 and BraTS 2020 datasets and achieved an average of 91.88% and 89.25% dice scores respectively, which are competitive among the relevant researches.","Nakagami imaging, Fuzzy c-means, Lesion segmentation, MRI, BraTS",Orcan Alpar and Rafael Dolezal and Pavel Ryska and Ondrej Krejcar,https://www.sciencedirect.com/science/article/pii/S003132032200156X,https://doi.org/10.1016/j.patcog.2022.108675,0031-3203,2022,108675,128,Pattern Recognition,Nakagami-Fuzzy imaging framework for precise lesion segmentation in MRI,article,ALPAR2022108675,
"Existing Few-Shot Learning (FSL) methods learn and recognize new classes with the help of prior knowledge. However, they cannot handle this task well in a cross-domain scenario when training and testing sets are from different domains, since the fact that prior knowledge in different domains often varies greatly. To solve this problem, in this paper, we propose a few-shot domain generalization method, which is designed to extract relationship embeddings using Forget-Update Modules named FUM. The relationship embedding considers valuable relational information between samples in a specific task, and the forget-update module takes into account differences between domains and adjusts the distribution of relational embeddings through forgetting and updating mechanisms based on specific tasks. To evaluate the few-shot domain generalization ability of FUM, extensive experiments on eight cross-domain scenarios and six same-domain scenarios are conducted, and the results show that FUM achieves superior performances compared to recent few-shot learning methods. Visualization results also show that the distribution of the relationship embeddings extracted by FUM has stronger few-shot domain generalization ability than the feature embeddings used in the existing FSL methods.","Few-shot classification, Domain adaptation, Few-shot domain generalization",Minglei Yuan and Chunhao Cai and Tong Lu and Yirui Wu and Qian Xu and Shijie Zhou,https://www.sciencedirect.com/science/article/pii/S0031320322001856,https://doi.org/10.1016/j.patcog.2022.108704,0031-3203,2022,108704,129,Pattern Recognition,A novel forget-update module for few-shot domain generalization,article,YUAN2022108704,
"Neural architecture search (NAS) has emerged in many domains to jointly learn the architectures and weights of neural networks. The core spirit behind NAS is to automatically search neural architectures for target tasks with better performance-efficiency trade-offs. However, existing approaches emphasize on only searching a single architecture with less human intervention to replace a human-designed neural network, yet making the search process almost independent of the domain knowledge. In this paper, we aim to apply NAS for human pose estimation and we ask: when NAS meets this localization task, can the articulated human body structure help to search better task-specific architectures? To this end, we first design a new neural architecture search space, Cell-based Neural Fabric (CNF), to learn micro as well as macro neural architecture using a differentiable search strategy. Then, by viewing locating human parts as multiple disentangled prediction sub-tasks, we exploit the compositionality of human body structure as guidance to search multiple part-specific CNFs specialized for different human parts. After the search, all these part-specific neural fabrics have been tailored with distinct micro and macro architecture parameters. The results show that such knowledge-guided NAS-based model outperforms a hand-crafted part-based baseline model, and the resulting multiple part-specific architectures gain significant performance improvement against a single NAS-based architecture for the whole body. The experiments on MPII and COCO datasets show that our models11Code is available at https://github.com/yangsenius/PoseNFS. achieve comparable performance against the state-of-the-art methods while being relatively lightweight.","Human pose estimation, Neural architecture search, Cell-based neural fabrics, Micro and macro search space, Prior knowledge, Part-specific",Sen Yang and Wankou Yang and Zhen Cui,https://www.sciencedirect.com/science/article/pii/S0031320322001339,https://doi.org/10.1016/j.patcog.2022.108652,0031-3203,2022,108652,128,Pattern Recognition,Searching part-specific neural fabrics for human pose estimation,article,YANG2022108652,
"With a limited query budget and only the final decision of a target model, how to find adversarial examples with low-magnitude distortion has attracted great attention among researchers. Recent solutions to this issue made use of the estimated normal vector at a boundary data point to search for adversarial examples. However, since the sampling independence between two sampling epochs, they still suffer from a prohibitively high query budget, which will get worse when the dimensionality of the attacked samples get increased. To push for further development, in this paper, we pay attention to a query-efficient method to estimate the normal vector for decision-based attack in high-dimensional space. Specifically, we propose a simple yet effective normal vector estimation framework for high-dimension decision-based attack via Sampling Distribution Reshaping, dubbed SDR. Next, SDR is incorporated into general geometric attack framework. Briefly, SDR leverages all the historically sampled noise to build a guiding vector, which will be used to reshape the next sampling distribution. Besides, we also extend SDR to different âp norms for p={2,â} and deploy low-frequency constraint to enhance the performance of SDR. Compared to peer decision-based attacks, SDR can reach the competitive âp norms for p={2,â}, according to extensive experimental evaluations against both defended and undefended classifiers. Since the simplicity and effectiveness of SDR, we think that reshaping the sampling distribution deserves further research in future works.","Adversarial examples, Decision-based attack, Image classification, Normal vector estimation, Distribution reshaping",Xuxiang Sun and Gong Cheng and Lei Pei and Junwei Han,https://www.sciencedirect.com/science/article/pii/S0031320322002096,https://doi.org/10.1016/j.patcog.2022.108728,0031-3203,2022,108728,129,Pattern Recognition,Query-efficient decision-based attack via sampling distribution reshaping,article,SUN2022108728,
"Imputation of missing data is an important but challenging issue because we do not know the underlying distribution of the missing data. Previous imputation models have addressed this problem by assuming specific kinds of missing distributions. However, in practice, the mechanism of the missing data is unknown, so the most general case of missing pattern needs to be considered for successful imputation. In this paper, we present cycle-consistent imputation adversarial networks to discover the underlying distribution of missing patterns closely under some relaxations. Using adversarial training, our model successfully learns the most general case of missing patterns. Therefore our method can be applied to a wide variety of imputation problems. We empirically evaluated the proposed method with numerical and image data. The result shows that our method yields the state-of-the-art performance quantitatively and qualitatively on standard datasets.","Imputation, Missing data, Cycle-consistent",Woojin Lee and Sungyoon Lee and Junyoung Byun and Hoki Kim and Jaewook Lee,https://www.sciencedirect.com/science/article/pii/S0031320322002011,https://doi.org/10.1016/j.patcog.2022.108720,0031-3203,2022,108720,129,Pattern Recognition,Variational cycle-consistent imputation adversarial networks for general missing patterns,article,LEE2022108720,
"Random Vector Functional Link (RVFL) Networks have received a lot of attention due to the fast training speed as the non-iterative solution characteristic. Currently, the main research direction of RVFLs has supervised learning, including semi-supervised and multi-label. There are hardly any unsupervised research results for RVFLs. In this paper, we propose the unsupervised RVFL (usRVFL), and the unsupervised framework is generic that can be used with other RVFL variants, thus we extend it to an ensemble deep variant, unsupervised deep RVFL (usdRVFL). The unsupervised method is based on the manifold regularization while the deep variant is related to the consensus clustering method, which can increase the capability and diversity of RVFLs. Our unsupervised approaches also benefit from fast training speed, even the deep variant offers a very competitive computation efficiency. Empirical experiments on several benchmark datasets demonstrated the effectiveness of the proposed algorithms.","Random vector functional link, Unsupervised learning, Consensus clustering, Manifold regularization",Minghui Hu and P.N. Suganthan,https://www.sciencedirect.com/science/article/pii/S0031320322002254,https://doi.org/10.1016/j.patcog.2022.108744,0031-3203,2022,108744,129,Pattern Recognition,Representation learning using deep random vector functional link networks for clustering,article,HU2022108744,
"Accurate segmentation of brain magnetic resonance images is a key step in quantitative analysis of brain images. Finite mixture model is one of the most widely used methods in brain magnetic resonance image segmentation. However, due to the presence of intensity inhomogeneity artifact and noise, the image histogram distribution of brain MR images may follow a heavy tailed distribution or asymmetric distribution, which makes traditional finite mixture model, such as Gaussian mixture model, hard to achieve accurate segmentation results. To alleviate these problems, a novel spatially constrained finite skew studentâs-t mixture model is proposed in this paper. Firstly, we propose anisotropic two-level spatial information, which combines the prior and posterior probabilities, to reduce the impact of noise. The proposed spatial information can preserve rich details, such as edges and corners. Secondly, we couple the anisotropic spatial information into the skew studentâs-t distribution to fit the intensity distribution of observation data with heavy tail distribution or asymmetric distribution. Thirdly, we use a linear combination of a set of orthogonal basis functions to model the intensity inhomogeneities. Finally, the objective function integrates both tissue segmentation and the bias field estimation. In the implementation, we used an improved expectation maximization (EM) algorithm to estimate the model parameters. The experimental results of our model on synthetic data and brain magnetic resonance images are better than other state-of-the-art segmentation methods.","Bias field, EM Algorithm, Skew studentâs-t distribution, Two-level spatial information",Ning Cheng and Chunzheng Cao and Jianwei Yang and Zhichao Zhang and Yunjie Chen,https://www.sciencedirect.com/science/article/pii/S003132032200139X,https://doi.org/10.1016/j.patcog.2022.108658,0031-3203,2022,108658,128,Pattern Recognition,A spatially constrained skew Studentâs-t mixture model for brain MR image segmentation and bias field correction,article,CHENG2022108658,
"Clustering is a subjective task, that is, several different results can be obtained from a single clustering hierarchy, depending on the observation scale. A local view of the data may necessitate more clusters, whereas a global view requires fewer clusters. It is, therefore, important to provide users with an appropriate clustering hierarchy and let them select the final clustering result based on their own observation scale. Thus, a new clustering method, named connection center evolution (CCE), was recently developed by Geng and Tang (2020). CCE provides gradual clustering results by iteratively merging cluster centers. However, theoretical evidence for its convergence is missing, and the center evolution requires a connectivity matrix of a significantly higher order as iterations proceed, resulting in higher computational costs. Accordingly, we present a convergence analysis of CCE using the properties of ergodic Markov chains and propose a faster algorithm using the enhanced connectivity graph derived from the convergence analysis. Empirical evidence from numerical experiments and theoretical proofs demonstrate the advantages of the proposed method.","Clustering, Center evolution, Convergence analysis, Ergodic Markov chain, Faster algorithm",Jaemin Lee and Minseok Han and Jong-Seok Lee,https://www.sciencedirect.com/science/article/pii/S0031320322001200,https://doi.org/10.1016/j.patcog.2022.108639,0031-3203,2022,108639,127,Pattern Recognition,Convergence analysis of connection center evolution and faster clustering,article,LEE2022108639,
"This paper presents an effective method for multiple talker localisation using only a single microphone in a room. One of the main challenge here is obtaining a model that can be used for estimating the localization parameter. This model must be sensitive to all possible speaker locations and correctly discriminate their positions. The reverberant speech signal in a room environment can be composited by the clean speech and the acoustic transfer function (ATF). The ATF is a useful tool to describe changes of the speech source, and the approaches based on ATF can thus be used to identify talker localizations with a single microphone. This paper presents two methods, referred to as Composite Reverberant Speech (CRS) model and Direct Training Reverberant Speech (DTRS) model, and uses these methods for obtaining the ATF of a room. The approaches based on proposed methods can successfully and accurately process multi-talker localization task with single microphone. Experiments also demonstrate the effectiveness of the proposed methods.","Gaussian mixture model (GMM), Acoustic transfer function (ATF), Talker localization",Xingchen Guo and Xuexin Xu and Xunquan Chen and Jinhui Chen and Rong Jia and Zhihong Zhang and Tetsuya Takiguchi and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320322001960,https://doi.org/10.1016/j.patcog.2022.108715,0031-3203,2022,108715,129,Pattern Recognition,Direction of arrival estimation for indoor environments based on acoustic composition model with a single microphone,article,GUO2022108715,
"Weakly supervised semantic segmentation is a challenging task that only takes image-level labels as supervision but produces pixel-level predictions for testing. To address such a challenging task, most current approaches generate pseudo pixel masks first that are then fed into a separate semantic segmentation network. However, these two-step approaches suffer from high complexity and being hard to train as a whole. In this work, we harness the image-level labels to produce reliable pixel-level annotations and design a fully end-to-end network to learn to predict segmentation maps. Concretely, we firstly leverage an image classification branch to generate class activation maps for the annotated categories, which are further pruned into tiny reliable object/background regions. Such reliable regions are then directly served as ground-truth labels for the segmentation branch, where both global information and local information sub-branches are used to generate accurate pixel-level predictions. Furthermore, a new joint loss is proposed that considers both shallow and high-level features. Despite its apparent simplicity, our end-to-end solution achieves competitive mIoU scores (val: 65.4%, test: 65.3%) on Pascal VOC compared with the two-step counterparts. By extending our one-step method to two-step, we get a new state-of-the-art performance on the Pascal VOC 2012 dataset(val: 69.3%, test: 69.2%). Code is available at: https://github.com/zbf1991/RRM.","Weakly supervised, Semantic segmentation, End-to-end, Attention",Bingfeng Zhang and Jimin Xiao and Yunchao Wei and Kaizhu Huang and Shan Luo and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S0031320322001443,https://doi.org/10.1016/j.patcog.2022.108663,0031-3203,2022,108663,128,Pattern Recognition,End-to-end weakly supervised semantic segmentation with reliable region mining,article,ZHANG2022108663,
"Kleinberg introduced three natural clustering properties, or axioms, and showed they cannot be simultaneously satisfied by any clustering algorithm. We present a new clustering property, Monotonic Consistency, which avoids the well-known problematic behaviour of Kleinbergâs Consistency axiom, and the impossibility result. Namely, we describe a clustering algorithm, Morse Clustering, inspired by Morse Theory in Differential Topology, which satisfies Kleinbergâs original axioms with Consistency replaced by Monotonic Consistency. Morse clustering uncovers the underlying flow structure on a set or graph and returns a partition into trees representing basins of attraction of critical vertices. We also generalise Kleinbergâs axiomatic approach to sparse graphs, showing an impossibility result for Consistency, and a possibility result for Monotonic Consistency and Morse clustering.","Data clustering, Graph clustering, Axiomatic clustering, Morse theory, Morse flow",Fabio Strazzeri and RubÃ©n J. SÃ¡nchez-GarcÃ­a,https://www.sciencedirect.com/science/article/pii/S0031320322001686,https://doi.org/10.1016/j.patcog.2022.108687,0031-3203,2022,108687,128,Pattern Recognition,Possibility results for graph clustering: A novel consistency axiom,article,STRAZZERI2022108687,
"Moving Objects Segmentation (MOS) is a fundamental task in many computer vision applications such as human activity analysis, visual object tracking, content based video search, traffic monitoring, surveillance, and security. MOS becomes challenging due to abrupt illumination variations, dynamic backgrounds, camouflage and scenes with bootstrapping. To address these challenges we propose a MOS algorithm exploiting multiple adversarial regularizations including conventional as well as least squares losses. More specifically, our model is trained on scene background images with the help of cross-entropy loss, least squares adversarial loss and â1 loss in image space working jointly to learn the dynamic background changes. During testing, our proposed method aims to generate test image background scenes by searching optimal noise samples using joint minimization of â1 loss in image space, â1 loss in feature space, and discriminator least squares loss. These loss functions force the generator to synthesize dynamic backgrounds similar to the test sequences which upon subtraction results in moving objects segmentation. Experimental evaluations on five benchmark datasets have shown excellent performance of the proposed algorithm compared to the twenty one existing state-of-the-art methods.","Moving objects segmentation, Generative adversarial network, Background subtraction",Maryam Sultana and Arif Mahmood and Soon Ki Jung,https://www.sciencedirect.com/science/article/pii/S003132032200200X,https://doi.org/10.1016/j.patcog.2022.108719,0031-3203,2022,108719,129,Pattern Recognition,Unsupervised moving object segmentation using background subtraction and optimal adversarial noise sample search,article,SULTANA2022108719,
"In contemporary society, surveillance anomaly detection, i.e., spotting anomalous events such as crimes or accidents in surveillance videos, is a critical task. As anomalies occur rarely, most training data consists of unlabeled videos without anomalous events, which makes the task challenging. Most existing methods use an autoencoder (AE) to learn to reconstruct normal videos; they then detect anomalies based on their failure to reconstruct the appearance of abnormal scenes. However, because anomalies are distinguished by appearance as well as motion, many previous approaches have explicitly separated appearance and motion informationfor example, using a pre-trained optical flow model. This explicit separation restricts reciprocal representation capabilities between two types of information. In contrast, we propose an implicit two-path AE (ITAE), a structure in which two encoders implicitly model appearance and motion features, along with a single decoder that combines them to learn normal video patterns. For the complex distribution of normal scenes, we suggest normal density estimation of ITAE features through normalizing flow (NF)-based generative models to learn the tractable likelihoods and identify anomalies using out-of-distribution detection. NF models intensify ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling on six benchmarks, including databases that contain various anomalies in real-world scenarios.","Video anomaly detection, Surveillance system, AutoEncoder, Normalizing flow",MyeongAh Cho and Taeoh Kim and Woo Jin Kim and Suhwan Cho and Sangyoun Lee,https://www.sciencedirect.com/science/article/pii/S0031320322001844,https://doi.org/10.1016/j.patcog.2022.108703,0031-3203,2022,108703,129,Pattern Recognition,Unsupervised video anomaly detection via normalizing flows with implicit latent features,article,CHO2022108703,
"Attribute reduction is one of the important applications in fuzzy rough set theory. However, most attribute reduction methods in fuzzy rough theory mainly focus on removing irrelevant or redundant attributes. There are few reports about the method of considering attribute interaction. For this reason, this paper proposes an interactive attribute reduction method for unlabeled mixed data. First, some uncertainty measures based on fuzzy complementary entropy are further defined. Then, based on the proposed uncertainty measure, the attribute evaluation criteria of maximal information, minimal redundancy, and maximal interactivity are developed respectively. As a result, the evaluation index of the attribute importance is established by using the idea of unsupervised maximal information-minimal redundancy-maximal interactivity. Finally, a corresponding algorithm is designed to select attributes. The experimental results show that the proposed algorithm has better performance.","Fuzzy rough set theory, Unsupervised attribute reduction, Complementary entropy, Maximal information, Minimal redundancy, Maximal interactivity, Mixed data",Zhong Yuan and Hongmei Chen and Tianrui Li,https://www.sciencedirect.com/science/article/pii/S0031320322001327,https://doi.org/10.1016/j.patcog.2022.108651,0031-3203,2022,108651,127,Pattern Recognition,Exploring interactive attribute reduction via fuzzy complementary entropy for unlabeled mixed data,article,YUAN2022108651,
"This paper presents SPARE, a self-supervised part erasing framework for ultra-fine-grained visual categorization. The key insight of our model is to learn discriminative representations by encoding a self-supervised module that performs random part erasing and prediction on the contextual position of the erased parts. This drives the network to exploit intrinsic structure of data, i.e., understanding and recognizing the contextual information of the objects, thus facilitating more discriminative part-level representation. This also enhances the learning capability of the model by introducing more diversified training part segments with semantic meaning. We demonstrate that our approach is able to achieve strong performance on seven publicly available datasets covering ultra-fine-grained visual categorization and fine-grained visual categorization tasks.","Self-Supervised part erasing, Ultra-fine-grained visual categorization, Fine-grained visual categorization, Random part erasing, Weakly-supervised part segmentation",Xiaohan Yu and Yang Zhao and Yongsheng Gao,https://www.sciencedirect.com/science/article/pii/S0031320322001728,https://doi.org/10.1016/j.patcog.2022.108691,0031-3203,2022,108691,128,Pattern Recognition,SPARE: Self-supervised part erasing for ultra-fine-grained visual categorization,article,YU2022108691,
"Data augmentation via randomly combining training instances and interpolating the corresponding labels has shown impressive gains in image classification. However, model attention regions are not necessarily meaningful in class semantics, especially for the case of limited supervision. In this paper, we present a semi-supervised classification model based on Class-Ambiguous Data with Attention Regularization, which is referred to as CADAR. Specifically, we adopt a Random Regional Interpolation (RRI) module to construct complex and effective class-ambiguous data, such that the model behavior can be regularized around decision boundaries. By aggregating the parameters of a classification network over training epochs to produce more reliable predictions on unlabeled data, RRI can also be applied to them as well as labeled data. Further, the classifier is enforced to apply consistent attention on the original and constructed data. This is important for inducing the model to learn discriminative features from the class-related regions. The experiment results demonstrate that CADAR significantly benefits from the constructed data and attention regularization, and thus achieves superior performance across multiple standard benchmarks and different amounts of labeled data.","Semi-supervised learning, Image classification, Attention regularization, Class-ambiguous data",Xiaoyang Huo and Xiangping Zeng and Si Wu and Hau-San Wong,https://www.sciencedirect.com/science/article/pii/S0031320322002084,https://doi.org/10.1016/j.patcog.2022.108727,0031-3203,2022,108727,129,Pattern Recognition,Attention regularized semi-supervised learning with class-ambiguous data for image classification,article,HUO2022108727,
"Unsupervised feature selection is an important topic in the fields of machine learning, pattern recognition and data mining. The representation methods include adaptive-graph-based methods and self-representation-based methods. The former methods have a longstanding and undiscovered problem about imbalanced neighbors, and the latter ones do not perform well when features are not linearly dependent. To deal with these problems, a novel unsupervised feature selection method is proposed to ensure k connectivity and eliminate more redundant features based on adaptive graph and dependency score (AGDS). Extensive experiments conducted on 13 benchmark datasets show the effectiveness of AGDS.","Unsupervised feature selection, Adaptive graph, Mutual information, Entropy",Pei Huang and Xiaowei Yang,https://www.sciencedirect.com/science/article/pii/S0031320322001030,https://doi.org/10.1016/j.patcog.2022.108622,0031-3203,2022,108622,127,Pattern Recognition,Unsupervised feature selection via adaptive graph and dependency score,article,HUANG2022108622,
"Group-level emotion recognition (ER) is a growing research area as the demands for assessing crowds of all sizes are becoming an interest in both the security arena as well as social media. This work extends the earlier ER investigations, which focused on either group-level ER on single images or within a video, by fully investigating group-level expression recognition on crowd videos. In this paper, we propose an effective deep feature level fusion mechanism to model the spatial-temporal information in the crowd videos. In our approach, the fusing process is performed on the deep feature domain by a generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that models spatial information relationships. Furthermore, we extend our proposed spatial NVPF approach to the spatial-temporal NVPF approach to learn the temporal information between frames. To demonstrate the robustness and effectiveness of each component in the proposed approach, three experiments were conducted: (i) evaluation on AffectNet database to benchmark the proposed EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii) examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos (GECV) dataset composed of 627 videos collected from publicly available sources. GECV dataset is a collection of videos containing crowds of people. Each video is labeled with emotion categories at three levels: individual faces, group of people, and the entire video frame.","Group-level emotion recognition, Facial features, Feature extraction, Feature fusion, Crowd videos",Kha Gia Quach and Ngan Le and Chi Nhan Duong and Ibsa Jalata and Kaushik Roy and Khoa Luu,https://www.sciencedirect.com/science/article/pii/S0031320322001273,https://doi.org/10.1016/j.patcog.2022.108646,0031-3203,2022,108646,128,Pattern Recognition,Non-volume preserving-based fusion to group-level emotion recognition on crowd videos,article,QUACH2022108646,
"Understanding the predictions made by Artificial Intelligence (AI) systems is becoming more and more important as deep learning models are used for increasingly complex and high-stakes tasks. Saliency mapping â a popular visual attribution method â is one important tool for this, but existing formulations are limited by either computational cost or architectural constraints. We therefore propose Hierarchical Perturbation, a very fast and completely model-agnostic method for interpreting model predictions with robust saliency maps. Using standard benchmarks and datasets, we show that our saliency maps are of competitive or superior quality to those generated by existing model-agnostic methods â and are over 20Ã faster to compute.","XAI, AI safety, Saliency mapping, Deep learning explanation, Interpretability, Prediction attribution",Jessica Cooper and Ognjen ArandjeloviÄ and David J Harrison,https://www.sciencedirect.com/science/article/pii/S0031320322002242,https://doi.org/10.1016/j.patcog.2022.108743,0031-3203,2022,108743,129,Pattern Recognition,"Believe the HiPe: Hierarchical perturbation for fast, robust, and model-agnostic saliency mapping",article,COOPER2022108743,
"Few-shot learning is currently enjoying a considerable resurgence of interest, aided by the recent advance of deep learning. Contemporary approaches based on weight-generation scheme delivers a straightforward and flexible solution to the problem. However, they did not fully consider both the representation power for unseen categories and weight generation capacity in feature learning, making it a significant performance bottleneck. This paper proposes a multi-level weight-centric feature learning to give full play to feature extractorâs dual roles in few-shot learning. Our proposed method consists of two essential techniques: a weight-centric training strategy to improve the featuresâ prototype-ability and a multi-level feature incorporating a mid- and relation-level information. The former increases the feasibility of constructing a discriminative decision boundary based on a few samples. Simultaneously, the latter helps improve the transferability for characterizing novel classes and preserve classification capability for base classes. We extensively evaluate our approach to low-shot classification benchmarks. Experiments demonstrate our proposed method significantly outperforms its counterparts in both standard and generalized settings and using different network backbones.","Fewshot learning, Low-shot learning, Multi-level features, Image classification",Mingjiang Liang and Shaoli Huang and Shirui Pan and Mingming Gong and Wei Liu,https://www.sciencedirect.com/science/article/pii/S0031320322001431,https://doi.org/10.1016/j.patcog.2022.108662,0031-3203,2022,108662,128,Pattern Recognition,Learning multi-level weight-centric features for few-shot learning,article,LIANG2022108662,
"Kernel possibilistic fuzzy C-means with local information (KWPFLICM) has important research significance of image segmentation, but it is very sensitive to high noise or outliers. To enhance the segmentation performance of the algorithm, this paper proposes a kernelized total Bregman divergence-driven possibilistic fuzzy clustering with local information (TKWPFLICM). Firstly, a polynomial kernel function is introduced to kernelize total Bregman divergence (TBD), and local neighborhood information of the pixel is used to modify it, which overcomes the shortcomings of Bregman divergence (BD) with rotation variability; Secondly, the modified kernelized TBD and possibilistic typicality are combined to further enhance the anti-noise ability of the algorithm; Finally, the modified kernelized TBD is introduced into the objective function of KWPFLICM algorithm, then a novel robust fuzzy clustering algorithm is derived by optimization theory. Experimental results show that compared with existing fuzzy clustering-related algorithms, the average SA improvement on TKWPFLICM algorithm is in the range of 0.791% to 33.237%. Therefore, TKWPFLICM algorithm has better anti-noise robustness and segmentation accuracy.","Image segmentation, Fuzzy clustering, Total Bregman divergence, Polynomial kernel function, Possibilistic typicality",Chengmao Wu and Xue Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322001674,https://doi.org/10.1016/j.patcog.2022.108686,0031-3203,2022,108686,128,Pattern Recognition,Total Bregman divergence-driven possibilistic fuzzy clustering with kernel metric and local information for grayscale image segmentation,article,WU2022108686,
"Dynamic systems are highly complex and hard to deal with due to their subject- and time-varying nature. The fact that most of the real world systems/events are of dynamic character makes modeling and analysis of such systems inevitable and charmingly useful. One promising estimation method that is capable of unlearning past information to deal with non-stationarity is Stochastic Learning Weak Estimator (SLWE) by Oommen and Rueda (2006). However, due to using a constant learning rate, it faces a trade-off between plasticity and stability. In this paper, we model SLWE as a random walk and provide rigorous theoretical analysis of asymptotic behavior of estimates to obtain a statistical model. Utilizing this model, we detect changes in stationarity to switch between exploratory and exploitative learning modes. Experimental evaluations on both synthetic and real world data show that the proposed method outperforms related algorithms in different types of drifts.","Stochastic learning, Concept drift, Change detection, Parameter estimation, Dynamic learning rate",KutalmÄ±Å CoÅkun and Borahan TÃ¼mer,https://www.sciencedirect.com/science/article/pii/S0031320322001832,https://doi.org/10.1016/j.patcog.2022.108702,0031-3203,2022,108702,129,Pattern Recognition,An adaptive estimation method with exploration and exploitation modes for non-stationary environments,article,COSKUN2022108702,
"Generally, medical content-based image retrieval (CBIR) systems select low-level visual features as image descriptors. However, these descriptors fail to provide clues for understanding the content of medical images in a similar way as a human expert, which makes the retrieval results inconsistent with the userâs intention. To solve this problem, we propose a closed-loop brain tumor retrieval system for MR images with an eye-tracking based relevance feedback mechanism. In our method, we first model the intention of the user by training a convolutional neural network based on the temporal and spatial features extracted from his/her eye-tracking data collected when inspecting the relevance between different images. Upon using visual features as a bridge, the relevancy degree to the query image of any of the database images is computed with our userâs intention model by transferring to it the eye movement data from the most visually similar image amongst images iteratively accumulated in the canvas. Our proposed retrieval system is implemented in an iterative manner. In each round of iteration, userâs eye movement data when inspecting the system returns are collected and the canvas collection of images is also updated by appending to it the user inspected system returns. With the updated canvas collections, the relevancy degree of database images can be recomputed and the system can begin a new round search of the most relevant images. Extensive experiments have been performed on a publicly available T1-weighted contrast-enhanced magnetic resonance image (CE-MRI) dataset that consists of three types of brain tumors (glioma, meningioma, and pituitary tumor) collected from 233 patients with a total of 3064 images across the axial, coronal, and sagittal views. Experimental results of 22 volunteers (11 males and 11 females, with an average age of 24.4 years) from our medical school show that upon implicit involvement of users in the brain tumor retrieving process, our proposed system significantly outperforms state-of-the-art methods and achieves Prec@10 to 99.94%, mAP to 97.95% after the third round of iteration.","CBIR, Brain tumor images, Eye-tracking, Intention similarity, Iterative retrieval, Relevance feedback",Mengli Sun and Wei Zou and Nan Hu and Jiajun Wang and Zheru Chi,https://www.sciencedirect.com/science/article/pii/S0031320322001315,https://doi.org/10.1016/j.patcog.2022.108650,0031-3203,2022,108650,127,Pattern Recognition,Iterative brain tumor retrieval for MR images based on userâs intention model,article,SUN2022108650,
"Hand pose estimation is a challenging task due to the large number of degrees of freedom and the frequent occlusions of joints. To address these challenges, we propose HandyPose, a single-pass, end-to-end trainable architecture for 2D hand pose estimation using a single RGB image as input. Adopting an encoder-decoder framework with multi-level features, along with a novel multi-level waterfall atrous spatial pooling module for multi-scale representations, our method achieves high accuracy in hand pose while maintaining manageable size complexity and modularity of the network. HandyPose takes a multi-scale approach to representing context by incorporating spatial information at various levels of the network to mitigate the loss of resolution due to pooling. Our advanced multi-level waterfall module leverages the efficiency of progressive cascade filtering while maintaining larger fields-of-view through the concatenation of multi-level features from different levels of the network in the waterfall module. The decoder incorporates both the waterfall and multi-scale features for the generation of accurate joint heatmaps in a single stage. Our results demonstrate state-of-the-art performance on popular datasets and show that HandyPose is a robust and efficient architecture for 2D hand pose estimation.","Hand pose estimation, Feature representations, Computer vision",Divyansh Gupta and Bruno Artacho and Andreas Savakis,https://www.sciencedirect.com/science/article/pii/S0031320322001558,https://doi.org/10.1016/j.patcog.2022.108674,0031-3203,2022,108674,128,Pattern Recognition,HandyPose: Multi-level framework for hand pose estimation,article,GUPTA2022108674,
"Graph convolutional network (GCN) is an effective neural network model for graph representation learning. However, standard GCN suffers from three main limitations: (1) most real-world graphs have no regular connectivity and node degrees can range from one to hundreds or thousands, (2) neighboring nodes are aggregated with fixed weights, and (3) node features within a node feature vector are considered equally important. Several extensions have been proposed to tackle the limitations respectively. This paper focuses on tackling all the proposed limitations. Specifically, we propose a new node-feature convolutional (NFC) layer for GCN. The NFC layer first constructs a feature map using features selected and ordered from a fixed number of neighbors. It then performs a convolution operation on this feature map to learn the node representation. In this way, we can learn the usefulness of both individual nodes and individual features from a fixed-size neighborhood. Experiments on three benchmark datasets show that NFC-GCN consistently outperforms state-of-the-art methods in node classification.","Graph, Representation learning, Graph convolutional networks, Convolutional neural networks",Li Zhang and Heda Song and Nikolaos Aletras and Haiping Lu,https://www.sciencedirect.com/science/article/pii/S003132032200142X,https://doi.org/10.1016/j.patcog.2022.108661,0031-3203,2022,108661,128,Pattern Recognition,Node-Feature Convolution for Graph Convolutional Networks,article,ZHANG2022108661,
"This paper introduces a novel multidimensional projection method of datasets. Our method called Graph Regularization Multidimensional Projection (GRMP) is based on a technique from the graph signal processing theory, the graph regularization. Initially, a similarity graph is built on the high-dimensional space where the dataset lies. A two-dimensional distribution of points is then created in the visual space using a phyllotactic distribution. The similarity graph is copied properly over the phyllotactic distribution and the graph regularization is applied to their coordinates, which are interpreted as graph signals. The graph regularization reorganizes the phyllotactic distribution by bringing together points that represent similar data in the high-dimensional space. We employ synthetic and real datasets to demonstrate the effectiveness of our method. Furthermore, since the solution of the graph regularization can still be approximated using a fast approximation mechanism based on the Chebyshev polynomials, our method is computationally efficient even for large graphs.","Mapping of patterns, Bidimensional mapping, Visualization, Multidimensional projection, Graph signal processing, Data analysis",Alcebiades {Dal Col} and Fabiano Petronetto,https://www.sciencedirect.com/science/article/pii/S0031320322001716,https://doi.org/10.1016/j.patcog.2022.108690,0031-3203,2022,108690,129,Pattern Recognition,Graph regularization multidimensional projection,article,DALCOL2022108690,
"Manifold learning reveals the intrinsic low-dimensional manifold structure of high-dimensional data and has achieved great success in a wide spectrum of applications. However, traditional manifold learning methods assume that all the data lie on a common manifold, hence fail to capture the complicated geometry structure of the real-world data lying on multiple manifolds. This paper proposes a novel Multi-manifold Discriminant Local Spline Embedding (MDLSE) algorithm for high-dimensional classification, which considers a more realistic scenario where data of the same class lies on the same manifold. On the basis of this assumption, MDLSE seeks to reconstruct multiple manifolds for different classes of data in the embedding and separate them as apart as possible. In order to preserve the geometry structure of all the manifolds, MDLSE employs thin plate splines to align the local patches within each manifold compatibly in the global embedding. Meanwhile, to separate the different manifolds, MDLSE utilizes discriminative information to ensure the neighboring data from different manifolds to be mapped far from each other. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of MDLSE over the other representative manifold learning algorithms. The advantage of MDLSE is often more obvious on smaller size of training data and in lower embedding dimensions.","Manifold learning, Dimension reduction, Classification, Thin plate spline, Multiple manifolds",Ping He and Xiaohua Xu and Xincheng Chang and Jie Ding and Suquan Chen,https://www.sciencedirect.com/science/article/pii/S0031320322001959,https://doi.org/10.1016/j.patcog.2022.108714,0031-3203,2022,108714,129,Pattern Recognition,Multi-manifold discriminant local spline embedding,article,HE2022108714,
"Unsupervised domain adaptation is used to effectively learn a classifier for data of the unlabeled target domain by utilizing the data of the source domain with sufficient labels but different distributions. In general, a transformation matrix is employed to acquire a common subspace where the distributions of the two domains are aligned, which is easy to lose lots of unique information of the two domains. To better preserve useful information during the transformation process, we propose a novel Cycle-Reconstructive Subspace Learning with Class Discriminability (CRSL) approach that uses two reconstructive matrixes through an iterative strategy to cycle-reconstruct data matrixes and update the common subspace. In this way, we learn the invariant features in the common subspace while better preserving global and local structures of the two original domains. Finally, we implement additional discriminative constraints such as intra-class aggregation and inter-class diffusion on the transformed features to ensure the class discriminability of data of the two domains. Extensive experiment results show that our conventional method outperforms state-of-the-art conventional methods and is comparable with advanced deep methods on four current domain adaptation datasets.","Domain adaptation, Subspace learning, Transfer learning, Knowledge transfer",Yayun Xu and Hua Yan,https://www.sciencedirect.com/science/article/pii/S0031320322001819,https://doi.org/10.1016/j.patcog.2022.108700,0031-3203,2022,108700,129,Pattern Recognition,Cycle-reconstructive subspace learning with class discriminability for unsupervised domain adaptation,article,XU2022108700,
"The measurement accuracy of pose parameters based on a single circular feature depends not only on the accuracy of camera calibration and feature extraction but also on the relative pose of the feature and cameraâdifferent poses correspond to different error transmission coefficients. To obtain the relationship between measurement errors and pose parameters, we propose an error analysis method based on geometric interpretation. The method characterises measurement error by the sensitivity the imaging feature has to the variation of pose parameters. In addition, the method can be extended to the error analysis work of other coplanar features' pose measurement algorithms. We conducted simulations on measurement errors of pose parameters under different poses, and the results show that the error distribution of pose parameters is in good agreement with the theoretical analysis. Moreover, we propose a method for judging and optimising outliers, and experimental results show the feasibility of this method.","Pose measurement, Monocular vision, Geometrical analysis, Outlier analysis, Optimisation",Zepeng Wang and Derong Chen and Jiulu Gong,https://www.sciencedirect.com/science/article/pii/S0031320322002072,https://doi.org/10.1016/j.patcog.2022.108726,0031-3203,2022,108726,129,Pattern Recognition,Pose error analysis method based on a single circular feature,article,WANG2022108726,
"Human activity recognition in videos has been widely studied and has recently gained significant advances with deep learning approaches; however, it remains a challenging task. In this paper, we propose a novel framework that simultaneously considers both implicit and explicit representations of human interactions by fusing information of local image where the interaction actively occurred, primitive motion with the posture of individual subjectâs body parts, and the co-occurrence of overall appearance change. Human interactions change, depending on how the body parts of each human interact with the other. The proposed method captures the subtle difference between different interactions using interacting body part attention. Semantically important body parts that interact with other objects are given more weight during feature representation. The combined feature of interacting body part attention-based individual representation and the co-occurrence descriptor of the full-body appearance change is fed into long short-term memory to model the temporal dynamics over time in a single framework. The experimental results on five widely used public datasets demonstrate the effectiveness of the proposed method to recognize human interactions from videos.","Human activity recognition, Human-human interaction, Interacting body part attention",Dong-Gyu Lee and Seong-Whan Lee,https://www.sciencedirect.com/science/article/pii/S0031320322001261,https://doi.org/10.1016/j.patcog.2022.108645,0031-3203,2022,108645,128,Pattern Recognition,Human interaction recognition framework based on interacting body part attention,article,LEE2022108645,
"Visual localization is critical to many robotics and computer vision applications. Absolute pose regression performs localization by encoding scene features followed by pose regression, which has achieved impressive results in localization. It recovers 6-DoF poses from captured scene data alone. However, current methods suffer from being retrained with specific source data whenever the scene changes, resulting in expensive computational costs, data privacy disclosure, and unreliable localization caused by the inability to memorize all data. In this paper, we propose a novel LiDAR-based absolute pose regression network with universal encoding to avoid redundant retraining and the loss of data privacy. Specifically, we propose using universal feature encoding for different scenes. Only the regressor needs to be retrained to achieve higher efficiency, and the training is performed using the encoded features without source data, which preserves data privacy. Then, we propose a memory regressor for memory-aware regression, where the hidden unit numbers in the regressor determine the memorization capacity. It can be used to derive and improve the upper bound of the capacity to enable more reliable localization. Then, it is possible to modify the regressor structure to adapt different memorization capacity requirements for different scene sizes. Extensive experiments on outdoor and indoor datasets validated the above analyses and demonstrated the effectiveness of the proposed method.","LiDAR localization, Absolute pose regression, Universal encoding, Privacy preserving, Memory-aware regression",Shangshu Yu and Cheng Wang and Chenglu Wen and Ming Cheng and Minghao Liu and Zhihong Zhang and Xin Li,https://www.sciencedirect.com/science/article/pii/S0031320322001662,https://doi.org/10.1016/j.patcog.2022.108685,0031-3203,2022,108685,128,Pattern Recognition,LiDAR-based localization using universal encoding and memory-aware regression,article,YU2022108685,
"Open domain recognition has attracted great attention in recent two years, which aims to assign a specific identification for each target sample in the presence of large domain discrepancy both in label space and data distributions. Most existing approaches rely on abundant prior information about the relationship of the label sets between the source and the target domain, which is a great limitation for their applications in practical wild. In this paper, a new Adaptive Open Domain Recognition (AODR) task is introduced, which can generalize to various openness and requires no prior information on the label set. To achieve this adaptive transfer task, a two-stage Progressive Adaptation Network is designed, whose learning process consists of multiple episodes. Each episode is performed to simulate an AODR task. Through training and refining multiple episodes, the basic model has progressively accumulated wealthy experience on predicting unseen categories in the presence of large domain discrepancy, which will well generalize to various openness. More specifically, Fusion Information Guided Feature Prototype Generation module is proposed to synthesize visual feature prototype conditioned on category semantic prototype in training stage. Further, Class-Aware Feature Prototype Alignment module is designed in refining stage to align the global feature prototype for each class between two domains. Experimental results verify that the proposed model not only has superiority on classifying the image instances of known and unknown classes, but also well adapts to various openness.","Open domain recognition, Image classification, Adaptive openness, Prototype learning, Unknown class recognition",Yuan Yuan and Xinxing He and Zhiyu Jiang,https://www.sciencedirect.com/science/article/pii/S0031320322001388,https://doi.org/10.1016/j.patcog.2022.108657,0031-3203,2022,108657,128,Pattern Recognition,Adaptive open domain recognition by coarse-to-fine prototype-based network,article,YUAN2022108657,
"There is still a huge gap in the accuracy of face recognition in public video surveillance scenarios. The far-sighted low-resolution (LR) frontal faces have holistic facial profiles but lack sufficient clearness, while the near-sighted high-resolution (HR) tilted faces show rich facial details yet incomplete facial structure suffering from the overhead self-occlusion of the head blocking the face. Following this observation, this paper proposes a dual-branch HR frontal face reconstruction network to explicitly exploit such coupled complementarity hidden in the far-near face images of the same subject, where one branch performs super-resolution (SR) of the LR frontal face and the other branch performs detail fusion and holistic compensation between multiple HR tilted faces as well as the super-resolved frontal result. In particular, we propose a secondary relevance attention mechanism to enhance the embedding of key features, which sequentially performs rough and precise feature matching and embedding, thus enabling coarse-to-fine progressive compensation. Further, scale-entangled densely connected blocks (SEDCB) are used to gradually integrate the relevance information at different scales (due to the different sighting distances) to promote the information interaction between the features of tilted faces. Besides, we also propose a ternary coupled sample pair (LR far-sighted frontal face, HR near-sighted tilted face, normal ground truth clear face) training scheme to supervise the network optimization. Extensive experimental results on two real-world tilt-view face datasets show that our method can not only reconstruct more realistic HR frontal faces but also facilitate the down-stream face identification task compared with the competing counterparts.","Face frontalization, Super-resolution, Information compensation, Far-near faces",Kangli Zeng and Zhongyuan Wang and Tao Lu and Jianyu Chen and Baojin Huang and Zhen Han and Xin Tian,https://www.sciencedirect.com/science/article/pii/S0031320322002357,https://doi.org/10.1016/j.patcog.2022.108754,0031-3203,2022,108754,129,Pattern Recognition,Realistic frontal face reconstruction using coupled complementarity of far-near-sighted face images,article,ZENG2022108754,
"Camouflaged object detection (COD) aims to detect out-of-attention regions in an image. Current binary segmentation solutions fail to tackle COD easily, since COD is more challenging due to object often accompany with weak boundaries, low contrast, or similar patterns to the background. That is, we need a more efficient scheme to address this problem. In this work, we propose a new COD framework called CubeNetÂ by introducing X connection to the standard encoder-decoder architecture. Specifically, CubeNetÂ consists of two square fusion decoder (SFD) and a sub edge decoder (SED). The special designed SFD takes full advantage of low-level and high-level features extracted from encoder-decoder blocks, providing more powerful representations at each stage. To explicitly modeling the weak boundaries of the objects, we introduced a SED between the two SFD. With such kind of holistic designs, these three decoder modules resolve the challenging ambiguity of camouflaged object detection. CubeNetÂ significantly advance the cutting-edge model on three challenging COD datasets (i.e., COD10K, CAMO, and CHAMELEON), and achieves the real-time (50fps) inference.","Camouflaged object detection, Neural network, Edge guidance, Novel feature aggregation",Mingchen Zhuge and Xiankai Lu and Yiyou Guo and Zhihua Cai and Shuhan Chen,https://www.sciencedirect.com/science/article/pii/S003132032200125X,https://doi.org/10.1016/j.patcog.2022.108644,0031-3203,2022,108644,127,Pattern Recognition,CubeNet: X-shape connection for camouflaged object detection,article,ZHUGE2022108644,
"In this paper, we propose a novel machine learning approach based on robust optimization. Our proposal defines the task of maximizing the two class accuracies of a binary classification problem as a Cobb-Douglas function. This function is well known in production economics and is used to model the relationship between two or more inputs as well as the quantity produced by those inputs. A robust optimization problem is defined to construct the decision function. The goal of the model is to classify each training pattern correctly, up to a given class accuracy, even for the worst possible data distribution. We demonstrate the theoretical advantages of the Cobb-Douglas function in terms of the properties of the resulting second-order cone programming problem. Important extensions are proposed and discussed, including the use of kernel functions and regularization. Experiments performed on several classification datasets confirm these advantages, leading to the best average performance in comparison to various alternative classifiers.","Cobb-Douglas, Minimax Probability Machine, Minimum Error Minimax Probability Machine, Second-order Cone Programming, Support Vector Machines",SebastiÃ¡n Maldonado and Julio LÃ³pez and Miguel Carrasco,https://www.sciencedirect.com/science/article/pii/S0031320322001820,https://doi.org/10.1016/j.patcog.2022.108701,0031-3203,2022,108701,128,Pattern Recognition,The Cobb-Douglas Learning Machine,article,MALDONADO2022108701,
"Recent progress in deep learning-based models has improved photo-realistic (or perceptual) single-image super-resolution significantly. However, despite their powerful performance, many methods are difficult to apply to real-world applications because of the heavy computational requirements. To facilitate the use of a deep model under such demands, we focus on keeping the network efficient while maintaining its performance. In detail, we design an architecture that implements a cascading mechanism on a residual network to boost the performance with limited resources via multi-level feature fusion. In addition, our proposed model adopts group convolution and recursive schemes in order to achieve extreme efficiency. We further improve the perceptual quality of the output by employing the adversarial learning paradigm and a multi-scale discriminator approach. The performance of our method is investigated through extensive internal experiments and benchmarks using various datasets. Our results show that our models outperform the recent methods with similar complexity, for both traditional pixel-based and perception-based tasks.","Super-resolution, Photo-realistic, Convolutional neural network, Efficient model, Adversarial learning, Multi-scale approach",Namhyuk Ahn and Byungkon Kang and Kyung-Ah Sohn,https://www.sciencedirect.com/science/article/pii/S0031320322001303,https://doi.org/10.1016/j.patcog.2022.108649,0031-3203,2022,108649,127,Pattern Recognition,Efficient deep neural network for photo-realistic image super-resolution,article,AHN2022108649,
"Segmentation is essential for medical image analysis to identify and localize diseases, monitor morphological changes, and extract discriminative features for further diagnosis. Skin cancer is one of the most common types of cancer globally, and its early diagnosis is pivotal for the complete elimination of malignant tumors from the body. This research develops an Artificial Intelligence (AI) framework for supervised skin lesion segmentation employing the deep learning approach. The proposed framework, called MFSNet (Multi-Focus Segmentation Network), uses differently scaled feature maps for computing the final segmentation mask using raw input RGB images of skin lesions. In doing so, initially, the images are preprocessed to remove unwanted artifacts and noises. The MFSNet employs the Res2Net backbone, a recently proposed convolutional neural network (CNN), for obtaining deep features used in a Parallel Partial Decoder (PPD) module to get a global map of the segmentation mask. In different stages of the network, convolution features and multi-scale maps are used in two boundary attention (BA) modules and two reverse attention (RA) modules to generate the final segmentation output. MFSNet, when evaluated on three publicly available datasets: PH2, ISIC 2017, and HAM10000, outperforms state-of-the-art methods, justifying the reliability of the framework. The relevant codes for the proposed approach are accessible at https://github.com/Rohit-Kundu/MFSNet.","Lesion Segmentation, Deep Learning, Parallel Partial Decoder, Attention Modules, Skin Melanoma",Hritam Basak and Rohit Kundu and Ram Sarkar,https://www.sciencedirect.com/science/article/pii/S0031320322001546,https://doi.org/10.1016/j.patcog.2022.108673,0031-3203,2022,108673,128,Pattern Recognition,MFSNet: A multi focus segmentation network for skin lesion segmentation,article,BASAK2022108673,
"Cluster analysis has attracted widespread attention in the past several decades. Generally speaking, clustering is considered as an important unsupervised learning method because its goal is to discover unknown subgroups in data without category label information. In this paper, we propose the â0 fusion penalized clustering model (â0-PClust), which is a novel clustering framework founded on the penalized regression method. Theoretically, we first analyze the existence of the optimal solutions of our model and deduce an upper bound of the tuning parameter. Then we define the Karush-Kuhn-Tucker point and P-stationary point of the â0-PClust model, and establish the relationship between them and local optimal solutions. Moreover, based on the P-stationary point of the â0-PClust model, we prove that the distances among different cluster centers are greater than a positive threshold. Computationally, we solve the â0-PClust model via the famous alternating direction method of multipliers, whose limit point is a P-stationary point and local optimal solution of the model. Finally, we conduct extensive experiments on both synthetic and real data sets. Experimental results show outstanding performance of our method in comparison with several state-of-the-art clustering methods.","Penalized clustering,  fusion penalty, Nonconvex discontinuous optimization, Alternating direction method of multipliers",Huangyue Chen and Lingchen Kong and Yan Li,https://www.sciencedirect.com/science/article/pii/S0031320322001704,https://doi.org/10.1016/j.patcog.2022.108689,0031-3203,2022,108689,128,Pattern Recognition,Nonconvex clustering via â0 fusion penalized regression,article,CHEN2022108689,
"Cloud detection is one of the critical tasks in remote sensing image pre-processing and it has attracted extensive research interest. In recent years, deep neural networks based cloud detection methods have surpassed the traditional methods (threshold-based methods and conventional machine learning-based methods). However, current approaches mainly focus on improving detection accuracy. The computation complexity and large model size are ignored. To tackle this problem, we propose a lightweight deep learning cloud detection model: Efficient Cloud Detection Network (ECDNet). This model is based on the encoder-decoder structure. In the encoder, a two-path architecture is proposed to extract the spatial and semantic information concurrently. One pathway is the detail branch. It is designed to capture low-level detail spatial features with only a few parameters. The other pathway is the semantic branch, which is mainly for capturing context features. In the semantic branch, a proposed dense pyramid module (DPM) is designed for multi-scale contextual information extraction. The number of parameters and calculations in DPM is greatly reduced by features reusing. Besides, a FusionBlock is developed to merge these two kinds of information. Then the extreme lightweight decoder recovers the cloud mask to the same scale as the input image step by step. To improve performance, boost loss is introduced without inference cost increment. We evaluate the proposed method on two public datasets: LandSat8 and MODIS. Extensive experiments demonstrate that the proposed ECDNet achieves comparable accuracy as the state-of-art cloud detection methods, and meantime has a much smaller model size and less computation burden.","Lightweight network, Efficient cloud detection, Dual-branch architecture",Chen Luo and Shanshan Feng and Xutao Li and Yunming Ye and Baoquan Zhang and Zhihao Chen and YingLing Quan,https://www.sciencedirect.com/science/article/pii/S0031320322001947,https://doi.org/10.1016/j.patcog.2022.108713,0031-3203,2022,108713,129,Pattern Recognition,ECDNet: A bilateral lightweight cloud detection network for remote sensing images,article,LUO2022108713,
"Knowledge graphs (KGs) are increasingly used to solve the data sparsity and cold start problems of collaborative filtering. Recently, graph neural networks (GNNs) have been applied to build KG-based recommender systems and achieved competitive performance. However, existing GNN-based methods are either limited in their ability to capture fine-grained semantics in a KG, or insufficient in effectively modeling user-item interactions. To address these issues, we propose a novel framework with collaborative and attentive graph convolutional networks for personalized knowledge-aware recommendation. Particularly, we model the user-item graph and the KG separately and simultaneously with an efficient graph convolutional network and a personalized knowledge graph attention network, where the former aims to extract informative collaborative signals, while the latter is designed to capture fine-grained semantics. Collectively, they are able to learn meaningful node representations for predicting user-item interactions. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method compared with state-of-the-arts.","Recommender system, Graph convolutional network, Attention mechanism, Knowledge graph",Quanyu Dai and Xiao-Ming Wu and Lu Fan and Qimai Li and Han Liu and Xiaotong Zhang and Dan Wang and Guli Lin and Keping Yang,https://www.sciencedirect.com/science/article/pii/S0031320322001091,https://doi.org/10.1016/j.patcog.2022.108628,0031-3203,2022,108628,128,Pattern Recognition,Personalized knowledge-aware recommendation with collaborative and attentive graph convolutional networks,article,DAI2022108628,
"Temporal action localization is a challenging task for video understanding. Most previous methods process each proposal independently and neglect the reasoning of proposal-proposal and proposal-context relations. We argue that the supplementary information obtained by exploiting these relations can enhance the proposal representation and further boost the action localization. To this end, we propose a dual relation network to model both proposal-proposal and proposal-context relations. Concretely, a proposal-proposal relation module is leveraged to learn discriminative supplementary information from relevant proposals, which allows the network to model their interaction based on appearance and geometric similarities. Meanwhile, a proposal-context relation module is employed to mine contextual clues by adaptively learning from the global context outside of region-based proposals. They effectively leverage the inherent correlation between actions and the long-term dependency with videos for high-quality proposal refinement. As a result, the proposed framework enables the model to distinguish similar action instances and locate temporal boundaries more precisely. Extensive experiments on the THUMOS14 dataset and ActivityNet v1.3 dataset demonstrate that the proposed method significantly outperforms recent state-of-the-art methods.","Temporal action localization, Relation reasoning",Kun Xia and Le Wang and Sanping Zhou and Gang Hua and Wei Tang,https://www.sciencedirect.com/science/article/pii/S0031320322002060,https://doi.org/10.1016/j.patcog.2022.108725,0031-3203,2022,108725,129,Pattern Recognition,Dual relation network for temporal action localization,article,XIA2022108725,
"For model-based reinforcement learning (MBRL), one of the key challenges is modeling error, which cripples the effectiveness of model planning and causes poor robustness during training. In this paper, we propose a bi-level Erlang Planning Network (EPN) architecture, which is composed of an upper-level agent and several multi-scale parallel sub-agents, trained in an iterative way. The proposed method focuses upon the expansion of representation by environment: a multi-perspective over the world model, which presents a varied way to represent an agentâs knowledge about the world that alleviates the problem of falling into local optimal points and enhances robustness during the progress of model planning. Moreover, our experiments evaluate EPN on a range of continuous-control tasks in MuJoCo, the evaluation results show that the proposed framework finds exemplar solutions faster and consistently reaches the state-of-the-art performance.","Model-based reinforcement learning, Multi-perspective, Bi-level, Planning, Trajectory imagination",Jiao Wang and Lemin Zhang and Zhiqiang He and Can Zhu and Zihui Zhao,https://www.sciencedirect.com/science/article/pii/S0031320322001492,https://doi.org/10.1016/j.patcog.2022.108668,0031-3203,2022,108668,128,Pattern Recognition,Erlang planning network: An iterative model-based reinforcement learning with multi-perspective,article,WANG2022108668,
"Automatic detection of empty spaces (gaps) between the displayed products as seen in the images of shelves of a supermarket is an interesting image segmentation problem. This paper presents the first known attempt to solve this commercially relevant challenge. The shelf image is first over-segmented into a number of superpixels to construct a graph of superpixels (SG). Subsequently, a graph convolutional network and a Siamese network are built to process the SG. Finally, a structural support vector machine based inference model is formulated based on SG for segmenting the gap and non-gap regions. In order to validate our method, we manually annotate the images of shelves of three benchmark datasets of retail products. We have achieved â¼70 to â¼85% segmentation accuracy (in terms of mean intersection-over-union) on the annotated datasets. A part of the annotated data is released at https://github.com/gapDetection/gapDetectionDatasets.","Gap detection, Retail store, Graph convolutional network, Siamese network, Structural support vector machine",Bikash Santra and Udita Ghosh and Dipti Prasad Mukherjee,https://www.sciencedirect.com/science/article/pii/S003132032200108X,https://doi.org/10.1016/j.patcog.2022.108627,0031-3203,2022,108627,127,Pattern Recognition,Graph-based modelling of superpixels for automatic identification of empty shelves in supermarkets,article,SANTRA2022108627,
"LiDAR-based 3D object detection pushes forward an immense influence on autonomous vehicles. Due to the limitation of the intrinsic properties of LiDAR, fewer points are collected at the objects farther away from the sensor. This imbalanced density of point clouds degrades the detection accuracy but is generally neglected by previous works. To address the challenge, we propose a novel two-stage 3D object detection framework, named SIENet. Specifically, we design the Spatial Information Enhancement (SIE) module to predict the spatial shapes of the foreground points within proposals, and extract the structure information to learn the representative features for further box refinement. The predicted spatial shapes are complete and dense point sets, thus the extracted structure information contains more semantic representation. Besides, we design the Hybrid-Paradigm Region Proposal Network (HP-RPN) which includes multiple branches to learn discriminate features and generate accurate proposals for the SIE module. Extensive experiments on the KITTI 3D object detection benchmark show that our elaborately designed SIENet outperforms the state-of-the-art methods by a large margin. Codes will be publicly available at https://github.com/Liz66666/SIENet.","3D object detection, Autonomous vehicles, Point cloud, LiDAR sensor, 3D shape completion",Ziyu Li and Yuncong Yao and Zhibin Quan and Jin Xie and Wankou Yang,https://www.sciencedirect.com/science/article/pii/S0031320322001650,https://doi.org/10.1016/j.patcog.2022.108684,0031-3203,2022,108684,128,Pattern Recognition,Spatial information enhancement network for 3D object detection from point cloud,article,LI2022108684,
"Given one task, it is difficult to generate CNN models for many different hardware platforms with extremely diverse computing power for this task. Repeating network pruning or architecture search for each platform is very time-consuming. In this paper, we propose properties that are required for this model generation problem: versatile (fits diverse applications and network structures), full-spectrum (generates models for devices with tiny to gigantic computing power), and swift (total training time for all platforms is short, and generated models have low latency). We show that existing methods do not satisfy these requirements and propose a VFS method (the V/F/S represents Versatile/Full-spectrum/Swift, respectively). VFS uses importance sampling to sample many submodels with versatile structures and with different input image resolutions. We propose new fine-tuning strategies that only need to fine-tune a best candidate submodel for few epochs for each platform. VFS satisfies all three requirements. It generates versatile models with low latency for diverse applications, is suitable for devices with a wide range of computing power differences, and the models which are generated by VFS achieve state-of-the-art accuracy.","Model generation, Convolutional neural networks, Structured pruning, Model compression",Huanyu Wang and Yongshun Zhang and Jianxin Wu,https://www.sciencedirect.com/science/article/pii/S0031320322002102,https://doi.org/10.1016/j.patcog.2022.108729,0031-3203,2022,108729,129,Pattern Recognition,"Versatile, fullâspectrum, and swift network sampling for model generation",article,WANG2022108729,
"Video-based person re-identification (Re-ID) aims to match identical person sequences captured across non-overlapping surveillance areas. It is an essential yet challenging task to effectively embed spatial and temporal information into the video feature representation. For one thing, we observe that different frames in the video can provide complementary information for each other. Also, local features which is lost due to target occlusion or visual ambiguity in one frame can be supplemented by the same pedestrian part in other frames. For another thing, graph neural network enables the contextual interactions between relevant regional features. Therefore, we propose a novel sparse graph wavelet convolution neural network (SGWCNN) for video-based person Re-ID. Distinct from previous graph-based Re-ID methods, we exploit the weighted sparse graph to model the semantic relation among the local patches of pedestrians in the video. Each local patch in one frame can extract supplementary information from highly related patches in other frames. Moreover, to effectively solve the problems of short time occlusion and pedestrian misalignment, the graph wavelet convolution neural network is adopted for feature propagation to refine regional features iteratively. Experiments and evaluation on three challenging benchmarks, that is, MARS, DukeMTMC-VideoReID, and iLIDS-VID, show that the proposed SGWCNN effectively improves the performance of video-based person re-identification.","Video-based person re-identification, Weighted sparse graph, Graph wavelet convolution neural network",Yingmao Yao and Xiaoyan Jiang and Hamido Fujita and Zhijun Fang,https://www.sciencedirect.com/science/article/pii/S0031320322001893,https://doi.org/10.1016/j.patcog.2022.108708,0031-3203,2022,108708,129,Pattern Recognition,A sparse graph wavelet convolution neural network for video-based person re-identification,article,YAO2022108708,
"This study presents the Auditory Cortex ResNet (AUCO ResNet), it is a biologically inspired deep neural network especially designed for sound classification and more specifically for Covid-19 recognition from audio tracks of coughs and breaths. Differently from other approaches, it can be trained end-to-end thus optimizing (with gradient descent) all the modules of the learning algorithm: mel-like filter design, feature extraction, feature selection, dimensionality reduction and prediction. This neural network includes three attention mechanisms namely the squeeze and excitation mechanism, the convolutional block attention module, and the novel sinusoidal learnable attention. The attention mechanism is able to merge relevant information from activation maps at various levels of the network. The net takes as input raw audio files and it is able to fine tune also the features extraction phase. In fact, a Mel-like filter is designed during the training, thus adapting filter banks on important frequencies. AUCO ResNet has proved to provide state of art results on many datasets. Firstly, it has been tested on many datasets containing Covid-19 cough and breath. This choice is related to the fact that that cough and breath are language independent, allowing for cross dataset tests with generalization aims. These tests demonstrate that the approach can be adopted as a low cost, fast and remote Covid-19 pre-screening tool. The net has also been tested on the famous UrbanSound 8K dataset, achieving state of the art accuracy without any data preprocessing or data augmentation technique.","Audio classification, Spectrograms, Attention mechanism, Covid, Pre-screening, Convolutional neural network",Vincenzo Dentamaro and Paolo Giglio and Donato Impedovo and Luigi Moretti and Giuseppe Pirlo,https://www.sciencedirect.com/science/article/pii/S0031320322001376,https://doi.org/10.1016/j.patcog.2022.108656,0031-3203,2022,108656,127,Pattern Recognition,AUCO ResNet: an end-to-end network for Covid-19 pre-screening from cough and breath,article,DENTAMARO2022108656,
"Multi-view facial expression recognition (FER) is a challenging computer vision task due to the large intra-class difference caused by viewpoint variations. This paper presents a novel orthogonal channel attention-based multi-task learning (OCA-MTL) approach for FER. The proposed OCA-MTL approach adopts a Siamese convolutional neural network (CNN) to force the multi-view expression recognition model to learn the same features as the frontal expression recognition model. To further enhance the recognition accuracy of non-frontal expression, the multi-view expression model adopts a multi-task learning framework that regards head pose estimation (HPE) as an auxiliary task. A separated channel attention (SCA) module is embedded in the multi-task learning framework to generate individual attention for FER and HPE. Furthermore, orthogonal channel attention loss is presented to force the model to employ different feature channels to represent the facial expression and head pose, thereby decoupling them. The proposed approach is performed on two public facial expression datasets to evaluate its effectiveness and achieves an average recognition accuracy rate of 88.41% under 13 viewpoints on Multi-PIE and 89.04% under 5 viewpoints on KDEF, outperforming state-of-the-art methods.","Multi-view facial expression recognition, Orthogonal channel attention, Multi-task learning, Siamese convolutional neural network, Separated channel attention module",Jingying Chen and Lei Yang and Lei Tan and Ruyi Xu,https://www.sciencedirect.com/science/article/pii/S0031320322002345,https://doi.org/10.1016/j.patcog.2022.108753,0031-3203,2022,108753,129,Pattern Recognition,Orthogonal channel attention-based multi-task learning for multi-view facial expression recognition,article,CHEN2022108753,
"Multi-task deep learning is promising to solve multi-label multi-instance visual recognition tasks. However, flexible information sharing in the task group might bring performance bottlenecks to an individual task. To tackle this problem, we propose a novel learning framework of multi-task Convolutional Neural Network (CNN) to enhance task attention through conditionally tuning the Task Transfer Connections (TTC) with adversarial learning. For the dynamic multi-task CNN, we set up a shared subnet to extract shared features across multiple tasks and a task discriminator shared by all layers to distinguish features of all subnets. The adversarial training is introduced between the shared subnet and the task discriminator to guide each task subnet to focus on its specific task. To apply adversarial learning to the complex labeling system of multiple tasks, we design an even-label strategy for the multi-task model with a shared subnet to make adversarial learning feasible for the complex labeling system of multiple tasks. As a result, the proposed model can constrain the shared subnetâs learning unbiased to any single task and achieve task attention for all task subnets. Experimental results of the ablation study and the TTC analysis validate the effectiveness of the proposed approach.","Deep learning, Adversarial learning, Multi-task learning",Yuchun Fang and Shiwei Xiao and Menglu Zhou and Sirui Cai and Zhaoxiang Zhang,https://www.sciencedirect.com/science/article/pii/S0031320322001534,https://doi.org/10.1016/j.patcog.2022.108672,0031-3203,2022,108672,128,Pattern Recognition,Enhanced task attention with adversarial learning for dynamic multi-task CNN,article,FANG2022108672,
"Existing methods can generate a high dynamic range (HDR) image from a single low dynamic range (LDR) image using convolutional neural networks (CNNs). However, they are too cumbersome to run on mobile devices with limited computational resources. In this work, we design a lightweight CNN, namely LiTMNet which takes a single LDR image as input and recovers the lost information in its saturated regions to reconstruct an HDR image. To avoid trading off the reconstruction quality for efficiency, LiTMNet does not only adapt a lightweight encoder for efficient feature extraction, but also contains newly designed upsampling blocks in the decoder to alleviate artifacts and further accelerate the reconstruction. The final HDR image is produced by nonlinearly blending the network prediction and the original LDR image. Qualitative and quantitative comparisons demonstrate that LiTMNet produces HDR images of high quality comparable with the current state of the art and is 38Ã faster as tested on a mobile device. Please refer to the supplementary video for additional visual results.","HDR image reconstruction, Lightweight CNN, Inverse tone mapping",Guotao Wu and Ran Song and Mingxin Zhang and Xiaolei Li and Paul L. Rosin,https://www.sciencedirect.com/science/article/pii/S0031320322001017,https://doi.org/10.1016/j.patcog.2022.108620,0031-3203,2022,108620,127,Pattern Recognition,LiTMNet: A deep CNN for efficient HDR image reconstruction from a single LDR image,article,WU2022108620,
"GraphSAGE is a widely-used graph neural network for classification, which generates node embeddings in two steps: sampling and aggregation. In this paper, we introduce causal inference into the GraphSAGE sampling stage, and propose Causal GraphSAGE (C-GraphSAGE) to improve the robustness of the classifier. In C-GraphSAGE, we use causal bootstrapping to obtain a weighting between the target node's neighbors and their label. Then, these weights are used to resample the node's neighbors to enforce the robustness of the sampling stage. Finally, an aggregation function is trained to integrate the features of the selected neighbors to obtain the embedding of the target node. Experimental results on the Cora, Pubmed, and Citeseer citation datasets show that the classification performance of C-GraphSAGE is equivalent to that of GraphSAGE, GCN, GAT, and RL-GraphSAGE in the case of no perturbation, and outperforms these as the perturbation ratio increases.","Causal GraphSAGE, GraphSAGE, Causal sampling, Robustness, Causal inference",Tao Zhang and Hao-Ran Shan and Max A. Little,https://www.sciencedirect.com/science/article/pii/S0031320322001777,https://doi.org/10.1016/j.patcog.2022.108696,0031-3203,2022,108696,128,Pattern Recognition,Causal GraphSAGE: A robust graph method for classification based on causal sampling,article,ZHANG2022108696,
"Recent developments in pattern analysis have motivated many researchers to focus on developing deep learning based solutions in various image processing applications. Fusing multi-modal images has been one such application area where the interest is combining different information coming from different modalities in a more visually meaningful and informative way. For that purpose, it is important to first extract salient features from each modality and then fuse them as efficiently and informatively as possible. Recent literature on fusing multi-modal images reports multiple deep solutions that combine both visible (RGB) and infra-red (IR) images. In this paper, we study the performance of various deep solutions available in the literature while seeking an answer to the question: âDo we really need deeper networks to fuse multi-modal images?â To have an answer for that question, we introduce a novel architecture based on Siamese networks to fuse RGB (visible) images with infrared (IR) images and report the state-of-the-art results. We present an extensive analysis on increasing the layer numbers in the architecture with the above-mentioned question in mind to see if using deeper networks (or adding additional layers) adds significant performance in our proposed solution. We report the state-of-the-art results on visually fusing given visible and IR image pairs in multiple performance metrics, while requiring the least number of trainable parameters. Our experimental results suggest that shallow networks (as in our proposed solutions in this paper) can fuse both visible and IR images as well as the deep networks that were previously proposed in the literature (we were able to reduce the total number of trainable parameters up to 96.5%, compare 2,625 trainable parameters to the 74,193 trainable parameters).","Multi-temporal fusion, Efficient learning, Multi-modal fusion",Sedat Ãzer and Mert Ege and Mehmet Akif Ãzkanoglu,https://www.sciencedirect.com/science/article/pii/S0031320322001935,https://doi.org/10.1016/j.patcog.2022.108712,0031-3203,2022,108712,129,Pattern Recognition,SiameseFuse: A computationally efficient and a not-so-deep network to fuse visible and infrared images,article,OZER2022108712,
"Analyzing the layout of a document to identify headers, sections, tables, figuresÂ etc. is critical to understanding its content. Deep learning based approaches for detecting the layout structure of document images have been promising. However, these methods require a large number of annotated examples during training, which are both expensive and time consuming to obtain. We describe here a synthetic document generator that automatically produces realistic documents with labels for spatial positions, extents and categories of the layout elements. The proposed generative process treats every physical component of a document as a random variable and models their intrinsic dependencies using a Bayesian Network graph. Our hierarchical formulation using stochastic templates allow parameter sharing between documents for retaining broad themes and yet the distributional characteristics produces visually unique samples, thereby capturing complex and diverse layouts. We empirically illustrate that a deep layout detection model trained purely on the synthetic documents can match the performance of a model that uses real documents.","Synthetic image generation, Bayesian network, Layout analysis",Natraj Raman and Sameena Shah and Manuela Veloso,https://www.sciencedirect.com/science/article/pii/S0031320322001418,https://doi.org/10.1016/j.patcog.2022.108660,0031-3203,2022,108660,128,Pattern Recognition,Synthetic document generator for annotation-free layout recognition,article,RAMAN2022108660,
"Semantic segmentation tasks based on weakly supervised conditions have been put forward to achieve a lightweight labeling process. For simple images that only include a few categories, research based on image-level annotations has achieved acceptable performance. However, when facing complex scenes, since image contains a large number of classes, it becomes challenging to learn visual appearance based on image tags. In this case, image-level annotations are not useful in providing information. Therefore, we set up a new task in which a single annotated pixel is provided for each category in a whole dataset. Based on the more lightweight and informative condition, a three step process is built for pseudo labels generation, which progressively implements each classâ optimal feature representation, image inference, and context-location based refinement. In particular, since high-level semantics and low-level imaging features have different discriminative abilities for each class under driving scenes, we divide categories into âobjectâ or âsceneâ and then provide different operations for the two types separately. Further, an alternate iterative structure is established to gradually improve segmentation performance, which combines CNN-based inter-image common semantic learning and imaging prior based intra-image modification process. Experiments on the Cityscapes dataset demonstrate that the proposed method provides a feasible way to solve weakly supervised semantic segmentation tasks under complex driving scenes.","Weakly supervised condition, Semantic segmentation, Complex driving scenes, Optimal feature setting",Xi Li and Huimin Ma and Sheng Yi and Yanxian Chen and Hongbing Ma,https://www.sciencedirect.com/science/article/pii/S0031320321001667,https://doi.org/10.1016/j.patcog.2021.107979,0031-3203,2021,107979,116,Pattern Recognition,Single annotated pixel based weakly supervised semantic segmentation under driving scenes,article,LI2021107979,
"Robust and discriminative image representation is a long-lasting battle in the computer vision and pattern recognition. Moment-based image representation model is effective in satisfying the core conditions of semantic description, due to its geometric invariance and independence. However, moment-based descriptors suffer from a contradiction between the robustness and discriminability, which limits the further improvement of description quality. In this paper, a set of generic moments along with a novel representation framework are proposed to mitigate this troublesome contradiction. We first define a new set of orthogonal moments, named Fractional-order Jacobi-Fourier Moments (FJFM), which is characterized by the generic nature and time-frequency analysis capability. We then develop a new framework to improve both the robustness and discriminability of image representation, called Mixed Low-order Moment Feature (MLMF), by fully exploiting the time-frequency property of FJFM. Extensive experimental results and a real-world application are provided to demonstrate the superior performance of our FJFM-based MLMF, with respect to robustness and discriminability.","Image representation, Fractional, Jacobi-Fourier moments, Robustness, Discriminability",Hongying Yang and Shuren Qi and Jialin Tian and Panpan Niu and Xiangyang Wang,https://www.sciencedirect.com/science/article/pii/S0031320321000856,https://doi.org/10.1016/j.patcog.2021.107898,0031-3203,2021,107898,115,Pattern Recognition,Robust and discriminative image representation: fractional-order Jacobi-Fourier moments,article,YANG2021107898,
"Pedestrian detection has emerged as a fundamental technology for autonomous cars, robotics, pedestrian search, and other applications. Although many excellent object detection algorithms can be used for pedestrian detection, it is still a challenging problem due to the complicated real-world scenarios, e.g., the detection of pedestrians in low-quality surveillance videos. In this paper, we aim to study the challenging topic of pedestrian detection in low-quality images. Low-quality images are interpreted as those taken with a low-resolution camera, heavy weather or a blurred scene, making it difficult to distinguish pedestrians from the background. To solve this problem, we first introduce a dataset called playground (PG) for low-quality image detection. Images from PG are shot using two different camera views, and pedestrian images are taken at different periods, including day and night. The dataset contains a total of 5,752 images with 31,041 annotations. The average size of the pedestrian is 87Ã41 and the image size is 480 Ã 640, indicating that these images are taken from very long distances. Then, we propose a super-resolution detection (SRD) network to enhance the resolution of low-quality images that can help distinguish pedestrians from the blurred background. Finally, based on these enhanced images, we adopt and improve the Faster R-CNN network to help relocate occluded pedestrians. Experimental results on this new dataset proved the efficiency and effectiveness of our algorithm on low-quality images.","Pedestrian detection, Low-quality, SRGAN, Faster R-CNN",Yi Jin and Yue Zhang and Yigang Cen and Yidong Li and Vladimir Mladenovic and Viacheslav Voronin,https://www.sciencedirect.com/science/article/pii/S0031320321000339,https://doi.org/10.1016/j.patcog.2021.107846,0031-3203,2021,107846,115,Pattern Recognition,Pedestrian detection with super-resolution reconstruction for low-quality image,article,JIN2021107846,
"In this paper11Dong Liang, Bin Kang and Xinyu Liu contributed equally to this work., we investigate cross-scene video foreground segmentation via supervised and unsupervised model communication. Traditional unsupervised background subtraction methods often face the challenging problem of updating the statistical background model online. In contrast, supervised foreground segmentation methods, such as those that are based on deep learning, rely on large amounts of training data, thereby limiting their cross-scene performance. Our method leverages segmented masks from a cross-scene trained deep model (spatio-temporal attention model (STAM), pyramid scene parsing network (PSPNet), or DeepLabV3+) to seed online updates for the statistical background model (CPB), thereby refining the foreground segmentation. More flexible than methods that require scene-specific training and more data-efficient than unsupervised models, our method outperforms state-of-the-art approaches on CDNet2014, WallFlower, and LIMU according to our experimental results. The proposed framework can be integrated into a video surveillance system in a plug-and-play form to realize cross-scene foreground segmentation.","Foreground segmentation, Model communication, Cross-scene, Online updates",Dong Liang and Bin Kang and Xinyu Liu and Pan Gao and Xiaoyang Tan and Shunâichi Kaneko,https://www.sciencedirect.com/science/article/pii/S0031320321001825,https://doi.org/10.1016/j.patcog.2021.107995,0031-3203,2021,107995,117,Pattern Recognition,Cross-scene foreground segmentation with supervised and unsupervised model communication,article,LIANG2021107995,
"In this paper, we focus on the unsupervised domain adaptation problem where an approximate inference model is to be learned from a labeled data domain and expected to generalize well to an unlabeled domain. The success of unsupervised domain adaptation largely relies on the cross-domain feature alignment. Previous work has attempted to directly align features by classifier-induced discrepancies. Nevertheless, a common feature space cannot always be learned via this direct feature alignment especially when large domain gaps exist. To solve this problem, we introduce a Gaussian-guided latent alignment approach to align the latent feature distributions of the two domains under the guidance of a prior. In such an indirect way, the distributions over the samples from the two domains will be constructed on a common feature space, i.e., the space of the prior, which promotes better feature alignment. To effectively align the target latent distribution with this prior distribution, we also propose a novel unpaired L1-distance by taking advantage of the formulation of the encoder-decoder. The extensive evaluations on nine benchmark datasets validate the superior knowledge transferability through outperforming state-of-the-art methods and the versatility of the proposed method by improving the existing work significantly.","Domain adaptation, Computer vision, Information theory",Jing Wang and Jiahong Chen and Jianzhe Lin and Leonid Sigal and Clarence W. {de Silva},https://www.sciencedirect.com/science/article/pii/S0031320321001308,https://doi.org/10.1016/j.patcog.2021.107943,0031-3203,2021,107943,116,Pattern Recognition,Discriminative feature alignment: Improving transferability of unsupervised domain adaptation by Gaussian-guided latent alignment,article,WANG2021107943,
"Hyperspectral Image classification plays an important role in the maintenance of remote image analysis, which has been attracting a lot of research interest. Although various approaches, including unsupervised and supervised methods, have been proposed, obtaining a satisfactory classification result is still a challenge. In this paper, an efficient transductive learning method using variational nonlocal graph theory for hyperspectral image classification is proposed. First, the nonlocal vector neighborhood similarity is employed to build sparse graph representation. Then the variational segmentation framework is extended to label space, and the vectorization nonlocal energy function is constructed. Next, a fast comprehensive alternating minimization iteration algorithm is designed to implement labels transductive learning. At the same time, the labeled sample constraints are doubled ensured with simplex projection. Finally, experiments on six widely used hyperspectral image datasets are implemented, compared with other state-of-the-art classification methods, the classification results demonstrate that the proposed method has higher classification performance. Benefiting from graph theory and transductive idea, the proposed classification method can propagate labels and overcome the very high dimensionality and limited labeling problem to some extent.","Transductive learning, Nonlocal graph, Label propagation, Variational method, Alternating direction method of multipliers, Hyperspectral image classification",Baoxiang Huang and Linyao Ge and Ge Chen and Milena Radenkovic and Xiaopeng Wang and Jinming Duan and Zhenkuan Pan,https://www.sciencedirect.com/science/article/pii/S0031320321001540,https://doi.org/10.1016/j.patcog.2021.107967,0031-3203,2021,107967,116,Pattern Recognition,Nonlocal graph theory based transductive learning for hyperspectral image classification,article,HUANG2021107967,
"One-class classification is a challenging subfield of machine learning in which so-called data descriptors are used to predict membership of a class based solely on positive examples of that class, and no counter-examples. A number of data descriptors that have been shown to perform well in previous studies of one-class classification, like the Support Vector Machine (SVM), require setting one or more hyperparameters. There has been no systematic attempt to date to determine optimal default values for these hyperparameters, which limits their ease of use, especially in comparison with hyperparameter-free proposals like the Isolation Forest (IF). We address this issue by determining optimal default hyperparameter values across a collection of 246 one-class classification problems derived from 50 different real-world datasets. In addition, we propose a new data descriptor, Average Localised Proximity (ALP) to address certain issues with existing approaches based on nearest neighbour distances. Finally, we evaluate classification performance using a leave-one-dataset-out procedure, and find strong evidence that ALP outperforms IF and a number of other data descriptors, as well as weak evidence that it outperforms SVM, making ALP a good default choice.","Data descriptors, Hyperparameters, Localised distance, Nearest neighbours, Novelty detection, One-class classification, OWA operators, Semi-supervised outlier detection",Oliver Urs Lenz and Daniel Peralta and Chris Cornelis,https://www.sciencedirect.com/science/article/pii/S0031320321001783,https://doi.org/10.1016/j.patcog.2021.107991,0031-3203,2021,107991,118,Pattern Recognition,Average Localised Proximity: A new data descriptor with good default one-class classification performance,article,LENZ2021107991,
"As a new method of human-computer interaction, inertial sensor-based in-air handwriting can provide natural and unconstrained interaction to express more complex and rich information in 3D space. However, most of the existing literature is mainly focused on in-air handwriting recognition (IAHR), which makes these works suffer from the poor readability of inertial signals and the lack of labeled samples. To address these two problems, we use an unsupervised domain adaptation method to recover the trajectory of inertial signals and generate inertial samples using handwritten trajectories. In this paper, we propose an Air-Writing Translator model to learn the bi-directional translation between trajectory domain and inertial domain in the absence of paired inertial and trajectory samples. Through latent-level adversarial learning and latent classification loss, the proposed model learns to extract domain-invariant features between the inertial signal and the trajectory while preserving semantic consistency during the translation across the two domains. In addition, the proposed framework can accept inputs of arbitrary length and translate between different sampling rates. Experiments on two public datasets, 6DMG (in-air handwriting dataset) and CT (handwritten trajectory dataset), are conducted and the results demonstrate that the proposed model can achieve reliable translation between inertial domain and trajectory domain. Empirically, our method also yields the best results in comparison to the state-of-the-art methods for IAHR.","In-air handwriting, Bi-directional inertia-Trajectory translation, Unsupervised domain adaptation, Latent-level adversarial learning",Songbin Xu and Yang Xue and Xin Zhang and Lianwen Jin,https://www.sciencedirect.com/science/article/pii/S0031320321001266,https://doi.org/10.1016/j.patcog.2021.107939,0031-3203,2021,107939,116,Pattern Recognition,A Novel Unsupervised domain adaptation method for inertia-Trajectory translation of in-air handwriting,article,XU2021107939,
"For a Level 3 automated vehicle, according to the SAE International Automation Levels definition (J3016), the identification of non-driving activities (NDAs) that the driver is engaging with is of great importance in the design of an intelligent take-over interface. Much of the existing literature focuses on the driver take-over strategy with associated Human-Machine Interaction design. This paper proposes a dual-camera based framework to identify and track NDAs that require visual attention. This is achieved by mapping the driver's gaze using a nonlinear system identification approach, on the object scene, recognised by a deep learning algorithm. A novel gaze-based region of interest (ROI) selection module is introduced and contributes about a 30% improvement in average success rate and about a 60% reduction in average processing time compared to the results without this module. This framework has been successfully demonstrated to identify five types of NDA required visual attention with an average success rate of 86.18%. The outcome of this research could be applicable to the identification of other NDAs and the tracking of NDAs within a certain time window could potentially be used to evaluate the driver's attention level for both automated and human-driving vehicles.","Driver behaviour, Level 3 automation, Computer vision, Non-driving related task, activities identification",Lichao Yang and Kuo Dong and Yan Ding and James Brighton and Zhenfei Zhan and Yifan Zhao,https://www.sciencedirect.com/science/article/pii/S0031320321001424,https://doi.org/10.1016/j.patcog.2021.107955,0031-3203,2021,107955,116,Pattern Recognition,Recognition of visual-related non-driving activities using a dual-camera monitoring system,article,YANG2021107955,
"Meta-learning is an effective tool to address the few-shot learning problem, which requires new data to be classified considering only a few training examples. However, when used for classification, it requires large labeled datasets, which are not always available in practice. In this paper, we propose an unsupervised meta-learning algorithm that learns from an unlabeled dataset and adapts to downstream human-specific tasks with few labeled data. The proposed algorithm constructs tasks using clustering embedding methods and data augmentation functions to satisfy two critical class distinction requirements. To alleviate the biases and the weak diversity problem introduced by data augmentation functions, the proposed algorithm uses two methods, which are shifting the feeding data between the inner-outer loops and a novel data augmentation function. We further provide theoretical analysis of the effect of augmentation data in the inner/outer loop. Experiments on the MiniImagenet and Omniglot datasets demonstrate that the proposed unsupervised meta-learning approach outperforms other tested unsupervised representation learning approaches and two recent unsupervised meta-learning baselines. Compared with supervised meta-learning approaches, certain results produced by our method are quite close to those produced by such methods trained on the human-designed labeled tasks.","Unsupervised learning, Meta-learning, Few-shot learning",Hui Xu and Jiaxing Wang and Hao Li and Deqiang Ouyang and Jie Shao,https://www.sciencedirect.com/science/article/pii/S0031320321001382,https://doi.org/10.1016/j.patcog.2021.107951,0031-3203,2021,107951,116,Pattern Recognition,Unsupervised meta-learning for few-shot learning,article,XU2021107951,
"Hausdorff distance (HD) is a popular similarity metric used in the comparison of images or 3D volumes. Although popular, its main weakness is computing power consumption, being one of the slowest set distances. In this work, a novel, parallel and locality-oriented Hausdorff distance implementation is proposed. Novel as it is the first time in the literature that an actual algorithmic implementation using morphological dilations is proposed and thoroughly evaluated. Parallel, as it is more robust in terms of parallelization than the state-of-the-art algorithm and local as it has an intrinsic sensitivity to voxels that are closer in space. This proposal can be faster than the state-of-the-art in several practical cases such as in medical imaging registrations (up to 8 times faster on average in one of the CPU experiments) and is faster in the worst-case (up to 22337 times faster in one of the CPU experiments). Worst-case scenarios and high resolution volumes also favor the proposed approach. Throughout the work, several sequential and parallel CPU and GPU implementations are evaluated and compared.","Hausdorff distance, Mathematical morphology, Similarity, Registration, Parallelism",Ãrick Oliveira Rodrigues,https://www.sciencedirect.com/science/article/pii/S003132032100176X,https://doi.org/10.1016/j.patcog.2021.107989,0031-3203,2021,107989,117,Pattern Recognition,An efficient and locality-oriented Hausdorff distance algorithm: Proposal and analysis of paradigms and implementations,article,RODRIGUES2021107989,
"Occlusion due to hair in dermoscopic images affects the diagnostic operation and the accuracy of its analysis of a skin lesion. Also, dermis hair has the following different characteristics: thin; overlapping; faded; of similar contrast or colour to the underlying skin; and obscuring/covering textured lesions. These make digital hair removal (DHR), which involves hair segmentation and hair gap inpainting, a challenging task. Thus, traditional hard-coded threshold-based hair removal methods are not effective, resulting in over-removal which loses important information of the skin lesion, or under-removal which cannot remove the hair effectively. In this paper, we propose a deep learning approach to DHR based on U-Net and a free-form image inpainting architecture. In hair segmentation, a well-labelled dataset is created and used to train U-Net in order to obtain accurate hair masks. In inpainting, a free-form image inpainting architecture (i.e., Gated convolution and SN-PatchGAN) which has been trained on millions of images is used to inpaint any hair gaps. We also propose an evaluation method to analyze the effect of hair removal based on a single dermoscopic image, named intra structural similarity (Intra-SSIM). The process of DHR is repeated until there is no change in the average value of Intra-SSIM. Using the ISIC 2018 dataset, the performance of the proposed method is shown to be better than other state-of-the-art methods.","Dermoscopy, Digital hair removal, Skin lesion segmentation, Deep learning",Wei Li and Alex Noel {Joseph Raj} and Tardi Tjahjadi and Zhemin Zhuang,https://www.sciencedirect.com/science/article/pii/S0031320321001813,https://doi.org/10.1016/j.patcog.2021.107994,0031-3203,2021,107994,117,Pattern Recognition,Digital hair removal by deep learning for skin lesion segmentation,article,LI2021107994,
"Spiking neural network (SNN) is a type of artificial neural network that uses biologically inspired neuron models and learning rules to develop artificial intelligence with capability parallel to human brain. Deep neural networks (DNNs), on the other hand, uses less biologically plausible neurons and training methods such as gradient descent, and has shown good accuracy in computer vision tasks. However, human brain can still outperform DNN in certain scenarios. For example, DNN experiences significant performance degradation when perturbation from various sources is present in the input, which makes DNN less reliable for systems interacting with physical world. In this paper, we present a hybrid deep network architecture with spike-assisted contextual information extraction (ScieNet) as a solution to the problem. ScieNet integrates a front-end SNN with a novel stochastic spike-timing-dependent plasticity (STDP) algorithm that extracts visual context from images. The back-end DNN is trained for classification given the contextual information. The integrated network demonstrates high resilience to input perturbations without relying on pre-training on perturbed inputs. We demonstrate ScieNet with various back-end DNNs for image classification using different datasets and considering both stochastic and structured input perturbations. Experimental results demonstrate significant improvement in accuracy on perturbed images, while maintaining state-of-the-art accuracy on clean images.","Deep learning, Noise robustness, Spiking neural network",Xueyuan She and Yun Long and Daehyun Kim and Saibal Mukhopadhyay,https://www.sciencedirect.com/science/article/pii/S0031320321001898,https://doi.org/10.1016/j.patcog.2021.108002,0031-3203,2021,108002,118,Pattern Recognition,ScieNet: Deep learning with spike-assisted contextual information extraction,article,SHE2021108002,
"Consensus maximization is widely used in robust model fitting, and it is usually solved by RANSAC-type methods in practice. However, these methods cannot guarantee global optimality and sometimes return the wrong solutions. A series of Branch-and-bound (BnB) based globally optimal methods have been proposed, most of which involve deriving a complex bound. Interval arithmetic was utilized to derive simple bounds for BnB in solving geometric matching problems in 2003. However, this idea was somewhat forgotten in the community because it seems natural that the simple interval arithmetic based bounds might be worse than those elaborate bounds. Recently, some new globally optimal algorithms without using BnB were developed for consensus maximization, but they can only work with a small number of data points and low outlier ratios. In this work, we draw the idea of simple bounds by interval arithmetic back on the map and demonstrate its practicability by making substantial extensions. Concretely, we give detailed derivation of solving robust model fitting problems with both linear and quasi-convex residuals and propose practical methods to use them under Unit-Norm constraint and in a high-dimensional problem. Extensive experiments show that the proposed method can handle practical problems with large number of data points and high outlier ratios. It outperforms state-of-the-art global, RANSAC-type, and deterministic methods in terms of both accuracy and efficiency in low-dimensional problems. The source code is publicly available.22https://github.com/YiruWangYuri/Demo-for-GoIA.","Consensus maximization, Globally optimization, Robust model fitting, Branch-and-bound, Interval arithmetic",Yiru Wang and Yinlong Liu and Xuechen Li and Chen Wang and Manning Wang and Zhijian Song,https://www.sciencedirect.com/science/article/pii/S0031320321000844,https://doi.org/10.1016/j.patcog.2021.107897,0031-3203,2021,107897,115,Pattern Recognition,Practical globally optimal consensus maximization by Branch-and-bound based on interval arithmetic,article,WANG2021107897,
"Correlation clustering (CC) is a clustering method using a signed graph as input without specifying the number of clusters a priori. It had been widely used in real applications, such as social network and text mining. However, its exact optimization or approximate algorithms often give unsatisfactory results, especially for large-scale signed graphs. This paper tackles this problem and proposes a novel CC algorithm, termed star-based learning correlation clustering (SL-CC). The proposed SL-CC contains two phases. The first is a scale reduction for signed graphs. We propose a special motif, called a star structure, for reducing the scale of signed graphs. We assign the vertices within a star structure to have the same cluster label and then merge these vertices as a new vertex in the graph so we can shrink a large-scale graph to a much small-scale one. The second is to give a learning schema for the local search on the reduced graphs. It can discover some important stars as seeds of clusters according to the graph structure, and then justify whether the other stars need to be merged with seeds or not. We also construct a new integer linear programing (ILP) model based on cycle inequalities to perform the local search with final clustering results. The experiments and comparisons of the proposed SL-CC with some existing CC methods on synthetic and real data sets with variant scale structures of signed graphs demonstrate the efficiency and usefulness of the SL-CC algorithm.","Correlation clustering, Graphs, Integer linear program (ILP), Star-based learning correlation clustering (SL-CC), Signed network",Jialin Hua and Jian Yu and Miin-Shen Yang,https://www.sciencedirect.com/science/article/pii/S0031320321001539,https://doi.org/10.1016/j.patcog.2021.107966,0031-3203,2021,107966,116,Pattern Recognition,Star-based learning correlation clustering,article,HUA2021107966,
"In this paper, we propose a novel Low-cost U-Net (LCU-Net) for the Environmental Microorganism (EM) image segmentation task to assist microbiologists in detecting and identifying EMs more effectively. The LCU-Net is an improved Convolutional Neural Network (CNN) based on U-Net, Inception, and concatenate operations. It addresses the limitation of single receptive field setting and the relatively high memory cost of U-Net. Experimental results show the effectiveness and potential of the proposed LCU-Net in the practical EM image segmentation field.","Environmental miroorganisms, Image segmentation, Deep convolutional neural networks, Low-cost",Jinghua Zhang and Chen Li and Sergey Kosov and Marcin Grzegorzek and Kimiaki Shirahama and Tao Jiang and Changhao Sun and Zihan Li and Hong Li,https://www.sciencedirect.com/science/article/pii/S0031320321000728,https://doi.org/10.1016/j.patcog.2021.107885,0031-3203,2021,107885,115,Pattern Recognition,LCU-Net: A novel low-cost U-Net for environmental microorganism image segmentation,article,ZHANG2021107885,
"We propose a random Fourier sampling scheme to enhance the accuracy of the high frequency pattern estimation for image reconstruction. This method is designed to work in a constrained â1 minimization based on the Fourier-Haar interplay revealing a column-wise maximum coherent structure that we provide. Essential in the scheme is to generate a data-driven density function by a small percentage of Fourier samples. The density function governs a random sampling procedure to acquire high frequency information, resulting in better reconstruction of the Haar wavelet coefficients. We also discuss a few examples of exact recovery of the Haar wavelet coefficients from which the proposed sampling scheme has emerged. Numerical experiments confirm superiority of the proposed sampling scheme to other conventional sampling schemes in the â1 framework.","Image reconstruction, High magnitude Fourier samples, Variable density random sampling, Constrained  minimization",Joo Dong Yun and Yunho Kim,https://www.sciencedirect.com/science/article/pii/S0031320321001771,https://doi.org/10.1016/j.patcog.2021.107990,0031-3203,2021,107990,117,Pattern Recognition,Two-stage adaptive random Fourier sampling method for image reconstruction,article,YUN2021107990,
"Person retrieval is an important vision task, aiming at matching the images of the same person under various camera views. The key challenge of person retrieval lies in the large intra-class variations among the person images. Therefore, how to learn discriminative feature representations becomes the core problem. In this paper, we propose a deep part-aware representation learning method for person retrieval. First, an improved triplet loss is introduced such that the global feature representations from the same identity are closely clustered. Meanwhile, a localization branch is proposed to automatically localize those discriminative person-wise parts or regions, only using identity labels in a weakly supervised manner. Via the learning simultaneously guided by the global branch and the localization branch, the proposed method can further improve the performance for person retrieval. Through an extensive set of ablation studies, we verify that the localization branch and the improved triplet loss each contributes to the performance boosts of the proposed method. Our model obtains superior (or comparable) performance compared to state-of-the-art methods for person retrieval on the four public person retrieval datasets. On the CUHK03-labeled dataset, for instance, the performance increases from 73.0% mAP and 77.9% rank-1 accuracy to 80.8% (+7.8%) mAP and 83.9% (+6.0%) rank-1 accuracy.","Person retrieval, Part-aware embedding, Improved triplet loss",Yang Zhao and Chunhua Shen and Xiaohan Yu and Hao Chen and Yongsheng Gao and Shengwu Xiong,https://www.sciencedirect.com/science/article/pii/S0031320321001254,https://doi.org/10.1016/j.patcog.2021.107938,0031-3203,2021,107938,116,Pattern Recognition,Learning deep part-aware embedding for person retrieval,article,ZHAO2021107938,
"Many video understanding tasks work in the offline setting by assuming that the input video is given from the start to the end. However, many real-world problems require the online setting, making a decision immediately using only the current and the past frames of videos such as in autonomous driving and surveillance systems. In this paper, we present a novel solution for online action detection by using a simple yet effective RNN-based networks called the Future Anticipation and Temporally Smoothing network (FATSnet). The proposed network consists of a module for anticipating the future that can be trained in an unsupervised manner with the cycle-consistency loss, and another component for aggregating the past and the future for temporally smooth frame-by-frame predictions. We also propose a solution to relieve the performance loss when running RNN-based models on very long sequences. Evaluations on TVSeries, THUMOSâ14, and BBDB show that our method achieve the state-of-the-art performances compared to the previous works on online action detection.","Online action detection, Cycle-consistency, Temporal smoothing, Video understanding",Young Hwi Kim and Seonghyeon Nam and Seon Joo Kim,https://www.sciencedirect.com/science/article/pii/S0031320321001412,https://doi.org/10.1016/j.patcog.2021.107954,0031-3203,2021,107954,116,Pattern Recognition,Temporally smooth online action detection using cycle-consistent future anticipation,article,KIM2021107954,
"In this paper, we propose deep learning frameworks based on the randomized neural network. Inspired by the principles of Random Vector Functional Link (RVFL) network, we present a deep RVFL network (dRVFL) with stacked layers. The parameters of the hidden layers of the dRVFL are randomly generated within a suitable range and kept fixed while the output weights are computed using the closed-form solution as in a standard RVFL network. We also propose an ensemble deep network (edRVFL) that can be regarded as a marriage of ensemble learning with deep learning. Unlike traditional ensembling approaches that require training several models independently from scratch, edRVFL is obtained by training a single dRVFL network once. Both dRVFL and edRVFL frameworks are generic and can be used with any RVFL variant. To illustrate this, we integrate the deep learning RVFL networks with a recently proposed sparse pre-trained RVFL (SP-RVFL). Experiments on 46 tabular UCI classification datasets and 12 sparse datasets demonstrate that the proposed deep RVFL networks outperform state-of-the-art deep feed-forward neural networks (FNNs).","Random Vector Functional Link (RVFL), Deep RVFL, Multi-layer RVFL, Ensemble deep learning, Randomized neural network, Extreme learning machine (ELM)",Qiushi Shi and Rakesh Katuwal and P.N. Suganthan and M. Tanveer,https://www.sciencedirect.com/science/article/pii/S0031320321001655,https://doi.org/10.1016/j.patcog.2021.107978,0031-3203,2021,107978,117,Pattern Recognition,Random vector functional link neural network based ensemble deep learning,article,SHI2021107978,
"Low-level features and deep features each have their own advantages and disadvantages in image representation. However, combining their advantages within a CBIR framework remains challenging. To address this problem, we propose a novel image-retrieval method: the deep-seated features histogram (DSFH). Its main highlights are: 1) Low-level features are extracted by simulating the human orientation selection and color perception mechanisms. This follows the human habit of looking at conspicuous regions and then less-conspicuous ones. 2) A novel method, ranking whitening, is proposed for extracting deep features via low-level features and combining them to obtain deep-seated features. 3) The proposed method is straightforward and reduces the vector dimensionality of the FC7 layer of a pre-trained VGG-16 network, and significantly improves image-retrieval precision. Comparative experiments demonstrate that the proposed method outperforms several state-of-the-art methods, including low-level feature-based, deep feature-based, and fused feature-based methods, in terms of precision/recall, area under the precision/recall curve metrics, and mean average precision. The proposed method provides efficient CBIR performance and not only has the power to discriminate low-level features, including color, texture, and shape, but can also match scenes of similar style.","Image retrieval, VGG-16 network, orientation selection, color perception, deep-seated features histogram",Guang-Hai Liu and Jing-Yu Yang,https://www.sciencedirect.com/science/article/pii/S0031320321001138,https://doi.org/10.1016/j.patcog.2021.107926,0031-3203,2021,107926,116,Pattern Recognition,Deep-seated features histogram: A novel image retrieval method,article,LIU2021107926,
"The objective of the present study is to design a correlation filter-based tracking method for robust visual object tracking. In the literature, numerous tracking methods have been proposed based on discriminative correlation filter (DCF) and obtained impressive performance. However, existing algorithms still face difficulties such as partial occlusion, clutter background, uncertainties, boundary effects (especially when the target search area is small) and other challenging visual factors. Furthermore, during the target detection process, the sudden changes in objects caused by illumination variations and partial/full occlusion degrade the performance. To tackle the drawbacks mentioned earlier, we propose a tracking algorithm concerning the aberrance suppressed correlation filters with spatio-temporal information for visual tracking. Specifically, we introduce a spatial regularization term into the correlation filter to suppresses the boundary effects. Following that, a temporal regularization is adopted into the DCF-based framework to achieve a more robust appearance model and further enhance the tracking performance. In addition, we introduce an approach to suppress the aberrance in response maps caused by the sudden changes. Technically, our proposed method can be directly solved by using the alternating direction method of multipliers (ADMM) technique with a low computational cost. Finally, extensive experimental results on OTB2013, OTB2015, TempleColor128 and UAV123 datasets demonstrate that the proposed method performs favorably against state-of-the-art methods.","Visual object tracking, Correlation filter, Spatio-temporal information, Radical changes",Dinesh Elayaperumal and Young Hoon Joo,https://www.sciencedirect.com/science/article/pii/S0031320321001096,https://doi.org/10.1016/j.patcog.2021.107922,0031-3203,2021,107922,115,Pattern Recognition,Aberrance suppressed spatio-temporal correlation filters for visual object tracking,article,ELAYAPERUMAL2021107922,
"As advanced facial manipulation technologies develop rapidly, one can easily modify an image by changing the identity or the facial expression of the target person, which threatens social security. To address this problem, face forgery detection becomes an important and challenging task. In this paper, we propose a novel network, called Pixel-Region Relation Network (PRRNet), to capture pixel-wise and region-wise relations respectively for face forgery detection. The main motivation is that a facial manipulated image is composed of two parts from different sources, and the inconsistencies between the two parts is a significant kind of evidence for manipulation detection. Specifically, PRRNet contains two serial relation modules, i.e. the Pixel-Wise Relation (PR) module and the Region-Wise Relation (RR) module. For each pixel in the feature map, the PR module captures its similarities with other pixels to exploit the local relations information. Then, the PR module employs a spatial attention mechanism to represent the manipulated region and the original region separately. With the representations of the two regions, the RR module compares them with multiple metrics to measure the inconsistency between these two regions. In particular, the final predictions are obtained totally based on whether the inconsistencies exist. PRRNet achieves the state-of-the-art detection performance on three recent proposed face forgery detection datasets. Besides, our PRRNet shows the robustness when trained and tested on different image qualities.","Face forgery detection, Forgery localization, Inconsistency detection, Relation learning",Zhihua Shang and Hongtao Xie and Zhengjun Zha and Lingyun Yu and Yan Li and Yongdong Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321001370,https://doi.org/10.1016/j.patcog.2021.107950,0031-3203,2021,107950,116,Pattern Recognition,PRRNet: Pixel-Region relation network for face forgery detection,article,SHANG2021107950,
"Computer-aided diagnosis has been extensively investigated for more rapid and accurate screening during the outbreak of COVID-19 epidemic. However, the challenge remains to distinguish COVID-19 in the complex scenario of multi-type pneumonia classification and improve the overall diagnostic performance. In this paper, we propose a novel periphery-aware COVID-19 diagnosis approach with contrastive representation enhancement to identify COVID-19 from influenza-A (H1N1) viral pneumonia, community acquired pneumonia (CAP), and healthy subjects using chest CT images. Our key contributions include: 1) an unsupervised Periphery-aware Spatial Prediction (PSP) task which is designed to introduce important spatial patterns into deep networks; 2) an adaptive Contrastive Representation Enhancement (CRE) mechanism which can effectively capture the intra-class similarity and inter-class difference of various types of pneumonia. We integrate PSP and CRE to obtain the representations which are highly discriminative in COVID-19 screening. We evaluate our approach comprehensively on our constructed large-scale dataset and two public datasets. Extensive experiments on both volume-level and slice-level CT images demonstrate the effectiveness of our proposed approach with PSP and CRE for COVID-19 diagnosis.","Automated COVID-19 diagnosis, Chest CT images, Periphery-aware spatial prediction (PSP), Contrastive representation enhancement (CRE)",Junlin Hou and Jilan Xu and Longquan Jiang and Shanshan Du and Rui Feng and Yuejie Zhang and Fei Shan and Xiangyang Xue,https://www.sciencedirect.com/science/article/pii/S0031320321001928,https://doi.org/10.1016/j.patcog.2021.108005,0031-3203,2021,108005,118,Pattern Recognition,Periphery-aware COVID-19 diagnosis with contrastive representation enhancement,article,HOU2021108005,
"Many warping methods for image stitching have been proposed to construct panoramic image mosaics free of artifacts. Existing methods heavily rely on coordinate correspondences between keypoints in stitching, which may not provide adequate constraints for alignment. In this paper, we discover and employ a new constraint â angle correspondences to address the above problem. The angle of a feature point represents the local directional structure of the point, which is an extension to its position and customarily ignored in image stitching. We propose to jointly consider the coordinates as well as the angles in keypoint correspondences. Such a strategy helps to generate a correct warping in the overlapping regions of the stitched image. In addition, we propose a novel constraint â mesh angle preservation to prevent undesired distortion in non-overlapping areas. Experiments in several challenging cases demonstrate that our method yields more accurate results with significantly less artifacts in comparison with state-of-the-art methods.","Image stitching, Image alignment, Homography estimation, Angle features",Yinqi Chen and Huicheng Zheng and Yiyan Ma and Zhiwei Yan,https://www.sciencedirect.com/science/article/pii/S0031320321001801,https://doi.org/10.1016/j.patcog.2021.107993,0031-3203,2021,107993,117,Pattern Recognition,Image stitching based on angle-consistent warping,article,CHEN2021107993,
"The time series cluster kernel (TCK) provides a powerful tool for analysing multivariate time series subject to missing data. TCK is designed using an ensemble learning approach in which Bayesian mixture models form the base models. Because of the Bayesian approach, TCK can naturally deal with missing values without resorting to imputation and the ensemble strategy ensures robustness to hyperparameters, making it particularly well suited for unsupervised learning. However, TCK assumes missing at random and that the underlying missingness mechanism is ignorable, i.e. uninformative, an assumption that does not hold in many real-world applications, such as e.g. medicine. To overcome this limitation, we present a kernel capable of exploiting the potentially rich information in the missing values and patterns, as well as the information from the observed data. In our approach, we create a representation of the missing pattern, which is incorporated into mixed mode mixture models in such a way that the information provided by the missing patterns is effectively exploited. Moreover, we also propose a semi-supervised kernel, capable of taking advantage of incomplete label information to learn more accurate similarities. Experiments on benchmark data, as well as a real-world case study of patients described by longitudinal electronic health record data who potentially suffer from hospital-acquired infections, demonstrate the effectiveness of the proposed methods.","Multivariate time series, Kernel methods, Missing data, Informative missingness, Semi-supervised learning",Karl {Ãyvind Mikalsen} and Cristina Soguero-Ruiz and Filippo {Maria Bianchi} and Arthur Revhaug and Robert Jenssen,https://www.sciencedirect.com/science/article/pii/S0031320321000832,https://doi.org/10.1016/j.patcog.2021.107896,0031-3203,2021,107896,115,Pattern Recognition,Time series cluster kernels to exploit informative missingness and incomplete label information,article,OYVINDMIKALSEN2021107896,
"This work addresses the challenging problem of unconstrained 3D hand pose estimation using monocular RGB images. Most of the existing approaches assume some prior knowledge of hand (such as hand locations and side information) is available for 3D hand pose estimation. This restricts their use in unconstrained environments. Therefore, we present an end-to-end framework that robustly predicts hand prior information and accurately infers 3D hand pose by learning ConvNet models while only using keypoint annotations. To enhance the hand detectorâs robustness, we propose a novel keypoint-based method to simultaneously predict hand regions and side labels, unlike existing methods that suffer from background color confusion caused by using segmentation or detection-based technology. Moreover, inspired by the human handâs biological structure, we introduce two geometric constraints directly into the 3D coordinates prediction that further improves its performance. Experimental results show that our proposed framework outperforms the state-of-art methods on standard benchmark datasets while providing robust predictions.","Hand detection, Hand tracking, Hand pose estimation, Computer vision, Deep learning",Sanjeev Sharma and Shaoli Huang,https://www.sciencedirect.com/science/article/pii/S0031320321000790,https://doi.org/10.1016/j.patcog.2021.107892,0031-3203,2021,107892,115,Pattern Recognition,An end-to-end framework for unconstrained monocular 3D hand pose estimation,article,SHARMA2021107892,
"Person re-identification (re-ID) methods need to extract representative, rich and discriminative features in order to deal with the effect of imperfect pedestrian detectors, illumination changes, occlusions, and background confusion. In this paper, a multi-level-attention embedding and multi-layer-feature fusion (MEMF) model is proposed for person re-ID. Specifically, a novel backbone network is designed, in which multi-level-attention blocks are embedded into a multi-layer-feature fusion architecture. Multi-level-attention blocks can highlight representative features and assist global feature expression, and multi-layer-feature fusion can increase the fine granularity of feature expression and obtain richer features. Besides, a new eigenvalue difference orthogonality (EDO) loss is designed to reduce the correlation between features. The final loss is defined as the combination of the cross-entropy loss and the EDO loss, which improves re-ID results. The proposed method is evaluated on four popular and challenging datasets. Detailed experiments demonstrate that the application of various elements of the MEMF model can help improve person re-ID performance. Compared with start-of-the-art methods, the MEMF model gets a promising result.","Person re-identification, Feature expression, Convolutional neural network",Jia Sun and Yanfeng Li and Houjin Chen and Bin Zhang and Jinlei Zhu,https://www.sciencedirect.com/science/article/pii/S0031320321001242,https://doi.org/10.1016/j.patcog.2021.107937,0031-3203,2021,107937,116,Pattern Recognition,MEMF: Multi-level-attention embedding and multi-layer-feature fusion model for person re-identification,article,SUN2021107937,
"The problem of inhomogeneous cluster densities has been a long-standing issue for distance-based and density-based algorithms in clustering and anomaly detection. These algorithms implicitly assume that all clusters have approximately the same density. As a result, they often exhibit a bias towards dense clusters in the presence of sparse clusters. Many remedies have been suggested; yet, we show that they are partial solutions which do not address the issue satisfactorily. To match the implicit assumption, we propose to transform a given dataset such that the transformed clusters have approximately the same density while all regions of locally low density become globally low densityâhomogenising cluster density while preserving the cluster structure of the dataset. We show that this can be achieved by using a new multi-dimensional Cumulative Distribution Function in a transform-and-shift method. The method can be applied to every dataset, before the dataset is used in many existing algorithms to match their implicit assumption without algorithmic modification. We show that the proposed method performs better than existing remedies.","Density-ratio, Density-based clustering, NN Anomaly detection, Inhomogeneous cluster densities, Scaling, Shift",Ye Zhu and Kai Ming Ting and Mark J. Carman and Maia Angelova,https://www.sciencedirect.com/science/article/pii/S0031320321001643,https://doi.org/10.1016/j.patcog.2021.107977,0031-3203,2021,107977,117,Pattern Recognition,CDF Transform-and-Shift: An effective way to deal with datasets of inhomogeneous cluster densities,article,ZHU2021107977,
"Motion information used in the existed video action recognition schemes is mixing of global motion(GM) and local motion(LM). In fact, GM & LM have their respective semantic concepts. Thus, it is promising to decouple GM and LM from the mixed motions. Numerous efforts have been made on the design of global motion models for video encoding, video dejittering, video denoising, and so on. Nevertheless, some of the models are too basic to cover the camera motions in action recognition while others are over-complicated. In this paper, we focus on the characteristic of the action recognition and propose a novel independent univariate GM model. It ignores camera rotation, which appears rarely in action recognition videos, and represents the GM in x and y direction respectively. Furthermore, GM is position invariant because it is from the universal camera motion. Pixels with global motions are subjected to the same parametric model and pixels with mixed motion can be seen as outliers. Motivated by this, we develop an iterative optimization scheme for GM estimation which removes the outlier points step by step and estimates global motions in a coarse-to-fine manner. Finally, the LM is estimated through a Spatio-temporal threshold-based method. Experimental results demonstrate that the proposed GM model makes a better trade-off between the model complexity and the robustness. And the iterative optimization scheme is more effective than the existed algorithms. The compared experiments using four popular action recognition models on UCF-101 (for action recognition) and NCAA (for group activity recognition) demonstrate that local motions are more effective than the mixed motions.","Global motion estimation, Iterative optimization, Independent univariate global motion model, Action recognition",Lifang Wu and Zhou Yang and Meng Jian and Jialie Shen and Yuchen Yang and Xianglong Lang,https://www.sciencedirect.com/science/article/pii/S0031320321001126,https://doi.org/10.1016/j.patcog.2021.107925,0031-3203,2021,107925,116,Pattern Recognition,Global motion estimation with iterative optimization-based independent univariate model for action recognition,article,WU2021107925,
"Typical deep-neural-network (DNN) based generative image models often (i)Â show limited ability to learn a disentangled latent representation, (ii)Â show limited controllability leading to undesirable side effects when manipulating selected attributes during image generation, and (iii)Â require large attribute-annotated training sets. We propose a generative DNN model for face images by explicitly disentangling geometry and appearance modeling to achieve selective controllability of the desired attributes with less side effects. To learn geometric variability, we leverage grayscale sketch representations to learn (i)Â a deformable mean template representing the population-mean face geometry and (ii)Â a generative model of deformations to model individual face-geometry variations, using dense image registration. We learn the appearance variability in a (color-image) space that we explicitly design by factoring out the geometric variability. We propose a variational formulation to enable semi-supervised learning when manually-annotated attributes are severely limited in the training set. Results on large datasets show that, compared to schemes using deformation models or variational learning, our method significantly improves face-image model fits and facial-feature controllability even with semi-supervised learning.","Deep learning, variational, semi-supervised, controllable generative image model, disentangled geometry and appearance, deformable template",Krishna Wadhwani and Suyash P. Awate,https://www.sciencedirect.com/science/article/pii/S0031320321001886,https://doi.org/10.1016/j.patcog.2021.108001,0031-3203,2021,108001,118,Pattern Recognition,Controllable Image Generation with Semi-supervised Deep Learning and Deformable-Mean-Template Based Geometry-Appearance Disentanglement,article,WADHWANI2021108001,
"Weakly supervised object localization (WSOL) methods utilize the internal feature responses of a classifier trained only on image-level labels. Classifiers tend to focus on the most discriminative part of the target object, instead of considering its full extent. Adversarial erasing (AE) techniques have been proposed to ameliorate this problem. These techniques erase the most discriminative part during training, thereby encouraging the classifiers to learn the less discriminative parts of the object. Despite the success of AE-based methods, we have observed that the hyperparameters fail to generalize across model architectures and datasets. Therefore, new sets of hyperparameters must be determined for each architecture and dataset. The selection of hyperparameters frequently requires strong supervision (e.g., pixel-level annotations or human inspection). Because WSOL is premised on the assumption that such strong supervision is absent, the applicability of AE-based methods is limited. In this paper, we propose the region-based dropout with attention prior (RDAP) algorithm, which features hyperparameter transferability. We combined AE with regional dropout algorithms that provide greater stability against the selection of hyperparameters. We empirically confirmed that the RDAP method achieved state-of-the-art localization accuracy on four architectures, namely VGG-GAP, InceptionV3, ResNet-50 SE, and PreResNet-18, and two datasets, namely CUB-200-2011 and ImageNet-1k, with a single set of hyperparameters.","Deep learning, Object localization, Weakly supervised learning, Region-based dropout, Attention prior",Junsuk Choe and Dongyoon Han and Sangdoo Yun and Jung-Woo Ha and Seong Joon Oh and Hyunjung Shim,https://www.sciencedirect.com/science/article/pii/S0031320321001369,https://doi.org/10.1016/j.patcog.2021.107949,0031-3203,2021,107949,116,Pattern Recognition,Region-based dropout with attention prior for weakly supervised object localization,article,CHOE2021107949,
"Most Zero-Shot Action Recognition (ZSAR) methods establish visual-semantic joint embedding space, which is based on commonly used visual features and semantic embeddings, to learn the correlation between actions. Nevertheless, extracting visual features without structural guidance would lead to sparse video features, which reflect the correlation of actions, fall into oblivion. Based on the Ventral & Dorsal Stream Theory (VD), we propose a VD-ZSAR method to extract irredundant visual feature, which can relieve relation ambiguity caused by redundant visual feature. And a visual-semantic joint embedding space is learned by combining nonredundant visual space with semantic space. Specifically, visual space is constructed by the motion cues perceived by Dorsal Stream, and the object cues perceived by Ventral Stream. Semantic space is constructed by sentence-to-vector generator. The visual-semantic joint embedding space is built by a nonlinear similarity metric learning mechanism, which can better implicitly reflect the correlation between actions. Extensive experiments on the Olympic, HDMB51 and UCF101 datasets validate the favorable performance of our proposed approach.","VD-ZSAR, Ventral & Dorsal Stream Theory, Nonlinear similarity metric learning mechanism",Meng Xing and Zhiyong Feng and Yong Su and Weilong Peng and Jianhai Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321001400,https://doi.org/10.1016/j.patcog.2021.107953,0031-3203,2021,107953,116,Pattern Recognition,Ventral & Dorsal Stream Theory based Zero-Shot Action Recognition,article,XING2021107953,
"Graph Convolutional Network (GCN) has already been successfully applied to skeleton-based action recognition. However, current GCNs in this task are lack of pooling operations such that the architectures are inherently flat, which not only increases the computational complexity but also requires larger memory space to keep the entire graph embedding. More seriously, a flat architecture forces the high-level semantic feature representations to have the same physical structure of the low-level input skeletons, which we argue is unreasonable and harmful for the final performance. To address these issues, we propose Tripool, a novel graph pooling method for 3D action recognition from skeleton data. Tripool provides to optimize a triplet pooling loss, in which both graph topology and global graph context are taken into consideration, to learn a hierarchical graph representation. The training process of graph pooling is efficient since it optimizes the graph topology by minimizing an upper bound of the pooling loss. Besides, Tripool also automatically generates an embedding matrix since the graph is changed after pooling. On one hand, Tripool reduces the computational cost by removing the redundant nodes. On the other hand it overcomes the limitation of the topology constrain for the high-level semantic representations, thus improves the final performance. Tripool can be combined with various graph neural networks in an end-to-end fashion. Comprehensive experiments on two current largest scale 3D datasets are conducted to evaluate our method. With our Tripool, we consistently get the best results in terms of various performance measures.","3D skeletal action recognition, ST-GCN, Graph pooling, Graph topology analysis",Wei Peng and Xiaopeng Hong and Guoying Zhao,https://www.sciencedirect.com/science/article/pii/S0031320321001084,https://doi.org/10.1016/j.patcog.2021.107921,0031-3203,2021,107921,115,Pattern Recognition,Tripool: Graph triplet pooling for 3D skeleton-based action recognition,article,PENG2021107921,
"Spectral clustering (SC) is a popular approach for gaining insights from complex networks. Conventional SC focuses on second-order structures (e.g. edges) without direct consideration of higher-order structures (e.g. triangles). This has motivated SC extensions that directly consider higher-order structures. However, both approaches are limited to considering a single order. To address this issue, this paper proposes a novel Mixed-Order Spectral Clustering (MOSC) framework to model both second-order and third-order structures simultaneously. To model mixed-order structures, we propose two new methods based on Graph Laplacian (GL) and Random Walks (RW). MOSC-GL combines edge and triangle adjacency matrices, with theoretical performance guarantee. MOSC-RW combines first-order and second-order random walks for a probabilistic interpretation. Moreover, we design mixed-order cut criteria to enable existing SC methods to preserve mixed-order structures, and develop new mixed-order evaluation metrics for structure-level evaluation. Experiments on community detection and superpixel segmentation show (1) the superior performance of the MOSC methods over existing SC methods, (2) enhanced performance of conventional SC due to mixed-order cut criteria, and (3) new insights of output clusters offered by the mixed-order evaluation metrics.","Spectral clustering, Higher-order structures, Mixed-order structures",Yan Ge and Pan Peng and Haiping Lu,https://www.sciencedirect.com/science/article/pii/S0031320321001515,https://doi.org/10.1016/j.patcog.2021.107964,0031-3203,2021,107964,117,Pattern Recognition,Mixed-order spectral clustering for complex networks,article,GE2021107964,
"Feature selection is an important procedure in machine learning because it can reduce the complexity of the final learning model and simplify the interpretation. In this paper, we propose a novel non-linear feature selection method that targets multi-class classification problems in the framework of support vector machines. The proposed method is achieved using a kernelized multi-class support vector machine with a fast version of recursive feature elimination. The proposed method selects features that work well for all classes, as the involved classifier simultaneously constructs multiple decision functions that separates each class from the others. We formulate the classifier as a large optimisation problem, and iteratively solve one decision function at a time, leading to a lower computational time complexity than when solving the large optimisation problem directly. The coefficients of the classifier are then used as a ranking criterion in the accelerated recursive feature elimination by adding batch elimination and a rechecking process. Experimental results on several datasets demonstrate the superior performance of the proposed feature selection method.","Feature selection, Multi-class support vector machine, Kernel machine, Recursive feature elimination",Yinan Guo and Zirui Zhang and Fengzhen Tang,https://www.sciencedirect.com/science/article/pii/S0031320321001758,https://doi.org/10.1016/j.patcog.2021.107988,0031-3203,2021,107988,117,Pattern Recognition,Feature selection with kernelized multi-class support vector machine,article,GUO2021107988,
"Dynamic Time Warping (DTW) is a popular similarity measure for aligning and comparing time series. Due to DTWâs high computation time, lower bounds are often employed to screen poor matches. Many alternative lower bounds have been proposed, providing a range of different trade-offs between tightness and computational efficiency. LB_KEOGH provides a useful trade-off in many applications. Two recent lower bounds, LB_IMPROVED and LB_ENHANCED, are substantially tighter than LB_KEOGH. All three have the same worst case computational complexityâlinear with respect to series length and constant with respect to window size. We present four new DTW lower bounds in the same complexity class. LB_PETITJEAN is substantially tighter than LB_IMPROVED, with only modest additional computational overhead. LB_WEBB is more efficient than LB_IMPROVED, while often providing a tighter bound. LB_WEBB is always tighter than LB_KEOGH. The parameter free LB_WEBB is usually tighter than LB_ENHANCED. A parameterized variant, LB_Webb_Enhanced, is always tighter than LB_ENHANCED. A further variant, LB_WEBB*, is useful for some constrained distance functions. In extensive experiments, LB_WEBB proves to be very effective for nearest neighbor search.","Dynamic time warping, Lower bound, Time series",Geoffrey I. Webb and FranÃ§ois Petitjean,https://www.sciencedirect.com/science/article/pii/S0031320321000820,https://doi.org/10.1016/j.patcog.2021.107895,0031-3203,2021,107895,115,Pattern Recognition,Tight lower bounds for dynamic time warping,article,WEBB2021107895,
"Most of the strategies for boundary image evaluation involve the comparison of computer-generated images with ground truth solutions. While this can be done in different manners, recent years have seen a dominance of techniques based on the use of confusion matrices. That is, techniques that, at the evaluation stage, interpret boundary detection as a classification problem. These techniques require a correspondence between the boundary pixels in the candidate image and those in the ground truth; that correspondence is further used to create the confusion matrix, from which evaluation statistics can be computed. The correspondence between boundary images faces different challenges, mainly related to the matching of potentially displaced boundaries. Interestingly, boundary image comparison relates to many other fields of study in literature, from object tracking to biometrical identification. In this work, we survey all existing strategies for boundary matching, we propose a taxonomy to embrace them all, and perform a usability-driven quantitative analysis of their behaviour.","Boundary image, Boundary evaluation, Linear feature matching, Image comparison",C. Lopez-Molina and C. Marco-Detchart and H. Bustince and B. {De Baets},https://www.sciencedirect.com/science/article/pii/S0031320321000704,https://doi.org/10.1016/j.patcog.2021.107883,0031-3203,2021,107883,115,Pattern Recognition,A survey on matching strategies for boundary image comparison and evaluation,article,LOPEZMOLINA2021107883,
"In this paper, a neural network-based color edge detector is constructed by learning a classifier using anisotropic directional derivative (ANDD) matrices of a color image as input. The training stage on a color edge dataset with ground truth (GT) edges includes calculation of ANDD matrices, generation of feature matrices, and training a classifier. For each training image, a set of ANDD matrices are calculated from the ANDDs with different parameter setups for training and from which a set of the color edge strength maps (CESMs) are extracted by the singular vector decomposition. The CESMs and the GTs on edges of the image are combined into a feature matrix for training. Using the feature matrices of all the training images as input, a classification neural network is trained and it outputs the probability of a pixel to be an edge pixel. In the detection stage, for a color image, its ANDD matrices, CESMs, and the color edge direction maps (CEDMs) are first computed and then the CESMs are input into the classification neural network to obtain the edge probability map (EPM) of the image. Finally, the non-maximum suppression and hysteresis thresholding are applied to the EPM and CEDMs to generate the binary edge map. The proposed detector attains better performance than the existing gradient-based detectors and is competitive with learning-based detectors on three commonly-used color image datasets for edge and contour detection.","Anisotropic directional derivative matrices, Color edge detector, Learning-based detector, Ground truth edges, Edge probability map",Ou Li and Peng-Lang Shui,https://www.sciencedirect.com/science/article/pii/S0031320321001916,https://doi.org/10.1016/j.patcog.2021.108004,0031-3203,2021,108004,118,Pattern Recognition,Color edge detection by learning classification network with anisotropic directional derivative matrices,article,LI2021108004,
"This paper presents an end-to-end neural network system to identify writers through handwritten word images, which jointly integrates global-context information and a sequence of local fragment-based features. The global-context information is extracted from the tail of the neural network by a global average pooling step. The sequence of local and fragment-based features is extracted from a low-level deep feature map which contains subtle information about the handwriting style. The spatial relationship between the sequence of fragments is modeled by the recurrent neural network (RNN) to strengthen the discriminative ability of the local fragment features. We leverage the complementary information between the global-context and local fragments, resulting in the proposed global-context residual recurrent neural network (GR-RNN) method. The proposed method is evaluated on four public data sets and experimental results demonstrate that it can provide state-of-the-art performance. In addition, the neural networks trained on gray-scale images provide better results than neural networks trained on binarized and contour images, indicating that texture information plays an important role for writer identification. The source code is available: https://github.com/shengfly/writer-identification.","Writer identification, Recurrent neural network, Residual network, Local and global features",Sheng He and Lambert Schomaker,https://www.sciencedirect.com/science/article/pii/S003132032100162X,https://doi.org/10.1016/j.patcog.2021.107975,0031-3203,2021,107975,117,Pattern Recognition,GR-RNN: Global-context residual recurrent neural networks for writer identification,article,HE2021107975,
"Unsupervised domain adaptation aims to learn a task classifier that performs well on the unlabeled target domain, by utilizing the labeled source domain. Inspiring results have been acquired by learning domain-invariant deep features via domain-adversarial training. However, its parallel design of task and domain classifiers limits the ability to achieve a finer category-level domain alignment. To promote categorical domain adaptation (CatDA), based on a joint category-domain classifier, we propose novel losses of adversarial training at both domain and category levels. Since the joint classifier can be regarded as a concatenation of individual task classifiers respectively for the two domains, our design principle is to enforce consistency of category predictions between the two task classifiers. Moreover, we propose a concept of vicinal domains whose instances are produced by a convex combination of pairs of instances respectively from the two domains. Intuitively, alignment of the possibly infinite number of vicinal domains enhances that of original domains. We propose novel adversarial losses for vicinal domain adaptation (VicDA) based on CatDA, leading to Vicinal and Categorical Domain Adaptation (ViCatDA). We also propose Target Discriminative Structure Recovery (TDSR) to recover the intrinsic target discrimination damaged by adversarial feature alignment. We also analyze the principles underlying the ability of our key designs to align the joint distributions. Extensive experiments on several benchmark datasets demonstrate that we achieve the new state of the art.","Unsupervised domain adaptation, Categorical domain adaptation, Vicinal domain adaptation, Cross-domain weighting, Domain augmentation",Hui Tang and Kui Jia,https://www.sciencedirect.com/science/article/pii/S0031320321000947,https://doi.org/10.1016/j.patcog.2021.107907,0031-3203,2021,107907,115,Pattern Recognition,Vicinal and categorical domain adaptation,article,TANG2021107907,
"This paper deals with the problem of integrating the most suitable feature selection methods for a given problem in order to achieve the best feature order. A new, adaptive and hybrid feature selection approach is proposed, which combines and utilizes multiple individual methods in order to achieve a more generalized solution. Various state-of-the-art feature selection methods are presented in detail with examples of their applications and an exhaustive evaluation is conducted to measure and compare the their performance with the proposed approach. Results prove that while the individual feature selection methods may perform with high variety on the test cases, the combined algorithm steadily provides noticeably better solution.","Adaptive, Hybrid Feature Selection (AHFS), Combination of methods, Statistics, Information theory, Exhausting evaluation",Zsolt JÃ¡nos Viharos and KrisztiÃ¡n BalÃ¡zs Kis and ÃdÃ¡m Fodor and MÃ¡tÃ© IstvÃ¡n BÃ¼ki,https://www.sciencedirect.com/science/article/pii/S0031320321001199,https://doi.org/10.1016/j.patcog.2021.107932,0031-3203,2021,107932,116,Pattern Recognition,"Adaptive, Hybrid Feature Selection (AHFS)",article,VIHAROS2021107932,
"Heterogeneous information networks usually contain different kinds of nodes and distinguishing types of relations, which can preserve more information than homogeneous information networks. Heterogeneous network representation learning attempts to learn a low-dimensional representation for each node and capture rich semantic information of the given network. Most of existing surveys focus on heterogeneous information network analysis and homogeneous information network representation learning. Although considerable research efforts concentrate on heterogeneous network representation learning, there are few surveys that systematically review the state-of-the-art heterogeneous network representation learning techniques. Motivated by this, we propose a taxonomy of heterogeneous network representation learning algorithms according to different approaches of capturing semantic information in heterogeneous networks, including path based algorithms and semantic unit based algorithms. Moreover, we introduce the typical heterogeneous network representation learning techniques in detail and make a comparative analysis of these techniques. In addition, the research challenges in terms of semantics preserving, data sparsity and scalability are discussed. To tackle these challenges, several potential future research directions for heterogeneous network representation learning are pointed out, including semantic relations extraction, dynamic heterogeneous networks, very large heterogeneous networks and heterogeneous networks construction.","Heterogeneous network, Network representation learning, Machine learning",Yu Xie and Bin Yu and Shengze Lv and Chen Zhang and Guodong Wang and Maoguo Gong,https://www.sciencedirect.com/science/article/pii/S0031320321001230,https://doi.org/10.1016/j.patcog.2021.107936,0031-3203,2021,107936,116,Pattern Recognition,A survey on heterogeneous network representation learning,article,XIE2021107936,
"We propose a framework for a fast exact shape retrieval called Low-complexity Arrays of Contour Signatures. The purposes are to match a shape against a database in constant time and to retrieve correct shapes very close to the query, while the latter may have undergone rigid transformations and noise. We present a shape signature based on prior works as well as a compact characterization of such signatures, a system of associative arrays allowing a short search time for retrieval and a technique of pairwise alignment. This method shows a good resilience to perturbations and is performed in constant computational time.","Shape recognition, Contour signature, Fast retrieval, Associative arrays",Florian Lardeux and Sylvain Marchand and Petra Gomez-KrÃ¤mer,https://www.sciencedirect.com/science/article/pii/S0031320321001874,https://doi.org/10.1016/j.patcog.2021.108000,0031-3203,2021,108000,118,Pattern Recognition,Low-complexity arrays of contour signatures for exact shape retrieval,article,LARDEUX2021108000,
"This manuscript presents a new method for fitting ellipses to two-dimensional data using the confocal hyperbola approximation to the geometric distance of points to ellipses. The proposed method was evaluated and compared to established methods on simulated and real-world datasets. First, it was revealed that the confocal hyperbola distance considerably outperforms other distance approximations such as algebraic and Sampson. Next, the proposed ellipse fitting method was compared with five reliable and established methods proposed by Halir, Taubin, Kanatani, Ahn and Szpak. The performance of each method as a function of rotation, aspect ratio, noise, and arc-length were examined. It was observed that the proposed ellipse fitting method achieved almost identical results (and in some cases better) than the gold standard geometric method of Ahn and outperformed the remaining methods in all simulation experiments. Finally, the proposed method outperformed the considered ellipse fitting methods in estimating the geometric parameters of cylindrical mechanical pipes from point clouds. The results of the experiments show that the confocal hyperbola is an excellent approximation to the true geometric distance and produces reliable and accurate ellipse fitting in practical settings.","ellipse fitting, cylinder parameter estimation, point to ellipse distance approximation, ellipse detection from images, mechanical pipe detection from point clouds, construction quality assurance",Reza Maalek and Derek D. Lichti,https://www.sciencedirect.com/science/article/pii/S0031320321001357,https://doi.org/10.1016/j.patcog.2021.107948,0031-3203,2021,107948,116,Pattern Recognition,New confocal hyperbola-based ellipse fitting with applications to estimating parameters of mechanical pipes from point clouds,article,MAALEK2021107948,
"With the advantages of low storage cost and extremely fast retrieval speed, deep hashing methods have attracted much attention for image retrieval recently. However, large-scale face image retrieval with significant intra-class variations is still challenging. Neither existing pairwise/triplet labels-based nor softmax classification loss-based deep hashing works can generate compact and discriminative binary codes. Considering these issues, we propose a center-based framework integrating end-to-end hashing learning and class centers learning simultaneously. The framework minimizes the intra-class variance by clustering intra-class samples into a learnable class center. To strengthen inter-class separability, it additionally imposes a novel regularization term to enlarge the Hamming distance between pairwise class centers. Moreover, a simple yet effective regression matrix is introduced to encourage intra-class samples to generate the same binary codes, which further enhances the hashing codes compactness. Experiments on four large-scale datasets show the proposed method outperforms state-of-the-art baselines under various code lengths and commonly-used evaluation metrics.","Deep supervised hashing, Class centers, Face image retrieval, Convolutional neural networks",Ming Zhang and Xuefei Zhe and Shifeng Chen and Hong Yan,https://www.sciencedirect.com/science/article/pii/S0031320321001631,https://doi.org/10.1016/j.patcog.2021.107976,0031-3203,2021,107976,117,Pattern Recognition,Deep center-based dual-constrained hashing for discriminative face image retrieval,article,ZHANG2021107976,
"Weakly-supervised semantic segmentation, as a promising solution to alleviate the burden of collecting per-pixel annotations, aims to train a segmentation model from partial weak annotations. Scribble on the object is one of the commonly used weak annotations and has shown to be sufficient for learning a decent segmentation model. Despite being effective, scribble-based weakly-supervised learning methods often lead to imprecise segmentation on object boundaries. This is mainly because the scribble annotations usually locate inside the objects and the dataset lacks annotations close to the semantic boundaries. To alleviate this issue, this paper proposes a simple-but-effective solution,Â i.e.,Â BoundaryMix, which generates pseudo training image-annotation pairs from the original images to supplement the missing semantic boundaries. Specifically, given a prediction of segmentation, we cut off the regions around the estimated boundaries, which are error-prone and replace them with the contents from another image, which in effect creates new samples with less ambiguity around semantic boundaries. With training on scribbles and the on-the-fly generated pseudo annotations, the network acquires better prediction capability around the boundary region and thus improves the overall segmentation performance. By conducting experiments on PASCAL VOC 2012 dataset and POTSDAM dataset with only scribble annotations, we demonstrate the excellent performance of the proposed method and the almost closed gap between scribble-supervised and fully-supervised image segmentation.","Weakly-supervised segmentation, Scribble, Boundary mix",Wanxuan Lu and Dong Gong and Kun Fu and Xian Sun and Wenhui Diao and Lingqiao Liu,https://www.sciencedirect.com/science/article/pii/S0031320321001114,https://doi.org/10.1016/j.patcog.2021.107924,0031-3203,2021,107924,117,Pattern Recognition,Boundarymix: Generating pseudo-training images for improving segmentation with scribble annotations,article,LU2021107924,
"Skeleton-based human interaction recognition is a challenging task requiring all abilities to recognize spatial, temporal, and interactive features. These abilities rarely co-exist in existing methods. Graph convolutional network (GCN) based methods fail to extract interactive features. Traditional interaction recognition methods cannot effectively capture spatial features from skeletons. Toward this end, we propose a novel Dyadic Relational Graph Convolutional Network (DR-GCN) for interaction recognition. Specifically, we make four contributions: (i) we design a Relational Adjacency Matrix (RAM) that represents dynamic relational graphs. These graphs are constructed combining both geometric features and relative attention from the two skeleton sequences; (ii) we propose a Dyadic Relational Graph Convolution Block (DR-GCB) that extracts spatial-temporal interactive features; (iii) we stack the proposed DR-GCBs to build DR-GCN and integrate our methods with an advanced model. (iv) Our models achieve state-of-the-art results on SBU and significant improvements on the mutual action sub-datasets of NTU-RGB+D and NTU-RGB+D 120.","3D skeleton-based interaction recognition, Multi-scale graph convolution networks, Graph inference",Liping Zhu and Bohua Wan and Chengyang Li and Gangyi Tian and Yi Hou and Kun Yuan,https://www.sciencedirect.com/science/article/pii/S0031320321001072,https://doi.org/10.1016/j.patcog.2021.107920,0031-3203,2021,107920,115,Pattern Recognition,Dyadic relational graph convolutional networks for skeleton-based human interaction recognition,article,ZHU2021107920,
"Sparse representation and cooperative learning are two representative technologies in the field of multi-view spectral clustering. The former can effectively extract features of multiple views by the removal of redundant information contained in each view. The latter can incorporate the diversity of each view. However, traditional sparse representation and cooperative learning algorithms are inadequate in preserving the internal geometric features of data by manifold regularization. In fact, general approaches rarely consider the similarities between the internal graph structures of individual views. Moreover, to achieve the optimal global feature learning, we present a novel two-step multi-view spectral clustering strategy, which combines the proposed sparse representation by adaptive graph learning with adaptive weighted cooperative learning. In the first step, the proposed matrix factorization by manifold regularization can strengthen the sparse features clustering discrimination of samples of each view. Specifically, the synchronization optimization method by introducing adaptive graph learning can better retain its internal complete structure of each view. This ensures the structure correlation of views through the usage of the sparse matrix and the optimal graph similarity matrix. In the second step, the adaptive weighted cooperative learning is performed on each view to get a global optimized matrix. In order to ensure that the global matrix is associated with various view features, graph learning is also performed on the global matrix. Experiment results on several multi-view datasets and single-view datasets show that the proposed method significantly outperformed the state-of-the-art algorithms.","Multi-view clustering, Sparse representation (sr), Adaptive graph learning (agl), Adaptive weighted cooperative learning (awcl), Global Optimized Matrix",Junpeng Tan and Zhijing Yang and Yongqiang Cheng and Jielin Ye and Bing Wang and Qingyun Dai,https://www.sciencedirect.com/science/article/pii/S0031320321001746,https://doi.org/10.1016/j.patcog.2021.107987,0031-3203,2021,107987,117,Pattern Recognition,SRAGL-AWCL: A two-step multi-view clustering via sparse representation and adaptive weighted cooperative learning,article,TAN2021107987,
"Prototype-based few-shot learning methods are promising in that they are simple yet effective to handle any-shot problems, and many prototype associated works are raised since then. However, these traditional prototype-based methods generally use only one single prototype to represent a class, which essentially cannot effectively estimate the complicated distribution of a class. To tackle this problem, we propose a novel Local descriptor-based Multi-Prototype Network (LMPNet) in this paper, a well-designed framework that generates an embedding space with multiple prototypes. Specifically, the proposed LMPNet employs local descriptors to represent each image, which can capture more informative and subtler cues of an image than the normally adopted image-level features. Moreover, to alleviate the uncertainty introduced by the fixed construction (averaging over samples) of prototypes, we introduce a channel squeeze and spatial excitation (sSE) attention module to learn multiple local descriptor-based prototypes for each class through end-to-end learning. Extensive experiments on both few-shot and fine-grained few-shot image classification tasks have been conducted on various benchmark datasets, including miniImageNet, tieredImageNet, Stanford Dogs, Stanford Cars, and CUB-200-2010. The experimental results of our LMPNet on above datasets show tangibly learning performance improvements and distinguishable outcomes over the baseline models.","Few-shot learning, Image classification, Local descriptors, Multiple prototypes, End-to-end learning",Hongwei Huang and Zhangkai Wu and Wenbin Li and Jing Huo and Yang Gao,https://www.sciencedirect.com/science/article/pii/S0031320321001229,https://doi.org/10.1016/j.patcog.2021.107935,0031-3203,2021,107935,116,Pattern Recognition,Local descriptor-based multi-prototype network for few-shot Learning,article,HUANG2021107935,
"Multi-view data may lose some instances in real applications. Most existing methods for clustering such incomplete multi-view data still have at least one of the following limitations: 1) The common relations among data points across all views are ignored. 2) The complementary multi-view information of original data representation is not well exploited. 3) Arbitrary incomplete scenarios or data with negative entries cannot be handled. To address these limitations, in this paper, we propose a novel Consensus Learning approach to Incomplete Multi-view Clustering (CLIMC). Specifically, a low-dimensional consensus representation is introduced to exploit complementary multi-view information from the original feature representation of available instances by integrating index matrices into matrix factorization. In addition, by combining self-representation, index matrices, and consensus term, a consensus similarity graph is leveraged to explore the underlying cross-view relations among data points. Further, the key of the proposed CLIMC is that the consensus representation is correlated with the similarity graph by a graph Laplacian regularization. Consequently, the compactness of the low-dimensional representation and the accuracy of similarity degree of the graph are reciprocally promoted. Extensive experiments on several multi-view datasets demonstrate the effectiveness of CLIMC over state-of-the-arts.","Multi-view clustering, Incomplete multi-view clustering, Consensus representation, Consensus similarity graph",Jianlun Liu and Shaohua Teng and Lunke Fei and Wei Zhang and Xiaozhao Fang and Zhuxiu Zhang and Naiqi Wu,https://www.sciencedirect.com/science/article/pii/S0031320321000777,https://doi.org/10.1016/j.patcog.2021.107890,0031-3203,2021,107890,115,Pattern Recognition,A novel consensus learning approach to incomplete multi-view clustering,article,LIU2021107890,
"A reliable tracker has the ability to adapt to change of objects over time, and is robust and accurate. We build such a tracker by extracting semantic features using robust Siamese networks and multi-granularity color features. It incorporates a semantic model that can capture high quality semantic features and an appearance model that can describe object at pixel, local and global levels effectively. Furthermore, we propose a novel selective traverse algorithm to allocate weights to semantic models and appearance models dynamically for better tracking performance. During tracking, our tracker updates appearance representations for objects based on the recent tracking results. The proposed tracker operates at speeds that exceed the real-time requirement, and outperforms nearly all other state-of-the-art trackers on OTB-2013/2015 and VOT-2016/2017 benchmarks.","Object tracking, Siamese network, Appearance adaption",Zhuoyi Zhang and Yifeng Zhang and Xu Cheng and Guojun Lu,https://www.sciencedirect.com/science/article/pii/S0031320321001904,https://doi.org/10.1016/j.patcog.2021.108003,0031-3203,2021,108003,118,Pattern Recognition,Siamese network for object tracking with multi-granularity appearance representations,article,ZHANG2021108003,
"To identify the localization of indoor sound source, especially when attempted using only a single microphone, it is a challenging problem to machine learning. To address these issues, this paper presents a distinct novel solution based on fusing visual and acoustic models. Therefore, we propose two novel approaches. First, to estimate orientation of vocal object in a stable manner, we employ the visual approach as estimation model, where we develop a robust image feature representation method that adopts Fourier analysis to efficiently extract polar descriptors. Second the distance information is estimated by calculating the signal difference between transmit receive ends. To implement these, we use phoneme-level hidden Markov models (HMMs) extracted from clean speech sound, to estimate the acoustic transfer function (ATF), which can capture the speech signal as a network of phoneme HMMs. And using the separated frame sequences of the ATF, we can indicate the signal difference between two positions, which can be used to estimate the distance of sound source. Experimental results show that the proposed method can simultaneously extract the sound source parameters of direction and distance, and thus improves the verification task of sound source localization.","Sound source localization, Acoustic transfer function, HMM, Polar HOG, SVM",Jinhui Chen and Ryoichi Takashima and Xingchen Guo and Zhihong Zhang and Xuexin Xu and Tetsuya Takiguchi and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320321000935,https://doi.org/10.1016/j.patcog.2021.107906,0031-3203,2021,107906,115,Pattern Recognition,Multimodal fusion for indoor sound source localization,article,CHEN2021107906,
"In recent years, impressive advances have been made in single-image super-resolution. Deep learning is behind much of this success. Deep(er) architecture design and external prior modeling are the key ingredients. The internal contents of the low-resolution input image are neglected with deep modeling, despite earlier works that show the power of using such internal priors. In this paper, we propose a variation of deep residual convolutional neural networks, which has been carefully designed for robustness and efficiency in both learning and testing. Moreover, we propose multiple strategies for model adaptation to the internal contents of the low-resolution input image and analyze their strong points and weaknesses. By trading runtime and using internal priors, we achieve improvements from 0.1 to 0.3Â dB PSNR over the reported results on standard datasets. Our adaptation especially favors images with repetitive structures or high resolutions. It indicates a more practical usage when our adaption approach applies to sequences or videos in which adjacent frames are strongly correlated in their contents. Moreover, the approach can be combined with other simple techniques, such as back-projection and enhanced prediction, to realize further improvements.","Internal prior, Model adaptation, Deep convolutional neural network, Projection skip connection",Yudong Liang and Radu Timofte and Jinjun Wang and Sanping Zhou and Yihong Gong and Nanning Zheng,https://www.sciencedirect.com/science/article/pii/S0031320321001187,https://doi.org/10.1016/j.patcog.2021.107931,0031-3203,2021,107931,116,Pattern Recognition,Single-Image super-resolution - When model adaptation matters,article,LIANG2021107931,
"Segmenting optic disc (OD) in abnormal fundus images is a challenge task because of many distractions such as illumination variations, blurry boundary, occlusion of retinal vessels and big bright lesions. Data-driven deep learning is effective and robust to illumination variations, blurry boundary and occlusion in the normal fundus images but sensitive to big bright lesions in abnormal images. In this paper, an automatic OD segmentation method fusing U-net with model-driven probability bubble approach is proposed in abnormal fundus images. The probability bubble is conceived according to the position relationship between retinal vessels and OD, and the localization result is fused into the output layer of U-net through calculating the joint probability. The proposed method takes the advantage of the deep learning architecture and improves the architectureâs performance by including the model-driven position constraint when lack of sufficient training data. Experiments show that the proposed method successfully removes the distraction of bright lesions in abnormal fundus images and obtains a satisfying OD segmentation on three public databases: Kaggle, MESSIDOR and NIVE, and it outperforms existing methods with a very high accuracy.","OD segmentation, U-Net, Model-driven, Probability bubble, Joint probability",Yinghua Fu and Jie Chen and Jiang Li and Dongyan Pan and Xuezheng Yue and Yiming Zhu,https://www.sciencedirect.com/science/article/pii/S0031320321001588,https://doi.org/10.1016/j.patcog.2021.107971,0031-3203,2021,107971,117,Pattern Recognition,Optic disc segmentation by U-net and probability bubble in abnormal fundus images,article,FU2021107971,
"Data Science aims to extract meaningful knowledge from unorganised data. Real datasets usually come in the form of a cloud of points. It is a requirement of numerous applications to visualise an overall shape of a noisy cloud of points sampled from a non-linear object that is more complicated than a union of disjoint clusters. The skeletonisation problem in its hardest form is to find a 1-dimensional skeleton that correctly represents the shape of the cloud. This paper compares different algorithms that solve the above skeletonisation problem for any point cloud and guarantee a successful reconstruction. For example, given a highly noisy point sample of an unknown underlying graph, a reconstructed skeleton should be geometrically close and homotopy equivalent to (has the same number of independent cycles as) the underlying graph. One of these algorithms produces a Homologically Persistent Skeleton (HoPeS) for any cloud without extra parameters. This universal skeleton contains subgraphs that provably represent the 1-dimensional shape of the cloud at any scale. Other subgraphs of HoPeS reconstruct an unknown graph from its noisy point sample with a correct homotopy type and within a small offset of the sample. The extensive experiments on synthetic and real data reveal for the first time the maximum level of noise that allows successful graph reconstructions.","Data skeletonisation, Noisy point sample, Persistent homology",P. Smith and V. Kurlin,https://www.sciencedirect.com/science/article/pii/S0031320321000893,https://doi.org/10.1016/j.patcog.2021.107902,0031-3203,2021,107902,115,Pattern Recognition,Skeletonisation algorithms with theoretical guarantees for unorganised point clouds with high levels of noise,article,SMITH2021107902,
"Excellent neural network architecture is built on the specific target task and device. As the target task or device is different, the neural architecture we need will be different, too. Rather than redesigning or searching a brand new one, adjusting the existing architecture automatically is an alternative yet efficient way. To this end, we propose a method to Shape the existing Neural Architectures Progressively (SNAP) to adapt the target task and device better. Inspired by the streamline of water drop shaped by air resistance, we define an information density criterion (play the role of resistance) to drive the network architecture reducing the size of the part with the lowest information density. Iteratively, a more adaptive architecture will be obtained progressively in a greedy way. Theoretically, we prove that the greedy strategy is reasonable and can shape a better architecture. Because of the small adjustment of architecture each time, new architecture can inherit the parameters in old architecture to avoid retraining it from scratch. So the proposed method is very efficient in no need of high computation cost. Experimental results show that proposed method can effectively improve the given network by adjusting its architecture. And it can generate different architectures for different tasks and devices to adapt them well. Compared with search-based auto-generated neural architectures, our approach can achieve comparable or even better performance in no need of tremendous computation resources.","Auto-generated neural architectures, Information density, Greedy strategy, Progressively, Efficient and adaptive",Zhiqiang Chen and Ting-Bing Xu and Weijian Liao and Zhengcheng Li and Jinpeng Li and Cheng-Lin Liu and Huiguang He,https://www.sciencedirect.com/science/article/pii/S0031320321001102,https://doi.org/10.1016/j.patcog.2021.107923,0031-3203,2021,107923,116,Pattern Recognition,SNAP: Shaping neural architectures progressively via information density criterion,article,CHEN2021107923,
"The early detection of COVID-19 is a challenging task due to its deadly spreading nature and existing fear in minds of people. Speech-based detection can be one of the safest tools for this purpose as the voice of the suspected can be easily recorded. The Mel Frequency Cepstral Coefficient (MFCC) analysis of speech signal is one of the oldest but potential analysis tools. The performance of this analysis mainly depends on the use of conversion between normal frequency scale to perceptual frequency scale and the frequency range of the filters used. Traditionally, in speech recognition, these values are fixed. But the characteristics of speech signals vary from disease to disease. In the case of detection of COVID-19, mainly the coughing sounds are used whose bandwidth and properties are quite different from the complete speech signal. By exploiting these properties the efficiency of the COVID-19 detection can be improved. To achieve this objective the frequency range and the conversion scale of frequencies have been suitably optimized. Further to enhance the accuracy of detection performance, speech enhancement has been carried out before extraction of features. By implementing these two concepts a new feature called COVID-19 Coefficient (C-19CC) is developed in this paper. Finally, the performance of these features has been compared.","Bio-inspired computing, COVID19, Speech signal",Tusar Kanti Dash and Soumya Mishra and Ganapati Panda and Suresh Chandra Satapathy,https://www.sciencedirect.com/science/article/pii/S0031320321001862,https://doi.org/10.1016/j.patcog.2021.107999,0031-3203,2021,107999,117,Pattern Recognition,Detection of COVID-19 from speech signal using bio-inspired based cepstral features,article,DASH2021107999,
"Fine-grained visual categorization (FGVC) has attracted extensive attention in recent years. The general pipeline of current FGVC techniques is to 1) locate the discriminative regions; 2) extract features from each region independently; and 3) feed the integrated features to a classifier. In this paper, we re-investigate the pipeline from the view of human visual recognition mechanisms. The perceiving of discriminative regions is a temporal processing by the human visual system (HVS) via the attention-shift mechanism. However, the existing independent feature extracting and one-pass feeding strategy ignore the inherent semantic relationships among discriminative regions, and thus is improper to model the attention-shift process properly. Therefore, in this paper, we propose a novel end-to-end FGVC network structure named Attention-Shift based Deep Neural Network (AS-DNN) to locate the discriminative regions automatically and encode the semantic correlations iteratively. AS-DNN consists of two channels: 1) the global perception channel Cglb and 2) the attention-shift channel Csft, simulating the global perception and the attention-shift mechanism, respectively. Experimental results show that AS-DNN achieves state-of-the-art performances by outperforming both the CNN-based weakly or strongly-supervised FGVC algorithms on several widely-used fine-grained datasets, and the visualization of attention regions exhibit that the proposed method can locate the discriminative regions robustly in complex backgrounds and postures.","Fine-grained visual categorization, Deep neural network, Human perception mechanism, Attention-shift, Encoder-decoder",Yi Niu and Yang Jiao and Guangming Shi,https://www.sciencedirect.com/science/article/pii/S0031320321001345,https://doi.org/10.1016/j.patcog.2021.107947,0031-3203,2021,107947,116,Pattern Recognition,Attention-shift based deep neural network for fine-grained visual categorization,article,NIU2021107947,
"Recent years have seen an increase in research on time series data mining (especially time-series clustering) owing to the widespread existence of time series in various fields. Techniques such as clustering can extract valuable information and potential patterns from time-series data. In this regard, the clustering analysis of multivariate time series is challenging because of the high dimensionality. Our study led us to develop a novel method based on complex networks for multivariate time series clustering (BCNC). BCNC includes a new method for mapping multivariate time series into complex networks and a new method to visualize multivariate time series. The solution is innovatively based on a relationship network and relies on the use of community detection technology to achieve complete multivariate time series clustering. The detailed algorithm and the simulation experiments of the proposed BCNC method are reported. The experimental results on various datasets show that BCNC is superior to traditional multivariate time series clustering methods.","Multivariate time series, Data mining, Clustering analysis, Complex network",Hailin Li and Zechen Liu,https://www.sciencedirect.com/science/article/pii/S0031320321001060,https://doi.org/10.1016/j.patcog.2021.107919,0031-3203,2021,107919,115,Pattern Recognition,Multivariate time series clustering based on complex network,article,LI2021107919,
"Facial landmark localization and expression recognition are two important and highly relevant topics in facial analysis. However, few works focus on using the complementary information between the two tasks to improve the performance. In this paper, we propose a residual multi-task learning framework to predict the two tasks simultaneously. Different from previous multi-task learning methods which directly train a deep multi-task network with additional branches and losses, we propose a novel residual learning module to further strengthen the linkages between the two tasks. Benefit from the proposed residual learning module, one task can learn complementary information from the other task, leading to the performance promotion. Another problem for the multi-task learning is the lack of training data with multi-task labels. For example, there is no landmark localization annotation for the two widely-used FER dataset (AffectNet and RAF), vice versa. To solve this problem, we propose an association learning method to further enhance the connection between the two tasks. Based on this connection, the dataset with single-task labels can be used in the multi-task learning. Extensive experiments are conducted on four popular datasets (i.e. 300-W, AFLW for landmark localization and AffectNet, RAF for expression recognition), demonstrating the effectiveness of the proposed algorithm.","Facial landmark localization, Facial expression recognition, Deep neural network, Multi-task learning",Boyu Chen and Wenlong Guan and Peixia Li and Naoki Ikeda and Kosuke Hirasawa and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320321000807,https://doi.org/10.1016/j.patcog.2021.107893,0031-3203,2021,107893,115,Pattern Recognition,Residual multi-task learning for facial landmark localization and expression recognition,article,CHEN2021107893,
"Document binarization is a key step in most document analysis tasks. However, historical-document images usually suffer from various degradations, making this a very challenging processing stage. The performance of document image binarization has improved dramatically in recent years by the use of Convolutional Neural Networks (CNNs). In this paper, a dual-task, T-shaped neural network is proposed that has the main task of binarization and an auxiliary task of image enhancement. The neural network for enhancement learns the degradations in document images and the specific CNN-kernel features can be adapted towards the binarization task in the training process. In addition, the enhancement image can be considered as an improved version of the input image, which can be fed into the network for fine-tuning, making it possible to design a chained-cascade network (CT-Net). Experimental results on document binarization competition datasets (DIBCO datasets) and MCS dataset show that our proposed method outperforms competing state-of-the-art methods in most cases.","Cascade T-Net, Binarization, Enhancement, DIBCO, Convolutional neural networks",Sheng He and Lambert Schomaker,https://www.sciencedirect.com/science/article/pii/S0031320321001977,https://doi.org/10.1016/j.patcog.2021.108010,0031-3203,2021,108010,118,Pattern Recognition,CT-Net: Cascade T-shape deep fusion networks for document binarization,article,HE2021108010,
"The discovery of discriminative patterns from high-dimensional data offers the possibility to learn from informative subspaces and pattern-centric features, paving the way to associative classifiers. Despite the success achieved by associative classifiers, such as random forests or XGBoost, they generally neglect discriminative subspaces with non-constant coherencies. Research on biclustering has for two decades highlighted the role of non-constant patterns in biomedical domains, including additive and order-preserving patterns. Still, their relevance for classification remains unexplored. This work assesses the impact of discriminative patterns with varying coherence and quality on associative classification. A novel classifier, FleBiC, is proposed as a result. FleBiC extends pattern-based biclustering with principles to match observations against non-constant and noise-tolerant patterns, address generalization difficulties, minimize scarcity of matches, support class disjunctions, and offer statistical guarantees. Results on biological and clinical data highlight the role of non-constant patterns, specially order-preserving patterns, for improving the performance of state-of-the-art classifiers.","Associative classification, Discriminative paterns, Biclustering, Non-constant patterns, Biomedical data, High-dimensional data",Rui Henriques and Sara C. Madeira,https://www.sciencedirect.com/science/article/pii/S003132032100087X,https://doi.org/10.1016/j.patcog.2021.107900,0031-3203,2021,107900,115,Pattern Recognition,FleBiC: Learning classifiers from high-dimensional biomedical data using discriminative biclusters with non-constant patterns,article,HENRIQUES2021107900,
"Matching local features on two or more images is fundamental for many applications in the field of computer vision and pattern recognition. Identifying and rejecting mismatches is an important part in the framework of feature matching, due to the putative correspondences always contaminated by mismatches with the error-prone local feature detectors. In this paper, we introduce a novel method, namely Guided Local Outlier Factor (GLOF) for feature matching with gross mismatches under multi-granularity neighborhood structure-preserving. We first construct a tentative correspondence set by matching multi-features. Then, we identify and remove mismatches. Inspired by the anomaly detection technique, putative correspondences are assigned to a particular score, so abnormal instances, i.e., mismatches can be classified by a user-defined threshold. More specially, the neighborhood preserving guides the local searching procedure. Moreover, to eliminate the fluctuation of the matching results with different sizes of local neighbors, we use the multi-granularity algorithm to average out the deviation. Experimental results demonstrate that the introduced approach is superior to several state-of-the-art methods in terms of mismatch rejection on publicly available datasets.","Feature matching, Mismatch removal, Rejecting outliers, Locality preserving, Image matching",Gang Wang and Yufei Chen,https://www.sciencedirect.com/science/article/pii/S0031320321001734,https://doi.org/10.1016/j.patcog.2021.107986,0031-3203,2021,107986,117,Pattern Recognition,Robust feature matching using guided local outlier factor,article,WANG2021107986,
"This study presents an accurate and robust method for fitting noisy and occlusion elliptic data. The nonlinear issue of ellipse fitting is interpreted as a set of LevenbergâMarquardt iterations (LMIs) by minimizing the geometric distance. For each iteration of the geometric fitting, the representations of the parameterized ellipses are mapped to the geometric error distances, which are implemented latently by an orthogonal angle segmentation of the ellipses. Moreover, dimension reduction is utilized by the LMIs to avoid misconvergence and expensive computations. The method based on two recent representative algorithms is verified by simulation and real-world experiments. The results suggest that the saliency capability of the new method to fit ellipse-specific profiles with severe noise and occlusion, which is better than or equal to those of the reference approaches, has potential applications in quality monitoring, three-dimensional reconstruction, and instrument calibration.","Ellipse fitting, Least squares, LevenbergâMarquardt iterations, Profile analysis",Tao Wang and Zhaoyao Shi and Bo Yu,https://www.sciencedirect.com/science/article/pii/S0031320321001217,https://doi.org/10.1016/j.patcog.2021.107934,0031-3203,2021,107934,116,Pattern Recognition,A parameterized geometric fitting method for ellipse,article,WANG2021107934,
"Multi-label canonical correlation analysis (ml-CCA) has been developed for cross-modal retrieval. However, the computation of ml-CCA involves dense matrices eigendecomposition, which can be computationally expensive. In addition, ml-CCA only takes semantic correlation into account which ignores the cross-modal feature correlation. In this paper, we propose a novel framework to simultaneously integrate the semantic correlation and feature correlation for cross-modal retrieval. By using the semantic transformation, we show that our model can avoid computing the covariance matrix explicitly which is a huge save of computational cost. Further analysis shows that our proposed method can be solved via singular value decomposition which has linear time complexity. Experimental results on three multi-label datasets have demonstrated the accuracy and efficiency of our proposed method.","Canonical correlation analysis, Semantic transformation, Cross-modal retrieval, Singular value decomposition",Xin Shu and Guoying Zhao,https://www.sciencedirect.com/science/article/pii/S0031320321000923,https://doi.org/10.1016/j.patcog.2021.107905,0031-3203,2021,107905,115,Pattern Recognition,Scalable multi-label canonical correlation analysis for cross-modal retrieval,article,SHU2021107905,
"It is well known that the performance of a kernel method highly depends on the choice of kernel parameter. A kernel path provides a compact representation of all optimal solutions, which can be used to choose the optimal value of kernel parameter along with cross validation (CV) method. However, none of these existing kernel path algorithms provides a unified implementation to various learning problems. To fill this gap, in this paper, we first study a general parametric quadratic programming (PQP) problem that can be instantiated to an extensive number of learning problems. Then we provide a generalized kernel path (GKP) for the general PQP problem. Furthermore, we analyze the iteration complexity and computational complexity of GKP. Extensive experimental results on various benchmark datasets not only confirm the identity of GKP with several existing kernel path algorithms, but also show that our GKP is superior to the existing kernel path algorithms in terms of generalization and robustness.","Kernel path, QR decomposition, Parametric quadratic programming, Cross validation",Bin Gu and Ziran Xiong and Shuyang Yu and Guansheng Zheng,https://www.sciencedirect.com/science/article/pii/S003132032100128X,https://doi.org/10.1016/j.patcog.2021.107941,0031-3203,2021,107941,116,Pattern Recognition,A kernel path algorithm for general parametric quadratic programming problem,article,GU2021107941,
"One of the important problems in computer-aided diagnosis of connective tissue disease is automatic recognition of staining patterns present in HEp-2 cells. In this regard, the paper introduces a novel approach for the recognition of staining patterns by HEp-2 cell indirect immunofluorescence image analysis. The proposed method assumes that a fixed set of local texture descriptors or scales may not be effective for classifying staining patterns into multiple classes. A particular set of descriptors or scales may be significant for classifying a pair of classes, but may not be relevant for other pairs of classes. The proposed approach, therefore, first selects a set of local texture descriptors under appropriate scales for each class-pair, and then forms the final feature set for multiple classes from the relevant descriptors of all possible pairs of classes. A novel framework, termed as Rough-Bayesian model, is introduced to evaluate the relevance of a descriptor and/or a scale. It is based on the merits of rough sets and Bayes decision theory. During the selection of relevant descriptor and/or scale, the proposed method takes care of the presence of both noisy pixels in an HEp-2 cell image and noisy HEp-2 cell images in a staining pattern class. The support vector machine is used to predict the staining patterns present in HEp-2 cell images. The performance of the proposed method, along with a comparison with state-of-the-art methods, is demonstrated on several HEp-2 cell image databases. An important finding is that the accuracy for classifying HEp-2 cell images is significantly increased if class-pair specific descriptors under appropriate scales are considered, instead of selecting a uniform set of descriptors and scales for multiple classes.","HEp-2 cell images, Staining pattern recognition, Texture analysis, Rough sets, Bayes decision theory",Debamita Kumar and Pradipta Maji,https://www.sciencedirect.com/science/article/pii/S0031320321001692,https://doi.org/10.1016/j.patcog.2021.107982,0031-3203,2021,107982,117,Pattern Recognition,Rough-Bayesian approach to select class-pair specific descriptors for HEp-2 cell staining pattern recognition,article,KUMAR2021107982,
"Optical coded targets allow to determine the relative pose of a camera, on a metric scale, from one image only. Furthermore, they are easily and efficiently detected, opening to a wide range of applications in robotics and computer vision. In this work we describe the effect of pixel saturation and non-ideal lens Point Spread Function, causing the apparent position of the corners and the edges of the target to change as a function of the camera exposure time. This effect, which we call exposure bias, is frequent in over- or underexposed images and introduces a systematic error in the estimated camera pose. We propose an algorithm that is able to estimate and correct for the exposure bias exploiting specific geometric features of a common target design based on concentric circles. Through rigorous laboratory experiments carried out in a highly controlled environment, we demonstrate that the proposed algorithm is seven times more precise and three times more accurate in the target distance estimation than the algorithms available in the literature.","Optical target, Target orientation, Image processing algorithm, Geometry, Ellipse fitting, Computer vision, Overexposure, Exposure compensation, Resection",E. Cledat and M. Rufener and D.A. Cucci,https://www.sciencedirect.com/science/article/pii/S0031320321001175,https://doi.org/10.1016/j.patcog.2021.107930,0031-3203,2021,107930,116,Pattern Recognition,Compensating over- and underexposure in optical target pose determination,article,CLEDAT2021107930,
"Monocular depth prediction is an important task in scene understanding. It aims to predict the dense depth of a single RGB image. With the development of deep learning, the performance of this task has made great improvements. However, two issues remain unresolved: (1) The deep feature encodes the wrong farthest region in a scene, which leads to a distorted 3D structure of the predicted depth; (2) The low-level features are insufficient utilized, which makes it even harder to estimate the depth near the edge with sudden depth change. To tackle these two issues, we propose the Boundary-induced and Scene-aggregated network (BS-Net). In this network, the Depth Correlation Encoder (DCE) is first designed to obtain the contextual correlations between the regions in an image, and perceive the farthest region by considering the correlations. Meanwhile, the Bottom-Up Boundary Fusion (BUBF) module is designed to extract accurate boundary that indicates depth change. Finally, the Stripe Refinement module (SRM) is designed to refine the dense depth induced by the boundary cue, which improves the boundary accuracy of the predicted depth. Several experimental results on the NYUD v2 dataset and the iBims-1 dataset illustrate the state-of-the-art performance of the proposed approach. And the SUN-RGBD dataset is employed to evaluate the generalization of our method. Code is available at https://github.com/XuefengBUPT/BS-Net.","Monocular depth prediction, Boundary-induced, Depth correlation",Feng Xue and Junfeng Cao and Yu Zhou and Fei Sheng and Yankai Wang and Anlong Ming,https://www.sciencedirect.com/science/article/pii/S0031320321000881,https://doi.org/10.1016/j.patcog.2021.107901,0031-3203,2021,107901,115,Pattern Recognition,Boundary-induced and scene-aggregated network for monocular depth prediction,article,XUE2021107901,
"This paper presents a new nested U-shape attention network (NUA-Net) with improved robustness of lesions for effective vascular segmentation in retinal imaging. Unlike most of the current deep learning approaches which rely on vanilla upsample module to recover distinguishable features for segmentation, our attention-based multi-scale network extends the U-shape segmentation network by introducing a novel multi-scale upsample attention (MSUA) module to enhance vessel features in a hierarchical structure. The new approach connects encoder-decoder branches through a nested skip-connection pyramid architecture to extract discriminating retinal features from the rich local details. Experimental evaluations on five publicly available databases DRIVE, STARE, CHASE_DB, IOSTAR and HRF show the NUA-Net achieves 0.8043â0.8511 (Sensitivity), 0.9741â0.99 (Specificity) and 0.9646â0.9794 (Accuracy) respectively. The benchmark by cross-testing and separate-testing presents a state-of-the-art performance and better vessel preservation compared with other approaches.","Vascular segmentation, Retinal imaging, Dense U-Net, Multi-scale attention, Deep learning",Ruohan Zhao and Qin Li and Jianrong Wu and Jane You,https://www.sciencedirect.com/science/article/pii/S0031320321001850,https://doi.org/10.1016/j.patcog.2021.107998,0031-3203,2021,107998,120,Pattern Recognition,A nested U-shape network with multi-scale upsample attention for robust retinal vascular segmentation,article,ZHAO2021107998,
"In recent years, a large number of meta-learning methods have been proposed to address few-shot learning problems and have shown superior performance. However, the explicit prior knowledge (e.g., concept graph) and weakly-supervised information are rarely explored in existing methods, which are usually free or cheap to collect. In this paper, we introduce a concept graph for the weakly-supervised few-shot learning, and propose a novel meta-learning framework, namely, MetaConcept. Our key idea is to learn a universal meta-learner inferring any-level classifier, so as to boost the classification performance of meta-learning on the novel classes. Specifically, we firstly propose a novel regularization with multi-level conceptual abstraction to train a universal meta-learner to infer not only an entity classifier but also a concept classifier at different levels via the concept graph (i.e., learn to abstract). Then, we propose a meta concept inference network as the universal meta-learner for the base learner, aiming to quickly adapt to a novel task by the joint inference of the abstract concepts and a few annotated samples. We have conducted extensive experiments on two weakly-supervised few-shot learning benchmarks, namely, WS-ImageNet-Pure and WS-ImageNet-Mix. Our experimental results show that (1) the proposed MetaConcept outperforms state-of-the-art methods with an improvement of 2% to 6% in classification accuracy; (2) the proposed MetaConcept is able to yield a good performance though merely training with weakly-labeled datasets.","Few-shot learning, Weakly-supervised learning, Meta-learning, Concept graph",Baoquan Zhang and Ka-Cheong Leung and Xutao Li and Yunming Ye,https://www.sciencedirect.com/science/article/pii/S0031320321001333,https://doi.org/10.1016/j.patcog.2021.107946,0031-3203,2021,107946,117,Pattern Recognition,Learn to abstract via concept graph for weakly-supervised few-shot learning,article,ZHANG2021107946,
"The popularity of smartphones with digital cameras makes photographing using smartphones an important daily activity. MoirÃ© patterns can easily appear when shooting objects with rich textures, such as computer screens, and will severely degrade the image quality. Image demoirÃ©ing is an important image restoration task that aims to remove moirÃ© patterns and reveal the underlying clean image. Two key properties of moirÃ© patternsâthe widely distributed frequency spectrum and the dynamic nature of moirÃ© texturesâchallenge the image demoirÃ©ing task. In this paper, we propose an improved Multi-scale convolutional network with Dynamic feature encoding for image DeMoirÃ©ing (MDDM+). We design two schemes in our network to respectively attack the broad frequency spectrum and the dynamic texture of moirÃ©: a multi-scale structure to process images at different spatial resolutions and a dynamic feature encoding module to encode the texture dynamically. To capture more moirÃ© and texture information from different frequencies, we further propose a novel L1 wavelet loss used to train our model. Extensive experiments on two benchmarks show that our proposed image demoirÃ©ing network can outperform the state of the arts in terms of fidelity as well as perception.","Image demoirÃ©ing, Screen shot images, MoirÃ© pattern, Dynamic feature encoding",Xi Cheng and Zhenyong Fu and Jian Yang,https://www.sciencedirect.com/science/article/pii/S0031320321001576,https://doi.org/10.1016/j.patcog.2021.107970,0031-3203,2021,107970,116,Pattern Recognition,Improved multi-scale dynamic feature encoding network for image demoirÃ©ing,article,CHENG2021107970,
"Motor imagery brain-computer interface (MI-BCI) has many promising applications but there are problems such as poor classification accuracy and robustness which need to be addressed. We propose a novel approach called time-frequency common spatial patterns (TFCSP) to enhance the robustness and accuracy of the electroencephalogram (EEG) signal classification. The proposed approach decomposes the EEG signal into time stages and frequency components to find the most robust and discriminative features. Common spatial patterns (CSP) are extracted from every decomposed time-frequency cell and unreliable features are removed while remaining features are weighted and regularized for the classification. Comparison on three publicly available datasets from BCI competition III and IV shows that the proposed TFCSP outperforms state-of-the-art methods. This demonstrates that adopting subject reaction time paradigm is useful to enhance the classification performance. It also shows that the complex CSP in the frequency domain significantly effective than the commonly used bandpass-filters in time domain. Finally, this work proves that weighting and regularizing CSP features are better techniques than selecting the leading CSP features because the former alleviates information loss.","Brain-computer interface, Common spatial patterns, Electroencephalography, Motor imagery, Signal decomposition",Vasilisa Mishuhina and Xudong Jiang,https://www.sciencedirect.com/science/article/pii/S0031320321001059,https://doi.org/10.1016/j.patcog.2021.107918,0031-3203,2021,107918,115,Pattern Recognition,Complex common spatial patterns on time-frequency decomposed EEG for brain-computer interface,article,MISHUHINA2021107918,
"Lung cancer is among the most common and deadliest cancers with a low 5-year survival rate. Timely diagnosis of lung cancer is, therefore, of paramount importance as it can save countless lives. In this regard, Computed Tomography (CT) scan is widely used for early detection of lung cancer, where human judgment is currently considered as the gold standard approach. Recently, there has been a surge of interest on development of automatic solutions via radiomics, as human-centered diagnosis is subject to inter-observer variability and is highly burdensome. Hand-crafted radiomics, serving as a radiologist assistant, requires fine annotations and pre-defined features. Deep learning radiomics solutions, however, have the promise of extracting the most useful features on their own in an end-to-end fashion without having access to the annotated boundaries. Among different deep learning models, Capsule Networks are proposed to overcome shortcomings of the Convolutional Neural Networks (CNNs) such as their inability to recognize detailed spatial relations. Capsule networks have so far shown satisfying performance in medical imaging problems. Capitalizing on their success, in this study, we propose a novel capsule network-based mixture of experts, referred to as the MIXCAPS. The proposed MIXCAPS architecture takes advantage of not only the capsule networkâs capabilities to handle small datasets, but also automatically splitting dataset through a convolutional gating network. MIXCAPS enables capsule network experts to specialize on different subsets of the data. Our results show that MIXCAPS outperforms a single capsule network, a single CNN, a mixture of CNNs, and an ensemble of capsule networks, with an average accuracy of 90.7%, average sensitivity of 89.5%, average specificity of 93.4% and average area under the curve of 0.956. Our experiments also show that there is a relation between the gate outputs and a couple of hand-crafted features, illustrating explainable nature of the proposed MIXCAPS. To further evaluate generalization capabilities of the proposed MIXCAPS architecture, additional experiments on a brain tumor dataset are performed showing potentials of MIXCAPS for detection of tumors related to other organs.","Tumor type classification, Capsule network, Mixture of experts",Parnian Afshar and Farnoosh Naderkhani and Anastasia Oikonomou and Moezedin Javad Rafiee and Arash Mohammadi and Konstantinos N. Plataniotis,https://www.sciencedirect.com/science/article/pii/S0031320321001291,https://doi.org/10.1016/j.patcog.2021.107942,0031-3203,2021,107942,116,Pattern Recognition,MIXCAPS: A capsule network-based mixture of experts for lung nodule malignancy prediction,article,AFSHAR2021107942,
"Deep learning has shown superiority in dealing with complicated and professional tasks (e.g., computer vision, audio, and language processing). However, research works have confirmed that Deep Neural Networks (DNNs) are vulnerable to carefully crafted adversarial perturbations, which cause DNNs confusion on specific tasks. In object detection domain, the background has little contributions to object classification, and the crafted adversarial perturbations added to the background do not improve the adversary effect in fooling deep neural detection models yet induce substantial distortions in generated examples. Based on such situation, we introduce an adversarial attack algorithm named Adaptive Object-oriented Adversarial Method (AO2AM). It aims to fool deep neural object detection networks with the adversarial examples by applying the adaptive cumulation of object-based gradients and adding the adaptive object-based adversarial perturbations merely onto objects rather than the whole frame of input images. AO2AM can effectively make the representations of generated adversarial samples close to the decision boundary in the latent space, and force deep neural detection networks to yield inaccurate locations and false classification in the process of object detection. Compared with existing adversarial attack methods which generate adversarial perturbations acting on the global scale of the original inputs, the adversarial examples produced by AO2AM can effectively fool deep neural object detection networks and maintain a high structural similarity with corresponding clean inputs. Performing adversarial attacks on Faster R-CNN, AO2AM gains attack success rate (ASR) over 98.00% on pre-processed Pascal VOC 2007&2012Â (Val), and reaches SSIM over 0.870. In Fooling SSD, AO2AM receives SSIM exceeding 0.980 on L2 norm constraint. On SSIM and Mean Attack Ratio, AO2AM outperforms adversarial attack methods based on global scale perturbations.","Object detection, Adversarial attack, Adaptive object-oriented perturbation",Yatie Xiao and Chi-Man Pun and Bo Liu,https://www.sciencedirect.com/science/article/pii/S003132032100090X,https://doi.org/10.1016/j.patcog.2021.107903,0031-3203,2021,107903,115,Pattern Recognition,Fooling deep neural detection networks with adaptive object-oriented adversarial perturbation,article,XIAO2021107903,
"Deep learning techniques have been increasingly applied to the diagnosis of Alzheimerâs disease (AD) and the conversion from mild cognitive impairment (MCI) to AD. Despite their prevalence, existing methods usually suffer from using either irrelevant brain regions or less-accurate landmarks. In this paper, we propose the iterative sparse and deep learning (ISDL) model for joint deep feature extraction and critical cortical region identification to diagnose AD and MCI. We first design a deep feature extraction (DFE) module to capture the local-to-global structural information derived from 62 cortical regions. Then we design a sparse regression module to identify the critical cortical regions and integrate it into the DFE module to exclude irrelevant cortical regions from the diagnosis process. The parameters of the two modules are updated alternatively and iteratively in an end-to-end manner. Our experimental results suggest the ISDL model provides a state-of-the-art solution to both AD-CN classification and MCI-to-AD prediction.","Alzheimerâs disease, Mild cognitive impairment, Deep learning, Sparse regression",Yuanyuan Chen and Yong Xia,https://www.sciencedirect.com/science/article/pii/S003132032100131X,https://doi.org/10.1016/j.patcog.2021.107944,0031-3203,2021,107944,116,Pattern Recognition,Iterative sparse and deep learning for accurate diagnosis of Alzheimerâs disease,article,CHEN2021107944,
"Though image segmentation models are plentiful and have many applications nowadays, it can be difficult to segment images with complex boundaries and serious intensity inhomogeneity. To some extent, the region-scalable fitting energy model can segment images suffering from intensity inhomogeneity since it considers image intensity as a function, but it relies on initial conditions dramatically. Nowadays, prior knowledge has been widely applied in image segmentation models, which can integrate automatic method and experts experience in one robust and fast segmentation model. In this paper we present a new model that can segment various images accurately by taking the advantages of the region-scalable fitting energy model and the advanced transcendental constraint from artificial experience. The proposed energy functional consists of a smooth length term, a target image data term and a transcendental constraint term. The transcendental constraint term plays a key role in the proposed model, which not only gives the accurate segmentation results but also provides us the chance to carry out the parallel computation. In the proposed-parallel model, the efficiency is improved a lot and the results become more precise compared with other methods. The split Bregman method is applied to minimize the energy functional. Furthermore, we present the convergence analysis and the time complexity analysis of our algorithm. Multiple experimental results and comparisons including parameters sensitivity discussion are shown to demonstrate the superiority of the proposed model such as high accuracy, robustness and efficiency.","Image segmentation, Split Bregman method, Parallel computaion, Transcendental constraint term, Level set formulation",Yunyun Yang and Ruofan Wang and Xiu Shu and Chong Feng and Ruicheng Xie and Wenjing Jia and Chunming Li,https://www.sciencedirect.com/science/article/pii/S0031320321001722,https://doi.org/10.1016/j.patcog.2021.107985,0031-3203,2021,107985,117,Pattern Recognition,Level set framework with transcendental constraint for robust and fast image segmentation,article,YANG2021107985,
"Ant colony optimization (ACO) is widely used in feature selection owing to its excellent global/local search capabilities and flexible graph representation. However, the current ACO-based feature selection methods are mainly applied to low-dimensional datasets. For thousands of dimensional datasets, the search for the optimal feature subset (OFS) becomes extremely difficult due to the exponential increase of the search space. In this paper, we propose a two-stage hybrid ACO for high-dimensional feature selection (TSHFS-ACO). As an additional stage, it uses the interval strategy to determine the size of OFS for the following OFS search. Compared to the traditional one-stage methods that determine the size of OFS and search for OFS simultaneously, the stage of checking the performance of partial feature number endpoints in advance helps to reduce the complexity of the algorithm and alleviate the algorithm from getting into a local optimum. Moreover, the advanced ACO algorithm embeds the hybrid model, which uses the featuresâ inherent relevance attributes and the classification performance to guide OFS search. The test results on eleven high-dimensional public datasets show that TSHFS-ACO is suitable for high-dimensional feature selection. The obtained OFS has state-of-the-art performance on most datasets. And compared with other ACO-based feature selection methods, TSHFS-ACO has a shorter running time.","Feature selection, Ant colony optimization, High-dimensional data, Classification, Optimal feature subset size",Wenping Ma and Xiaobo Zhou and Hao Zhu and Longwei Li and Licheng Jiao,https://www.sciencedirect.com/science/article/pii/S0031320321001205,https://doi.org/10.1016/j.patcog.2021.107933,0031-3203,2021,107933,116,Pattern Recognition,A two-stage hybrid ant colony optimization for high-dimensional feature selection,article,MA2021107933,
"In this study, we proposed and validated a multi-atlas and diffeomorphism guided 3D fully convolutional network (FCN) ensemble model (M-FCN) for segmenting brain anatomical regions of interest (ROIs) from structural magnetic resonance images (MRIs). A novel multi-atlas and diffeomorphism based encoding block and ROI patches with adaptive sizes were used. In the multi-atlas and diffeomorphism based encoding block, both MRI intensity profiles and expert priors from deformed atlases were encoded and fed to the proposed FCN. Utilizing patches with adaptive sizes enabled more efficient network training and testing. To incorporate both local and global contextual information of a specific ROI, we employed a long skip connection between the layer of the encoding block and the layer of the encoding-decoding block. To relieve over-fitting of the proposed FCN model on the training data, we adopted an ensemble strategy in the learning procedure. Systematic evaluations were performed on two brain MRI datasets, aiming respectively at segmenting 14 subcortical and ventricular structures and 54 whole-brain ROIs. Compared with two state-of-the-art segmentation methods including a multi-atlas based segmentation method and an existing 3D FCN segmentation model, the proposed method exhibited superior segmentation performance.","Brain segmentation, Fully convolutional network, Multi-atlas, Diffeomorphism, Adaptive-size patches, Ensemble model",Jiong Wu and Xiaoying Tang,https://www.sciencedirect.com/science/article/pii/S0031320321000911,https://doi.org/10.1016/j.patcog.2021.107904,0031-3203,2021,107904,115,Pattern Recognition,Brain segmentation based on multi-atlas and diffeomorphism guided 3D fully convolutional network ensembles,article,WU2021107904,
"We introduce pattern injection local search (PILS), an optimization strategy that uses pattern mining to explore high-order local-search neighborhoods, and illustrate its application on the vehicle routing problem. PILS operates by storing a limited number of frequent patterns from elite solutions. During the local search, each pattern is used to define one move in which 1)Â incompatible edges are disconnected, 2)Â the edges defined by the pattern are reconnected, and 3)Â the remaining solution fragments are optimally reconnected. Each such move is accepted only in case of solution improvement. As visible in our experiments, this strategy results in a new paradigm of local search, which complements and enhances classical search approaches in a controllable amount of computational time. We demonstrate that PILS identifies useful high-order moves that would otherwise not be found by enumeration, and that it significantly improves the performance of state-of-the-art population-based and neighborhood-centered metaheuristics.","Local search, Pattern mining, Combinatorial optimization, Vehicle routing problem",Florian Arnold and Ãtalo Santana and Kenneth SÃ¶rensen and Thibaut Vidal,https://www.sciencedirect.com/science/article/pii/S0031320321001448,https://doi.org/10.1016/j.patcog.2021.107957,0031-3203,2021,107957,116,Pattern Recognition,PILS: Exploring high-order neighborhoods by pattern mining and injection,article,ARNOLD2021107957,
"Object detection is an important field in computer vision. Nevertheless, a research area that has so far not received much attention is the study into the effectiveness of anchor matching strategy and imbalance in anchor-based object detection, in particular small object detection. It is clear that the objects with larger sizes tend to match more anchors than smaller ones. This matching imbalance may result in poor performance in detecting small objects. It can be alleviated by paying more attention to the objects that match with fewer anchors. We propose an innovative flexible loss function for object detection, which is compatible with popular anchor-based detection methods. The proposed method, called the scale-balanced loss, does not add any extra computational cost to the original pipelines. By re-weighting strategy, the proposed method significantly improves the accuracy of multi-scale object detection, especially for small objects. Comprehensive experiments indicate that the scale-balanced loss achieved excellent generalization performance when separately applied to some popular detection methods. The scale-balanced loss attained up to 15% improvements on recall rates of small and medium objects in both the PASCAL VOC and MS COCO dataset. It is also beneficial to the AP result on MS COCO with an improvement of more than 1.5%.","Object detection, Neural network, Matching imbalance",Kai Shuang and Zhiheng Lyu and Jonathan Loo and Wentao Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321001849,https://doi.org/10.1016/j.patcog.2021.107997,0031-3203,2021,107997,117,Pattern Recognition,Scale-balanced loss for object detection,article,SHUANG2021107997,
"The recent performance of facial landmark detection has been significantly improved by using deep Convolutional Neural Networks (CNNs), especially the Heatmap Regression Models (HRMs). Although their performance on common benchmark datasets has reached a high level, the robustness of these models still remains a challenging problem in the practical use under noisy conditions of realistic environments. Contrary to most existing work focusing on the design of new models, we argue that improving the robustness requires rethinking many other aspects, including the use of datasets, the format of landmark annotation, the evaluation metric as well as the training and detection algorithm itself. In this paper, we propose a novel method for robust facial landmark detection, using a loss function based on the 2D Wasserstein distance combined with a new landmark coordinate sampling relying on the barycenter of the individual probability distributions. Our method can be plugged-and-play on most state-of-the-art HRMs with neither additional complexity nor structural modifications of the models. Further, with the large performance increase, we found that current evaluation metrics can no longer fully reflect the robustness of these models. Therefore, we propose several improvements to the standard evaluation protocol. Extensive experimental results on both traditional evaluation metrics and our evaluation metrics demonstrate that our approach significantly improves the robustness of state-of-the-art facial landmark detection models.","Facial landmark detection, Face alignment, Heatmap regression, Wasserstein distance",Yongzhe Yan and Stefan Duffner and Priyanka Phutane and Anthony Berthelier and Christophe Blanc and Christophe Garcia and Thierry Chateau,https://www.sciencedirect.com/science/article/pii/S0031320321001321,https://doi.org/10.1016/j.patcog.2021.107945,0031-3203,2021,107945,116,Pattern Recognition,2D Wasserstein loss for robust facial landmark detection,article,YAN2021107945,
"As an important and challenging task for intelligent video surveillance systems, video anomaly detection is generally referred to as automatic recognition of video frames that contain abnormal targets, behavior or events. Although it has been widely applied in real scenes, anomaly detection remains a challenging task because of the vague definition of anomaly and the lack of the anomaly samples. Inspired by the widespread application of Generative Adversarial Network (GAN), we propose an end-to-end pipeline called NM-GAN which assembles an encode-decoder reconstruction network and a CNN-based discrimination network in a GAN-like architecture. The generalization ability of the reconstruction network is properly modulated via the adversarial learning around reconstruction error maps and noise maps. Meanwhile, the discrimination network is trained to distinguish anomaly samples from normal samples based on the reconstruction error maps. Finally, the output of the discrimination network is transferred to evaluate anomaly score of the input frame. The thorough proof-of-principle experiments and ablation tests on several popular datasets reveal that the proposed model enhance the generalization ability of the reconstruction network and the distinguishability of the discrimination network significantly. The comparison with the state-of-the-art shows that the proposed NM-GAN model outperforms most competing models in precision and stability.","Video anomaly detection, Generative adversarial network, Noise modulation, Reconstruction error map, Generalization ability",Dongyue Chen and Lingyi Yue and Xingya Chang and Ming Xu and Tong Jia,https://www.sciencedirect.com/science/article/pii/S0031320321001564,https://doi.org/10.1016/j.patcog.2021.107969,0031-3203,2021,107969,116,Pattern Recognition,NM-GAN: Noise-modulated generative adversarial network for video anomaly detection,article,CHEN2021107969,
"Piecewise Linear Approximation is one of the most commonly used strategies to represent time series effectively and approximately. This approximation divides the time series into non-overlapping segments and approximates each segment with a straight line. Many suboptimal methods were proposed for this purpose. This paper proposes a new optimal approach, called OSFS, based on feasible space (FS) Liu etÂ al. (2008)[1], that minimizes the number of segments of the approximation and guarantees the error bound using the Lâ-norm. On the other hand, a new performance measure combined with the OSFS method has been used to evaluate the performance of some suboptimal methods and that of the optimal method that minimizes the holistic approximation error (L2-norm). The results have shown that the OSFS method is optimal and demonstrates the advantages of Lâ-norm over L2-norm.","Data representation, Optimal time series segmentation, Error bound guarantee, -norm",Ãngel Carmona-Poyato and NicolÃ¡s Luis FernÃ¡ndez-Garcia and Francisco JosÃ© Madrid-Cuevas and Antonio Manuel DurÃ¡n-Rosal,https://www.sciencedirect.com/science/article/pii/S0031320321001047,https://doi.org/10.1016/j.patcog.2021.107917,0031-3203,2021,107917,115,Pattern Recognition,A new approach for optimal offline time-series segmentation with error bound guarantee,article,CARMONAPOYATO2021107917,
"Quadratic discriminant analysis (QDA) is a widely used statistical tool to classify observations from different multivariate Normal populations. The generalized quadratic discriminant analysis (GQDA) classification rule/classifier, which generalizes the QDA and the minimum Mahalanobis distance (MMD) classifiers to discriminate between populations with underlying elliptically symmetric distributions competes quite favorably with the QDA classifier when it is optimal and performs much better when QDA fails under non-Normal underlying distributions with heavy tail, e.g. Cauchy distribution. However, the classification rule in GQDA is still based on the sample mean vector and the sample dispersion matrix of a training set, which are extremely non-robust under data contamination. In real world, however, it is quite common to face data which are highly vulnerable to outliers and so the lack of robustness of the classical estimators of the mean vector and the dispersion matrix reduces the efficiency of the GQDA classifier significantly, increasing the misclassification errors. The present paper investigates the performance of the GQDA classifier when the classical estimators of the mean vector and the dispersion matrix used therein are replaced by various robust counterparts. Applications to various real data sets as well as simulation studies reveal far better performance of the proposed robust versions of the GQDA classifier. A comparative study has been made to advocate the appropriate choice of the robust estimators to be used in a specific situation.","Linear discriminant analysis, Quadratic discriminant analysis, Generalized quadratic discriminant analysis, Robust estimators",Abhik Ghosh and Rita SahaRay and Sayan Chakrabarty and Sayan Bhadra,https://www.sciencedirect.com/science/article/pii/S0031320321001680,https://doi.org/10.1016/j.patcog.2021.107981,0031-3203,2021,107981,117,Pattern Recognition,Robust generalised quadratic discriminant analysis,article,GHOSH2021107981,
"Object detection through convolutional neural networks is reaching unprecedented levels of precision. However, a detailed analysis of the results shows that the accuracy in the detection of small objects is still far from being satisfactory. A recent trend that will likely improve the overall object detection success is to use the spatial information operating alongside temporal video information. This paper introduces STDnet-ST, an end-to-end spatio-temporal convolutional neural network for small object detection in video. We define small as those objects under 16Ã16Â px, where the features become less distinctive. STDnet-ST is an architecture that detects small objects over time and correlates pairs of the top-ranked regions with the highest likelihood of containing those small objects. This permits to link the small objects across the time as tubelets. Furthermore, we propose a procedure to dismiss unprofitable object links in order to provide high quality tubelets, increasing the accuracy. STDnet-ST is evaluated on the publicly accessible USC-GRAD-STDdb, UAVDT and VisDrone2019-VID video datasets, where it achieves state-of-the-art results for small objects.","Small object detection, Spatio-temporal convolutional network, Object linking",Brais Bosquet and Manuel Mucientes and VÃ­ctor M. Brea,https://www.sciencedirect.com/science/article/pii/S0031320321001163,https://doi.org/10.1016/j.patcog.2021.107929,0031-3203,2021,107929,116,Pattern Recognition,STDnet-ST: Spatio-temporal ConvNet for small object detection,article,BOSQUET2021107929,
"For signature verification systems, micro deformations can be defined as the small differences in the same strokes of signatures or special writing habits of different signers. These micro deformations can reveal the core distinction between the genuine signatures and skilled forgeries. In this paper, we prove that Convolutional Neural Networks (CNNs) have the potential to extract those micro deformations by max-pooling. More specifically, the micro deformations can be determined by watching the location coordinates of the maximum values in pooling windows of max-pooling. Extensive analysis and experiments demonstrate that it is possible to achieve state-of-the-art performance by using this location information as a new feature for capturing micro deformations, along with convolutional features. The proposed method outperforms the state-of-the-art systems on four publicly available datasets of different languages, i.e., English (GPDSsynthetic, CEDAR), Persian (UTSig), and Hindi (BHSig260).","Offline signature verification, Micro deformations, Max-pooling",Yuchen Zheng and Brian Kenji Iwana and Muhammad Imran Malik and Sheraz Ahmed and Wataru Ohyama and Seiichi Uchida,https://www.sciencedirect.com/science/article/pii/S0031320321001953,https://doi.org/10.1016/j.patcog.2021.108008,0031-3203,2021,108008,118,Pattern Recognition,Learning the micro deformations by max-pooling for offline signature verification,article,ZHENG2021108008,
"Visual Question Answering (VQA) as an important task in understanding vision and language has been proposed and aroused wide interests. In previous VQA methods, Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are generally used to extract visual and textual features respectively, and then the correlation between these two features is explored to infer the answer. However, CNN mainly focuses on extracting local spatial information and RNN pays more attention on exploiting sequential architecture and long-range dependencies. It is difficult for them to integrate the local features with their global dependencies to learn more effective representations of the image and question. To address this problem, we propose a novel model, i.e., Dual Self-Attention with Co-Attention networks (DSACA), for VQA. It aims to model the internal dependencies of both the spatial and sequential structure respectively by using the newly proposed self-attention mechanism. Specifically, DSACA mainly contains three submodules. The visual self-attention module selectively aggregates the visual features at each region by a weighted sum of the features at all positions. The textual self-attention module automatically emphasizes the interdependent word features by integrating associated features among the sentence words. Besides, the visual-textual co-attention module explores the close correlation between visual and textual features learned from self-attention modules. The three modules are integrated into an end-to-end framework to infer the answer. Extensive experiments performed on three generally used VQA datasets confirm the favorable performance of DSACA compared with state-of-the-art methods.","Self-attention, Visual-textual co-attention, Visual question answering",Yun Liu and Xiaoming Zhang and Qianyun Zhang and Chaozhuo Li and Feiran Huang and Xianghong Tang and Zhoujun Li,https://www.sciencedirect.com/science/article/pii/S0031320321001436,https://doi.org/10.1016/j.patcog.2021.107956,0031-3203,2021,107956,117,Pattern Recognition,Dual self-attention with co-attention networks for visual question answering,article,LIU2021107956,
"Attention-based scene text recognizers have gained huge success, which leverages a more compact intermediate representation to learn 1d- or 2d- attention by a RNN-based encoder-decoder architecture. However, such methods suffer from attention-driftproblem because high similarity among encoded features leads to attention confusion under the RNN-based local attention mechanism. Moreover, RNN-based methods have low efficiency due to poor parallelization. To overcome these problems, we propose the MASTER, a self-attention based scene text recognizer that (1) not only encodes the input-output attention but also learns self-attention which encodes feature-feature and target-target relationships inside the encoder and decoder and (2) learns a more powerful and robust intermediate representation to spatial distortion, and (3) owns a great training efficiency because of high training parallelization and a high-speed inference because of an efficient memory-cache mechanism. Extensive experiments on various benchmarks demonstrate the superior performance of our MASTER on both regular and irregular scene text. Pytorch code can be found at https://github.com/wenwenyu/MASTER-pytorch, and Tensorflow code can be found at https://github.com/jiangxiluning/MASTER-TF.","Scene text recognition, Transformer, Non-local network, Memory-cached mechanism",Ning Lu and Wenwen Yu and Xianbiao Qi and Yihao Chen and Ping Gong and Rong Xiao and Xiang Bai,https://www.sciencedirect.com/science/article/pii/S0031320321001679,https://doi.org/10.1016/j.patcog.2021.107980,0031-3203,2021,107980,117,Pattern Recognition,MASTER: Multi-aspect non-local network for scene text recognition,article,LU2021107980,
"The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.","Pruning, Layer-wise relevance propagation (LRP), Convolutional neural network (CNN), Interpretation of models, Explainable AI (XAI)",Seul-Ki Yeom and Philipp Seegerer and Sebastian Lapuschkin and Alexander Binder and Simon Wiedemann and Klaus-Robert MÃ¼ller and Wojciech Samek,https://www.sciencedirect.com/science/article/pii/S0031320321000868,https://doi.org/10.1016/j.patcog.2021.107899,0031-3203,2021,107899,115,Pattern Recognition,Pruning by explaining: A novel criterion for deep neural network pruning,article,YEOM2021107899,
"Clustering aims to partition an input dataset into distinct groups according to some distance or similarity measurements. One of the most widely used clustering method nowadays is the k-means algorithm because of its simplicity and efficiency. In the last few decades, k-means and its various extensions have been formulated to solve the practical clustering problems. However, existing clustering methods are often presented in a single-layer formulation (i.e., shallow formulation). As a result, the mapping between the obtained low-level representation and the original input data may contain rather complex hierarchical information. To overcome the drawbacks of low-level features, deep learning techniques are adopted to extract deep representations and improve the clustering performance. In this paper, we propose a robust deep k-means model to learn the hidden representations associate with different implicit lower-level attributes. By using the deep structure to hierarchically perform k-means, the hierarchical semantics of data can be exploited in a layerwise way. Data samples from the same class are forced to be closer layer by layer, which is beneficial for clustering task. The objective function of our model is derived to a more trackable form such that the optimization problem can be tackled more easily and the final robust results can be obtained. Experimental results over 12 benchmark data sets substantiate that the proposed model achieves a breakthrough in clustering performance, compared with both classical and state-of-the-art methods.","-means algorithm, Robust clustering, Deep learning",Shudong Huang and Zhao Kang and Zenglin Xu and Quanhui Liu,https://www.sciencedirect.com/science/article/pii/S0031320321001837,https://doi.org/10.1016/j.patcog.2021.107996,0031-3203,2021,107996,117,Pattern Recognition,Robust deep k-means: An effective and simple method for data clustering,article,HUANG2021107996,
"Robust and accurate multi-source matching is a difficult task due to significant nonlinear radiometric differences, background clutter, and geometric deformation in corresponding regions. Motivated by these existing problems, a discriminating yet robust combined descriptor for multi-source image matching, called deformed contour segment similarity (DCSS), is proposed in this work. First, the proposed DCSS, which is constructed by histogram of the combined contour features rather than the commonly used corner point and gradient, presents the accurate correspondence between image pairs and improves the descriptive ability to radiometric differences. Second, the deformed curve is presented via a finite-dimensional matrix Lie group to determine the similarity metric with an explicit geodesic solution. The geodesic distance, which indicates the nearest distance between curves in fluid space, is defined as the weight coefficient of the constructed histogram to enhance the robustness of the descriptor. The proposed algorithm utilizes the holistic contour information for the scoring and ranking of the shape similarity hypothesis, which can effectively reduce the influence of partially missing contours. Finally, a precise bilateral matching rule is used to perform the matching between the corresponding contour segments. Some experiments are carried out on various infrared-visible image data sets. The results demonstrate that the proposed DCSS achieves more robust and accurate matching performance than many popular multi-source image matching methods.","Image matching, Multi-source image, Histogram",Quan Wu and Guili Xu and Yuehua Cheng and Zhengsheng Wang and Zhenhua Li,https://www.sciencedirect.com/science/article/pii/S0031320321001552,https://doi.org/10.1016/j.patcog.2021.107968,0031-3203,2021,107968,117,Pattern Recognition,Deformed contour segment matching for multi-source images,article,WU2021107968,
"Cost-sensitive face recognition is a challenging problem in pattern recognition. Due to the high-dimensional face features, cost-sensitive face recognition usually conducts feature extraction in advance, followed by the learning of classifier in reduced subspace. However, the pre-extracted face features are kept fixed and may suboptimal for subsequent classifier learning, which will degrade the final face recognition performance. Besides, most of face learners are cost insensitive. Even the cost-sensitive methods proposed for face recognition, they only incorporate the cost information in feature extraction or classification phase as an alternative. There is no doubt that some cost-sensitive information will be lost in their cost insensitive steps. To deal with these issues, this paper proposes to incorporate feature extraction and classification in a unified cost-sensitive framework for face recognition. The experimental results on three public face benchmarks, including Extended Yale B, CMU PIE and LFW datasets, demonstrate that the proposed method can significantly reduce the overall misclassification loss of face recognition system as well as the classification errors associated with high costs, when comparing with eleven state-of-the-art face learners and nine cost-sensitive methods.","Cost-sensitive, Feature extraction, Classification, Face recognition",Jianwu Wan and Yinjuan Chen and Bing Bai,https://www.sciencedirect.com/science/article/pii/S003132032100114X,https://doi.org/10.1016/j.patcog.2021.107927,0031-3203,2021,107927,115,Pattern Recognition,Joint feature extraction and classification in a unified framework for cost-sensitive face recognition,article,WAN2021107927,
"Biclustering (co-clustering, two-mode clustering), as one of the classical unsupervised learning methods, has been applied in many different fields in recent years. Different types of biclustering methods have been developed such as probabilistic methods, two-way clustering methods, variance minimization methods, and so on. However, few regression-based methods have been proposed to the best of our knowledge. Such methods have been applied in traditional clustering, which can improve both the computational efficiency and the clustering accuracy. In this paper, we present a penalized regression-based method for localizing the biclusters (PRbiclust). By imposing Truncated LASSO Penalty (TLP) and group TLP terms to penalize the column vectors and the row vectors in the regression model, the structure of biclusters in the data matrix is recovered. The model is formulated as an optimization problem with nonconvex penalties, and a computationally efficient algorithm is proposed to solve it. Convergence of the algorithm is proved. To extract the biclusters from the recovered data matrix, we propose a graph-based localization method. An evaluation criterion is also proposed to measure the efficiency of bicluster localization when noise entries exist. We apply the proposed method to both simulated datasets with different setups and a real dataset. Experiments show that this method can well capture the bicluster structure, and performs better than the existing works.","Biclustering, Penalized regression-based model, Alternating direction method of multipliers (ADMM), Difference of convex (DC) programming",Hanjia Gao and Zhengjian Bai and Weiguo Gao and Shuqin Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321001710,https://doi.org/10.1016/j.patcog.2021.107984,0031-3203,2021,107984,117,Pattern Recognition,Penalized -regression-based bicluster localization,article,GAO2021107984,
"Hyperspectral anomaly detection (HAD) is a research endeavor of high practical relevance within remote sensing scene interpretation. In this work, we propose an unsupervised approach, dual feature extraction network (DFEN) for HAD, to gradually build up ever-greater discrimination between the original data and background. In particular, we impose an end-to-end discriminative learning loss on two networks. Among them, adversarial learning aims to keep the original spectrum while Gaussian constrained learning intends to learn the background distribution in the potential space. To extract the anomaly, we calculate spatial and spectral anomaly scores based on mean squared error (MSE) spatial distance and orthogonal projection divergence (OPD) spectral distance between two latent feature matrices. Finally, the comprehensive detection result is obtained by a simple dot product between two domains to further reduce the false alarm rate. Experiments have been conducted on eight real hyperspectral data sets captured by different sensors over different scenes, which show that the proposed DFEN method is superior to other compared methods in detection accuracy or false alarm rate.","Anomaly detection, Hyperspectral image, Adversarial learning, Gaussian constraint learning, Unsupervised learning",Weiying Xie and Jie Lei and Shuo Fang and Yunsong Li and Xiuping Jia and Mingsuo Li,https://www.sciencedirect.com/science/article/pii/S0031320321001795,https://doi.org/10.1016/j.patcog.2021.107992,0031-3203,2021,107992,118,Pattern Recognition,Dual feature extraction network for hyperspectral image analysis,article,XIE2021107992,
"Semantic segmentation is a challenging task which requires both solid unanimous global context and rich spatial information. Recent methods ignore adaptively capturing of valid feature. The lack of useful multi-scale information filtering hinders further explicit feature generation. In this paper, we develop a novel network named GPNet, which can densely capture and filter the multi-scale information in a gated and pair-wise manner. Specifically, a Gated Pyramid Module (GPM) is designed to incorporate dense and growing receptive fields from both low-level and high-level features. In GPM we build a gated path to select useful context among multi-scale information. Moreover, a Cross-Layer Attention Module (CLAM) is proposed to reuse the context information from shallow layers to guide the deep features. Comprehensive experimental evaluations are conducted on popular semantic segmentation benchmarks including Cityscapes and ADE20K. Our GPNet achieves the mIoU score of 82.5% and 45.81% on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results using ResNet-101 as the backbone.","Deep learning, Semantic segmentation, Context embedding, Gated mechanism, Attention",Yu Zhang and Xin Sun and Junyu Dong and Changrui Chen and Qingxuan Lv,https://www.sciencedirect.com/science/article/pii/S0031320321001278,https://doi.org/10.1016/j.patcog.2021.107940,0031-3203,2021,107940,115,Pattern Recognition,GPNet: Gated pyramid network for semantic segmentation,article,ZHANG2021107940,
"Attention mechanism has made great progress in image captioning, where semantic words or local regions are selectively embedded into the language model. However, current attention-based image captioning methods ignore the fine-grained semantic information and their interaction with visual regions. Inspired by the activity of human in describing an image: divergent observation and convergent attention, we propose a novel divergent-convergent attention (DCA) model to tackle the problems of the current attention model in image captioning. In our DCA model, divergent observation is mainly reflected in the multi-perspective inputs: a visual collection coming from object detection and three semantic components of scene graph made of objects, attributes and relations respectively. Then the convergent attention merges these multi-perspective inputs by adaptively deciding which perspective is crucial and which element in the focused perspective dominates in the attention process through a hierarchical structure. Our model also makes use of the interaction between visual objects and semantic components to achieve complementary advantages. Above all, owing to the interaction between divergent visual and semantic components, and the gradual convergence of attention, our model can attend to the corresponding local region more precisely under the guidance of semantic components. Besides, with the assistance of the visual components, the DCA model can effectively utilize the fine-grained semantic components to generate more descriptive sentences. Experiments on the MS COCO dataset demonstrate the superiority of our proposed method.","Image Captioning, Divergent Observation, Convergent Attention",Junzhong Ji and Zhuoran Du and Xiaodan Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321001151,https://doi.org/10.1016/j.patcog.2021.107928,0031-3203,2021,107928,115,Pattern Recognition,Divergent-convergent attention for image captioning,article,JI2021107928,
"VLAD (Vector of Locally Aggregated Descriptors) has been widely adopted in image representation. However, the VLAD algorithm seeks for the algebraic sum of the residue vectors between the descriptors and the centroid of cluster they belong to, and this could decrease the discriminative power of feature representations. To this end, this paper originally proposes a VLAAD (Vector of Locally and Adaptively Aggregated Descriptors) framework to adaptively assign a weight to each residue vector. First, we compute the weights using the magnitude of each residue vector, and encapsulate the weighted VLAD block into ResNet to form an end-to-end Weighted NetVLAD method. To further enhance the discriminative power of the features, we subsequently replace the magnitude-based weight computation with a gating scheme to achieve automatic weight estimation. The enhanced version is named as Gated NetVLAD method. The experimental results on CIFAR-10, MNIST Digits, Pittsburgh Google street view and ImageNet-Dog datasets demonstrate the promotion in classification accuracy and retrieval mAP using VLAAD against several state-of-the-art methods.","VLAD, Deep learning, Weighting scheme, Gating scheme, Feature representation",Jian Zhang and Yunyin Cao and Qun Wu,https://www.sciencedirect.com/science/article/pii/S0031320321001394,https://doi.org/10.1016/j.patcog.2021.107952,0031-3203,2021,107952,116,Pattern Recognition,Vector of Locally and Adaptively Aggregated Descriptors for Image Feature Representation,article,ZHANG2021107952,
"Accurately matching visual and textual data in cross-modal retrieval has been widely studied in the multimedia community. To address these challenges posited by the heterogeneity gap and the semantic gap, we propose integrating Shannon information theory and adversarial learning. In terms of the heterogeneity gap, we integrate modality classification and information entropy maximization adversarially. For this purpose, a modality classifier (as a discriminator) is built to distinguish the text and image modalities according to their different statistical properties. This discriminator uses its output probabilities to compute Shannon information entropy, which measures the uncertainty of the modality classification it performs. Moreover, feature encoders (as a generator) project uni-modal features into a commonly shared space and attempt to fool the discriminator by maximizing its output information entropy. Thus, maximizing information entropy gradually reduces the distribution discrepancy of cross-modal features, thereby achieving a domain confusion state where the discriminator cannot classify two modalities confidently. To reduce the semantic gap, Kullback-Leibler (KL) divergence and bi-directional triplet loss are used to associate the intra- and inter-modality similarity between features in the shared space. Furthermore, a regularization term based on KL-divergence with temperature scaling is used to calibrate the biased label classifier caused by the data imbalance issue. Extensive experiments with four deep models on four benchmarks are conducted to demonstrate the effectiveness of the proposed approach.","Cross-modal retrieval, Shannon information theory, Adversarial learning, Modality uncertainty, Data imbalance",Wei Chen and Yu Liu and Erwin M. Bakker and Michael S. Lew,https://www.sciencedirect.com/science/article/pii/S0031320321001709,https://doi.org/10.1016/j.patcog.2021.107983,0031-3203,2021,107983,117,Pattern Recognition,Integrating information theory and adversarial learning for cross-modal retrieval,article,CHEN2021107983,
"By and large, existing Intellectual Property (IP) protection on deep neural networks typically i) focus on image classification task only, and ii) follow a standard digital watermarking framework that was conventionally used to protect the ownership of multimedia and video content. This paper demonstrates that the current digital watermarking framework is insufficient to protect image captioning tasks that are often regarded as one of the frontiers AI problems. As a remedy, this paper studies and proposes two different embedding schemes in the hidden memory state of a recurrent neural network to protect the image captioning model. From empirical points, we prove that a forged key will yield an unusable image captioning model, defeating the purpose of infringement. To the best of our knowledge, this work is the first to propose ownership protection on image captioning task. Also, extensive experiments show that the proposed method does not compromise the original image captioning performance on all common captioning metrics on Flickr30k and MS-COCO datasets, and at the same time it is able to withstand both removal and ambiguity attacks. Code is available atÂ https://github.com/jianhanlim/ipr-imagecaptioning","Image captioning, Ownership protection, Deep neural network, Recurrent neural network, Long short-term memory",Jian Han Lim and Chee Seng Chan and Kam Woh Ng and Lixin Fan and Qiang Yang,https://www.sciencedirect.com/science/article/pii/S0031320321004659,https://doi.org/10.1016/j.patcog.2021.108285,0031-3203,2022,108285,122,Pattern Recognition,"Protect, show, attend and tell: Empowering image captioning models with ownership protection",article,LIM2022108285,
"Sharing the raw or an abstract representation of a labelled dataset on cloud platforms can potentially expose sensitive information of the data to an adversary, e.g., in the case of an emotion classification task from text, an adversary-agnostic abstract representation of the text data may eventually lead an adversary to identify the demographics of the authors, such as their gender and age. In this paper, we propose a universal defense mechanism against such malicious attempts of stealing sensitive information from data shared on cloud platforms. More specifically, our proposed method employs an informative subspace based multi-objective approach to obtain a sensitive information aware encoding of the data representation. A number of experiments conducted on both standard text and image datasets demonstrate that our proposed approach is able to reduce the effectiveness of the adversarial task (i.e., in other words is able to better protect the sensitive information of the data) without significantly reducing the effectiveness of the primary task itself.","Privacy preserving representation learning, Informative subspace, Multi-objective learning, Defence against information stealing adversarial attacks",Chandan Biswas and Debasis Ganguly and Partha Sarathi Mukherjee and Ujjwal Bhattacharya and Yufang Hou,https://www.sciencedirect.com/science/article/pii/S0031320321004817,https://doi.org/10.1016/j.patcog.2021.108301,0031-3203,2022,108301,122,Pattern Recognition,Privacy-aware supervised classification: An informative subspace based multi-objective approach,article,BISWAS2022108301,
"Obtaining a high dynamic range (HDR) image from multiple low dynamic range images with different exposures is an important step in various computer vision tasks. One of the ongoing challenges in the field is to generate HDR images without ghosting artifacts. Motivated by an observation that such artifacts are particularly noticeable in the gradient domain, in this paper, we propose an HDR imaging approach that aggregates the information from multiple LDR images with guidance from image gradient domain. The proposed method generates artifact-free images by integrating the image gradient information and the image context information in the pixel domain. The context information in a large area helps to reconstruct the contents contaminated by saturation and misalignments. Specifically, an additional gradient stream and the supervision in the gradient domain are applied to incorporate the gradient information in HDR imaging. To use the context information captured from a large area while preserving spatial resolution, we adopt dilated convolutions to extract multi-scale features with rich context information. Moreover, we build a new dataset containing 40 groups of real-world images from diverse scenes with ground truth to validate the proposed model. The samples in the proposed dataset include more challenging moving objects inducing misalignments. Extensive experimental results demonstrate that our proposed model outperforms previous methods on different datasets in terms of both quantitative measure and visual perception quality.","High dynamic range imaging, Deep learning, Exposure fusion, Ghosting artifacts, Image gradients",Qingsen Yan and Dong Gong and Javen Qinfeng Shi and Anton van {den Hengel} and Jinqiu Sun and Yu Zhu and Yanning Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321005227,https://doi.org/10.1016/j.patcog.2021.108342,0031-3203,2022,108342,122,Pattern Recognition,High dynamic range imaging via gradient-aware context aggregation network,article,YAN2022108342,
"X-ray security screening is widely used to maintain aviation/transport security, and its significance poses a particular interest in automated screening systems. This paper aims to review computerised X-ray security imaging algorithms by taxonomising the field into conventional machine learning and contemporary deep learning applications. The first part briefly discusses the classical machine learning approaches utilised within X-ray security imaging, while the latter part thoroughly investigates the use of modern deep learning algorithms. The proposed taxonomy sub-categorises the use of deep learning approaches into supervised and unsupervised learning, with a particular focus on object classification, detection, segmentation and anomaly detection tasks. The paper further explores well-established X-ray datasets and provides a performance benchmark. Based on the current and future trends in deep learning, the paper finally presents a discussion and future directions for X-ray security imagery.","Review, Survey, X-Ray security imaging, Deep learning",Samet Akcay and Toby Breckon,https://www.sciencedirect.com/science/article/pii/S0031320321004258,https://doi.org/10.1016/j.patcog.2021.108245,0031-3203,2022,108245,122,Pattern Recognition,Towards automatic threat detection: A survey of advances of deep learning within X-ray security imaging,article,AKCAY2022108245,
"Face alignment in-the-wild still faces great challenges due to that i) partial occlusion blurs the inter-features spatial relations of faces and ii) traditional CNN makes the network more difficult to capture the spatial positional relations between landmarks. To address the issues above, we propose a face alignment algorithm named Dual-attentional Spatial-aware Capsule Network (DSCN). Firstly, the spatial-aware module builds a more accurate inter-features spatial constrained model with the hourglass capsule network (HGCaps) as the backbone, which can effectively enhance its robustness against occlusions. Then, two sorts of attention mechanisms, namely capsule attention and spatial attention, are added to the attention-guided module to make the network focus more on the advantageous features and suppress other unrelated ones for more effective feature recalibration. Our method achieves 1.08% failure rate on the COFW dataset, which is much lower than the current state-of-the-art algorithms. The mean error under 300W dataset and WFLW dataset are respectively 3.91% and 5.66%, which shows that DSCN is more robust to occlusion and outperforms state-of-the-art methods in the literature.","Face alignment, Hourglass capsule network, Adaptively local constrained dynamic routing, Capsule attention, Spatial attention",Jinyan Ma and Jing Li and Bo Du and Jia Wu and Jun Wan and Yafu Xiao,https://www.sciencedirect.com/science/article/pii/S0031320321004775,https://doi.org/10.1016/j.patcog.2021.108297,0031-3203,2022,108297,122,Pattern Recognition,Robust face alignment by dual-attentional spatial-aware capsule networks,article,MA2022108297,
"Video processing has become a popular research direction in computer vision due to its various applications such as video summarization, action recognition, etc. Recently, deep learning-based methods have achieved impressive results in action recognition. However, these methods need to process a full video sequence to recognize the action, even though many of the frames in the video sequence are similar and non-essential to recognizing a particular action. Additionally, these non-essential frames increase the computational cost and can confuse a method in action recognition. Instead, the important frames called keyframes not only are helpful in recognizing an action but also can reduce the processing time of each video sequence in classification or in other applications, e.g. summarization. As well, current methods in video processing have not yet been demonstrated in an online fashion. Motivated by the above, we propose an online learnable module for keyframe extraction. This module can be used to select key shots in video and thus, can be applied to video summarization. The extracted keyframes can be used as input to any deep learning-based classification model to recognize action. We also propose a plugin module to use the semantic word vector as input along with keyframes and a novel train/test strategy for the classification models. To our best knowledge, this is the first time such an online module and train/test strategy have been proposed. The experimental results on many commonly used datasets in video summarization and in action recognition have demonstrated the effectiveness of the proposed module.","Online keyframes, Learnable threshold, Video summarization, Action recognition",G M Mashrur {E Elahi} and Yee-Hong Yang,https://www.sciencedirect.com/science/article/pii/S0031320321004532,https://doi.org/10.1016/j.patcog.2021.108273,0031-3203,2022,108273,122,Pattern Recognition,Online learnable keyframe extraction in videos and its application with semantic word vector in action recognition,article,EELAHI2022108273,
"Moment invariants have been successfully applied to pattern detection tasks in 2D and 3D scalar, vector, and matrix valued data. However so far no flexible basis of invariants exists, i.e., no set that is optimal in the sense that it is complete and independent for every input pattern. In this paper, we prove that a basis of moment invariants can be generated that consists of tensor contractions of not more than two different moment tensors each under the conjecture of the set of all possible tensor contractions to be complete. This result allows us to derive the first generator algorithm that produces flexible bases of moment invariants with respect to orthogonal transformations by selecting a single non-zero moment to pair with all others in these two-factor products. Since at least one non-zero moment can be found in every non-zero pattern, this approach always generates a complete set of descriptors.","Pattern detection, Rotation invariant, Moment invariants, Generator approach, Basis, Flexible, Vector, Tensor",Roxana Bujack and Xinhua Zhang and TomÃ¡Å¡ Suk and David Rogers,https://www.sciencedirect.com/science/article/pii/S0031320321004933,https://doi.org/10.1016/j.patcog.2021.108313,0031-3203,2022,108313,123,Pattern Recognition,Systematic generation of moment invariant bases for 2D and 3D tensor fields,article,BUJACK2022108313,
"Sequential data clustering is a challenging task in data mining (e.g., motion recognition and video segmentation). For good performance in dealing with complex local correlation and high-dimensional structure of sequential data, representation based methods have become one of the hot topics for sequential data clustering, in which subspace clustering is a representative tool. Subspace clustering methods divide the sequence into disjoint segments according to a locally continuous and connected representation of raw data. Although the subspace clustering methods maintain the successive property of sequential data well, there exist redundant connections in the intersection of two subsequences, which will destroy the integrity of a cluster and easily cause the chained partition of the sequence. So it is necessary to learn a more specific structure representation of a sequence to preserves both sequential information and efficient connections. Besides, the representation that conducive to clustering should have sparsity and connectivity under some assumptions. To this end, we propose a novel method to learn the support structure representation of sequence, which can extract sufficient information about instances and get the compact structure of sequential data. Furthermore, a new subspace clustering method is proposed based on the representation based method. Theoretical analysis and experimental results show the effectiveness of the proposed method.","Sequential data, Clustering, Support structure representation",Xiumei Wang and Dingning Guo and Peitao Cheng,https://www.sciencedirect.com/science/article/pii/S0031320321005069,https://doi.org/10.1016/j.patcog.2021.108326,0031-3203,2022,108326,122,Pattern Recognition,Support structure representation learning for sequential data clustering,article,WANG2022108326,
"X-ray imagery security screening is an essential component of transportation and logistics. In recent years, some researchers have used computer vision algorithms to replace inefficient and tedious manual baggage inspection. However, X-ray images are complicated, and objects overlap with one another in a semi-transparent state, which underperforms the existing object detection frameworks. To solve the severe overlapping problem of X-ray images, we propose a foreground and background separation (FBS) X-ray prohibited item detection framework, which separates prohibited items from other items to exclude irrelevant information. First, we design a target foreground and use recursive training to adaptively approximate the real foreground. Thereafter, with the constraints of X-ray imaging characteristics, a decoder is employed to separate the prohibited items from other irrelevant items to obtain the foreground and background (FB). Finally, we use the attention module to make the detection framework focus more on the foreground. Our method is evaluated on a synthetic dataset with FB ground truth and two public datasets with only bounding box annotations. Extensive experimental results demonstrate that our method significantly outperforms state-of-the-art solutions. Furthermore, experiments are performed in the case where only a small number of images contain the FB ground truth. The results indicate that our method requires only a small number of FB ground truths to obtain a performance equivalent to that of all FB ground truths.","X-ray imagery, Object detection, Foreground and background separation (FBS), Recursive training",Fangtao Shao and Jing Liu and Peng Wu and Zhiwei Yang and Zhaoyang Wu,https://www.sciencedirect.com/science/article/pii/S0031320321004416,https://doi.org/10.1016/j.patcog.2021.108261,0031-3203,2022,108261,122,Pattern Recognition,Exploiting foreground and background separation for prohibited item detection in overlapping X-Ray images,article,SHAO2022108261,
"Face recognition has a wide range of applications like video surveillance, security, access control, etc. Over the past decade, the field of face recognition has matured and grown at par with the latest advancements in technology, particularly deep learning. Convolution Neural Networks have surpassed human accuracy in Face Recognition on popular evaluation tests such as LFW. However, most existing models evaluate their performance with an assumption of the availability of full facial information. The COVID-19 pandemic has laid forth challenges to this assumption, and to the performance of existing methods and leading-edge algorithms in the field of face recognition. This is in the wake of an explosive increase in the number of people wearing face masks. The reduced amount of facial information available to a recognition system from a masked face impacts their discrimination ability. In this context, we design and conduct a series of experiments comparing the masked face recognition performances of CNN architectures available in literature and exploring possible alterations in loss functions, architectures, and training methods that can enable existing methods to fully extract and leverage the limited facial information available in a masked face. We evaluate existing CNN-based face recognition systems for their performance against datasets composed entirely of masked faces, in contrast to the existing standard evaluations where masked or occluded faces are a rare occurrence. The study also presents evidence denoting an increased impact of network depth on performance compared to standard face recognition. Our observations indicate that substantial performance gains can be achieved by the introduction of masked faces in the training set. The study also inferred that various parameter settings determined suitable for standard face recognition are not ideal for masked face recognition. Through empirical analysis we derived new value recommendations for these parameters and settings.","Face recognition, Convolutional neural networks, Masked face, COVID-19",Govind Jeevan and Geevar C. Zacharias and Madhu S. Nair and Jeny Rajan,https://www.sciencedirect.com/science/article/pii/S003132032100488X,https://doi.org/10.1016/j.patcog.2021.108308,0031-3203,2022,108308,122,Pattern Recognition,An empirical study of the impact of masks on face recognition,article,JEEVAN2022108308,
"Handwritten document images can be highly affected by degradation for different reasons: Paper ageing, daily-life scenarios (wrinkles, dust, etc.), bad scanning process and so on. These artifacts raise many readability issues for current Handwritten Text Recognition (HTR) algorithms and severely devalue their efficiency. In this paper, we propose an end to end architecture based on Generative Adversarial Networks (GANs) to recover the degraded documents into a clean and readable form. Unlike the most well-known document binarization methods, which try to improve the visual quality of the degraded document, the proposed architecture integrates a handwritten text recognizer that promotes the generated document image to be more readable. To the best of our knowledge, this is the first work to use the text information while binarizing handwritten documents. Extensive experiments conducted on degraded Arabic and Latin handwritten documents demonstrate the usefulness of integrating the recognizer within the GAN architecture, which improves both the visual quality and the readability of the degraded document images. Moreover, we outperform the state of the art in H-DIBCO challenges, after fine tuning our pre-trained model with synthetically degraded Latin handwritten images, on this task.","Handwritten document image binarization, Document enhancement, Handwriting text recognition, Generative adversarial networks, Recurrent neural networks",Sana {Khamekhem Jemni} and Mohamed Ali Souibgui and Yousri Kessentini and Alicia FornÃ©s,https://www.sciencedirect.com/science/article/pii/S0031320321005501,https://doi.org/10.1016/j.patcog.2021.108370,0031-3203,2022,108370,123,Pattern Recognition,Enhance to read better: A Multi-Task Adversarial Network for Handwritten Document Image Enhancement,article,KHAMEKHEMJEMNI2022108370,
"How to improve the representational power of visual features extracted by deep convolutional neural networks is of crucial importance for high-quality image super-resolution. To address this issue, we propose a multi-attention augmented network, which mainly consists of content-, orientation- and position-aware modules. Specifically, we develop an attention augmented U-net structure to form the content-aware module in order to learn and combine multi-scale informative features within a large receptive field. To better reconstruct image details in different directions, we design a set of pre-defined sparse kernels to construct the orientation-aware module, which can extract more representative multi-orientation features and enhance the discriminative capacity in stacked convolutional stages. Then these extracted features are adaptively fused through channel attention mechanism. In upscale stage, the position-aware module adopts a novel self-attention to reweight the element-wise value of final low-resolution feature maps, for further suppressing the possible artifacts. Experimental results demonstrate that our method obtains better reconstruction accuracy and perceptual quality against state-of-the-art methods.","Super-resolution, Multi-scale U-net, pre-defined sparse kernels, Attention mechanism",Rui Chen and Heng Zhang and Jixin Liu,https://www.sciencedirect.com/science/article/pii/S003132032100529X,https://doi.org/10.1016/j.patcog.2021.108349,0031-3203,2022,108349,122,Pattern Recognition,Multi-attention augmented network for single image super-resolution,article,CHEN2022108349,
"Although Convolution Neural Networks (CNN) have achieved great success in many applications of computer vision in recent years, rotation invariance is still a difficult problem for CNN. Especially for some images, the content can appear in the image at any angle of rotation, such as medical images, microscopic images, remote sensing images and astronomical images. In this paper, we propose a novel convolution operation, called Gradient-Aligned Convolution (GAConv), which can help CNN achieve rotation invariance by replacing vanilla convolutions in CNN. GAConv is implemented with a prior pixel-level gradient alignment operation before regular convolution. With GAConv, Gradient-Aligned CNN (GACNN) can achieve rotation invariance without any data augmentation, feature-map augmentation, and filter enrichment. In GACNN, rotation invariance does not learn from the training set, but bases on the network model. Different from the vanilla CNN, GACNN will output invariant results for all rotated versions of an object, no matter whether the network is trained or not. This means that we only need to train the network with one canonical version of the object and all other rotated versions of this object should be recognized with the same accuracy. Classification experiments have been conducted to evaluate GACNN compared with some rotation invariant approaches. GACNN achieved the best results on the 360â rotated test set of MNIST-rotation, Plankton-sub-rotation, and Galaxy Zoo 2.","Gradient alignment, Rotation equivariant convolution, Rotation invariant neural network",You Hao and Ping Hu and Shirui Li and Jayaram K. Udupa and Yubing Tong and Hua Li,https://www.sciencedirect.com/science/article/pii/S0031320321005343,https://doi.org/10.1016/j.patcog.2021.108354,0031-3203,2022,108354,122,Pattern Recognition,Gradient-Aligned convolution neural network,article,HAO2022108354,
"Lung cancer classification in screening computed tomography (CT) scans is one of the most crucial tasks for early detection of this disease. Many lives can be saved if we are able to accurately classify malignant/cancerous lung nodules. Consequently, several deep learning based models have been proposed recently to classify lung nodules as malignant or benign. Nevertheless, the large variation in the size and heterogeneous appearance of the nodules makes this task an extremely challenging one. We propose a new Progressive Growing Channel Attentive Non-Local (ProCAN) network for lung nodule classification. The proposed method addresses this challenge from three different aspects. First, we enrich the Non-Local network by adding channel-wise attention capability to it. Second, we apply Curriculum Learning principles, whereby we first train our model on easy examples before hard ones. Third, as the classification task gets harder during the Curriculum learning, our model is progressively grown to increase its capability of handling the task at hand. We examined our proposed method on two different public datasets and compared its performance with state-of-the-art methods in the literature. The results show that the ProCAN model outperforms state-of-the-art methods and achieves an AUC of 98.05% and an accuracy of 95.28% on the LIDC-IDRI dataset. Moreover, we conducted extensive ablation studies to analyze the contribution and effects of each new component of our proposed method.","Self-Attention, Non-local network, Nodule classification, Curriculum learning, Deep learning",Mundher Al-Shabi and Kelvin Shak and Maxine Tan,https://www.sciencedirect.com/science/article/pii/S0031320321004891,https://doi.org/10.1016/j.patcog.2021.108309,0031-3203,2022,108309,122,Pattern Recognition,ProCAN: Progressive growing channel attentive non-local network for lung nodule classification,article,ALSHABI2022108309,
"Ball possession statistics in a soccer match is evaluated by counting the number of valid passes by both teams. The valid passes are determined by monitoring the start and end of a ball passing event initiated by a player. In this work, we map pass detection as detection of split and merge of nodes of a flow network. The players and ball represent nodes in the network. A group is formed by the objects (ball and players) which are spatially close to each other. Objects belonging to the same group are allowed to split or merge. We use this group relation to check if the objects split or merge in the sequence of frames. A constraint is added to the network to make sure that two objects can split only if the objects were previously merged. Flow through the split or merge node of the network denotes a ball pass event. Additional nodes like appear and disappear are added to the network to map the possibility that new objects could appear or old objects may disappear to and from the frame. The minimum cost path in the flow network provides the solution for valid pass events. Experimental evaluation shows that our proposal is at least 4% better in estimating ball possession statistics and 8% better in pass detection of a soccer match seen in a broadcast video than that of competitive methods.","Ball possession statistics, Cost-flow network, Group similarity, Pass event detection, Soccer",Saikat Sarkar and Dipti Prasad Mukherjee and Amlan Chakrabarti,https://www.sciencedirect.com/science/article/pii/S0031320321005185,https://doi.org/10.1016/j.patcog.2021.108338,0031-3203,2022,108338,122,Pattern Recognition,From soccer video to ball possession statistics,article,SARKAR2022108338,
"Long-tailed distribution in the dataset is one of the major problems of the scene graph generation task. Previous methods attempt to alleviate this by introducing human commonsense knowledge in the form of statistical correlations between object pairs. However, the reasoning path they used is usually composable and the prior knowledge they employed is generally image-specific, making the knowledge learning less flexible, stable and holistic. In this paper, we propose Atom Correlation Based Graph Propagation (AC-GP) for the scene graph generation task. Specifically, diverse atom correlations between objects and their relationships are explored by separating relationships to form new semantic nodes and decomposing the compound reasoning paths. Based on these atom correlations, the knowledge graphs are introduced for the feature enhancement by information propagating in the global category space. By exploiting atom correlations, the introduced prior knowledge can be more common and easy to learn. Moreover, propagating the knowledge in the global category space enables the model aware of more comprehensive and holistic knowledge. As a result, the model capacity and stability can be effectively improved to mine infrequent and missed relationships. Experimental results on two benchmark datasets: Visual Relation Detection (VRD) and Visual Genome (VG) show the superiority of the proposed AC-GP over strong baseline methods.","Scene graph generation, Long-tailed distribution, Knowledge graph, Atom correlation, Category space",Bingqian Lin and Yi Zhu and Xiaodan Liang,https://www.sciencedirect.com/science/article/pii/S0031320321004805,https://doi.org/10.1016/j.patcog.2021.108300,0031-3203,2022,108300,122,Pattern Recognition,Atom correlation based graph propagation for scene graph generation,article,LIN2022108300,
"In this paper, we present a weighted tensor Schatten-p quasi-norm (0<p<1) regularizer for 3D array datasets in order to recover the low-rank part and the sparse part, respectively. Corresponding algorithms associated with augmented Lagrangian multipliers are established and the constructed sequence converges to the desirable Karush-Kuhn-Tucker (KKT) point, which is mathematically validated in detail. Although the proposed weighted tensor Schatten-p quasi-norm is non-convex, it appears not only to less penalize the singular values but also to be effective in capturing the low-rank property. The main findings in this paper are the appropriate choice of p depends on specific tasks: low-rank data set recovery usually requires relatively large value of p, while sparse data set recovery needs relatively small value of p. And the weights chosen in our tensor Schatten-p quasi-norm are inversely to the singular values exponentially for promoting the sensitivity to different singular values. Experimental results for video inpainting (tensor completion), image recovery and salient object detection (tensor robust principal component analysis) have been shown that the proposed approach outperforms various latest approaches in literature.","Tensor completion, Tensor robust principle component analysis, T-SVD, Tensor nuclear norm, Weighted tensor schatten- quasi-norm",Ming Yang and Qilun Luo and Wen Li and Mingqing Xiao,https://www.sciencedirect.com/science/article/pii/S003132032100491X,https://doi.org/10.1016/j.patcog.2021.108311,0031-3203,2022,108311,122,Pattern Recognition,Nonconvex 3D array image data recovery and pattern recognition under tensor framework,article,YANG2022108311,
"Current synthetic aperture radar (SAR) ship classifiers using convolutional neural networks (CNNs) offer state-of-the-art performance. Yet, they still have two defects potentially hindering accuracy progress â polarization insufficient utilization and traditional feature abandonment. Therefore, we propose a polarization fusion network with geometric feature embedding (PFGFE-Net) to solve them. PFGFE-Net achieves the polarization fusion (PF) from the input data, feature-level, and decision-level. Moreover, the geometric feature embedding (GFE) enriches expert experience. Results on OpenSARShip reveal PFGFE-Net's excellent performance.","Synthetic aperture radar (SAR), Ship classification, Convolutional neural network, Polarization fusion (PF), Geometric feature embedding (GFE)",Tianwen Zhang and Xiaoling Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321005458,https://doi.org/10.1016/j.patcog.2021.108365,0031-3203,2022,108365,123,Pattern Recognition,A polarization fusion network with geometric feature embedding for SAR ship classification,article,ZHANG2022108365,
"Few-Shot Learning (FSL) is a challenging and practical learning pattern, aiming to solve a target task which has only a few labeled examples. Currently, the field of FSL has made great progress, but largely in the supervised setting, where a large auxiliary labeled dataset is required for offline training. However, the unsupervised FSL (UFSL) problem where the auxiliary dataset is fully unlabeled has been seldom investigated despite of its significant value. This paper focuses on the more general and challenging UFSL problem and presents a novel method named Coarse-to-Fine Pseudo Supervision-guided Meta-Learning (C2FPS-ML) for unsupervised few-shot object classification. It first obtains prior knowledge from an unlabeled auxiliary dataset during unsupervised meta-training, and then use the prior knowledge to assist the downstream few-shot classification task. Coarse-to-Fine Pseudo Supervisions in C2FPS-ML aim to optimize meta-task sampling process in unsupervised meta-training stage which is one of the dominant factors for improving the performance of meta-learning based FSL algorithms. Human can learn new concepts progressively or hierarchically following the coarse-to-fine manners. By simulating this humanâs behaviour, we develop two versions of C2FPS-ML for two different scenarios: one is natural object dataset and another one is other kinds of dataset (e.g., handwritten character dataset). For natural object dataset scenario, we propose to exploit the potential hierarchical semantics of the unlabeled auxiliary dataset to build a tree-like structure of visual concepts. For another scenario, progressive pseudo supervision is obtained by forming clusters in different similarity aspects and is represented by a pyramid-like structure. The obtained structure is applied as the supervision to construct meta-tasks in meta-training stage, and prior knowledge from the unlabeled auxiliary dataset is learned from the coarse-grained level to the fine-grained level. The proposed method sets the new state of the art on the gold-standard miniImageNet and achieves remarkable results on Omniglot while simultaneously increases efficiency.","Unsupervised few-shot learning, Meta-learning, Clustering, Object classification",Yawen Cui and Qing Liao and Dewen Hu and Wei An and Li Liu,https://www.sciencedirect.com/science/article/pii/S0031320321004763,https://doi.org/10.1016/j.patcog.2021.108296,0031-3203,2022,108296,122,Pattern Recognition,Coarse-to-fine pseudo supervision guided meta-task optimization for few-shot object classification,article,CUI2022108296,
"With increasing number of COVID-19 cases globally, all the countries are ramping up the testing numbers. While the RT-PCR kits are available in sufficient quantity in several countries, others are facing challenges with limited availability of testing kits and processing centers in remote areas. This has motivated researchers to find alternate methods of testing which are reliable, easily accessible and faster. Chest X-Ray is one of the modalities that is gaining acceptance as a screening modality. Towards this direction, the paper has two primary contributions. Firstly, we present the COVID-19 Multi-Task Network (COMiT-Net) which is an automated end-to-end network for COVID-19 screening. The proposed network not only predicts whether the CXR has COVID-19 features present or not, it also performs semantic segmentation of the regions of interest to make the model explainable. Secondly, with the help of medical professionals, we manually annotate the lung regions and semantic segmentation of COVID19 symptoms in CXRs taken from the ChestXray-14, CheXpert, and a consolidated COVID-19 dataset. These annotations will be released to the research community. Experiments performed with more than 2500 frontal CXR images show that at 90% specificity, the proposed COMiT-Net yields 96.80% sensitivity.","X-Ray, COVID-19, Detection, Diagnostics, Deep learning, Explainable artificial intelligence, Multi-task learning",Aakarsh Malhotra and Surbhi Mittal and Puspita Majumdar and Saheb Chhabra and Kartik Thakral and Mayank Vatsa and Richa Singh and Santanu Chaudhury and Ashwin Pudrod and Anjali Agrawal,https://www.sciencedirect.com/science/article/pii/S0031320321004246,https://doi.org/10.1016/j.patcog.2021.108243,0031-3203,2022,108243,122,Pattern Recognition,Multi-task driven explainable diagnosis of COVID-19 using chest X-ray images,article,MALHOTRA2022108243,
"In reinforcement learning (RL), the intrinsic reward estimation is necessary for policy learning when the extrinsic reward is sparse or absent. To this end, Unified Curiosity-driven Learning with Smoothed intrinsic reward Estimation (UCLSE) is proposed to address the sparse extrinsic reward problem from the perspective of completeness of intrinsic reward estimation. We further propose state distribution-aware weighting method and policy-aware weighting method to dynamically unify two mainstream intrinsic reward estimation methods. In this way, the agent can explore the environment more effectively and efficiently. Under this framework, we propose to employ an attention module to extract task-relevant features for a more precise estimation of intrinsic reward. Moreover, we propose to improve the robustness of policy learning by smoothing the intrinsic reward with a batch of transitions close to the current transition. Extensive experimental results on Atari games demonstrate that our method outperforms the state-of-the-art approaches in terms of both score and training efficiency.","Reinforcement learning, Unified curiosity-driven exploration, Robust intrinsic reward, Task-relevant feature",Fuxian Huang and Weichao Li and Jiabao Cui and Yongjian Fu and Xi Li,https://www.sciencedirect.com/science/article/pii/S003132032100532X,https://doi.org/10.1016/j.patcog.2021.108352,0031-3203,2022,108352,123,Pattern Recognition,Unified curiosity-Driven learning with smoothed intrinsic reward estimation,article,HUANG2022108352,
"Deciding on the unimodality of a dataset is an important problem in data analysis and statistical modeling. It allows to obtain knowledge about the structure of the dataset, i.e. whether data points have been generated by a probability distribution with a single or more than one peaks. Such knowledge is very useful for several data analysis problems, such as for deciding on the number of clusters and determining unimodal projections. We propose a technique called UU-test (Unimodal Uniform test) to decide on the unimodality of a one-dimensional dataset. The method operates on the empirical cumulative density function (ecdf) of the dataset. It attempts to build a piecewise linear approximation of the ecdf that is unimodal and models the data sufficiently in the sense that the data corresponding to each linear segment follows the uniform distribution. A unique feature of this approach is that in the case of unimodality, it also provides a statistical model of the data in the form of a Uniform Mixture Model. We present experimental results in order to assess the ability of the method to decide on unimodality and perform comparisons with the well-known dip-test approach. In addition, in the case of unimodal datasets we evaluate the Uniform Mixture Models provided by the proposed method using the test set log-likelihood and the two-sample Kolmogorov-Smirnov (KS) test.","Unimodal data, Unimodality test, Statistical modeling, Uniform mixture model",Paraskevi Chasani and Aristidis Likas,https://www.sciencedirect.com/science/article/pii/S0031320321004520,https://doi.org/10.1016/j.patcog.2021.108272,0031-3203,2022,108272,122,Pattern Recognition,The UU-test for statistical modeling of unimodal data,article,CHASANI2022108272,
"Convolutional layers convolve the input feature maps to generate valuable output features, and they help deep learning methods significantly in solving complex problems. In order to tackle problems efficiently, deep learning solutions should ensure that the parameters of the model do not increase significantly with the complexity of the problem. Pointwise convolutions are primarily used for parameter reduction in many deep learning architectures. They are convolutional filters of kernel size 1Ã1. The pointwise convolution, however, ignores the spatial information around the points it is processing. This design is by choice, in order to reduce the overall parameters and computations. However, we hypothesize that this shortcoming of pointwise convolution has a significant impact on network performance. We propose a novel alternative design for pointwise convolution, which uses spatial information from the input efficiently. Our approach extracts spatial context information from the input at two scales and further refines the extracted context based on the channel importance. Finally, we add the refined context to the output of the pointwise convolution. This is the first work that improves pointwise convolution by incorporating context information. Our design significantly improves the performance of the networks without substantially increasing the number of parameters and computations. We perform experiments on coarse/fine-grained image classification, few-shot fine-grained classification, and on object detection. We further perform various ablation experiments to validate the significance of the different components used in our design. Lastly, we show experimentally that our proposed technique can be combined with existing state-of-the-art network performance improvement approaches to further improve the network performance.","Contextual pointwise convolution, Convolutional neural network (CNN), Image classification, Deep learning",Pravendra Singh and Pratik Mazumder and Vinay P. Namboodiri,https://www.sciencedirect.com/science/article/pii/S0031320321004647,https://doi.org/10.1016/j.patcog.2021.108284,0031-3203,2022,108284,122,Pattern Recognition,Context extraction module for deep convolutional neural networks,article,SINGH2022108284,
"In this paper, we propose a multiscale hierarchical attention approach for supervised video summarization. Different from most existing supervised methods which employ bidirectional long short-term memory networks, our method exploits the underlying hierarchical structure of video sequences and learns both the short-range and long-range temporal representations via a intra-block and a inter-block attention. Specifically, we first separate each video sequence into blocks of equal length and employ the intra-block and inter-block attention to learn local and global information, respectively. Then, we integrate the frame-level, block-level, and video-level representations for the frame-level importance score prediction. Next, we conduct shot segmentation and compute shot-level importance scores. Finally, we perform key shot selection to produce video summaries. Moreover, we extend our method into a two-stream framework, where appearance and motion information is leveraged. Experimental results on the SumMe and TVSum datasets validate the effectiveness of our method against state-of-the-art methods.","Video summarization, Hierarchical structure, Attention models, Multiscale temporal representation, Two-stream framework",Wencheng Zhu and Jiwen Lu and Yucheng Han and Jie Zhou,https://www.sciencedirect.com/science/article/pii/S0031320321004921,https://doi.org/10.1016/j.patcog.2021.108312,0031-3203,2022,108312,122,Pattern Recognition,Learning multiscale hierarchical attention for video summarization,article,ZHU2022108312,
"Segmentation of infections from CT scans is important for accurate diagnosis and follow-up in tackling the COVID-19. Although the convolutional neural network has great potential to automate the segmentation task, most existing deep learning-based infection segmentation methods require fully annotated ground-truth labels for training, which is time-consuming and labor-intensive. This paper proposed a novel weakly supervised segmentation method for COVID-19 infections in CT slices, which only requires scribble supervision and is enhanced with the uncertainty-aware self-ensembling and transformation-consistent techniques. Specifically, to deal with the difficulty caused by the shortage of supervision, an uncertainty-aware mean teacher is incorporated into the scribble-based segmentation method, encouraging the segmentation predictions to be consistent under different perturbations for an input image. This mean teacher model can guide the student model to be trained using information in images without requiring manual annotations. On the other hand, considering the output of the mean teacher contains both correct and unreliable predictions, equally treating each prediction in the teacher model may degrade the performance of the student network. To alleviate this problem, the pixel level uncertainty measure on the predictions of the teacher model is calculated, and then the student model is only guided by reliable predictions from the teacher model. To further regularize the network, a transformation-consistent strategy is also incorporated, which requires the prediction to follow the same transformation if a transform is performed on an input image of the network. The proposed method has been evaluated on two public datasets and one local dataset. The experimental results demonstrate that the proposed method is more effective than other weakly supervised methods and achieves similar performance as those fully supervised.","COVID-19, infection segmentation, weakly supervised learning, transformation consistency, uncertainty",Xiaoming Liu and Quan Yuan and Yaozong Gao and Kelei He and Shuo Wang and Xiao Tang and Jinshan Tang and Dinggang Shen,https://www.sciencedirect.com/science/article/pii/S0031320321005215,https://doi.org/10.1016/j.patcog.2021.108341,0031-3203,2022,108341,122,Pattern Recognition,Weakly Supervised Segmentation of COVID19 Infection with Scribble Annotation on CT Images,article,LIU2022108341,
"Cutting plane methods play a significant role in modern solvers for tackling mixed-integer programming (MIP) problems. Proper selection of cuts would remove infeasible solutions in the early stage, thus largely reducing the computational burden without hurting the solution accuracy. However, the major cut selection approaches heavily rely on heuristics, which strongly depend on the specific problem at hand and thus limit their generalization capability. In this paper, we propose a data-driven and generalizable cut selection approach, named Cut Ranking, in the settings of multiple instance learning. To measure the quality of the candidate cuts, a scoring function, which takes the instance-specific cut features as inputs, is trained and applied in cut ranking and selection. In order to evaluate our method, we conduct extensive experiments on both synthetic datasets and real-world datasets. Compared with commonly used heuristics for cut selection, the learning-based policy has shown to be more effective, and is capable of generalizing over multiple problems with different properties. Cut Ranking has been deployed in an industrial solver for large-scale MIPs. In the online A/B testing of the product planning problems with more than 107 variables and constraints daily, Cut Ranking has achieved the average speedup ratio of 12.42% over the production solver without any accuracy loss of solution.","Mixed-Integer programming, Cutting plane, Multiple instance learning, Generalization ability",Zeren Huang and Kerong Wang and Furui Liu and Hui-Ling Zhen and Weinan Zhang and Mingxuan Yuan and Jianye Hao and Yong Yu and Jun Wang,https://www.sciencedirect.com/science/article/pii/S0031320321005331,https://doi.org/10.1016/j.patcog.2021.108353,0031-3203,2022,108353,123,Pattern Recognition,Learning to select cuts for efficient mixed-integer programming,article,HUANG2022108353,
"In facial landmark localization tasks, various occlusions heavily degrade the localization accuracy due to the partial observability of facial features. This paper proposes a structural relation network (SRN) for occlusion-robust landmark localization. Unlike most existing methods that simply exploit the shape constraint, the proposed SRN aims to capture the structural relations among different facial components. These relations can be considered a more powerful shape constraint against occlusion. To achieve this, a hierarchical structural relation module (HSRM) is designed to hierarchically reason the structural relations that represent both long- and short-distance spatial dependencies. Compared with existing network architectures,the HSRM can efficiently model the spatial relations by leveraging its geometry-aware network architecture, which reduces the semantic ambiguity caused by occlusion. Moreover, the SRN augments the training data by synthesizing occluded faces. To further extend our SRN for occluded video data, we formulate the occluded face synthesis as a Markov decision process (MDP). Specifically, it plans the movement of the dynamic occlusion based on an accumulated reward associated with the performance degradation of the pre-trained SRN. This procedure augments hard samples for robust facial landmark tracking. Extensive experimental results indicate that the proposed method achieves outstanding performance on occluded and masked faces. Code is available at https://github.com/zhuccly/SRN","Facial landmark localization, Relational reasoning, Long short-distance dependency, Biometrics",Congcong Zhu and Xiaoqiang Li and Jide Li and Songmin Dai and Weiqin Tong,https://www.sciencedirect.com/science/article/pii/S0031320321005057,https://doi.org/10.1016/j.patcog.2021.108325,0031-3203,2022,108325,122,Pattern Recognition,Reasoning structural relation for occlusion-robust facial landmark localization,article,ZHU2022108325,
"Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vector- or matrix-based, and cannot handle tensorial data directly. In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for high-dimensional image classification with very small number of training samples. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TT-based kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. This reduces the storage and computation complexity of kernel matrix construction from exponential to polynomial. The validity proof and computation complexity of the proposed TT-based kernel functions are provided elaborately. Extensive experiments are performed on high-dimensional fMRI and color images datasets, which demonstrates the superiority of the proposed scheme compared with the state-of-the-art techniques.","Image classification, Tensor, Support tensor machine",Cong Chen and Kim Batselier and Wenjian Yu and Ngai Wong,https://www.sciencedirect.com/science/article/pii/S0031320321005173,https://doi.org/10.1016/j.patcog.2021.108337,0031-3203,2022,108337,122,Pattern Recognition,Kernelized support tensor train machines,article,CHEN2022108337,
"We extend our previous work on Inductive Conformal Prediction (ICP) for multi-label text classification and present a novel approach for addressing the computational inefficiency of the Label Powerset (LP) ICP, arrising when dealing with a high number of unique labels. We present experimental results using the original and the proposed efficient LP-ICP on two English and one Czech language data-sets. Specifically, we apply the LP-ICP on three deep Artificial Neural Network (ANN) classifiers of two types: one based on contextualised (bert) and two on non-contextualised (word2vec) word-embeddings. In the LP-ICP setting we assign nonconformity scores to label-sets from which the corresponding p-values and prediction-sets are determined. Our approach deals with the increased computational burden of LP by eliminating from consideration a significant number of label-sets that will surely have p-values below the specified significance level. This reduces dramatically the computational complexity of the approach while fully respecting the standard CP guarantees. Our experimental results show that the contextualised-based classifier surpasses the non-contextualised-based ones and obtains state-of-the-art performance for all data-sets examined. The good performance of the underlying classifiers is carried on to their ICP counterparts without any significant accuracy loss, but with the added benefits of ICP, i.e. the confidence information encapsulated in the prediction sets. We experimentally demonstrate that the resulting prediction sets can be tight enough to be practically useful even though the set of all possible label-sets contains more than 1e+16 combinations. Additionally, the empirical error rates of the obtained prediction-sets confirm that our outputs are well-calibrated.","Text classification, Multi-label, Word2vec, Bert, Conformal prediction, Label powerset, Computational efficiency, Nonconformity measure, Confidence measure",Lysimachos Maltoudoglou and Andreas Paisios and Ladislav Lenc and JiÅÃ­ MartÃ­nek and Pavel KrÃ¡l and Harris Papadopoulos,https://www.sciencedirect.com/science/article/pii/S0031320321004519,https://doi.org/10.1016/j.patcog.2021.108271,0031-3203,2022,108271,122,Pattern Recognition,Well-calibrated confidence measures for multi-label text classification with a large number of labels,article,MALTOUDOGLOU2022108271,
"Class imbalance is an inherent characteristic of multi-label data that hinders most multi-label learning methods. One efficient and flexible strategy to deal with this problem is to employ sampling techniques before training a multi-label learning model. Although existing multi-label sampling approaches alleviate the global imbalance of multi-label datasets, it is actually the imbalance level within the local neighbourhood of minority class examples that plays a key role in performance degradation. To address this issue, we propose a novel measure to assess the local label imbalance of multi-label datasets, as well as two multi-label sampling approaches, namely Multi-Label Synthetic Oversampling based on Local label imbalance (MLSOL) and Multi-Label Undersampling based on Local label imbalance (MLUL). By considering all informative labels, MLSOL creates more diverse and better labeled synthetic instances for difficult examples, while MLUL eliminates instances that are harmful to their local region. Experimental results on 13 multi-label datasets demonstrate the effectiveness of the proposed measure and sampling approaches for a variety of evaluation metrics, particularly in the case of an ensemble of classifiers trained on repeated samples of the original data.","Multi-label learning, Class imbalance, Oversampling and undersampling, Local label imbalance, Ensemble methods",Bin Liu and Konstantinos Blekas and Grigorios Tsoumakas,https://www.sciencedirect.com/science/article/pii/S003132032100474X,https://doi.org/10.1016/j.patcog.2021.108294,0031-3203,2022,108294,122,Pattern Recognition,Multi-label sampling based on local label imbalance,article,LIU2022108294,
"Recovering 3D voxelized shapes with fine details from single-view 2D images is an extremely challenging and ill-conditioned problem. Most of the existing methods learn the 3D reconstruction process by encoding the 3D shapes and the 2D images into the same low-dimensional latent vector, which lacks the capacity to capture detailed features in the surface of the 3D object shapes. To address this issue, we propose to explore rich intermediate representation for 3D shape reconstruction by using a newly designed network architecture. We first use a two-steam network to infer the depth map and the topology-specific mean shape from the given 2D image, which forms the intermediate representation prediction branch. The intermediate representations capture the global spatial structure and the visible surface geometric structure, which are important for reconstructing high-quality 3D shapes. Based on the obtained intermediate representation, a novel shape transformation network is then proposed to reconstruct the fine details of the whole 3D object shapes. The experimental results on the challenging ShapeNet and Pix3D datasets show that our approach outperforms the existing state-of-the-art methods.","3D Reconstruction, Shape transformation, Intermediate representations",Yang Yang and Junwei Han and Dingwen Zhang and Qi Tian,https://www.sciencedirect.com/science/article/pii/S0031320321004751,https://doi.org/10.1016/j.patcog.2021.108295,0031-3203,2022,108295,122,Pattern Recognition,Exploring rich intermediate representations for reconstructing 3D shapes from 2D images,article,YANG2022108295,
"This paper presents a novel Feature Wise Normalization approach for the effective normalization of data. In this approach, each feature is normalized independently with one of the methods from the pool of normalization methods. It is in contrast to the conventional approach which normalizes the data with one method only and as a result, yields suboptimal performance. Additionally, generalization and superiority among normalization methods are also not ensured owing to different machine learning mechanisms for solving classification tasks. The proposed approach benefits from the collective response of multiple methods to normalize the data better as individual features become a normalization unit. The selection of methods is a combinatorial problem that can be solved with optimization algorithms. For this purpose, Antlion optimization is considered that combines the search of methods with the fine-tuning of classifier parameters. Twelve methods are used to create the pool beside the original scale, and the obtained data is evaluated on four learning algorithms. Experiments are performed on 18 benchmark datasets to show the efficacy of the proposed approach in contrast to conventional normalization.","Data normalization, -nearest neighbor classification, Machine learning, Metaheuristic optimization, Naive bayes classification, Neural networks, Support vector machines",Dalwinder Singh and Birmohan Singh,https://www.sciencedirect.com/science/article/pii/S0031320321004878,https://doi.org/10.1016/j.patcog.2021.108307,0031-3203,2022,108307,122,Pattern Recognition,Feature wise normalization: An effective way of normalizing data,article,SINGH2022108307,
"Modeling image sets as points on Grassmann manifold has attracted increasing interests in computer vision community and has been applied to many applications. However, such approaches have suffered from the limitation that high computational cost on Grassmann manifold must be involved, especially high-dimensional ones. In this paper, we propose an unsupervised robust dimensionality reduction algorithm for Grassmann manifold based on Neighborhood Preserving Embedding (GNPE). We first introduce two strategies to construct the coefficients-based similarity graph to eliminate the effects of errors. Then, a projection is learned from the high-dimensional Grassmann manifold to the relative low-dimensional one with more discriminative capability, where the local neighborhood structure is well preserved. To address the issue that the estimated similarity graph is unreliable with noise and outliers, we further propose a unified learning framework which performs similarity learning and projection learning simultaneously. By leveraging the interactions between these two essential tasks, we can capture accurate structures and learn discriminative projections. The proposed method can be optimized by an efficient iterative algorithm. Experiments on various image set classification and clustering tasks clearly show that our model achieves consistent improvements in terms of both effectiveness and efficiency.","Neighborhood preserving embedding, Dimensionality reduction, Grassmann manifold, Twin learning",Dong Wei and Xiaobo Shen and Quansen Sun and Xizhan Gao and Zhenwen Ren,https://www.sciencedirect.com/science/article/pii/S003132032100515X,https://doi.org/10.1016/j.patcog.2021.108335,0031-3203,2022,108335,122,Pattern Recognition,Neighborhood preserving embedding on Grassmann manifold for image-set analysis,article,WEI2022108335,
"This work presents a new deep learning approach for keystroke biometrics based on a novel Distance Metric Learning method (DML). DML maps input data into a learned representation space that reveals a âsemanticâ structure based on distances. In this work, we propose a novel DML method specifically designed to address the challenges associated to free-text keystroke identification where the classes used in learning and inference are disjoint. The proposed SetMargin Loss (SM-L) extends traditional DML approaches with a learning process guided by pairs of sets instead of pairs of samples, as done traditionally. The proposed learning strategy allows to enlarge inter-class distances while maintaining the intra-class structure of keystroke dynamics. We analyze the resulting representation space using the mathematical problem known as Circle Packing, which provides neighbourhood structures with a theoretical maximum inter-class distance. We finally prove experimentally the effectiveness of the proposed approach on a challenging task: keystroke biometric identification over a large set of 78,000 subjects. Our method achieves state-of-the-art accuracy on a comparison performed with the best existing approaches.","Keystroke biometrics, Circle packing, Deep learning, DML",Aythami Morales and Julian Fierrez and Alejandro Acien and Ruben Tolosana and Ignacio Serna,https://www.sciencedirect.com/science/article/pii/S0031320321004635,https://doi.org/10.1016/j.patcog.2021.108283,0031-3203,2022,108283,122,Pattern Recognition,SetMargin loss applied to deep keystroke biometrics with circle packing interpretation,article,MORALES2022108283,
"We introduce an online variational Bayesian model for tracking changes in a non-stationary, multivariate, temporal signal, using as an example the changing frequency and amplitude of a noisy sinusoidal signal over time. The model incorporates each observation as it arrives and then discards it, and places priors over precision hyperparameters to ensure that (i) the posterior probability distributions do not become overly tight, which would impede its ability to recognise and track changes, and (ii) no values in the system are able to continuously increase and hence exceed the numerical representation of the programming language. It is thus able to perform truly online processing for an infinitely long set of observations. Only a single round of updates in the variational Bayesian scheme per observation is used, and the complexity of the algorithm is constant in time. The proposed method is demonstrated on a large number of synthetic datasets, comparing the results from the full model (with precision hyperparameters as variables with priors) with those from the base model where the precision hyperparameters are fixed values. The full model is also demonstrated on a set of real climate data.","Online learning/processing, Variational methods, Bayes procedures",J. Christmas,https://www.sciencedirect.com/science/article/pii/S0031320321005203,https://doi.org/10.1016/j.patcog.2021.108340,0031-3203,2022,108340,122,Pattern Recognition,"Non-stationary, online variational Bayesian learning, with circular variables",article,CHRISTMAS2022108340,
"In this paper, we propose a novel semi-supervised active salient object detection (SOD) method that actively acquires a small subset of the most discriminative and representative samples for labeling. Two main contributions have been made to prevent the method from being overwhelmed by labeling similar distributed samples. First, we design a saliency encoder-decoder with adversarial discriminator to generate a confidence map, representing the network uncertainty on the current prediction. Then, we select the least confident (discriminative) samples from the unlabeled pool to form the âcandidate labeled poolâ. Second, we train a Variational Auto-Encoder (VAE) to select and add the most representative data from the âcandidate labeled poolâ into the labeled pool by comparing their corresponding features in the latent space. Within our framework, these two networks are optimized conditioned on the states of each other progressively. Experimental results on six benchmarking SOD datasets demonstrate that our annotation-efficient learning based salient object detection method, reaching to 14% labeling budget, can be on par with the state-of-the-art fully-supervised deep SOD models. The source code is publicly available via our project page: https://github.com/JingZhang617/Semi-sup-active-self-sup-Learning.","Salient object detection, Annotation-efficient Learning, Active learning, Variational Auto-Encoder",Yunqiu Lv and Bowen Liu and Jing Zhang and Yuchao Dai and Aixuan Li and Tong Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321005446,https://doi.org/10.1016/j.patcog.2021.108364,0031-3203,2022,108364,123,Pattern Recognition,Semi-supervised Active Salient Object Detection,article,LV2022108364,
"The field of electrocardiogram (ECG) biometrics has received considerable attention in recent years. Although some promising methods have been proposed, it is challenging to design a robust and precise method to improve the recognition performance of ECG signals with noise and sample variation. While the advantage of improved local binary pattern (LBP) for establishing identities has been widely recognized, extracting the latent semantics from multiple LBP features has attracted little attention. We propose a robust multi-feature collective non-negative matrix factorization (RMCNMF) model to handle noise and sample variation in ECG Biometrics. We extract multiple LBP histograms as feature descriptors from segmented ECG signals, and propose a multi-feature learning framework that learns unified representations in the shared latent semantic space via collective non-negative matrix factorization. To further enhance the discrimination of learned representations, we integrate label information and multiple norms in the proposed model, which not only preserves intra- and inter-subject similarities but also mitigates the influence of noise and sample variation. RMCNMF can be solved by an efficient iteration method, for which we provide a convergence analysis in detail. Extensive experiments on four ECG databases show that it performs competitively with state-of-the-art methods.","ECG biometrics, Collective non-negative matrix factorization, Multiple features, Local binary pattern, Label information",Yuwen Huang and Gongping Yang and Kuikui Wang and Haiying Liu and Yilong Yin,https://www.sciencedirect.com/science/article/pii/S0031320321005562,https://doi.org/10.1016/j.patcog.2021.108376,0031-3203,2022,108376,123,Pattern Recognition,Robust multi-feature collective non-negative matrix factorization for ECG biometrics,article,HUANG2022108376,
"In recent years, underwater image enhancement methods based on deep learning have achieved remarkable results. Since the images obtained in complex underwater scenarios lack a ground truth, these algorithms mainly train models on underwater images synthesized from in-air images. Synthesized underwater images are different from real-world underwater images; this difference leads to the limited generalizability of the training model when enhancing real-world underwater images. In this work, we present an underwater image enhancement method that does not require training on synthetic underwater images and eliminates the dependence on underwater ground-truth images. Specifically, a novel domain adaptation framework for real-world underwater image enhancement inspired by transfer learning is presented; it transfers in-air image dehazing to real-world underwater image enhancement. The experimental results on different real-world underwater scenes indicate that the proposed method produces visually satisfactory results.","Underwater image enhancement, Transfer learning, Domain adaptation, Cycle-consistent adversarial network",Qun Jiang and Yunfeng Zhang and Fangxun Bao and Xiuyang Zhao and Caiming Zhang and Peide Liu,https://www.sciencedirect.com/science/article/pii/S0031320321005045,https://doi.org/10.1016/j.patcog.2021.108324,0031-3203,2022,108324,122,Pattern Recognition,Two-step domain adaptation for underwater image enhancement,article,JIANG2022108324,
"Recently, the deep learning method that integrates image features has gradually become a hot development trend in hyperspectral image classification. However, these studies did not fully consider the fusion of image features, and did not remove the interference to the classification process caused by the difference in the size of the objects. These factors hinder the further improvement of the classification effect. To eliminate these drawbacks, this paper proposes a more effective fusion scheme (MSF-MIF), which realizes the fusion from the perspective of location characteristics and channel characteristics through 3D convolution and spatial feature concatenation. In view of the size discrepancy of the objects to be classified, this method extracts features from several input patches of different scales and uses the novel calculation method proposed to fuse them, which minimizes the interference caused by size differences. In addition, this research also tried to quote the coordinate attention structure for the first time that combines spatial and spectral attention features to further improve the classification performance. Experimental results on three commonly used data sets prove that this framework has achieved a breakthrough in classification accuracy.","Hyperspectral image(HSI), Multi-scale fusion, Fusion calculation, Coordinate attention, Image patch, 3D convolution",Lina Yang and Fengqi Zhang and Patrick Shen-Pei Wang and Xichun Li and Zuqiang Meng,https://www.sciencedirect.com/science/article/pii/S0031320321005288,https://doi.org/10.1016/j.patcog.2021.108348,0031-3203,2022,108348,122,Pattern Recognition,Multi-scale spatial-spectral fusion based on multi-input fusion calculation and coordinate attention for hyperspectral image classification,article,YANG2022108348,
"Anomaly detection plays an important role in surveillance video since it maintains public safety efficiently with low cost. In current works, anomaly detection methods based on reconstruction with deep learning has been extensively studied for the powerful representation capacity. These methods use convolutional neural networks to learn model for describing normality at training and detect anomalies according to reconstruction error at testing. However, excessive representation capacity of neural networks will also bring disadvantages to anomaly detection when it is powerful enough to reconstruct abnormal information. For this reason, we proposed two solutions; firstly, a cascade model which conducts pixel reconstruction followed by optical flow prediction is designed. The conversion from frame to optical flow learns the correlation between object appearance and motion, while pixel reconstruction enlarges the optical flow prediction error to conduct effective anomaly detection. Secondly, the generalization ability evaluation based on pseudo-anomaly is proposed, which is used to evaluate the ability of model to represent anomaly, thus selecting an optimal model for anomaly detection. The selected model achieves AUC 88.9% on Avenue, 82.6% on Ped1, 97.7% on Ped2, and 70.7% on ShanghaiTech datasets. Extensive ablation experiments have verified the effectiveness of our method. Code will be released at https://github.com/Xia-Chen/Cascade_Reconstruction.","Anomaly detection, pixel reconstruction, optical flow prediction, generalization ability evaluation",Yuanhong Zhong and Xia Chen and Jinyang Jiang and Fan Ren,https://www.sciencedirect.com/science/article/pii/S0031320321005161,https://doi.org/10.1016/j.patcog.2021.108336,0031-3203,2022,108336,122,Pattern Recognition,A cascade reconstruction model with generalization ability evaluation for anomaly detection in videos,article,ZHONG2022108336,
"Constructing adversarial perturbations for deep neural networks is an important direction of research. Crafting image-dependent adversarial perturbations using white-box feedback has hitherto been the norm for such adversarial attacks. However, black-box attacks are much more practical for real-world applications. Universal perturbations applicable across multiple images are gaining popularity due to their innate generalizability. There have also been efforts to restrict the perturbations to a few pixels in the image. This helps to retain visual similarity with the original images making such attacks hard to detect. This paper marks an important step that combines all these directions of research. We propose the DEceit algorithm for constructing effective universal pixel-restricted perturbations using only black-box feedback from the target network. We conduct empirical investigations using the ImageNet validation set on the state-of-the-art deep neural classifiers by varying the number of pixels to be perturbed from a meager 10Â pixels to as high as all pixels in the image. We find that perturbing only about 10% of the pixels in an image using DEceit achieves a commendable and highly transferable Fooling Rate while retaining the visual quality. We further demonstrate that DEceit can be successfully applied to image-dependent attacks as well. In both sets of experiments, we outperform several state-of-the-art methods.","Adversarial attack, Black-box attack, Convolutional image classifier, Differential evolution, Sparse universal attack",Arka Ghosh and Sankha Subhra Mullick and Shounak Datta and Swagatam Das and Asit Kr. Das and Rammohan Mallipeddi,https://www.sciencedirect.com/science/article/pii/S0031320321004593,https://doi.org/10.1016/j.patcog.2021.108279,0031-3203,2022,108279,122,Pattern Recognition,A black-box adversarial attack strategy with adjustable sparsity and generalizability for deep image classifiers,article,GHOSH2022108279,
"COVID-19 has emerged as one of the deadliest pandemics that has ever crept on humanity. Screening tests are currently the most reliable and accurate steps in detecting severe acute respiratory syndrome coronavirus in a patient, and the most used is RT-PCR testing. Various researchers and early studies implied that visual indicators (abnormalities) in a patient's Chest X-Ray (CXR) or computed tomography (CT) imaging were a valuable characteristic of a COVID-19 patient that can be leveraged to find out virus in a vast population. Motivated by various contributions to open-source community to tackle COVID-19 pandemic, we introduce SARS-Net, a CADx system combining Graph Convolutional Networks and Convolutional Neural Networks for detecting abnormalities in a patient's CXR images for presence of COVID-19 infection in a patient. In this paper, we introduce and evaluate the performance of a custom-made deep learning architecture SARS-Net, to classify and detect the Chest X-ray images for COVID-19 diagnosis. Quantitative analysis shows that the proposed model achieves more accuracy than previously mentioned state-of-the-art methods. It was found that our proposed model achieved an accuracy of 97.60% and a sensitivity of 92.90% on the validation set.","Convolutional neural network, Graph convolutional network, COVID-19 detection, Chest X-ray, Deep learning",Aayush Kumar and Ayush R Tripathi and Suresh Chandra Satapathy and Yu-Dong Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321004350,https://doi.org/10.1016/j.patcog.2021.108255,0031-3203,2022,108255,122,Pattern Recognition,SARS-Net: COVID-19 detection from chest x-rays by combining graph convolutional network and convolutional neural network,article,KUMAR2022108255,
"Unmanned and intelligent technologies are the future development trend in the business field. It is of great significance for the connotation analysis and application characterization of massive interactive data. Particularly, during major epidemics or disasters, how to provide business services safely and securely is crucial. Specifically, providing users with resilient and guaranteed communication services is a challenging business task when the communication facilities are damaged. Unmanned aerial vehicles (UAVs), with flexible deployment and high maneuverability, can be used to serve as aerial base stations (BSs) to establish emergency networks. However, it is challenging to control multiple UAVs to provide efficient and fair communication quality of service (QoS) to users due to their limited communication service capabilities. In this paper, we propose a learning-based resilience guarantee framework for multi-UAV collaborative QoS management. We formulate this problem as a partial observable Markov decision process and solve it with proximal policy optimization (PPO), which is a policy-based deep reinforcement learning method. A centralized training and decentralized execution paradigm is used, where the experience collected by all UAVs is used to train the shared control policy. Each UAV takes actions based on the partial environment information it observes. In addition, the design of the reward function considers the average and variance of the communication QoS of all users. Extensive simulations are conducted for performance evaluation. The simulation results indicate that (1) the trained policies can adapt to different scenarios and provide resilient and guaranteed communication QoS to users, (2) increasing the number of UAVs can compensate for the lack of service capabilities of UAVs, (3) when UAVs have local communication service capabilities, the policies trained with PPO have better performance compared with the policies trained with other algorithms.","Unmanned business, Communication service, Multi-UAV, Deep reinforcement learning, QoS-aware, System resilience",Chengchao Bai and Peng Yan and Xiaoqiang Yu and Jifeng Guo,https://www.sciencedirect.com/science/article/pii/S0031320321003538,https://doi.org/10.1016/j.patcog.2021.108166,0031-3203,2022,108166,122,Pattern Recognition,Learning-based resilience guarantee for multi-UAV collaborative QoS management,article,BAI2022108166,
"Currently, the use of deep learning for solving ordinal classification problems, where categories follow a natural order, has not received much attention. In this paper, we propose an unimodal regularisation based on the beta distribution applied to the cross-entropy loss. This regularisation encourages the distribution of the labels to be a soft unimodal distribution, more appropriate for ordinal problems. Given that the beta distribution has two parameters that must be adjusted, a method to automatically determine them is proposed. The regularised loss function is used to train a deep neural network model with an ordinal scheme in the output layer. The results obtained are statistically analysed and show that the combination of these methods increases the performance in ordinal problems. Moreover, the proposed beta distribution performs better than other distributions proposed in previous works, achieving also a reduced computational cost.","Ordinal regression, Unimodal distribution, Convolutional network, Beta distribution, Stick-breaking",VÃ­ctor {Manuel Vargas} and Pedro Antonio GutiÃ©rrez and CÃ©sar HervÃ¡s-MartÃ­nez,https://www.sciencedirect.com/science/article/pii/S0031320321004908,https://doi.org/10.1016/j.patcog.2021.108310,0031-3203,2022,108310,122,Pattern Recognition,Unimodal regularisation based on beta distribution for deep ordinal regression,article,MANUELVARGAS2022108310,
"Pose guided person image generation aims to transform a source person image to a target pose. It is an ill-posed problem as we often need to generate pixels that are invisible in the source image. Recent works focus on designing new architectures of deep neural networks and show promising performance. However, they simply adopt loss functions widely used in generic image generation tasks, e.g., adversarial loss, L1-norm loss, perceptual loss, and style loss, which fail to consider the unique structural patterns of a person. In addition, it remains unclear how each individual loss and their combinations impact the generated person images. The goal of this paper is to have a comprehensive study of loss functions for pose guided person image generation. After revisiting these generic loss functions, we consider the structural similarity (SSIM) index as a loss function since it is widely used as the evaluation metric and can capture the perceptual quality of generated images. In addition, motivated by the observation that a person can be divided into part regions with homogeneous pixel values or texture, we extend the SSIM loss into a novel Part-based SSIM (PSSIM) loss to explicitly account for the articulated body structure. A new PSSIM metric is then proposed naturally to access the quality of generated person images. In order to have a deep investigation of loss functions, we conduct extensive experiments including single-loss analysis, multi-loss combination analysis, optimal loss combination search, and comparison with state-of-the-art methods. Both quantitative and qualitative results indicate that (1) using different loss functions significantly impacts the generated person images, (2) the combination of adversarial loss, perceptual loss, and PSSIM loss is the optimal choice for person image generation, and (3) the proposed PSSIM loss is complementary to prior losses and helps improve the performance of state-of-the art methods. We have made the source code publicly available at https://github.com/shyern/Pose-Transfer-pSSIM.git.","Person image generation, Loss function analysis, Structure similarity index",Haoyue Shi and Le Wang and Nanning Zheng and Gang Hua and Wei Tang,https://www.sciencedirect.com/science/article/pii/S0031320321005318,https://doi.org/10.1016/j.patcog.2021.108351,0031-3203,2022,108351,122,Pattern Recognition,Loss functions for pose guided person image generation,article,SHI2022108351,
"Abundant data are essential for improving the performance of machine learning algorithms. Thus, if only limited data are available, data synthesis can be used to enlarge datasets. Data synthesis methods based on the covariance matrix are useful because of their fast data synthesis capabilities. However, artificial datasets generated via classical techniques show statistical discrepancies when compared to original datasets. To address this problem, we developed a new data synthesis method that preserves the correlation (between features) observed in the original dataset. This preservation was realized by considering not only the correlation but also the random noises used in data synthesis process. This method was applied to various biosignals (i.e., electrocortiography, electromyogram, and electrocardiogram), wherein data points are insufficient. Several classifiers (i.e., convolutional neural network, support vector machine, and k-nearest neighbor) were used to verify that the classification accuracy can be improved by the proposed data synthesis method.","Data synthesis, Correlation, Artificial dataset, Random noise",Wonseok Yang and Woochul Nam,https://www.sciencedirect.com/science/article/pii/S0031320321004222,https://doi.org/10.1016/j.patcog.2021.108241,0031-3203,2022,108241,122,Pattern Recognition,Data synthesis method preserving correlation of features,article,YANG2022108241,
"Clustering algorithms play a fundamental role as tools in decision-making and sensible automation processes. Due to the widespread use of these applications, a robustness analysis of this family of algorithms against adversarial noise has become imperative. To the best of our knowledge, however, only a few works have currently addressed this problem. In an attempt to fill this gap, in this work, we propose a black-box adversarial attack for crafting adversarial samples to test the robustness of clustering algorithms. We formulate the problem as a constrained minimization program, general in its structure and customizable by the attacker according to her capability constraints. We do not assume any information about the internal structure of the victim clustering algorithm, and we allow the attacker to query it as a service only. In the absence of any derivative information, we perform the optimization with a custom approach inspired by the Abstract Genetic Algorithm (AGA). In the experimental part, we demonstrate the sensibility of different single and ensemble clustering algorithms against our crafted adversarial samples on different scenarios. Furthermore, we perform a comparison of our algorithm with a state-of-the-art approach showing that we are able to reach or even outperform its performance. Finally, to highlight the general nature of the generated noise, we show that our attacks are transferable even against supervised algorithms such as SVMs, random forests and neural networks.","Adversarial learning, Unsupervised learning, Clustering, Robustness evaluation, Machine learning security",Antonio Emanuele CinÃ  and Alessandro Torcinovich and Marcello Pelillo,https://www.sciencedirect.com/science/article/pii/S0031320321004866,https://doi.org/10.1016/j.patcog.2021.108306,0031-3203,2022,108306,122,Pattern Recognition,A black-box adversarial attack for poisoning clustering,article,CINA2022108306,
"Fine-grained action recognition involves comparison of similar actions of variable-length size consisting of subtle interactions between human and specific objects. Hence, we propose a dynamic kernel-based approach to handle the variable-length patterns for effective recognition of fine-grained actions. Initially, we extract local spatio-temporal features for each video to capture appearance and motion information effectively. An action-independent Gaussian mixture model (AIGMM) is trained on the extracted features of all fine-grained actions to analyze spatio-temporal information and preserve the local similarities among fine-grained actions. Then, the statistics of AIGMM, namely, mean, covariance, and posteriors are used to build the kernels for finding the similarity between any two fine-grained actions by mapping statistics to kernel feature space. We demonstrate the effectiveness of proposed approach using three dynamic kernels i.e., GMM mean interval kernel, supervector kernel, intermediate matching kernel on four varieties of fine-grained action datasets, namely, MERL, JIGSAWS, KSCGR, and MPII cooking2","Fine-grained action recognition, Spatio-temporal features, Gaussian mixture model, Dynamic kernels",Sravani Yenduri and Nazil Perveen and Vishnu Chalavadi and Krishna Mohan C,https://www.sciencedirect.com/science/article/pii/S0031320321004623,https://doi.org/10.1016/j.patcog.2021.108282,0031-3203,2022,108282,122,Pattern Recognition,Fine-grained action recognition using dynamic kernels,article,YENDURI2022108282,
"Optical flow, which expresses pixel displacement, is widely used in many computer vision tasks to provide pixel-level motion information. However, with the remarkable progress of the convolutional neural network, recent state-of-the-art approaches are proposed to solve problems directly on feature-level. Since the displacement of feature vector is not consistent with the pixel displacement, a common approach is to forward optical flow to a neural network and fine-tune this network on the task dataset. With this method, they expect the fine-tuned network to produce tensors encoding feature-level motion information. In this paper, we rethink about this de facto paradigm and analyze its drawbacks in the video object detection task. To mitigate these issues, we propose a novel network (IFF-Net) with an In-network Feature Flow estimation module (IFF module) for video object detection. Without resorting to pre-training on any additional dataset, our IFF module is able to directly produce feature flow which indicates the feature displacement. Our IFF module consists of a shallow module, which shares the features with the detection branches. This compact design enables our IFF-Net to accurately detect objects, while maintaining a fast inference speed. Furthermore, we propose a transformation residual loss (TRL) based on self-supervision, which further improves the performance of our IFF-Net. Our IFF-Net outperforms existing methods and achieves new state-of-the-art performance on ImageNet VID.","Video object detection, Feature flow, Object detection, Video analysis, Deep convolutional neural network (DCNN)",Ruibing Jin and Guosheng Lin and Changyun Wen and Jianliang Wang and Fayao Liu,https://www.sciencedirect.com/science/article/pii/S0031320321005033,https://doi.org/10.1016/j.patcog.2021.108323,0031-3203,2022,108323,122,Pattern Recognition,Feature flow: In-network feature flow estimation for video object detection,article,JIN2022108323,
"In this study, we aim to improve the accuracy of image splicing detection. We propose a progressive image splicing detection method that can detect the position and shape of spliced region. Because image splicing is likely to destroy or change the consistent correlation pattern introduced by color filter array (CFA) interpolation process, we first used a covariance matrix to reconstruct the R, G and B channels of image and utilized the inconsistencies of the CFA interpolation pattern to extract forensics feature. Then, these forensics features were used to perform coarse-grained detection, and texture strength features were used to perform fine-grained detection. Finally, an edge smoothing method was applied to realize precise localization. As compared to the state-of-the-art CFA-based image splicing detection methods, the proposed method has a high-level detection accuracy and strong robustness against content-preserving manipulations and JPEG compression.","Image splicing detection, CFA interpolation algorithm, Forensics features, Texture strength features, Edges smoothing",Xiaofeng Wang and Yan Wang and Jinjin Lei and Bin Li and Qin Wang and Jianru Xue,https://www.sciencedirect.com/science/article/pii/S0031320321005276,https://doi.org/10.1016/j.patcog.2021.108347,0031-3203,2022,108347,122,Pattern Recognition,Coarse-to-fine-grained method for image splicing region detection,article,WANG2022108347,
"Most existing RGB-D salient object detectors make use of the complementary information of RGB-D images to overcome the challenging scenarios, e.g., low contrast, clutter backgrounds. However, these models generally neglect the fact that one of the input images may be poor in quality. This will adversely affect the discriminative ability of cross-modal features when the two channels are fused directly. To address this issue, a novel end-to-end RGB-D salient object detection model is proposed in this paper. At the core of our model is a Semantic-Guided Modality-Weight Map Generation (SG-MWMG) sub-network, producing modality-weight maps to indicate which regions on both modalities are high-quality regions, given input RGB-D images and the guidance of their semantic information. Based on it, a Bi-directional Multi-scale Cross-modal Feature Fusion (Bi-MCFF) module is presented, where the interactions of the features across different modalities and scales are exploited by using a novel bi-directional structure for better capturing cross-scale and cross-modal complementary information. The experimental results on several benchmark datasets verify the effectiveness and superiority of the proposed method over some state-of-the-art methods.","RGB-D salient object detection, Discriminative unimodal feature selection, Semantic information, Multi-scale cross-modal feature fusion",Nianchang Huang and Yongjiang Luo and Qiang Zhang and Jungong Han,https://www.sciencedirect.com/science/article/pii/S0031320321005392,https://doi.org/10.1016/j.patcog.2021.108359,0031-3203,2022,108359,122,Pattern Recognition,Discriminative unimodal feature selection and fusion for RGB-D salient object detection,article,HUANG2022108359,
"Domain gaps between different datasets limit the generalization ability of CNN models. Precise evaluation on the domain gap has potential to assist the promotion of CNN generalization ability. This paper proposes a computational framework to evaluate gaps between different domains, e.g., judging which one of source domains is closer to the target domain. Our model is based on the observation that, given a well-trained classifier on the source domain, the entropy of its classification scores of the output layer can be used as an indicator of the domain gap. For instance, smaller domain gap generally corresponds to smaller entropy of classification scores. To further boost the discriminative power in distinguishing domain gaps, a novel training strategy is proposed to supervise the model to produce smaller entropy on one source domain and larger entropy on other source domains. This supervision leads to an efficient and discriminative domain gap evaluation model. Extensive experiments on multiple datasets including faces, vehicles, fashions, and persons, etc. show that our method can reasonably measure domain gaps. We further conduct experiments on domain adaptive person ReID task and our method is adopted to pre-trained model selection, pre-trained model fusion, source dataset fusion, and source dataset selection. As shown in the experiments, our method substantially boosts the ReID accuracy. To the best of our knowledge, this is an original work focusing on computational domain gap evaluation. Our code is available at https://github.com/liu-xb/DomainGapEvaluation.","Domain gap evaluation, CNN, Domain adaptive learning",Xiaobin Liu and Shiliang Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321004738,https://doi.org/10.1016/j.patcog.2021.108293,0031-3203,2022,108293,122,Pattern Recognition,Who is closer: A computational method for domain gap evaluation,article,LIU2022108293,
"Hyperspectral super-resolution (HSR) fuses a low-resolution hyperspectral image (HSI) and a high-resolution multispectral image (MSI) to obtain a high-resolution HSI (HR-HSI). In this paper, we propose a new model called coupled tensor ring factorization (CTRF) for HSR. The proposed CTRF approach simultaneously learns the tensor ring core tensors of the HR-HSI from a pair of HSI and MSI. The CTRF model can separately exploit the low-rank property of each class (SectionÂ 3.3), which has not been explored in previous coupled tensor models. Meanwhile, the model inherits the simple representation of coupled matrix/canonical polyadic factorization and flexible low-rank exploration of coupled Tucker factorization. We further introduce spectral nuclear norm regularization to explore the global spectral low-rank property. The experiments demonstrated the advantage of the proposed nuclear norm regularized CTRF model compared to previous matrix/tensor and deep learning methods.","Coupled tensor ring decomposition, Super-resolution, Hyperspectral, Multispectral",Wei He and Yong Chen and Naoto Yokoya and Chao Li and Qibin Zhao,https://www.sciencedirect.com/science/article/pii/S003132032100460X,https://doi.org/10.1016/j.patcog.2021.108280,0031-3203,2022,108280,122,Pattern Recognition,Hyperspectral super-resolution via coupled tensor ring factorization,article,HE2022108280,
"Foreground-background segmentation (FBS) is one of the prime tasks for automated video-based applications like traffic analysis and surveillance. The different practical scenarios like weather degraded videos, irregular moving objects, dynamic background, etc., make FBS a challenging task. The existing FBS algorithms mainly depend on one of the three different factors, namely (1) complicated training process, (2) additionally trained modules for other applications, or (3) neglect the inter-frame spatio-temporal structural dependencies. In this paper, a novel multi-frame-based adversarial learning network is proposed with multi-scale inception and residual module for FBS. As, FBS is a temporal enlightenment-based problem, a temporal encoding mechanism with decreasing variable intervals is proposed for the input frame selection. The proposed network comprises multi-scale inception and residual connection-based dense modules to learn prominent features of the foreground object(s). Also, feedback of the estimated foreground map of previous frame is utilized to exhibit more temporal consistency. Learning of the network is concentrated in different ways like cross-data, disjoint, and global training-testing for FBS. The qualitative and quantitative experimental analysis of the proposed approach is done on three benchmark datasets for FBS. Experimental analysis on three benchmark datasets proves the significance of the proposed approach as compared to state-of-the-art FBS approaches.","Temporal sampling, Multi-scale adversarial learning, Foreground-background segmentation and video surveillance",Prashant W. Patil and Akshay Dudhane and Sachin Chaudhary and Subrahmanyam Murala,https://www.sciencedirect.com/science/article/pii/S0031320321005306,https://doi.org/10.1016/j.patcog.2021.108350,0031-3203,2022,108350,122,Pattern Recognition,Multiâframe based adversarial learning approach for video surveillance,article,PATIL2022108350,
"Conditional Random Fields (CRF) are frequently applied for labeling and segmenting sequence data. Morency etÂ al. (2007) introduced hidden state variables in a labeled CRF structure in order to model the latent dynamics within class labels, thus improving the labeling performance. Such a model is known as Latent-Dynamic CRF (LDCRF). We present Factored LDCRF (FLDCRF), a structure that allows multiple latent dynamics of the class labels to interact with each other. Including such latent-dynamic interactions leads to improved labeling performance on single-label and multi-label sequence modeling experiments across two different datasets, viz., UCI gesture phase data and UCI opportunity data. FLDCRF outperforms all state-of-the-art sequence models, viz., CRF, LDCRF, LSTM, LSTM-CRF, Factorial CRF, Coupled CRF and a multi-label LSTM model across experiments in this paper. In addition, FLDCRF offers easier model selection and is more consistent across validation and test data than LSTM models. FLDCRF is also much faster to train compared to LSTM, even without a GPU. FLDCRF outshines the best LSTM model by â¼4% on a single-label task on the UCI gesture phase data and outperforms LSTM models by â¼2% on average on the multi-label sequence tagging experiment on the UCI opportunity data.","Conditional Random Fields, Sequence labeling, Multi-task learning, Latent-Dynamic models, Probabilistic graphical models",Satyajit Neogi and Justin Dauwels,https://www.sciencedirect.com/science/article/pii/S0031320321004179,https://doi.org/10.1016/j.patcog.2021.108236,0031-3203,2022,108236,122,Pattern Recognition,Factored Latent-Dynamic Conditional Random Fields for single and multi-label sequence modeling,article,NEOGI2022108236,
"The Coronavirus (COVID-19) pandemic impelled several research efforts, from collecting COVID-19 patientsâ data to screening them for virus detection. Some COVID-19 symptoms are related to the functioning of the respiratory system that influences speech production; this suggests research on identifying markers of COVID-19 in speech and other human generated audio signals. In this article, we give an overview of research on human audio signals using âArtificial Intelligenceâ techniques to screen, diagnose, monitor, and spread the awareness about COVID-19. This overview will be useful for developing automated systems that can help in the context of COVID-19, using non-obtrusive and easy to use bio-signals conveyed in human non-speech and speech audio productions.","COVID-19, Digital health, Audio processing, Computational paralinguistics",Gauri Deshpande and Anton Batliner and BjÃ¶rn W. Schuller,https://www.sciencedirect.com/science/article/pii/S0031320321004696,https://doi.org/10.1016/j.patcog.2021.108289,0031-3203,2022,108289,122,Pattern Recognition,AI-Based human audio processing for COVID-19: A comprehensive overview,article,DESHPANDE2022108289,
"Heterogeneous Domain Adaptation (HDA) addresses the transfer learning problems where data from the source and target domains are of different modalities (e.g., texts and images) or feature dimensions (e.g., features extracted with different methods). It is useful for multi-modal data analysis. Traditional domain adaptation algorithms assume that the representations of source and target samples reside in the same feature space, hence are likely to fail in solving the heterogeneous domain adaptation problem. Contemporary state-of-the-art HDA approaches are usually composed of complex optimization objectives for favourable performance and are therefore computationally expensive and less generalizable. To address these issues, we propose a novel Cross-Domain Structure Preserving Projection (CDSPP) algorithm for HDA. As an extension of the classic LPP to heterogeneous domains, CDSPP aims to learn domain-specific projections to map sample features from source and target domains into a common subspace such that the class consistency is preserved and data distributions are sufficiently aligned. CDSPP is simple and has deterministic solutions by solving a generalized eigenvalue problem. It is naturally suitable for supervised HDA but has also been extended for semi-supervised HDA where the unlabelled target domain samples are available. Extensive experiments have been conducted on commonly used benchmark datasets (i.e. Office-Caltech, Multilingual Reuters Collection, NUS-WIDE-ImageNet) for HDA as well as the Office-Home dataset firstly introduced for HDA by ourselves due to its significantly larger number of classes than the existing ones (65Â vs 10, 6 and 8). The experimental results of both supervised and semi-supervised HDA demonstrate the superior performance of our proposed method against contemporary state-of-the-art methods.","Heterogeneous domain adaptation, Cross-domain projection, Image classification, Text classification",Qian Wang and Toby P. Breckon,https://www.sciencedirect.com/science/article/pii/S0031320321005422,https://doi.org/10.1016/j.patcog.2021.108362,0031-3203,2022,108362,123,Pattern Recognition,Cross-domain structure preserving projection for heterogeneous domain adaptation,article,WANG2022108362,
"Graph clustering based on embedding aims to divide nodes with higher similarity into several mutually disjoint groups, but it is not a trivial task to maximumly embed the graph structure and node attributes into the low dimensional feature space. Furthermore, most of the current advanced methods of graph nodes clustering adopt the strategy of separating graph embedding technology and clustering algorithm, and ignore the potential relationship between them. Therefore, we propose an innovative end-to-end graph clustering framework with joint strategy to handle the complex problem in a non-Euclidean space. In terms of learning the graph embedding, we propose a new variational graph auto-encoder algorithm based on the Graph Convolution Network (GCN), which takes into account the boosting influence of joint generative model of graph structure and node attributes on the embedding output. On the basis of embedding representation, we implement a self-training mechanism through the construction of auxiliary distribution to further enhance the prediction of node categories, thereby realizing the unsupervised clustering mode. In addition, the loss contribution of each cluster is normalized to prevent large clusters from distorting the embedding space. Extensive experiments on real-world graph datasets validate our design and demonstrate that our algorithm has highly competitive in graph clustering over state-of-the-art methods.","Graph convolution neural network, Variational graph embedding, Graph clustering, Variational graph auto-encoder",Lin Guo and Qun Dai,https://www.sciencedirect.com/science/article/pii/S0031320321005148,https://doi.org/10.1016/j.patcog.2021.108334,0031-3203,2022,108334,122,Pattern Recognition,Graph Clustering via Variational Graph Embedding,article,GUO2022108334,
"In this paper, a novel non-parametric clustering algorithm which is based on the concept of divide-and-merge is proposed. The proposed algorithm is based on two primary phases, after data cleaning: (i) the Division phase and (ii) the Merging phase. In the initial phase of division, the data is divided into an optimized number of small sub-clusters utilizing all the dimensions of the data. In the second phase of merging, the small sub-clusters obtained as a result of division are merged according to an advanced statistical metric to form the actual clusters in the data. The proposed algorithm has the following merits: (i) ability to discover both convex and non-convex shaped clusters, (ii) ability to discover clusters different in densities, (iii) ability to detect and remove outliers/noise in the data (iv) easily tunable or fixed hyperparameters (v) and its usability for high dimensional data. The proposed algorithm is extensively tested on 20 benchmark datasets including both, the synthetic and the real datasets and is found better/competing to the existing state-of-the-art parametric and non-parametric clustering algorithms.","Clustering, Data projection, Joint probability density estimation, Non-parametric techniques",Atiq Ur Rehman and Samir Brahim Belhaouari,https://www.sciencedirect.com/science/article/pii/S0031320321004854,https://doi.org/10.1016/j.patcog.2021.108305,0031-3203,2022,108305,122,Pattern Recognition,Divide well to merge better: A novel clustering algorithm,article,REHMAN2022108305,
"A fast parallelable Jacobi iteration type optimization method for non-smooth convex composite optimization is presented. Traditional gradient-based techniques cannot solve the problem. Smooth approximate functions are attempted to be used as a replacement of those non-smooth terms without compromising the accuracy. Recently, proximal mapping concept has been introduced into this field. Techniques which utilize proximal average based proximal gradient have been used to solve the problem. The state-of-art methods only utilize first-order information of the smooth approximate function. We integrate both first and second-order techniques to use both first and second-order information to boost the convergence speed. A convergence rate with a lower bound of O(1k2) is achieved by the proposed method and a super-linear convergence is enjoyed when there is proper second-order information. In experiments, the proposed method converges significantly better than the state of art methods which enjoy O(1k) convergence.","non-smooth, proximal mapping, quasi-Newton",W.H. Chai and S.S. Ho and H.C. Quek,https://www.sciencedirect.com/science/article/pii/S0031320321004611,https://doi.org/10.1016/j.patcog.2021.108281,0031-3203,2022,108281,122,Pattern Recognition,A Novel Quasi-Newton Method for Composite Convex Minimization,article,CHAI2022108281,
"Least squares regression (LSR) is an important machine learning method for feature extraction, feature selection, and image classification. For the training samples, there are correlations among samples from the same class. Therefore, many LSR-based methods utilize this property to pursue discriminative representation. However, if the training samples contain noise or outliers, it will be hard to obtain the exact inter-class correlation. To address this problem, in this paper, a novel LSR-based method is proposed, named low-rank inter-class sparsity based semi-flexible target least squares regression (LIS_StLSR). Firstly, the low-rank representation method is utilized to achieve the intrinsic characteristics of the training samples. Afterwards, the low-rank inter-class sparsity constraint is used to force the projected data to have an exact common sparsity structure in each class, which will be robust to noise and outliers in the training samples. This step can also reduce margins of samples from the same class and enlarge margins of samples from different classes to make the projection matrix discriminative. The low-rank representation and the discriminative projection matrix are jointly learned such that they can be boosted mutually. Moreover, a semi-flexible regression target matrix is introduced to measure the regression error more accurately, thus the regression performance can be enhanced to improve the classification accuracy. Experiments are implemented on the different databases of Yale B, AR, LFW, CASIA NIR-VIS, 15-Scene SPF, COIL-20, and Caltech 101, illustrating that the proposed LIS_StLSR outperforms many state-of-the-art methods.","Least squares regression, Low-rank inter-class sparsity, Feature representation, Image classification",Shuping Zhao and Jigang Wu and Bob Zhang and Lunke Fei,https://www.sciencedirect.com/science/article/pii/S0031320321005264,https://doi.org/10.1016/j.patcog.2021.108346,0031-3203,2022,108346,123,Pattern Recognition,Low-rank inter-class sparsity based semi-flexible target least squares regression for feature representation,article,ZHAO2022108346,
"Image captioning is a hot research topic bridging computer vision and natural language processing during the past several decades. It has achieved great progress with the help of large-scale datasets and deep learning techniques. Though the variety of image captioning models (ICMs), the performance of ICMs have got stuck in a bottleneck judging from the publicly published results. Considering the marginal performance gains brought by recent ICMs, we raise the following question: âwhat about the performances of the recent ICMs achieve on in-the-wild images? To clarify this question, we compare existing ICMs by evaluating their generalization ability. Specifically, we propose a novel method based on maximum discrepancy competition to diagnose existing ICMs. Firstly, we establish a new test set containing only informative images selected by adopting maximum discrepancy competition on the existing ICMs, from an arbitrary large-scale raw image set. Secondly, a small-scale and low-cost subjective annotation experiment is conducted on the new test set. Thirdly, we rank the generalization ability of the existing ICMs by comparing their performances on the new test set. Finally, the keys of different ICMs are demonstrated based on a detailed analysis of experimental results. Our analysis yields several interesting findings, including that 1) Using simultaneously low- and high-level object features may be an effective tool to boost the generalization ability for the Transformer based ICMs. 2) Self-attention mechanism may provide better modelling ability for inter- and intra-modal data than other attention-based mechanisms. 3) Constructing an ICM with a multistage language decoder may be a promising way to improve its performance.","Image captioning, Model comparison, Attention mechanism",Boyang Wan and Wenhui Jiang and Yu-Ming Fang and Minwei Zhu and Qin Li and Yang Liu,https://www.sciencedirect.com/science/article/pii/S0031320321005380,https://doi.org/10.1016/j.patcog.2021.108358,0031-3203,2022,108358,122,Pattern Recognition,Revisiting image captioning via maximum discrepancy competition,article,WAN2022108358,
"Recently, deep neural networks (DNNs) have shown serious vulnerability to adversarial examples with imperceptible perturbation to clean images. To counter this issue, many powerful defensive methods (e.g., ComDefend) focus on rectifying the adversarial examples with well-trained models from a large training dataset (e.g., clean-adversarial image pairs). However, such methods rely heavily on the learned external priors from an external large training dataset, while neglecting the rich image internal priors of the input itself, thus limiting the generalization of the defense models against the adversarial examples with biased image statistics from the external training dataset. Motivated by deep image prior that can capture rich image statistics from a single image, we propose an effective Deep Image Prior Driven Defense (DIPDefend) method against adversarial examples. With a DIP generator to fit the target/adversarial input, we find that our image reconstruction exhibits quite interesting learning preference from a feature learning perspectives, i.e., the early stage primarily learns the robust features resistant to adversarial perturbation, followed by learning non-robust features that are sensitive to adversarial perturbation. Besides, we develop an adaptive stopping strategy that adapts our method to diverse images. In this way, the proposed model obtains a unique defender for each individual adversarial input, thus being robust to various attackers. Experimental results demonstrate the superiority of our method over the state-of-the-art defense methods against white-box and black-box adversarial attacks.","Deep neural network, Adversarial example, Image prior, Defense",Tao Dai and Yan Feng and Bin Chen and Jian Lu and Shu-Tao Xia,https://www.sciencedirect.com/science/article/pii/S0031320321004295,https://doi.org/10.1016/j.patcog.2021.108249,0031-3203,2022,108249,122,Pattern Recognition,Deep image prior based defense against adversarial examples,article,DAI2022108249,
"The land-covers within an observed remote sensing scene are usually of different scales; therefore, the ensemble of multi-scale information is a commonly used strategy to achieve more accurate scene interpretation; however, this process suffers from being time-consuming. In terms of this issue, this paper proposes a scale distillation network to explore the possibility that single-scale classification network can achieve the same (or even better) classification performance compared with multi-scale one. The proposed scale distillation network consists of a cumbersome multi-scale teacher network and a lightweight single-scale student network. The former is trained for multi-scale information learning, and the latter improves the classification accuracy by accepting the knowledge from the multi-scale teacher network and its true label. The experimental results show the advantages of scale distillation on hyperspectral image classification. The single-scale student network can even achieve higher evaluation accuracy than the multi-scale teacher network. In addition, a faithful explainable scale network is designed to visually explain the trained scale distillation network. The traditional deep neural network is a black-box and lacks interpretability. The explanation of the trained network can explore more hidden information from the predictions. We visually explain the prediction results of scale distillation network, and the results show that the explainable scale network can more precisely analyze the relationship between the learned scale features and the land-cover categories. Moreover, the possible application of the explainable scale network on classification is further discussed in this study.","Hyperspectral image classification, Knowledge distillation, Scale distillation, Explainable scale network",Cheng Shi and Li Fang and Zhiyong Lv and Minghua Zhao,https://www.sciencedirect.com/science/article/pii/S0031320321004969,https://doi.org/10.1016/j.patcog.2021.108316,0031-3203,2022,108316,122,Pattern Recognition,Explainable scale distillation for hyperspectral image classification,article,SHI2022108316,
"Semantic segmentation in a supervised learning manner has achieved significant progress in recent years. However, its performance usually drops dramatically due to the data-distribution discrepancy between seen and unseen domains when we directly deploy the trained model to segment the images of unseen (or new coming) domains. To this end, we propose a novel domain generalization framework for the generalizable semantic segmentation task, which enhances the generalization ability of the model from two different views, including the training paradigm and the test strategy. Concretely, we exploit the model-agnostic learning to simulate the domain shift problem, which deals with the domain generalization from the training scheme perspective. Besides, considering the data-distribution discrepancy between seen source and unseen target domains, we develop the target-specific normalization scheme to enhance the generalization ability. Furthermore, when images come one by one in the test stage, we design the image-based memory bank (Image Bank in short) with style-based selection policy to select similar images to obtain more accurate statistics of normalization. Extensive experiments highlight that the proposed method produces state-of-the-art performance for the domain generalization of semantic segmentation on multiple benchmark segmentation datasets, i.e., Cityscapes, Mapillary.","Domain generalization, Semantic segmentation, Model-agnostic learning, Target-specific normalization",Jian Zhang and Lei Qi and Yinghuan Shi and Yang Gao,https://www.sciencedirect.com/science/article/pii/S0031320321004726,https://doi.org/10.1016/j.patcog.2021.108292,0031-3203,2022,108292,122,Pattern Recognition,Generalizable model-agnostic semantic segmentation via target-specific normalization,article,ZHANG2022108292,
"Dynamic-to-static image translation aims to convert the dynamic scene into static so that dynamic elements are eliminated from the image. Recent works typically see the problem as an image-to-image translation task, and perform the learned feature mapping over the whole dynamic image to synthesize the static image, which leads to unnecessary detail loss in original static regions. To that end, we delicately formulate it as an image inpainting-like problem to fill the missing static pixels in dynamic regions while retaining original static regions. We achieve this by proposing a coarse-to-fine framework. At coarse stage, we utilize a simple encoder-decoder network to rough out the static image. Using the coarse predicted image, we explicitly infer a more accurate dynamic mask to identify both dynamic objects and their shadows, so that the task could be effectively converted to an image inpainting problem. At fine stage, we recover the missing static pixels in the estimated dynamic regions on the basis of their coarse predictions. We enhance the coarse predicted contents by proposing a mutual texture-structure attention module, which enables the dynamic regions to borrow textures and structures separately from distant locations based on contextual similarity. Several losses are combined as the training objective function to generate excellent results with global consistency and fine details. Qualitative and quantitative experiments verify the superiority of our method in restoring high-quality static contents over state-of-the-art models. In addition, we evaluate the usefulness of the recovered static images by using them as query images to improve visual place recognition in dynamic scenes.","Dynamic-to-static image translation, Shadow detection, Attention mechanism, Visual place recognition",Teng Wang and Lin Wu and Changyin Sun,https://www.sciencedirect.com/science/article/pii/S0031320321005537,https://doi.org/10.1016/j.patcog.2021.108373,0031-3203,2022,108373,123,Pattern Recognition,A coarse-to-fine approach for dynamic-to-static image translation,article,WANG2022108373,
"ABSTRACT Due to the nature of Arabic handwriting, segmenting words into characters/graphemes is the most difficult and critical task of the recognition system. The present paper proposes an approach to segment handwritten Arabic words into graphemes based on a directed Convolutional Neural Network (CNN) and Mathematical Morphology Operations (MMO). Arabic script is cursive, which means that almost all graphemes are connected via horizontal links; therefore, a technique to remove links will facilitate the segmentation of graphemes. In general, an MMO such as erosion seems suitable for getting the job done, but since Arabic handwriting is difficult, MMOs cause information loss and suffer from many issues such as diacritics and over-traces, which lead to over/under/bad segmentations. To overcome limitations, the present paper addresses these issues in the following order: the over-traces issue is addressed for the first time in the literature; a robust algorithm for diacritics extraction is provided; and finally, the main segmentation algorithm adopts a strategy based on a Partial Dilation (PD)-Global Erosion (GE) technique to combat the information loss issue. The PD phase amplifies important regions, while GE eliminates links between graphemes. The complementarity between PD and GE facilitates the extraction of graphemes and creates resistance against information loss. To properly tackle these difficult problems, this article exploits the robustness of CNNs, so a new directed CNN model is suggested. The idea is to draw the model's attention to certain targeted features, which are selected according to the nature of the problem addressed. The proposed directed CNN is used in all phases of the segmentation process. The experimental results are very encouraging and show that the proposed directed CNN model outperformed basic CNN in many experiments. The results also reveal that the followed strategy improved the ability of MMOs to perform segmentation and to compete with other approaches in this research area.","handwritten Arabic graphemes segmentation, directed convolutional neural network, mathematical morphology, over-traces, partial dilation, global erosion",Mohsine Elkhayati and Youssfi Elkettani and Mohammed Mourchid,https://www.sciencedirect.com/science/article/pii/S0031320321004684,https://doi.org/10.1016/j.patcog.2021.108288,0031-3203,2022,108288,122,Pattern Recognition,Segmentation of Handwritten Arabic Graphemes Using a Directed Convolutional Neural Network and Mathematical Morphology Operations,article,ELKHAYATI2022108288,
"The sudden outbreak of COVID-19 has resulted in tough challenges for the field of biometrics due to its spread via physical contact, and the regulations of wearing face masks. Given these constraints, voice biometrics can offer a suitable contact-less biometric solution; they can benefit from models that classify whether a speaker is wearing a mask or not. This article reviews the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 COMputational PARalinguistics challengE (ComParE), which focused on the following classification task: Given an audio chunk of a speaker, classify whether the speaker is wearing a mask or not. First, we report the collection of the Mask Augsburg Speech Corpus (MASC) and the baseline approaches used to solve the problem, achieving a performance of 71.8% Unweighted Average Recall (UAR). We then summarise the methodologies explored in the submitted and accepted papers that mainly used two common patterns: (i) phonetic-based audio features, or (ii) spectrogram representations of audio combined with Convolutional Neural Networks (CNNs) typically used in image processing. Most approaches enhance their models by adapting ensembles of different models and attempting to increase the size of the training data using various techniques. We review and discuss the results of the participants of this sub-challenge, where the winner scored a UAR of 80.1%. Moreover, we present the results of fusing the approaches, leading to a UAR of 82.6%. Finally, we present a smartphone app that can be used as a proof of concept demonstration to detect in real-time whether users are wearing a face mask; we also benchmark the run-time of the best models.","COVID-19, Deep learning, Masks, Voice biometrics, Acoustic modelling",Mostafa M. Mohamed and Mina A. Nessiem and Anton Batliner and Christian Bergler and Simone Hantke and Maximilian Schmitt and Alice Baird and Adria Mallol-Ragolta and Vincent Karas and Shahin Amiriparian and BjÃ¶rn W. Schuller,https://www.sciencedirect.com/science/article/pii/S0031320321005410,https://doi.org/10.1016/j.patcog.2021.108361,0031-3203,2022,108361,122,Pattern Recognition,Face mask recognition from audio: The MASC database and an overview on the mask challenge,article,MOHAMED2022108361,
"Image restoration (IR) has been widely used in many computer vision applications. The model-based IR methods have clear theoretical bases. However, numerous hyper-parameters need to be set empirically, which is often challenging and time-consuming. Because of the powerful nonlinear fitting ability, deep convolutional neural networks (CNNs) have been widely used in IR tasks in recent years. However, it is challenging to design new network architecture to further significantly improve the IR performance. Inspired by the plug and play (P&P) methods, we first decouple the original IR problem into two subproblems with the variable splitting technique. Then, derived from the model-based methods, a novel deep CNN framework in the transformation domain is proposed to mimic the optimization process of the two subproblems. The proposed framework is driven effectively by the target vector update (TVU) module. Extensive experiments demonstrate the effectiveness of our proposed method over other state-of-the-art IR methods.","Image restoration, Plug and play method, Convolutional neural network framework, Transformation domain, Target vector update module",Sen Zhai and Chao Ren and Zhengyong Wang and Xiaohai He and Linbo Qing,https://www.sciencedirect.com/science/article/pii/S0031320321005136,https://doi.org/10.1016/j.patcog.2021.108333,0031-3203,2022,108333,122,Pattern Recognition,An effective deep network using target vector update modules for image restoration,article,ZHAI2022108333,
"Point cloud segmentation is the foundation of 3D environmental perception for modern intelligent systems. To solve this problem and image segmentation, conditional random fields (CRFs) are usually formulated as discrete models in label space to encourage label consistency, which is actually a kind of postprocessing. In this paper, we reconsider the CRF in feature space for point cloud segmentation because it can capture the structure of features well to improve the representation ability of features rather than simply smoothing. Therefore, we first model the point cloud features with a continuous quadratic energy model and formulate its solution process as a message-passing graph convolution, by which it can be easily integrated into a deep network. We theoretically demonstrate that the message passing in the graph convolution is equivalent to the mean-field approximation of a continuous CRF model. Furthermore, we build an encoder-decoder network based on the proposed continuous CRF graph convolution (CRFConv), in which the CRFConv embedded in the decoding layers can restore the details of high-level features that were lost in the encoding stage to enhance the location ability of the network, thereby benefiting segmentation. Analogous to the CRFConv, we show that the classical discrete CRF can also work collaboratively with the proposed network via another graph convolution to further improve the segmentation results. Experiments on various point cloud benchmarks demonstrate the effectiveness and robustness of the proposed method. Compared with the state-of-the-art methods, the proposed method can also achieve competitive segmentation performance.","Point cloud segmentation, Conditional random fields, Message passing, Graph convolution, Mean-field approximation",Fei Yang and Franck Davoine and Huan Wang and Zhong Jin,https://www.sciencedirect.com/science/article/pii/S0031320321005379,https://doi.org/10.1016/j.patcog.2021.108357,0031-3203,2022,108357,122,Pattern Recognition,Continuous conditional random field convolution for point cloud segmentation,article,YANG2022108357,
"Meta-learning aims to train a classifier on collections of tasks, such that it can recognize new classes given few samples from each. However, current approaches encounter overfitting and poor generalization since the internal representation learning is obstructed by backgrounds and noises in limited samples. To alleviate those issues, we propose the Unsupervised Descriptor Selection (UDS) to tackle few-shot learning tasks. Specifically, a descriptor selection module is proposed to localize and select semantic meaningful regions in feature maps without supervision. The selected features are then mapped into novel vectors by a task-related aggregation module to enhance internal representations. With a simple network structure, UDS makes adaptation between tasks more efficient, and improves the performance in few-shot learning. Extensive experiments with various backbones are conducted on Caltech-UCSD Bird and miniImageNet, indicate that UDS achieves the comparable performance to state-of-the-art methods, and improves the performance of prior meta-learning methods.","Meta-learning, Few-shot classification, Unsupervised localization, Descriptor selection",Zhengping Hu and Zijun Li and Xueyu Wang and Saiyue Zheng,https://www.sciencedirect.com/science/article/pii/S0031320321004842,https://doi.org/10.1016/j.patcog.2021.108304,0031-3203,2022,108304,122,Pattern Recognition,Unsupervised descriptor selection based meta-learning networks for few-shot classification,article,HU2022108304,
"In this study, we present an incremental machine learning framework called Adaptive Decision Forest (ADF), which produces a decision forest to classify new records. Based on our two novel theorems, we introduce a new splitting strategy called iSAT, which allows ADF to classify new records even if they are associated with previously unseen classes. ADF is capable of identifying and handling concept drift; it, however, does not forget previously gained knowledge. Moreover, ADF is capable of handling big data if the data can be divided into batches. We evaluate ADF on nine publicly available natural datasets and one synthetic dataset, and compare the performance of ADF against the performance of eight state-of-the-art techniques. We also examine the effectiveness of ADF in some challenging situations. Our experimental results, including statistical sign test and Nemenyi test analyses, indicate a clear superiority of the proposed framework over the state-of-the-art techniques.","Incremental learning, Decision forest algorithm, Concept drift, Big data, Online learning",Md Geaur Rahman and Md Zahidul Islam,https://www.sciencedirect.com/science/article/pii/S0031320321005252,https://doi.org/10.1016/j.patcog.2021.108345,0031-3203,2022,108345,122,Pattern Recognition,Adaptive Decision Forest: An incremental machine learning framework,article,RAHMAN2022108345,
"Dilated convolution kernels are constrained by their shared dilation, keeping them from being aware of diverse spatial contents at different locations. We address such limitations by formulating the dilation as trainable weights with respect to individual positions. We propose Adaptive Dilation Convolutional Neural Networks (ADCNN), a light-weighted extension that allows convolutional kernels to adjust their dilation value based on different contents at the pixel level. Unlike previous content-adaptive models, ADCNN dynamically infers pixel-wise dilation via modeling feed-forward inter-patterns, which provides a new perspective for developing adaptive network structures other than sampling kernel spaces. Our evaluation results indicate ADCNNs can be easily integrated into various backbone networks and consistently outperform their regular counterparts on various visual tasks.","Adaptive dilated convolution, Representation learning, Image classification",Jie Yao and Dongdong Wang and Hao Hu and Weiwei Xing and Liqiang Wang,https://www.sciencedirect.com/science/article/pii/S0031320321005495,https://doi.org/10.1016/j.patcog.2021.108369,0031-3203,2022,108369,123,Pattern Recognition,ADCNN: Towards learning adaptive dilation for convolutional neural networks,article,YAO2022108369,
"It is well known that detail features and context semantics are conducive to improving object detection performance. However, the current single-prediction detectors do not well incorporate these two types of information together. To alleviate the limitation of single-prediction on the use of multiple types of information, we propose a dual detection branch network (DDBN) with adjacent feature compensation and customized training strategy for semantic diversity predictions. Different from the conventional single-prediction models, our DDBN is in the form of a single model with dual different semantic predictions. In particular, two types of adjacent feature compensations are designed to extract detail and context information from different perspectives. Also, a specialized training strategy is customized for our DDBN to well explore the diversity of predictions for improving the performance of object detection. We conduct extensive experiments on three datasets, i.e., DOTA, MS-COCO, and Pascal-VOC, and the experimental results strongly demonstrate the efficacy of our proposed model.","Adjacent feature compensation, Dual detection branch network, Diversity enhancement strategy, Object detection",Qifeng Lin and Chengjiang Long and Jianhui Zhao and Gang Fu and Zhiyong Yuan,https://www.sciencedirect.com/science/article/pii/S0031320321004957,https://doi.org/10.1016/j.patcog.2021.108315,0031-3203,2022,108315,122,Pattern Recognition,DDBN: Dual detection branch network for semantic diversity predictions,article,LIN2022108315,
"In this paper, we investigate the task of Fine-grained Sketch-based Image Retrieval (FG-SBIR), which uses hand-drawn sketches as input queries to retrieve the relevant images at the fine-grained instance level. The sketches and images come from different modalities, thus the similarity computation needs to consider both fine-grained and cross-modal characteristics. Existing solutions only focus on fine-grained details or spatial contexts, while ignoring the channel context and spatial sequence information. To mitigate such challenging problems, we propose a novel deep FG-SBIR model, which aims at inferring attention maps along channel dimension and spatial dimension, improving modules of channel attention and spatial attention, and exploring Transformer to enhance the modelâs ability for constructing and understanding spatial sequence information. We focus not only on the correlation information between two modalities of sketch and image, but also on the discrimination information inside the single modality. Mutual Loss is especially proposed to enhance the traditional triplet loss, and promote the internal discrimination ability of the model on a single modality. Extensive experiments show that our AE-Net obtains promising results on Sketchy, which is the largest public dataset available for FG-SBIR at present.","Fine-grained sketch-based image retrieval (FG-SBIR), Residual channel attention, Local self-spatial attention, Contrastive learning, Spatial sequence transformer",Yangdong Chen and Zhaolong Zhang and Yanfei Wang and Yuejie Zhang and Rui Feng and Tao Zhang and Weiguo Fan,https://www.sciencedirect.com/science/article/pii/S0031320321004714,https://doi.org/10.1016/j.patcog.2021.108291,0031-3203,2022,108291,122,Pattern Recognition,AE-Net: Fine-grained sketch-based image retrieval via attention-enhanced network,article,CHEN2022108291,
"Research on group activity recognition mostly leans on the standard two-stream approach (RGB and Optical Flow) as their input features. Few have explored explicit pose information, with none using it directly to reason about the persons interactions. In this paper, we leverage the skeleton information to learn the interactions between the individuals straight from it. With our proposed method GIRN, multiple relationship types are inferred from independent modules, that describe the relations between the body joints pair-by-pair. Additionally to the joints relations, we also experiment with the previously unexplored relationship between individuals and relevant objects (e.g. volleyball). The individuals distinct relations are then merged through an attention mechanism, that gives more importance to those individuals more relevant for distinguishing the group activity. We evaluate our method in the Volleyball dataset, obtaining competitive results to the state-of-the-art. Our experiments demonstrate the potential of skeleton-based approaches for modeling multi-person interactions.","Group activity recognition, Skeleton information, Relational network, Attention mechanisms",Mauricio Perez and Jun Liu and Alex C. Kot,https://www.sciencedirect.com/science/article/pii/S0031320321005409,https://doi.org/10.1016/j.patcog.2021.108360,0031-3203,2022,108360,122,Pattern Recognition,Skeleton-based relational reasoning for group activity analysis,article,PEREZ2022108360,
"Deep Neural Networks (DNNs) are demonstrated to be vulnerable to adversarial examples, which are crafted by adding adversarial perturbations to the legitimate examples. To address this issue, some defense methods have been proposed. Among them, the adversarial training (AT) is a popular method to improve the robustness of DNNs. However, theory analysis has shown that in the adversarial training framework, the improvement of the robustness will lead to a decline of standard accuracy. In this paper, we propose a modularized defense framework, namely Adversarial Domain Adaptation to Defense ((AD)2). Different from all adversarial training methods, (AD)2 detects adversarial example using a generative algorithm and applies the adversarial domain adaptation method to remove adversarial perturbation. Experimental results show that (AD)2 is effective to remove the adversarial perturbation and mitigate the odds between the robustness and standard accuracy for DNNs.","Deep learning, Adversarial example, Domain adaptation",Keji Han and Bin Xia and Yun Li,https://www.sciencedirect.com/science/article/pii/S0031320321004830,https://doi.org/10.1016/j.patcog.2021.108303,0031-3203,2022,108303,122,Pattern Recognition,(AD)2: Adversarial domain adaptation to defense with adversarial perturbation removal,article,HAN2022108303,
"Human activities recognition (HAR) and human intent recognition (HIR) are important for medical diagnosis and human-robot interaction. HAR and HIR usually rely on the signals of some wearable sensors, such as inertial measurement unit (IMU), but these signals may be user-dependent, which degrades the performance of the recognition algorithm on new subjects. Traditional supervised learning methods require labeling signals and training specific classifiers for each new subject, which is burdensome. To deal with this problem, this paper proposes a novel non-adversarial cross-subject adaptation method called Gaussian-guided feature alignment (GFA). The proposed GFA metric quantifies the discrepancy between the labeled features of source subjects and the unlabeled features of target subjects so that minimizing the GFA metric leads to the alignment of the source and target features. The GFA metric is estimated by calculating the divergence between the feature distribution and Gaussian distribution, as well as the mean squared error of the mean and variance between source and target features. This paper analytically proves the effect of the GFA metric and validates its performance using three public human activity datasets. Experimental results show that the proposed GFA achieves 1% higher target classification accuracy and 0.5% lower variance than state-of-the-art methods in case of cross-subject validation. These results indicate that the proposed GFA is feasible for improving the generalization of the HAR and HIR.","Domain adaptation, Feature alignment, Human activity recognition, Human intent recognition, Sensor fusion",Kuangen Zhang and Jiahong Chen and Jing Wang and Yuquan Leng and Clarence W. {de Silva} and Chenglong Fu,https://www.sciencedirect.com/science/article/pii/S0031320321005124,https://doi.org/10.1016/j.patcog.2021.108332,0031-3203,2022,108332,122,Pattern Recognition,Gaussian-guided feature alignment for unsupervised cross-subject adaptation,article,ZHANG2022108332,
"The k-Nearest Neighbor (kNN) algorithm is widely used in the supervised learning field and, particularly, in search and classification tasks, owing to its simplicity, competitive performance, and good statistical properties. However, its inherent inefficiency prevents its use in most modern applications due to the vast amount of data that the current technological evolution generates, being thus the optimization of kNN-based search strategies of particular interest. This paper introduces the caKD+ algorithm, which tackles this limitation by combining the use of feature learning techniques, clustering methods, adaptive search parameters per cluster, and the use of pre-calculated K-Dimensional Tree structures, and results in a highly efficient search method. This proposal has been evaluated using 10 datasets and the results show that caKD+ significantly outperforms 16 state-of-the-art efficient search methods while still depicting such an accurate performance as the one by the exhaustive kNN search.","-Nearest Neighbor, Efficient search, Clustering, Feature learning",Antonio Javier Gallego and Juan RamÃ³n Rico-Juan and Jose J. Valero-Mas,https://www.sciencedirect.com/science/article/pii/S0031320321005367,https://doi.org/10.1016/j.patcog.2021.108356,0031-3203,2022,108356,122,Pattern Recognition,Efficient k-nearest neighbor search based on clustering and adaptive k values,article,GALLEGO2022108356,
"As one of the important dimensionality reduction techniques, unsupervised feature selection (UFS) has enjoyed amounts of popularity over the last few decades, which can not only improve learning performance, but also enhance interpretability and reduce computational costs. The existing UFS methods often model the data in the original feature space, which cannot fully exploit the discriminative information. In this paper, to address this issue, we investigate how to strengthen the relationship between UFS and the feature subspace, so as to select relevant features more straightforwardly and effectively. Methodologically, a novel UFS approach, referred to as Graph Regularized Local Linear Embedding (GLLE), is proposed by integrating local linear embedding (LLE) and manifold regularization constrained in feature subspace into a unified framework. To be more specific, we explicitly define a feature selection matrix composed of 0 and 1, which can realize the process of UFS. For the purpose of modelling the feature selection matrix, we propose to preserve the local linear reconstruction relationship among neighboring data points in the feature subspace, which corresponds to LLE constrained in the feature subspace. To make the feature selection matrix more accurate, we propose to use manifold regularization as an assistant of LLE to find the relevant and representative features such that the selected features can make each sample under the feature subspace be accordance with the manifold assumption. A tailored iterative algorithm based on Alternative Direction Method of Multipliers (ADMM) is designed to solve the proposed optimization problem. Extensive experiments on twelve real-world benchmark datasets are conducted, and the more promising results are achieved compared with the state-of-the-arts approaches.","Unsupervised feature selection, Local linear embedding, Graph Laplacian, Manifold regularization",Jianyu Miao and Tiejun Yang and Lijun Sun and Xuan Fei and Lingfeng Niu and Yong Shi,https://www.sciencedirect.com/science/article/pii/S0031320321004799,https://doi.org/10.1016/j.patcog.2021.108299,0031-3203,2022,108299,122,Pattern Recognition,Graph regularized locally linear embedding for unsupervised feature selection,article,MIAO2022108299,
"Micro-expression recognition has become challenging, as it is extremely difficult to extract the subtle facial changes of micro-expressions. Recently, several approaches have proposed various expression-shared features algorithms for micro-expression recognition. However, these approaches do not reveal the specific discriminative characteristics, which leads to sub-optimal performance. This paper proposes a novel Feature Refinement (FeatRef) with expression-specific feature learning and fusion for micro-expression recognition that aims to obtain salient and discriminative features for specific expressions and predicts expressions by fusing expression-specific features. FeatRef consists of an expression proposal module with an attention mechanism and a classification branch. First, an inception module is designed based on optical flow to obtain expression-shared features. Second, to extract salient and discriminative features for specific expressions, expression-shared features are fed into an expression proposal module with attention factors and proposal loss. Last, in the classification branch, category labels are predicted via a fusion of expression-specific features. Experiments on three publicly available databases validate the effectiveness of FeatRef under different protocols. The results on public benchmarks demonstrate that FeatRef provides salient and discriminative information for micro-expression recognition. The results also show that FeatRef achieves better or competitive performance with existing state-of-the-art methods on micro-expression recognition.","Micro-expression recognition, Deep learning, Attention mechanism, Shared feature, Feature refinement",Ling Zhou and Qirong Mao and Xiaohua Huang and Feifei Zhang and Zhihong Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321004556,https://doi.org/10.1016/j.patcog.2021.108275,0031-3203,2022,108275,122,Pattern Recognition,Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition,article,ZHOU2022108275,
"Attributed graphs refer to graphs where both node links and node attributes are observable for analysis. Attributed graph embedding enables joint representation learning of node links and node attributes. Different from classical graph embedding methods such as Deepwalk and node2vec that first project node links into low-dimensional vectors which are then linearly concatenated with node attribute vectors as node representation, attributed graph embedding fully explores data dependence between node links and attributes by either using node attributes as class labels to supervise structure learning from node links, or reversely using node links to supervise the learning from node attributes. However, existing attributed graph embedding models are designed in continuous Euclidean spaces which often introduce data redundancy and impose challenges to storage and computation costs. In this paper, we study a new problem of discrete embedding for attributed graphs that can learn succinct node representations. Specifically, we present a Binarized Attributed Network Embedding model (BANE for short) to learn binary node representation by factorizing a Weisfeiler-Lehman proximity matrix under the constraint of binary node representation. Furthermore, based on BANE, we propose a new Low-bit Quantization for Attributed Network Representation learning model (LQANR for short) to learn even more compact node representation of bit-width values. Theoretical analysis and empirical studies on real-world datasets show that the new discrete embedding models outperform benchmark methods.","Attributed graphs, Graph embedding, Weisfeiler-Lehman graph kernels, Learning to hash, Low-bit quantization",Hong Yang and Ling Chen and Shirui Pan and Haishuai Wang and Peng Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321005483,https://doi.org/10.1016/j.patcog.2021.108368,0031-3203,2022,108368,123,Pattern Recognition,Discrete embedding for attributed graphs,article,YANG2022108368,
"Multi-camera video surveillance environment has a variety of emerging research problems among, which person re-identification is the premier one. Unsupervised person re-identification has been explored less in literature than the supervised approach. Images acquired from the video surveillance systems are unlabeled, which denotes that it is naturally an unsupervised learning problem. The state-of-the-art unsupervised methods seek external annotations support such as incorporating transfer learning techniques, partial labeling of train images, etc., which makes them not purely unsupervised and unsuitable for practical real-world surveillance settings. Identity mismatch happens due to the similar costumes and complex environmental factors. To resolve this issue, we introduce a new framework named Spatio-Temporal Association Rule based Deep Annotation-free Clustering (STAR-DAC) which incrementally clusters the unlabeled person re-identification images based on visual features and performs cluster fine-tuning through the mined spatio-temporal association rules. STAR formulations leveraged upto 75% of images for reliable sample selection through cluster fine-tuning. STAR based fine-tune algorithm aims to attain ground-truth labels of an unlabeled dataset and eliminate cluster outliers to stabilize the evaluation. Experiments are performed on image and video-based benchmark person re-identification datasets such as DukeMTMC re-ID, Market1501, MSMT17, CUHK03, GRID and Dukevideo re-ID, iLIDSVid, ViPer respectively. Experimental results clearly show that the proposed STAR-DAC framework outperforms the state-of-the-art methods in case of large scale datasets with multiple cameras.","Unsupervised person re-identification, Clustering, Labeling, Spatio-temporal, Deep learning",Sridhar Raj S and Munaga V.N.K. Prasad and Ramadoss Balakrishnan,https://www.sciencedirect.com/science/article/pii/S0031320321004672,https://doi.org/10.1016/j.patcog.2021.108287,0031-3203,2022,108287,122,Pattern Recognition,Spatio-Temporal association rule based deep annotation-free clustering (STAR-DAC) for unsupervised person re-identification,article,S2022108287,
"Visual localization to compute 6DoF camera pose from a given image has wide applications. Both local and global descriptors are crucial for visual localization. Most of the existing visual localization methods adopt a two-stage strategy: image retrieval first is performed by global descriptors, and then 2D-3D correspondences are made by local descriptors from 2D query image points and its nearest neighbor candidates which are the 3D points visible by these retrieved images. The above two stages are serially performed in these methods. However, due to the fact that 3D points obtained from the retrieval feedback are only rely on global descriptors, these methods cannot fully take the advantages of both local and global descriptors. In this paper, we propose a novel parallel search framework, which fully leverages advantages of both local and global descriptors to get nearest neighbor candidates of a 2D query image point. Specifically, besides using deep learning based global descriptors, we also utilize local descriptors to construct random tree structures for obtaining nearest neighbor candidates of the 2D query image point. We propose a new probability model and a new deep learning based local descriptor when constructing the random trees. In addition, a weighted Hamming regularization term to keep discriminativeness after binarization is given in loss function for the proposed local descriptor. The loss function co-trains both real and binary local descriptors of which the results are integrated into the random trees. Experiments on challenging benchmarks show that the proposed localization method can significantly improve the robustness and accuracy compared with the ones which get nearest neighbor candidates of a query local feature just based on either local or global descriptors.","Visual localization, 6DoF pose, Parallel search, Learning based descriptor",Pengju Zhang and Chaofan Zhang and Bingxi Liu and Yihong Wu,https://www.sciencedirect.com/science/article/pii/S0031320321005240,https://doi.org/10.1016/j.patcog.2021.108344,0031-3203,2022,108344,122,Pattern Recognition,Leveraging local and global descriptors in parallel to search correspondences for visual localization,article,ZHANG2022108344,
"Image-to-video person re-identification (I2V ReID), which aims to retrieve human targets between image-based queries and video-based galleries, has recently become a new research focus. However, the appearance misalignment and modality misalignment in both images and videos caused by pose variations, camera views, misdetections, and different data types, make I2V ReID still challenging. To this end, we propose a deep I2V ReID pipeline based on three-dimensional semantic appearance alignment (3D-SAA) and cross-modal interactive learning (CMIL) to address the aforementioned two challenges. Specifically, in the 3D-SAA module, the aligned local appearance images extracted by dense 3D human appearance estimation are in conjunction with global image and video embedding streams to learn more fine-grained identity features. The aligned local appearance images are further semantically aggregated by the proposed multi-branch aggregation network to weaken the negligible body parts. Moreover, to overcome the influence of modality misalignment, a CMIL module enables the communication between global image and video streams by interactively propagating the temporal information in videos to the channels of image feature maps. Extensive experiments on challenging MARS, DukeMTMC-VideoReID and iLIDS-VID datasets, show the superiority of our approach.","Person re-identification, Cross-modal learning, Appearance alignment",Wei Shi and Hong Liu and Mengyuan Liu,https://www.sciencedirect.com/science/article/pii/S0031320321004945,https://doi.org/10.1016/j.patcog.2021.108314,0031-3203,2022,108314,122,Pattern Recognition,Image-to-video person re-identification using three-dimensional semantic appearance alignment and cross-modal interactive learning,article,SHI2022108314,
"The data imbalance problem is a frequent bottleneck in the classification performance of neural networks. In this paper, we propose a novel supervised discriminative feature generation (DFG) method for a minority class dataset. DFG is based on the modified structure of a generative adversarial network consisting of four independent networks: generator, discriminator, feature extractor, and classifier. To augment the selected discriminative features of the minority class data by adopting an attention mechanism, the generator for the class-imbalanced target task is trained, and the feature extractor and classifier are regularized using the pre-trained features from a large source data. The experimental results show that the DFG generator enhances the augmentation of the label-preserved and diverse features, and the classification results are significantly improved on the target task. The feature generation model can contribute greatly to the development of data augmentation methods through discriminative feature generation and supervised attention methods.","Imbalanced classification, Generative adversarial networks, Discriminative feature generation, Transfer learning, Feature map regularization",Sungho Suh and Paul Lukowicz and Yong Oh Lee,https://www.sciencedirect.com/science/article/pii/S0031320321004829,https://doi.org/10.1016/j.patcog.2021.108302,0031-3203,2022,108302,122,Pattern Recognition,Discriminative feature generation for classification of imbalanced data,article,SUH2022108302,
"Incomplete multi-view clustering partitions multi-view data suffering from missing views, for which matrix factorization approaches seek the latent representation of incomplete multi-view data and constitute one effective category of methods. To exploit data properties further, manifold structure preserving is also incorporated into matrix factorization. However, previous methods optimized the data similarity matrix in the manifold structure preserving term as an unknown variable, which is not guaranteed to faithfully represent the similarities of the original multi-view data and also increases the computational difficulty. To overcome these drawbacks, in this paper, we propose Incomplete Multi-view Clustering with Cosine Similarity (IMCCS). In IMCCS, we directly calculate the cosine similarity in the original multi-view space to strengthen the ability of preserving the manifold structure of the original multi-view data. There is no need to introduce the additional variable. The manifold structure preserving term with cosine similarity and the matrix factorization term are integrated into a unified objective function. An iterative algorithm with gradient descent is designed to solve this objective. Extensive experiments on multi-view datasets show that IMCCS outperforms state-of-the-art incomplete multi-view clustering methods.","Multi-view learning, Missing view, Cosine similarity, Gradient descent, Matrix factorization",Jun Yin and Shiliang Sun,https://www.sciencedirect.com/science/article/pii/S0031320321005513,https://doi.org/10.1016/j.patcog.2021.108371,0031-3203,2022,108371,123,Pattern Recognition,Incomplete multi-view clustering with cosine similarity,article,YIN2022108371,
"Can one learn to diagnose COVID-19 under extreme minimal supervision? Since the outbreak of the novel COVID-19 there has been a rush for developing automatic techniques for expert-level disease identification on Chest X-ray data. In particular, the use of deep supervised learning has become the go-to paradigm. However, the performance of such models is heavily dependent on the availability of a large and representative labelled dataset. The creation of which is a heavily expensive and time consuming task, and especially imposes a great challenge for a novel disease. Semi-supervised learning has shown the ability to match the incredible performance of supervised models whilst requiring a small fraction of the labelled examples. This makes the semi supervised paradigm an attractive option for identifying COVID-19. In this work, we introduce a graph based deep semi-supervised framework for classifying COVID-19 from chest X-rays. Our framework introduces an optimisation model for graph diffusion that reinforces the natural relation among the tiny labelled set and the vast unlabelled data. We then connect the diffusion prediction output as pseudo-labels that are used in an iterative scheme in a deep net. We demonstrate, through our experiments, that our model is able to outperform the current leading supervised model with a tiny fraction of the labelled examples. Finally, we provide attention maps to accommodate the radiologistâs mental model, better fitting their perceptual and cognitive abilities. These visualisation aims to assist the radiologist in judging whether the diagnostic is correct or not, and in consequence to accelerate the decision.","COVID-19, Chest X-ray, Semi-Supervised learning, Deep learning, Explainability",Angelica I. Aviles-Rivero and Philip Sellars and Carola-Bibiane SchÃ¶nlieb and Nicolas Papadakis,https://www.sciencedirect.com/science/article/pii/S0031320321004544,https://doi.org/10.1016/j.patcog.2021.108274,0031-3203,2022,108274,122,Pattern Recognition,GraphXCOVID: Explainable deep graph diffusion pseudo-Labelling for identifying COVID-19 on chest X-rays,article,AVILESRIVERO2022108274,
"Hashing based methods have gained great success for cross-modal similarity search, due to its fast query speed and low storage cost. However, there are some challenging problems that need to be further solved: 1) Many approaches are sensitive to noises and outliers, because â2 norm is utilized in the objective function, the error may be amplified. 2) Most existing methods take relaxation or rounding scheme to generate binary codes, causing a large quantization loss. 3) Many supervised cross-media algorithms usually take a large nÃn matrix to preserve the similarity relationship, leading to large calculation and making them unscalable. To mitigate these challenges, we develop a novel cross-media search algorithm, i.e., robust and discrete matrix factorization hashing, dubbed RDMH. The method takes a two-step strategy. In the first phase, the â2,1 norm is utilized to improve the robustness, which makes our model not sensitive to noises and outliers. We can learn the hash codes directly by the proposed discrete optimization method instead of relaxation scheme, avoiding the large quantization loss. Moreover, RDMH correlates the hash codes and semantic labels directly instead of manipulating the large similarity matrix. In the second phase, we propose an autoencoder strategy to learn the hash functions, more valuable information can be preserved and making the hash functions more powerful. Comprehensive experiments on several databases demonstrate the superior performance and efficacy of the developed RDMH.","Cross-modal retrieval, Hashing, Autoencoder, Discrete optimization,",Donglin Zhang and Xiao-Jun Wu,https://www.sciencedirect.com/science/article/pii/S0031320321005239,https://doi.org/10.1016/j.patcog.2021.108343,0031-3203,2022,108343,122,Pattern Recognition,Robust and discrete matrix factorization hashing for cross-modal retrieval,article,ZHANG2022108343,
"Automatic facial action unit (AU) recognition has attracted great attention but still remains a challenging task, as subtle changes of local facial muscles are difficult to thoroughly capture. Most existing AU recognition approaches leverage geometry information in a straightforward 2D or 3D manner, which either ignore 3D manifold information or suffer from high computational costs. In this paper, we propose a novel geodesic guided convolution (GeoConv) for AU recognition by embedding 3D manifold information into 2D convolutions. Specifically, the kernel of GeoConv is weighted by our introduced geodesic weights, which are negatively correlated to geodesic distances on a coarsely reconstructed 3D morphable face model. Moreover, based on GeoConv, we further develop an end-to-end trainable framework named GeoCNN for AU recognition. Extensive experiments on BP4D and DISFA benchmarks show that our approach significantly outperforms the state-of-the-art AU recognition methods.","Geodesic guided convolution, 3D morphable face model, Facial action unit recognition, Emotion recognition",Yuedong Chen and Guoxian Song and Zhiwen Shao and Jianfei Cai and Tat-Jen Cham and Jianmin Zheng,https://www.sciencedirect.com/science/article/pii/S0031320321005355,https://doi.org/10.1016/j.patcog.2021.108355,0031-3203,2022,108355,122,Pattern Recognition,GeoConv: Geodesic guided convolution for facial action unit recognition,article,CHEN2022108355,
"Recently, exploring features from different layers in fully convolutional networks (FCNs) has gained substantial attention to capture context information for semantic segmentation. This paper presents a novel encoder-decoder architecture, called contextual ensemble network (CENet), for semantic segmentation, where the contextual cues are aggregated via densely usampling the convolutional features of deep layer to the shallow deconvolutional layers. The proposed CENet is trained in terms of end-to-end segmentation to match the resolution of input image, and allows us to fully explore contextual features through ensemble of dense deconvolutions. We evaluate our CENet on two widely-used semantic segmentation datasets: PASCAL VOC 2012 and CityScapes. The experimental results demonstrate our CENet achieves superior performance with respect to recent state-of-the-art results. Furthermore, we also evaluate CENet on MS COCO dataset and ISBI 2012 dataset for the task of instance segmentation and biological segmentation, respectively. The experimental results show that CENet obtains promising results on these two datasets.","Ensemble deconvolution, Semantic segmentation, FCNs, Context aggregation, Encoder-decoder networks",Quan Zhou and Xiaofu Wu and Suofei Zhang and Bin Kang and Zongyuan Ge and Longin {Jan Latecki},https://www.sciencedirect.com/science/article/pii/S0031320321004702,https://doi.org/10.1016/j.patcog.2021.108290,0031-3203,2022,108290,122,Pattern Recognition,Contextual ensemble network for semantic segmentation,article,ZHOU2022108290,
"With the advancement of deep models, research work on image captioning has led to a remarkable gain in raw performance over the last decade, along with increasing model complexity and computational cost. However, surprisingly works on compression of deep networks for image captioning task has received little to no attention. For the first time in image captioning research, we provide an extensive comparison of various unstructured weight pruning methods on three different popular image captioning architectures, namely Soft-Attention, Up-Down and Object Relation Transformer. Following this, we propose a novel end-to-end weight pruning method that performs gradual sparsification based on weight sensitivity to the training loss. The pruning schemes are then extended with encoder pruning, where we show that conducting both decoder pruning and training simultaneously prior to the encoder pruning provides good overall performance. Empirically, we show that an 80% to 95% sparse network (up to 75% reduction in model size) can either match or outperform its dense counterpart. The code and pre-trained models for Up-Down and Object Relation Transformer that are capable of achieving CIDEr scores >120 on the MS-COCO dataset but with only 8.7 MB and 14.5 MB in model size (size reduction of 96% and 94% respectively against dense versions) are publicly available at https://github.com/jiahuei/sparse-image-captioning.","Image captioning, Deep network compression, Deep learning",Jia Huei Tan and Chee Seng Chan and Joon Huang Chuah,https://www.sciencedirect.com/science/article/pii/S003132032100546X,https://doi.org/10.1016/j.patcog.2021.108366,0031-3203,2022,108366,122,Pattern Recognition,End-to-End Supermask Pruning: Learning to Prune Image Captioning Models,article,TAN2022108366,
"Multi-view clustering has become a hot yet challenging topic, due mainly to the independence of and information complementarity between different views. Although good results are achieved to a certain extent from typical methods including multi-view based k-means clustering, sparse cooperative representation clustering and subspace clustering, they still suffer from several drawbacks or limitations: (1) When each view is sparse decomposed, it still contains some hidden information for mining, such as the structure of samples, the intra-class similarity measure, and the inter-class diversity discrimination, etc. (2) Most of the existing multi-view methods only consider the local features within each view, but fail to effectively balance the importance of and combine information among different views in a diversified way. To tackle these issues, we propose a novel multi-view diversity learning model based on robust bilinear error decomposition (BED). The BED term with a low rank sparse constraint is an improved non-negative matrix factorization (NMF), which is used to extract the hidden structure information in sparse decomposition and useful diversity discrimination information in error matrix. The preservation of local features and selection of important views are achieved by adaptive weighted manifold learning. Furthermore, the Hilbert Schmidt independence criterion is used as a diversity learning term for mutual learning and fusion among views. Finally, the proposed robust low-rank multi-view diversity learning spectral clustering method is evaluated and benchmarked with eight state-of-the-art methods. Experiments in six real datasets have fully validated the significantly improved accuracy and efficiency of the proposed methodology for effective clustering of multi-view images.","Low-rank Representation (LRR), Multi-view Subspace Clustering (MVSC), Hilbert Schmidt Independence Criterion (HSIC), Non-negative Matrix Factorization (NMF), Adaptive-Weighting Manifold Learning (AWML)",Junpeng Tan and Zhijing Yang and Jinchang Ren and Bing Wang and Yongqiang Cheng and Wing-Kuen Ling,https://www.sciencedirect.com/science/article/pii/S0031320321004787,https://doi.org/10.1016/j.patcog.2021.108298,0031-3203,2022,108298,122,Pattern Recognition,A Novel Robust Low-rank Multi-view Diversity Optimization Model with Adaptive-Weighting Based Manifold Learning,article,TAN2022108298,
"Measures of distance or how data points are positioned relative to each other are fundamental in pattern recognition. The concept of depth measures how deep an arbitrary point is positioned in a dataset, and is an interesting concept in this regard. However, while this concept has received a lot of attention in the statistical literature, its application within pattern recognition is still limited. To increase the applicability of the depth concept in pattern recognition, we address the well-known computational challenges associated with the depth concept, by suggesting to estimate depth using incremental quantile estimators. The suggested algorithm can not only estimate depth when the dataset is known in advance, but can also track depth for dynamically varying data streams by using recursive updates. The tracking ability of the algorithm was demonstrated based on a real-life application associated with detecting changes in human activity from real-time accelerometer observations. Given the flexibility of the suggested approach, it can detect virtually any kind of changes in the distributional patterns of the observations, and thus outperforms detection approaches based on the Mahalanobis distance.","Data stream, Incremental quantile estimator, Distributional patterns, Real-time analytics, Tukey depth",Hugo L. Hammer and Anis Yazidi and HÃ¥vard Rue,https://www.sciencedirect.com/science/article/pii/S0031320321005197,https://doi.org/10.1016/j.patcog.2021.108339,0031-3203,2022,108339,122,Pattern Recognition,Estimating Tukey depth using incremental quantile estimators,article,HAMMER2022108339,
"Despite the impressive performance of random forests (RF), its theoretical properties have not been thoroughly understood. In this paper, we propose a novel RF framework, dubbed multinomial random forest (MRF), to analyze its consistency and privacy-preservation. Instead of deterministic greedy split rule or with simple randomness, the MRF adopts two impurity-based multinomial distributions to randomly select a splitting feature and a splitting value, respectively. Theoretically, we prove the consistency of MRF and analyze its privacy-preservation within the framework of differential privacy. We also demonstrate with multiple datasets that its performance is on par with the standard RF. To the best of our knowledge, MRF is the first consistent RF variant that has comparable performance to the standard RF. The code is available at https://github.com/jiawangbai/Multinomial-Random-Forest.","Random forest, Consistency, Differential privacy, Classification",Jiawang Bai and Yiming Li and Jiawei Li and Xue Yang and Yong Jiang and Shu-Tao Xia,https://www.sciencedirect.com/science/article/pii/S0031320321005112,https://doi.org/10.1016/j.patcog.2021.108331,0031-3203,2022,108331,122,Pattern Recognition,Multinomial random forest,article,BAI2022108331,
"In real-world application scenarios, object detection usually encounters two technical challenges, i.e., high accuracy and high speed. Although the latest detection frameworks based on anchor-free detection have achieved outstanding performance, they cannot be widely used in real-world scenarios due to their model complexity and slow speed. In this paper, inspired by cross-context attention mechanism of human visual systems, we propose a light but effective single-shot detection framework using Cross-context Attention-guided Network (CCAGNet) to balance the accuracy and speed. CCAGNet uses attention-guided mechanism to highlight the interaction of object-synergy regions, and suppresses non-object-synergy regions by combining Cross-context Attention Mechanism (CCAM), Receptive Field Attention Mechanism (RFAM), and Semantic Fusion Attention Mechanism (SFAM). The main contribution of our work includes establishing a novel attention mechanism that takes the context information of channel, spatial, cross- and adjacent-regions into consideration simultaneously. Extensive experiments demonstrate the feasibility and effectiveness of our method on the public benchmark datasets. To the best of our knowledge, CCAGNet obtains the state-of-the-art performance on both PascalVOC and MSCOCO with the excellent trade-off between accuracy and speed among single-shot detectors. Especially, the Average Precision (AP) metric is significantly improved by 17.0% on small object detection on MSCOCO.","Cross-context attention-guided network, Cross-context attention mechanism, Receptive field attention mechanism, Semantic fusion attention mechanism, Accuracy and speed balance",Shuyu Miao and Shanshan Du and Rui Feng and Yuejie Zhang and Huayu Li and Tianbi Liu and Lin Zheng and Weiguo Fan,https://www.sciencedirect.com/science/article/pii/S0031320321004386,https://doi.org/10.1016/j.patcog.2021.108258,0031-3203,2022,108258,122,Pattern Recognition,Balanced single-shot object detection using cross-context attention-guided network,article,MIAO2022108258,
"Video copy-move forgery detection (VCMFD) is a significant and greatly challenging task due to a variety of difficulties, including a huge amount of video information, diverse forgery types, rich forgery objects, and homogenous forgery sources. These difficulties raise four unresolved key challenges in VCMFD: i) ineffective detection in some popular forgery cases; ii) inefficient matching in processing numerous video pixels with hundred-dimensional features under dozens of matching iterations; iii) high false positive (FP) in detecting forgery videos; iv) low trade-off of efficiency and effectiveness in filling forgery region, and even failing in indicating forgeries at the pixel level. In this paper, a novel VCMFD method is proposed to address these issues: i) an innovatively improved SIFT structure that can address the thorough feature extraction in all video copy-move forgery cases; ii) a novel fast keypoint-label matching (FKLM) algorithm is proposed that creates some keypoint-label groups so that every high-dimensional feature is assigned into one of these groups. As a result, matching of video pixels can be directly done on a small number of keypoint-label groups only, leading to a nearly 500% raise in matching efficiency; iii) a new coarse-to-fine filtering relying on intrinsic attributes of exact keypoint-matches is designed to more effectively reduce the false keypoint-matches; iv) the adaptive block filling relying on true keypoint-matches contributes to the accurate and efficient suspicious region filling, even at the pixel level. Finally, the suspicious region locations with the forgery vision persistence concept indicate forgery videos. Compared to the state-of-art methods, the experiments show that our proposed method achieves the best detection accuracy, lowest FP, and improved at least 16% and 8% of F1 scores on the GRIP 2.0 dataset and a combination of SULFA 2.0 & REWIND datasets. Furthermore, the proposed method is with low computational time (4.45 s/Mpixels), which is about 1/2-1/3 times of the latest DFMI-BM (8.02 s/Mpixels) and PM-2D (13.1 s/Mpixels) methods.","Video copy-move forgery detection, Thorough feature extraction, Fast keypoint-label matching, Coarse-to-fine filtering, Adaptive block filling",Jun-Liu Zhong and Yan-Fen Gan and Chi-Man Vong and Ji-Xiang Yang and Jing-Hong Zhao and Jia-Hua Luo,https://www.sciencedirect.com/science/article/pii/S0031320321004660,https://doi.org/10.1016/j.patcog.2021.108286,0031-3203,2022,108286,122,Pattern Recognition,Effective and efficient pixel-level detection for diverse video copy-move forgery types,article,ZHONG2022108286,
"Multiple Instance Learning (MIL) is a fundamental method for weakly supervised object detection (WSOD), but experiences difficulty in excluding local optimal solutions and may miss objects or falsely localize object parts. In this paper, we introduce discrepantly collaborative modules into MIL and thereby create discrepant multiple instance learning (D-MIL), pursuing optimal solutions in a simple-yet-effective way. D-MIL adopts multiple MIL learners to pursue discrepant yet complementary solutions indicating object parts, which are fused with a collaboration module for precise object localization. D-MIL implements a new âteachers-studentsâ model, where MIL learners act as âteachersâ and object detectors as âstudentsâ. Multiple teachers provide rich yet complementary information, which are absorbed by students and transferred back to reinforce the performance of teachers. Experiments show that D-MIL significantly improves the baseline while achieves state-of-the-art performance on the challenging MS-COCO object detection benchmark.","Weakly supervised detection, Multiple instance learning, Learner discrepancy, Collaborative learning",Wei Gao and Fang Wan and Jun Yue and Songcen Xu and Qixiang Ye,https://www.sciencedirect.com/science/article/pii/S0031320321004143,https://doi.org/10.1016/j.patcog.2021.108233,0031-3203,2022,108233,122,Pattern Recognition,Discrepant multiple instance learning for weakly supervised object detection,article,GAO2022108233,
"Point correspondence is a fundamental problem in pattern recognition and computer vision, which can be tackled by graph matching. Since graph matching is basically an NP-complete problem, some approximate methods are proposed to solve it. Continuous relaxation offers an effective approximate method for graph matching problem. However, the discrete constraint is not taken into consideration in the optimization step. In this paper, a fast normalized cut based graph matching method is proposed, where the discrete constraint is introduced into the optimization step. Specifically, first a semidefinite positive affinity matrix based form objective function is constructed by introducing a regularization term which is related to the discrete constraint. Then the fast normalized cut algorithm is utilized to find the continuous solution. Last, the discrete solution of graph matching is obtained by a multiplicative update algorithm. Experiments on both synthetic points and real-world images validate the effectiveness of the proposed method by comparing it with the state-of-the-art methods.","Graph matching, Fast normalized cut, Discrete constraint, Multiplicative update",Jing Yang and Xu Yang and Zhang-Bing Zhou and Zhi-Yong Liu,https://www.sciencedirect.com/science/article/pii/S003132032100409X,https://doi.org/10.1016/j.patcog.2021.108228,0031-3203,2022,108228,122,Pattern Recognition,Graph matching based on fast normalized cut and multiplicative update mapping,article,YANG2022108228,
"Scene graph generation (SGG) plays an important role in deep understanding of the visual scene. Despite the empirical success of traditional methods in many applications, they still have several challenges in the high computational complexity of dense graph and the inaccurate pruning of sparse graph. To tackle these problems, we propose a novel deep sparse graph attention network to mine the rich contextual clues and simultaneously preserve the statistical co-occurrence knowledge of SGG. Specifically, our Relationship Measurement Network (RelMN) is adapted to first classify all object pairs in dense graph as the foreground and background categories to filter the false relationships and then construct a sparse graph efficiently. Meanwhile, we design a novel feature aggregation and update method via graphical message passing to jointly learn the node and edge features for object recognition and relationship classification in the graph attention network. Extensive experimental results on the large scale VG and VRD datasets demonstrate our proposed method outperforms several state-of-the-art approaches.","Scene graph generation, Statistical co-occurrence knowledge, Relationship measurement network, Graph attention network, Sparse graph",Hao Zhou and Yazhou Yang and Tingjin Luo and Jun Zhang and Shuohao Li,https://www.sciencedirect.com/science/article/pii/S0031320321005471,https://doi.org/10.1016/j.patcog.2021.108367,0031-3203,2022,108367,123,Pattern Recognition,A unified deep sparse graph attention network for scene graph generation,article,ZHOU2022108367,
"Spectral clustering became a popular choice for data clustering for its ability of uncovering clusters of different shapes. However, it is not always preferable over other clustering methods due to its computational demands. One of the effective ways to bypass these computational demands is to perform spectral clustering on a subset of points (data representatives) then generalize the clustering outcome, this is known as approximate spectral clustering (ASC). ASC uses sampling or quantization to select data representatives. This makes it vulnerable to 1) performance inconsistency (since these methods have a random step either in initialization or training), 2) local statistics loss (because the pairwise similarities are extracted from data representatives instead of data points). We proposed a refined version of k-nearest neighbor graph, in which we keep data points and aggressively reduce number of edges for computational efficiency. Local statistics were exploited to keep the edges that do not violate the intra-cluster distances and nullify all other edges in the k-nearest neighbor graph. We also introduced an optional step to automatically select the number of clusters C. The proposed method was tested on synthetic and real datasets. Compared to ASC methods, the proposed method delivered a consistent performance despite significant reduction of edges.","Spectral clustering, Approximate spectral clustering, -nearest neighbor graph, Local scale similarity",Mashaan Alshammari and John Stavrakakis and Masahiro Takatsuka,https://www.sciencedirect.com/science/article/pii/S003132032100056X,https://doi.org/10.1016/j.patcog.2021.107869,0031-3203,2021,107869,114,Pattern Recognition,Refining a k-nearest neighbor graph for a computationally efficient spectral clustering,article,ALSHAMMARI2021107869,
"Text in videos usually acts as important semantic cues, which is helpful to video analysis. Video text detection is considered as one of the most difficult tasks in document analysis due to the following two challenges: 1) the difficulties caused by video scenes, i.e., motion blur, illumination changes, and occlusion; 2) the properties of text including variants of fonts, languages, orientations, and shapes. Most existing methods try to improve the video text detection through video text tracking, but treat these two tasks separately. This can significantly increase the amount of calculations and cannot take full advantage of the supervisory information of both tasks. In this work, we introduce explainable descriptor, combines appearance, geometry and PHOC features, to establish a bridge between detection and tracking and build an end-to-end video text detection model with online tracking to address these challenges together. By integrating these two branches into one trainable framework, they can promote each other and the computational cost is significantly reduced. Besides, the introduce explainable descriptor also make our end-to-end model have inherent interpretability. Experiments on existing video text benchmarks including ICDAR 2013 Video, DOST, Minetto and YVT verify the role of explainable descriptors in improving model expression ability and the proposed method significantly outperforms state-of-the-art methods. Our method improves F-score by more than 2% on all datasets and achieves 81.52% on the MOTA of the Minetto dataset.","End-to-end, Video text detection, Online tracking",Hongyuan Yu and Yan Huang and Lihong Pi and Chengquan Zhang and Xuan Li and Liang Wang,https://www.sciencedirect.com/science/article/pii/S003132032030594X,https://doi.org/10.1016/j.patcog.2020.107791,0031-3203,2021,107791,113,Pattern Recognition,End-to-end video text detection with online tracking,article,YU2021107791,
"Traditional outlier detection methods create a model for data and then label as outliers for objects that deviate significantly from this model. However, when dat has many outliers, outliers also pollute the model. The model then becomes unreliable, thus rendering most outlier detectors to become ineffective. To solve this problem, we propose a mean-shift outlier detector. This detector employs a mean-shift technique to modify data and cancel the bias caused by the outliers. The mean-shift technique replaces every object by the mean of its k-nearest neighbors which essentially removes the effect of outliers before clustering without the need to know the outliers. In addition, it also detects outliers based on the distance shifted. Our experiments show that the proposed method works well regardless of the number of outliers in the data. This method outperforms all state-of-the-art methods tested, with both real-world numeric datasets as well as generated numeric and string datasets.","Outlier detection, Anomaly detection, Mean-shift, Medoid-shift, Clustering, Noise filtering, Outlier filtering",Jiawei Yang and Susanto Rahardja and Pasi FrÃ¤nti,https://www.sciencedirect.com/science/article/pii/S0031320321000613,https://doi.org/10.1016/j.patcog.2021.107874,0031-3203,2021,107874,115,Pattern Recognition,Mean-shift outlier detection and filtering,article,YANG2021107874,
"Plant leaf identification is a significant challenge in the fields of computer vision and pattern recognition. This article presents a new approach to plant leaf identification, one that integrates shape and texture characteristics. First, we introduce the shape and texture features used by the proposed plant leaf recognition method. The proposed multiscale triangle descriptor (MTD) is employed to characterize the shape information of a plant leaf, and the local binary pattern histogram Fourier (LBP-HF) is used as the texture feature. Then, the shape and texture features of a leaf image are combined by weighted distance measurement, where L1 distance and chi-square distance are used for shape and texture features, respectively. The proposed approach provides a robust descriptor for the task of plant leaf recognition by combining the complementary MTD and LBP-HF features. The proposed approach has been thoroughly evaluated on three benchmark leaf datasets, including the Flavia, Swedish and MEW2012 leaf datasets. Our method achieves 77.6%, 85.7%, and 67.5% retrieval accuracy on the Flavia, Swedish and MEW2012 leaf datasets, respectively, while the corresponding classification accuracy is 99.1%, 98.4%, 95.6%. The recognition performance of our method is better or comparable to prior state-of-the-art plant leaf recognition method.","Plant leaf recognition, Multiscale triangle descriptor, Local binary pattern, Shape descriptor, Texture feature",Chengzhuan Yang,https://www.sciencedirect.com/science/article/pii/S0031320320306129,https://doi.org/10.1016/j.patcog.2020.107809,0031-3203,2021,107809,112,Pattern Recognition,Plant leaf recognition by integrating shape and texture features,article,YANG2021107809,
"There is a growing importance of feature extraction in transferring valuable knowledge from a source domain to a different but related target domain. However, when the target data are contaminated by unpredictable and complex noises, the ability of most existing feature extraction methods would be limited. In this paper, we deeply investigate the robust property of Kernel Mean P-Power Error Loss (KMPE-Loss), and thus propose a novel Robust Transfer Feature Learning (RTFL) method to enhance the robustness of domain adaptation. The key idea of RTFL is to learn a shared transformation by: 1) detecting and neglecting the contaminated target points without any specific assumption on noises; 2) reconstructing the remaining clean target points using the corresponding source-domain neighborhood; 3) incorporating a relative entropy based regularization to reap theoretic advantages. Consequently, the distribution difference between two domains is accurately reduced for knowledge transfer. We propose an alternative procedure to optimize RTFL with explicitly guaranteed convergence. As an extension, the transformation based matrix in RTFL is restricted to a small dimension basis, admitting the highly reduced computation complexity. Extensive experiments in various domain adaptation tasks demonstrate the superiority of our methods.","Domain adaptation, Linear transformation, Correntropy, Kernel mean p-power error loss, Half-quadratic optimization",Wei Wang and Hao Wang and Zhi-Yong Ran and Ran He,https://www.sciencedirect.com/science/article/pii/S0031320321000571,https://doi.org/10.1016/j.patcog.2021.107870,0031-3203,2021,107870,114,Pattern Recognition,Learning Robust Feature Transformation for Domain Adaptation,article,WANG2021107870,
"Online social media (OSM) has become a hotbed for the rapid dissemination of disinformation or rumour. Therefore, rumour detection, especially early rumour detection (ERD), is very challenging given the limited, incomplete and noisy information. Although there are some researches on earlier rumour detection, most of their studies require a larger dataset or a longer detection time span, i.e., the rumour detection efficiency needs to be improved. In this paper, we focus on a shorter detection time span which also means fewer online posts to achieve the task of ERD. We proposed a novel post-based augmentation representation approach to process post content of rumour events in the early stages of their dissemination, i.e., backward compression mapping mechanism (BCMM). In addition, we combine BCMM with gated recurrent unit (GRU) to represent post content, topology network of posts and metadata extracted from post datasets. We apply a three-layers GRU to enhance the representation of dataset within one hour after the occurrence of a social media event, i.e., BCMM-GRU. The steps are as follows: (1) we input the first-hour data into the first layer; (2) the first 40Â min of data are channelled into the second layer with the output of the first layer making a full mapping to the second layer simultaneously; (3) the first 20Â min of data are sent to the third layer while the output of the second layer applies a full mapping to the third layer simultaneously. The evaluation of BCMM-GRUâs performance entails applying k-fold cross-validation (CV) set-up on four available real-life rumour event datasets. The experimental results are superior to the baselines and model variants and achieve a high accuracy of 80.09% and F1-score of 80.18%.","Early rumour detection, Topology network, Metadata, Backward compression mapping, Semantic augmentation",Yongcong Luo and Jing Ma and Chai Kiat Yeo,https://www.sciencedirect.com/science/article/pii/S0031320321000054,https://doi.org/10.1016/j.patcog.2021.107818,0031-3203,2021,107818,113,Pattern Recognition,BCMM: A novel post-based augmentation representation for early rumour detection on social media,article,LUO2021107818,
"Line segment detectors based on local image domain passively fit a line segment from a set of pixels, but no constraint on line geometry is set in the grouping process. Therefore, unstable pixels, such as the pixels in grass, clouds, or weak gradient edges, may cause false positives and fractures. This paper proposes the detector named AG3line, which employs an efficient active grouping strategy. In AG3line, the pixel for the next grouping is calculated actively with the line geometry and it can even be accurate to one pixel. To reduce the fracture caused by unstable pixels, when the adjacent pixel cannot satisfy the grouping rules, the candidate pixels for the next grouping are expanded with the line geometry constraint. To furtherly control false positives, AG3line then validates and refines the line segments by exploiting both the line geometry and the alignment of gradient magnitude. When AG3line was evaluated utilizing the image dataset with the ground truth, it outperformed both the classical and the latest detectors.The implementation of AG3line is available at https://github.com/weidong-whu/AG3line.","Line extraction, Active grouping, Anchor map, Alignment of gradient magnitude",Yongjun Zhang and Dong Wei and Yansheng Li,https://www.sciencedirect.com/science/article/pii/S0031320321000212,https://doi.org/10.1016/j.patcog.2021.107834,0031-3203,2021,107834,113,Pattern Recognition,AG3line: Active grouping and geometry-gradient combined validation for fast line segment extraction,article,ZHANG2021107834,
"We propose a real-time approach for sufficient dimension reduction. Compared with popular sufficient dimension reduction methods including sliced inverse regression and principal support vector machines, the proposed principal least squares support vector machines approach enjoys better estimation of the central subspace. Furthermore, this new proposal can be used in the presence of streamed data for quick real-time updates. It is demonstrated through simulations and real data applications that our proposal performs better and faster than existing algorithms in the literature.","Central subspace, Ladle estimator, Online sliced inverse regression, Principal support vector machines, Streamed data",Andreas Artemiou and Yuexiao Dong and Seung Jun Shin,https://www.sciencedirect.com/science/article/pii/S0031320320305719,https://doi.org/10.1016/j.patcog.2020.107768,0031-3203,2021,107768,112,Pattern Recognition,Real-time sufficient dimension reduction through principal least squares support vector machines,article,ARTEMIOU2021107768,
"Despite the great performance in layout analysis tasks made by semantic segmentation, they usually need a large number of annotated images for training and are difficult to learn a new category which is absent in the training categories. Meta-learning and few-shot segmentation have been developed to solve the above two difficulties. In this paper, we propose a novel method dubbed Few-Shot Prototype Alignment Regularization Network (FS-PARN). The FS-PARN method is inspired by recent studies in both metric learning and few-shot segmentation, which just need a few annotated images to solve the above two difficulties. Our FS-PARN method can make better use of the information of the support set by metric learning and have a better effect on image segmentation. It learns classification prototype within an embedding space and then completes pixel classification by matching each pixel on the query image with the learned prototype. In addition to obtaining high-quality prototypes through metric learning methods, our FS-PARN method also introduces prototype alignment regularization between support and query sets to make segmentation better. Notably, our FS-PARN model achieves the mean-IoU score of 28.8% and 31.7% on the practical document image datasets, Â i.e.Â  PASCAL-5i, DSSE-200, and Layout Analysis Dataset, for 1-shot and 5-shot settings respectively.","Meta-learning, Few-shot learning, Metric learning, Semantic segmentation",Yujie Li and Pengfei Zhang and Xing Xu and Yi Lai and Fumin Shen and Lijiang Chen and Pengxiang Gao,https://www.sciencedirect.com/science/article/pii/S0031320321000698,https://doi.org/10.1016/j.patcog.2021.107882,0031-3203,2021,107882,115,Pattern Recognition,Few-shot prototype alignment regularization network for document image layout segementation,article,LI2021107882,
"Weakly-supervised semantic segmentation aims at tackling the dense labeling task using weak supervision so as to reduce human annotation efforts. For weakly-supervised semantic segmentation using only image-level annotation, we propose a novel model of Learning with Saliency and Incremental Supervision Updating (LSISU), in which both the guidances of saliency prior and class information are jointly used and the segmentation supervision is dynamically updated. In the proposed LSISU, we present an image saliency objective complementary to classification loss, by which the trained weakly-supervised deep network can effectively deal with object co-occurrence problem. Meanwhile, we make full use of the class-wise pooling strategy to generate initial mask estimation of high quality. Given an initial annotation, a segmentation network is learned along with incremental supervision updating, which plays a role of region expansion and corrects the falsely estimated supervision for training images. The incremental supervision updating is performed on the fly and involves repeated usage of a fully connected conditional random field algorithm. LSISU achieves superior segmentation performance in terms of mIoU metric on benchmark datasets, which are 62.5% on the PASCAL VOC 2012 test set and 30.1% on the COCO val set.","Weakly-supervised, Semantic segmentation, Convolution neural networks",Wenfeng Luo and Meng Yang and Weishi Zheng,https://www.sciencedirect.com/science/article/pii/S0031320321000455,https://doi.org/10.1016/j.patcog.2021.107858,0031-3203,2021,107858,115,Pattern Recognition,Weakly-supervised semantic segmentation with saliency and incremental supervision updating,article,LUO2021107858,
"Under big data, large number of features as well as their complex data types makes traditional feature selection and knowledge reasoning in CBR system not adapt to new condition. To solve these problems, first, this paper proposes Weighted Relative Probability Change of Solution Parameters (WRPCSP) algorithm to execute feature selection. Then, this paper integrates Bayesian network (BN) with CBR system for knowledge reasoning. Based on probability calculation and reasoning, WRPCSP algorithm together with BN allows the proposed CBR system to well work under big data. In addition, to overcome the efficiency problem caused by large number of features, this paper also proposes Group-Outside (GO) algorithm to assign the computing task of big data for parallel data processing. GO algorithm can make the computing capacity of Hadoop fully utilized to gain the least time costing for parallel data processing. Finally, lots of experiments are performed to validate the proposed method.","WRPCSP algorithm, GO algorithm, Case based reasoning, Machine learning",Yuan Guo and Bing Zhang and Y. Sun and K. Jiang and K. Wu,https://www.sciencedirect.com/science/article/pii/S0031320320306087,https://doi.org/10.1016/j.patcog.2020.107805,0031-3203,2021,107805,112,Pattern Recognition,Machine learning based feature selection and knowledge reasoning for CBR system under big data,article,GUO2021107805,
"Convolutional neural networks have been successful lately enabling companies to develop neural-based products, which demand an expensive process, involving data acquisition and annotation; and model generation, usually requiring experts. With all these costs, companies are concerned about the security of their models against copies and deliver them as black-boxes accessed by APIs. Nonetheless, we argue that even black-box models still have some vulnerabilities. In a preliminary work, we presented a simple, yet powerful, method to copy black-box models by querying them with natural random images. In this work, we consolidate and extend the copycat method: (i) some constraints are waived; (ii) an extensive evaluation with several problems is performed; (iii) models are copied between different architectures; and, (iv) a deeper analysis is performed by looking at the copycat behavior. Results show that natural random images are effective to generate copycats for several problems.","Deep learning, Convolutional neural network, Neural network attack, Stealing network knowledge, Knowledge distillation",Jacson Rodrigues Correia-Silva and Rodrigo F. Berriel and Claudine Badue and Alberto F. {De Souza} and Thiago Oliveira-Santos,https://www.sciencedirect.com/science/article/pii/S0031320321000170,https://doi.org/10.1016/j.patcog.2021.107830,0031-3203,2021,107830,113,Pattern Recognition,Copycat CNN: Are random non-Labeled data enough to steal knowledge from black-box models?,article,CORREIASILVA2021107830,
"The main learning strategy of the PCANet is using Principal Component Analysis (PCA) for learning the convolutional filters from the data. The assumption that all the image patches are sampled from a single Gaussian component is implicitly taken, which is too strong. In this paper, the image patches are modeled using mixtures of probabilistic principal component analysis (MPPCA) and the corresponding MPPCANet (PCANet constructed using mixtures of probabilistic principal component analysis) is proposed. The proposed model is applied to the few-shot learning scenario. In the proposed framework, the image patches are assumed to come from several suppositions of Gaussian components. In the process of estimating the parameters of the MPPCA model, the clustering of the training image patches and the principal components of each cluster are simultaneously obtained. The number of mixture components is automatically determined during the optimization procedure. The theoretical insights of the proposed MPPCANet is elaborated by comparing with our prior work, CPCANet (PCANet with clustering-based filters). The proposed MPPCANet is evaluated on several benchmarking visual data sets in the experiment. It is compared with the original PCANet, CPCANet and several state-of-the-art methods. The experimental results show that the proposed MPPCANet has improved significantly the recognition capability of the original PCANet under the few-shot learning scenario. The performance of the MPPCANet is also better than the CPCANet in most cases.","Feedforward learning, PCANet, Mixtures of probabilistic principal component analysis",Yu Song and Changsheng Chen,https://www.sciencedirect.com/science/article/pii/S0031320320305951,https://doi.org/10.1016/j.patcog.2020.107792,0031-3203,2021,107792,113,Pattern Recognition,MPPCANet: A feedforward learning strategy for few-shot image classification,article,SONG2021107792,
"We focus on unsupervised domain adaptation (UDA) in image segmentation. Existing works address this challenge largely by aligning inter-domain representations, which may lead over-alignment that impairs the semantic structures of images and further target-domain segmentation performance. We design a scale variance minimization (SVMin) method by enforcing the intra-image semantic structure consistency in the target domain. Specifically, SVMin leverages an intrinsic property that simple scale transformation has little effect on the semantic structures of images. It thus introduces certain supervision in the target domain by imposing a scale-invariance constraint while learning to segment an image and its scale-transformation concurrently. Additionally, SVMin is complementary to most existing UDA techniques and can be easily incorporated with consistent performance boost but little extra parameters. Extensive experiments show that our method achieves superior domain adaptive segmentation performance as compared with the state-of-the-art. Preliminary studies show that SVMin can be easily adapted for UDA-based image classification.","Unsupervised domain adaptation, Image segmentation, Semantic structure, Variance minimization, Adversarial learning",Dayan Guan and Jiaxing Huang and Shijian Lu and Aoran Xiao,https://www.sciencedirect.com/science/article/pii/S0031320320305677,https://doi.org/10.1016/j.patcog.2020.107764,0031-3203,2021,107764,112,Pattern Recognition,Scale variance minimization for unsupervised domain adaptation in image segmentation,article,GUAN2021107764,
"Few-shot learning learns to classify unseen data with few training samples in hand and has attracted increasing attentions recently. In this paper, we propose a novel Temperature Network to tackle few-shot learning tasks motivated by three crucial factors that are seldom considered in the existing literature. First, to encourage compact intra-class distribution, a general improvement for prototype-based methods is proposed to ensure compact intra-class distribution and the effectiveness is theoretically and experimentally validated. Second, the proposed Temperature Network can implicitly generate query-specific prototypes and thus enjoys a more effective distribution-aware metric. Third, to further strengthen the generalization ability of the proposed model, a novel and simple large-margin based method is developed by leveraging the temperature function and we gradually tune the learning temperature to stabilize the training process. Moreover, we note that the commonly used datasets in few-shot learning are actually contrived from large-scale datasets, and thus may not represent a real few-shot problem. We propose a real-life few shot problem, i.e., Dermnet skin disease, to comprehensively evaluate the performance of few-shot learning methods. Experiments conducted on conventional datasets as well as the proposed skin disease dataset demonstrate the superiority of the proposed method over other state-of-the-art methods. The source code of our method is available.11https://github.com/zwvews/TemperatureNetwork.git","Few-shot learning, Metric learning, Skin lesion classification, Temperature function",Wei Zhu and Wenbin Li and Haofu Liao and Jiebo Luo,https://www.sciencedirect.com/science/article/pii/S0031320320306002,https://doi.org/10.1016/j.patcog.2020.107797,0031-3203,2021,107797,112,Pattern Recognition,Temperature network for few-shot learning with distribution-aware large-margin metric,article,ZHU2021107797,
"In real-world applications, data instances are not only related to high-dimensional features, but also interconnected with each other. However, the interconnection information has not been fully exploited for feature selection. To address this issue, we propose a novel feature selection algorithm, called dual space latent representation learning for unsupervised feature selection (DSLRL), which exploits the internal association information of data space and feature space to guide feature selection. Firstly, based on latent representation learning in data space, DSLRL produces dual space latent representation learning, which characterizes the inherent structure of data space and feature space, respectively. Secondly, in order to overcome the problem of the lack of label information, DSLRL optimizes the low-dimensional latent representation matrix of data space as a pseudo-label matrix to provide clustering indicators. Moreover, the latent representation matrix of feature space is unified with the transformation matrix to benefit the matching of the data matrix and the clustering indicator matrix. In addition, DSLRL uses non-negative and orthogonal conditions to constrain the sparse transform matrix, making it more accurate for evaluating features. Finally, an alternating method is employed to optimize the objective function. Compared with seven state-of-the-art algorithms, experimental results on twelve datasets show the effectiveness of DSLRL.","Latent representation learning, Unsupervised feature selection, Dual space, Sparse regression",Ronghua Shang and Lujuan Wang and Fanhua Shang and Licheng Jiao and Yangyang Li,https://www.sciencedirect.com/science/article/pii/S0031320321000601,https://doi.org/10.1016/j.patcog.2021.107873,0031-3203,2021,107873,114,Pattern Recognition,Dual space latent representation learning for unsupervised feature selection,article,SHANG2021107873,
"While Zero Shot Learning models can recognize new classes without training examples, they often fails to incorporate both seen and unseen classes together at the test time, which is known as the Generalized Zero-shot Learning (GZSL) problem. This paper identifies a bottleneck issue when attributes are not well-defined, reliable, inaccurate in quantitative representations, or suffering from the visual-semantic discrepancy. We propose a Generic Plug-in Attribute Correction (GPAC) module which can effectively accommodate conventional ZSL in GZSL tasks. Different from existing embedding-based approaches which often lose the favor of transparency in attributes, our key challenge is to fully preserve the original meaning of the attributes and make it complementary and interpretable to upgrade existing ZSL models. To this end, we propose a novel nonnegative constraint with iterative Stochastic Gradient Descent toolbox to effectively fit our GPAC module into previous ZSL models. Extensive experiments on five popular datasets show that our method can effectively correct attributes and make conventional ZSL can achieve state-of-the-art performance on GZSL tasks. It is also a good practice for future models when incorporating prior human knowledge.","Generalized zero shot learning (GZSL), Attribute correction, Orthogonal constraint, pluggable module",Haofeng Zhang and Haoyue Bai and Yang Long and Li Liu and Ling Shao,https://www.sciencedirect.com/science/article/pii/S0031320320305707,https://doi.org/10.1016/j.patcog.2020.107767,0031-3203,2021,107767,112,Pattern Recognition,A plug-in attribute correction module for generalized zero-shot learning,article,ZHANG2021107767,
"Current deep learning-based texture recognition methods extract spatial orderless features from pre-trained deep learning models that are trained on large-scale image datasets. These methods either produce high dimensional features or have multiple steps like dictionary learning, feature encoding and dimension reduction. In this paper, we propose a novel end-to-end learning framework that not only overcomes these limitations, but also demonstrates faster learning. The proposed framework incorporates a residual pooling layer consisting of a residual encoding module and an aggregation module. The residual encoder preserves the spatial information for improved feature learning and the aggregation module generates orderless feature for classification through a simple averaging. The feature has the lowest dimension among previous deep texture recognition approaches, yet it achieves state-of-the-art performance on benchmark texture recognition datasets such as FMD, DTD, 4D Light and one industry dataset used for metal surface anomaly detection. Additionally, the proposed method obtains comparable results on the MIT-Indoor scene recognition dataset. Our codes are available at https://github.com/maoshangbo/DRP-Texture-Recognition.","Texture recognition, Residual pooling",Shangbo Mao and Deepu Rajan and Liang Tien Chia,https://www.sciencedirect.com/science/article/pii/S0031320321000042,https://doi.org/10.1016/j.patcog.2021.107817,0031-3203,2021,107817,112,Pattern Recognition,Deep residual pooling network for texture recognition,article,MAO2021107817,
"This paper proposes a novel algorithm for fast and accurate Hausdorff distance (HD) computation. The Hausdorff distance is used to measure the similarity between two point sets in various applications. However, it is hard to compute the HD algorithm efficiently between very large-scale point sets while ensuring the accuracy of the HD. The directed HD algorithm has two loops (called the outer loop and the inner loop) for calculating MAX-MIN distance, and the state-of-the-art algorithms, such as the Early break method and the Diffusion search method, focused on reducing the iterations of the inner loop. Our algorithm, however, concentrates on reducing the iterations of the outer loop. The proposed method simultaneously computes the temporary HD and temporary minimum distances of points corresponding to the outer loop using the opposite HD computation with very small systematic samples. Thereafter, a strategy of ruling out is employed to exclude non-contributing points. The new approach reduces the problems of different grid sizes and highly overlapping point sets as well as the very large-scale point sets. 3-D point clouds and real brain tumor segmentation (MRI 3-D volumes) are used for comparing the performance of the proposed algorithm and the state-of-the-art HD algorithms. In experimental results with 3-D point clouds, the proposed method is more than at least 1.5 times as faster as the compared algorithms. And, in experimental results with MRI 3-D volumes, the proposed method achieves a better performance than the compared algorithms over all pairs regardless of the grid size. Thus, as a whole, the proposed algorithm outperforms the compared algorithms.","Hausdorff distance, Computational complexity, Point matching, 3-D point sets",Jegoon Ryu and Sei-ichiro Kamata,https://www.sciencedirect.com/science/article/pii/S0031320321000443,https://doi.org/10.1016/j.patcog.2021.107857,0031-3203,2021,107857,114,Pattern Recognition,An efficient computational algorithm for Hausdorff distance based on points-ruling-out and systematic random sampling,article,RYU2021107857,
"Recently softmax based loss functions have surged to advance image classification and face verification. Most efforts boost discrimination of the softmax loss by using novel angular margins in varying ways, but few analyze where the discrimination truly comes from whilst considering the power of relieving the overfitting to enhance the softmax loss. In this paper, we firstly delve into such mainstream of softmax based loss functions in theory, and recognize the importance of easing overfitting to the softmax loss. In terms of such analysis, this paper intends to bring the softmax loss up to the competitive level with current well-behaved loss functions. We do this in two ways: (1) regularizing the softmax to relieve the overfitting by learning the log-probability centers, and (2) rescaling deep embeddings of the softmax with a constant scale to further enhance inter-class separability in Euclidean space. We call the resulting loss function rLogCenter loss for short. Simple and interpretable as our loss is, it guides CNNs to induce performance gains in the experiments of both image classification and face verification.","Deep discriminative embedding, Softmax loss, Easing overfitting",Huayue Cai and Xiang Zhang and Long Lan and Guohua Dong and Chuanfu Xu and Xinwang Liu and Zhigang Luo,https://www.sciencedirect.com/science/article/pii/S003132032100039X,https://doi.org/10.1016/j.patcog.2021.107852,0031-3203,2021,107852,114,Pattern Recognition,Learning deep discriminative embeddings via joint rescaled features and log-probability centers,article,CAI2021107852,
"Multi-label classification (MLC) studies the problem where each instance is associated with multiple relevant labels, which leads to the exponential growth of output space. It confronts with the great challenge for the exploration of the latent label relationship and the intrinsic correlation between feature and label spaces. MLC gave rise to a framework named label compression (LC) to obtain a compact space for efficient learning. Nevertheless, most existing LC methods failed to consider the influence of the feature space or misguided by original problematic features, which may result in performance degradation instead. In this paper, we present a compact learning (CL) framework to embed the features and labels simultaneously and with mutual guidance. The proposal is a versatile concept that does not rigidly adhere to some specific embedding methods, and is independent of the subsequent learning process. Following its spirit, a simple yet effective implementation called compact multi-label learning (CMLL) is proposed to learn a compact low-dimensional representation for both spaces. CMLL maximizes the dependence between the embedded spaces of the labels and features, and minimizes the loss of label space recovery concurrently. Theoretically, we provide a general analysis for different embedding methods. Practically, we conduct extensive experiments to validate the effectiveness of the proposed method.","Machine learning, Multi-label classification, Label compression, Compact learning",Jiaqi Lv and Tianran Wu and Chenglun Peng and Yunpeng Liu and Ning Xu and Xin Geng,https://www.sciencedirect.com/science/article/pii/S0031320321000200,https://doi.org/10.1016/j.patcog.2021.107833,0031-3203,2021,107833,113,Pattern Recognition,Compact learning for multi-label classification,article,LV2021107833,
"Intensive research interests have been paid for the vision and language communities. Especially, image captioning task aims to generate natural language descriptions from the image content. Oppositely, image synthesis task aims to generate realistic images from natural language descriptions. Moreover, both of them can achieve promising results by using Long Short-Term Memory (LSTM), which models the sequence dynamics at each time step as hidden state. Nevertheless, the research on dynamics is often limited in the individual task, while there is no progress exploring the mutual relationship between dynamics in different tasks. In this work, we present a novel coupled-dynamic formulation that can iteratively reduce the distance between task-dependent dynamics in the training process. To embed adverse information into individual network, we construct dual-loss architectures to interactively align dynamics. We evaluate the proposed framework on Flickr8k, Flickr30k and MSCOCO datasets. Experimental results show that our approach can boost dual tasks together and achieve competing performances against state-of-the-art methods.","Image captioning, Image synthesis, Coupled dynamics",Ning Xu and Hongshuo Tian and Yanhui Wang and Weizhi Nie and Dan Song and An-An Liu and Wu Liu,https://www.sciencedirect.com/science/article/pii/S0031320321000169,https://doi.org/10.1016/j.patcog.2021.107829,0031-3203,2021,107829,113,Pattern Recognition,Coupled-dynamic learning for vision and language: Exploring Interaction between different tasks,article,XU2021107829,
"Feature selection (FS) is an important data processing method in pattern recognition and data mining. Due to not considering characteristics of the FS problem itself, traditional particle update mechanisms and swarm initialization strategies adopted in most particle swarm optimization (PSO) limit their performance on dealing with high-dimensional FS problems. Focused on it, this paper proposes a novel feature selection algorithm based on bare bones PSO (BBPSO) with mutual information. Firstly, an effective swarm initialization strategy based on label correlation is developed, making full use of the correlation between features and class labels to accelerate the convergence of swarm. Then, in order to enhance the exploitation performance of the algorithm, two local search operators, i.e., the supplementary operator and the deletion operator, are developed based on feature relevance-redundancy. Furthermore, an adaptive flip mutation operator is designed to help particles jump out of local optimal solutions. We apply the proposed algorithm to typical datasets based on the K-Nearest Neighbor classifier (K-NN), and compare it with eleven state-of-the-art algorithms, SFS, PTA, SGA, BPSO, PSO(4-2), HPSO-LS, Binary BPSO, NaFA, IBFA, KPLS-mRMR and SMBA-CSFS. The experimental results show that the proposed algorithm can achieve a feature subset with better performance, and is a highly competitive FS algorithm.","Feature selection, Particle swarm, Swarm initialization, Mutual information, Local search",Xian-fang Song and Yong Zhang and Dun-wei Gong and Xiao-yan Sun,https://www.sciencedirect.com/science/article/pii/S0031320320306075,https://doi.org/10.1016/j.patcog.2020.107804,0031-3203,2021,107804,112,Pattern Recognition,Feature selection using bare-bones particle swarm optimization with mutual information,article,SONG2021107804,
"Neural network-based models have recently shown excellent performance in various kinds of tasks. However, a large amount of labeled data is required to train deep networks, and the cost of gathering labeled training data for every kind of domain is prohibitively expensive. Domain adaptation tries to solve this problem by transferring knowledge from labeled source domain data to unlabeled target domain data. Previous research tried to learn domain-invariant features of source and target domains to address this problem, and this approach has been used as a key concept in various methods. However, domain-invariant features do not mean that a classifier trained on source data can be directly applied to target data because it does not guarantee that data distribution of the same classes will be aligned across two domains. In this paper, we present novel generalization upper bounds for domain adaptation that motivates the need for class-conditional domain invariant learning. Based on this theoretical framework, we then propose a class-conditional domain invariant learning method that can learn a feature space in which features in the same class are expected to be mapped nearby. We empirically experimented that our model showed state-of-the-art performance on standard datasets and showed effectiveness by visualization of latent space.","Domain adaptation, Generalization bound, Class-conditional domain invariant learning, PAC learning complexity, Transfer Learning",Woojin Lee and Hoki Kim and Jaewook Lee,https://www.sciencedirect.com/science/article/pii/S0031320320305665,https://doi.org/10.1016/j.patcog.2020.107763,0031-3203,2021,107763,112,Pattern Recognition,Compact class-conditional domain invariant learning for multi-class domain adaptation,article,LEE2021107763,
"This paper presents an unsupervised interest point detection and description method named Properties Optimization Point (POP), which provides a unified objective to optimize different properties of interest point. First, the proposed objective formulates the interest point set as a latent variable, which is flexible to integrate different properties. With the latent variable, the probability formulations are designed for four traditional properties (sparsity, repeatability, invariability, discriminability). Second, a novel property termed as informativeness indicating the information complexity of a local area is designed to determine the areas containing high information, from which interest points are encouraged to be extracted. Third, an efficient approximate Expectation Maximization is proposed to optimize the non-differentiable objective which integrates the above five properties. Finally, POP is instantiated with fully convolutional networks. Experimental results demonstrate that POP outperforms state-of-the-art methods on a number of image matching benchmarks containing both planar and non-planar scenes.","Interest point, Unsupervised learning, Expectation maximization, Properties, Convolution neural network",Pei Yan and Yihua Tan and Yuan Tai and Dongrui Wu and Hanbin Luo and Xiaolong Hao,https://www.sciencedirect.com/science/article/pii/S0031320320306117,https://doi.org/10.1016/j.patcog.2020.107808,0031-3203,2021,107808,112,Pattern Recognition,Unsupervised learning framework for interest point detection and description via properties optimization,article,YAN2021107808,
"Most of manifold learning based feature extraction methods are two-step methods, which first construct a weighted neighborhood graph and then use the pre-constructed graph to perform subspace learning. As a result, these methods fail to use the underlying correlation structure of data to learn an adaptive graph to preciously characterize the similarity relationship between samples. To address this problem, we propose a novel unsupervised feature extraction method called low-rank adaptive graph embedding (LRAGE), which can perform subspace learning and adaptive probabilistic neighborhood graph embedding simultaneously based on reconstruction error minimization. The proposed LRAGE is imposed with low-rank constraint for the sake of exploring the underlying correlation structure of data and learning more informative projection. Moreover, the L2,1-norm penalty is imposed on the regularization to further enhance the robustness of LRAGE. Since the resulting objective function has no closed-form solutions, an iterative optimization algorithm is elaborately designed. The convergence of the proposed algorithm is proved and the corresponding computational complexity analysis is also presented. In addition, we explore the potential properties of the proposed LRAGE by comparing it with several similar models on both synthetic and real-world data sets. Extensive experiments on five well-known face data sets and three non-face data sets demonstrate the superiority of the proposed LRAGE.","Low-rank regression, Jointly sparse learning, Adaptive graph embedding, Unsupervised feature extraction",Jianglin Lu and Hailing Wang and Jie Zhou and Yudong Chen and Zhihui Lai and Qinghua Hu,https://www.sciencedirect.com/science/article/pii/S0031320320305616,https://doi.org/10.1016/j.patcog.2020.107758,0031-3203,2021,107758,113,Pattern Recognition,Low-rank adaptive graph embedding for unsupervised feature extraction,article,LU2021107758,
"Human gait conveys significant information that can be used for identity recognition and emotion recognition. Recent studies have focused more on gait identity recognition than emotion recognition and regarded these two recognition tasks as independent and unrelated. How to train a unified model to effectively recognize the identity and emotion from gait at the same time is a novel and challenging problem. In this paper, we propose a novel Attention Enhanced Temporal Graph Convolutional Network (AT-GCN) for gait-based recognition and motion prediction. Enhanced by spatial and temporal attention, the proposed model can capture discriminative features in spatial dependency and temporal dynamics. We also present a multi-task learning architecture, which can jointly learn representations for multiple tasks. It helps the emotion recognition task with limited data considerably benefit from the identity recognition task and helps the recognition tasks benefit from the auxiliary prediction task. Furthermore, we present a new dataset (EMOGAIT) that consists of 1, 440 real gaits, annotated with identity and emotion labels. Experimental results on two datasets demonstrate the effectiveness of our approach and show that our approach achieves substantial improvements over mainstream methods for identity recognition and emotion recognition.","Gait recognition, Gait emotion recognition, Graph convolutional network, Spatial-temporal attention GCN, Multi-task learning network",Weijie Sheng and Xinde Li,https://www.sciencedirect.com/science/article/pii/S0031320321000558,https://doi.org/10.1016/j.patcog.2021.107868,0031-3203,2021,107868,114,Pattern Recognition,Multi-task learning for gait-based identity recognition and emotion recognition using attention enhanced temporal graph convolutional network,article,SHENG2021107868,
"In the field of computer vision, methods that use fully supervised learning and fixed deep network structures need to be improved. Currently, many studies are devoted to designing neural architecture search methods to use neural networks in a more flexible way. However, most of these methods use fully supervised learning at the cost of extraordinary GPU training time. In view of the above problems, we propose a semi-supervised generative adversarial network and search network architecture based on block structure. Use real pictures and generated pictures with corresponding real tags and pseudo tags for training, to achieve the purpose of semi-supervised learning. By setting the layerâs hyperparameters to a variable and flexible stacking block structure, network architecture search is achieved. The proposed method realizes image generation and extends to image classification. In the experimental results in SectionÂ 4, the training time is greatly reduced and the model performance is improved, which illustrates the efficiency of our method. The code can be found in https://github.com/AICV-CUMT/STASGAN.","Semi-supervised, GANs, Network architecture search, Image generation, Image classification",Man Zhang and Yong Zhou and Jiaqi Zhao and Shixiong Xia and Jiaqi Wang and Zizheng Huang,https://www.sciencedirect.com/science/article/pii/S0031320320305975,https://doi.org/10.1016/j.patcog.2020.107794,0031-3203,2021,107794,112,Pattern Recognition,Semi-supervised blockwisely architecture search for efficient lightweight generative adversarial network,article,ZHANG2021107794,
"In this paper, a single-stage 3D object detection framework, 3D-CenterNet, is proposed for accurate 3D object detection from point clouds. We find that the center position is more critical for accurate bounding box detection than the other two parameters, the size and the orientation. Motivated by this discovery, we propose the center regression module (CRM) to regress the centersâ location from the point-wise features. In CRM, the representative points belonging to objects are sampled to regress the center locations of the corresponding objects. The semantic and geometric information related to the estimated centers is aggregated for the following location refinement and other parametersâ estimation. The 3D-CenterNet stacks the CRMs to improve the accuracy of the estimated centers gradually. The size and orientation of the bounding boxes are decoded from the high dimensional center-wise features. The experiments on the KITTI benchmark and the SUN RGB-D datasets show that our proposed 3D-CenterNet achieves high-quality results in real time.","3D object detection, Point cloud, Deep learning",Qi Wang and Jian Chen and Jianqiang Deng and Xinfang Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321000716,https://doi.org/10.1016/j.patcog.2021.107884,0031-3203,2021,107884,115,Pattern Recognition,3D-CenterNet: 3D object detection network for point clouds with center estimation priority,article,WANG2021107884,
"Understanding chest CT imaging of the coronavirus disease 2019 (COVID-19) will help detect infections early and assess the disease progression. Especially, automated severity assessment of COVID-19 in CT images plays an essential role in identifying cases that are in great need of intensive clinical care. However, it is often challenging to accurately assess the severity of this disease in CT images, due to variable infection regions in the lungs, similar imaging biomarkers, and large inter-case variations. To this end, we propose a synergistic learning framework for automated severity assessment of COVID-19 in 3D CT images, by jointly performing lung lobe segmentation and multi-instance classification. Considering that only a few infection regions in a CT image are related to the severity assessment, we first represent each input image by a bag that contains a set of 2D image patches (with each cropped from a specific slice). A multi-task multi-instance deep network (called M2UNet) is then developed to assess the severity of COVID-19 patients and also segment the lung lobe simultaneously. Our M2UNet consists of a patch-level encoder, a segmentation sub-network for lung lobe segmentation, and a classification sub-network for severity assessment (with a unique hierarchical multi-instance learning strategy). Here, the context information provided by segmentation can be implicitly employed to improve the performance of severity assessment. Extensive experiments were performed on a real COVID-19 CT image dataset consisting of 666 chest CT images, with results suggesting the effectiveness of our proposed method compared to several state-of-the-art methods.","COVID-19, CT, Severity assessment, Lung lobe segmentation, Multi-instance learning",Kelei He and Wei Zhao and Xingzhi Xie and Wen Ji and Mingxia Liu and Zhenyu Tang and Yinghuan Shi and Feng Shi and Yang Gao and Jun Liu and Junfeng Zhang and Dinggang Shen,https://www.sciencedirect.com/science/article/pii/S0031320321000157,https://doi.org/10.1016/j.patcog.2021.107828,0031-3203,2021,107828,113,Pattern Recognition,Synergistic learning of lung lobe segmentation and hierarchical multi-instance classification for automated severity assessment of COVID-19 in CT images,article,HE2021107828,
"Automatic Natural language interpretation of medical images is an emerging field of Artificial Intelligence (AI). The task combines two fields of AI; computer vision and natural language processing. This is a challenging task that goes beyond object detection, segmentation, and classification because it also requires the understanding of the relationship between different objects of an image and the actions performed by these objects as visual representations. Image interpretation is helpful in many tasks like helping visually impaired persons, information retrieval, early childhood learning, producing human like natural interaction between robots, and many more applications. Recently this work fascinated researchers to use the same approach by using more complex biomedical images. It has been applied from generating single sentence captions to multi sentence paragraph descriptions. Medical image captioning can assist and speed up the diagnosis process of medical professionals and generated report can be used for many further tasks. This is a comprehensive review of recent yearsâ research of medical image captioning published in different international conferences and journals. Their common parameters are extracted to compare their methods, performance, strengths, limitations, and our recommendations are discussed. Further publicly available datasets and evaluation measures used for deep-learning based captioning of medical images are also discussed.","Attention mechanism, Automatic captioning, Convolutional neural network (cnn), Deep learning, Encoder-decoder framework, Image captioning, Long-Short-Term-Memory (LSTM), Medical image caption",Hareem Ayesha and Sajid Iqbal and Mehreen Tariq and Muhammad Abrar and Muhammad Sanaullah and Ishaq Abbas and Amjad Rehman and Muhammad Farooq Khan Niazi and Shafiq Hussain,https://www.sciencedirect.com/science/article/pii/S0031320321000431,https://doi.org/10.1016/j.patcog.2021.107856,0031-3203,2021,107856,114,Pattern Recognition,Automatic medical image interpretation: State of the art and future directions,article,AYESHA2021107856,
"Correlation filter based trackers attribute to its calculation in the frequency domain can efficiently locate targets in a relatively fast speed. This characteristic however also limits its generalization in some specific scenarios. The reasons that they still fail to achieve superior performance to state-of-the-art (SOTA) trackers are possibly due to two main aspects. The first is that while tracking the objects whose energy is lower than the background, the tracker may occur drift or even lose the target. The second is that the biased samples may be inevitably selected for model training, which can easily lead to inaccurate tracking. To tackle these shortcomings, a novel energy-aware correlation filter (EACOFT) based tracking method is proposed, in our approach the energy between the foreground and the background is adaptively balanced, which enables the target of interest always having a higher energy than its background. The samplesâ qualities are also evaluated in real time, which ensures that the samples used for template training are always helpful with tracking. In addition, we also propose an optimal bottom-up and top-down combined strategy for template training, which plays an important role in improving both the effectiveness and robustness of tracking. As a result, our approach achieves a great improvement on the basis of the baseline tracker, especially under the background clutter and fast motion challenges. Extensive experiments over multiple tracking benchmarks demonstrate the superior performance of our proposed methodology in comparison to a number of the SOTA trackers.","Visual tracking, Energy-aware correlation filter (EACOFT), Enhanced feature, Top-down and bottom-up strategy",Qiaoyuan Liu and Jinchang Ren and Yuru Wang and Yuanbo Wu and Haijiang Sun and Huimin Zhao,https://www.sciencedirect.com/science/article/pii/S0031320320305690,https://doi.org/10.1016/j.patcog.2020.107766,0031-3203,2021,107766,112,Pattern Recognition,EACOFT: An energy-aware correlation filter for visual tracking,article,LIU2021107766,
"Although the CNNs are a very powerful tool for image retrieval, the need of training datasets properly adapted to the application at hand hinders the usefulness of such networks, specially since the datasets need to be free of noise to avoid spoiling the learning process. An ad hoc preprocessing of the dataset to mitigate the noise is a possible solution, but it is usually non-trivial and requires significant human intervention. In this paper, we pave the road for training CNNs for image retrieval with noisy datasets. In particular, we propose a novel Bag Exponential Loss function that, inspired by the Multiple Instance Learning framework, works with bags of matching images instead of single pairs, and allows a dynamical weighting of the relevance of each sample as the training progresses. The formulation of the proposed model is general enough and may serve to other purposes than dealing with noise if parameters are chosen appropriately. Extensive experimental results show the superior performance of the proposed loss with respect to the current state-of-the-art as well as its ability to cope with noisy training sets. Pytorch code available in https://github.com/tmcortes/BELoss","Image retrieval, Noise, Multiple instance learning, Loss functions",TomÃ¡s MartÃ­nez-CortÃ©s and IvÃ¡n GonzÃ¡lez-DÃ­az and Fernando DÃ­az-de-MarÃ­a,https://www.sciencedirect.com/science/article/pii/S0031320320306142,https://doi.org/10.1016/j.patcog.2020.107811,0031-3203,2021,107811,112,Pattern Recognition,Training deep retrieval models with noisy datasets: Bag exponential loss,article,MARTINEZCORTES2021107811,
"Region Proposal Network (RPN) is the cornerstone of two-stage object detectors. It generates a sparse set of object proposals and alleviates the extrem foreground-background class imbalance problem during training. However, we find that the potential of the detector has not been fully exploited due to the IoU distribution imbalance and inadequate quantity of the training samples generated by RPN. With the increasing intersection over union (IoU), the exponentially smaller numbers of positive samples would lead to the distribution skewed towards lower IoUs, which hinders the optimization of detector at high IoU levels. In this paper, to break through the limitations of RPN, we propose IoU-Uniform R-CNN, a simple but effective method that directly generates training samples with uniform IoU distribution for the regression branch as well as the IoU prediction branch. Besides, we improve the performance of IoU prediction branch by eliminating the feature offsets of RoIs at inference, which helps the NMS procedure by preserving accurately localized bounding box. Extensive experiments on the PASCAL VOC and MS COCO dataset show the effectiveness of our method, as well as its compatibility and adaptivity to many object detection architectures. The code is made publicly available at https://github.com/zl1994/IoU-Uniform-R-CNN.","Object detection, Two-stage detector, RPN, IoU distribution imbalance",Li Zhu and Zihao Xie and Liman Liu and Bo Tao and Wenbing Tao,https://www.sciencedirect.com/science/article/pii/S0031320321000030,https://doi.org/10.1016/j.patcog.2021.107816,0031-3203,2021,107816,112,Pattern Recognition,IoU-uniform R-CNN: Breaking through the limitations of RPN,article,ZHU2021107816,
"Person Re-identification (ReID) has witnessed remarkable improvements in the past couple of years. However, its applications in real-world scenarios are limited by the disparity among different cameras and datasets. In general, it remains challenging to generalize ReID algorithms from one domain to another, especially when the target domain is unknown. To solve this issue, we develop a 3D-guided adversarial transform (3D-GAT) network which explores the transfer ability of source training data to facilitate learning domain-independent knowledge. Being aware of a 3D model and human poses, 3D-GAT makes use of image-to-image translation to synthesize person images in different conditions whilst preserving features for identification as much as possible. With these augmented training data, it is easier for ReID approaches to perceive how a person can appear differently under varying viewpoints and poses, most of which are not seen in the training data, and thus achieve higher ReID accuracy especially in an unknown domain. Extensive experiments conducted on Market-1501, DukeMTMC-reID and CUHK03 demonstrate the effectiveness of our proposed approach, which is competitive to the baseline models in the original dataset and sets the new state-of-the-art in direct transfer to other datasets.","Person re-identification, Domain transfer, 3D Models, Training with synthesized image data",Hengheng Zhang and Ying Li and Zijie Zhuang and Lingxi Xie and Qi Tian,https://www.sciencedirect.com/science/article/pii/S0031320320306026,https://doi.org/10.1016/j.patcog.2020.107799,0031-3203,2021,107799,112,Pattern Recognition,3D-GAT: 3D-Guided adversarial transform network for person re-identification in unseen domains,article,ZHANG2021107799,
"Salient Object Detection (SOD) is a fundamental problem in the field of computer vision. This paper presents a novel Spatial Context-Aware Network (SCA-Net) for SOD in images. Compared with other recent deep learning based SOD algorithms, SCA-Net can more effectively aggregate multi-level deep features. A Long-Path Context Module (LPCM) is employed to grant better discrimination ability to feature maps that incorporate coarse global information. Consequently, a more accurate initial saliency map can be obtained to facilitate subsequent predictions. SCA-Net also adopts a Short-Path Context Module (SPCM) to progressively enforce the interaction between local contextual cues and global features. Extensive experiments on five large-scale benchmarks demonstrate that SCA-Net achieves favorable performance against very recent state-of-the-art algorithms.","Salient object detection, Context-aware methods, Deep learning",Yuqiu Kong and Mengyang Feng and Xin Li and Huchuan Lu and Xiuping Liu and Baocai Yin,https://www.sciencedirect.com/science/article/pii/S0031320321000546,https://doi.org/10.1016/j.patcog.2021.107867,0031-3203,2021,107867,114,Pattern Recognition,Spatial context-aware network for salient object detection,article,KONG2021107867,
"Sketch to digital image matching refers to the problem of matching a sketch image (often drawn by hand or created by a software) against a gallery of digital images (captured via an acquisition device such as a digital camera). Automated sketch to digital image matching has applicability in several day to day tasks such as similar object image retrieval, forensic sketch matching in law enforcement scenarios, or profile linking using caricature face images on social media. As opposed to the digital images, sketch images are generally edge-drawings containing limited (or no) textural or colour based information. Further, there is no single technique for sketch generation, which often results in varying artistic or software styles, along with the interpretation bias of the individual creating the sketch. Beyond the variations observed across the two domains (sketch and digital image), automated sketch to digital image matching is further marred by the challenge of limited training data and wide intra-class variability. In order to address the above problems, this research proposes a novel Discriminative Shared Transform Learning (DSTL) algorithm for sketch to digital image matching. DSTL learns a shared transform for data belonging to the two domains, while modeling the class variations, resulting in discriminative feature learning. Two models have been presented under the proposed DSTL algorithm: (i) Contractive Model (C-Model) and (ii) Divergent Model (D-Model), which have been formulated with different supervision constraints. Experimental analysis on seven datasets for three case studies of sketch to digital image matching demonstrate the efficacy of the proposed approach, highlighting the importance of each component, its input-agnostic behavior, and improved matching performance.","Face recognition, Sketch to digital image matching, sketch based image retrieval, Caricature face recognition, Transform learning",Shruti Nagpal and Maneet Singh and Richa Singh and Mayank Vatsa,https://www.sciencedirect.com/science/article/pii/S0031320321000029,https://doi.org/10.1016/j.patcog.2021.107815,0031-3203,2021,107815,114,Pattern Recognition,Discriminative shared transform learning for sketch to image matching,article,NAGPAL2021107815,
"This paper proposes a novel feature representation method for color images, namely quaternionic extended local binary pattern (QxLBP) with adaptive structural pyramid pooling (ASPP). First, we propose a QxLBP operator to encode local neighboring information and complementary modulus and phase information in the quaternion domain of color images. In QxLBP, an extended quaternionic representation (EQR) is proposed which introduces an information term into the real part of a quaternion. The resulting EQR enables us to flexibly encode discriminative features and handle multichannel image data. Second, we propose ASPP as a multiresolution pooling way to aggregate local features. Unlike the traditional spatial pyramid pooling which is sensitive to image rotation and spatial changes, ASPP is structure-oriented pooling which can adaptively aggregate the encoded features into multiresolution histogram representations. Experiments on four benchmark image datasets demonstrate that the proposed method achieves the state-of-the-art performance for color texture classification and scene categorization.","Color image, Quaternion, Local binary pattern, Spatial pooling, Image classification, Scene categorization",Tiecheng Song and Liangliang Xin and Chenqiang Gao and Tianqi Zhang and Yao Huang,https://www.sciencedirect.com/science/article/pii/S0031320321000789,https://doi.org/10.1016/j.patcog.2021.107891,0031-3203,2021,107891,115,Pattern Recognition,Quaternionic extended local binary pattern with adaptive structural pyramid pooling for color image representation,article,SONG2021107891,
"We propose a Behavior Regularized Prototypical Network (BR-ProtoNet) for few-shot image classification in semi-supervised scenarios. To learn a generalizable metric, we exploit readily-available unlabeled data and construct complementary constraints to regularize the modelâs behavior. Specifically, we match the label spaces between each episode and the whole training set. The predictions on the unlabeled data over different episodes can be aggregated to capture more reliable category information. We further construct new instances via adversarial perturbation and interpolation. These instances regularize the modelâs behavior over the neighborhoods of the original ones and along the interpolation paths among them. In addition, they ensure the learnt embedding space possesses the property of proximity preservation. The regularization of these aspects is incorporated into the optimization process of BR-ProtoNet on partially labeled data. We have conducted thorough experiments on multiple challenging benchmarks. The results suggest that the metric learning can significantly benefit from the proposed regularization, and thus leading to the state-of-the-art performance in semi-supervised few-shot image classification.","Few-shot learning, Semi-supervised learning, Image classification, Prototypical networks",Shixin Huang and Xiangping Zeng and Si Wu and Zhiwen Yu and Mohamed Azzam and Hau-San Wong,https://www.sciencedirect.com/science/article/pii/S0031320320305689,https://doi.org/10.1016/j.patcog.2020.107765,0031-3203,2021,107765,112,Pattern Recognition,Behavior regularized prototypical networks for semi-supervised few-shot image classification,article,HUANG2021107765,
"Object detection in very high resolution (VHR) optical remote sensing (RS) images is one of the most fundamental but challenging tasks in the field of RS image analysis. To reduce the computational complexity of redundant information and improve the efficiency of image processing, visual saliency models have been widely applied in this field. In this paper, a novel saliency detection model based on Contrast-weighted Dictionary Learning (CDL) is proposed for VHR optical RS images. Specifically, the proposed CDL learns salient and non-salient atoms from positive and negative samples to construct a discriminant dictionary, in which a contrast-weighted term is proposed to encourage the contrast-weighted patterns to be present in the learned salient dictionary while discouraging them from being present in the non-salient dictionary. Then, we measure the saliency by combining the coefficients of the sparse representation (SR) and reconstruction errors. Furthermore, by using the proposed joint saliency measure, a variety of saliency maps are generated based on the discriminant dictionary. Finally, a fusion method based on global gradient optimization is proposed to integrate multiple saliency maps. Experimental results on four datasets demonstrate that the proposed model outperforms other state-of-the-art methods.","Contrast-weighted dictionary, Dictionary learning, Gradient optimization, Remote sensing, Saliency detection",Zhou Huang and Huai-Xin Chen and Tao Zhou and Yun-Zhi Yang and Chang-Yin Wang and Bi-Yuan Liu,https://www.sciencedirect.com/science/article/pii/S0031320320305604,https://doi.org/10.1016/j.patcog.2020.107757,0031-3203,2021,107757,113,Pattern Recognition,Contrast-weighted dictionary learning based saliency detection for VHR optical remote sensing images,article,HUANG2021107757,
"We propose a new regularization term for CapsNet that significantly improves the generalization power of the original method from small training data while requiring much fewer parameters, making it suitable for large input images. We also propose a very efficient DNN architecture that integrates CapsNet with ResNet to obtain the advantages of the two architectures. CapsNet allows a powerful understanding of the objectsâ components and their positions, while ResNet provides efficient feature extraction and description. Our approach is general, and we demonstrate it on the problem of signature identification from images. To show our approach superiority, we provide several evaluations with different protocols. We also show that our approach outperforms the state-of-the-art on this problem with thorough experiments on three publicly available datasets CEDAR, MCYT, and UTSig.","Regularized capsule neural network, Residual neural network, CapsNet regularization, CapsNet and ResNet conjugation, Signature recognition",Mahdi Jampour and Saeid Abbaasi and Malihe Javidi,https://www.sciencedirect.com/science/article/pii/S0031320321000388,https://doi.org/10.1016/j.patcog.2021.107851,0031-3203,2021,107851,120,Pattern Recognition,CapsNet regularization and its conjugation with ResNet for signature identification,article,JAMPOUR2021107851,
"Annotating a large-scale image dataset is very tedious, yet necessary for training person re-identification (re-ID) models. To alleviate such a problem, we present an active redundancy reduction (ARR) framework via training an effective re-ID model with the least labeling efforts. The proposed ARR framework actively selects informative and diverse samples for annotation by estimating their uncertainty and intra-diversity, thus it can significantly reduce the annotation workload. Moreover, we propose a computer-assisted identity recommendation module embedded in the ARR framework to help human annotators to rapidly and accurately label the selected samples. Extensive experiments were carried out on several public re-ID datasets to demonstrate the existence of data redundancy. Experimental results indicate that our method can reduce 57%, 63%, and 49% annotation efforts on the Market1501, MSMT17, and CUHK03, respectively, while maximizing the performance of the re-ID model.","Person re-identification, Redundancy reduction, Active learning",Xin Xu and Lei Liu and Xiaolong Zhang and Weili Guan and Ruimin Hu,https://www.sciencedirect.com/science/article/pii/S0031320321000145,https://doi.org/10.1016/j.patcog.2021.107827,0031-3203,2021,107827,113,Pattern Recognition,Rethinking data collection for person re-identification: active redundancy reduction,article,XU2021107827,
"Accurate segmentation of the optic disc (OD) regions from colour fundus images is a critical procedure for computer-aided diagnosis of glaucoma. We present a novel deep learning network to automatically identify the OD regions. On the basis of the classical U-Net framework, we define a unique sub-network and a decoding convolutional block. The sub-network is used to preserve important textures and facilitate their detections, while the decoding block is used to improve the contrast of the regions-of-interest with their background. We integrate these two components into the classical U-Net framework to improve the accuracy and reliability of segmenting the OD regions depicted on colour fundus images. We train and evaluate the developed network using three publicly available datasets (i.e., MESSIDOR, ORIGA, and REFUGE). The results on an independent testing set (nÂ =Â 1,970 images) show a segmentation performance with an average Dice similarity coefficient (DSC), intersection over union (IOU), and Matthew's correlation coefficient (MCC) of 0.9377, 0.8854, and 0.9383 when trained on the global field-of-view images, respectively, and 0.9735, 0.9494, and 0.9594 when trained on the local disc region images. When compared with the other three classical networks (i.e., the U-Net, M-Net, and Deeplabv3) on the same testing datasets, the developed network demonstrates a relatively higher performance.","Segmentation, Colour fundus images, Optic disc, Deep learning, U-Net",Lei Wang and Juan Gu and Yize Chen and Yuanbo Liang and Weijie Zhang and Jiantao Pu and Hao Chen,https://www.sciencedirect.com/science/article/pii/S0031320320306130,https://doi.org/10.1016/j.patcog.2020.107810,0031-3203,2021,107810,112,Pattern Recognition,Automated segmentation of the optic disc from fundus images using an asymmetric deep learning network,article,WANG2021107810,
"As a relatively new biometric trait, Finger-Knuckle-Print (FKP) plays a vital role in establishing a personal authentication system in modern society due to its rich discriminative features, low time cost in image capture and user-friendliness. However, most existing KFP descriptors are hand-crafted and fail to work well with limited training samples. In this paper, we propose a feature learning method for few-shot FKP recognition by jointly learning compact multi-view hash codes (JLCMHC) of a FKP image. We first form the multi-view data vectors (MVDV) to exploit the multiple feature-specific information from a FKP image. Then, we learn a feature projection to encode the MVDV into compact binary codes in an unsupervised manner, where 1) the variance of the learned feature codes on each view is maximized and 2) the difference of the inter-view binary codes is enlarged, so that the redundant information in MVDV is reduced and more informative features can be obtained. Lastly, we pool the binary codes into block-wise statistics features as the final descriptor for FKP representation and recognition. Experimental results on the existing benchmark FKP databases clearly show that the JLCMHC method outperforms the state-of-the-art FKP descriptors.","FKP biometrics, Multi-view features jointly learning, Few-show learning, Compact FKP descriptor",Lunke Fei and Bob Zhang and Jie Wen and Shaohua Teng and Shuyi Li and David Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321000819,https://doi.org/10.1016/j.patcog.2021.107894,0031-3203,2021,107894,115,Pattern Recognition,Jointly learning compact multi-view hash codes for few-shot FKP recognition,article,FEI2021107894,
"High-dimensional problem poses significant challenges for dictionary learning based classification architecture. Joint Dimension Reduction and Dictionary Learning (JDRDL) framework shows great potential for overcoming the challenges caused by high dimensionality. However, most of the existing JDRDL approaches do not consider the complex nonlinear relationships within high-dimensional data, which limits their classification performance. To overcome this problem, a novel joint dimension reduction and dictionary learning framework is proposed in this paper for high-dimensional data classification. Firstly, at dimension reduction stage, an autoencoder is employed to learn a nonlinear mapping that reduces dimensionality and preserves nonlinear structure of the high-dimensional data. Then, at dictionary learning stage, the locality constraint with label embedding, which takes the locality and label information into account together, is incorporated into the learning process to preserve desirable nonlinear local structure and enhance class discrimination. Moreover, the mapping function and dictionary are optimized simultaneously to enhance the performance. Encouraging experimental results on multiple benchmark datasets confirm that the proposed framework is effective and efficient for high-dimensional data classification.","High-dimensional data classification, Dimension reduction, Dictionary learning, Autoencoder",Yanxia Li and Yi Chai and Han Zhou and Hongpeng Yin,https://www.sciencedirect.com/science/article/pii/S0031320320305963,https://doi.org/10.1016/j.patcog.2020.107793,0031-3203,2021,107793,112,Pattern Recognition,A novel dimension reduction and dictionary learning framework for high-dimensional data classification,article,LI2021107793,
"In the last decade, deep learning models have yielded impressive performance on visual object recognition and image classification. However these methods still rely on learning visual data distributions and show difficulties in dealing with complex scenarios where visual appearance only is not enough to effectively tackle them. This is the case, for instance, of fine-grained image classification in domain-specific applications for which it is very complex to employ data-driven models because of the lack of large amounts of samples and that, instead, can be solved by resorting to specialized human knowledge. However, encoding this specialized knowledge and injecting it into deep models is not trivial. In this paper, we address this problem by: a) employing computational ontologies to model specialized knowledge in a structured representation and, b) building a hybrid visual-semantic classification framework. The classification method performs inference over a Bayesian Network graph, whose structure depends on the knowledge encoded in an ontology and evidences are built using the outputs of deep networks. We test our approach on a fine-grained classification task, employing an extremely complex dataset containing images from several fruit varieties as well as visual and semantic annotations. Since the classification is done at the variety level (e.g., discriminating between different cherry varieties), appearance changes slightly and expert domain knowledge â making using of contextual information â is required to perform classification accurately. Experimental results show that our approach significantly outperforms standard deep learningâbased classification methods over the considered scenario as well as existing methods leveraging semantic information for classification. These results demonstrate, on one hand, the difficulty of purely-visual deep methods in tackling small and highly-specialized datasets and, on the other hard, the capabilities of our approach to effectively encode and use semantic knowledge for enhanced accuracy.","Fine-grained visual classification, Computational ontologies, Belief networks",S. Palazzo and F. Murabito and C. Pino and F. Rundo and D. Giordano and M. Shah and C. Spampinato,https://www.sciencedirect.com/science/article/pii/S0031320320306099,https://doi.org/10.1016/j.patcog.2020.107806,0031-3203,2021,107806,112,Pattern Recognition,Exploiting structured high-level knowledge for domain-specific visual classification,article,PALAZZO2021107806,
"Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SR methods mainly focus on wider or deeper architecture design, neglecting to discover the latent relationship of features, hence limiting the representational ability of networks. To address this issue, we propose a channel attention and spatial graph convolutional network (CASGCN) for more powerful feature obtaining and feature correlations modeling. The CASGCN is formed by several channel attention and spatial graph (CASG) blocks that incorporate global spatial and channel inter-dependencies for rendering features of each pixel. Inside the CASG block, channel branch and spatial branch are first arranged in a paralleled way, and then are concatenated to effectively learn the representation of each image pixel. Specifically, we use attention mechanism to extract informative features in channel branch while the spatial-aware graph is used in spatial branch to model the global self-similar information. Furthermore, the adjacency matrix in spatial-aware graph is dynamically generated via the Gram matrix to model global correlations between pixels and is shared across the whole network without auxiliary parameters. Extensive experiments on SISR with different degradation models show the effectiveness of our CASGCN in terms of quantitative and visual results.","Image super-resolution, Graph convolutional, Adjacent matrix",Yue Yang and Yong Qi,https://www.sciencedirect.com/science/article/pii/S0031320320306014,https://doi.org/10.1016/j.patcog.2020.107798,0031-3203,2021,107798,112,Pattern Recognition,Image super-resolution via channel attention and spatial graph convolutional network,article,YANG2021107798,
"Recently, a series of cascaded pose regression based facial landmark localization methods under occlusion have been proposed. However, partial occlusions and pose variations will break the entire structure of the face which poses obstacles to global regression. Moreover, there lack techniques to evaluate the reliability of the regression results during the regression process. In this paper, we propose a Two-Stage Cascaded Pose Regression(TSCPR) for facial landmark localization under occlusion. In the first stage, a global cascaded pose regression with robust initialization is performed to get localization results for the original face and its mirror image. The localization difference between the original image and the mirror image is used to determine whether the localization of each landmark is reliable, while unreliable localization can be adjusted. In the second stage, the global results are divided into multiple parts, which are further refined by local regressions. Finally, multiple refined local results are rated and adjusted to get the final output. We evaluated the proposed method on widely used datasets COFW, LFPW, HELEN, 300-W and Menpo-Semifrontal. The experimental results show that the proposed method can outperform the state-of-the-arts.","Occlusion, Cascaded pose regression, Mirror error, Facial landmark localization, Face alignment",Ziye Tong and Junwei Zhou,https://www.sciencedirect.com/science/article/pii/S0031320321000534,https://doi.org/10.1016/j.patcog.2021.107866,0031-3203,2021,107866,115,Pattern Recognition,Face alignment using two-stage cascaded pose regression and mirror error correction,article,TONG2021107866,
"The method proposed in this paper belongs to the family of orthogonal non-negative matrix factorization (ONMF) methods designed to solve clustering problems. Unlike some existing ONMF methods that explicitly constrain the orthogonality of the coefficient matrix in the cost function to derive their clustering models, the proposed method integrates it implicitly, so that it results in a new optimization model with a penalty term. The latter is added to impose a scale relationship between the scatter of the cluster centroids and that of the data points. The solution of the new model involves deriving a new parametrized update scheme for the basis matrix, which makes it possible to improve the performance of the clustering by adjusting a parameter. The proposed clustering algorithm, which we call âpairwise Feature Relationship preservation-based NMFâ (FR-NMF), is evaluated on several real-life and synthetic datasets and compared to eight existing NMF-based clustering models. The results obtained show the effectiveness of the proposed algorithm.","NMF, Orthogonal NMF, Clustering, Unsupervised learning, Low-rank matrix factorization,",Rachid Hedjam and Abdelhamid Abdesselam and Farid Melgani,https://www.sciencedirect.com/science/article/pii/S0031320321000017,https://doi.org/10.1016/j.patcog.2021.107814,0031-3203,2021,107814,112,Pattern Recognition,NMF with feature relationship preservation penalty term for clustering problems,article,HEDJAM2021107814,
"The idea of the paper concentrates on an iterative learning process in Graph Convolution Networks (GCNs) involved in two vital steps: one is a message propagation (message passing) step to aggregate neighboring node features via aggregators performed, and another is an encoding output step to encode node feature representations by using updaters. In our model, we propose a novel affinity-aware encoding as an updater in GCNs, which aggregates the neighboring nodes of a node while updating this nodeâs features. By utilizing affinity values of our encoding, we order the neighboring nodes to determine the correspondence between encoding functions and the neighboring nodes. Furthermore, to explicitly reduce the model size, we propose a lightweight variant of our updater that integrates Depth-wise Separable Convolution (DSC) into it, namely Depth-wise Separable Graph Convolution (DSGC). Comprehensive experiments conducted on graph data demonstrate that our modelsâ accuracy improved significantly for graphs of low-dimensional node features. Also, performed in the low-dimensional node feature space we provide state-of-the-art results on two metrics (Macro-f1 and Matthews correlation coefficient (MCC)). Besides, our models are robust when taking different low-dimensional feature selection strategies.","Graph convolutional networks, Affinity-aware encoding, Updater, Depth-wise separable graph convolution, Low-Dimensional node features",Wei Dong and Junsheng Wu and Zongwen Bai and Yaoqi Hu and Weigang Li and Wei Qiao and Marcin WoÅºniak,https://www.sciencedirect.com/science/article/pii/S0031320320305914,https://doi.org/10.1016/j.patcog.2020.107788,0031-3203,2021,107788,112,Pattern Recognition,MobileGCN applied to low-dimensional node feature learning,article,DONG2021107788,
"Existing deep trackers use deep convolutional neural networks to extract powerful features or directly predict the position of the target. For most deep trackers, it is hard to improve their performance by replacing the original backbone with a more powerfully heavyweight network directly. In this paper, we propose a novel mutual-learning-based training methodology for visual object tracking. By re-training the backbone network with this novel methodology, we can improve the tracking performance simply and effectively. We demonstrate this novel training methodology with two mainstream tracking approaches: correlation-filter-based approach and tracking-by-detection-based approach. First, we reformulate a correlation-filter-based tracker as a fully convolutional network and design an end-to-end tracking framework. With this framework, we can enhance the backbone network in a mutual learning way. Second, we integrate our training methodology into a typical tracking-by-detection-based tracker, and then we improve the tracking performance with a simple offline training process. Extensive experiments on the OTB2013, OTB2015, VOT2017 and LaSOT benchmarks demonstrate that the tracking performance can be improved effectively by using the proposed mutual-learning-based training methodology.","Visual object tracking, Deep learning, Mutual learning",Haojie Zhao and Gang Yang and Dong Wang and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320320305999,https://doi.org/10.1016/j.patcog.2020.107796,0031-3203,2021,107796,112,Pattern Recognition,Deep mutual learning for visual object tracking,article,ZHAO2021107796,
"The current pandemic, caused by the outbreak of a novel coronavirus (COVID-19) in December 2019, has led to a global emergency that has significantly impacted economies, healthcare systems and personal wellbeing all around the world. Controlling the rapidly evolving disease requires highly sensitive and specific diagnostics. While RT-PCR is the most commonly used, it can take up to eight hours, and requires significant effort from healthcare professionals. As such, there is a critical need for a quick and automatic diagnostic system. Diagnosis from chest CT images is a promising direction. However, current studies are limited by the lack of sufficient training samples, as acquiring annotated CT images is time-consuming. To this end, we propose a new deep learning algorithm for the automated diagnosis of COVID-19, which only requires a few samples for training. Specifically, we use contrastive learning to train an encoder which can capture expressive feature representations on large and publicly available lung datasets and adopt the prototypical network for classification. We validate the efficacy of the proposed model in comparison with other competing methods on two publicly available and annotated COVID-19 CT datasets. Our results demonstrate the superior performance of our model for the accurate diagnosis of COVID-19 based on chest CT images.","COVID-19 diagnosis, Few-shot learning, Contrastive learning, Chest CT images",Xiaocong Chen and Lina Yao and Tao Zhou and Jinming Dong and Yu Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321000133,https://doi.org/10.1016/j.patcog.2021.107826,0031-3203,2021,107826,113,Pattern Recognition,Momentum contrastive learning for few-shot COVID-19 diagnosis from chest CT images,article,CHEN2021107826,
"Local feature description is to assign a unique signature to a key-point such that it becomes distinctive from the others regardless of changes in viewpoint, illumination, rotation, scale as well as distortions and noise. This paper proposes a novel approach to construct such a descriptor. For preserving both homogeneous and heterogeneous features of a given support region, we interweave the texture information so that the key-point is more likely to be assigned a distinctive signature and neighboring key-points will be less likely to share the same texture information. The main idea behind our descriptor is to increase the areas of our observations in the given scene while the length of the local support region is fixed. Gradient magnitude and divergence, as measurement parameters of texture information, are applied to a group of pixels instead of employing a pixel-wise strategy that make the descriptor more resistant to noise, distortions and illumination variation. The required storage of the proposed descriptor is just 72 floats and its computational complexity is much lower than those of existing ones. A comparative study between the proposed method and the selected state-of-the-art ones over multiple publicly accessible datasets with different characteristics shows its superiority, robustness and computational efficiency under various geometric changes, illumination variation, distortions and noise. The code and supplementary materials can be found atÂ https://github.com/mogvision/InterTex-Feature-Descriptor.","Interest point, Descriptive signature, Interwoven texture, Locality, Globality, Robustness",Morteza Ghahremani and Yitian Zhao and Bernard Tiddeman and Yonghuai Liu,https://www.sciencedirect.com/science/article/pii/S003132032100008X,https://doi.org/10.1016/j.patcog.2021.107821,0031-3203,2021,107821,113,Pattern Recognition,Interwoven texture-based description of interest points in images,article,GHAHREMANI2021107821,
"This study considers the task of clustering for data characterized by peculiar quantitative features in that they express performance according to different indicators or criteria. Performance is supposed to be optimized in one way or the other, i.e. maximized or minimized. This peculiar type of data introduces a comparative context that is not generally taken into account in the field of pattern recognition, in general, and clustering, in particular. In the present study, we introduce different concepts and develop tools that facilitate the evaluation of data partitions in this comparative context leading to the consideration of asymmetric preference relationships between objects and between clusters. We show their usefulness on the basis of artificial data and also by analyzing the results produced on real data by means of clustering methods.","Clustering, -means, Multicriteria, Partial ordering, Partition, Preference, Quality assessment",Jean Rosenfeld and Yves De Smet and Olivier Debeir and Christine Decaestecker,https://www.sciencedirect.com/science/article/pii/S0031320321000376,https://doi.org/10.1016/j.patcog.2021.107850,0031-3203,2021,107850,114,Pattern Recognition,Assessing partially ordered clustering in a multicriteria comparative context,article,ROSENFELD2021107850,
"The COVID-19 pandemic has affected many countries, posing a threat to human health and safety, and putting tremendous pressure on the medical system. This paper proposes a novel SLAM technology using RGB and depth images to improve hospital operation efficiency, reduce the risk of doctor-patient cross-infection, and curb the spread of the COVID-19. Most current visual SLAM researches assume that the environment is stationary, which makes handling real-world scenarios such as hospitals a challenge. This paper proposes a method that effectively deals with SLAM problems for scenarios with dynamic objects, e.g., people and movable objects, based on the semantic descriptor extracted from images with help of a knowledge graph. Specifically, our method leverages a knowledge graph to construct a priori movement relationship between entities and establishes high-level semantic information. Built upon this knowledge graph, a semantic descriptor is constructed to describe the semantic information around key points, which is rotation-invariant and robust to illumination. The seamless integration of the knowledge graph and semantic descriptor helps eliminate the dynamic objects and improves the accuracy of tracking and positioning of robots in dynamic environments. Experiments are conducted using data acquired from healthcare facilities, and semantic maps are established to meet the needs of robots for delivering medical services. In addition, to compare with the state-of-the-art methods, a publicly available dataset is used in our evaluation. Compared with the state-of-the-art methods, our proposed method demonstrated great improvement with respect to both accuracy and robustness in dynamic environments. The computational efficiency is also competitive.","COVID-19 pandemic, Visual SLAM, Dynamic scenes, Semantic descriptors, Knowledge graph",Baofu Fang and Gaofei Mei and Xiaohui Yuan and Le Wang and Zaijun Wang and Junyang Wang,https://www.sciencedirect.com/science/article/pii/S0031320321000091,https://doi.org/10.1016/j.patcog.2021.107822,0031-3203,2021,107822,113,Pattern Recognition,Visual SLAM for robot navigation in healthcare facility,article,FANG2021107822,
"Existing person search methods typically focus on improving person detection accuracy. This ignores the model inference efficiency, which however is fundamentally significant for real-world applications. In this work, we address this limitation by investigating the scalability problem of person search involving both model accuracy and inference efficiency simultaneously. Specifically, we formulate a Hierarchical Distillation Learning (HDL) approach. With HDL, we aim to comprehensively distil the knowledge of a strong teacher model with strong learning capability to a lightweight student model with weak learning capability. To facilitate the HDL process, we design a simple and powerful teacher model for joint learning of person detection and person re-identification matching in unconstrained scene images. Extensive experiments show the modelling advantages and cost-effectiveness superiority of HDL over the state-of-the-art person search methods on three large person search benchmarks: CUHK-SYSU, PRW, and DukeMTMC-PS.","Person search, Person re-identification, Person detection, Knowledge distillation, Scalability, Model inference efficiency",Wei Li and Shaogang Gong and Xiatian Zhu,https://www.sciencedirect.com/science/article/pii/S0031320321000492,https://doi.org/10.1016/j.patcog.2021.107862,0031-3203,2021,107862,114,Pattern Recognition,Hierarchical distillation learning for scalable person search,article,LI2021107862,
"Soft computing provides the framework for dealing with the uncertainty and imprecision inherent in real-life applications. Soft computing has become a long-standing notable paradigm for medical image processing. A typical fuzzy clustering uses the fuzzy membership function. Nevertheless, there is an alternative membership representation, known as typicality or possibilistic membership. Unlike fuzzy membership that is probabilistic in nature, typicality represents an absolute membership and it is the degree of belonging of an object to a class that does not depend on its distances from the other classes. However, both fuzzy membership and typicality play important role in assigning membership to an object. This study proposes a novel clustering model that creates a vague environment enriched with the concept of fuzzy membership and typicality, while the use of type-reduction plays an essential role in capturing all the vagueness present in the data set. The proposed model is called type-reduced vague possibilistic fuzzy clustering (TVPFC), and we use MRI images to demonstrate its superior robustness over that of FCM (fuzzy c-means), PCM (possibilistic c-means), VCM (vague c-means) and IPFCM (interval-valued possibilistic fuzzy c-means).","Fuzzy membership, Typicality, Vague set, Type-reduction, Medical images",Ankita Bose and Kalyani Mali,https://www.sciencedirect.com/science/article/pii/S0031320320305872,https://doi.org/10.1016/j.patcog.2020.107784,0031-3203,2021,107784,112,Pattern Recognition,Type-reduced vague possibilistic fuzzy clustering for medical images,article,BOSE2021107784,
"With the rapid increase of multi-modal data through the internet, cross-modal matching or retrieval has received much attention recently. It aims to use one type of data as query and retrieve results from the database of another type. For this task, the most popular approach is the latent subspace learning, which learns a shared subspace for multi-modal data, so that we can efficiently measure cross-modal similarity. Instead of adopting traditional regularization terms, we hope that the latent representation could recover the multi-modal information, which works as a reconstruction regularization term. Besides, we assume that different view features for samples of the same category share the same representation in the latent space. Since the number of classes is generally smaller than the number of samples and the feature dimension, therefore the latent feature matrix of training instances should be low-rank. We try to learn the optimal latent representation, and propose a reconstruction based term to recover original multi-modal data and a low-rank term to regularize the learning of subspace. Our method can deal with both supervised and unsupervised cross-modal retrieval tasks. For those situations where the semantic labels are not easy to obtain, our proposed method can also work very well. We propose an efficient algorithm to optimize our framework. To evaluate the performance of our method, we conduct extensive experiments on various datasets. The experimental results show that our proposed method is very efficient and outperforms the state-of-the-art subspace learning approaches.","Cross-modal retrieval, Low-rank subspace learning, Reconstruction regularization",Jianlong Wu and Xingxu Xie and Liqiang Nie and Zhouchen Lin and Hongbin Zha,https://www.sciencedirect.com/science/article/pii/S0031320320306166,https://doi.org/10.1016/j.patcog.2020.107813,0031-3203,2021,107813,113,Pattern Recognition,Reconstruction regularized low-rank subspace learning for cross-modal retrieval,article,WU2021107813,
"Ship detection in SAR images is a challenging task due to two difficulties. (1) Because of the long observation distance, ships in SAR images are small with low resolution, leading to high false negative. (2) Because of the complex onshore background, ships are easily confused with other objects with similar appearance. To solve these problems, we propose an effective and stable single-stage detector called CenterNet++. Our model mainly consists of three modules, i.e., feature refinement module, feature pyramids fusion module, and head enhancement module. Firstly, to address small objects detection problem, we design a feature refinement module for extracting multi-scale contextual information. Secondly, feature pyramids fusion module is developed for generating more powerful semantic information. Finally, to alleviate the impact of complex background, head enhancement module is proposed for a balance between foreground and background. To prove the effectiveness and robustness of the proposed method, we make extensive experiments on three popular SAR image datasets, i.e., AIR-SARShip, SSDD, SAR-Ship. The experimental results show that our CenterNet++ reaches state-of-the-art performance on all datasets. In addition, compared with the baseline CenterNet, the proposed method achieves a remarkable accuracy improvement with negligible increase in time cost.","Ship detection, Synthetic aperture radar (SAR), Deep learning",Haoyuan Guo and Xi Yang and Nannan Wang and Xinbo Gao,https://www.sciencedirect.com/science/article/pii/S0031320320305902,https://doi.org/10.1016/j.patcog.2020.107787,0031-3203,2021,107787,112,Pattern Recognition,A CenterNet++ model for ship detection in SAR images,article,GUO2021107787,
"Deep metric learning leverages well-designed distance measurement and a sample selection strategy to learn a discriminative feature space. Among the various deep metric learning formulations, triplet loss is built based on a 3-tuple that can simultaneously minimise the distance between the items in the positive pair and maximise the distance between those in the negative pair. However, this endeavour requires a critical selection of triplet samples to guide the training process. In this paper, we propose a layered Triplet loss to solve the fine-grained image classification problem. Unlike the existing triplet loss, which selects samples from only a single criterion, we construct the loss function with the âcoarse to fineâ scheme. This scheme can separate the coarse-level classes while clustering the fine-level samples within a certain margin. An ontology-based sampling method is proposed to enable the network to mine more reasonable hard triplets. Semantic knowledge is employed to assign the visually similar classes to the same learning task, from which hard triplets can be generated. Finally, the softmax tree classifier is used to classify the hierarchical features. The experimental results on multiple datasets demonstrate the effectiveness of the proposed method.","Metric learning, Triplet network, Layered ontology, Layered triplet loss, Multi-task learning",Guiqing He and Feng Li and Qiyao Wang and Zongwen Bai and Yuelei Xu,https://www.sciencedirect.com/science/article/pii/S0031320321000765,https://doi.org/10.1016/j.patcog.2021.107889,0031-3203,2021,107889,115,Pattern Recognition,A hierarchical sampling based triplet network for fine-grained image classification,article,HE2021107889,
"The inherent relations among multiple face analysis tasks, such as landmark detection, head pose estimation, gender recognition and face attribute estimation are crucial to boost the performance of each task, but have not been thoroughly explored since typically these multiple face analysis tasks are handled as separate tasks. In this paper, we propose a novel deep multi-task adversarial learning method to localize facial landmark, estimate head pose and recognize gender jointly or estimate multiple face attributes simultaneously through exploring their dependencies from both image representation-level and label-level. Specifically, the proposed method consists of a deep recognition network R and a discriminator D. The deep recognition network is used to learn the shared middle-level image representation and conducts multiple face analysis tasks simultaneously. Through multi-task learning mechanism, the recognition network explores the dependencies among multiple face analysis tasks from image representation-level. The discriminator is introduced to enforce the distribution of the multiple face analysis tasks to converge to that inherent in the ground-truth labels. During training, the recognizer tries to confuse the discriminator, while the discriminator competes with the recognizer through distinguishing the predicted label combination from the ground-truth one. Though adversarial learning, we explore the dependencies among multiple face analysis tasks from label-level. Experimental results on benchmark databases demonstrate the effectiveness of the proposed method for multi-task face analyses.","Multi-task learning, Adversarial learning, Face analyses",Shangfei Wang and Shi Yin and Longfei Hao and Guang Liang,https://www.sciencedirect.com/science/article/pii/S0031320321000248,https://doi.org/10.1016/j.patcog.2021.107837,0031-3203,2021,107837,114,Pattern Recognition,Multi-task face analyses through adversarial learning,article,WANG2021107837,
"Conventional image-motion based methods for structure from motion first compute optical flow, then solve for the 3D motion parameters based on the epipolar constraint, and finally recover the 3D geometry of the scene. However, errors in optical flow due to regularization can lead to large errors in 3D motion and structure. This paper investigates whether performance and consistency can be improved by avoiding optical flow estimation in the early stages of the structure-from-motion pipeline, and it proposes a new direct method based on image gradients (normal flow) only. Our main idea lies in a reformulation of the positive-depth constraint â the basis for estimating egomotion from normal flow â as a continuous piecewise differentiable function, which allows the use of well-known minimization techniques to solve for 3D motion. The 3D motion estimate is then refined and structure estimated adding a regularization based on depth. Experimental comparisons on standard synthetic datasets and the real-world driving benchmark dataset Kitti using three different optic flow algorithms show that the method achieves better accuracy in all but one case. Furthermore, it outperforms existing normal flow based 3D motion estimation techniques. Finally, the recovered 3D geometry is shown to be also very accurate.","3D motion, Egomotion, Structure from motion, Normal flow",Francisco Barranco and Cornelia FermÃ¼ller and Yiannis Aloimonos and Eduardo Ros,https://www.sciencedirect.com/science/article/pii/S0031320320305628,https://doi.org/10.1016/j.patcog.2020.107759,0031-3203,2021,107759,113,Pattern Recognition,Joint direct estimation of 3D geometry and 3D motion using spatio temporal gradients,article,BARRANCO2021107759,
"Targeting at boosting business revenue, purchase prediction based on user behavior is crucial to e-commerce. However, it is not a well-explored topic due to a lack of relevant datasets. Specifically, no public dataset provides both price and discount information varying on time, which play an essential role in the userâs decision making. Besides, existing learn-to-rank methods cannot explicitly predict the purchase possibility for a specific user-item pair. In this paper, we propose a two-step graph-based model, where the graph model is applied in the first step to learn representations of both users and items over click-through data, and the second step is a classifier incorporating the price information of each transaction record. To evaluate the model performance, we propose a transaction-based framework focusing on the purchased items and their context clicks, which contain items that a user is interested in but fails to choose after comparison. Our experiments show that exploiting the price and discount information can significantly enhance prediction accuracy.","Purchase prediction, Graph-based method, e-commerce, Transaction-level data",Zongxi Li and Haoran Xie and Guandong Xu and Qing Li and Mingming Leng and Chi Zhou,https://www.sciencedirect.com/science/article/pii/S003132032100011X,https://doi.org/10.1016/j.patcog.2021.107824,0031-3203,2021,107824,113,Pattern Recognition,Towards purchase prediction: A transaction-based setting and a graph-based method leveraging price information,article,LI2021107824,
"Deep metric learning methods aim to measure similarity of data points (e.g. images) by calculating their distance in a high dimensional embedding space. These methods are usually trained by optimizing a ranking loss function, which is designed to bring together samples from the same class while separating them from samples from all other classes. The most challenging part of these methods is the selection of samples that contribute to effective network training. In this paper we present Bag of Negatives (BoN), a fast hard negative mining method, that provides a set, triplet or pair of potentially relevant training samples. BoN is an efficient method that selects a bag of hard negatives based on a novel online hashing strategy. We show the superiority of BoN against state-of-the-art hard negative mining methods in terms of accuracy and training time over three large datasets.","Deep metric learning, Instance retrieval, Re-identification, Siamese networks, Online hashing",Bojana GajiÄ and Ariel Amato and Carlo Gatta,https://www.sciencedirect.com/science/article/pii/S0031320320305987,https://doi.org/10.1016/j.patcog.2020.107795,0031-3203,2021,107795,112,Pattern Recognition,Fast hard negative mining for deep metric learning,article,GAJIC2021107795,
"The Euclidean Minimum Sum-of-Squares Clustering(MSSC) is one of the most important models for the clustering problem. Due to its NP-hardness, the problem continues to receive much attention in the scientific literature and several heuristic procedures have been proposed. Recent research has been devoted to the improvement of the classical K-MEANS algorithm, either by suitably selecting its starting configuration or by using it as a local search method within a global optimization algorithm. This paper follows this last approach by proposing a new implementation of a Memetic Differential Evolution (MDE) algorithm specifically designed for the MSSC problem and based on the repeated execution of K-MEANS from selected configurations. In this paper we describe how to adapt MDE to the clustering problem and we show, through a vast set of numerical experiments, that the proposed method has very good quality, measured in terms of the minimization of the objective function, as well as a very good efficiency, measured in the number of calls to the local optimization routine, with respect to state of the art methods.","Global optimization, Clustering, Minimum sum-of-squares, Hybrid genetic algorithm, K-MEANS",Pierluigi Mansueto and Fabio Schoen,https://www.sciencedirect.com/science/article/pii/S0031320321000364,https://doi.org/10.1016/j.patcog.2021.107849,0031-3203,2021,107849,114,Pattern Recognition,Memetic differential evolution methods for clustering problems,article,MANSUETO2021107849,
"Automatic pulmonary nodules classification is significant for early diagnosis of lung cancers. Recently, deep learning techniques have enabled remarkable progress in this field. However, these deep models are typically of high computational complexity and work in a black-box manner. To combat these challenges, in this work, we aim to build an efficient and (partially) explainable classification model. Specially, we use neural architecture search (NAS) to automatically search 3D network architectures with excellent accuracy/speed trade-off. Besides, we use the convolutional block attention module (CBAM) in the networks, which helps us understand the reasoning process. During training, we use A-Softmax loss to learn angularly discriminative representations. In the inference stage, we employ an ensemble of diverse neural networks to improve the prediction accuracy and robustness. We conduct extensive experiments on the LIDC-IDRI database. Compared with previous state-of-the-art, our model shows highly comparable performance by using less than 1/40 parameters. Besides, empirical study shows that the reasoning process of learned networks is in conformity with physiciansâ diagnosis. Related code and results have been released at: https://github.com/fei-hdu/NAS-Lung.","Pulmonary nodule classification, Convolutional neural network, Neural architecture search, Computer-aided diagnoses, Convolutional block attention module",Hanliang Jiang and Fuhao Shen and Fei Gao and Weidong Han,https://www.sciencedirect.com/science/article/pii/S0031320321000121,https://doi.org/10.1016/j.patcog.2021.107825,0031-3203,2021,107825,113,Pattern Recognition,"Learning efficient, explainable and discriminative representations for pulmonary nodules classification",article,JIANG2021107825,
"Anomaly detection in surveillance videos is attracting an increasing amount of attention. Despite the competitive performance of recent methods, they lack theoretical performance analysis, particularly due to the complex deep neural network architectures used in decision making. Additionally, online decision making is an important but mostly neglected factor in this domain. Much of the existing methods that claim to be online, depend on batch or offline processing in practice. Motivated by these research gaps, we propose an online anomaly detection method in surveillance videos with asymptotic bounds on the false alarm rate, which in turn provides a clear procedure for selecting a proper decision threshold that satisfies the desired false alarm rate. Our proposed algorithm consists of a multi-objective deep learning module along with a statistical anomaly detection module, and its effectiveness is demonstrated on several publicly available data sets where we outperform the state-of-the-art algorithms. All codes are available at https://github.com/kevaldoshi17/Prediction-based-Video-Anomaly-Detection-.","Computer vision, Video surveillance, Anomaly detection, Asymptotic performance analysis, Deep learning, Online detection",Keval Doshi and Yasin Yilmaz,https://www.sciencedirect.com/science/article/pii/S0031320321000522,https://doi.org/10.1016/j.patcog.2021.107865,0031-3203,2021,107865,114,Pattern Recognition,Online anomaly detection in surveillance videos with asymptotic bound on false alarm rate,article,DOSHI2021107865,
"Motion analysis is one of the most fundamental and challenging problems in the field of computer vision, which can be widely applied in many areas, such as autonomous driving, action recognition, scene understanding, and robotics. In general, the displacement field between subsequent frames can be divided into two types: optical flow and scene flow. The optical flow represents the pixel motion of adjacent frames. In contrast, the scene flow is a 3D motion field of the dynamic scene between two frames. Traditional approaches for the estimation of optical flow and scene flow usually leverage the variational technique, which can be solved as an energy minimization process. In recent years, deep learning has emerged as a powerful technique for learning feature representations directly from data. It has led to remarkable progress in the field of optical flow and scene flow estimation. In this paper, we provide a comprehensive survey of optical flow and scene flow estimation. First, we briefly review the pioneering approaches that use variational technique and then we delve in detail into the deep learning-based approaches. Furthermore, we present insightful observations on evaluation issues, specifically benchmark datasets, evaluation metrics, and state-of-the-art performance. Finally, we give the promising directions for future research. To the best of our knowledge, we are the first to review both optical flow and scene flow estimation, and the first to cover both traditional and deep learning-based approaches.","Motion analysis, Optical flow, Scene flow, Variational model, Deep learning, Convolutional neural networks (CNNs)",Mingliang Zhai and Xuezhi Xiang and Ning Lv and Xiangdong Kong,https://www.sciencedirect.com/science/article/pii/S0031320321000480,https://doi.org/10.1016/j.patcog.2021.107861,0031-3203,2021,107861,114,Pattern Recognition,Optical flow and scene flow estimation: A survey,article,ZHAI2021107861,
"The cyclist trajectory prediction is critical for the local path planning of autonomous vehicles. Based on the assumption that cyclist's movement is limited by its dynamics and subjected to interactions with environments, a novel LSTM based cyclist trajectory prediction model which utilizes multiple interactions with surroundings and motion feature in a unified framework is proposed. Road features describing road boundary and static obstacles are employed to address cyclist's interaction with the road. To address interactions with pedestrians, other cyclists and vehicles, object features including object attributes and relative positions are utilized. The focal attention mechanism is employed to reveal the importance of features at each time-steps. By feeding features into LSTM encoder, the movement in the next two seconds is predicted. Experiments were conducted on two datasets, and results show that the presented model outperforms the state-of-art models in most cases.","Trajectory prediction, Interaction, Cyclist, LSTM, Focal attention mechanism",Zhi Huang and Jun Wang and Lei Pi and Xiaolin Song and Lingfang Yang,https://www.sciencedirect.com/science/article/pii/S0031320320306038,https://doi.org/10.1016/j.patcog.2020.107800,0031-3203,2021,107800,112,Pattern Recognition,LSTM based trajectory prediction model for cyclist utilizing multiple interactions with environment,article,HUANG2021107800,
"Attention models are widely used in Vision-language (V-L) tasks to perform the visual-textual correlation. Humans perform such a correlation with a strong linguistic understanding of the visual world. However, even the best performing attention model in V-L tasks lacks such a high-level linguistic understanding, thus creating a semantic gap between the modalities. In this paper, we propose an attention mechanism - Linguistically-aware Attention (LAT) - that leverages object attributes obtained from generic object detectors along with pre-trained language models to reduce this semantic gap. LAT represents visual and textual modalities in a common linguistically-rich space, thus providing linguistic awareness to the attention process. We apply and demonstrate the effectiveness of LAT in three V-L tasks: Counting-VQA, VQA, and Image captioning. In Counting-VQA, we propose a novel counting-specific VQA model to predict an intuitive count and achieve state-of-the-art results on five datasets. In VQA and Captioning, we show the generic nature and effectiveness of LAT by adapting it into various baselines and consistently improving their performance.","Attention models, Visual question answering, Counting in visual question answering, Image captioning",Gouthaman KV and Athira Nambiar and Kancheti Sai Srinivas and Anurag Mittal,https://www.sciencedirect.com/science/article/pii/S0031320320306154,https://doi.org/10.1016/j.patcog.2020.107812,0031-3203,2021,107812,112,Pattern Recognition,Linguistically-aware attention for reducing the semantic gap in vision-language tasks,article,KV2021107812,
"Scalable algorithms of variational posterior approximation allow Bayesian nonparametrics such as Dirichlet process mixture to scale up to larger dataset at fractional cost. Recent algorithms, notably the stochastic variational inference performs local learning from minibatch. The main problem with stochastic variational inference is that it relies on closed form solution. Stochastic gradient ascent is a modern approach to machine learning and is widely deployed in the training of deep neural networks. In this work, we explore using stochastic gradient ascent as a fast algorithm for the posterior approximation of Dirichlet process mixture. However, stochastic gradient ascent alone is not optimal for learning. In order to achieve both speed and performance, we turn our focus to stepsize optimization in stochastic gradient ascent. As as intermediate approach, we first optimize stepsize using the momentum method. Finally, we introduce Fisher information to allow adaptive stepsize in our posterior approximation. In the experiments, we justify that our approach using stochastic gradient ascent do not sacrifice performance for speed when compared to closed form coordinate ascent learning on these datasets. Lastly, our approach is also compatible with deep ConvNet features as well as scalable to large class datasets such as Caltech256 and SUN397.","Dirichlet process mixture, Stochastic gradient ascent, Fisher information, Scalable algorithm",Kart-Leong Lim and Xudong Jiang,https://www.sciencedirect.com/science/article/pii/S0031320320305860,https://doi.org/10.1016/j.patcog.2020.107783,0031-3203,2021,107783,112,Pattern Recognition,Variational posterior approximation using stochastic gradient ascent with adaptive stepsize,article,LIM2021107783,
"Maximum margin of twin spheres support vector machine (MMTSSVM) is an efficient method for imbalanced data classification. As an extension to enhance noise insensitivity of MMTSSVM, MMTSSVM with pinball loss (Pin-MMTSM) has a good generalization performance. However, it is not efficient enough for large-scale data. Inspired by the sparse solution of SVMs, in this paper, we propose a safe accelerative approach to reduce the computational cost. Unlike the existing safe screening rules, where only one variable changes with the parameters. We utilize bound estimation-based to derive the upper and lower bounds of center and radius. With our approach, the inactive samples are discarded before solving the problem, thus it can reduce the computational cost. One important advantage of our approach is safety, i.e., we can obtain the same solution as solving original problem both in linear and non-linear cases. Moreover, it is obvious that our acceleration approach is independent of the solver. To further accelerate the computational speed, a decomposition method is employed. Experiments on three artificial datasets and twelve benchmark datasets clearly demonstrate the effectiveness of our approach. At last, we extend bound estimation-based method to Î½-SVM, theoretical analysis and experimental results both verify its feasibility and effectiveness.","Maximum margin, Pinball loss, Imbalanced data, Bound estimation, Upper and lower bounds",Min Yuan and Yitian Xu,https://www.sciencedirect.com/science/article/pii/S0031320321000479,https://doi.org/10.1016/j.patcog.2021.107860,0031-3203,2021,107860,114,Pattern Recognition,Bound estimation-based safe acceleration for maximum margin of twin spheres machine with pinball loss,article,YUAN2021107860,
"Sequence-to-sequence models have recently become very popular for tackling handwritten word recognition problems. However, how to effectively integrate an external language model into such recognizer is still a challenging problem. The main challenge while training a language model is to deal with the language model corpus which is usually different to the one used for training the handwritten word recognition system. Thus, the bias between both word corpora leads to incorrectness on the transcriptions, providing similar or even worse performances on the recognition task. In this work, we introduce Candidate Fusion, a novel way to integrate an external language model to a sequence-to-sequence architecture. Moreover, it provides suggestions from an external language knowledge, as a new input to the sequence-to-sequence recognizer. Hence, Candidate Fusion provides two improvements. On the one hand, the sequence-to-sequence recognizer has the flexibility to not only combine the information from itself and the language model, but also choose the importance of the information provided by the language model. On the other hand, the external language model has the ability to adapt itself to the training corpus and even learn the most common errors produced from the recognizer. Finally, by conducting comprehensive experiments, the Candidate Fusion proves to outperform the state-of-the-art language models for handwritten word recognition tasks.","Handwritten word recognition, Sequence-to-sequence models, Language model, Candidate fusion",Lei Kang and Pau Riba and Mauricio Villegas and Alicia FornÃ©s and MarÃ§al RusiÃ±ol,https://www.sciencedirect.com/science/article/pii/S0031320320305938,https://doi.org/10.1016/j.patcog.2020.107790,0031-3203,2021,107790,112,Pattern Recognition,Candidate fusion: Integrating language modelling into a sequence-to-sequence handwritten word recognition architecture,article,KANG2021107790,
"In this paper, we propose semi-supervised kernel matrix learning (SS-KML) using adaptive constraint-based seed propagation (ACSP). Conventional SS-KML methods such as pairwise constraint propagation (PCP) and kernel propagation (KP) have achieved outstanding performance in data classification. However, they are likely to distort the global data structure because of using hard constraints in their semi-definite problems (SDPs) for constraint propagation. Moreover, given a large number of pairwise constraints and a large amount of samples, they tend to be incredibly complex, thus being hard to be applied to real-life complex problems such as internet-scale image categorization. To address this problem, we utilize adaptive constraints to effectively maintain the inherent coherence of samples and successfully propagate constraint information into all samples. Moreover, we adopt seed propagation to remarkably reduce the computational complexity of SS-KML. Experimental results demonstrate that ACSP achieves a significant improvement in performance over PCP and KP in terms of both effectiveness and efficiency.","Adaptive constraints, Constraint propagation, Kernel learning, Seed propagation, Semi-supervised kernel matrix learning",Meng Jian and Cheolkon Jung,https://www.sciencedirect.com/science/article/pii/S0031320320305537,https://doi.org/10.1016/j.patcog.2020.107750,0031-3203,2021,107750,112,Pattern Recognition,Semi-supervised kernel matrix learning using adaptive constraint-based seed propagation,article,JIAN2021107750,
"In this paper, we present a novel thermodynamically based analysis method for directed networks, and in particular for time-evolving networks in the finance domain. Based on an analogy with a dilute gas in statistical mechanics, we develop a partition function for a network composed of directed motifs. The method relies on the decomposition of directed networks into a series of frequently occurring graphlets, or motifs. According to the connection between a directed network and the dilute gas, the network motifs have the same topological structure as the low-order interactions between particles in the gas. This means that we can use the so-called cluster expansion from statistical mechanics to develop a partition function for the motif decomposition. In prior work, we have reported a detailed analysis of the cluster expansion for the case of undirected graphs, and showed how the resulting motif entropy can be used to analyse time evolving networks [1]. In this paper we extend this work to the case of directed graphs to compute thermodynamic quantities including energy, entropy and temperature for the directed network. The three thermodynamic quantities constitute the thermodynamic framework for the analysis of directed network evolution. We apply our thermodynamic framework to the financial and biological domains to represent real world complex systems as time-varying directed networks. Experimental results successfully demonstrate the effectiveness of the thermodynamic framework in representing the evolution of directed network structure and anomalous event detection.","Cluster expansion, Motif, Directed network entropy",Dongdong Chen and Xingchen Guo and Jianjia Wang and Jiatong Liu and Zhihong Zhang and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320321000595,https://doi.org/10.1016/j.patcog.2021.107872,0031-3203,2021,107872,114,Pattern Recognition,Thermodynamic motif analysis for directed stock market networks,article,CHEN2021107872,
"This paper addresses the problem of change-point detection in sequences of high-dimensional and heterogeneous observations, which also possess a periodic temporal structure. Due to the dimensionality problem, when the time between change points is of the order of the dimension of the model parameters, drifts in the underlying distribution can be misidentified as changes. To overcome this limitation, we assume that the observations lie in a lower-dimensional manifold that admits a latent variable representation. In particular, we propose a hierarchical model that is computationally feasible, widely applicable to heterogeneous data and robust to missing instances. Additionally, the observationsâ periodic dependencies are captured by non-stationary periodic covariance functions. The proposed technique is particularly well suited to (and motivated by) the problem of detecting changes in human behavior using smartphones and its application to relapse detection in psychiatric patients. Finally, we validate the technique on synthetic examples and we demonstrate its utility in the detection of behavioral changes using real data acquired by smartphones.","Change-point detection, Circadian models, Heterogeneous data, Latent variable models, Non-stationary periodic covariance functions",Pablo Moreno-MuÃ±oz and David RamÃ­rez and Antonio ArtÃ©s-RodrÃ­guez,https://www.sciencedirect.com/science/article/pii/S0031320321000078,https://doi.org/10.1016/j.patcog.2021.107820,0031-3203,2021,107820,113,Pattern Recognition,Change-point detection in hierarchical circadian models,article,MORENOMUNOZ2021107820,
"Computed tomography (CT) and X-ray are effective methods for diagnosing COVID-19. Although several studies have demonstrated the potential of deep learning in the automatic diagnosis of COVID-19 using CT and X-ray, the generalization on unseen samples needs to be improved. To tackle this problem, we present the contrastive multi-task convolutional neural network (CMT-CNN), which is composed of two tasks. The main task is to diagnose COVID-19 from other pneumonia and normal control. The auxiliary task is to encourage local aggregation though a contrastive loss: first, each image is transformed by a series of augmentations (Poisson noise, rotation, etc.). Then, the model is optimized to embed representations of a same image similar while different images dissimilar in a latent space. In this way, CMT-CNN is capable of making transformation-invariant predictions and the spread-out properties of data are preserved. We demonstrate that the apparently simple auxiliary task provides powerful supervisions to enhance generalization. We conduct experiments on a CT dataset (4,758 samples) and an X-ray dataset (5,821 samples) assembled by open datasets and data collected in our hospital. Experimental results demonstrate that contrastive learning (as plugin module) brings solid accuracy improvement for deep learning models on both CT (5.49%-6.45%) and X-ray (0.96%-2.42%) without requiring additional annotations. Our codes are accessible online.","Computed tomography, X-ray, COVID-19, Deep learning, Multi-task learning, Contrastive learning",Jinpeng Li and Gangming Zhao and Yaling Tao and Penghua Zhai and Hao Chen and Huiguang He and Ting Cai,https://www.sciencedirect.com/science/article/pii/S0031320321000352,https://doi.org/10.1016/j.patcog.2021.107848,0031-3203,2021,107848,114,Pattern Recognition,Multi-task contrastive learning for automatic CT and X-ray diagnosis of COVID-19,article,LI2021107848,
"Due to the environmental differences, many face anti-spoofing methods fail to generalize to unseen scenarios. In light of this, we propose a unified unsupervised and semi-supervised domain adaptation network (USDAN) for cross-scenario face anti-spoofing, aiming at minimizing the distribution discrepancy between the source and the target domains. Specifically, two modules, i.e., marginal distribution alignment module (MDA) and conditional distribution alignment module (CDA), are designed to seek a domain-invariant feature space via adversarial learning and make the features of the same class compact, respectively. By adding/removing the CDA module, the network can be easily switched for semi-supervised/unsupervised setting, in which sense our method is named with âunifiedâ. Moreover, the adaptive cross-entropy loss and normalization techniques are further incorporated to improve the generalization. Extensive experimental results show that the proposed USDAN outperforms state-of-the-art methods on several public datasets.","Face anti-spoofing, Face presentation attack detection, Domain adaptation, Deep learning",Yunpei Jia and Jie Zhang and Shiguang Shan and Xilin Chen,https://www.sciencedirect.com/science/article/pii/S0031320321000753,https://doi.org/10.1016/j.patcog.2021.107888,0031-3203,2021,107888,115,Pattern Recognition,Unified unsupervised and semi-supervised domain adaptation network for cross-scenario face anti-spoofing,article,JIA2021107888,
"Gaussian mixture models (GMMs) are a family of generative models used extensively in many machine learning applications. The modeling power of GMMs is directly linked to the number of components. Memory, computational load and lack of enough data hinders using GMMs with large number of components. To tackle this problem, GMMs with a tying scheme that we call flexibly tied GMM was proposed in the literature of the speech recognition community. In the literature, a coordinate-descent EM algorithm was proposed for estimating the parameters of flexibly tied GMMs. In this paper, we aim at reintroducing flexibly tied GMMs to the pattern recognition community. We rigorously investigate various optimization methods and see none of the out-of-the-box optimization methods can solve the parameter estimation problem due to the complexity of the cost function. To this end, we develop a fast Newton EM algorithm that combined with the coordinate descent EM algorithm, it significantly outperforms pure coordinate descent EM and all other optimization algorithms. Furthermore, we propose a computation factorization technique to increase the speed and decrease memory requirement of both Newton and coordinate descent EM algorithms in the case of large number of components. Experimental results on many datasets verifies the efficacy of the proposed algorithm. It also verifies that flexibly tied GMM outperforms both basic GMM and other types of tied GMMs on the datasets in terms of the log-likelihood. We also evaluate the performance of flexibly tied GMM on a clustering problem, and show that it can outperform basic GMM and kmeans algorithm.","Gaussian mixture model, Parameter sharing, Tied GMM, Computation factorization and reduction, Newton method, Fast minimal residual method, Clustering",Hadi Asheri and Reshad Hosseini and Babak Nadjar Araabi,https://www.sciencedirect.com/science/article/pii/S0031320321000236,https://doi.org/10.1016/j.patcog.2021.107836,0031-3203,2021,107836,114,Pattern Recognition,A new EM algorithm for flexibly tied GMMs with large number of components,article,ASHERI2021107836,
"The artistic community is increasingly relying on automatic computational analysis for authentication and classification of artistic paintings. In this paper, we identify hidden patterns and relationships present in artistic paintings by analysing their complexity, a measure that quantifies the sum of characteristics of an object. Specifically, we apply Normalized Compression (NC) and the Block Decomposition Method (BDM) to a dataset of 4,266 paintings from 91 authors and examine the potential of these information-based measures as descriptors of artistic paintings. Both measures consistently described the equivalent types of paintings, authors, and artistic movements. Moreover, combining the NC with a measure of the roughness of the paintings creates an efficient stylistic descriptor. Furthermore, by quantifying the local information of each painting, we define a fingerprint that describes critical information regarding the artistsâ style, their artistic influences, and shared techniques. More fundamentally, this information describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. Finally, we demonstrate that regional complexity and two-point height difference correlation function are useful auxiliary features that improve current methodologies in style and author classification of artistic paintings. The whole study is supported by an extensive website (http://panther.web.ua.pt) for fast author characterization and authentication.","Image analysis, Data compression, BDM, Artistic paintings, Algorithmic information theory",Jorge Miguel Silva and Diogo Pratas and Rui Antunes and SÃ©rgio Matos and Armando J. Pinho,https://www.sciencedirect.com/science/article/pii/S0031320321000510,https://doi.org/10.1016/j.patcog.2021.107864,0031-3203,2021,107864,114,Pattern Recognition,Automatic analysis of artistic paintings using information-based measures,article,SILVA2021107864,
"Domain Adaptation (DA) aims to generalize the classifier learned from a well-labeled source domain to an unlabeled target domain. Existing DA methods usually assume that rich labels could be available in the source domain. However, we usually confront the source domain with a large number of unlabeled data but only a few labeled data, and thus, how to transfer knowledge from this sparsely-labeled source domain to the target domain is still a challenge, which greatly limits its application in the wild. This paper proposes a novel Sparsely-Labeled Source Assisted Domain Adaptation (SLSA-DA) algorithm to address the challenge with limited labeled source domain samples. Specifically, due to the label scarcity problem, the projected clustering is first conducted on both the source and target domains, so that the discriminative structures of data could be exploited elegantly. Then label propagation is adopted to propagate the labels from those limited labeled source samples to the whole unlabeled data progressively, so that the cluster labels are revealed correctly. Finally, we jointly align the marginal and conditional distributions to mitigate the cross-domain mismatching problem, and optimize those three procedures iteratively. However, it is nontrivial to incorporate the above three procedures into a unified optimization framework seamlessly since some variables to be optimized are implicitly involved in their formulas, thus they could not benefit to each other. Remarkably, we prove that the projected clustering and conditional distribution alignment could be reformulated into other formulations, thus the implicit variables are embedded in different optimization steps. As such, the variables related to those three quantities could be optimized in a unified optimization framework and benefit to each other, and improve the recognition performance obviously. Extensive experiments have verified that our approach could deal with the challenge in the SLSA-DA setting, and achieve the best performances across different real-world cross-domain visual recognition tasks. Our preliminary Matlab code is available at https://github.com/WWLoveTransfer/SLSA-DA/.","Domain adaptation, Sparsely-labeled source, Semi-supervised clustering, Label propagation",Wei Wang and Shenglun Chen and Yuankai Xiang and Jing Sun and Haojie Li and Zhihui Wang and Fuming Sun and Zhengming Ding and Baopu Li,https://www.sciencedirect.com/science/article/pii/S0031320320306063,https://doi.org/10.1016/j.patcog.2020.107803,0031-3203,2021,107803,112,Pattern Recognition,Sparsely-labeled source assisted domain adaptation,article,WANG2021107803,
"In this paper, an end-to-end multi-task deep neural network was proposed for simultaneous script identification and Keyword Spotting (KWS) in multi-lingual hand-written and printed document images. We introduced a unified approach which addresses both challenges cohesively, by designing a novel CNN-BLSTM architecture. The script identification stage involves local and global features extraction to allow the network to cover more relevant information. Contrarily to the traditional feature fusion approaches which build a linear feature concatenation, we employed a compact bi-linear pooling to capture pairwise correlations between these features. The script identification result is, then, injected in the KWS module to eliminate characters of irrelevant scripts and perform the decoding stage using a single-script mode. All the network parameters were trained in an end-to-end fashion using a multi-task learning that jointly minimizes the NLL loss for the script identification and the CTC loss for the KWS. Our approach was evaluated on a variety of public datasets of different languages and writing types.. Experiments proved the efficacy of our deep multi-task representation learning compared to the state-of-the-art systems for both of keyword spotting and script identification tasks.","CBP, CTC, Keyword spotting, Script identification, Handwritten",Ahmed Cheikhrouhou and Yousri Kessentini and Slim Kanoun,https://www.sciencedirect.com/science/article/pii/S0031320321000194,https://doi.org/10.1016/j.patcog.2021.107832,0031-3203,2021,107832,113,Pattern Recognition,Multi-task learning for simultaneous script identification and keyword spotting in document images,article,CHEIKHROUHOU2021107832,
"Because of the invisible human keypoints in images caused by illumination, occlusion and overlap, it is likely to produce unreasonable human pose prediction for most of the current human pose estimation methods. In this paper, we design a novel generative adversarial network (GAN) to improve the localization accuracy of visible joints when some joints are invisible. The network consists of two simple but efficient modules, i.e., Cascade Feature Network (CFN) and Graph Structure Network (GSN). First, the CFN utilizes the prediction maps from the previous stages to guide the prediction maps in the next stage to produce accurate human pose. Second, the GSN is designed to contribute to the localization of invisible joints by passing message among different joints. According to GAN, if the prediction pose produced by the generator G cannot be distinguished by the discriminator D, the generator network G has successfully obtained the underlying dependence of human joints. We conduct experiments on three widely used human pose estimation benchmark datasets, i.e., LSP, MPII and COCO, whose results show the effectiveness of our proposed framework.","Human pose estimation, Cascade feature network, Graph structure network, Generative adversarial network",Lei Tian and Peng Wang and Guoqiang Liang and Chunhua Shen,https://www.sciencedirect.com/science/article/pii/S0031320321000509,https://doi.org/10.1016/j.patcog.2021.107863,0031-3203,2021,107863,115,Pattern Recognition,An adversarial human pose estimation network injected with graph structure,article,TIAN2021107863,
"Nowadays, pancreas segmentation in CT scans has gained more and more attention for computer-assisted diagnosis of inflammation (pancreatitis) or cancer. Despite the thrilling success of deep convolutional neural networks (DCNNs) in automatic pancreas segmentation, the heavy computational complexity of such networks impedes the deployment in clinical applications. To alleviate this issue, this paper establishes a novel end-to-end DCNN model for pursuing high-accurate automatic pancreas segmentation but with low computational cost. Specifically, built upon a simplified FCN architecture, we propose two novel network modules, named as the scale-transferrable feature fusion module (STFFM) and prior propagation module (PPM), respectively, for pancreas segmentation. Equipped with the scale-transferrable operation, STFFM can learn rich fusion features but with very lightweight network architecture. By dynamically adapting the spatial prior to the input slice data as well as the deep feature maps, PPM enables the network model to explore informative spatial priors for pancreas segmentation. Comprehensive experiments on the NIH dataset and the MSD dataset are conducted to evaluate the proposed approach. The obtained experimental results demonstrate that our approach can effectively reduce the computational cost and simultaneously archive the outperforming performance when compared to the state-of-the-art methods.","Pancreas segmentation, Lightweight DCNN, Localization, Segmentation, Spatial prior",Dingwen Zhang and Jiajia Zhang and Qiang Zhang and Jungong Han and Shu Zhang and Junwei Han,https://www.sciencedirect.com/science/article/pii/S0031320320305653,https://doi.org/10.1016/j.patcog.2020.107762,0031-3203,2021,107762,114,Pattern Recognition,Automatic pancreas segmentation based on lightweight DCNN modules and spatial prior propagation,article,ZHANG2021107762,
"With wide applications in surveillance and human-robot interaction, view-invariant human action recognition is critical, however, challenging, due to the action occlusion and information loss caused by view change. Current methods mainly seek for a common feature space for different views. However, such solutions become invalid when there exist few common features,Â e.g. large view change. To tackle the problem, we propose an Unsupervised AttentioN Transfer (UANT) approach for view-invariant action recognition. Other than transferring feature knowledge, UANT transfers attention from one selected reference view to arbitrary views, which correctly emphasizes crucial body joints and their relations for view-invariant representation. In addition, the attention calculation method taking into account both recognition contribution and reliability of skeleton joints generates effective attention. Experiments showed its effectiveness for correctly locating crucial body joints in action sequences. We exhaustively evaluate our approach on the UESTC and the NTU dataset, performing unsupervised view-invariant evaluations,Â i.e. X-view and Arbitrary-view recognition. Experiment results demonstrate its superiority in view-invariant representation and recognition.","View-invariant recognition, Cross-view evaluation, Attention learning, Transfer learning",Yanli Ji and Yang Yang and Heng Tao Shen and Tatsuya Harada,https://www.sciencedirect.com/science/article/pii/S0031320320306105,https://doi.org/10.1016/j.patcog.2020.107807,0031-3203,2021,107807,113,Pattern Recognition,View-invariant action recognition via Unsupervised AttentioN Transfer (UANT),article,JI2021107807,
"Content-Based Mammogram Retrieval (CBMR) methods using Multi-View Information Fusion (MVIF) have triggered a growing interest in the last years given their ability to help radiologists make the right breast-cancer related decision. To further improve the retrieval performance, this paper introduces an efficient MVIF-CBMR method based on late fusion that combines retrieval result-level of Medio-Lateral Oblique (MLO) and Cranio-Caudal (CC) views. The proposed method adopts a coupled multi-index with a dynamic distance to evaluate the similarity between mammograms, which allows to fully exert the discriminative power of the complementary of MLO-CC features. Furthermore, the ROI dataset signature indexing step uses a hashing technique to optimize the computational time for retrieving relevant images. Thus, the proposed method takes two query ROIs corresponding to two different views (MLO and CC) as input and displays the most similar ROIs to each view using a dynamic similarity assessment. The retrieved ROIs can therefore be analyzed according to their clinical cases for the final decision-making relative to the query ROIs. The experiments realized on the challenging Digital Database for Screening Mammography (DDSM) dataset have proved the effectiveness and the efficiency of the proposed method.","Multi-view information fusion, Multidimensional indexing, Locality sensitive hashing, Content-based mammogram retrieval, Dynamic similarity",Amira Jouirou and Abir BaÃ¢zaoui and Walid Barhoumi,https://www.sciencedirect.com/science/article/pii/S0031320320305896,https://doi.org/10.1016/j.patcog.2020.107786,0031-3203,2021,107786,112,Pattern Recognition,Multi-view content-based mammogram retrieval using dynamic similarity and locality sensitive hashing,article,JOUIROU2021107786,
"Visual place recognition has attracted widespread research interest in multiple fields such as computer vision and robotics. Recently, researchers have employed advanced deep learning techniques to tackle this problem. While an increasing number of studies have proposed novel place recognition methods based on deep learning, few of them has provided a whole picture about how and to what extent deep learning has been utilized for this issue. In this paper, by delving into over 200 references, we present a comprehensive survey that covers various aspects of place recognition from deep learning perspective. We first present a brief introduction of deep learning and discuss its opportunities for recognizing places. After that, we focus on existing approaches built upon convolutional neural networks, including off-the-shelf and specifically designed models as well as novel image representations. We also discuss challenging problems in place recognition and present an extensive review of the corresponding datasets. To explore the future directions, we describe open issues and some new tools, for instance, generative adversarial networks, semantic scene understanding and multi-modality feature learning for this research topic. Finally, a conclusion is drawn for this paper.","Visual place recognition, Deep learning, Visual SLAM, Survey",Xiwu Zhang and Lei Wang and Yan Su,https://www.sciencedirect.com/science/article/pii/S003132032030563X,https://doi.org/10.1016/j.patcog.2020.107760,0031-3203,2021,107760,113,Pattern Recognition,Visual place recognition: A survey from deep learning perspective,article,ZHANG2021107760,
"Visual dialog is a task that two agents: Question-BOT (Q-BOT) and Answer-BOT (A-BOT), which communicate in natural language on the situation of information asymmetry. Q-BOT generates questions based on an image caption and a historical dialog. A-BOT answers the questions grounded on the image. Moreover, we play a cooperative âimage guessingâ game between Q-BOT and A-BOT, so that Q-BOT can select an unseen image from a set of images. However, as the valid information of the image caption and the historical dialog fades along the interaction, existing methods usually generate irrelevant and homogenous questions, which are worthless to the visual dialog system. To tackle this issue, we propose an Attentive Memory Network (AMN) to fully exploit the image caption and historical dialog information. Specifically, the attentive memory network mainly consists of a memory network and a fusion module. The memory network holds long term historical dialog information and gives each round of the dialog a different weight. Aside from the historical dialog information, the fusion module in Q-BOT and A-BOT further uses the image caption and the image feature, respectively. The caption information assists Q-BOT with the attentive generation of the questions, and the image feature helps A-BOT produce precise answers. With the AMN, the generated questions are diverse and concentrated, and the corresponding answers are accurate. The experimental results on VisDial v1.0 show the effectiveness of our proposed model, which outperforms the state-of-the-art methods.","Visual dialog, Attentive memory network, Reinforcement learning",Lei Zhao and Xinyu Lyu and Jingkuan Song and Lianli Gao,https://www.sciencedirect.com/science/article/pii/S0031320321000108,https://doi.org/10.1016/j.patcog.2021.107823,0031-3203,2021,107823,114,Pattern Recognition,GuessWhich? Visual dialog with attentive memory network,article,ZHAO2021107823,
"Diffusion technique is powerful for semi-supervised image segmentation since the geometry of the data manifold can be captured by the affinity propagation. Conventional diffusion methods focus on single label, which however ignore interactions among labels. This workstudies a generalized diffusion framework that considers label group for diffusion (LGD). The proposed framework can effectively capture the interactions among image elements via tensor product graph (TPG). A multivariate affinity framework is proposed to learn on higher-order TPG. Label pair diffusion algorithm is naturally derivedfrom the framework by considering second-order affinity (LG(2)D). We theoretically show that conventional label diffusion is the simplest case of the proposed framework (LG(1)D). Extensive experiments on image segmentation and image pair co-segmentation datasets demonstrate the superior performance of the proposed framework.","Image segmentation, Image pair co-segmentation, Label group diffusion, Geometry manifold, Tensor product graph",Tao Wang and Zexuan Ji and Jian Yang and Quansen Sun and Xiaobo Shen and Zhenwen Ren and Qi Ge,https://www.sciencedirect.com/science/article/pii/S0031320320305926,https://doi.org/10.1016/j.patcog.2020.107789,0031-3203,2021,107789,112,Pattern Recognition,Label group diffusion for image and image pair segmentation,article,WANG2021107789,
"Generating an image from its textual description requires both a certain level of language understanding and common sense knowledge about the spatial relations of the physical entities being described. In this work, we focus on inferring the spatial relation between entities, a key step in the process of composing scenes based on text. More specifically, given a caption containing a mention to a subject and the location and size of the bounding box of that subject, our goal is to predict the location and size of an object mentioned in the caption. Previous work did not use the caption text information, but a manually provided relation holding between the subject and the object. In fact, the used evaluation datasets contain manually annotated ontological triplets but no captions, making the exercise unrealistic: a manual step was required; and systems did not leverage the richer information in captions. Here we present a system that uses the full caption, and Relations in Captions (REC-COCO), a dataset derived from MS-COCO which allows to evaluate spatial relation inference from captions directly. Our experiments show that: (1) it is possible to infer the size and location of an object with respect to a given subject directly from the caption; (2) the use of full text allows to place the object better than using a manually annotated relation. Our work paves the way for systems that, given a caption, decide which entities need to be depicted and their respective location and sizes, in order to then generate the final image.","Text-to-image synthesis, Natural language understanding, Spatial relations, Deep learning",Aitzol Elu and Gorka Azkune and Oier Lopez {de Lacalle} and Ignacio Arganda-Carreras and Aitor Soroa and Eneko Agirre,https://www.sciencedirect.com/science/article/pii/S0031320321000340,https://doi.org/10.1016/j.patcog.2021.107847,0031-3203,2021,107847,113,Pattern Recognition,Inferring spatial relations from textual descriptions of images,article,ELU2021107847,
"We perform the classification of ancient Roman Republican coins via recognizing their reverse motifs where various objects, faces, scenes, animals, and buildings are minted along with legends. Most of these coins are eroded due to their age and varying degrees of preservation, thereby affecting their informative attributes for visual recognition. Changes in the positions of principal symbols on the reverse motifs also cause huge variations among the coin types. Lastly, in-plane orientations, uneven illumination, and a moderate background clutter further make the classification task non-trivial and challenging. To this end, we present a novel network model, CoinNet, that employs compact bilinear pooling, residual groups, and feature attention layers. Furthermore, we gathered the largest and most diverse image dataset of the Roman Republican coins that contains more than 18,000 images belonging to 228 different reverse motifs. On this dataset, our model achieves a classification accuracy of more than 98% and outperforms the conventional bag-of-visual-words based approaches and more recent state-of-the-art deep learning methods. We also provide a detailed ablation study of our network and its generalization capability.","Coins dataset, Compact bilinear pooling, Convolutional networks, Visual attention, Residual blocks, Deep learning in artâs history, Roman Republican coins",Hafeez Anwar and Saeed Anwar and Sebastian Zambanini and Fatih Porikli,https://www.sciencedirect.com/science/article/pii/S0031320321000583,https://doi.org/10.1016/j.patcog.2021.107871,0031-3203,2021,107871,114,Pattern Recognition,Deep ancient Roman Republican coin classification via feature fusion and attention,article,ANWAR2021107871,
"Collaborative Representation-based Classifier (CRC) has shown its advantages and impressive results in face recognition. To further imporve the performance of CRC, we propose a novel dimensionality reduction method termed Multiple Discriminant Analysis for Collaborative Representation-based Classification (MDA-CRC). Considering the labeling criterion of CRC is class-specific, MDA-CRC solves a group of binary classification problems where specific feature subspaces are learned for each class. In each binary classification problem, an orthogonal discriminant analysis method based on collaborative representation is adopted. Hence, MDA-CRC can improve the discriminant ability of collaborative representation and be consistent with the labeling criterion of CRC simultaneously. Further, the convergence of MDA-CRC is proven. Extensive experiments on several benchmark datasets demonstrate the effectiveness of MDA-CRC.","Collaborative representation, Orthogonal discriminative projection, Face recognition, Binary classification",Zhichao Zheng and Huaijiang Sun and Ying Zhou,https://www.sciencedirect.com/science/article/pii/S0031320321000066,https://doi.org/10.1016/j.patcog.2021.107819,0031-3203,2021,107819,112,Pattern Recognition,Multiple discriminant analysis for collaborative representation-based classification,article,ZHENG2021107819,
"Stroke classification and text line grouping are important tasks in online handwritten document segmentation. In the past, the two tasks were usually performed using different models which are trained independently and perform sequentially. This cannot optimize the integration of contextual information and the system may suffer from error accumulation in stroke classification. In this paper, we propose a method for joint text/non-text stroke classification and text line grouping in online handwritten documents using attention based graph neural network. In our framework, the stroke classification and text line grouping problems are formulated as node classification and node clustering problems in a relational graph, which is constructed based on the temporal and spatial relationship between strokes. We propose a new graph network architecture, called edge pooling attention network (EPAT) to efficiently aggregate information between the features of neighboring nodes and edges. The proposed model is trained by multi-task learning with cross entropy loss for node classification and distance metric loss for node clustering. In experiments on two online handwritten document datasets IAMOnDo and Kondate, the proposed method is demonstrated effective, yielding superior performance in both stroke classification and text line grouping.","Online handwritten documents, Stroke classification, Text line grouping, Graph neural networks, Edge pooling attention networks",Jun-Yu Ye and Yan-Ming Zhang and Qing Yang and Cheng-Lin Liu,https://www.sciencedirect.com/science/article/pii/S0031320321000467,https://doi.org/10.1016/j.patcog.2021.107859,0031-3203,2021,107859,114,Pattern Recognition,Joint stroke classification and text line grouping in online handwritten documents with edge pooling attention networks,article,YE2021107859,
"In this paper, we focus on the derivation of blur moment invariants. Blur moment invariants are image moment-based features, which preserve their values when the image is convolved by a point-spread function (PSF). Suppose a PSF has N-fold rotational symmetry, we prove its geometric moments of the same order are linearly dependent. Depending on this property, a new approach is proposed to determine whether an existing similarity or affine moment invariant also has invariance to N-fold symmetric blur. Unlike earlier work, this method is not based on complicated operators and construction formulas. We use it to analyse classical moment-based features, and surprisingly find that five of Hu moment invariants are naturally invariant to N-fold symmetric blur. Meanwhile, we first prove the existence of moment invariants to both affine transform and N-fold symmetric blur. The experiments using synthetic and real blur image datasets are carried out to test these expectations. And the results show that five Hu moment invariants outperform some widely used blur moment invariants and non-moment image features in image retrieval, classification and template matching.","Blurred image, Blur invariants, Moment invariants, Spatial transform, -fold symmetry, Object recognition, Template matching",Hanlin Mo and Hongxiang Hao and Hua Li,https://www.sciencedirect.com/science/article/pii/S0031320321000741,https://doi.org/10.1016/j.patcog.2021.107887,0031-3203,2021,107887,115,Pattern Recognition,Geometric moment invariants to spatial transform and N-fold symmetric blur,article,MO2021107887,
"Internet financing is an important alternative to banks where individuals or SMEs borrow money using online trading platforms. A central problem for internet financing is how to identify the most influential factors that are closely related to the credit risks. This problem is inherently challenging because the raw data of internet financing is often associated with complex structural correlations and usually contains many irrelevant and redundant features. To effectively identify the most salient features for credit risk evaluation in internet financing, we develop a new multiple structural interacting elastic net model for feature selection (MSIEN). Our idea is based on converting the original vectorial features into structure-based feature graph representations to encapsulate structural relationship between pairwise samples, and defining two new information theoretic criteria. One criterion maximizes joint relevance of different pairwise feature combinations in relation to the target feature graph and the other minimizes the redundancy between pairwise features. Then two structural interaction matrices are obtained with the elements representing the proposed information theoretic measures. To identify the most informative features, we formulate a new optimization model which combines the interaction matrices and an elastic net regularization model for the feature subset selection problem. We exploit an efficient iterative optimization algorithm to solve the proposed problem and also provide the theoretical analyses on its convergence property and computational complexity. Finally, experimental results on datasets of internet financing demonstrate the effectiveness of the proposed MSIEN method.","Credit risk, Feature selection, Elastic net, Sparse learning, Structural interaction, Internet financing",Lixin Cui and Lu Bai and Yanchao Wang and Xin Jin and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320321000224,https://doi.org/10.1016/j.patcog.2021.107835,0031-3203,2021,107835,114,Pattern Recognition,Internet financing credit risk evaluation using multiple structural interacting elastic net feature selection,article,CUI2021107835,
"Domain adaptation aims at leveraging rich knowledge in the source domain to build an accurate classifier in the different but related target domain. Most prior methods attempt to align features or reduce domain discrepancy by means of statistical properties yet ignore the differences among samples. In this paper, we put forward a novel solution based on collaborative representation for classifier adaptation. Similar to instance re-weighting, we aim to learn an adaptive classifier by multi-stage inference and instance rearranging. Specifically, a curriculum learning based sample selection scheme is proposed, then the chosen samples are integrated into training set iteratively. Due to the distribution mismatch of two domains, we propose distance-aware sparsity regularization to learn more flexible representations. Extensive experiments verify that the proposed method is comparable or superior to the state-of-the-art methods.","Domain adaptation, Collaborative representation, Curriculum learning, Classifier boosting",Chao Han and Deyun Zhou and Yu Xie and Maoguo Gong and Yu Lei and Jiao Shi,https://www.sciencedirect.com/science/article/pii/S0031320320306051,https://doi.org/10.1016/j.patcog.2020.107802,0031-3203,2021,107802,113,Pattern Recognition,Collaborative representation with curriculum classifier boosting for unsupervised domain adaptation,article,HAN2021107802,
"Action recognition and localization in untrimmed videos in weakly supervised scenario is a challenging problem of great application prospects. Limited by the information available in video-level labels, it is a promising attempt to fully leverage the instructive knowledge learned on trimmed videos to facilitate analysis of untrimmed videos, considering that there are abundant trimmed videos which are publicly available and well segmented with semantic descriptions. In order to enforce effective trimmed-untrimmed augmentation, this paper presents a novel framework of embedding-modeling iterative optimization network, referred to as IONet. In the proposed method, action classification modeling and shared subspace embedding are learned jointly in an iterative way, so that robust cross-domain knowledge transfer is achieved. With a carefully designed two-stage self-attentive representation learning workflow for untrimmed videos, irrelevant backgrounds are eliminated and fine-grained temporal relevance can be robustly explored. Extensive experiments are conducted on two benchmark datasets, i.e., THUMOS14 and ActivityNet1.3, and experimental results clearly corroborate the efficacy of our method. Source code is available on GitHub.22https://github.com/HCShi/IONet.","Action recognition, Temporal action localization, Attention mechanism, Generative adversarial networks, Subspace embedding",Xiao-Yu Zhang and Haichao Shi and Changsheng Li and Peng Li and Zekun Li and Peng Ren,https://www.sciencedirect.com/science/article/pii/S0031320321000182,https://doi.org/10.1016/j.patcog.2021.107831,0031-3203,2021,107831,113,Pattern Recognition,Weakly-supervised action localization via embedding-modeling iterative optimization,article,ZHANG2021107831,
"The random subspace method, also known as the pillar of random forests, is good at making precise and robust predictions. However, there is as yet no straightforward way to combine it with deep learning. In this paper, we therefore propose Neural Random Subspace (NRS), a novel deep learning based random subspace method. In contrast to previous forest methods, NRS enjoys the benefits of end-to-end, data-driven representation learning, as well as pervasive support from deep learning software and hardware platforms, hence achieving faster inference speed and higher accuracy. Furthermore, as a non-linear component to be encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear feature representations in CNNs more efficiently than contemporary, higher-order pooling methods, producing excellent results with negligible increase in parameters, floating point operations (FLOPs) and real running time. Compared with random subspaces, random forests and gradient boosting decision trees (GBDTs), NRS demonstrates superior performance on 35 machine learning datasets. Moreover, on both 2D image and 3D point cloud recognition tasks, integration of NRS with CNN architectures achieves consistent improvements with only incremental cost.","Random subspace, Ensemble learning, Deep neural networks",Yun-Hao Cao and Jianxin Wu and Hanchen Wang and Joan Lasenby,https://www.sciencedirect.com/science/article/pii/S003132032030604X,https://doi.org/10.1016/j.patcog.2020.107801,0031-3203,2021,107801,112,Pattern Recognition,Neural random subspace,article,CAO2021107801,
"This paper presents the Attentional Combination Network (ACN), which is a highly accurate face alignment method that is tolerant of occlusion. The method combines a coordinate regression network and a heatmap regression network with a spatial attention. The coordinate regression generates the coordinates of facial landmark points directly such that they are fitted to the input face on the whole. The heatmap regression generates the heatmap of facial landmark points such that each channel provides good localization of the detail of its facial landmark point. These independent regressions compensate for each other complementarily such that the overall fitting tendency of the coordinate regression compensates for the inaccurate alignment of the heatmap regression due to missing local information, and the detailed localization of the heatmap regression compensates for the relatively inaccurate alignment of the coordinate regression. The proposed ACN uses coordinate-to-heatmap and the heatmap-to-coordinate conversion networks to combine two heterogeneous regressions, and to generate the final coordinates of the facial landmark points. The ACN use the spatial attention mechanism to effectively reject impeditive local features that are caused by the occlusion. In experiments on several benchmarks, the proposed ACN achieved state-of-the-art accuracy","Face alignment, Facial landmark localization, Attentional combination network, Converting networks",Hyunsung Park and Daijin Kim,https://www.sciencedirect.com/science/article/pii/S0031320320305641,https://doi.org/10.1016/j.patcog.2020.107761,0031-3203,2021,107761,114,Pattern Recognition,ACN: Occlusion-tolerant face alignment by attentional combination of heterogeneous regression networks,article,PARK2021107761,
"Supervised graph convolution network (GCN) based hashing algorithms have achieved good results by recognizing images according to the relationships between objects, but they are hard to be applied to label-free scenarios. Besides, most existing unsupervised deep hashing algorithms neglect the relationships between different samples and thus fail to achieve high precision. To address this problem, we propose NRDH, an unsupervised Deep Hashing method with Node Representation for image retrieval, which adopts unsupervised GCN to integrate the relationships between samples into image visual features. NRDH consists of node representation learning stage and hash function learning stage. In the first stage, we treat each image as a node of a graph and design GCN-based AutoEncoder, which can integrate the relationships between samples into node representation. In the second stage, we use above node representations to guide the network and help learn the hash function to fast achieve an end-to-end hash model to generate semantic hash codes. Extensive experiments on CIFAR-10, MS-COCO and FLICKR25K show NRDH can achieve higher performance and outperform the state-of-the-art unsupervised deep hashing methods.","Deep hashing, GCN, Node representation, Image retrieval",Yangtao Wang and Jingkuan Song and Ke Zhou and Yu Liu,https://www.sciencedirect.com/science/article/pii/S0031320320305884,https://doi.org/10.1016/j.patcog.2020.107785,0031-3203,2021,107785,112,Pattern Recognition,Unsupervised deep hashing with node representation for image retrieval,article,WANG2021107785,
"Trajectory prediction is a crucial element of many automated tasks, such as autonomous navigation or video surveillance. To automatically predict the motion of an agent (e.g., pedestrian or car), the model needs to efficiently represent human motion and âunderstandâ the external stimuli that may influence human behavior. In this work we propose a methodology to model the motion of agents in a video scene. Our method is based on space-varying sparse motion fields, which simultaneously characterize diverse motion patterns in the scene and implicitly learn contextual cues about the static environment, namely obstacles and semantic constraints. The sparse motion fields are applied to the task of long-term trajectory prediction using a probabilistic generative approach. Several benchmark data sets are used to demonstrate the potential of the proposed approach and show that our method achieves competitive state-of-the-art performances.","Human motion analysis, Trajectory prediction, Sparse motion fields",Catarina Barata and Jacinto C. Nascimento and JoÃ£o M. Lemos and Jorge S. Marques,https://www.sciencedirect.com/science/article/pii/S0031320320304349,https://doi.org/10.1016/j.patcog.2020.107631,0031-3203,2021,107631,110,Pattern Recognition,Sparse motion fields for trajectory prediction,article,BARATA2021107631,
"Knowledge distillation is an effective way to transfer the knowledge from a pre-trained teacher model to a student model. Co-distillation, as an online variant of distillation, further accelerates the training process and paves a new way to explore the âdark knowledgeâ by training n models in parallel. In this paper, we explore the âdivergent examplesâ, which can make the classifiers have different predictions and thus induce the âdark knowledgeâ, and we propose a novel approach named Adversarial Co-distillation Networks (ACNs) to enhance the âdark knowledgeâ by generating extra divergent examples. Note that we do not involve any extra dataset, and we only utilize the standard training set to train the entire framework. ACNs are end-to-end frameworks composed of two parts: an adversarial phase consisting of Generative Adversarial Networks (GANs) to generate the divergent examples and a co-distillation phase consisting of multiple classifiers to learn the divergent examples. These two phases are learned in an iterative and adversarial way. To guarantee the quality of the divergent examples and the stability of ACNs, we further design âWeakly Residual Connectionâ module and âRestricted Adversarial Searchâ module to assist in the training process. Extensive experiments with various deep architectures on different datasets well demonstrate the effectiveness of our approach.","Knowledge distillation, Data augmentation, Generative adversarial nets, Divergent examples, Image classification",Haoran Zhang and Zhenzhen Hu and Wei Qin and Mingliang Xu and Meng Wang,https://www.sciencedirect.com/science/article/pii/S0031320320304623,https://doi.org/10.1016/j.patcog.2020.107659,0031-3203,2021,107659,111,Pattern Recognition,Adversarial co-distillation learning for image recognition,article,ZHANG2021107659,
"In this paper, we propose a probabilistic framework for solving the task of âVisual Dialogâ. Solving this task requires reasoning and understanding of visual modality, language modality, and common sense knowledge to answer. Various architectures have been proposed to solve this task by variants of multi-modal deep learning techniques that combine visual and language representations. However, we believe that it is crucial to understand and analyze the sources of uncertainty for solving this task. Our approach allows for estimating uncertainty and also aids a diverse generation of answers. The proposed approach is obtained through a probabilistic representation module that provides us with representations for image, question and conversation history, a module that ensures that diverse latent representations for candidate answers are obtained given the probabilistic representations and an uncertainty representation module that chooses the appropriate answer that minimizes uncertainty. We thoroughly evaluate the model with a detailed ablation analysis, comparison with state of the art and visualization of the uncertainty that aids in the understanding of the method. Using the proposed probabilistic framework, we thus obtain an improved visual dialog system that is also more explainable.","CNN, LSTM, Uncertainty, Aleatoric uncertainty, Epistemic uncertainty vision and language, Visual dialog, VQA, Answer generation, Question generation, Bayesian deep learning",Badri N. Patro and  Anupriy and Vinay P. Namboodiri,https://www.sciencedirect.com/science/article/pii/S0031320320303897,https://doi.org/10.1016/j.patcog.2020.107586,0031-3203,2021,107586,110,Pattern Recognition,Probabilistic framework for solving visual dialog,article,PATRO2021107586,
"Recent advances in machine learning and prevalence of digital medical images have opened up an opportunity to address the challenging brain tumor segmentation (BTS) task by using deep convolutional neural networks. However, different from the RGB image data that are very widespread, the medical image data used in brain tumor segmentation are relatively scarce in terms of the data scale but contain the richer information in terms of the modality property. To this end, this paper proposes a novel cross-modality deep feature learning framework to segment brain tumors from the multi-modality MRI data. The core idea is to mine rich patterns across the multi-modality data to make up for the insufficient data scale. The proposed cross-modality deep feature learning framework consists of two learning processes: the cross-modality feature transition (CMFT) process and the cross-modality feature fusion (CMFF) process, which aims at learning rich feature representations by transiting knowledge across different modality data and fusing knowledge from different modality data, respectively. Comprehensive experiments are conducted on the BraTS benchmarks, which show that the proposed cross-modality deep feature learning framework can effectively improve the brain tumor segmentation performance when compared with the baseline methods and state-of-the-art methods.","Brain tumor segmentation, Cross-modality feature transition, Cross-modality feature fusion, Feature learning",Dingwen Zhang and Guohai Huang and Qiang Zhang and Jungong Han and Junwei Han and Yizhou Yu,https://www.sciencedirect.com/science/article/pii/S0031320320303654,https://doi.org/10.1016/j.patcog.2020.107562,0031-3203,2021,107562,110,Pattern Recognition,Cross-modality deep feature learning for brain tumor segmentation,article,ZHANG2021107562,
"Deep learning based action recognition methods require large amount of labelled training data. However, labelling large-scale video data is time consuming and tedious. In this paper, we consider a more challenging few-shot action recognition problem where the training samples are few and rare. To solve this problem, memory network has been designed to use an external memory to remember the experience learned in training and then apply it to few-shot prediction during testing. However, existing memory-based methods just update the visual information with fixed label embeddings in the memory, which cannot adapt well to novel activities during testing. To alleviate the issue, we propose a novel end-to-end cross-modal memory network for few-shot activity recognition. Specifically, the proposed memory architecture stores the dynamic visual and textual semantics for some high-level attributes related to human activities. And the learned memory can provide effective multi-modal information for new activity recognition in the testing stage. Extensive experimental results on two video datasets, including HMDB51 and UCF101, indicate that our method could achieve significant improvements over other previous methods.","Few-shot learning, Activity recognition, Cross-modal memory",Lingling Zhang and Xiaojun Chang and Jun Liu and Minnan Luo and Mahesh Prakash and Alexander G. Hauptmann,https://www.sciencedirect.com/science/article/pii/S0031320320301515,https://doi.org/10.1016/j.patcog.2020.107348,0031-3203,2020,107348,108,Pattern Recognition,Few-shot activity recognition with cross-modal memory network,article,ZHANG2020107348,
"While Structure from Motion achieves great success in 3D reconstruction, it still meets challenges on large scale scenes. Incremental SfM approaches are robust to outliers, but are limited by low efficiency and easy suffer from drift problem. Though Global SfM methods are more efficient than incremental approaches, they are sensitive to outliers, and would also meet memory limitation and time bottleneck. In this work, large scale SfM is deemed as a graph problem, where graph are respectively constructed in image clustering step and local reconstructions merging step. By leveraging the graph structure, we are able to handle large scale dataset in divide-and-conquer manner. Firstly, images are modelled as graph nodes, with edges are retrieved from geometric information after feature matching. Then images are divided into independent clusters by a image clustering algorithm, and followed by a subgraph expansion step, the connection and completeness of scenes are enhanced by walking along a maximum spanning tree, which is utilized to construct overlapping images between clusters. Secondly, Image clusters are distributed into servers to execute SfM in parallel mode. Thirdly, after local reconstructions complete, we construct a minimum spanning tree to find accurate similarity transformations. Then the minimum spanning tree is transformed into a Minimum Height Tree to find a proper anchor node, and is further utilized to prevent error accumulation. We evaluate our approach on various kinds of datasets and our approach shows superiority over the state-of-the-art in accuracy and efficiency. Our algorithm is open-sourced in https://github.com/AIBluefisher/GraphSfM.","Clustering, Structure from motion, Minimum spanning tree",Yu Chen and Shuhan Shen and Yisong Chen and Guoping Wang,https://www.sciencedirect.com/science/article/pii/S003132032030340X,https://doi.org/10.1016/j.patcog.2020.107537,0031-3203,2020,107537,107,Pattern Recognition,Graph-based parallel large scale structure from motion,article,CHEN2020107537,
"Existing monocular depth estimation methods are unsatisfactory due to the inaccurate inference of depth details and the loss of spatial information. In this paper, we present a novel detail-preserving network (DPNet), i.e., a dual-branch network architecture that fully addresses the above problems and facilitates the depth map inference. Specifically, in contextual branch (CB), we propose an effective and efficient nonlocal spatial attention module by introducing non-local filtering strategy to explicitly exploit the pixel relationship in spatial domain, which can bring significant promotion on depth details inference. Meanwhile, we design a spatial branch (SB) to preserve the spatial information and generate high-resolution features from input color image. A refinement module (RM) is then proposed to fuse the heterogeneous features from both spatial and contextual branches to obtain a high quality depth map. Experimental results show that the proposed method outperforms SOTA methods on benchmark RGB-D datasets.","Depth estimation, Detail-preserving, Spatial, Attention, Depth map",Xinchen Ye and Shude Chen and Rui Xu,https://www.sciencedirect.com/science/article/pii/S0031320320303812,https://doi.org/10.1016/j.patcog.2020.107578,0031-3203,2021,107578,109,Pattern Recognition,DPNet: Detail-preserving network for high quality monocular depth estimation,article,YE2021107578,
"Weak signal detection is a challenging yet significant problem in the field of radio communication. Although hand-crafted filters are widely used in signal processing, they are challenged by the weak signal detection task with unknown background noise especially in the range of 0-5dB. In this paper, we propose the learning modulation filter networks (LMFNs) to improve the detection performance. The approach is based on a two-stage optimization scheme which addresses filter learning, attention mechanism and classification in a unified framework. Modulation filters are built to enhance the capacity of the learned filters, and the attention mechanism further characterizes the saliency properties of the input signal. LMFNs reduce the storage size of the network while achieving the state-of-the-art performance by a significant margin compared to traditional cognitive radio approaches. We establish a weak signal dataset that contains unmanned aerial vehicle (UAV) communication signals in a real-terrain environment. The source code and dataset will be made publicly available soon.","Weak signal detection, Filter learning, Attention, Modulation classification, Wireless communication",Duona Zhang and Wenrui Ding and Baochang Zhang and Chunhui Liu and Jungong Han and David Doermann,https://www.sciencedirect.com/science/article/pii/S0031320320303939,https://doi.org/10.1016/j.patcog.2020.107590,0031-3203,2021,107590,109,Pattern Recognition,Learning modulation filter networks for weak signal detection in noise,article,ZHANG2021107590,
"Due to a variety of motions across different frames, it is highly challenging to learn an effective spatiotemporal representation for accurate video saliency prediction (VSP). To address this issue, we develop an effective spatiotemporal feature alignment network tailored to VSP, mainly including two key sub-networks: a multi-scale deformable convolutional alignment network (MDAN) and a bidirectional convolutional Long Short-Term Memory (Bi-ConvLSTM) network. The MDAN learns to align the features of the neighboring frames to the reference one in a coarse-to-fine manner, which can well handle various motions. Specifically, the MDAN owns a pyramidal feature hierarchy structure that first leverages deformable convolution (Dconv) to align the lower-resolution features across frames, and then aggregates the aligned features to align the higher-resolution features, progressively enhancing the features from top to bottom. The output of MDAN is then fed into the Bi-ConvLSTM for further enhancement, which captures the useful long-time temporal information along forward and backward timing directions to effectively guide attention orientation shift prediction under complex scene transformation. Finally, the enhanced features are decoded to generate the predicted saliency map. The proposed model is trained end-to-end without any intricate post processing. Extensive evaluations on four VSP benchmark datasets demonstrate that the proposed method achieves favorable performance against state-of-the-art methods. The source codes and all the results will be released at https://github.com/cj4L/ESAN-VSP.","Video saliency prediction, Feature alignment, Deformable convolution, Bidirectional ConvLSTM",Jin Chen and Huihui Song and Kaihua Zhang and Bo Liu and Qingshan Liu,https://www.sciencedirect.com/science/article/pii/S0031320320304180,https://doi.org/10.1016/j.patcog.2020.107615,0031-3203,2021,107615,109,Pattern Recognition,Video saliency prediction using enhanced spatiotemporal alignment network,article,CHEN2021107615,
"3D motion estimation is an important prerequisite for the autonomous operation of vehicles and robots in dynamic environments. This work presents FeatFlow, a novel neural network architecture to estimate 3D motions from unstructured point clouds. Specifically, we learn deep geometric features to estimate the dense scene flow and the ego-motion of the platform. We build a scene flow estimation pipeline by an encoder-decoder architecture which comprises three novel modules: feature extractor, motion embedder, and flow decoder. By using a point-score layer to assign scores to the extracted features in a learning procedure, the feature extractor effectively extracts keypoints and features that are most significant for estimating the relative transformation between two consecutive point clouds. The whole model adaptively learns the required robust descriptors to represent a variety of point motions at the object or scene level. We evaluated our approach on synthetic data from FlyingThings3D, and real-world LiDAR scans from KITTI and Oxford RobotCar. Our network successfully generalizes to datasets with different patterns, outperforming various baselines and achieving state-of-the-art performance.","Feature learning, Motion estimation, Point clouds, Scene flow, Scan-matching, Ego-motion",Qing Li and Cheng Wang and Xin Li and Chenglu Wen,https://www.sciencedirect.com/science/article/pii/S0031320320303770,https://doi.org/10.1016/j.patcog.2020.107574,0031-3203,2021,107574,111,Pattern Recognition,FeatFlow: Learning geometric features for 3D motion estimation,article,LI2021107574,
"Several works have studied clustering strategies that combine classical clustering algorithms and deep learning methods. These strategies generally improve clustering performance, however deep autoencoder setting issues impede the robustness of these approaches. To alleviate the impact of hyperparameters setting, we propose a model which combines spectral clustering and deep autoencoder strengths in an ensemble framework. Our proposal does not require any pretraining and includes the three following steps: generating various deep embeddings from the original data, constructing a sparse and low-dimensional ensemble affinity matrix based on anchors strategy and applying spectral clustering to obtain the common space shared by multiple deep representations. While the anchors strategy ensures an efficient merging of the encodings, the fusion of various deep representations enables to mitigate the deep networks setting issues. Experiments on various benchmark datasets demonstrate the potential and robustness of our approach compared to state-of-the-art deep clustering methods.","Spectral clustering, Unsupervised ensemble learning, Autoencoder,",SÃ©verine Affeldt and Lazhar Labiod and Mohamed Nadif,https://www.sciencedirect.com/science/article/pii/S0031320320303253,https://doi.org/10.1016/j.patcog.2020.107522,0031-3203,2020,107522,108,Pattern Recognition,Spectral clustering via ensemble deep autoencoder learning (SC-EDAE),article,AFFELDT2020107522,
"The model we developed is a novel comprehensive solution to compress and accelerate the Visual Question Answering systems. In our algorithm Convolutional Neural Network is compressed with Long Short Term Memory to accelerate processing simultaneously. We propose to conduct various decomposition methods and regression strategies on different layers, including Canonical Polyadic, Tucker, and Tensor Train to decompose Fully Connected layers in CNN and LSTM. The Flattening Layer and Fully Connected layer at the end of the model are replaced with Tensor Regression layers. In order to compress the parameter further, the feature flow between the layers is compressed by Tensor Contraction layer. The proposed tensor decomposition model was evaluated on VQA 2.0 dataset with Pythia as baseline model. Our proposed model achieved from 77% to 91% of compression ratio, and only from 1% to 5% accuracy drop.","Tensor decomposition, Tensor regression layer, Tensor contraction layer, Visual question answering",Zongwen Bai and Ying Li and Marcin WoÅºniak and Meili Zhou and Di Li,https://www.sciencedirect.com/science/article/pii/S0031320320303411,https://doi.org/10.1016/j.patcog.2020.107538,0031-3203,2021,107538,110,Pattern Recognition,DecomVQANet: Decomposing visual question answering deep network via tensor decomposition and regression,article,BAI2021107538,
"Deep metric learning (DML) has been designed to maximize the inter-class variance that is the distance between embedding features belonging to different classes. Since conventional DML techniques do not consider the statistical characteristics of the embedding space, or they calculate similarity using only a given feature, they make it difficult to adaptively reflect the characteristics of the feature distribution during the learning process. This paper proposes a virtual metric loss (VML) incorporating with embedding features by using virtual samples produced through linear discriminant analysis (LDA). This study is valuable in that it proposes a new metric that can learn inter-class variance of embedding features by integrating discriminant analysis and metric learning which have a common purpose of inter-class variance analysis. In addition, we theoretically analyze the eigenvalue equation problem and the degree of stabilization in the embedding space. We have verified the performance of the proposed VML through extensive experiments on large and few-shot retrieval datasets. For example, in the CUB200-2011 dataset, the VML showed a recall rate about 0.7% higher than a state-of-the-art method. We also explored a new similarity through virtual samples and adjusted the difficulty of embedding features, thereby confirming the possibility of expanding virtual samples into various fields of pattern recognition.","Linear discriminant analysis, Deep metric learning, Retrieval task",Dae Ha Kim and Byung Cheol Song,https://www.sciencedirect.com/science/article/pii/S0031320320304465,https://doi.org/10.1016/j.patcog.2020.107643,0031-3203,2021,107643,110,Pattern Recognition,Virtual sample-based deep metric learning using discriminant analysis,article,KIM2021107643,
"Deep spatio-temporal neural networks have shown promising performance for video super-resolution (VSR) in recent years. However, most of them heavily rely on accuracy motion estimations. In this paper, we propose a novel spatio-temporal matching network (STMN) for video super-resolution, which works on the wavelet domain to reduce dependence on motion estimations. Specifically, our STMN consists of three major components: a temporal fusion wavelet network (TFWN), a non-local matching network (NLMN), and a global wavelet domain residual connection (GWDRC). TFWN adaptively extracts temporal fusion wavelet maps via three 3d convolutional layers and a discrete wavelet transform (DWT) decomposition layer. The extracted temporal fusion wavelet maps are rich in spatial information and knowledge of different frequencies from consecutive frames, which are feed to NLMN for learning deep wavelet representations. NLMN integrates super-resolution and denoising into a unified module by pyramidally stacking non-local matching residual blocks (NLMRB). At last, GWDRC reconstructs the super-resolved frames from the deep wavelet representations by using global wavelet domain residual information. Consequently, our STMN can efficiently enhance reconstruction quality by capturing different frequencies wavelet representations in consecutive frames, and does not require any motion compensation. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of our method compared with state-of-the-art methods.","Deep matching, Wavelet domain, Non-local matching, Residual learning, Video super-resolution",Xiaobin Zhu and Zhuangzi Li and Jungang Lou and Qing Shen,https://www.sciencedirect.com/science/article/pii/S0031320320304222,https://doi.org/10.1016/j.patcog.2020.107619,0031-3203,2021,107619,110,Pattern Recognition,Video super-resolution based on a spatio-temporal matching network,article,ZHU2021107619,
"In computer vision, the research community has been looking to how to benefit from weakly supervised learning that utilizes easily obtained image-level labels to train neural network models. The existing deep convolutional neural networks for weakly supervised learning, however, generally do not fully exploit the label dependencies in an image. To make full use of this information, in this paper, we propose a new framework for weakly supervised learning of deep convolutional neural networks, introducing graph convolutional networks to capture the semantic label co-occurrence in an image. Moreover, we propose a novel initialization method for label embedding in graph convolutional networks, which enables a smoother optimization for interrelationships learning. Extensive experiments and comparisons on four public benchmark datasets (PASCAL VOC 2007, PASCAL VOC 2012, Microsoft COCO, and NUS-WIDE) show the superior performance of our approach in both image classification and weakly supervised pointwise object localization. These results lead us to conclude that the label dependencies in the input image can provide valuable evidence for learning strongly localized features.","Deep learning, Learning systems, Convolutional neural networks, Predictive models, Image classification, Graph theory",Yongsheng Liu and Wenyu Chen and Hong Qu and S.M. Hasan Mahmud and Kebin Miao,https://www.sciencedirect.com/science/article/pii/S003132032030399X,https://doi.org/10.1016/j.patcog.2020.107596,0031-3203,2021,107596,109,Pattern Recognition,Weakly supervised image classification and pointwise localization with graph convolutional networks,article,LIU2021107596,
"Artificial neural networks have been shown significant performance in various image-to-image conversion tasks. However, complex conversions often require a large number of images for model training. Therefore, we propose a convolutional model for image-to-image conversions using a pipeline of simpler image processing modules. To verify our proposed approach, we use a document image binarization as the task. Document image binarization is an important process that affects the accuracy of document analysis and recognition. In this paper, we propose a novel document binarization method called Cascading Modular U-NetsÂ (CMU-Nets). CMU-Nets consist of pre-trained modular modules useful for overcoming the problem of a shortage of training images. We also propose a novel cascading scheme for improving overall cascading model performance. We verify the proposed model on all available Document Image Binarization CompetitionÂ (DIBCO) and the Handwritten-DIBCOÂ (H-DIBCO) datasets.","Convolutional neural network, U-Net, Document image binarization, DIBCO, H-DIBCO",Seokjun Kang and Brian Kenji Iwana and Seiichi Uchida,https://www.sciencedirect.com/science/article/pii/S0031320320303800,https://doi.org/10.1016/j.patcog.2020.107577,0031-3203,2021,107577,109,Pattern Recognition,Complex image processing with less dataâDocument image binarization by integrating multiple pre-trained U-Net modules,article,KANG2021107577,
"In this paper, a novel approach to abnormal event detection in crowded scenes is presented based on a new low-rank and compact coefficient dictionary learning (LRCCDL) algorithm. First, based on the background subtraction and binarization of surveillance videos, we construct a feature space by extracting the histogram of maximal optical flow projection (HMOFP) feature of the foreground from a normal training frame set. Second, in the training stage, a new joint optimization of the nuclear-norm and l2, 1-norm is applied to obtain a compact coefficient low-rank dictionary. Third, in the detection stage, l2, 1-norm optimization is utilized to obtain the reconstruction coefficient vectors of the testing samples. Note that the l2, 1-norm forces the reconstruction vectors of all the testing samples to compactly surround the same center in the training stage, such that the reconstruction errors of abnormal testing samples are different from those of normal ones. Finally, a reconstruction cost (RC) is introduced to detect abnormal frames. Experimental results on both global and local abnormal event detection show the effectiveness of our algorithm. Based on comparisons with state-of-the-art methods employing various criteria, the proposed algorithm achieves comparable detection results.","LRCCDL, Reconstruction cost, Abnormal event detection, Crowded scenes, Surveillance videos",Ang Li and Zhenjiang Miao and Yigang Cen and Xiao-Ping Zhang and Linna Zhang and Shiming Chen,https://www.sciencedirect.com/science/article/pii/S0031320320301588,https://doi.org/10.1016/j.patcog.2020.107355,0031-3203,2020,107355,108,Pattern Recognition,Abnormal event detection in surveillance videos based on low-rank and compact coefficient dictionary learning,article,LI2020107355,
"Domain adaptation aims to learn an adaptive classifier for target data using the labelled source data from a different distribution. Most proposed works construct cross-domain classifier by exploring one-sided property of the input data, i.e., either geometric or statistical property. Therefore they may ignore the complementarity between the two properties. Moreover, many previous methods implement knowledge transfer with two separated steps: divergence minimization and classifier construction, which degrades the adaptation robustness. In order to address such problems, we propose a unified cross-domain classification method via geometric and statistical adaptations (UCGS). UCGS models the divergence minimization and classifier construction in a unified way based on structural risk minimization principle and coupled adaptations theory. Specifically, UCGS constructs an adaptive model by simultaneously minimizing the structural risk on labelled source data, using Maximum Mean Discrepancy (MMD) criterion to implement statistical adaptation, and flexibly employing the NystrÃ¶m method to explore the geometric connections between domains. A domain-invariant graph is successfully constructed to link the two domains geometrically. The standard supervised methods can be used to instantiate UCGS to handle inter-domain classification problems. Comprehensive experiments show the superiority of UCGS on several real-world datasets.","Domain adaptation, Statistical adaptation, Maximum mean discrepancy (MMD), Geometric adaptation, NystrÃ¶m method",Weifeng Liu and Jinfeng Li and Baodi Liu and Weili Guan and Yicong Zhou and Changsheng Xu,https://www.sciencedirect.com/science/article/pii/S0031320320304611,https://doi.org/10.1016/j.patcog.2020.107658,0031-3203,2021,107658,110,Pattern Recognition,Unified Cross-domain Classification via Geometric and Statistical Adaptations,article,LIU2021107658,
"The performance of deep learning (DL) models is highly dependent on the quality and size of the training data, whose annotations are often expensive and hard to obtain. This work proposes a new strategy to train DL models by Learning Optimal samples Weights (LOW), making better use of the available data. LOW determines how much each sample in a batch should contribute to the training process, by automatically estimating its weight in the loss function. This effectively forces the model to focus on more relevant samples. Consequently, the models exhibit a faster convergence and better generalization, specially on imbalanced data sets where class distribution is long-tailed. LOW can be easily integrated to train any DL model and can be combined with any loss function, while adding marginal computational burden to the training process. Additionally, the analysis of how sample weights change during training provides insights on what the model is learning and which samples or classes are more challenging. Results on popular computer vision benchmarks and on medical data sets show that DL models trained with LOW perform better than with other state-of-the-art strategies.11Code is available at https://github.com/cajosantiago/LOW.","Deep learning, Sample weighting, Imbalanced data sets",Carlos Santiago and Catarina Barata and Michele Sasdelli and Gustavo Carneiro and Jacinto C. Nascimento,https://www.sciencedirect.com/science/article/pii/S0031320320303885,https://doi.org/10.1016/j.patcog.2020.107585,0031-3203,2021,107585,110,Pattern Recognition,LOW: Training deep neural networks by learning optimal sample weights,article,SANTIAGO2021107585,
"Existing Visible-Thermal Person Re-identification (VT-REID) methods usually adopt two-stream networks for cross-modality images. The two streams are trained to extract features from different modality images respectively. In contrast, we design a Modality Adversarial Neural Network (MANN) to solve VT-REID problem. Our proposed MANN includes a one-stream feature extractor and a modality discriminator. The heterogeneous images are processed by the feature extractor to generate modality-invariant features. And the designed modality discriminator aims to distinguish whether the extracted features are from visible or thermal modality. Moreover, our advanced dual-constrained triplet loss is introduced for better cross-modality matching performance. The experiments on two cross-modality person re-identification datasets show that MANN can effectively learn modality-invariant features and outperform state-of-the-art methods by a large margin.","Cross-modality, Adversarial learning, Person re-identification",Yi Hao and Jie Li and Nannan Wang and Xinbo Gao,https://www.sciencedirect.com/science/article/pii/S0031320320303368,https://doi.org/10.1016/j.patcog.2020.107533,0031-3203,2020,107533,107,Pattern Recognition,Modality adversarial neural network for visible-thermal person re-identification,article,HAO2020107533,
"Recently, text-to-image synthesis has achieved great progresses with the advancement of the Generative Adversarial Network (GAN). However, training the GAN models requires a large amount of pairwise image-text data, which is extremely labor-intensive to collect. In this paper, we make the first attempt to train a text-to-image synthesis model in an unsupervised manner, which does not require any human labeled image-text pair data. Specifically, we first rely on the visual concepts to bridge two independent image and sentence sets and thereby yield the pseudo image-text pair data, based on which one GAN model can thereby be initialized. One novel visual concept discrimination loss is proposed to train both generator and discriminator, which not only encourages the image expressing the true local visual concepts but also ensures the noisy visual concepts contained in the pseudo sentence being suppressed. Afterwards, one global semantic consistency regarding to the real sentence is used to adapt the pretrained GAN model to real sentences. Experimental results demonstrate that our proposed unsupervised training strategy is able to generate favorable images for given sentences, which even outperforms some existing models trained in the supervised manner. The code of this paper is available at https://github.com/dylls/Unsupervised_Text-to-Image_Synthesis.","Text-to-image synthesis, Generative adversarial network (GAN), Unsupervised training",Yanlong Dong and Ying Zhang and Lin Ma and Zhi Wang and Jiebo Luo,https://www.sciencedirect.com/science/article/pii/S0031320320303769,https://doi.org/10.1016/j.patcog.2020.107573,0031-3203,2021,107573,110,Pattern Recognition,Unsupervised text-to-image synthesis,article,DONG2021107573,
"Many clustering algorithms fail when clusters are of arbitrary shapes, of varying densities, or the data classes are unbalanced and close to each other, even in two dimensions. A novel clustering algorithm âDenMuneâ is presented to meet this challenge. It is based on identifying dense regions using mutual nearest neighborhoods of size K, where K is the only parameter required from the user, besides obeying the mutual nearest neighbor consistency principle. The algorithm is stable for a wide range of values of K. Moreover, it is able to automatically detect and remove noise from the clustering process as well as detecting the target clusters. It produces robust results on various low and high dimensional datasets relative to several known state of the art clustering algorithms.","Clustering, Mutual neighbors, Dimensionality reduction, Arbitrary shapes, Pattern recognition, Nearest neighbors, Density peak",Mohamed Abbas and Adel El-Zoghabi and Amin Shoukry,https://www.sciencedirect.com/science/article/pii/S0031320320303927,https://doi.org/10.1016/j.patcog.2020.107589,0031-3203,2021,107589,109,Pattern Recognition,DenMune: Density peak based clustering using mutual nearest neighbors,article,ABBAS2021107589,
"Visible-near infrared (VIS-NIR) face matching is a challenging issue in heterogeneous face recognition due to the large spectrum domain discrepancy as well as the over-fitting on insufficient pairwise VIS and NIR images during training. This paper proposes a coupled adversarial learning (CAL) approach for the VIS-NIR face matching by performing adversarial learning on both image and feature levels. On the image level, we learn a transformation network from unpaired NIR-VIS images to transform a NIR image to VIS domain. Cycle loss, global intensity loss and local texture loss are employed to better capture the discrepancy between NIR and VIS domains. The synthesized NIR or VIS images can be further used to alleviate the over-fitting problem in a semi-supervised way. On the feature level, we seek a shared feature space in which the heterogeneous face matching problem can be approximately treated as a homogeneous face matching problem. An adversarial loss and an orthogonal constraint are employed to reduce the spectrum domain discrepancy and the over-fitting problem, respectively. Experimental results show that CAL not only synthesizes high-quality VIS or NIR images, but also obtains state-of-the-art recognition results.","Adversarial learning, Heterogeneous face recognition, Deep representation",Ran He and Yi Li and Xiang Wu and Lingxiao Song and Zhenhua Chai and Xiaolin Wei,https://www.sciencedirect.com/science/article/pii/S0031320320304210,https://doi.org/10.1016/j.patcog.2020.107618,0031-3203,2021,107618,110,Pattern Recognition,Coupled adversarial learning for semi-supervised heterogeneous face recognition,article,HE2021107618,
"Dimensionality reduction is a critical step in the learning process that plays an essential role in various applications. The most popular methods for dimensionality reduction, SVD and PCA, for instance, only work on one-dimensional data. This means that for higher-order data like matrices or more generally tensors, data should be fold to the vector format. Thus, this approach ignores the spatial relationships of features and increases the probability of overfitting as well. Due to the mentioned issues, several methods like Generalized Low-Rank Approximation of Matrices (GLRAM) and Multilinear PCA (MPCA) proposed to deal with multi-dimensional data in their original format. Consequently, the spatial relationships of features preserved and the probability of overfitting diminished. Besides, the time and space complexity in such methods are less than vector-based ones. However, since the multilinear approach needs fewer parameters, its search space is much smaller than that of the vector-based one. To solve the previous problems of multilinear methods like GLRAM, we proposed a novel extension of GLRAM in which instead one transformation pair use multiple left and right transformation pairs on the projected data. Consequently, this provides the problem with a larger feasible region and smaller reconstruction error. This article provides several analytical discussions and experimental results that confirm the quality of the proposed method.","Machine learning, Matrix data classification, Kronecker product, Dimensionality reduction, SVD, GLRAM",Soheil Ahmadi and Mansoor Rezghi,https://www.sciencedirect.com/science/article/pii/S0031320320303484,https://doi.org/10.1016/j.patcog.2020.107545,0031-3203,2020,107545,108,Pattern Recognition,Generalized low-rank approximation of matrices based on multiple transformation pairs,article,AHMADI2020107545,
"Emotion plays a vital role in human daily life, and EEG signals are widely used in emotion recognition. Due to individual variability, training a generic emotion recognition model across different subjects is difficult. The conventional method involves the collection of a large amount of calibration data to build subject-specific models. Recently, developing an effective brain-computer interface with a short calibration time has become a challenge. To solve this problem, we propose a domain adaptation SPD matrix network (daSPDnet) that can successfully capture an intrinsic emotional representation shared between different subjects. Our method jointly exploits feature adaptation with distribution confusion and sample adaptation with centroid alignment. We compute the SPD matrix based on the covariance as a feature and make a novel attempt to combine prototype learning with the Riemannian metric. Extensive experiments are conducted on the DREAMER and DEAP datasets, and the results show the superiority of our proposed method.","EEG, Emotion recognition, Domain adaptation, SPD matrix, Riemannian manifold, Prototype learning",Yixin Wang and Shuang Qiu and Xuelin Ma and Huiguang He,https://www.sciencedirect.com/science/article/pii/S0031320320304295,https://doi.org/10.1016/j.patcog.2020.107626,0031-3203,2021,107626,110,Pattern Recognition,A prototype-based SPD matrix network for domain adaptation EEG emotion recognition,article,WANG2021107626,
"Human activity recognition (HAR) technology that analyzes data acquired from various types of sensing devices, including vision sensors and embedded sensors, has motivated the development of various context-aware applications in emerging domains, e.g., the Internet of Things (IoT) and healthcare. Even though a considerable number of HAR surveys and review articles have been conducted previously, the major/overall HAR subject has been ignored, and these studies only focus on particular HAR topics. Therefore, a comprehensive review paper that covers major subjects in HAR is imperative. This survey analyzes the latest state-of-the-art research in HAR in recent years, introduces a classification of HAR methodologies, and shows advantages and weaknesses for methods in each category. Specifically, HAR methods are classified into two main groups, which are sensor-based HAR and vision-based HAR, based on the generated data type. After that, each group is divided into subgroups that perform different procedures, including the data collection, pre-processing methods, feature engineering, and the training process. Moreover, an extensive review regarding the utilization of deep learning in HAR is also conducted. Finally, this paper discusses various challenges in the current HAR topic and offers suggestions for future research.","Human activity recognition, Action recognition, Sensors, Vision, Human-centric sensing, Deep learning, Context-awareness",L. {Minh Dang} and Kyungbok Min and Hanxiang Wang and Md. {Jalil Piran} and Cheol {Hee Lee} and Hyeonjoon Moon,https://www.sciencedirect.com/science/article/pii/S0031320320303642,https://doi.org/10.1016/j.patcog.2020.107561,0031-3203,2020,107561,108,Pattern Recognition,Sensor-based and vision-based human activity recognition: A comprehensive survey,article,MINHDANG2020107561,
"In this paper, we aim to tackle the one-shot person re-identification problem where only one image is labelled for each person, while other images are unlabelled. This task is challenging due to the lack of sufficient labelled training data. To tackle this problem, we propose to iteratively guess pseudo labels for the unlabelled image samples, which are later used to update the re-identification model together with the labelled samples. A new sampling mechanism is designed to select unlabelled samples to pseudo labelled samples based on the distance matrix, and to form a training triplet batch including both labelled samples and pseudo labelled samples. We also design an HSoften-Triplet-Loss to soften the negative impact of the incorrect pseudo label, considering the unreliable nature of pseudo labelled samples. Finally, we deploy an adversarial learning method to expand the image samples to different camera views. Our experiments show that our framework achieves a new state-of-the-art one-shot Re-ID performance on Market-1501 (mAP 42.7%) and DukeMTMC-Reid dataset (mAP 40.3%). Code is available on https://github.com/detectiveli/PSMA.","Re-ID, One-shot, Semi-supervised, GAN",Hui Li and Jimin Xiao and Mingjie Sun and Eng Gee Lim and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S0031320320304179,https://doi.org/10.1016/j.patcog.2020.107614,0031-3203,2021,107614,110,Pattern Recognition,Progressive sample mining and representation learning for one-shot person re-identification,article,LI2021107614,
"The key to the success of an Error-Correcting Output Code (ECOC) algorithm is the effective codematrix, which represents a set of class reassignment schemes for decomposing a multiclass problem into a set of binary class problems. This paper proposes a new method, which uses Ternary digit Operators based Genetic Programming (GP) to generate effective ECOC codematrix (TOGP-ECOC for short). In our GP, each terminal node stores a ternary digit string, representing a column and a related feature subset; each non-terminal node represents a ternary digit operator, which produces a new column based on its child nodes. In this way, each individual is interpreted as an ECOC codematrix along with a set of corresponding feature subsets, serving the solution for the multiclass classification task. When a new individual is produced, a legality checking process is carried out to verify whether the transformed codematrix follows the ECOC constraints. The illegal one is corrected according to different strategies. Besides, a local optimization algorithm is designed to prune redundant columns and improve the performance of each individual. Our experiments compared TOGP-ECOC with some well known ECOC algorithms on various data sets, and the results confirm the superiority of our algorithm. Our source code is available at: https://github.com/MLDMXM2017/TOGP-ECOC.","Error-correcting output code, Ternary digit operator, Genetic programming, Feature selection",Liang Yi-Fan and Liu Chang and Wang Han-Rui and Liu Kun-Hong and Yao Jun-Feng and She Ying-Ying and Dai Gui-Ming and Yuna Okina,https://www.sciencedirect.com/science/article/pii/S0031320320304453,https://doi.org/10.1016/j.patcog.2020.107642,0031-3203,2021,107642,110,Pattern Recognition,A novel error-correcting output codes based on genetic programming and ternary digit operators,article,YIFAN2021107642,
"Network representation learning has attracted increasing attention recently due to its applicability in network analysis. However, most existing network representation learning models only focus on preserving fragmentary aspects of network information, either node proximities or fixed semantic information. In this paper, we propose a novel algorithm named Rich Heterogeneous Information Preserving Network Representation Learning (HIRL), which integrates the high-order proximity among nodes and semantic information into a generic framework by exploiting a flexible autoencoder network. Based on the proposed algorithm, we can explore the hidden information in heterogeneous information networks through any custom form of path schema, and represents different types of nodes in a continuous and common vector space. Moreover, the proposed HIRL is applicable to homogeneous information networks. Extensive experimental results demonstrate that our approach can effectively preserve the information in networks under various path schemas, and performs better on real-world applications such as network reconstruction, link prediction, and node classification compared with the state-of-the-art methods.","Network representation learning, Heterogeneous information, Autoencoder",Bin Yu and Jinzhi Hu and Yu Xie and Chen Zhang and Zhouhua Tang,https://www.sciencedirect.com/science/article/pii/S0031320320303678,https://doi.org/10.1016/j.patcog.2020.107564,0031-3203,2020,107564,108,Pattern Recognition,Rich heterogeneous information preserving network representation learning,article,YU2020107564,
"Ordinal regression is one of the most influential tasks of supervised learning. Support vector ordinal regression (SVOR) is an appealing method to tackle ordinal regression problems. However, due to the complexity in the formulation of SVOR and the high cost of kernel computation, traditional SVOR solvers are inefficient for large-scale training. To address this problem, in this paper, we first highlight a special SVOR formulation whose thresholds are described implicitly, so that the dual formulation is concise to apply the state-of-the-art asynchronous parallel coordinate descent algorithm, such as AsyGCD. To further accelerate the training for SVOR, we propose two novel asynchronous parallel coordinate descent algorithms, called AsyACGD and AsyORGCD respectively. AsyACGD is an accelerated extension of AsyGCD using active set strategy. AsyORGCD is specifically designed for SVOR that it can keep the ordered thresholds when it is training so that it can obtain good performance with lower time. Experimental results on several large-scale ordinal regression datasets demonstrate the superiority of our proposed algorithms.","Asynchronous parallel, Coordinate descent, Support vector, Ordinal regression",Bin Gu and Xiang Geng and Wanli Shi and Yingying Shan and Yufang Huang and Zhijie Wang and Guansheng Zheng,https://www.sciencedirect.com/science/article/pii/S0031320320303952,https://doi.org/10.1016/j.patcog.2020.107592,0031-3203,2021,107592,109,Pattern Recognition,Solving large-scale support vector ordinal regression with asynchronous parallel coordinate descent algorithms,article,GU2021107592,
"Existing domain adaptation approaches focus on taking advantage of easy samples, i.e, target samples which are easier for adaptation. In previous work, tough, or hard, target samples are generally regarded as outliers or just being left to chance. As a result, the adaptation of tough target samples remains as a challenging problem in the community. In this paper, we report three novel ideas for domain adaptation: 1) splitting target samples into easy and tough ones; 2) deploying different strategies for samples with different adaptation difficulties; 3) leveraging easy samples to facilitate tough ones. Furthermore, we present a novel approach, named challenging tough sample networks (CTSN), to practice the three ideas and tame tough samples. Specifically, in our approach, a CNN with domain adaptation layers is first used to rapidly handle the easy samples and identify the tough ones. Then, a GAN with two classifiers is tailored to adapt the tough samples. The GAN leverages classification discrepancy and easy samples to tame the tough ones. Extensive experiments on both classic and large-scale benchmarks verify that both easy and tough samples do exist in real-world datasets and our approach is able to handle them.","Domain adaptation, transfer learning, adversarial learning",Lin Zuo and Mengmeng Jing and Jingjing Li and Lei Zhu and Ke Lu and Yang Yang,https://www.sciencedirect.com/science/article/pii/S0031320320303435,https://doi.org/10.1016/j.patcog.2020.107540,0031-3203,2021,107540,110,Pattern Recognition,Challenging tough samples in unsupervised domain adaptation,article,ZUO2021107540,
"The problem of semi-supervised video object segmentation has been popularly tackled by fine-tuning a general-purpose segmentation deep network on the annotated frame using hundreds of iterations of gradient descent. The time-consuming fine-tuning process, however, makes these methods difficult to use in practical applications. We propose a novel architecture called Annotation Guided U-net (AGUnet) for fast one-shot video object segmentation (VOS). AGUnet can quickly adapt a model trained on static images to segmenting the given target in a video by only several iterations of gradient descent. Our AGUnet is inspired by interactive image segmentation, where the interested target is segmented by using user annotated foreground. However, in AGUnet we use a fully-convolutional Siamese network to automatically annotate the foreground and background regions and fuse such annotation information into the skip connection of a U-net for VOS. Our AGUnet can be trained end-to-end effectively on static images instead of video sequences as required by many previous methods. The experiments show that AGUnet runs much faster than current state-of-the-art one-shot VOS algorithms while achieving competitive accuracy, and it has high generalization capability.","Fully-convolutional Siamese network, U-net, Interactive image segmentation, Video object segmentation",Yingjie Yin and De Xu and Xingang Wang and Lei Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320303836,https://doi.org/10.1016/j.patcog.2020.107580,0031-3203,2021,107580,110,Pattern Recognition,AGUnet: Annotation-guided U-net for fast one-shot video object segmentation,article,YIN2021107580,
"Despite the promising preliminary results, none of existing deep learning based cross-view classification methods simultaneously takes into account both view consistency learning and class-specificity distribution of the extracted features, resulting in unstable classification performance. Moreover, most existing cross-view classification methods are sensitive to scale due to the scale issue of view representations, resulting in unstable view-consistent representations. In this paper, we propose a new deep adversarial network for cross-view classification that attempts to learn robust view-consistent representations by combing the thought of adversarial learning and metric learning in Fisher criterion. Meanwhile, a class-specificity distribution term, which is measured by â12-norm, is employed to make the view-consistent representations with the same label to further have a common distribution in dimension space while view-representations with different labels have different distribution in the intrinsic dimension space. We formulate the aforementioned two concerns into a unified optimization framework. Extensive experiments on several real-world datasets indicate the effectiveness of our method over the other state-of-the-arts.","Cross-view, View consistency, Class-specificity distribution, Adversarial learning",Siyang Deng and Wei Xia and Quanxue Gao and Xinbo Gao,https://www.sciencedirect.com/science/article/pii/S0031320320304362,https://doi.org/10.1016/j.patcog.2020.107633,0031-3203,2021,107633,110,Pattern Recognition,Cross-view classification by joint adversarial learning and class-specificity distribution,article,DENG2021107633,
"In this paper, we aim to address the unsupervised domain adaptation problem where the data in the target domain are much more diverse compared with the data in the source domain. In particular, this problem is formulated as discovering and incorporating latent domains underlying target data of interest for unsupervised domain adaptation. More specifically, the discovery of the latent target domains is based on three criteria, including the maximization of compactness and distinctiveness of the data in the individual latent target-domain, as well as the minimization of total divergence from the latent target-domains to the source domain. For each pair formed by a latent target domain and the source domain, we learn a feature space where the discrepancy between the source domain and the specific latent target domain is shrunk. Finally, we consider the projected source domain data on the learned latent feature spaces as different views of the source domain, and propose an extended multiple kernel learning algorithm to train a more robust and precise classifier for predicting the unlabeled target data. The effectiveness of our proposed method is demonstrated on various benchmark datasets for object recognition and human activity recognition. Moreover, we also show that our proposed method can be treated as an effective complement to the deep learning based unsupervised domain adaptation.","Unsupervised domain adaptation, Latent domain, Multiple kernel learning",Haoliang Li and Wen Li and Shiqi Wang,https://www.sciencedirect.com/science/article/pii/S0031320320303393,https://doi.org/10.1016/j.patcog.2020.107536,0031-3203,2020,107536,108,Pattern Recognition,Discovering and incorporating latent target-domains for domain adaptation,article,LI2020107536,
"Dewarping is a necessary preprocessing step to recognize text from a distorted camera captured document image. According to recent literature, deep learning-based approaches perform with higher accuracy in similar domains. The deep learning-based neural networks are not yet fully explored in the domain of dewarping. To fill this gap, we propose a dewarping approach based on the convolutional neural network. A large number of images are required to train such networks. However, it is a tedious job to capture such a large number of images. Hence, it is required to generate synthetic warped images for the training phase of the deep learning-based neural network. The existing synthetic warped image generation methods are heuristic-based. In this paper, we propose a novel mathematical model for the generation of warped images. The proposed model takes some parameters such as depth of the surface, camera angle, and camera position and generates the corresponding warped image. These parameters are the ground truth for that particular warped image. We use a Convolutional Neural Network (CNN) based model to estimate the warping parameters from a 2D warped image for dewarping. In the training phase of CNN based model, the synthetic images and their corresponding ground truth are used. Next, the trained model is used to dewarp the unknown warped images. The performance of the proposed warping model is analyzed. Finally, the proposed dewarping method is compared with existing approaches. In both cases, the results are encouraging.","Dewarping, Artificial neural netwroks, Synthetic image generation",Arpan Garai and Samit Biswas and Sekhar Mandal,https://www.sciencedirect.com/science/article/pii/S0031320320304246,https://doi.org/10.1016/j.patcog.2020.107621,0031-3203,2021,107621,109,Pattern Recognition,A theoretical justification of warping generation for dewarping using CNN,article,GARAI2021107621,
"Blind image deblurring is a very challenging inverse problem due to the severe ill-posedness caused by the unknown kernel and the latent clear image. To tackle this problem, appropriate smoothing regularizations and image priors are usually employed and incorporated into the associated variational models to alleviate the inherent ill-posedness. In this paper, we first propose a strongly imposed zero patch minimum constraint for the latent image, which helps alleviate the ill-posedness of the inverse problem for blind image deblurring. Then, we retrieve important fine details by assigning the patch minimum information obtained from the blurred image back to the latent image to further enhance its structure. Finally, we introduce an adaptive regularizer which was shown to have significantly better edge-preserving property than the total variation regularizer for the image restoration of degraded images. Operator splitting techniques are used to accomplish an efficient numerical implementation of the proposed variational model. A number of numerical experiments and comparisons with some state-of-the-art methods are conducted to demonstrate the effective performance of the newly proposed method.","Sparsity, Patch minimum information, Blind image deblurring, Kernel estimation, Adaptive regularization, Variational model",Po-Wen Hsieh and Pei-Chiang Shao,https://www.sciencedirect.com/science/article/pii/S0031320320304003,https://doi.org/10.1016/j.patcog.2020.107597,0031-3203,2021,107597,109,Pattern Recognition,Blind image deblurring based on the sparsity of patch minimum information,article,HSIEH2021107597,
"Document images captured in natural scenes with a hand-held camera often suffer from geometric distortions and cluttered backgrounds. In this paper, we propose a simple yet efficient deep model named Adversarial Gated Unwarping Network (AGUN) to rectify these images. In this model, the rectification task is recast as a dense grid prediction problem. We thereby develop a pyramid encoder-decoder architecture to predict the unwarping grid at multiple resolutions in a coarse-to-fine fashion. Based on the observation that the structural visual cues, e.g., text-lines, text blocks, lines in tables, which are critical for the estimation of unwarping mapping, are non-uniformly distributed in the images, three gated modules are introduced to guide the network focusing on these informative cues rather than other interferences such as blank areas and complex backgrounds. To generate more visually pleasing rectification results, we further adopt adversarial training mechanism to implicitly constrain the unwarping grid estimation. Our model can rectify arbitrarily distorted document images with complicated page layouts and cluttered backgrounds. Experiments on the public benchmark dataset and the synthetic dataset demonstrate that our approach outperforms the state-of-the-art methods in terms of OCR accuracy and several widely used quantitative evaluation metrics.","Distorted document image, Geometric rectification, Gated module, Deep learning",Xiyan Liu and Gaofeng Meng and Bin Fan and Shiming Xiang and Chunhong Pan,https://www.sciencedirect.com/science/article/pii/S0031320320303794,https://doi.org/10.1016/j.patcog.2020.107576,0031-3203,2020,107576,108,Pattern Recognition,Geometric rectification of document images using adversarial gated unwarping network,article,LIU2020107576,
"With the improvement of multi-view data collection technology, multi-view learning has become a hot research area. How to deal with diverse and complex data is one of the challenging problems in multi-view learning. However, it is hard for traditional multi-view subspace learning methods to find an effective subspace dimension and deal with outliers simultaneously. In this paper, we propose a novel method, named as Multi-view Subspace Learning via Bidirectional Sparsity(SLBS), which is effective to overcome the above difficulties and learn a better representation. Specifically, we divide the shared subspace into two parts. One is a row sparse matrix to do a secondary extraction of features and the other is a column sparse matrix to reduce the influence of outliers. The proposed model is a non-convex problem which is difficult to be solved. To address this problem, we develop an efficient algorithm and analyze its convergence and computational complexity. Finally, compared with other multi-view subspace learning methods, the extensive experimental results on real-world datasets present the effectiveness of our SLBS.","Multi-view clustering, Subspace learning, Bidirectional sparsity, Non-convex optimization",Ruidong Fan and Tingjin Luo and Wenzhang Zhuge and Sheng Qiang and Chenping Hou,https://www.sciencedirect.com/science/article/pii/S0031320320303277,https://doi.org/10.1016/j.patcog.2020.107524,0031-3203,2020,107524,108,Pattern Recognition,Multi-view subspace learning via bidirectional sparsity,article,FAN2020107524,
"Convolutional neural networks (CNNs) have been successfully applied in many computer vision applications [1], especially in image classification tasks, where most of the structures have been designed manually. With the aid of skip connection and dense connection, the depths of the models are becoming âdeeperâ and the filters of layers are getting âwiderâ in order to tackle the challenge of large-scale datasets. However, large-scale models in convolutional layers become inefficient due to the redundant channels from input feature maps. In this paper, we aim to automatically optimize the topology of the DenseNet, in which unnecessary convolutional kernels are reduced. To achieve this, we present a training pipeline that generates the network structure using a genetic algorithm. We first propose two encoding methods that can represent the structure of the model using a fixed-length binary string. A three-step based evolutionary process consisting of selection, crossover, and mutation is proposed to optimize the structure. We also present a pretrained weight inheritance method which can largely reduce the total time consumption of the genetic process. Experimental results have demonstrated that our proposed model can achieve comparable accuracy to the state-of-the-art models, across a wide range of image recognition and classification datasets, whilst significantly reducing the number of parameters.","Deep convolutional neural networks, Genetic algorithms, Parameter reduction, Structure optimization, DenseNet",Zhenyu Fang and Jinchang Ren and Stephen Marshall and Huimin Zhao and Song Wang and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320320304118,https://doi.org/10.1016/j.patcog.2020.107608,0031-3203,2021,107608,109,Pattern Recognition,Topological optimization of the DenseNet with pretrained-weights inheritance and genetic channel selection,article,FANG2021107608,
"Knowledge-based Visual Question Answering (KVQA) requires external knowledge beyond the visible content to answer questions about an image. This ability is challenging but indispensable to achieve general VQA. One limitation of existing KVQA solutions is that they jointly embed all kinds of information without fine-grained selection, which introduces unexpected noises for reasoning the correct answer. How to capture the question-oriented and information-complementary evidence remains a key challenge to solve the problem. Inspired by the human cognition theory, in this paper, we depict an image by multiple knowledge graphs from the visual, semantic and factual views. Thereinto, the visual graph and semantic graph are regarded as image-conditioned instantiation of the factual graph. On top of these new representations, we re-formulate Knowledge-based Visual Question Answering as a recurrent reasoning process for obtaining complementary evidence from multimodal information. To this end, we decompose the model into a series of memory-based reasoning steps, each performed by a Graph-based Read, Update, and Control (GRUC) module that conducts parallel reasoning over both visual and semantic information. By stacking the modules multiple times, our model performs transitive reasoning and obtains question-oriented concept representations under the constrain of different modalities. Finally, we perform graph neural networks to infer the global-optimal answer by jointly considering all the concepts. We achieve a new state-of-the-art performance on three popular benchmark datasets, including FVQA, Visual7W-KB and OK-VQA, and demonstrate the effectiveness and interpretability of our model with extensive experiments. The source code is available at: https://github.com/astro-zihao/gruc","Cross-modal knowledge reasoning, Multimodal knowledge graphs, Compositional reasoning module, Knowledge-based visual question answering, Explainable reasoning",Jing Yu and Zihao Zhu and Yujing Wang and Weifeng Zhang and Yue Hu and Jianlong Tan,https://www.sciencedirect.com/science/article/pii/S0031320320303666,https://doi.org/10.1016/j.patcog.2020.107563,0031-3203,2020,107563,108,Pattern Recognition,Cross-modal knowledge reasoning for knowledge-based visual question answering,article,YU2020107563,
"In this paper we propose a novel probabilistic framework for document segmentation exploiting human perceptual recognition of text regions from complicated layouts. In particular, we conceptualize text homogeneity as the Gestalt pattern displayed in text regions, characterized by proximately and symmetrically arranged units with similar morphological and texture features. We model this pattern in the local region of a connected component (CC) using an hierarchical formulation, which simulates a random walk-and-check on a graph encoding the neighborhood of the CC. The proposed formulation allows an effective computation of what we call the probabilistic local text homogeneity (PLTH) using a weighted summation of the weights of the graph, which are derived from a probabilistic description of the homogeneity between neighboring CCs and computed through Bayesian cue integration. The proposed PLTH enables a multi-aspect analysis, where various primitives such as geometrical configuration, morphological features, texture characterization and location priors are integrated in one computational probabilistic model. This enables an effective text and non-text classification of CCs preceding any grouping process, which is currently absent in document segmentation. Experimental results show that our segmentation method based on the proposed PLTH model improves upon the state-of-the-art.","Probabilistic local text homogeneity, Random walk-and-check simulation, Bayesian cue integration, Text homogeneity pattern, Document image segmentation",Tan Lu and Ann Dooms,https://www.sciencedirect.com/science/article/pii/S0031320320303940,https://doi.org/10.1016/j.patcog.2020.107591,0031-3203,2021,107591,109,Pattern Recognition,Probabilistic homogeneity for document image segmentation,article,LU2021107591,
"Representation learning methods have attracted considerable attention for learning-based face super-resolution in recent years. Conventional methods perform local models learning on low-resolution (LR) manifold and face reconstruction on high-resolution (HR) manifold respectively, leading to unsatisfactory reconstruction performance when the acquired LR face images are severely degraded (e.g., noisy, blurred). To tackle this issue, this paper proposes an efficient multilayer locality-constrained matrix regression (MLCMR) framework to learn the representation of the input LR patch and meanwhile preserve the manifold of the original HR space. Particularly, MLCMR uses nuclear norm regularization to capture the structural characteristic of the representation residual and applies an adaptive neighborhood selection scheme to find the HR patches that are compatible with its neighbors. Also, MLCMR iteratively applies the manifold structure of the desired HR space to induce the representation weights learning in the LR space, aims at reducing the inconsistency gap between different manifolds. Experimental results on widely used FEI database and real-world faces have demonstrated that compared with several state-of-the-art face super-resolution approaches, our proposed approach has the capability of obtaining better results both in objective metrics and visual quality.","Nuclear norm, Matrix based regression, Face super-resolution, Position-patch",Guangwei Gao and Yi Yu and Jin Xie and Jian Yang and Meng Yang and Jian Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320303423,https://doi.org/10.1016/j.patcog.2020.107539,0031-3203,2021,107539,110,Pattern Recognition,Constructing multilayer locality-constrained matrix regression framework for noise robust face super-resolution,article,GAO2021107539,
"In this paper, we propose a novel method for projecting data from multiple modalities to a new subspace optimized for one-class classification. The proposed method iteratively transforms the data from the original feature space of each modality to a new common feature space along with finding a joint compact description of data coming from all the modalities. For data in each modality, we define a separate transformation to map the data from the corresponding feature space to the new optimized subspace by exploiting the available information from the class of interest only. We also propose different regularization strategies for the proposed method and provide both linear and non-linear formulations. The proposed Multimodal Subspace Support Vector Data Description outperforms all the competing methods using data from a single modality or fusing data from all modalities in four out of five datasets.","Feature transformation, Multimodal data, One-class classification, Support vector data description, Subspace learning",Fahad Sohrab and Jenni Raitoharju and Alexandros Iosifidis and Moncef Gabbouj,https://www.sciencedirect.com/science/article/pii/S0031320320304519,https://doi.org/10.1016/j.patcog.2020.107648,0031-3203,2021,107648,110,Pattern Recognition,Multimodal subspace support vector data description,article,SOHRAB2021107648,
"Because of the large texture and spatial structure discrepancies between face sketches and photos, face sketch recognition becomes a challenging problem in face recognition community. For example, in law enforcement and security, the specific face sketch generation process could introduce some inevitable biases which results in poor face sketch recognition performance. In order to mimic the modality gap introduced by the biases during face sketch creation process, the novel iterative local re-ranking with attribute guided synthesis method is proposed for face sketch recognition, which does not require any extra manually annotation or human interaction. The clues of face attributes are utilized to generate images with varying local characteristic from probe sketches, which could help eliminate the unavoidable biases. Considering the special property of face sketches, the iterative local re-ranking algorithm is designed to encode the contextual information integrated with local invariant discriminative information for matching sketches with photos. Experimental results on multiple face sketch databases demonstrate that the proposed method achieves superior performances compared with state-of-the-art methods.","Face sketch recognition, Facial attribute, Re-ranking",Decheng Liu and Xinbo Gao and Nannan Wang and Chunlei Peng and Jie Li,https://www.sciencedirect.com/science/article/pii/S0031320320303824,https://doi.org/10.1016/j.patcog.2020.107579,0031-3203,2021,107579,109,Pattern Recognition,Iterative local re-ranking with attribute guided synthesis for face sketch recognition,article,LIU2021107579,
"There has been an exponential growth in the amount of visual data on a daily basis acquired from single or multi-view surveillance camera networks. This massive amount of data requires efficient mechanisms such as video summarization to ensure that only significant data are reported and the redundancy is reduced. Multi-view video summarization (MVS) is a less redundant and more concise way of providing information from the video content of all the cameras in the form of either keyframes or video segments. This paper presents an overview of the existing strategies proposed for MVS, including their advantages and drawbacks. Our survey covers the genericsteps in MVS, such as the pre-processing of video data, feature extraction, and post-processing followed by summary generation. We also describe the datasets that are available for the evaluation of MVS. Finally, we examine the major current issues related to MVS and put forward the recommendations for future research1[1]https://github.com/tanveer-hussain/MVS-Survey.","Computer vision, Multi-view video summarization, Multi-sensor management, Multi-camera networks, Machine learning, Features fusion, Big data, Video summarization survey",Tanveer Hussain and Khan Muhammad and Weiping Ding and Jaime Lloret and Sung Wook Baik and Victor Hugo C. {de Albuquerque},https://www.sciencedirect.com/science/article/pii/S0031320320303708,https://doi.org/10.1016/j.patcog.2020.107567,0031-3203,2021,107567,109,Pattern Recognition,A comprehensive survey of multi-view video summarization,article,HUSSAIN2021107567,
"The reconstruction of shredded documents consists of coherently arranging fragments of paper (shreds) to recover the original document(s). A great challenge in computational reconstruction is to properly evaluate the compatibility between the shreds. While traditional pixel-based approaches are not robust to real shredding, more sophisticated solutions compromise significantly time performance. The solution presented in this work extends our previous deep learning method for single-page reconstruction to a more realistic/complex scenario: the reconstruction of several mixed shredded documents at once. In our approach, the compatibility evaluation is modeled as a two-class (valid or invalid) pattern recognition problem. The model is trained in a self-supervised manner on samples extracted from simulated-shredded documents, which obviates manual annotation. Experimental results on three datasets â including a new collection of 100 strip-shredded documents produced for this work â have shown that the proposed method outperforms the competing ones on complex scenarios, achieving accuracy superior to 90%.","Deep learning, Self-supervised learning, Fully convolutional neural networks, Document reconstruction, Forensics, Optimization search",Thiago M. PaixÃ£o and Rodrigo F. Berriel and Maria C.S. Boeres and Alessandro L. Koerich and Claudine Badue and Alberto F. {De Souza} and Thiago Oliveira-Santos,https://www.sciencedirect.com/science/article/pii/S0031320320303381,https://doi.org/10.1016/j.patcog.2020.107535,0031-3203,2020,107535,107,Pattern Recognition,Self-supervised deep reconstruction of mixed strip-shredded text documents,article,PAIXAO2020107535,
"Most existing crowd counting methods require object location-level annotation which is labor-intensive and time-consuming to obtain. In contrast, weaker annotations that only label the total count of objects can be easy to obtain in many practical scenarios. This paper focuses on the problem of weakly-supervised crowd counting which learns a model from a small amount of location-level annotations (fully-supervised) and a large amount of count-level annotations (weakly-supervised). Our study reveals that the most straightforward, that is, directly regressing the integral of density map to the object count, fails to provide satisfactory performance. As an alternative solution, we propose a method by taking advantage of the fact that the total count can be estimated via different-but-equivalent density maps. Our key idea is to enforce the consistency between those density maps and total object count on weakly labeled images as regularization terms. We realize this idea by using multiple density map estimation branches and a carefully devised asymmetry training strategy, called Multiple Auxiliary Tasks Training (MATT). Through extensive experiments on existing datasets and a newly proposed dataset, we validate the effectiveness of the proposed weakly-supervised method and demonstrate its superior performance over existing solutions.","Crowd counting, Count-level annotation, Weak supervision, Auxiliary tasks learning, Asymmetry training",Yinjie Lei and Yan Liu and Pingping Zhang and Lingqiao Liu,https://www.sciencedirect.com/science/article/pii/S0031320320304192,https://doi.org/10.1016/j.patcog.2020.107616,0031-3203,2021,107616,109,Pattern Recognition,Towards using count-level weak supervision for crowd counting,article,LEI2021107616,
"In this work, we address the task of scene text retrieval: given a text query, the system returns all images containing the queried text. The proposed model uses a single shot CNN architecture that predicts bounding boxes and builds a compact representation of spotted words. In this way, this problem can be modeled as a nearest neighbor search of the textual representation of a query over the outputs of the CNN collected from the totality of an image database. Our experiments demonstrate that the proposed model outperforms previous state-of-the-art, while offering a significant increase in processing speed and unmatched expressiveness with samples never seen at training time. Several experiments to assess the generalization capability of the model are conducted in a multilingual dataset, as well as an application of real-time text spotting in videos.","Image retrieval, Scene text detection, Scene text recognition, Word spotting, Convolutional neural networks, Region proposal networks, PHOC",AndrÃ©s Mafla and RubÃ¨n Tito and Sounak Dey and LluÃ­s GÃ³mez and MarÃ§al RusiÃ±ol and Ernest Valveny and Dimosthenis Karatzas,https://www.sciencedirect.com/science/article/pii/S0031320320304593,https://doi.org/10.1016/j.patcog.2020.107656,0031-3203,2021,107656,110,Pattern Recognition,Real-time Lexicon-free Scene Text Retrieval,article,MAFLA2021107656,
"In this paper, we propose a new single shot method for multi-person 3D human pose estimation in complex images. The model jointly learns to locate the human joints in the image, to estimate their 3D coordinates and to group these predictions into full human skeletons. The proposed method deals with a variable number of people and does not need bounding boxes to estimate the 3D poses. It leverages and extends the Stacked Hourglass Network and its multi-scale feature learning to manage multi-person situations. Thus, we exploit a robust 3D human pose formulation to fully describe several 3D human poses even in case of strong occlusions or crops. Then, joint grouping and human pose estimation for an arbitrary number of people are performed using the associative embedding method. Our approach significantly outperforms the state of the art on the challenging CMU Panoptic and a previous single shot method on the MuPoTS-3D dataset. Furthermore, it leads to good results on the complex and synthetic images from the newly proposed JTA Dataset.","Multi-person, 3D, Human pose, Deep learning",Abdallah Benzine and Bertrand Luvison and Quoc Cuong Pham and Catherine Achard,https://www.sciencedirect.com/science/article/pii/S003132032030337X,https://doi.org/10.1016/j.patcog.2020.107534,0031-3203,2021,107534,112,Pattern Recognition,Single-shot 3D multi-person pose estimation in complex images,article,BENZINE2021107534,
"Due to the high cost of manual annotation, learning directly from the web has attracted broad attention. One issue that limits the performance of current webly supervised models is the problem of visual polysemy. In this work, we present a novel framework that resolves visual polysemy by dynamically matching candidate text queries with retrieved images. Specifically, our proposed framework includes three major steps: we first discover and then dynamically select the text queries according to the keyword-based image search results, we employ the proposed saliency-guided deep multi-instance learning (MIL) network to remove outliers and learn classification models for visual disambiguation. Compared to existing methods, our proposed approach can figure out the right visual senses, adapt to dynamic changes in the search results, remove outliers, and jointly learn the classification models. Extensive experiments and ablation studies on CMU-Poly-30 and MIT-ISD datasets demonstrate the effectiveness of our proposed approach.","Visual disambiguation, Image search, Text queries, Web images",Zeren Sun and Yazhou Yao and Jimin Xiao and Lei Zhang and Jian Zhang and Zhenmin Tang,https://www.sciencedirect.com/science/article/pii/S0031320320304234,https://doi.org/10.1016/j.patcog.2020.107620,0031-3203,2021,107620,110,Pattern Recognition,Exploiting textual queries for dynamically visual disambiguation,article,SUN2021107620,
"When the labelling information is not deterministic, traditional supervised learning algorithms cannot be applied. In this case, stochastic supervision models provide a valuable alternative to classification. However, these models are restricted in several aspects, which critically limits their applicability. In this paper, we provide four generalisations of stochastic supervision models, extending them to asymmetric assessments, multiple classes, feature-dependent assessments and multi-modal classes, respectively. Corresponding to these generalisations, we derive four new EM algorithms. We show the effectiveness of our generalisations through illustrative examples of simulated datasets, as well as real-world examples of three famous datasets, the MNIST dataset, the CIFAR-10 dataset and the EMNIST dataset.","EM algorithms, Imperfect supervision, Finite mixture model, Stochastic supervision",Xiaoou Lu and Yangqi Qiao and Rui Zhu and Guijin Wang and Zhanyu Ma and Jing-Hao Xue,https://www.sciencedirect.com/science/article/pii/S0031320320303782,https://doi.org/10.1016/j.patcog.2020.107575,0031-3203,2021,107575,109,Pattern Recognition,Generalisations of stochastic supervision models,article,LU2021107575,
"Hashing methods for cross-modal retrieval has drawn increasing research interests and has been widely studied in recent years due to the explosive growth of multimedia big data. However, a significant phenomenon which has been ignored is that there is a large gap between the results of cross-modal hashing in most cases. For example, the results of Text-to-Image frequently outperform that of Image-to-Text with a large margin. In this paper, we propose a strategy named semantic augmentation to improve and balance the results of cross-modal hashing. An intermediate semantic space is constructed to re-align the feature representations that embedded with weak semantic information. By using the intermediate semantic space, the semantic information of visual features can be further augmented before being sent to cross-modal hashing algorithms. Extensive experiments are carried out on four datasets via seven state-of-the-art cross-modal hashing methods. Compared against the results without semantic augmentation, the Image-to-Text results of these methods with semantic augmentation are improved considerably, which demonstrates the effectiveness of the proposed semantic augmentation strategy in bridging the gap between the results of cross-modal retrieval. Additional experiments are conducted on the real-valued, semi-supervised, semi-paired, partial-paired, and unpaired cross-modal retrieval methods, the results further indicates the effectiveness of our strategy in improving performance of cross-modal retrieval.","Cross-modal hashing, Semantic gap, Semantic augmentation, Cross-modal retrieval",Fangming Zhong and Zhikui Chen and Geyong Min and Feng Xia,https://www.sciencedirect.com/science/article/pii/S0031320320303265,https://doi.org/10.1016/j.patcog.2020.107523,0031-3203,2020,107523,107,Pattern Recognition,A novel strategy to balance the results of cross-modal hashing,article,ZHONG2020107523,
"Recently developed regularization techniques improve the networks generalization by only considering the global context. Therefore, the network tends to focus on a few most discriminative subregions of an image for prediction accuracy, leading the network being sensitive to unseen or noisy data. To address this disadvantage, we introduce the concept of local context mapping by predicting patch-level labels and combine it with a method of local data augmentation by grid-based mixing, called GridMix. Through our analysis of intermediate representations, we show that our GridMix can effectively regularize the network model. Finally, our evaluation results indicate that GridMix outperforms state-of-the-art techniques in classification and adversarial robustness, and it achieves a comparable performance in weakly supervised object localization.","Deep learning, Network regularization, Data augmentation",Kyungjune Baek and Duhyeon Bang and Hyunjung Shim,https://www.sciencedirect.com/science/article/pii/S0031320320303976,https://doi.org/10.1016/j.patcog.2020.107594,0031-3203,2021,107594,109,Pattern Recognition,GridMix: Strong regularization through local context mapping,article,BAEK2021107594,
"The shift from one-hot to distributed representation, popularly referred to as word embedding has changed the landscape of natural language processing (nlp) and information retrieval (ir) communities. In the domain of document images, we have always appreciated the need for learning a holistic word image representation which is popularly used for the task of word spotting. The representations proposed for word spotting is different from word embedding in text since the later captures the semantic aspects of the word which is a crucial ingredient to numerous nlp and ir tasks. In this work, we attempt to encode the notion of semantics into word image representation by bringing the advancements from the textual domain. We propose two novel forms of representations where the first form is designed to be inflection invariant by focusing on the approximate linguistic root of the word, while the second form is built along the lines of recent textual word embedding techniques such as Word2Vec. We observe that such representations are useful for both traditional word spotting and also enrich the search results by accounting the semantic nature of the task. We conduct our experiments on the challenging document images taken from historical-modern collections, handwritten-printed domains, and Latin-Indic scripts. For the purpose of semantic evaluation, we have prepared a large synthetic word image dataset and report interesting results for the standard semantic evaluation metrics such as word analogy and word similarity.","Word image embedding, Word spotting, Semantic spotting",Praveen Krishnan and C.V. Jawahar,https://www.sciencedirect.com/science/article/pii/S0031320320303459,https://doi.org/10.1016/j.patcog.2020.107542,0031-3203,2020,107542,108,Pattern Recognition,Bringing semantics into word image representation,article,KRISHNAN2020107542,
"Deep learning based human brain network classification has gained increasing attention in recent years. However, current methods remain limited in exploring the topological structure information of a brain network. In this paper, we propose a kind of new convolutional kernels with an element-wise weighting mechanism (CKEW) to extract hierarchical topological features of brain networks, in which each weight is assigned to an element with a unique neuroscientific meaning. In addition, a novel classification framework based on CKEW is presented to diagnose brain diseases and explore the most important original features by a tracing feature analysis method efficiently. Experimental results on two autism spectrum disorder (ASD) datasets and an attention deficit hyperactivity disorder (ADHD) dataset with functional magnetic resonance imaging (fMRI) data demonstrate that our method can more accurately distinguish subject groups compared to several state-of-the-art methods in cerebral disease classification, and abnormal connectivity patterns and brain regions identified are more likely to become biomarkers associated with a cerebral disease.","Brain network classification, Convolutional kernels, Element-wise weighting mechanism, Topological features, Abnormal connectivity patterns",Junzhong Ji and Xinying Xing and Yao Yao and Junwei Li and Xiaodan Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320303733,https://doi.org/10.1016/j.patcog.2020.107570,0031-3203,2021,107570,109,Pattern Recognition,Convolutional kernels with an element-wise weighting mechanism for identifying abnormal brain connectivity patterns,article,JI2021107570,
"Subset selection for multiple linear regression aims to construct a regression model that minimizes errors by selecting a small number of explanatory variables. Once a model is built, various statistical tests and diagnostics are conducted to validate the model and to determine whether the regression assumptions are met. Most traditional approaches require human decisions at this step. For example, the user may repeat adding or removing a variable until a satisfactory model is obtained. However, this trial-and-error strategy cannot guarantee that a subset that minimizes the errors while satisfying all regression assumptions will be found. In this paper, we propose a fully automated model building procedure for multiple linear regression subset selection that integrates model building and validation based on mathematical programming. The proposed model minimizes mean squared errors while ensuring that the majority of the important regression assumptions are met. We also propose an efficient constraint to approximate the constraint for the coefficient t-test. When no subset satisfies all of the considered regression assumptions, our model provides an alternative subset that satisfies most of these assumptions. Computational results show that our model yields better solutions (i.e., satisfying more regression assumptions) compared to the state-of-the-art benchmark models while maintaining similar explanatory power.","Regression diagnostics, Subset selection, Mathematical programming",Seokhyun Chung and Young Woong Park and Taesu Cheong,https://www.sciencedirect.com/science/article/pii/S003132032030368X,https://doi.org/10.1016/j.patcog.2020.107565,0031-3203,2020,107565,108,Pattern Recognition,A mathematical programming approach for integrated multiple linear regression subset selection and validation,article,CHUNG2020107565,
"Chinese herbs play a critical role in Traditional Chinese Medicine. Due to different recognition granularity, they can be recognized accurately only by professionals with much experience. It is expected that they can be recognized automatically using new techniques like machine learning. However, there is no Chinese herbal image dataset available. Simultaneously, there is no machine learning method which can deal with Chinese herbal image recognition well. Therefore, this paper begins with building a new standard Chinese-Herbs dataset. Subsequently, a new Attentional Pyramid Networks (APN) for Chinese herbal recognition is proposed, where both novel competitive attention and spatial collaborative attention are proposed and then applied. APN can adaptively model Chinese herbal images with different feature scales. Finally, a new framework for Chinese herbal recognition is proposed as a new application of APN. Experiments are conducted on our constructed dataset and validate the effectiveness of our methods.","Pyramid networks, Attention mechanism, Multi-scale features, Chinese herbal recognition, Chinese herbs image datasets",Yingxue Xu and Guihua Wen and Yang Hu and Mingnan Luo and Dan Dai and Yishan Zhuang and Wendy Hall,https://www.sciencedirect.com/science/article/pii/S0031320320303617,https://doi.org/10.1016/j.patcog.2020.107558,0031-3203,2021,107558,110,Pattern Recognition,Multiple attentional pyramid networks for Chinese herbal recognition,article,XU2021107558,
"In multi-label classification, each instance is associated with a set of pre-specified labels. One common approach is to use Binary Relevance (BR) paradigm to learn each label by a base classifier separately. Use of k-Nearest Neighbor (kNN) as the base classifier (denoted as BRkNN) is a simple, descriptive and powerful approach. In binary relevance a highly imbalanced view of dataset is used. However, kNN is known to perform poorly on imbalanced data. One approach to deal with this is to define the distance function in a parametric form and use the training data to adjust the parameters (i.e. adjusting boundaries between classes) by optimizing a performance measure customized for imbalanced data e.g. F-measure. Prototype Weighting (PW) scheme presented in the literature (Paredes & Vidal, 2006) uses gradient descent to specify the parameters by minimizing the classification error-rate on training data. This paper presents a generalized version of PW. First, instead of minimizing the error-rate proposed in PW, the generalized PW supports also other objective functions that use elements of confusion matrix (including F-measure). Second, PW originally presented for 1NN is extended to the general case of kNN (i.e., k>=1). For problems having highly overlapped classes, it is expected to perform better since a value of k>1 produces smoother decision boundaries which in turn can improve generalization. In multi-label problems with many labels or problems with highly overlapped classes, the proposed generalized PW is expected to significantly improve the performance as it involves many decision boundaries. The performance of the proposed method has been compared with state-of-the-art methods in multi-label classification containing 6 lazy classifiers based on kNN. Experiments show that the proposed method significantly outperforms other methods.","Multi-label classification, Binary relevance, Nearest neighbor, Adaptive distance measure, Prototype weighting",Niloofar Rastin and Mansoor Zolghadri Jahromi and Mohammad Taheri,https://www.sciencedirect.com/science/article/pii/S0031320320303290,https://doi.org/10.1016/j.patcog.2020.107526,0031-3203,2021,107526,114,Pattern Recognition,A generalized weighted distance k-Nearest Neighbor for multi-label problems,article,RASTIN2021107526,
"Emergence of large datasets and resilience of convolutional models have enabled successful training of very large semantic segmentation models. However, high capacity implies high computational complexity and therefore hinders real-time operation. We therefore study compact architectures which aim at high accuracy in spite of modest capacity. We propose a novel semantic segmentation approach based on shared pyramidal representation and fusion of heterogeneous features along the upsampling path. The proposed pyramidal fusion approach is especially effective for dense inference in images with large scale variance due to strong regularization effects induced by feature sharing across the resolution pyramid. Interpretation of the decision process suggests that our approach succeeds by acting as a large ensemble of relatively simple models, as well as due to large receptive range and strong gradient flow towards early layers. Our best model achieves 76.4% mIoU on Cityscapes test and runs in real time on low-power embedded devices.","Semantic segmentation, Real-time inference, Shared resolution pyramid, Computer vision, Deep learning",Marin OrÅ¡iÄ and SiniÅ¡a Å egviÄ,https://www.sciencedirect.com/science/article/pii/S0031320320304143,https://doi.org/10.1016/j.patcog.2020.107611,0031-3203,2021,107611,110,Pattern Recognition,Efficient semantic segmentation with pyramidal fusion,article,ORSIC2021107611,
"In this paper, a novel control-based kernel learning approach is proposed for inferring online binary classification tasks. Following a carefully designed alternating optimization scheme, the learning problems are transformed into two optimal feedback control problems for a series of linear, controllable systems. Model parameters including weights and kernel bandwidth can be efficiently updated by solving the control problems. These consequently lead to our control-based adaptive online kernel classification algorithm (CAOKC). The bandwidth, although nonlinear in our model, can still be updated accurately after linearization. Thus, compared with the existing benchmark algorithms with fixed kernels, the CAOKC algorithm is able to achieve a more adaptive, robust classification performance with better prediction accuracy by regarding the bandwidth as an adjustable parameter. The results presented in this paper also demonstrate how optimal control can provide novel insights and be an effective approach for addressing various learning tasks. Numerical results on benchmark synthetic and realistic datasets are provided to illustrate our method.","Online classification, Kernel learning, Adaptive learning, Adjustable bandwidth, Control-based approach",Jiaming Zhang and Hanwen Ning,https://www.sciencedirect.com/science/article/pii/S0031320320303691,https://doi.org/10.1016/j.patcog.2020.107566,0031-3203,2020,107566,108,Pattern Recognition,Online kernel classification with adjustable bandwidth using control-based learning approach,article,ZHANG2020107566,
"Graphs have become increasingly popular in modeling structures and interactions in a wide variety of problems during the last decade. Graph-based clustering and semi-supervised classification techniques have shown impressive performance. This paper proposes a graph learning framework to preserve both the local and global structure of data. Specifically, our method uses the self-expressiveness of samples to capture the global structure and adaptive neighbor approach to respect the local structure. Furthermore, most existing graph-based methods conduct clustering and semi-supervised classification on the graph learned from the original data matrix, which doesnât have explicit cluster structure, thus they might not achieve the optimal performance. By considering rank constraint, the achieved graph will have exactly c connected components if there are c clusters or classes. As a byproduct of this, graph learning and label inference are jointly and iteratively implemented in a principled way. Theoretically, we show that our model is equivalent to a combination of kernel k-means and k-means methods under certain condition. Extensive experiments on clustering and semi-supervised classification demonstrate that the proposed method outperforms other state-of-the-art methods.","Similarity graph, Rank constraint, Clustering, Semi-supervised classification, Local ang global structure, Kernel method",Zhao Kang and Chong Peng and Qiang Cheng and Xinwang Liu and Xi Peng and Zenglin Xu and Ling Tian,https://www.sciencedirect.com/science/article/pii/S0031320320304301,https://doi.org/10.1016/j.patcog.2020.107627,0031-3203,2021,107627,110,Pattern Recognition,Structured graph learning for clustering and semi-supervised classification,article,KANG2021107627,
"The density peak clustering algorithm treats local density peaks as cluster centers, and groups non-center data points by assuming that one data point and its nearest higher-density neighbor are in the same cluster. While this algorithm is shown to be promising in some applications, its clustering results are found to be sensitive to density kernels, and large density differences across clusters tend to result in wrong cluster centers. In this paper we attribute these problems to the inconsistency between the assumption and implementation adopted in this algorithm. While the assumption is based totally on relative density relationship, this algorithm adopts absolute density as one criterion to identify cluster centers. This observation prompts us to present a cluster center identification criterion based only on relative density relationship. Specifically, we define the concept of subordinate to describe the relative density relationship, and use the number of subordinates as a criterion to identify cluster centers. Our approach makes use of only relative density relationship and is less influenced by density kernels and density differences across clusters. In addition, we discuss the problems of two existing density kernels, and present an average-distance based kernel. In data clustering experiments we validate the new criterion and density kernel respectively, and then test the whole algorithm and compare with some other clustering algorithms.","Density based clustering, Density peak, Cluster center, Relative density relationship",Jian Hou and Aihua Zhang and Naiming Qi,https://www.sciencedirect.com/science/article/pii/S0031320320303575,https://doi.org/10.1016/j.patcog.2020.107554,0031-3203,2020,107554,108,Pattern Recognition,Density peak clustering based on relative density relationship,article,HOU2020107554,
"In various pattern classification problems, semi-supervised learning methods have shown its effectiveness in utilizing unlabeled data to yield better performance than some supervised and unsupervised learning methods. Semi-supervised discriminant embedding (SDE) is a semi-supervised extension of local discriminant embedding (LDE). However, when dealing with high dimensional data, SDE often suffers from the small-sample-size (SSS) problem. In order to settle this problem, an exponential semi-supervised discriminant embedding (ESDE) method was proposed in [F. Dornaika, Y. EI Traboulsi. Matrix exponential based semi-supervised discriminant embedding for image classification, Pattern Recognition, 61 (2017): 92â103], which makes use of the tool of matrix exponential. Despite its high discriminative ability, the computational overhead of ESDE is very large for high dimensional data. In order to cure this drawback, the first contribution of this paper is to propose a fast implementation on the ESDE method. The key is to equivalently transform the large matrix problem of size d into a much smaller one of size n, where d is the data dimension and n is the number of training samples, with dÂ â«Â n in practice. On the other hand, in many real world applications, it is likely that whole labeled training set is unavailable beforehand, and the training data is obtained incrementally. Many incremental semi-supervised learning methods have been proposed to deal with this problem, to the best of our knowledge, however, there are no incremental algorithms for matrix exponential discriminant methods till now. To fill in this gap, the second contribution of this paper is to propose incremental ESDE algorithms for incremental learning problems. Numerical experiments on some real-world data sets show the numerical behavior of the proposed algorithms.","Semi-supervised discriminant embedding (SDE), Local discriminant embedding (LDE), Exponential semi-supervised discriminant embedding (ESDE), Small-sample-size problem (SSS), Incremental algorithm, Dimensionality reduction",Yingdi Lu and Gang Wu,https://www.sciencedirect.com/science/article/pii/S0031320320303332,https://doi.org/10.1016/j.patcog.2020.107530,0031-3203,2020,107530,108,Pattern Recognition,Fast and incremental algorithms for exponential semi-supervised discriminant embedding,article,LU2020107530,
"Existing high-performance object detection methods greatly benefit from the powerful representation ability of deep convolutional neural networks (CNNs). Recent researches show that integration of high-order statistics remarkably improves the representation ability of deep CNNs. However, high-order statistics for object detection lie in two challenges. Firstly, previous methods insert high-order statistics into deep CNNs as global representations, which lose spatial information of inputs, and so are not applicable to object detection. Furthermore, high-order statistics have special structures, which should be considered for proper use of high-order statistics. To overcome above challenges, this paper proposes a Multi-scale Structural Kernel Representation (MSKR) for improving performance of object detection. Our MSKR is developed based on the polynomial kernel approximation, which does not only draw into high-order statistics but also preserve the spatial information of input. To consider geometry structures of high-order representations, a feature power normalization method is introduced before computation of kernel representation. Comparing with the most commonly used first-order statistics in existing CNN-based detectors, our MSKR can generate more discriminative representations, and so be flexibly integrated into deep CNNs for improving performance of object detection. By adopting the proposed MSKR to existing object detection methods (i.e., Faster R-CNN, FPN, Mask R-CNN and RetinaNet), it achieves clear improvement on three widely used benchmarks, while obtaining very competitive performance with state-of-the-art methods.","Object detection, High-order statistics, Polynomial kernel, Matrix power normalization",Hao Wang and Qilong Wang and Peihua Li and Wangmeng Zuo,https://www.sciencedirect.com/science/article/pii/S0031320320303964,https://doi.org/10.1016/j.patcog.2020.107593,0031-3203,2021,107593,110,Pattern Recognition,Multi-scale structural kernel representation for object detection,article,WANG2021107593,
"In this paper, we propose a deep relational network which exploits multi-scale information of facial images for kinship verification. Unlike most existing deep learning based facial kinship verification methods which employ convolutional neural networks to extract holistic features, we present a deep model to exploit facial kinship relationship from local regions. For each given pair of face images, our method uses two convolutional neural networks which share parameters to extract different scales of features, which are expected to provide global contextual information of face images. We split a set of features at the same scale into multiple groups, where different groups capture information of different local regions. For each pair of local feature groups which are extracted from the same scale and position, we propose a relation network to reason their relationship, and use a verification network to infer the kin relation based on the results of local relations from different facial regions. We conduct experiments on two widely used facial kinship datasets: KinFaceW-I and KinFaceW-II, and our experimental results are presented to demonstrate the effectiveness of our approach.","Facial analysis, Kinship verification, Biometrics, Feature learning",Haibin Yan and Chaohui Song,https://www.sciencedirect.com/science/article/pii/S0031320320303447,https://doi.org/10.1016/j.patcog.2020.107541,0031-3203,2021,107541,110,Pattern Recognition,Multi-scale deep relational reasoning for facial kinship verification,article,YAN2021107541,
"Deep learning has received increasing attention in the last decade. Its amazing success, is partly attributed to the evolution of normalization and activation techniques. However, less works have devoted to explore both modules together. This work, therefore, aims at pushing for a deeper understanding on the effect of normalization and activation together analytically. We design a generic method which integrates both normalization and activation together as a whole, named as the Generic Shift-Normalization-Activation Approach (GSNA), in reserving richer information propagation in neural networks. A rigorous mathematical analysis was performed to investigate the benefits of the designed method, such as its computation complexity, performance potential as well as optimization over trainable parameter initialization. Further, extensive experiments are conducted to demonstrate the superiority and generality of the designed method in many computer vision benchmarking tasks, such as CIFAR-10/100, SVHN, ImageNet32Â ÃÂ 32, etc. To explore its generality, we also conduct some experiments on natural language understanding tasks like text classification, natural language inference, and some variational generative task as well. More interestingly, GSNA can be naturally incorporated into the existing neural networks with arbitrary architectures, demonstrating its generic effectiveness in deep learning field.","Activation, Normalization, CNN, Shifting, Deep learning",Zhi Chen and Pin-Han Ho,https://www.sciencedirect.com/science/article/pii/S003132032030412X,https://doi.org/10.1016/j.patcog.2020.107609,0031-3203,2021,107609,109,Pattern Recognition,A generic shift-norm-activation approach for deep learning,article,CHEN2021107609,
"Semi-supervised learning is important and has become more widespread because obtaining labeled data is expensive and labor-intensive. In this paper, we focus on the challenging semi-supervised person Re-identification (ReID) task, which is a metric learning problem based on the assumption that unlabeled data is open-set. To address this problem, we propose the Transductive Semi-Supervised Metric Learning (TSSML) framework. In TSSML, we propose a graph-based transductive hard mining method for deeply mining hard triplets in unlabeled data and a degree-based relationship confidence scoring method for further reducing incorrect triplets. Moreover, we investigate the feature consistency loss (FCL) and adopt the curriculum learning strategy to improve the representation learning for semi-supervised ReID. Extensive experiments have been conducted on three large-scale ReID datasets and demonstrate the effectiveness of our TSSML framework.","Person re-identification, Transductive learning, Semi-supervised learning, Graph, Confidence score",Xinyuan Chang and Zhiheng Ma and Xing Wei and Xiaopeng Hong and Yihong Gong,https://www.sciencedirect.com/science/article/pii/S0031320320303721,https://doi.org/10.1016/j.patcog.2020.107569,0031-3203,2020,107569,108,Pattern Recognition,Transductive semi-supervised metric learning for person re-identification,article,CHANG2020107569,
"In this paper, we propose a dual subspace discriminative projection learning (DSDPL) framework for multi-category image classification. Our approach reflects the notion that images are composed of class-shared information, class-specific information, and sparse noise. Unlike traditional subspace learning methods, DSDPL serves to decompose original high dimensional data, via learned projection matrices, into class-shared and class-specific subspaces. The learned projection matrices are jointly constrained with l2,1 sparse norm and LDA terms while the reconstructive properties of DSDPL reduce information loss, leading to greater stability within low dimensional subspaces. Regression-based terms are also included to facilitate a more robust classification approach, using extracted class-specific features for better classification. Our approach is examined on five different datasets for face, object and scene classifications. Experimental results demonstrate not only the superiority and versatility of DSDPL over current benchmark approaches, but also a more robust classification approach with low sample size training data.","Pattern recognition, Feature extraction, Subspace learning, Image classification, Subspace discriminative projection",Gregg Belous and Andrew Busch and Yongsheng Gao,https://www.sciencedirect.com/science/article/pii/S0031320320303848,https://doi.org/10.1016/j.patcog.2020.107581,0031-3203,2021,107581,111,Pattern Recognition,Dual subspace discriminative projection learning,article,BELOUS2021107581,
"Per-Pixel Cross entropy (PPCE) is a commonly used loss on semantic segmentation tasks. However, it suffers from a number of drawbacks. Firstly, PPCE only depends on the probability of the ground truth class since the latter is usually one-hot encoded. Secondly, PPCE treats all pixels independently and does not take the local structure into account. While perceptual losses (e.g. matching prediction and ground truth in the embedding space of a pre-trained VGG network) would theoretically address these concerns, it does not constitute a practical solution as segmentation masks follow a distribution that differs largely from natural images. In this paper, we introduce a SEMantic EDge-Aware strategy (SEMEDA) to solve these issues. Inspired by perceptual losses, we propose to match the âprobability textureâ of predicted segmentation mask and ground truth through a proxy network trained for semantic edge detection on the ground truth masks. Through thorough experimental validation on several datasets, we show that SEMEDA steadily improves the segmentation accuracy with negligible computational overhead and can be added with any popular segmentation networks in an end-to-end training framework.","Semantic segmentation, Loss function, Computer vision",Yifu Chen and Arnaud Dapogny and Matthieu Cord,https://www.sciencedirect.com/science/article/pii/S0031320320303605,https://doi.org/10.1016/j.patcog.2020.107557,0031-3203,2020,107557,108,Pattern Recognition,SEMEDA: Enhancing segmentation precision with semantic edge aware loss,article,CHEN2020107557,
"Person re-identificationÂ (re-ID) focuses on matching the same person across non-overlapping camera views. Most existing methods require tedious manual annotation and can only learn a unitary transformation for images across views, which severely lack of scalability and suffer from view-specific biases. To address these issues, we put forward a View-Specific Semi-supervised Subspace LearningÂ (VS-SSL) approach that can learn specific projections for each view, utilizing limited labeled data to guide the training while leveraging abundant unlabeled data simultaneously. Moreover, a novel re-ranking strategy is proposed to boost the performance further, which re-estimates the similarity between probe and galleries according to the overlap ratio between their expanded neighbors and their position in each otherâs ranking list. The effectiveness of the proposed framework is evaluated on several widely-used datasetsÂ (VIPeR, PRID450S, PRID2011, CUHK01 and Market-1501), yielding superior performance for both semi-supervised and supervised re-ID.","Person re-identification, Metric learning, Semi-supervised learning, Re-ranking",Jieru Jia and Qiuqi Ruan and Yi Jin and Gaoyun An and Shiming Ge,https://www.sciencedirect.com/science/article/pii/S003132032030371X,https://doi.org/10.1016/j.patcog.2020.107568,0031-3203,2020,107568,108,Pattern Recognition,View-specific subspace learning and re-ranking for semi-supervised person re-identification,article,JIA2020107568,
"Recent works have shown that convolutional neural networks (CNNs) are parameter redundant, which limits the application of CNNs in Mobile devices with limited memory and computational resources. In this paper, two novel and efficient lightweight CNNs architectures are proposed, which are called DenseDsc and Dense2Net. Two proposed CNNs are densely connected and the dense connectivity facilitates feature re-use in the networks. Dense2Net adopts efficient group convolution and DenseDsc adopts more efficient depthwise separable convolution. The novel dense blocks of DenseDsc and Dense2Net improve the parameter efficiency. The proposed DenseDsc and Dense2Net are evaluated on highly competitive classification benchmark datasets (CIFAR and ImageNet). The experimental results show that DenseDsc and Dense2Net have higher accuracy than DenseNet with similar parameters or FLOPs. Compared with other efficient CNNs with less than 0.5Â M parameters for CIFAR, Dense2Net and DenseDsc achieve state-of-the-art results on CIFAR-10 and CIFAR-100, respectively. DenseDsc and Dense2Net are very competitive in efficient CNNs with less than 1.0Â M parameters on CIFAR. Furthermore, Dense2Net achieves state-of-the-art results on ImageNet in manual CNNs with less than 10Â M parameters.","Convolutional neural networks, Classification, Parameter efficiency, Densely connected",Guoqing Li and Meng Zhang and Jiaojie Li and Feng Lv and Guodong Tong,https://www.sciencedirect.com/science/article/pii/S0031320320304131,https://doi.org/10.1016/j.patcog.2020.107610,0031-3203,2021,107610,109,Pattern Recognition,Efficient densely connected convolutional neural networks,article,LI2021107610,
"Atrous Spatial Pyramid Pooling (ASPP) is a module that can collect semantic information distributed in different scopes. However, because of the limited number of sampling ranges of ASPP, much valuable global features and contextual information cannot be sufficiently sampled, which degrades the representation ability of the segmentation network. Besides, due to the sparse distribution of the effective sampling points in the atrous convolution kernels of ASPP, large amount of local detail characteristics are easily discarded. To overcome the above two problems, a new Cascaded Hierarchical Atrous Pyramid Pooling (CHASPP) module, consisting of two cascaded components, is proposed. Each component is a hierarchical pyramid pooling structure containing two layers of atrous convolutions with the aim to densify the sampling distribution. On the foundation of such a hierarchical structure, another same structure is appended to form a cascaded module which can further enlarge the diversity of sampling ranges. Based on this cascaded module, not only rich local detail characteristics can be comprehensively presented, but also important global contextual information can be effectively exploited to improve the prediction accuracy. To demonstrate the performance of our CHASPP module, experiments on the benchmarks PASCAL VOC 2012 and Cityscape are conducted.","Semantic segmentation, Atrous convolution, Atrous spatial pyramid pooling(ASPP), Hierarchical pyramid pooling, Cascaded module",Xuhang Lian and Yanwei Pang and Jungong Han and Jing Pan,https://www.sciencedirect.com/science/article/pii/S0031320320304258,https://doi.org/10.1016/j.patcog.2020.107622,0031-3203,2021,107622,110,Pattern Recognition,Cascaded hierarchical atrous spatial pyramid pooling module for semantic segmentation,article,LIAN2021107622,
"Change detection of heterogeneous remote sensing images is an important and challenging topic, which has found a wide range of applications in many fields, especially in the emergency situation resulting from nature disaster. However, the difference in imaging mechanism of heterogeneous sensors makes it difficult to carry out a direct comparison of images. In this paper, we propose a new change detection method based on similarity measurement between heterogeneous images. The method constructs a graph for each patch based on the nonlocal patch similarity to establish a connection between heterogeneous data, and then measures the change level by measuring how much the graph structure of one image still conforms to that of the other image. The graph structures are compared in the same domain, so it can avoid the leakage of heterogeneous data and bring more robust change detection results. Experiments demonstrate the effective performance of the proposed nonlocal patch similarity based heterogeneous change detection method.","Unsupervised change detection, Heterogeneous data, Nonlocal similarity, Graph",Yuli Sun and Lin Lei and Xiao Li and Hao Sun and Gangyao Kuang,https://www.sciencedirect.com/science/article/pii/S0031320320304015,https://doi.org/10.1016/j.patcog.2020.107598,0031-3203,2021,107598,109,Pattern Recognition,Nonlocal patch similarity based heterogeneous remote sensing change detection,article,SUN2021107598,
"This paper deals with supervised classification of multivariate time series. In particular, the goal is to propose a filter method to select a subset of time series. Consequently, we adopt the framework proposed by Brown al. [1]. The key point in this framework is the computation of the mutual information between the features, which allows us to measure the relevance of each feature subset. In our case, where the features are a time series, we use an adaptation of existing nonparametric mutual information estimators based on the k-nearest neighbor. Specifically, for the purpose of bringing these methods to the time series scenario, we rely on the use of dynamic time warping dissimilarity. Our experimental results show that our method is able to strongly reduce the number of time series while keeping or increasing the classification accuracy.","Multivariate time series, Supervised classification, Feature susbset selection, Mutual information",Josu Ircio and Aizea Lojo and Usue Mori and Jose A. Lozano,https://www.sciencedirect.com/science/article/pii/S0031320320303289,https://doi.org/10.1016/j.patcog.2020.107525,0031-3203,2020,107525,108,Pattern Recognition,Mutual information based feature subset selection in multivariate time series classification,article,IRCIO2020107525,
"The two-stage strategy has been widely used in image classification. However, these methods barely take the classification criteria of the first stage into consideration in the second prediction stage. In this paper, we propose a novel Two-Stage Representation method (TSR), and convert it to a Single-Teacher Single-Student (STSS) problem in our two-stage knowledge transfer framework for image classification. Specifically, the first stage classifier is formulated as the teacher, which holds the âgate valueâ to supervise the student classifier in the second stage. To transfer knowledge from the teacher classifier, we seek the nearest neighbours of the test sample to generate a set of candidate target classes in the first stage. Then, a student classifier learns from the samples belonging to these candidate classes in the second stage. Under the supervision of the teacher classifier, the teacher approves the student only if it obtains a higher score than the âgate valueâ. In actuality, the proposed framework generates a stronger classifier by staging two weaker classifiers in a novel way. The experiments on several databases show that our proposed framework is effective, which outperforms multiple popular classification methods.","Image classification, Teacher-student model, Two-stage classification, Sparse representation",Jianhang Zhou and Shaoning Zeng and Bob Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320303320,https://doi.org/10.1016/j.patcog.2020.107529,0031-3203,2020,107529,107,Pattern Recognition,Two-stage knowledge transfer framework for image classification,article,ZHOU2020107529,
"Feature Selection (FS) plays an important role in learning and classification tasks. Its objective is to select the relevant and non-redundant features. Considering the huge number of features in real-world applications, FS methods using batch learning technique cannot resolve big data problems especially when data arrive sequentially. In this paper, we proposed an online feature selection system which resolves this problem. The proposed OFS system called MOANOFS (Multi-Objective Automated Negotiation based Online Feature Selection) explore the recent advances of online machine learning techniques and a conflict resolution technique (Automated Negotiation) for the purpose of enhancing the classification performance of ultra-high dimensional databases. MOANOFS uses two decision levels. In the first level, we decided which k(s) among the learners (or OFS methods) are the trustful ones (with high confidence or trust value). These elected k learners would participate in the second level where we integrated our proposed Multilateral Automated Negotiation based OFS (MANOFS) method. This would enable us to finally decide which features are the most relevant. We showed that MOANOFS system achieves high accuracy with several real text classification datasets as 20Newsgroups, RCV1.","Feature selection, Online learning, Multi-objective automated negotiation, Trust, Classification, Big data",Fatma BenSaid and Adel M. Alimi,https://www.sciencedirect.com/science/article/pii/S0031320320304325,https://doi.org/10.1016/j.patcog.2020.107629,0031-3203,2021,107629,110,Pattern Recognition,Online feature selection system for big data classification based on multi-objective automated negotiation,article,BENSAID2021107629,
"Keystroke dynamics-based authentication (KDA) is one of the human behavioral biometric-based user authentication methods based on the unique typing pattern of a person. Previous KDA studies on mobile devices primarily focused on fixed-length text-based KDA, such as passwords and personal identification numbers. This can strengthen the login system and prevent abnormal usage of impostors based on certain attack methods, such as shoulder surfing and smudge attacks. However, this method possesses a limitation that continuous monitoring is not possible after login. To solve this problem, KDA based on freely typed text was studied; however, there are only a few studies on this technique. Further, the performance authentication based on these studies is insufficient for a real-world implementation. In this paper, we propose a novel freely typed text-based KDA method for mobile devices named FACT, i.e., user authentication on mobile devices based on free text, accelerator, coordinate, and time. We collected data from three different smartphone sensors while typing in two languages (English and Korean), and 17 variables were extracted for a set of keystroke data. A total of six authentication methods were employed and the proposed FACT yielded an equal error rate lower than 1% with only one reference keystroke set; moreover, it demonstrated a perfect protection capability while using Korean when more than four reference keystroke sets were used. To contribute to the research and industrial community, we have publicized our collected keystroke dataset so that anyone who conducts a KDA study or develops a KDA-related mobile service can use the dataset without any restrictions.","Mobile user authentication, Keystroke dynamics, Freely typed text, Anomaly detection, Heterogeneous data fusion",Junhong Kim and Pilsung Kang,https://www.sciencedirect.com/science/article/pii/S0031320320303599,https://doi.org/10.1016/j.patcog.2020.107556,0031-3203,2020,107556,108,Pattern Recognition,Freely typed keystroke dynamics-based user authentication for mobile devices based on heterogeneous features,article,KIM2020107556,
"Due to the extensive practical value of time series prediction, many excellent algorithms have been proposed. Most of these methods are developed assuming that massive labeled training data are available. However, this assumption might be invalid in some actual situations. To address this limitation, a transfer learning framework with deep architectures is proposed. Since convolutional neural network (CNN) owns favorable feature extraction capability and can implement parallelization more easily, we propose a deep transfer learning method resorting to the architecture of CNN, termed as DTr-CNN for short. It can effectively alleviate the available labeled data absence and leverage useful knowledge to the current prediction. Notably, in our method, transfer learning process is implemented across different datasets. For a given target domain, in real-world scenarios, relativity of truly available potential source datasets may not be obvious, which is challenging and rarely referred to in most existing time series prediction methods. Aiming at this problem, the incorporation of Dynamic Time Warping (DTW) and Jensen-Shannon (JS) divergence is adopted for the selection of the appropriate source domain. Effectiveness of the proposed method is empirically underpinned by the experiments conducted on one group of synthetic and two groups of practical datasets. Besides, an additional experiment on NN5 dataset is conducted.","Time series prediction, Deep learning, Transfer learning, Convolutional neural network (CNN)",Rui Ye and Qun Dai,https://www.sciencedirect.com/science/article/pii/S0031320320304209,https://doi.org/10.1016/j.patcog.2020.107617,0031-3203,2021,107617,109,Pattern Recognition,Implementing transfer learning across different datasets for time series forecasting,article,YE2021107617,
"Recurrence quantification analysis (RQA) is an acknowledged method for the characterization of experimental time series. We propose a parametric version of RQA, pRQA, allowing a fast processing of spatial arrays of time series, once each is modeled by an autoregressive stochastic process. This method relies on the analytical derivation of asymptotic expressions for five current RQA measures as a function of the model parameters. By avoiding the construction of the recurrence plot of the time series, pRQA is computationally efficient. As a proof of principle, we apply pRQA to pattern recognition in multichannel electroencephalographic (EEG) data from a patient with a brain tumor.","Recurrence plots, Recurrence quantification analysis, Autoregressive stochastic processes, Asymptotic recurrence measures, Multichannel data, EEG Data",Sofiane Ramdani and Anthony Boyer and StÃ©phane Caron and FranÃ§ois Bonnetblanc and FrÃ©dÃ©ric Bouchara and Hugues Duffau and Annick Lesne,https://www.sciencedirect.com/science/article/pii/S0031320320303757,https://doi.org/10.1016/j.patcog.2020.107572,0031-3203,2021,107572,109,Pattern Recognition,Parametric recurrence quantification analysis of autoregressive processes for pattern recognition in multichannel electroencephalographic data,article,RAMDANI2021107572,
"Circle detection is a critical issue in pattern recognition and image analysis. Conventional geometry-based methods such as tangent or symmetry are sensitive to noise or occlusion. Area computation is more robust against noise, because it avoids differential calculations. Inspired by this characteristic, we present a novel method for fast circle detection using inscribed triangles. The proposed algorithm, which is robust to noise and resistant to occlusion, first extracts circular arcs by approximating line segments and identifying inflection points and sharp corners. To speed up the computation, irrelevant segments are filtered out through the triangle inequality. Arcs that belong to the same circle are then combined according to the position constraint and the inscribed triangle constraint. The circle parameters are further estimated by inscribed triangles based upon the Theil-Sen estimator and linear error refinement without the dependence of least-square fitting but still with the equivalent accuracy. Finally, candidate circles are verified to prune false positives through an inlier ratio rule, which jointly considers both distance and angle deviations. Extensive experiments are conducted on synthetic images including overlapping circles, and real images from four diverse datasets (three publicly available and one we built). Results are compared with those of representative state-of-the-art methods, and the proposed method is demonstrated to embraces several advantages: resistant to occlusion, more robust to noise, and better performance and efficiency.","Circle detection, Inscribed triangle, Parameter estimation, Hough transform",Mingyang Zhao and Xiaohong Jia and Dong-Ming Yan,https://www.sciencedirect.com/science/article/pii/S0031320320303915,https://doi.org/10.1016/j.patcog.2020.107588,0031-3203,2021,107588,109,Pattern Recognition,An occlusion-resistant circle detector using inscribed triangles,article,ZHAO2021107588,
"The K-means clustering algorithm is well-known for its easy computational approach. In this algorithm, essential cluster-level information is captured by the K cluster centroids. However, how many such centroids can reveal the structure of the underlying data depends upon the choice of K. In this paper, we propose a clustering algorithm in which the number of cluster K can be learned as well as it performs the clustering. Our work revolves around two observations: i) a large-sized random sampled dataset may have a similar distribution as the original data, and ii) for the true number of clusters their centroids, generated from a sampled datasets, approximate the cluster centroids generated from the original dataset. The first observation has paved the way to provide a scalable solution, and the second one forms the key aspect of building the proposed algorithm. We have tested our method on several real and synthetic datasets. Our method can solve a few pertinent issues of clustering a dataset: 1) detection of a single cluster in the absence of any other cluster in a dataset, 2) the presence of hierarchy, 3) clustering of a high dimensional dataset, 4) robustness over dataset having cluster imbalance, and 5) robustness to noise. We have observed significant improvement in speed and quality for predicting cluster numbers as well as the composition of clusters in a large dataset.","clustering, Bipartite graph, Perfect matching,  algorithm, Stability",Jayasree Saha and Jayanta Mukherjee,https://www.sciencedirect.com/science/article/pii/S0031320320304283,https://doi.org/10.1016/j.patcog.2020.107625,0031-3203,2021,107625,110,Pattern Recognition,CNAK: Cluster number assisted K-means,article,SAHA2021107625,
"Recently, situation recognition as a new challenging task for image understanding has gained great attention, which needs to simultaneously predict the main activity (verb) and its associated objects (noun entities) in a structured and detailed way. Several methods have been proposed to handle this task, but usually they cannot effectively model the relationships between the activity and the objects. In this paper, we propose a Relational Graph Neural Network (RGNN) for situation recognition, which builds a neural graph on the activity and the objects, and models the triplet relationships between the activity and pairs of objects through message passing between graph nodes. Moreover, we propose a two-stage training strategy to optimize the model. A progressive supervised learning is first adopted to obtain an initial prediction for the activity and the objects. Then, the initial predictions are refined by using a policy-gradient method to directly optimize the non-differentiable value-all metric. To verify the effectiveness of our method, we perform extensive experiments on the Imsitu dataset which is currently the only available dataset for situation recognition. Experimental results show that our approach outperforms the state-of-the-art methods on verb and value metrics, and demonstrates better relationships between the activity and the objects.","Situation recognition, Relationship modeling, Graph neural network, Reinforcement learning",Ya Jing and Junbo Wang and Wei Wang and Liang Wang and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S0031320320303472,https://doi.org/10.1016/j.patcog.2020.107544,0031-3203,2020,107544,108,Pattern Recognition,Relational graph neural network for situation recognition,article,JING2020107544,
"Despite the remarkable success of deep neural networks on many visual tasks, they have been proved to be vulnerable to adversarial examples. For visual tasks, adversarial examples are images added with visually imperceptible perturbations that result in failure for recognition. Previous works have demonstrated that adversarial perturbations can cause neural networks to fail on object detection. But these methods focus on generating an adversarial perturbation for a specific image, which is the image-specific perturbation. This paper tries to extend such image-level adversarial perturbations to detector-level, which are universal (image-agnostic) adversarial perturbations. Motivated by this, we propose a Universal Dense Object Suppression (U-DOS) algorithm to derive the universal adversarial perturbations against object detection and show that such perturbations with visual imperceptibility can lead the state-of-the-art detectors to fail in finding any objects in most images. Compared to image-specific perturbations, the results of image-agnostic perturbations are more interesting and also pose more challenges in AI security, because they are more convenient to be applied in the real physical world. We also analyze the generalization of such universal adversarial perturbations across different detectors and datasets under the black-box attack settings, showing itâs a simple but promising adversarial attack approach against object detection. Furthermore, we validate the class-specific universal perturbations, which can remove the detection results of the target class and keep others unchanged.","Adversarial examples, Object detection, Universal adversarial perturbation",Debang Li and Junge Zhang and Kaiqi Huang,https://www.sciencedirect.com/science/article/pii/S0031320320303873,https://doi.org/10.1016/j.patcog.2020.107584,0031-3203,2021,107584,110,Pattern Recognition,Universal adversarial perturbations against object detection,article,LI2021107584,
"We investigate the intersection between hierarchical and superpixel image segmentation. Two strategies are considered: (i) the classical region merging, that creates a dense hierarchy with a higher number of levels, and (ii) the recursive execution of some superpixel algorithm, which generates a sparse hierarchy with fewer levels. We show that, while dense methods can capture more intermediate or higher-level object information, sparse methods are considerably faster and usually with higher boundary adherence at finer levels. We first formalize the two strategies and present a sparse method, which is faster than its superpixel algorithm and with similar boundary adherence. We then propose a new dense method to be used as post-processing from the intermediate level, as obtained by our sparse method, upwards. This combination results in a unique strategy and the most effective hierarchical segmentation method among the compared state-of-the-art approaches, with efficiency comparable to the fastest superpixel algorithms.","Superpixel segmentation, Hierarchical image segmentation, Image foresting transform, Iterative spanning forest, Graph-based image segmentation, Irregular image pyramid",Felipe Lemes GalvÃ£o and Silvio Jamil Ferzoli GuimarÃ£es and Alexandre Xavier FalcÃ£o,https://www.sciencedirect.com/science/article/pii/S0031320320303356,https://doi.org/10.1016/j.patcog.2020.107532,0031-3203,2020,107532,108,Pattern Recognition,Image segmentation using dense and sparse hierarchies of superpixels,article,GALVAO2020107532,
"Bipartite spectral graph partition (BSGP) is a school of the most well-known algorithms designed for the bipartite graph partition problem. It is also a fundamental mathematical model widely used in the tasks of co-clustering and fast spectral clustering. In BSGP, the key is to find the minimal normalized cuts (Ncuts) of bipartite graph. However, the convolutional BSGP algorithms usually need to use the singular value decomposition (SVD) to find the minimal Ncuts, which is computational prohibitive. Under this circumstance, the application range of those methods would be limited when the volume of the dataset is huge or the dimension of features is high. To overcome this problem, this paper proposes a novel weighted bilateral k-means (WBKM) algorithm and applies it for co-clustering and fast spectral clustering. Specifically, WBKM is a relaxation of the problem of finding the minimal Ncuts of bipartite graph, so it can be seen as a new solution for the minimal-Ncuts problem in bipartite graph. Different from the conventional relaxation ways, WBKM relaxes the minimal-Ncuts problem to a Non-negative decomposition problem which can be solved by an efficient iterative method. Therefore, the running speed of the proposed method is much faster. Besides, as our model can directly output the clustering results without any help of post-procedures, its solution tends to be more close to the ideal solution of the minimal-Ncuts problem than that of the conventional BSGP algorithms. To demonstrate the effectiveness and efficiency of the proposed method, extensive experiments on various types of datasets are conducted. Compared with other state-of-the-art methods, the proposed WBKM not only has faster computational speed, but also achieves more promising clustering results.","Fast co-clustering, Clustering, Normalized cuts, Weighted bilateral -means",Kun Song and Xiwen Yao and Feiping Nie and Xuelong Li and Mingliang Xu,https://www.sciencedirect.com/science/article/pii/S0031320320303630,https://doi.org/10.1016/j.patcog.2020.107560,0031-3203,2021,107560,109,Pattern Recognition,Weighted bilateral K-means algorithm for fast co-clustering and fast spectral clustering,article,SONG2021107560,
"The COVID-19 outbreak continues to threaten the health and life of people worldwide. It is an immediate priority to develop and test a computer-aided detection (CAD) scheme based on deep learning (DL) to automatically localize and differentiate COVID-19 from community-acquired pneumonia (CAP) on chest X-rays. Therefore, this study aims to develop and test an efficient and accurate deep learning scheme that assists radiologists in automatically recognizing and localizing COVID-19. A retrospective chest X-ray image dataset was collected from open image data and the Xiangya Hospital, which was divided into a training group and a testing group. The proposed CAD framework is composed of two steps with DLs: the Discrimination-DL and the Localization-DL. The first DL was developed to extract lung features from chest X-ray radiographs for COVID-19 discrimination and trained using 3548 chest X-ray radiographs. The second DL was trained with 406-pixel patches and applied to the recognized X-ray radiographs to localize and assign them into the left lung, right lung or bipulmonary. X-ray radiographs of CAP and healthy controls were enrolled to evaluate the robustness of the model. Compared to the radiologistsâ discrimination and localization results, the accuracy of COVID-19 discrimination using the Discrimination-DL yielded 98.71%, while the accuracy of localization using the Localization-DL was 93.03%. This work represents the feasibility of using a novel deep learning-based CAD scheme to efficiently and accurately distinguish COVID-19 from CAP and detect localization with high accuracy and agreement with radiologists.","COVID-19, Computer-aided detection (CAD), Community-acquired pneumonia (CAP), Deep learning (DL), Chest X-ray (CXR)",Zheng Wang and Ying Xiao and Yong Li and Jie Zhang and Fanggen Lu and Muzhou Hou and Xiaowei Liu,https://www.sciencedirect.com/science/article/pii/S0031320320304167,https://doi.org/10.1016/j.patcog.2020.107613,0031-3203,2021,107613,110,Pattern Recognition,Automatically discriminating and localizing COVID-19 from community-acquired pneumonia on chest X-rays,article,WANG2021107613,
"Large-scale data with human annotations is of crucial importance for training deep convolutional neural network (DCNN) to ensure stable and reliable performance. However, accurate annotations, such as bounding box and pixel-level annotations, demand expensive labeling efforts, which has prevented wide application of DCNN in industries. Focusing on the problem of surface defect detection, this paper proposes a weakly supervised learning method named Category-Aware object Detection network (CADN) to tackle the dilemma. CADN is trained with image tag annotations only and performs image classification and defect localization simultaneously. The weakly supervised learning is achieved by extracting category-aware spatial information in a classification pipeline. CADN could be equipped with either a lighter or a larger backbone network as the feature extractor resulting in better real-time performance or higher accuracy. To address the two conflicting objectives simultaneously, both of which are significant concerns in industrial applications, knowledge distillation strategy is adopted to force the learned features of a lighter CADN to mimic that of a larger CADN. Accordingly, the accuracy of the lighter CADN is improved while high real-time performance is maintained. The proposed approach is verified on our own defect dataset as well as on an open-source defect dataset. As demonstrated, satisfied performance is achieved by the proposed method, which could meet industrial requirements completely. Meanwhile, the method minimizes human efforts involved in image labelling, thus promoting the applications of DCNN in industries.","Weakly supervised learning, Automated surface inspection, Defect detection, Knowledge distillation",Jiabin Zhang and Hu Su and Wei Zou and Xinyi Gong and Zhengtao Zhang and Fei Shen,https://www.sciencedirect.com/science/article/pii/S0031320320303745,https://doi.org/10.1016/j.patcog.2020.107571,0031-3203,2021,107571,109,Pattern Recognition,CADN: A weakly supervised learning-based category-aware object detection network for surface defect detection,article,ZHANG2021107571,
"Data heterogeneity such as task heterogeneity, view heterogeneity, and instance heterogeneity often co-exist in many real-world applications including insider threat detection, traffic prediction, brain image analysis, quality control in manufacturing processes, etc. However, most of the existing techniques might not take fully advantage of the rich heterogeneity. To address this problem, we propose a novel graph-based approach named M3 to simultaneously model triple heterogeneity in a principled framework. The main idea is to employ the hybrid graphs to jointly model the task relatedness, view consistency, and bag-instance correlation by enhancing the labeling consistency between nearby nodes on the graphs. Furthermore, we analyze the generalization performance of the proposed method based on Rademacher complexity, which sheds light on the benefits of jointly modeling multiple types of heterogeneity. The resulting optimization problem is challenging since the objective function is non-smooth and non-convex. We propose an iterative algorithm based on block coordinate descent and bundle method to solve the problem. Experimental results on various datasets demonstrate the effectiveness of the proposed method.","Heterogeneous learning, Multi-task learning, Multi-view learning, Multi-instance learning",Pei Yang and Qi Tan and Jingrui He,https://www.sciencedirect.com/science/article/pii/S0031320320303228,https://doi.org/10.1016/j.patcog.2020.107519,0031-3203,2020,107519,107,Pattern Recognition,Complex heterogeneity learning: A theoretical and empirical study,article,YANG2020107519,
"Kernel ridge regression (KRR) is an efficient method for regression task. However, KRR has a deficiency in finding appropriate type of kernel functions and their parameters. To overcome this shortcoming, a novel kernel ensemble framework is developed. In this ensemble framework, each kernel regressor is associated with a weight that can be adaptively determined according to its contribution to the regression result. By this way, more appropriate kernels and more accurate parameters can be learned directly from data without any manual intervention, which results in better performance in regression. In addition, to overcome the influence of existing outliers, the regressor loss is modeled as a sparse signal, thus a Sparse Loss induced Kernel Ensemble Regression (SLiKER) method is obtained. Experimental results on several UCI regression and computer vision datasets show that our proposed approach obtains best regression and classification performances among the state-of-art comparative methods.","Multiple kernels, Ensemble regression, Sparse loss, Classification",Xiang-Jun Shen and ChengGong Ni and Liangjun Wang and Zheng-Jun Zha,https://www.sciencedirect.com/science/article/pii/S0031320320303903,https://doi.org/10.1016/j.patcog.2020.107587,0031-3203,2021,107587,109,Pattern Recognition,SLiKER: Sparse loss induced kernel ensemble regression,article,SHEN2021107587,
"Ensembles of deep convolutional neural networks (CNNs), which integrate multiple deep CNN models to achieve better generalization for an artificial intelligence application, now play an important role in ensemble learning due to the dominant position of deep learning. However, the usage of ensembles of deep CNNs is still not adequate because the increasing complexity of deep CNN architectures and the emerging data with large dimensionality have made the training stage and testing stage of ensembles of deep CNNs inevitably expensive. To alleviate this situation, we propose a new approach that finds multiple models converging to local minima in subparameter space for ensembles of deep CNNs. The subparameter space here refers to the space constructed by a partial selection of parameters, instead of the entire set of parameters, of a deep CNN architecture. We show that local minima found in the subparameter space of a deep CNN architecture can in fact be effective for ensembles of deep CNNs to achieve better generalization. Moreover, finding local minima in the subparameter space of a deep CNN architecture is more affordable at the training stage, and the multiple models at the found local minima can also be selectively fused to achieve better ensemble generalization while limiting the expense to a single deep CNN model at the testing stage. Demonstrations of MobilenetV2, Resnet50 and InceptionV4 (deep CNN architectures from lightweight to complex) on ImageNet, CIFAR-10 and CIFAR-100, respectively, lead us to believe that finding local minima in the subparameter space of a deep CNN architecture could be leveraged to broaden the usage of ensembles of deep CNNs.","Ensemble learning, Ensemble selection, Ensemble fusion, Deep convolutional neural network",Yongquan Yang and Haijun Lv and Ning Chen and Yang Wu and Jiayi Zheng and Zhongxi Zheng,https://www.sciencedirect.com/science/article/pii/S003132032030385X,https://doi.org/10.1016/j.patcog.2020.107582,0031-3203,2021,107582,109,Pattern Recognition,Local minima found in the subparameter space can be effective for ensembles of deep convolutional neural networks,article,YANG2021107582,
"The capability of multi column convolutional networks in identifying local invariant features helps improve its performance on image classification tasks to a large extent. Suppression of non maximal activations in a convolutional network, however, can lead to loss of valuable information, as scalar activations typically only ,encode the presence (or absence) of a feature in an input image, providing no additional information. Capsule networks, on other hand, learn richer representations by propagating non-maximal activations to higher layers, encoding the agreement between neurons at various layers on the presence (or absence) of a feature into a fixed-length vector. Traditional capsule networks, however encodes agreements for micro and macro-level features of an input image with same precedence. Such an uniform agreement protocol can hinder the repsentation capability of a network, especially for datasets that contain objects with independently deformable components. To address this, we propose a novel two-phase dynamic routing protocol that computes agreements between neurons at various layers for micro and macro-level features, following a hierarchical learning paradigm. Experiments on seven publicly available datasets show that a multi-column capsule network that encodes an input image following our routing protocol performs competitively or better than contemporary multi-column convolutional architectures andtraditional capsule networks on a classification task.Implementations of the networks used in this paper have been made available at: github.com/DVLP-CMATERJU/TwoPhaseDynamicRouting.","Dynamic routing, Routing by agreement, Multi-level, Multi-column neural network, Capsule networks",Bodhisatwa Mandal and Ritesh Sarkhel and Swarnendu Ghosh and Nibaran Das and Mita Nasipuri,https://www.sciencedirect.com/science/article/pii/S0031320320303988,https://doi.org/10.1016/j.patcog.2020.107595,0031-3203,2021,107595,109,Pattern Recognition,Two-phase Dynamic Routing for Micro and Macro-level Equivariance in Multi-Column Capsule Networks,article,MANDAL2021107595,
"In this paper, we introduce a large-scale dataset, called SCUT-HCCDoc, to address challenging detection and recognition problems of handwritten Chinese text (HCT) in the camera-captured documents. Despite extensive studies of optical character recognition (OCR) and offline handwriting recognition for document images, text detection and recognition in the camera-captured documents remains an unsolved problem that is worth for extensive study and investigation. With recent advances in deep learning, researchers have proposed useful architectures for feature learning, detection, and recognition for the scene text. However, the performance of deep learning methods highly depends on the amount and diversity of training data. Previous OCR and offline HCT datasets were built under specific constraints, and most of the recent scene text datasets are for non-handwritten text. Hence, there is a lack of a comprehensive scene handwritten text benchmark. This study focuses on scenes with handwritten Chinese text. We introduce the SCUT-HCCDoc database for HCT detection, recognition and spotting. SCUT-HCCDoc contains 12,253 camera-captured document images with 116,629 text lines and 1,155,801 characters. The diversity of SCUT-HCCDoc can be described at three levels: (1) image-level diversity: image appearance and geometric variances caused by camera-captured settings (such as perspective, background, and resolution) and different applications (such as note-taking, test papers, and homework); (2) text-level diversity: variances of text line length, rotation, etc.; (3) character-level diversity: variances of character categories (up to 6109 classes with additional English letters, and digits), character size, individual writing style, etc. Three kinds of baseline experiments were conducted, where we used several popular text detection methods for text line detection, CTC-based/attention-based methods for text line recognition, and combine text detectors with CTC-based recognizer to achieve end-to-end text spotting. The results indicate the diversity of SCUT-HCCDoc and the challenges of HCT understanding in document images. The dataset is available at https://github.com/HCIILAB/SCUT-HCCDoc_Dataset_Release.","Document analysis and recognition, Handwritten Chinese text recognition, Handwritten Chinese text detection, Benchmark dataset",Hesuo Zhang and Lingyu Liang and Lianwen Jin,https://www.sciencedirect.com/science/article/pii/S0031320320303629,https://doi.org/10.1016/j.patcog.2020.107559,0031-3203,2020,107559,108,Pattern Recognition,SCUT-HCCDoc: A new benchmark dataset of handwritten Chinese text in unconstrained camera-captured documents,article,ZHANG2020107559,
"We analyze the drawbacks of DBSCAN and its variants, and find the grid technique, which is used in Fast-DBSCAN and Ï-approximate DBSCAN, is almost useless in high dimensional data space. Because it usually yields considerable redundant distance computations. In order to tame these problems, two techniques are proposed: one is to use Ïµ2-norm ball to identify Inner Core Blocks within which all points are core points, it has higher efficiency than grid technique for finding more core points at one time; the other is a fast approximate algorithm for judging whether two Inner Core Blocks are density-reachable from each other. Besides, cover tree is also used to accelerate the process of density computations. Based on the three techniques, an approximate approach, namely BLOCK-DBSCAN, is proposed for large scale data, which runs in about O(nlogâ(n)) expected time and obtains almost the same result as DBSCAN. BLOCK-DBSCAN has two versions, i.e., L2 version can work well for relatively high dimensional data, and Lâ version is suitable for high dimensional data. Experimental results show that BLOCK-DBSCAN is promising and outperforms NQDBSCAN, Ï-approximate DBSCAN and AnyDBC.","DBSCAN, -approximate DBSCAN, BLOCK-DBSCAN, Core block",Yewang Chen and Lida Zhou and Nizar Bouguila and Cheng Wang and Yi Chen and Jixiang Du,https://www.sciencedirect.com/science/article/pii/S0031320320304271,https://doi.org/10.1016/j.patcog.2020.107624,0031-3203,2021,107624,109,Pattern Recognition,BLOCK-DBSCAN: Fast clustering for large scale data,article,CHEN2021107624,
"Data annotation using visual inspection (supervision) of each training sample can be laborious. Interactive solutions alleviate this by helping experts propagate labels from a few supervised samples to unlabeled ones based solely on the visual analysis of their feature space projection (with no further sample supervision). We present a semi-automatic data annotation approach based on suitable feature space projection and semi-supervised label estimation. We validate our method on the popular MNIST dataset and on images of human intestinal parasites with and without fecal impurities, a large and diverse dataset that makes classification very hard. We evaluate two approaches for semi-supervised learning from the latent and projection spaces, to choose the one that best reduces user annotation effort and also increases classification accuracy on unseen data. Our results demonstrate the added-value of visual analytics tools that combine complementary abilities of humans and machines for more effective machine learning.","Semi-supervised learning, Unsupervised feature learning, Interactive data annotation, Autoencoder-neural networks, Data visualization",BÃ¡rbara C. Benato and Jancarlo F. Gomes and Alexandru C. Telea and Alexandre X. FalcÃ£o,https://www.sciencedirect.com/science/article/pii/S0031320320304155,https://doi.org/10.1016/j.patcog.2020.107612,0031-3203,2021,107612,109,Pattern Recognition,Semi-automatic data annotation guided by feature space projection,article,BENATO2021107612,
"The self-labeled technique is a type of semi-supervised classification that can be used when labeled data are lacking. Although existing self-labeled techniques show promise in many areas of classification and pattern recognition, they commonly incorrectly label data. The reasons for this problem are the shortage of labeled data and the inappropriate distribution of data in problem space. To deal with this problem, we propose in this paper a synthetic, labeled data generation method based on accuracy and density. Positions of generated data are improved through a multi-objective evolutionary algorithm with two objectives â accuracy and density. The density function generates data with an appropriate distribution and diversity in feature space, whereas the accuracy function eliminates outlier data. The advantage of the proposed method over existing ones is that it simultaneously considers accuracy and distribution of generated data in feature space. We have applied the new proposed method on four self-labeled techniques with different features, i.e., Democratic-co, Tri-training, Co-forest, and Co-bagging. The results show that the proposed method is superior to existing methods in terms of classification accuracy. Also, the superiority of the proposed method is confirmed over other data generation methods such as SMOTE, Borderline SMOTE, Safe-level SMOTE and SMOTE-RSB.","Self-labeled, Semi-supervised learning, Evolutionary multi-objective optimization, Data density function, NSGA-II",Zahra Donyavi and Shahrokh Asadi,https://www.sciencedirect.com/science/article/pii/S0031320320303460,https://doi.org/10.1016/j.patcog.2020.107543,0031-3203,2020,107543,108,Pattern Recognition,Diverse training dataset generation based on a multi-objective optimization for semi-Supervised classification,article,DONYAVI2020107543,
"The studies of hidden complex structures in data have popularized the use of graph-based learning methods in semi-supervised and unsupervised learning tasks. Kernelized graph-based methods are proven to perform better, but these methods suffer from the issue of appropriate kernel selection. Instead of using multiple views, these methods generally use a single view. But multi-view methods need a proper weight assignment technique to each view in proportion to their contribution to the learning task. To solve this, a novel Self-weighted Multi-view Multiple Kernel Learning (SMVMKL) framework is proposed using multiple kernels on multiple views that automatically assigns appropriate weight to each kernel of each view without introducing an additional parameter. But the real-world data that is either noisy or corrupt with outliers which may effect the performance of the proposed SMVMKL method. To deal with this, a Robust Self-weighted Multi-view Multiple Kernel Learning (RSMVMKL) framework using the l2,1-norm has also been proposed that reduces the effect of outliers present in the data set. Both the proposed methods have been evaluated on multiple benchmark data sets and result in a performance comparable with the other state-of-the-art multi-view methods considered in this paper.","Robust, Clustering, Semi-supervised classification, Multiple kernels, Multiple views",Supratim Manna and Jessy Rimaya Khonglah and Anirban Mukherjee and Goutam Saha,https://www.sciencedirect.com/science/article/pii/S0031320320304313,https://doi.org/10.1016/j.patcog.2020.107628,0031-3203,2021,107628,110,Pattern Recognition,Robust kernelized graph-based learning,article,MANNA2021107628,
"Multilingual handwritten text recognition is often accomplished in two cascaded steps: script identification and handwriting recognition. However, this scheme is not optimal due to error accumulation. To perform simultaneous script identification and handwriting recognition, in this paper, we propose a new framework named multilingual text recognition networks (MuLTReNets). Specifically, the system has four major modules: feature extractor, script identifier, handwriting recognizer and auto-weighter. The feature extractor integrates both spatial and temporal knowledge to encode text images into features shared by the script identifier and recognizer. The script identifier predicts script category from a variable-length sequence incorporating an auto-weighter for balancing different scripts, while the handwriting recognizer adopts long-short term memory (LSTM) and Connectionist Temporal Classification (CTC) to accomplish sequence decoding. Via multi-task learning, the proposed framework can benefit both two multilingual recognition schemes: unified recognition with merged alphabet (MuLTReNetV1) and cascaded script identification-single script recognition with joint training (MuLTReNetV2). We evaluated the performance of the proposed method on handwritten text databases of five languages, which are English, French, Kannada, Urdu, and Bangla. Experimental results demonstrate that our method performs superiorly for both script identification and handwriting recognition. The accuracy of script identification reaches 99.9%. While in handwriting recognition, the proposed system not only outperforms cascade systems but also surpasses systems particularly designed for specific scripts.","Multrenets, Auto-weighter, Separable MDLSTM, Multilingual handwritten text recognition, Multi-task learning",Zhuo Chen and Fei Yin and Xu-Yao Zhang and Qing Yang and Cheng-Lin Liu,https://www.sciencedirect.com/science/article/pii/S0031320320303587,https://doi.org/10.1016/j.patcog.2020.107555,0031-3203,2020,107555,108,Pattern Recognition,MuLTReNets: Multilingual text recognition networks for simultaneous script identification and handwriting recognition,article,CHEN2020107555,
"The random k-labelsets ensemble (RAkEL) is a multi-label learning strategy that integrates many single-label learning models. Each single-label model is constructed using a label powerset (LP) technique based on a randomly generated size-k label subset. Although RAkEL can improve the generalization capability and reduce the complexity of the original LP method, the quality of the randomly generated label subsets could be low. On the one hand, the transformed classes may be difficult to separate in the feature space, negatively affecting the performance; on the other hand, the classes might be highly imbalanced, resulting in difficulties in using the existing single-label algorithms. To solve these problems, we propose an active k-labelsets ensemble (ACkEL) paradigm. Borrowing the idea of active learning, a label-selection criterion is proposed to evaluate the separability and balance level of the classes transformed from a label subset. Subsequently, by randomly selecting the first label or label subset, the remaining ones are iteratively chosen based on the proposed criterion. ACkEL can be realized in both the disjoint and overlapping modes, which adopt pool-based and stream-based frameworks, respectively. Experimental comparisons demonstrate the feasibility and effectiveness of the proposed methods.","Multi-label learning, -Labelsets Ensemble, Label powerset, Separability",Ran Wang and Sam Kwong and Xu Wang and Yuheng Jia,https://www.sciencedirect.com/science/article/pii/S0031320320303861,https://doi.org/10.1016/j.patcog.2020.107583,0031-3203,2021,107583,109,Pattern Recognition,Active k-labelsets ensemble for multi-label classification,article,WANG2021107583,
"Space target recognition is the basic task of space situational awareness and has developed significantly in the last decade. This paper proposes a hybrid convolutional neural network with partial semantic information for space target recognition, which joints the global features and partial semantic information. Firstly, we propose a two-stage target detection network based on the characteristics of deep space targets. Secondly, we use the Mask R-CNN to segment the main components of the detected satellite. Thirdly, the recognized target and the segmented components are sent to the hybrid extractor to train the hybrid network. What we have done is to find the proper weights of the partial semantic information that plays different importance. The loss function of the hybrid network integrates the global-based and component-based loss with different weights. In comparison with several sets of comparative experiments, the proposed method has achieved a satisfactory result. Besides, we have simulated some real space target images by data processing and achieved a competitive performance in both the simulated dataset and the public dataset.","Space target recognition, Hybrid convolutional neural network with partial semantic information, Data augmentation, Components segment",Xi Yang and Tan Wu and Nannan Wang and Yan Huang and Bin Song and Xinbo Gao,https://www.sciencedirect.com/science/article/pii/S0031320320303344,https://doi.org/10.1016/j.patcog.2020.107531,0031-3203,2020,107531,108,Pattern Recognition,HCNN-PSI: A hybrid CNN with partial semantic information for space target recognition,article,YANG2020107531,
"Sequence labeling is a fundamental task in natural language processing and has been widely studied. Recently, RNN-based sequence labeling models have increasingly gained attentions. Despite superior performance achieved by learning the long short-term (i.e., successive) dependencies, the way of sequentially processing inputs might limit the ability to capture the non-continuous relations over tokens within a sentence. To tackle the problem, we focus on how to effectively model successive and discrete dependencies of each token for enhancing the sequence labeling performance. Specifically, we propose an innovative attention-based model (called position-aware self-attention, i.e., PSA) as well as a well-designed self-attentional context fusion layer within a neural network architecture, to explore the positional information of an input sequence for capturing the latent relations among tokens. Extensive experiments on three classical tasks in sequence labeling domain, i.e., Â part-of-speech (POS) tagging, named entity recognition (NER) and phrase chunking, demonstrate our proposed model outperforms the state-of-the-arts without any external knowledge, in terms of various metrics.","Equence labeling, Self-attention, Discrete context dependency",Wei Wei and Zanbo Wang and Xianling Mao and Guangyou Zhou and Pan Zhou and Sheng Jiang,https://www.sciencedirect.com/science/article/pii/S0031320320304398,https://doi.org/10.1016/j.patcog.2020.107636,0031-3203,2021,107636,110,Pattern Recognition,Position-aware self-attention based neural sequence labeling,article,WEI2021107636,
"Biological studies have shown that the interaction between neurons are based on neurotransmitters, which transmit signals between neurons, and that one neuron sends information to another neuron by releasing a number of different neurotransmitters, which play different roles. Motivated by this biological discovery, a novel neural networks model is proposed by extending the dimension of connection weights from one to multiple, i.e. there are multiple not only one connections between each two units. The number of dimensions of connection weight represents the number of categories of neurotransmitters and different components of the weight correspond to different neurotransmitters. In order to make these neurotransmitters collaborate and compete appropriately, the input and output for each unit in our proposed model have been heuristically defined. From the biological perspective, the proposed neural network is much closer to biological neural network. From the viewpoint of new model structure, the characteristic that the activation of each hidden unit is based on several filters, can improve the interpretability of features learned by the proposed neural network. Experimental results on MNIST, NORB and several other data sets have demonstrated that the performances of traditional neural networks can be improved by extending the dimension of connection weight between units, and the idea of multiple connection weights provides a new paradigm for the design of neural networks.","Neural network, Neurotransmitter, Interpretability, Extending dimension",Jiangshe Zhang and Junying Hu and Junmin Liu,https://www.sciencedirect.com/science/article/pii/S0031320320302843,https://doi.org/10.1016/j.patcog.2020.107481,0031-3203,2020,107481,107,Pattern Recognition,Neural network with multiple connection weights,article,ZHANG2020107481,
"The text-to-image synthesis task aims to generate photographic images conditioned on semantic text descriptions. To ensure the sharpness and fidelity of generated images, this task tends to generate high-resolution images (e.g., 1282 or 2562). However, as the resolution increases, the network parameters and complexity increases dramatically. Recent works introduce network structures with extensive parameters and heavy computations to guarantee the production of high-resolution images. As a result, these models come across problems of the unstable training process and high training cost. To tackle these issues, in this paper, we propose an effective information compensation based approach, namely Lightweight Dynamic Conditional GAN (LD-CGAN). LD-CGAN is a compact and structured single-stream network, and it consists of one generator and two independent discriminators to regularize and generate 642 and 1282 images in one feed-forward process. Specifically, the generator of LD-CGAN is composed of three major components: (1) Conditional Embedding (CE), which is an automatically unsupervised learning process aiming at disentangling integrated semantic attributes in the text space; (2) Conditional Manipulating Modular (CM-M) in Conditional Manipulating Block (CM-B), which is designed to continuously provide the image features with the compensation information (i.e., the disentangled attribute); and (3) Pyramid Attention Refine Block (PAR-B), which is used to enrich multi-scale features by capturing spatial importance between multi-scale context. Consequently, experiments conducted under two benchmark datasets, CUB and Oxford-102, indicate that our generated 1282 images can achieve comparable performance with 2562 images generated by the state-of-the-arts on two evaluation metrics: Inception Score (IS) and Visual-semantic Similarity (VS). Compared with the current state-of-the-art HDGAN, our LD-CGAN significantly decreases the number of parameters and computation time by 86.8% and 94.9%, respectively.","Text-to-image synthesis, Conditional generative adversarial network (CGAN), Network complexity, Disentanglement process, Entanglement process, Information compensation, Pyramid attentive fusion",Lianli Gao and Daiyuan Chen and Zhou Zhao and Jie Shao and Heng Tao Shen,https://www.sciencedirect.com/science/article/pii/S0031320320301874,https://doi.org/10.1016/j.patcog.2020.107384,0031-3203,2021,107384,110,Pattern Recognition,Lightweight dynamic conditional GAN with pyramid attention for text-to-image synthesis,article,GAO2021107384,
"Detecting text located on the torsos of marathon runners and sports players in video is a challenging issue due to poor quality and adverse effects caused by flexible/colorful clothing, and different structures of human bodies or actions. This paper presents a new unified method for tackling the above challenges. The proposed method fuses gradient magnitude and direction coherence of text pixels in a new way for detecting candidate regions. Candidate regions are used for determining the number of temporal frame clusters obtained by K-means clustering on frame differences. This process in turn detects key frames. The proposed method explores Bayesian probability for skin portions using color values at both pixel and component levels of temporal frames, which provides fused images with skin components. Based on skin information, the proposed method then detects faces and torsos by finding structural and spatial coherences between them. We further propose adaptive pixels linking a deep learning model for text detection from torso regions. The proposed method is tested on our own dataset collected from marathon/sports video and three standard datasets, namely, RBNR, MMM and R-ID of marathon images, to evaluate the performance. In addition, the proposed method is also tested on the standard natural scene datasets, namely, CTW1500 and MS-COCO text datasets, to show the objectiveness of the proposed method. A comparative study with the state-of-the-art methods on bib number/text detection of different datasets shows that the proposed method outperforms the existing methods.","Video text analysis, Gradient direction, Bayesian classifier, Face detection, Torso detection, Deep learning, Text detection",Sauradip Nag and Palaiahnakote Shivakumara and Umapada Pal and Tong Lu and Michael Blumenstein,https://www.sciencedirect.com/science/article/pii/S003132032030279X,https://doi.org/10.1016/j.patcog.2020.107476,0031-3203,2020,107476,107,Pattern Recognition,A new unified method for detecting text from marathon runners and sports players in video (PR-D-19-01078R2),article,NAG2020107476,
"In this study, we attempted to develop a method for accelerating parameter optimization of an object detector ensemble over large image datasets by using simulated annealing. We propose a novel sampling-based evaluation method that considers the minimum portion of the dataset required in each iteration to maintain solution quality. This approach can be considered a noisy evaluation of the energy. The sample sizes required during the search process are theoretically determined by adapting the convergence results for noisy evaluation. To determine applicability, we prepared and optimized two ensembles for diabetic retinopathy pre-screening based on microaneurysm detection with convolutional neural network-based and traditional object detectors. Our experimental results indicate that the proposed sampling-based evaluation method substantially reduced the computational time required for optimizing the parameters of the ensembles while preserving solution quality.","Diabetic retinopathy, Ensemble, Microaneurysm detection, Parameter optimization, Sampling-based evaluation, Simulated annealing",JÃ¡nos TÃ³th and Henrietta TomÃ¡n and AndrÃ¡s Hajdu,https://www.sciencedirect.com/science/article/pii/S0031320320303137,https://doi.org/10.1016/j.patcog.2020.107510,0031-3203,2020,107510,107,Pattern Recognition,Efficient sampling-based energy function evaluation for ensemble optimization using simulated annealing,article,TOTH2020107510,
"Generative Adversarial Networks (GAN) receive great attention recently due to its excellent performance in image generation, transformation, and super-resolution. However, less emphasis or study has been put on GAN for classification with super-resolution. Moreover, though GANs may fabricate images which perceptually looks realistic, they usually fabricate some fake details especially in character data; this would impose further difficulties when they are input for classification. In this paper, we propose a novel Generative Adversarial Classifier (GAC) for low-resolution handwriting character recognition. Specifically, we design an additional classifier component in GAC, leading to a novel three-player GAN model which is not only able to generate high-quality super-resolved images, but also favorable for classification. Experimental results show that our proposed method can obtain remarkable performance in handwriting characters with 8Â ÃÂ  super-resolution, achieving new state-of-the-art on benchmark dataset CASIA-HWDB1.1, and MNIST.","Super-Resolution, Generative adversarial networks (GAN), Handwriting characters recognition",Zhuang Qian and Kaizhu Huang and Qiu-Feng Wang and Jimin Xiao and Rui Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320302569,https://doi.org/10.1016/j.patcog.2020.107453,0031-3203,2020,107453,107,Pattern Recognition,Generative adversarial classifier for handwriting characters super-resolution,article,QIAN2020107453,
"Multi-task learning is a promising machine learning branch, which aims to improve the generalization of the prediction models by sharing knowledge among tasks. Most of the existing multi-task learning methods rely on predefined task relationships and guide the learning process of models by linear regularization terms. On the one hand, improper setting of task relationships may result in negative knowledge transfer; on the other hand, these methods also suffer from the insufficiency of representation ability. To overcome these problems, this paper focuses on attention-based deep multi-task learning method, and provides a novel deep multi-task learning method, namely, Deep Multi-task Learning with Relational Attention (DMLRA). In particular, we first provide a task-specific attention module to specify features for different learning tasks, because different prediction tasks may rely on different parts of the shared feature set. Then, we design a relational attention module to learn relationships among multiple tasks automatically, and transfer positive and negative knowledge among multiple tasks accordingly. Moreover, we provide a joint deep multi-task learning framework to combine task-specific module and relational attention module. Finally, we apply our method on a multi-criteria business success assessment problem, both classical and the state-of-the-art multi-task learning methods are employed to provide baseline performance. The experiments are conducted on real-world datasets, results demonstrate the superiority of our method over the existing methods.","Multi-task learning, Attention, Site selection",Jiejie Zhao and Bowen Du and Leilei Sun and Weifeng Lv and Yanchi Liu and Hui Xiong,https://www.sciencedirect.com/science/article/pii/S0031320320302727,https://doi.org/10.1016/j.patcog.2020.107469,0031-3203,2021,107469,110,Pattern Recognition,Deep multi-task learning with relational attention for business success prediction,article,ZHAO2021107469,
"This paper proposes a novel image decomposition model for scene depth recovery from low-quality depth measurements and its corresponding high resolution color image. Through our observation, the depth map mainly contains smooth regions separated by additive step discontinuities, and can be simultaneously decomposed into a local smooth surface and an approximately piecewise constant component. Therefore, the proposed unified model combines the least square polynomial approximation (for smooth surface) and a sparsity-promoting prior (for piecewise constant) to better portray the 2D depth signal intrinsically. As we know, the representation of the piecewise constant signal in gradient domain is extremely sparse. Previous researches using total variation filter based on L1-norm or Lp-norm (0â¯<â¯pâ¯<â¯1) are both sub-optimal when addressing the tradeoff between enhancing the sparsity and keeping the model convex. We propose a novel non-convex penalty based on Moreau envelope, which promotes the prior sparsity and simultaneously maintains the convexity of the whole model for each variable. We prove the convexity of the proposed model and give the convergence analysis of the algorithm. We also introduce an iterative reweighted strategy applied on the sparsity prior to deal with the depth-color inconsistent problem and to locate the depth boundaries. Moreover, we provide an accelerated algorithm to deal with the problem of non-uniform down-sampling when transforming the depth observation matrix into the Fourier domain for fast processing. Experimental results demonstrate that the proposed method can handle various types of depth degradation and achieve promising performance in terms of recovery accuracy and running time.","Image decomposition, Depth recovery, Depth discontinuities, Depth cameras",Xinchen Ye and Mingliang Zhang and Jingyu Yang and Xin Fan and Fangfang Guo,https://www.sciencedirect.com/science/article/pii/S0031320320303095,https://doi.org/10.1016/j.patcog.2020.107506,0031-3203,2020,107506,107,Pattern Recognition,A sparsity-promoting image decomposition model for depth recovery,article,YE2020107506,
"Semantic segmentation is an end-to-end task that requires both semantic and spatial accuracy. It is important for deep learning-based segmentation methods to effectively utilize the high-level feature map whose semantic information is abundant and the low-level feature map whose spatial information is accurate. However, existing segmentation networks typically cannot take full advantage of these two kinds of feature maps, leading to inferior performance. This paper attempts to overcome this challenge by introducing two novel structures. On the one hand, we propose a structure called stride spatial pyramid pooling (SSPP) to capture multiscale semantic information from the high-level feature map. Compared with existing pyramid pooling methods based on the atrous convolution, the SSPP structure is able to gather more information from the high-level feature map with faster inference speed, which improves the utilization efficiency of the high-level feature map significantly. On the other hand, we propose a dual attention decoder consisting of a channel attention branch and a spatial attention branch to make full use of the high- and low-level feature maps simultaneously. The dual attention decoder can result in a more âsemanticâ low-level feature map and a high-level feature map with more accurate spatial information, which bridges the gap between these two kinds of feature maps and benefits their fusion. We evaluate the proposed model on several publicly available semantic image segmentation benchmarks including PASCAL VOC 2012, Cityscapes and COCO-Stuff. The qualitative and quantitative results demonstrate that our method can achieve the state-of-the-art performance.","Semantic segmentation, Convolutional neural networks, Pyramid pooling, Attention mechanism",Chengli Peng and Jiayi Ma,https://www.sciencedirect.com/science/article/pii/S0031320320303010,https://doi.org/10.1016/j.patcog.2020.107498,0031-3203,2020,107498,107,Pattern Recognition,Semantic segmentation using stride spatial pyramid pooling and dual attention decoder,article,PENG2020107498,
"The task of the proposed method is semi-supervised video object segmentation where only the ground-truth segmentation of the first frame is provided. The existing approaches rely on selecting the region of interest for model update; however it is rough and inflexible, leading to performance degradation. To overcome this limitation, a novel approach is proposed which utilizes reinforcement learning to select optimal adaptation areas for each frame, based on the historical segmentation information. The RL model learns to take optimal actions to adjust the region of interest inferred from the previous frame for online model updating. To speed up the model adaption, a novel multi-branch tree based exploration method is designed to quickly select the best state action pairs. The proposed method is evaluated on three common video object segmentation datasets including DAVIS 2016, SegTrack V2 and Youtube-Object. The results show that the proposed work improves the state-of-the-art of the mean region similarity to 87.1% on the DAVIS 2016 dataset, and to 79.5% on the Youtube-Object dataset. Meanwhile, competitive performance is obtained on the SegTrack V2 dataset. Code is at https://github.com/insomnia94/ARG.","Model adaptation, Video object segmentation, Reinforcement learning, Training accelerate",Mingjie Sun and Jimin Xiao and Eng Gee Lim and Yanchun Xie and Jiashi Feng,https://www.sciencedirect.com/science/article/pii/S0031320320302685,https://doi.org/10.1016/j.patcog.2020.107465,0031-3203,2020,107465,106,Pattern Recognition,Adaptive ROI generation for video object segmentation using reinforcement learning,article,SUN2020107465,
"Deep neural networks have rapidly become an indispensable tool in many classification applications. However, the inclusion of deep learning methods in medical diagnostic systems has come at the cost of diminishing their explainability. This significantly reduces the safety of a diagnostic system, since the physician is unable to interpret and validate the output. Therefore, in this work we aim to address this major limitation and improve the explainability of a skin cancer diagnostic system. We propose to leverage two sources of information: (i) medical knowledge, in particular the taxonomic organization of skin lesions, which will be used to develop a hierarchical neural network; and (ii) recent advances in channel and spatial attention modules, which can identify interpretable features and regions in dermoscopy images. We demonstrate that the proposed approach achieves competitive results in two dermoscopy data sets (ISIC 2017 and 2018) and provides insightful information about its decisions, thus increasing the safety of the model.","Hierarchical deep learning, Explainability, Channel attention, Spatial attention, Safety-critical CADS, Skin cancer",Catarina Barata and M. Emre Celebi and Jorge S. Marques,https://www.sciencedirect.com/science/article/pii/S0031320320302168,https://doi.org/10.1016/j.patcog.2020.107413,0031-3203,2021,107413,110,Pattern Recognition,Explainable skin lesion diagnosis using taxonomies,article,BARATA2021107413,
"Multi-view subspace clustering aims at separating data points into multiple underlying subspaces according to their multi-view features. Existing low-rank tensor representation-based multi-view subspace clustering algorithms are robust to noise and can preserve the high-order correlations of multi-view features. However, they may suffer from two common problems: (1) the local structures and different importance of each view feature are often neglected; (2) the low-rank representation tensor and affinity matrix are learned separately. To address these issues, we propose a unified framework to learn the Graph regularized Low-rank representation Tensor and Affinity matrix (GLTA) for multi-view subspace clustering. In the proposed GLTA framework, the tensor singular value decomposition-based tensor nuclear norm is adopted to explore the high-order cross-view correlations. The manifold regularization is exploited to preserve the local structures embedded in high-dimensional space. The importance of different features is automatically measured when constructing the final affinity matrix. An iterative algorithm is developed to solve GLTA using the alternating direction method of multipliers. Extensive experiments on seven challenging datasets demonstrate the superiority of GLTA over the state-of-the-art methods.","Multi-view subspace clustering, Low-rank tensor representation, Tensor-singular value decomposition, Adaptive weights, Local manifold",Yongyong Chen and Xiaolin Xiao and Yicong Zhou,https://www.sciencedirect.com/science/article/pii/S0031320320302442,https://doi.org/10.1016/j.patcog.2020.107441,0031-3203,2020,107441,106,Pattern Recognition,Multi-view subspace clustering via simultaneously learning the representation tensor and affinity matrix,article,CHEN2020107441,
"Online temporal action localization from an untrimmed video stream is a challenging problem in computer vision. It is challenging because of i) in an untrimmed video stream, more than one action instance may appear, including background scenes, and ii) in online settings, only past and current information is available. Therefore, temporal priors, such as the average action duration of training data, which have been exploited by previous action detection methods, are not suitable for this task because of the high intra-class variation in human actions. We propose a novel online action detection framework that considers actions as a set of temporally ordered subclasses and leverages a future frame generation network to cope with the limited information issue associated with the problem outlined above. Additionally, we augment our data by varying the lengths of videos to allow the proposed method to learn about the high intra-class variation in human actions. We evaluate our method using two benchmark datasets, THUMOSâ14 and ActivityNet, for an online temporal action localization scenario and demonstrate that the performance is comparable to state-of-the-art methods that have been proposed for offline settings.","Online action detection, Untrimmed video stream, Future frame generation, 3D convolutional neural network, Long short-term memory",Da-Hye Yoon and Nam-Gyu Cho and Seong-Whan Lee,https://www.sciencedirect.com/science/article/pii/S0031320320301990,https://doi.org/10.1016/j.patcog.2020.107396,0031-3203,2020,107396,106,Pattern Recognition,A novel online action detection framework from untrimmed video streams,article,YOON2020107396,
"A challenging issue of clustering in real-word application is to detect clusters with arbitrary shapes and densities in complex data. Many conventional clustering algorithms are capable of detecting non-spherical clusters, but their performance is limited when processing data with complex shapes and multiple density peaks in a cluster without knowing the number of clusters. This paper proposes an adaptive core fusion-based density peak clustering (CFDPC) for detecting clusters in any shape and density adaptively. An initial clustering based on automatic finding of density peaks is proposed first. An adaptive searching approach is then proposed to find core points, and a within-cluster similarity-based core fusion strategy is proposed to obtain the final clustering results. The CFDPC where the number of clusters arises intuitively is simple and efficient. The performance of CFDPC is successfully verified in clustering several benchmark complex datasets with diverse shapes and densities.","Clustering, Density peak, Core fusion, Arbitrary shape, Arbitrary density",Fang Fang and Lei Qiu and Shenfang Yuan,https://www.sciencedirect.com/science/article/pii/S0031320320302557,https://doi.org/10.1016/j.patcog.2020.107452,0031-3203,2020,107452,107,Pattern Recognition,Adaptive core fusion-based density peak clustering for complex data with arbitrary shapes and densities,article,FANG2020107452,
"Inhomogenous image segmentation has been a research challenge in recent years. To deal with this difficulty, we propose a new local and global active contour model based on Jeffreys divergence. First, unlike the local data fitting energy of the region-scalable fitting model, a new local data fitting energy based on Jeffreys divergence is proposed instead of Euclidean distance, which achieves relatively better segmentation. Second, to improve the versatility of the model, a new global data fitting energy based on Jeffreys divergence is proposed. Finally, the adaptive weights of the local and global data fitting energies are developed to increase the robustness to the initial curve. Experiments on real-world and medical images with inhomogeneities indicate that the proposed model can obtain accurate segmentation results efficiently and is not strictly dependent on setting up initial curves.","Active contour model, Inhomogenous image segmentation, Local and global data fitting energies, Jeffreys divergence, Adaptive weight",Bin Han and Yiquan Wu,https://www.sciencedirect.com/science/article/pii/S003132032030323X,https://doi.org/10.1016/j.patcog.2020.107520,0031-3203,2020,107520,107,Pattern Recognition,Active contour model for inhomogenous image segmentation based on Jeffreys divergence,article,HAN2020107520,
"Multiple object tracking (MOT) aims to model the temporal relationship among detected objects and associate them into trajectories. Thus, one major challenge of MOT lies in the confusion from noisy object detection results. In this paper, we propose Tracklet-Plane Matching (TPM), a new approach which improves the performance of MOT by modeling and reducing the interferences from noisy or confusing object detections. TPM first constructs good temporally-related object detections into short tracklets. Then, a tracklet-plane matching process is introduced to organize related tracklets into planes and associate them into long trajectories. The tracklet-plane matching process assigns visually confusing tracklets into different tracklet planes according to their contextual information, thus properly reducing the confusion among similar tracklets. At the same time, it also allows association among temporally non-neighboring or overlapping tracklets, which provides good flexibility to handle confusion from noisy detections. Under this process, a tracklet-importance evaluation scheme and a representative-based similarity modeling scheme are introduced. These two schemes can properly evaluate the reliability of detection results and identify reliable ones during association so that the impact of noisy or confusing detections can be well-mitigated. Experimental results on benchmark datasets demonstrate that the proposed approach outperforms the state-of-the-art MOT methods.","Multiple object tracking, Tracklet, Tracklet-plane, Representative-selection network",Jinlong Peng and Tao Wang and Weiyao Lin and Jian Wang and John See and Shilei Wen and Erui Ding,https://www.sciencedirect.com/science/article/pii/S0031320320302831,https://doi.org/10.1016/j.patcog.2020.107480,0031-3203,2020,107480,107,Pattern Recognition,TPM: Multiple object tracking with tracklet-plane matching,article,PENG2020107480,
"I.I.D.22I.I.D.: Independent and Identically Distributed hypothesis between training and testing data is the basis of numerous image classification methods. Such property can hardly be guaranteed in practice where the Non-IIDness is common, causing instable performances of these models. In literature, however, the Non-I.I.D.33Non-I.I.D: Non-Independent and Identically Distributed image classification problem is largely understudied. A key reason is lacking of a well-designed dataset to support related research. In this paper, we construct and release a Non-I.I.D. image dataset called NICO44NICO: Non-I.I.D. Image dataset with Contexts, which uses contexts to create Non-IIDness consciously. Compared to other datasets, extended analyses prove NICO can support various Non-I.I.D. situations with sufficient flexibility. Meanwhile, we propose a baseline model with ConvNet structure for General Non-I.I.D. image classification, where distribution of testing data is unknown but different from training data. The experimental results demonstrate that NICO can well support the training of ConvNet model from scratch, and a batch balancing module can help ConvNets to perform better in Non-I.I.D. settings.","Non-I.I.D, Dataset, Context, Bias, ConvNet, Batch balancing",Yue He and Zheyan Shen and Peng Cui,https://www.sciencedirect.com/science/article/pii/S0031320320301862,https://doi.org/10.1016/j.patcog.2020.107383,0031-3203,2021,107383,110,Pattern Recognition,Towards Non-I.I.D. image classification: A dataset and baselines,article,HE2021107383,
"Similarity learning is a kind of machine learning algorithm that aims to measure the relevance between given objects. However, conventional similarity learning algorithms usually measure the distance between the entire given objects in the latent feature space. Consequently, the obtained similarity scores only represent how close are the entire given objects, but are incapable of demonstrating which part of them are similar to each other and how semantically similar are they. To address the above problems, in this paper, we propose a self-attention driven adversarial similarity learning network. Discriminative self-attention weights are firstly assigned to different regions of the given objects. The similarity learning step measures the relevance between these self-attention weighted feature maps of given objects under various topic vectors. The topic vectors are conditioned to capture and preserve hidden semantic information within data distribution by a generator-discriminator model with adversarial loss. This model aims to generate objects from topic vectors and propagates the difference between the generated and the real objects back to the similarity learning step, which forces the topic vectors to not only assign discriminative similarity scores to different object pairs but also further mine the hidden semantic information within data distribution. The final similarity scores represent how tight the given objects are connected to the topics. In addition, the regions with higher self-attention weights make more contribution to the discriminative similarity scores. The effectiveness of the proposed method is demonstrated through evaluations based on image retrieval task and document retrieval task and compared against various state-of-the-art algorithms in the field. The visualization results of topic vectors and self-attention weighted feature maps are demonstrated to make our proposed method explainable.","Self-attention mechanism, Adversarial loss, Similarity learning network, Explainable deep learning",Xinjian Gao and Zhao Zhang and Tingting Mu and Xudong Zhang and Chaoran Cui and Meng Wang,https://www.sciencedirect.com/science/article/pii/S0031320320301345,https://doi.org/10.1016/j.patcog.2020.107331,0031-3203,2020,107331,105,Pattern Recognition,Self-attention driven adversarial similarity learning network,article,GAO2020107331,
"Pruning studies up to date focused on uncovering a smaller network by removing redundant units, and fine-tuning to compensate accuracy drop as a result. In this study, unlike the others, we propose an approach to uncover a smaller network that is competent only in a specific task, similar to top-down attention mechanism in human visual system. This approach doesnât require fine-tuning and is proposed as a fast and effective alternative of training from scratch when the network focuses on a specific task in the dataset. Pruning starts from the output and proceeds towards the input by computing neuron importance scores in each layer and propagating them to the preceding layer. In the meantime, neurons determined as worthless are pruned. We applied our approach on three benchmark datasets: MNIST, CIFAR-10 and ImageNet. The results demonstrate that the proposed pruning method typically reduces computational units and storage without harming accuracy significantly.","Deep learning, Computer vision, Network pruning, Network compressing, Top-down attention, Perceptual visioning",Cagri Kaplan and Abdullah Bulbul,https://www.sciencedirect.com/science/article/pii/S0031320320302715,https://doi.org/10.1016/j.patcog.2020.107468,0031-3203,2021,107468,110,Pattern Recognition,Goal driven network pruning for object recognition,article,KAPLAN2021107468,
"Multimodal hashing methods have gained considerable attention in recent years due to their effectiveness and efficiency for cross-modal similarity searches. Existing multimodal hashing methods either learn unified hash codes for different modalities or learn individual hash codes for each modality and then explore cross-correlations between them. Generally, learning unified hash codes tends to preserve the shared properties of multimodal data and learning individual hash codes tends to preserve the specific properties of each modality. There remains a crucial bottleneck regarding how to learn hash codes that simultaneously preserve the shared properties and specific properties of multimodal data. Therefore, we present a joint and individual matrix factorization hashing (JIMFH) method, which not only learns unified hash codes for multimodal data to preserve their common properties but also learns individual hash codes for each modality to retain its specific properties. The proposed JIMFH learns unified hash codes by joint matrix factorization, which jointly factorizes all modalities into a shared latent semantic space. In addition, JIMFH learns individual hash codes by individual matrix factorization, which separately factorizes each modality into a modal-specific latent semantic space. Finally, unified hash codes and individual hash codes are combined to obtain the final hash codes. In this way, hash codes learned by JIMFH can preserve both the shared properties and specific properties of multimodal data, and therefore the retrieval performance is enhanced. Comprehensive experiments show that the proposed JIMFH performs much better than many state-of-the-art methods on cross-modal retrieval applications.","Hashing, Multimodal, Retrieval, Cross-modal, Matrix factorization",Di Wang and Quan Wang and Lihuo He and Xinbo Gao and Yumin Tian,https://www.sciencedirect.com/science/article/pii/S003132032030282X,https://doi.org/10.1016/j.patcog.2020.107479,0031-3203,2020,107479,107,Pattern Recognition,Joint and individual matrix factorization hashing for large-scale cross-modal retrieval,article,WANG2020107479,
"Low-rank quaternion tensor completion method, a novel approach to recovery color videos and images, is proposed in this paper. We respectively reconstruct a color image and a color video as a quaternion matrix (second-order tensor) and a third-order quaternion tensor by encoding the red, green, and blue channel pixel values on the three imaginary parts of a quaternion. Different from some traditional models which treat color pixel as a scalar and represent color channels separately, whereas, during the quaternion-based reconstruction, it is significant that the inherent color structures of color images and color videos can be completely preserved. Under the definition of Tucker rank, the global low-rank prior to quaternion tensor is encoded as the nuclear norm of unfolding quaternion matrices. Then, by applying the ADMM framework, we provide the tensor completion algorithm for any order (Â â¥Â 2) quaternion tensors, which theoretically can be well used to recover missing entries of any multidimensional data with color structures. Simulation results for color videos and color images recovery show the superior performance and efficiency of the proposed method over some state-of-the-art existing ones.","Quaternion, Color videos, Color images, Tensor completion, Low-rank",Jifei Miao and Kit Ian Kou and Wankai Liu,https://www.sciencedirect.com/science/article/pii/S0031320320303083,https://doi.org/10.1016/j.patcog.2020.107505,0031-3203,2020,107505,107,Pattern Recognition,Low-rank quaternion tensor completion for recovering color videos and images,article,MIAO2020107505,
"A novel linear multivariate decision tree classifier, Binary Decision Tree based on K-means Splitting (BDTKS), is presented in this paper. The unsupervised K-means clustering is recursively integrated into the binary tree, building a hierarchical classifier. The introduction of the unsupervised K-means clustering provides the powerful generalization ability for the resulting BDTKS model. Then, the good generalization ability of BDTKS ensures the classification performance. A novel non-split condition with an easy-setting hyperparameter which focuses more on minority classes of the current node is proposed and applied in the BDTKS model, avoiding ignoring the minority classes in the class imbalance cases. Furthermore, the K-means centroid based BDTKS model is converted into the hyperplane based decision tree, speeding up the process of classification. Extensive experiments on the publicly available data sets have demonstrated that the proposed BDTKS matches or outperforms the previous decision trees.","Hierarchical classifier, Binary tree, Multivariate decision tree, K-means, Supervised classification",Fei Wang and Quan Wang and Feiping Nie and Zhongheng Li and Weizhong Yu and Fuji Ren,https://www.sciencedirect.com/science/article/pii/S0031320320303241,https://doi.org/10.1016/j.patcog.2020.107521,0031-3203,2020,107521,107,Pattern Recognition,A linear multivariate binary decision tree classifier based on K-means splitting,article,WANG2020107521,
"Person re-identification, a branch of image retrieval, is an increasingly important public safety application. When monitoring larger areas, it is crucial to correctly match the same person in different camera views. With the emergence of deep learning and large-scale data, metric learning has significantly improved person re-identification performance, but the extent to which deep features affect metric learning performance is unknown. However, given the large number of approaches, datasets, evaluation indices, and experimental environments, comparing metric learning methods directly is difficult. To obtain a more comprehensive empirical evaluation of the person re-identification, here we summarize the different types of features and metric learning approaches from a label attributes perspective. Then, by combining advanced approaches to data enhancement and feature extraction, we conduct comprehensive experiments on metric learning methods with two datasets. For fairness, all methods use a unified code library that includes two data enhancement schemes, eight feature extraction algorithms, and eight metric learning methods. Our results show that, the relations of loss function with deep feature space and metric learning.","Person re-identification, Deep features, Metric learning, Empirical comparison",Wanyin Wu and Dapeng Tao and Hao Li and Zhao Yang and Jun Cheng,https://www.sciencedirect.com/science/article/pii/S0031320320302272,https://doi.org/10.1016/j.patcog.2020.107424,0031-3203,2021,107424,110,Pattern Recognition,Deep features for person re-identification on metric learning,article,WU2021107424,
"Persistent homology and persistent entropy have recently become useful tools for patter recognition. In this paper, we find requirements under which persistent entropy is stable to small perturbations in the input data and scale invariant. In addition, we describe two new stable summary functions combining persistent entropy and the Betti curve. Finally, we use the previously defined summary functions in a material classification task to show their usefulness in machine learning and pattern recognition.","Persistent homology, Persistent entropy, Stability, Dimensionality reduction",Nieves Atienza and Rocio Gonzalez-DÃ­az and Manuel Soriano-Trigueros,https://www.sciencedirect.com/science/article/pii/S0031320320303125,https://doi.org/10.1016/j.patcog.2020.107509,0031-3203,2020,107509,107,Pattern Recognition,On the stability of persistent entropy and new summary functions for topological data analysis,article,ATIENZA2020107509,
"The performance of subspace-based methods such as matched subspace detector (MSD) and MSD with interaction effects (MSDinter) heavily depends on the background subspace and the target subspace. Nonetheless, constructing a representative target subspace is challenging due to the limited availability of target spectra in a collected hyperspectral image. In this paper, we propose two new hyperspectral target detection methods termed data-augmented MSD (DAMSD) and data-augmented MSDinter (DAMSDI) that can effectively solve the scarcity problem of target spectra and from which a representative target-background mixed subspace can be learned. We first synthesise target-background mixed spectra based on classical hyperspectral mixing models and then learn a target-background mixed subspace via principal component analysis. Compared with MSD and MSDinter, the learned mixed subspace is more representative as spectral variability of target spectra is explained to the largest extent and it leads to an improvement in computational speed and numerical stability. We demonstrate the efficacy of DAMSD and DAMSDI for subpixel target detection on two public hyperspectral image datasets.","Hyperspectral imaging, Matched subspace detector (MSD), Subpixel target detection, Data augmentation",Xiaochen Yang and Mingzhi Dong and Ziyu Wang and Lianru Gao and Lefei Zhang and Jing-Hao Xue,https://www.sciencedirect.com/science/article/pii/S0031320320302673,https://doi.org/10.1016/j.patcog.2020.107464,0031-3203,2020,107464,106,Pattern Recognition,Data-augmented matched subspace detector for hyperspectral subpixel target detection,article,YANG2020107464,
"Diagrammatic reasoning (DR) problems are well known. However, solving DR problems represented in 4Â ÃÂ 1 Ravenâs Progressive Matrix (RPM) form using computer vision and pattern recognition has not yet been tried. Emergence of deep learning techniques aided by advanced computing can be exploited to solve such DR problems. In this paper, we propose a new learning framework by combining LSTM and Convolutional LSTM to solve 4Â ÃÂ 1 DR problems. Initially, the elementary geometrical shapes in such problems are detected using a typical CNN-based detector. Next, relations of various shapes are analyzed and a high-level feature set is produced and processed in the LSTM framework. A new 4Â ÃÂ 1 DR dataset has been prepared and made available to the research community. We believe, it will be helpful in advancing this research further. We have compared our method with some of the existing frameworks that can be used for solving RPM-guided DR problems. We have recorded 18â20% increase in the average prediction accuracy as compared to the prior frameworks when applied to RPM-guided DR problems. We believe the CV research community will be interested to carry out similar research, particularly to investigate the feasibility of solving other types of known DR problems.","Abstract reasoning, Raven,s Progressive Matrices (RPM), Diagrammatic reasoning, Visual IQ test",Arif Ahmed Sekh and Debi Prosad Dogra and Samarjit Kar and Partha Pratim Roy and Dilip K. Prasad,https://www.sciencedirect.com/science/article/pii/S0031320320302156,https://doi.org/10.1016/j.patcog.2020.107412,0031-3203,2020,107412,106,Pattern Recognition,Can we automate diagrammatic reasoning?,article,SEKH2020107412,
"Domain adaptation is a significant and popular issue of solving distribution discrepancy among different domains in computer vision. Generally, previous works proposed are mainly devoted to reducing domain shift between source domain with labeled data and target domain without labels. Adversarial learning in deep networks has already been widely applied to learn disentangled and transferable features between two different domains to minimize domains distribution discrepancy. However, these methods rarely consider class distributions among source data during adversarial learning, and they pay little attention to these transferable regions among source and target domains images. In this paper, we propose a Generative Attention Adversarial Classification Network (GAACN) model for unsupervised domain adaptation. To learn a joint feature distribution between source and target domains, we present an improved generative adversarial network (GAN) following the feature extractor. Firstly, the discriminator of GAN discriminates the distribution of domains and the classes distribution among source data during adversarial learning, so that our feature extractor can learn a joint feature distribution between source and target domains and maintain the classes consistent simultaneously. Secondly, we present an attention module embedded in GAN, which allows the discriminator to discriminate the transferable regions among the images of source and target domains. Lastly, we propose a simple and efficient method which allocates pseudo-labels for unlabeled target data, and it can improve the performance of our model GAACN while mitigating negative transfer. Extensive experiments demonstrate that our proposed model achieves perfect results on several standard domain adaptation datasets.","Unsupervised domain adaptation, Generated adversarial network, Attention learning, Pseudo labels",Wendong Chen and Haifeng Hu,https://www.sciencedirect.com/science/article/pii/S0031320320302430,https://doi.org/10.1016/j.patcog.2020.107440,0031-3203,2020,107440,107,Pattern Recognition,Generative attention adversarial classification network for unsupervised domain adaptation,article,CHEN2020107440,
"Finding geometric primitives in 3D point clouds is a fundamental task in many engineering applications such as robotics, autonomous-vehicles and automated industrial inspection. Among all solid shapes, cylinders are frequently found in a variety of scenes, comprising natural or man-made objects. Despite their ubiquitous presence, automated extraction and fitting can become challenging if performed âin-the-wildâ, when the number of primitives is unknown or the point cloud is noisy and not oriented. In this paper we pose the problem of extracting multiple cylinders in a scene by means of a Game-Theoretic inlier selection process exploiting the geometrical relations between pairs of axis candidates. First, we formulate the similarity between two possible cylinders considering the rigid motion aligning the two axes to the same line. This motion is represented with a unitary dual-quaternion so that the distance between two cylinders is induced by the length of the shortest geodesic path in SE(3). Then, a Game-Theoretical process exploits such similarity function to extract sets of primitives maximizing their inner mutual consensus. The outcome of the evolutionary process consists in a probability distribution over the sets of candidates (ie axes), which in turn is used to directly estimate the final cylinder parameters. An extensive experimental section shows that the proposed algorithm offers a high resilience to noise, since the process inherently discards inconsistent data. Compared to other methods, it does not need point normals and does not require a fine tuning of multiple parameters.","Cylinder extraction, Dual quaternions, Point clouds, Industrial inspection, Game theory",Filippo Bergamasco and Mara Pistellato and Andrea Albarelli and Andrea Torsello,https://www.sciencedirect.com/science/article/pii/S0031320320302466,https://doi.org/10.1016/j.patcog.2020.107443,0031-3203,2020,107443,107,Pattern Recognition,Cylinders extraction in non-oriented point clouds as a clustering problem,article,BERGAMASCO2020107443,
"Support vector machine (SVM) provides a good classification and regression ability, especially, for small sample learning. However, in practice, the learning ability of implemented SVM is occasionally far from the expected level. Group method of data handling neural network (GMDH-NN) has been applied in various fields for pattern recognition and data mining. It makes it possible to automatically find interrelations in data, to select an optimal structure of network or model and to improve the accuracy of existing algorithms. In this work we propose to take the advantages of GMDH-NN for further increasing the classification performance of SVM. One weakness of the symmetric regularity criterion of GMDH-NN is that if one of the input attributes has a relatively big range, then it may overcome the other attributes. Thus, we first define a standardized symmetric regularity criterion (SSRC) to evaluate and select the candidate models, and optimize a classifier ensemble selection approach. Secondly, we define a novel structure of initial model of GMDH-NN which is from the posterior probability outputs of SVMs. These probabilistic outputs are generated from the improved Plattâs probabilistic outputs. Thirdly, in real classification tasks, different classifiers usually have different classification advantages. So we use probabilistic SVM as base learner and integrate the probabilistic SVMs with GMDH-NN, and then propose a special classifier ensemble selection approach for probabilistic SVM classifiers based on GMDH-NN called GMDH-PSVM. Moreover, we use the Borda sorting and Random weighted Borda sorting to discuss the results of our experiments. Experiments on standard UCI datasets demonstrate the effectiveness of our method.","Probabilistic SVM, Group method of data handling, Ensemble selection, Regularity criterion, Borda sorting",Lixiang Xu and Xiaofeng Wang and Lu Bai and Jin Xiao and Qi Liu and Enhong Chen and Xiaoyi Jiang and Bin Luo,https://www.sciencedirect.com/science/article/pii/S003132032030176X,https://doi.org/10.1016/j.patcog.2020.107373,0031-3203,2020,107373,106,Pattern Recognition,Probabilistic SVM classifier ensemble selection based on GMDH-type neural network,article,XU2020107373,
"For performing multi-class classification, deep neural networks almost always employ a One-vs-All (OvA) classification scheme with as many output units as there are classes in a dataset. The problem of this approach is that each output unit requires a complex decision boundary to separate examples from one class from all other examples. In this paper, we propose a novel One-vs-One (OvO) classification scheme for deep neural networks that trains each output unit to distinguish between a specific pair of classes. This method increases the number of output units compared to the One-vs-All classification scheme but makes learning correct decision boundaries much easier. In addition to changing the neural network architecture, we changed the loss function, created a code matrix to transform the one-hot encoding to a new label encoding, and changed the method for classifying examples. To analyze the advantages of the proposed method, we compared the One-vs-One and One-vs-All classification methods on three plant recognition datasets (including a novel dataset that we created) and a dataset with images of different monkey species using two deep architectures. The two deep convolutional neural network (CNN) architectures, Inception-V3 and ResNet-50, are trained from scratch or pre-trained weights. The results show that the One-vs-One classification method outperforms the One-vs-All method on all four datasets when training the CNNs from scratch. However, when using the two classification schemes for fine-tuning pre-trained CNNs, the One-vs-All method leads to the best performances, which is presumably because the CNNs had been pre-trained using the One-vs-All scheme.","Deep learning, Computer vision, Multi-class classification, One-vs-One classification, Plant recognition",Pornntiwa Pawara and Emmanuel Okafor and Marc Groefsema and Sheng He and Lambert R.B. Schomaker and Marco A. Wiering,https://www.sciencedirect.com/science/article/pii/S0031320320303319,https://doi.org/10.1016/j.patcog.2020.107528,0031-3203,2020,107528,108,Pattern Recognition,One-vs-One classification for deep neural networks,article,PAWARA2020107528,
"Recently, computer vision has achieved remarkable accomplishments in many domains under the thriving of deep learning. Scene flow estimation turns from the classical manual feature construction to the deep convolutional neural network (DCNN) approaches. In this paper, we review recent works about scene flow, mainly focusing on DCNN methods. We present some milestones of scene flow in recent years, and categorize these methods into supervised and unsupervised based methods. Meanwhile, we also review some multi-task methods related to scene flow. At last, we present a performance comparison among different methods.","Scene flow, Optical flow, Depth estimation, Deep learning",Jiajie Liu and Han Li and Ruihong Wu and Qingyun Zhao and Yiyou Guo and Long Chen,https://www.sciencedirect.com/science/article/pii/S0031320320301813,https://doi.org/10.1016/j.patcog.2020.107378,0031-3203,2020,107378,106,Pattern Recognition,A survey on deep learning methods for scene flow estimation,article,LIU2020107378,
"Reflection symmetry is a very commonly occurring feature in both natural and man-made objects, which helps in understanding objects better and makes them visually pleasing. Detection of reflection symmetry is a fundamental problem in the field of computer vision and computer graphics which aids in understanding and representing reflective symmetric objects. In this work, we attempt the problem of detecting the 3D global reflection symmetry of a 3D object represented as a point cloud. The main challenge is to handle outliers, missing parts, and perturbations from the perfect reflection symmetry. We propose a descriptor-free approach, in which, we pose the problem of reflection symmetry detection as an optimization problem and provide a closed-form solution. We show that the proposed method achieves state-of-the-art performance on the standard dataset.","Reflection symmetry, Point cloud, Optimization",Rajendra Nagar and Shanmuganathan Raman,https://www.sciencedirect.com/science/article/pii/S0031320320302867,https://doi.org/10.1016/j.patcog.2020.107483,0031-3203,2020,107483,107,Pattern Recognition,3DSymm: Robust and Accurate 3D Reflection Symmetry Detection,article,NAGAR2020107483,
"In this paper, we propose regularized lightweight deep convolutional neural network models, capable of effectively operating in real-time on-drone for high-resolution video input. Furthermore, we study the impact of hinge loss against the cross entropy loss on the classification performance, mainly in binary classification problems. Finally, we propose a novel regularization method motivated by the Quadratic Mutual Information, in order to improve the generalization ability of the utilized models. Extensive experiments on various binary classification problems involved in autonomous systems are performed, indicating the effectiveness of the proposed models. The experimental evaluation on four datasets indicates that hinge loss is the optimal choice for binary classification problems, considering lightweight deep models. Finally, the effectiveness of the proposed regularizer in enhancing the generalization ability of the proposed models is also validated.","Hinge loss, Cross entropy loss, Binary classification problems, Quadratic mutual information, Regularizer, Lightweight models, Real-time, Convolutional neural networks, Deep learning",Maria Tzelepi and Anastasios Tefas,https://www.sciencedirect.com/science/article/pii/S0031320320302107,https://doi.org/10.1016/j.patcog.2020.107407,0031-3203,2020,107407,106,Pattern Recognition,Improving the performance of lightweight CNNs for binary classification using quadratic mutual information regularization,article,TZELEPI2020107407,
"Automatic building extraction and delineation from airborne LiDAR point cloud data of urban environments is still a challenging task due to the variety and complexity at which buildings appear. The Medial Axis Transform (MAT) is able to describe the geometric shape and topology of an object, but has never been applied for building roof outline extraction. It represents the shape of an object by its centerline, or skeleton structure instead of its boundary. Notably, end points of the MAT in principle coincide with corner points of building outlines. However, the MAT is sensitive to small boundary irregularities, which makes shape detection in airborne point clouds challenging. We propose a robust MAT-based method for detecting building corner points, which are then connected to form a building boundary polygon. First, we approximate the 2D MAT of a set of building edge points acquired by the alpha-shape algorithm to derive a so-called building roof skeleton. We then propose a hierarchical corner-aware segmentation to cluster skeleton points based on their properties which are the so-called separation angle, radius of the maximally inscribe circle, and defining edge point indices. From each segment, a corner point is then estimated by extrapolating the position of the zero radius inscribed circle based on the skeleton point positions within the segment. Our experiment uses point cloud datasets of Makassar, Indonesia and EYE-Amsterdam, The Netherlands. The average positional accuracy of the building outline results for Makassar and EYE-Amsterdam is 65â¯cm and 70â¯cm, respectively, which meet one-meter base map accuracy criteria. The results imply that skeletonization is a promising tool to extract relevant geometric information on e.g. building outlines even from far from perfect geographical point cloud data.","Skeleton, Point cloud, Building outline, Medial axis transform, Segmentation",Elyta Widyaningrum and Ravi Y. Peters and Roderik C. Lindenbergh,https://www.sciencedirect.com/science/article/pii/S0031320320302508,https://doi.org/10.1016/j.patcog.2020.107447,0031-3203,2020,107447,106,Pattern Recognition,Building outline extraction from ALS point clouds using medial axis transform descriptors,article,WIDYANINGRUM2020107447,
"Traditional manifold learning methods generally include a single projection stage that maps high-dimensional data into lower-dimensional space. However, these methods cannot guarantee that the projection matrix is optimal for classification, which limits their practical application. To address this issue, we propose a two-stage projection matrix optimization model termed self-adaptive manifold discriminant analysis (SAMDA). In pre-training projection stage, SAMDA obtains an initial projection matrix by constructing an interclass graph and an intraclass graph under the graph embedding (GE) framework. In weight optimization stage, a maximal manifold margin criterion is developed to further optimize the weights of projection matrix by feature similarity. A self-adaptive optimization process is introduced to increase the margins among different manifolds in low-dimensional space and extract discriminant features that are beneficial to classification. Experimental results on PaviaU, Indian Pines and Heihe data sets demonstrate that the proposed SAMDA method can achieve better classification results than some state-of-the-art methods.","Hyperspectral remote sensing, Feature extraction, Self-adaptive optimization, Manifold margin, Discriminant features",Hong Huang and Zhengying Li and Haibo He and Yule Duan and Song Yang,https://www.sciencedirect.com/science/article/pii/S0031320320302909,https://doi.org/10.1016/j.patcog.2020.107487,0031-3203,2020,107487,107,Pattern Recognition,Self-adaptive manifold discriminant analysis for feature extraction from hyperspectral imagery,article,HUANG2020107487,
"Online multi-object tracking needs to overcome the intrinsic detector deficiencies, e.g., missing detections, false alarms, and inaccurate detection responses, to grow multiple object trajectories without using future information. Various distractions exist during this growing process like background clutters, similar targets, and occlusions, which present a great challenge. We in this work propose a method for learning a distractor-aware discriminative model that can handle continuous missed and inaccurate detection problems due to the occlusion or the motion blur. To deal with target appearance variations, a relational attention learning mechanism is proposed to capture the distinctive target appearances by selectively aggregating features from history states with weights extracted from their appearance topological relationship. Based on the discrimination model, a multi-stage tracking pipeline is designed for automatic trajectory initialization,propagation, and termination. Extensive experimental analyses and comparisons demonstrate its state-of-the-art performance on widely used challenging MOT16 and MOT17 benchmarks. The source code of this work is released to facilitate further studies on the multi-object tracking problem.11Implementation code link: https://github.com/ZongweiZhou1/DDLTracker","Multi-object tracking, Distractor-aware discrimination learning, Relational attention learning",Zongwei Zhou and Wenhan Luo and Qiang Wang and Junliang Xing and Weiming Hu,https://www.sciencedirect.com/science/article/pii/S0031320320303150,https://doi.org/10.1016/j.patcog.2020.107512,0031-3203,2020,107512,107,Pattern Recognition,Distractor-aware discrimination learning for online multiple object tracking,article,ZHOU2020107512,
"Current orthogonal matching pursuit (OMP) algorithms calculate the correlation between two vectors using the inner product operation and minimize the mean square error, which are both suboptimal when there are non-Gaussian noises or outliers in the observation data. To overcome these problems, a new OMP algorithm is developed based on information theoretic learning (ITL), which is built on the following new techniques: (1) an ITL-based correlation (ITL-Correlation) is developed as a new similarity measure which can better exploit higher-order statistics of the data, and is robust against many different types of noise and outliers in a sparse representation framework; (2) a non-second order statistic measurement and minimization method is developed to improve the robustness of OMP by overcoming the limitation of Gaussianity inherent in a cost function based on second-order moments. The experimental results on both simulated and real-world data consistently demonstrate the superiority of the proposed OMP algorithm in data recovery, image reconstruction, and classification.","Orthogonal matching pursuit, Information theoretic learning, ITL-Correlation, Kernel minimization, Data recovery, Image reconstruction, Image classification",Miaohua Zhang and Yongsheng Gao and Changming Sun and Michael Blumenstein,https://www.sciencedirect.com/science/article/pii/S0031320320302181,https://doi.org/10.1016/j.patcog.2020.107415,0031-3203,2020,107415,107,Pattern Recognition,A robust matching pursuit algorithm using information theoretic learning,article,ZHANG2020107415,
"Existing deep trackers can be roughly divided into either matching-based or classification-based methods. The formers are fast but not very robust; while the latter ones introduce more discriminative information but often very slow. In this work, we present a novel real-time robust tracking method to take full use of the benefits from both kinds of networks. First, we propose a matching-classification network switching (MCS) framework to integrate the matching, classification, verification networks and conduct dynamic switching among them. Second, to speed up online update, we devlop a meta learning method as a critical component in our classification network. The meta classifier is trained offline to obtain general discriminative ability and updated online to the current frame just through one iteration. Extensive experiments are conducted on two popular benchmark datasets. Both qualitative and quantitative evaluations show that our tracker performs favorably against other state-of-the-art trackers with real-time performance.","Visual Tracking, Deep Learning, Ensemble learning",Peixia Li and Boyu Chen and Dong Wang and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320320302223,https://doi.org/10.1016/j.patcog.2020.107419,0031-3203,2020,107419,107,Pattern Recognition,Visual tracking by dynamic matching-classification network switching,article,LI2020107419,
"Just like its remarkable achievements in many computer vision tasks, the convolutional neural networks (CNN) provide an end-to-end solution in handwritten Chinese character recognition (HCCR) with great success. However, the process of learning discriminative features for image recognition is difficult in cases where little data is available. In this paper, we propose a matching network which builds a connection between template characters and handwritten characters inspired by the human learning process of writing Chinese characters. The matching network replaces the parameters in the softmax regression layer with the features extracted from the template character images. After the training process has been finished, the powerful discriminative features help us to generalize the predictive power not just to new data, but to entire new Chinese characters that never appear in the training set before. Experiments performed on the ICDAR-2013 offline HCCR datasets have shown that the proposed method achieves a comparable performance to current CNN-based classifiers. Besides, the matching network has a very promising generalization ability to new Chinese characters that never appear in the existing training set.","HCCR, CNN, Matching Network, Deep Learning",Zhiyuan Li and Qi Wu and Yi Xiao and Min Jin and Huaxiang Lu,https://www.sciencedirect.com/science/article/pii/S0031320320302740,https://doi.org/10.1016/j.patcog.2020.107471,0031-3203,2020,107471,107,Pattern Recognition,Deep Matching Network for Handwritten Chinese Character Recognition,article,LI2020107471,
"The large margin distribution machine (LDM) combines the working principle of support vector machine (SVM) and the margin distribution to directly improve the algorithm's generalization. The margin distribution can be expressed with the margin mean and margin variance. It has been proved to be an efficient algorithm for binary classification. Inspired by the LDM, a novel classifier termed as LMD-NPSVM is proposed to improve the generalization performance of the nonparallel support vector machine (NPSVM) in this paper. Firstly, to meet the structure of NPSVM, the large margin distribution is reconstructed. Then, the linear LMD-NPSVM is built by introducing the reconstructed margin distribution into NPSVM. In addition, the linear case is extended to the nonlinear case with a kernel trick. All experiments show that our LMD-NPSVM is superior to the state-of-the-art algorithms in generalization performance.","Pattern classification, Nonparallel support vector machine, Margin distribution, Generalization performance",Liming Liu and Maoxiang Chu and Rongfen Gong and Yongcheng Peng,https://www.sciencedirect.com/science/article/pii/S0031320320301771,https://doi.org/10.1016/j.patcog.2020.107374,0031-3203,2020,107374,106,Pattern Recognition,Nonparallel support vector machine with large margin distribution for pattern classification,article,LIU2020107374,
"Early exits are capable of providing deep learning models with adaptive computational graphs that can readily adapt on-the-fly to the available resources. Despite their advantages, existing early exit methods suffer from many limitations which limit their performance, e.g., they ignore the information extracted from previous exit layers, they are unable to efficiently handle feature maps with large sizes, etc. To overcome these limitations we propose a Bag-of-Features (BoF)-based method that is capable of constructing efficient hierarchical early exit layers with minimal computational overhead, while also providing an adaptive inference method that allows for early stopping the inference process when the network is confident enough for its output, leading to significant performance benefits. To this end, the BoF model is extended and adapted to the needs of early exits by constructing additive shared histogram spaces that gradually refine the information extracted from the various layers of a network, in a hierarchical manner, while also employing a classification layer reuse strategy to further reduce the number of parameters needed per exit layer. Note that the proposed method is generic and can be readily combined with any neural network architecture. The effectiveness of the proposed method is demonstrated using five different image datasets, proving that early exits can be readily transformed into a practical tool, which can be effectively used in various real-world embedded applications.","Adaptive inference, Early exits, Bag-of-Features, Deep convolutional neural networks, Hierarchical representations",Nikolaos Passalis and Jenni Raitoharju and Anastasios Tefas and Moncef Gabbouj,https://www.sciencedirect.com/science/article/pii/S0031320320301497,https://doi.org/10.1016/j.patcog.2020.107346,0031-3203,2020,107346,105,Pattern Recognition,Efficient adaptive inference for deep convolutional neural networks using hierarchical early exits,article,PASSALIS2020107346,
"In cross-dataset person re-identification, it is challenging to address the problem of domain shift between training and test data. Although unsupervised domain adaptation methods have been developed, the performance is still much weaker compared with that of supervised methods because these models cannot follow a supervised optimization in unlabeled target domains. To address this problem, a transductive structure alignment-based self-reconstruction dictionary learning approach is proposed in this paper for cross-dataset person re-identification (PRID). Specifically, visual-attribute embedding is first learned to achieve knowledge transfer from the source domain to the target domain. In this process, visual-attribute structures are aligned via class prototype dictionaries to promote the discrimination of predicted semantic attributes by exploiting structure information between the visual feature and class prototype. Moreover, to mitigate domain shift, domain-invariant visual-attribute self-reconstruction is integrated into our dictionary learning framework. An identifier is then constructed by integrating the discriminativeness of attribute and compatibility matrix shared both source domain and target domain. Finally, the pre-learned model is tuned by selecting samples from the target domain which are not labeled but assigned pseudo-labels. Extensive experimental results on benchmark datasets show that our approach outperforms several state-of-the-art approaches.","Person re-identification, Self-supervised strategy, Domain adaptation, Structure alignment, Self-reconstruction",Huafeng Li and Zhenyu Kuang and Zhengtao Yu and Jiebo Luo,https://www.sciencedirect.com/science/article/pii/S003132032030217X,https://doi.org/10.1016/j.patcog.2020.107414,0031-3203,2020,107414,106,Pattern Recognition,Structure alignment of attributes and visual features for cross-dataset person re-identification,article,LI2020107414,
"Statistical models have been widely adopted for image set classification owing to their capacity in characterizing the data distribution more flexibly and faithfully. However, these methods typically suffer from the problem that the query image set has weak statistical correlations with the training sets, which leads to larger fluctuations in performance. To address this problem, we propose a semi-supervised fuzzy discriminative learning framework based on Log-Euclidean multivariate Gaussians descriptor to facilitate more robust image set classification. Specifically, by using the semi-supervised setting which definitely has access to the labeled training data and the available unlabeled testing data, we adopt manifold distance metric to construct a âfully trustedâ graph and derive two new data dependent probabilistic kernels to strongly reflect the underlying connection relationships between the training and query Gaussian manifold components. The resulted kernel representations are eventually integrated into a kernel fuzzy discriminant framework to enhance the compactness of intra-class Gaussian components and enlarge the margin for inter-class Gaussian components. Thus, more discriminating power of our learning machine is obtained for the classification of the query image set. Extensive experiments on several datasets well demonstrate the effectiveness of the proposed method compared with other image set algorithms.","Semi-supervised learning, Data dependent kernel, Gaussian descriptor, Image set classification, Fuzzy discriminant analysis",Wenzhu Yan and Quansen Sun and Huaijiang Sun and Yanmeng Li,https://www.sciencedirect.com/science/article/pii/S0031320320303034,https://doi.org/10.1016/j.patcog.2020.107500,0031-3203,2020,107500,107,Pattern Recognition,Semi-supervised learning framework based on statistical analysis for image set classification,article,YAN2020107500,
"In this study, we present an updated predictor DimiG 2.0, which uses a semi-supervised multi-label graph convolutional network (GCN) to infer disease-associated microRNAs (miRNAs) on an interaction network between protein coding genes (PCGs) and miRNAs using disease-PCG associations. DimiG 2.0 benefits from integrating the hierarchy of diseases into the GCN. DimiG 2.0 has the following updates: 1) It incorporates the hierarchy of diseases to regularize the GCN, encouraging diseases in the hierarchy to share similar miRNAs. 2) It integrates the PCGs with interacting partners but without associated diseases into model training, these unlabeled PCGs increase the size of the constructed interaction network. 3) It is able to predict associated miRNAs for 1017 diseases (updated from 248). 4) It updates expression data across tissues from the latest GTEx v7, and the expression values are quantified in Transcripts Per Million (TPM). Our results show that DimiG 2.0 outperforms state-of-the-art semi-supervised and supervised methods on the constructed benchmarked sets.","microRNAs, Protein coding genes, Interaction network, Graph convolutional network, Disease hierarchy",Xiaoyong Pan and Hong-Bin Shen,https://www.sciencedirect.com/science/article/pii/S0031320320301886,https://doi.org/10.1016/j.patcog.2020.107385,0031-3203,2020,107385,105,Pattern Recognition,Scoring disease-microRNA associations by integrating disease hierarchy into graph convolutional networks,article,PAN2020107385,
"Skeleton-based action recognition aims to recognize human actions by exploring the inherent characteristics from the given skeleton sequences and has attracted far more attention due to its great important potentials in practical applications. Previous methods have illustrated that learning discriminative spatial and temporal features from the skeleton sequences is a crucial factor to recognize human actions. Nevertheless, how to model spatio-temporal evolutions is still a challenging problem. In this work, we propose a novel model with hierarchical spatial reasoning and temporal stack learning network (HSR-TSL) to explore the discriminative spatial and temporal features for human action recognition, which consists of a hierarchical spatial reasoning network (HSRN) and a temporal stack learning network (TSLN). Specifically, the HSRN employs a hierarchical residual graph neural network to capture two-level spatial features: intra spatial information of each part and body-level structural information between each part. The TSLN models the detailed temporal dynamics of skeleton sequences by a composition of multiple skip-clip LSTMs. During training, we develop a clip-based incremental loss to effectively optimize the model. We perform extensive experiments on five challenging benchmarks to verify the effectiveness of each component of our model. The comparison results illustrate that our approach significantly boosts the performances for skeleton-based action recognition.","Skeleton-based action recognition, Hierarchical spatial reasoning, Temporal stack learning, Clip-based incremental loss",Chenyang Si and Ya Jing and Wei Wang and Liang Wang and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S0031320320303149,https://doi.org/10.1016/j.patcog.2020.107511,0031-3203,2020,107511,107,Pattern Recognition,Skeleton-based action recognition with hierarchical spatial reasoning and temporal stack learning network,article,SI2020107511,
"Image registration is a prerequisite for infrared (IR) and visible (VIS) image fusion. In practical application, most scenes are not planar and there is significant distinctness between IR and VIS cameras. Therefore, for non-rigid IR and VIS image registration, non-linear transformation is more applicable than affine transformation. Typically, non-linear transformation is modeled with point feature. However, this can degrade the generalization ability of transformation model and increase computational complexity. Aim at this problem, we propose an enhanced affine transformation (EAT) for non-rigid IR and VIS image registration. In this paper, image registration is transformed into point set registration and then the optimal EAT model constructed by global deformation is estimated from local feature. At first, a Gaussian-fields-based objective function is established and simplified by using the potential correspondence between an image pair. With the combination of affine and polynomial transformation, the EAT model is then proposed to describe the regular pattern of non-rigid and global deformation between an image pair. Finally, a coarse-to-fine strategy based on quasi-Newton method is designed and applied to determine the optimal transformation coefficients from edge point feature of IR and VIS images, in order to accomplish non-rigid image registration. The qualitative and quantitative comparisons on synthesized point sets and real images demonstrate that the proposed method is superior over the state-of-the-art methods in the accuracy and efficiency of image registration.","Registration, Non-rigid transformation, Infrared image, Image fusion",Chaobo Min and Yan Gu and Yingjie Li and Feng Yang,https://www.sciencedirect.com/science/article/pii/S0031320320301801,https://doi.org/10.1016/j.patcog.2020.107377,0031-3203,2020,107377,106,Pattern Recognition,Non-rigid infrared and visible image registration by enhanced affine transformation,article,MIN2020107377,
"Ensemble learning is an effective technique to learn the information of data by combining multiple models. But usually the combined models are supervised learning algorithms which need a lot of labeled data to tune their parameters. Some ensemble learning algorithms were proposed to exploit the information of unlabeled data. These methods had to learn the samples with pseudo-labels due to the scarcity of labeled data. But itâs inevitable for the samples with pseudo-labels to bring wrong information during training process. In this paper, we will propose a novel graph-based boosting (GBB) algorithm to learn labeled and unlabeled data. GBB is a framework combining many models linearly. And pseudo-labels will not occur during training process. GBB will assign a new weighting vector for the labeled samples and a transformed similarity matrix for all samples to train the combined model at each iteration. We also extend GBB, termed as weighted GBB (WGBB), to learn imbalanced data by adding a weighting vector for the labeled data. Finally, 14 relatively balanced datasets and 22 imbalanced datasets are used to validate the performances of GBB and WGBB respectively. Experimental results illustrate that GBB can achieve a competitive performance and WGBB has an obvious advantage to handle classification problem of imbalanced data, comparing with other related algorithms.","Graph, Boosting, Semi-supervised learning, Imbalance learning",Zheng Liu and Wei Jin and Ying Mu,https://www.sciencedirect.com/science/article/pii/S003132032030220X,https://doi.org/10.1016/j.patcog.2020.107417,0031-3203,2020,107417,106,Pattern Recognition,Graph-based boosting algorithm to learn labeled and unlabeled data,article,LIU2020107417,
"This paper considers the problem of positive unlabeled (PU) learning. In this context, we propose a two-stage GAN-based model. More specifically, the main contribution is to incorporate a biased PU risk within the standard GAN discriminator loss function. In this manner, the discriminator is constrained to steer the generator to converge towards the unlabeled samples distribution while diverging from the positive samples distribution. Consequently, the proposed model, referred to as D-GAN, exclusively learns the counter-examples distribution without prior knowledge. Experimental results on simple and complex image datasets demonstrate that our approach outperforms state-of-the-art PU methods without prior by overcoming issues such as sensitivity to prior knowledge or first-stage overfitting.","Generative adversarial networks (GANs), Generative models, Semi-supervised learning, Partially supervised learning, Deep learning",Florent Chiaroni and Ghazaleh Khodabandelou and Mohamed-Cherif Rahal and Nicolas Hueber and Frederic Dufaux,https://www.sciencedirect.com/science/article/pii/S0031320320303307,https://doi.org/10.1016/j.patcog.2020.107527,0031-3203,2020,107527,107,Pattern Recognition,Counter-examples generation from a positive unlabeled image dataset,article,CHIARONI2020107527,
"Unconstrained text recognition is an important computer vision task, featuring a wide variety of different sub-tasks, each with its own set of challenges. One of the biggest promises of deep neural networks has been the convergence and automation of feature extractors from input raw signals, allowing for the highest possible performance with minimum required domain knowledge. To this end, we propose a data-efficient, end-to-end neural network model for generic, unconstrained text recognition. In our proposed architecture we strive for simplicity and efficiency without sacrificing recognition accuracy. Our proposed architecture is a fully convolutional network without any recurrent connections trained with the CTC loss function. Thus it operates on arbitrary input sizes and produces strings of arbitrary length in a very efficient and parallelizable manner. We show the generality and superiority of our proposed text recognition architecture by achieving state-of-the-art results on seven public benchmark datasets, covering a wide spectrum of text recognition tasks, namely: Handwriting Recognition, CAPTCHA recognition, OCR, License Plate Recognition, and Scene Text Recognition. Our proposed architecture has won the ICFHR2018 Competition on Automated Text Recognition on a READ Dataset.","Text recognition, Optical character recognition, Handwriting recognition, CAPTCHA Solving, License plate recognition, Convolutional neural network, Deep learning",Mohamed Yousef and Khaled F. Hussain and Usama S. Mohammed,https://www.sciencedirect.com/science/article/pii/S0031320320302855,https://doi.org/10.1016/j.patcog.2020.107482,0031-3203,2020,107482,108,Pattern Recognition,"Accurate, data-efficient, unconstrained text recognition with convolutional neural networks",article,YOUSEF2020107482,
"The Coyote Optimization Algorithm (COA) is a bio-inspired optimization algorithm based on the intelligent behavior of coyotes. COA was proposed recently and it considers the social organization of the coyotes and its adaptation to the environment in order to solve continuous optimization problems. In addition, it is a population-based algorithm and it can be classified as both, swarm intelligence and evolutionary heuristics, because contributes with a different algorithmic structure. This paper proposes a binary version of the COA, named Binary COA (BCOA) applying to select the optimal feature subset for classification, based on the hyperbolic transfer function in a wrapper model. By this way, the features are selected based on the performance evaluation of a classification algorithm. We tested the effectiveness of the BCOA wrapper with the NaÃ¯ve Bayes classifier and were used seven public domain benchmark datasets to compare the proposed approach in terms of classification accuracy, number of selected features and computational cost with other state-of-art algorithms of the literature. The results shown that BCOA was able to find subsets with few features while it still performs well in terms of classification accuracy.","Wrapper feature selection, Classification, Coyote optimization algorithm (COA), Bio-inspired optimization;Metaheuristics, Binary COA",Rodrigo Clemente {Thom de Souza} and Camila Andrade {de Macedo} and Leandro {dos Santos Coelho} and Juliano Pierezan and Viviana Cocco Mariani,https://www.sciencedirect.com/science/article/pii/S0031320320302739,https://doi.org/10.1016/j.patcog.2020.107470,0031-3203,2020,107470,107,Pattern Recognition,Binary coyote optimization algorithm for feature selection,article,THOMDESOUZA2020107470,
"For many real-world applications, predicting a price range is more practical and desirable than predicting a concrete value. In this case, price prediction can be regarded as a classification problem. Although deep forest is recognized as the best solution to many classification problems, a crucial issue limits its direct application to price prediction, i.e., it treated all the misclassifications equally no matter how far away they are from the real classes, since their impacts on the accuracy are the same. This is unreasonable to price prediction as the misclassification should be as close to the real price range as possible even if they have to be wrongly classified. To address this issue, we propose a cost-sensitive deep forest for price prediction, which maintains the high accuracy of deep forest, and propels the misclassifications to be closer to the real price range to reduce the cost of misclassifications. To make the classification more meaningful, we develop a discretization method to pre-define the classes of price, by modifying the conventional K-means method. The experimental results based on multiple real-world datasets (i.e., car sharing, house renting and real estate selling) show that, the cost-sensitive deep forest can significantly reduce the cost in comparison with the conventional deep forest and other baselines, while keeping satisfactory accuracy.","Cost-sensitive Deep Forest, Ensemble Deep Learning, Price Prediction, Modified K-means",Chao Ma and Zhenbing Liu and Zhiguang Cao and Wen Song and Jie Zhang and Weiliang Zeng,https://www.sciencedirect.com/science/article/pii/S0031320320303022,https://doi.org/10.1016/j.patcog.2020.107499,0031-3203,2020,107499,107,Pattern Recognition,Cost-sensitive deep forest for price prediction,article,MA2020107499,
"We present a novel training strategy that allows convolutional encoder-decoder networks, to complete partially observed data by means of hallucination. As input, it takes data from a partially observed domain, for which no complete ground truth is available, and data from an unpaired prior knowledge domain and trains the network in an end-to-end manner. This strategy is demonstrated for the task of completing 2-D road layouts as well as 3-D vehicle shapes. In contrast to alternative approaches, our strategy is compatible with networks that use skip connections, to improve detail in the completed output, while not requiring adversarial supervision. To demonstrate its benefits, our training strategy is benchmarked against two state-of-the-art baselines, one using a two-step auto-encoder training strategy and one using an adversarial strategy. Our novel strategy achieves an improvement up to +12% F-measure on the Cityscapes dataset. The learned network intrinsically generalizes better than the baselines on unseen datasets, which is demonstrated by an improvement up to +24% F-measure on the unseen KITTI dataset. Moreover, our approach outperforms the baselines using the same backbone network on the 3-D shape completion benchmark by reducing the Hamming distance with 15%.","Completion, Partial observation, Weak supervision, Prior knowledge",Chenyang Lu and Gijs Dubbelman,https://www.sciencedirect.com/science/article/pii/S0031320320302296,https://doi.org/10.1016/j.patcog.2020.107426,0031-3203,2020,107426,107,Pattern Recognition,Learning to complete partial observations from unpaired prior knowledge,article,LU2020107426,
"Most of the classification approaches assume that the sample distribution among classes is balanced. Still, such an assumption leads to biased performance over the majority class. This paper proposes an enhanced automatic twin support vector machine â (EATWSVM) to deal with imbalanced data, which incorporates a kernel representation within a TWSVM-based optimization. To learn the kernel function, we impose a Gaussian similarity, ruled by a Mahalanobis distance, and couple a centered kernel alignment-based approach to improving the data separability. Besides, we suggest a suitable range to fix the regularization parameters concerning both the datasetâ imbalance ratio and overlap. Lastly, we adopt One-vs-One and One-vs-Rest frameworks to extend our EATWSVM formulation for multi-class tasks. Obtained results on synthetic and real-world datasets show that our approach outperforms state-of-the-art methods concerning classification performance and training time.","Imbalanced data, Kernel methods, Twin support vector machines",C. Jimenez-CastaÃ±o and A. Alvarez-Meza and A. Orozco-Gutierrez,https://www.sciencedirect.com/science/article/pii/S0031320320302454,https://doi.org/10.1016/j.patcog.2020.107442,0031-3203,2020,107442,107,Pattern Recognition,Enhanced automatic twin support vector machine for imbalanced data classification,article,JIMENEZCASTANO2020107442,
"In theory, graph matching is a combinatorial problem. One state-of-the-art technique in graph matching, called spectral matching, relaxes the matching problem for consistent correspondence into spectral decomposition of the affinity matrix of graphs, but the most variations of spectral based algorithms suffer from their O(n4) memory requirement. In this paper we propose a probabilistic spectral matching approach, in which the graph matching problem is formulated as an ergodic Markov chain, and the process of matching is addressed to reach the steady-state of the Markov chain. The approach decomposes the probability transition matrix, and solves the matching problem in O(n2) space complexity using limited computing resource and RAM. This property makes the approach suitable for super-large graphs matching (for example, graphs with the number of points over 1000). We evaluate our algorithm on both the synthetic and the real datasets, and demonstrate that the proposed approach is significantly faster, and consumes smaller memory than SM, RRWM and FaSM with no loss of accuracy.","Spectral matching, Graph matching, Ergodic markov chain, Space complexity",Yali Zheng and Lili Pan and Jiye Qian and Hongliang Guo,https://www.sciencedirect.com/science/article/pii/S0031320320302211,https://doi.org/10.1016/j.patcog.2020.107418,0031-3203,2020,107418,106,Pattern Recognition,Fast matching via ergodic markov chain for super-large graphs,article,ZHENG2020107418,
"A novel optimization framework for joint unsupervised clustering and kernel learning is derived. Sparse nonnegative matrix factorization of kernel covariance matrices is utilized to categorize data according to their information content. It is demonstrated that a pertinent kernel covariance matrix for clustering can be constructed such that it is block diagonal within arbitrary row and column permutations, while each diagonal block has rank one. To achieve this, a linear combination of a dictionary of kernels is sought such that it has rank equal to the number of clusters while a certain kernel eigenvalue is maximized by a novel difference of convex functions formulation. We establish that the proposed algorithm converges to a stationary solution. Numerical tests with different datasets demonstrate the effectiveness of the proposed scheme whose performance is very close to supervised methods, and performs better than unsupervised alternatives without the need of painstaking parameter tuning.","Clustering, Matrix factorization, Correlation analysis, Kernel learning",Akshay Malhotra and Ioannis D. Schizas,https://www.sciencedirect.com/science/article/pii/S0031320320303216,https://doi.org/10.1016/j.patcog.2020.107518,0031-3203,2020,107518,108,Pattern Recognition,On unsupervised simultaneous kernel learning and data clustering,article,MALHOTRA2020107518,
"Hashing has gained great attention in large-scale image retrieval due to efficient storage and fast search. Recently, many deep hashing approaches have achieved good results since deep neural network owns powerful learning capability. However, these deep hashing approaches can perform deep features learning and binary-like codes learning synchronously, the information loss between binary-like codes and binary codes will increase due to the binarization operation. A further deficiency is that binary-like codes learning based on deep feature representations is a shallow learning procedure, which cannot fully exploit deep feature representations to generate hash codes. To solve the above problems, we propose a Deep Learning Supervised Hashing (DLSH) method which adopts deep structure to learn binary codes based on deep feature representations for large-scale image retrieval. Specifically, we integrate deep features learning module, deep mapping module and binary codes learning module in one unified architecture. The network is trained in an end-to-end way. In addition, a new objective function is designed to preserve the balancing property and semantic similarity of binary codes by incorporating the semantic similarity term and the balanceable property term. Experimental results on four benchmarks demonstrate that the proposed approach outperforms several state-of-the-art hashing methods.","Image retrieval, Supervised hashing, CNN, RNN, Deep learning",Yaxiong Chen and Xiaoqiang Lu and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320320301710,https://doi.org/10.1016/j.patcog.2020.107368,0031-3203,2020,107368,105,Pattern Recognition,Supervised deep hashing with a joint deep network,article,CHEN2020107368,
"Local binary patterns (LBP) are considered to be one of the most computationally efficient descriptor that can also be combined jointly among different variants to increase accuracy. In this study, we propose a method to obtain more discriminative 2D LBP features by optimizing projections of a joint LBP distribution onto the marginal histograms. To find a more efficient representation of the feature vector, we seek the least redundant marginal histograms of a joint LBP distribution via optimizing several constraints. In this way, we aim to have a more compact yet accurate feature vector in contrast to the methods that flatten the joint distribution. Experiments we perform on five popular texture datasets show that the feature vectors optimized with the proposed method provide higher recognition rates with the same size vectors and comparable results even with lower dimensional vectors. We also compare the proposed algorithm to more recent texture recognition methods based on convolutional neural networks and show that it can still provide comparable results even though the resulting feature vectors are smaller by orders of magnitude.","Local binary patterns, Texture recognition, Feature representations, Feature optimization, Deep texture features, Mutual information",Llukman Cerkezi and Cihan Topal,https://www.sciencedirect.com/science/article/pii/S0031320320302764,https://doi.org/10.1016/j.patcog.2020.107473,0031-3203,2020,107473,107,Pattern Recognition,Towards more discriminative features for texture recognition,article,CERKEZI2020107473,
"We present a new approach based on the Slope Chain Code to determine whether a curve is rotational symmetrical and its order of symmetry. The proposed approach works for open and closed perfectly symmetrical or quasi-symmetrical 2D curves. Simple operations on the SCC and its invariant properties are central to our methodology. To evaluate the proposed methodology, we use 1400 curves from a public database. For the symmetrical/asymmetrical classification task, a recall (R) of 0.86, a balanced accuracy (BA) of 0.92, and a precision (P) of 0.87 were obtained. For the quasi-symmetrical/quasi-asymmetrical classification task, R=0.77, BA=0.83, and P=0.70 were obtained. For the order of rotational symmetry detection task, the following performance was achieved: R=0.97, BA=0.98, and P=0.95 for a symmetrical set of curves, and R=0.98, BA=0.98, and P=0.90 for a quasi-symmetrical set of curves. We conclude our presentation demonstrating the usefulness of our methodology with three practical applications","Rotational-symmetry detection, Slope chain code, Chain coding, 2D Curves",Wendy Aguilar and Montserrat Alvarado-Gonzalez and Edgar GarduÃ±o and Carlos Velarde and Ernesto Bribiesca,https://www.sciencedirect.com/science/article/pii/S0031320320302247,https://doi.org/10.1016/j.patcog.2020.107421,0031-3203,2020,107421,107,Pattern Recognition,Detection of rotational symmetry in curves represented by the slope chain code,article,AGUILAR2020107421,
"Density peaks clustering (DPC) is as an efficient clustering algorithm due for using a non-iterative process. However, DPC and most of its improvements suffer from the following shortcomings: (1) highly sensitive to its cutoff distance parameter, (2) ignoring the local structure of data in computing local densities, (3) using a crisp kernel to calculate local densities, and (4) suffering from the cause of chain reaction. To address these issues, in this paper a new method called DPC-DBFN is proposed. The proposed method uses a fuzzy kernel for improving separability of clusters and reducing the impact of outliers. DPC-DBFN uses a density-based kNN graph for labeling backbones. This strategy prevents the chain reaction and effectively assigns true labels to those instances located on the border regions to effectively cluster data with various shapes and densities. The DPC-DBFN is evaluated on some real-world and synthetic datasets. The experimental results show the effectiveness and robustness of the proposed algorithm.","Fuzzy kernel, Density peaks clustering, Noise detection, Label propagation",Abdulrahman Lotfi and Parham Moradi and Hamid Beigy,https://www.sciencedirect.com/science/article/pii/S0031320320302521,https://doi.org/10.1016/j.patcog.2020.107449,0031-3203,2020,107449,107,Pattern Recognition,Density peaks clustering based on density backbone and fuzzy neighborhood,article,LOTFI2020107449,
"Channel pruning is an important method to speed up CNN modelâs inference. Previous filter pruning algorithms regard importance evaluation and model fine-tuning as two independent steps. This paper argues that combining them into a single end-to-end trainable system will lead to better results. We propose an efficient channel selection layer, namely AutoPruner, to find less important filters automatically in a joint training manner. Our AutoPruner takes previous activation responses as an input and generates a true binary index code for pruning. Hence, all the filters corresponding to zero index values can be removed safely after training. By gradually erasing several unimportant filters, we can prevent an excessive drop in model accuracy. Compared with previous state-of-the-art pruning algorithms (including training from scratch), AutoPruner achieves significantly better performance. Furthermore, ablation experiments show that the proposed novel mini-batch pooling and binarization operations are vital for the success of model pruning.","Neural network pruning, Model compression, CNN acceleration",Jian-Hao Luo and Jianxin Wu,https://www.sciencedirect.com/science/article/pii/S0031320320302648,https://doi.org/10.1016/j.patcog.2020.107461,0031-3203,2020,107461,107,Pattern Recognition,AutoPruner: An end-to-end trainable filter pruning method for efficient deep model inference,article,LUO2020107461,
"In this paper, we investigate the problem of Temporal Action Proposal (TAP) generation, which plays a fundamental role in large-scale untrimmed video analysis but remains largely unsolved. Most of the prior works proposed the temporal actions by predicting the temporal boundaries or actionness scores of video units. Nevertheless, context information among surrounding video units has not been adequately explored, which may result in severe loss of information. In this work, we propose a context-aware temporal action proposal network which makes full use of the contextual information in two aspects: 1) To generate initial proposals, we design a Bi-directional Parallel LSTMs to extract the visual features of a video unit by considering its contextual information. Therefore, the prediction of temporal boundaries and actionness scores will be more accurate because it knows what happened in the past and what will happen in the future; and 2) To refine the initial proposals, we design an action-attention based re-ranking network which considers both surrounding proposal and initial actionness scores to assign true action proposals with high confidence scores. Extensive experiments are conducted on two challenging datasets for both temporal action proposal generation and detection tasks, demonstrating the effectiveness of the proposed approach. In particular, on THUMOSâ14 dataset, our method significantly surpasses state-of-the-art methods by 7.73% on AR@50. Our code is released at: https://github.com/Rheelt/TAPG.","Temporal action proposal generation and detection, Deep learning, Untrimmed video analysis",Lianli Gao and Tao Li and Jingkuan Song and Zhou Zhao and Heng Tao Shen,https://www.sciencedirect.com/science/article/pii/S0031320320302806,https://doi.org/10.1016/j.patcog.2020.107477,0031-3203,2020,107477,107,Pattern Recognition,Play and rewind: Context-aware video temporal action proposals,article,GAO2020107477,
"Image inpainting refers to the process of restoring the mask regions of damaged images. Existing inpainting algorithms have exhibited outstanding performance on certain inpainting tasks that are focused on recovering small masks or square masks. Tasks that attempt to reconstruct large proportion of damaged images can still be improved. Although many attention-related algorithms have been proposed to solve image inpainting tasks, most of them ignore the requirements to balancing the detail and style level. In this paper, we propose a novel image inpainting method for large-scale irregular masks. We introduce a special multistage attention module that considers structure consistency and detail fineness. The proposed multistage attention module operates in a coarse to-fine manner, where the early stage performs large feature patch swapping and ensures the global consistency in images, and the next stage swaps small patches to refine the texture. Then, we adopt a partial convolution strategy to avoid the misuse of invalid data during convolution. Several losses are combined as the training objective function to generate excellent results with global consistency and exquisite detail. Qualitative and quantitative experiments on the Paris StreetView, CelebA, and Places2 datasets demonstrate the superior performance of the proposed approach compared with state-of-the-art models.","Image inpainting, Irregular mask, Deep learning, Attention mechanism, Unet-like network",Ning Wang and Sihan Ma and Jingyuan Li and Yipeng Zhang and Lefei Zhang,https://www.sciencedirect.com/science/article/pii/S003132032030251X,https://doi.org/10.1016/j.patcog.2020.107448,0031-3203,2020,107448,106,Pattern Recognition,Multistage attention network for image inpainting,article,WANG2020107448,
"In this paper, we propose a novel scene text detection method named TextMountain. The key idea of TextMountain is making full use of border-center information. Different from previous works that treat center-border as a binary classification problem, we predict text center-border probability (TCBP) and text center-direction (TCD). The TCBP is just like a mountain whose top is text center and foot is text border. The mountaintop can separate text instances which cannot be easily achieved using semantic segmentation map and its rising direction can plan a road to top for each pixel on mountain foot at the group stage. The TCD helps TCBP learning better. Our label rules will not lead to the ambiguous problem with the transformation of angle, so the proposed method is robust to multi-oriented text and can also handle well curved text. In inference stage, each pixel at the mountain foot needs to search the path to the mountaintop and this process can be efficiently completed in parallel, yielding the efficiency of our method compared with others. The experiments on MLT, ICDAR2015, RCTW-17 and SCUT-CTW1500 datasets demonstrate that the proposed method achieves better or comparable performance in terms of both accuracy and efficiency. It is worth mentioning our method achieves an F-measure of 76.85% on MLT which outperforms the previous methods by a large margin. Code will be made available.","Scene text detection, Curved text, Multi-oriented text, CNN, Deep learning",Yixing Zhu and Jun Du,https://www.sciencedirect.com/science/article/pii/S0031320320301394,https://doi.org/10.1016/j.patcog.2020.107336,0031-3203,2021,107336,110,Pattern Recognition,TextMountain: Accurate scene text detection via instance segmentation,article,ZHU2021107336,
"This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a Dirichlet prior. To infer the parameters of DirVAE, we utilize the stochastic gradient method by approximating the inverse cumulative distribution function of the Gamma distribution, which is a component of the Dirichlet distribution. This approximation on a new prior led an investigation on the component collapsing, and DirVAE revealed that the component collapsing originates from two problem sources: decoder weight collapsing and latent value collapsing. The experimental results show that 1) DirVAE generates the result with the best log-likelihood compared to the baselines; 2) DirVAE produces more interpretable latent values with no collapsing issues which the baselines suffer from; 3) the latent representation from DirVAE achieves the best classification accuracy in the (semi-)supervised classification tasks on MNIST, OMNIGLOT, COIL-20, SVHN, and CIFAR-10 compared to the baseline VAEs; and 4) the DirVAE augmented topic models show better performances in most cases.","Representation learning, Variational autoencoder, Deep generative model, Multi-modal latent representation, Component collapse",Weonyoung Joo and Wonsung Lee and Sungrae Park and Il-Chul Moon,https://www.sciencedirect.com/science/article/pii/S0031320320303174,https://doi.org/10.1016/j.patcog.2020.107514,0031-3203,2020,107514,107,Pattern Recognition,Dirichlet Variational Autoencoder,article,JOO2020107514,
"Objective Clinical and dermoscopy images (multi-modality image pairs) are routinely used sequentially in the assessment of skin lesions. Clinical images characterize a lesion's geometry and color; dermoscopy depicts vascularity, dots and globules from the sub-surface of the lesion. Together these modalities provide labels to characterize a skin lesion. Recently, convolutional neural networks (CNNs), due to the ability to learn low-level features and high-level semantic information in an end-to-end architecture, have been shown to be the state-of-the-art in skin lesion classification. Most of the CNN methods have relied on dermoscopy alone. In the few published papers that support multi-modalities, the methods are based on âlate-fusionâ to integrate extracted clinical and dermoscopy image features separately. These late-fusion methods tend to ignore the accessible complementary image features between the paired images at the early stage of the CNN architecture. Methods We propose a hyper-connected CNN (HcCNN) to classify skin lesions. Compared to existing multi-modality CNNs, our HcCNN has an additional hyper-branch that integrates intermediary image features in a hierarchical manner. The hyper-branch enables the network to learn more complex combinations between the images at all, early and late, stages of the network. We also coupled the HcCNN with a multi-scale attention block (MsA) to prioritize semantically important subtle regions in the two modalities across various image scales. Results Our HcCNN achieved an average accuracy of 74.9% for multi-label classification on the 7-point Checklist dataset, which is a well-benchmarked public dataset. Conclusions: Our method is more accurate than the state-of-the-art methods and, in particular, our method achieved consistent and the best results in datasets with imbalanced label distributions.","Classification, Melanoma, Convolutional neural networks (cnns)",Lei Bi and David Dagan Feng and Michael Fulham and Jinman Kim,https://www.sciencedirect.com/science/article/pii/S0031320320303058,https://doi.org/10.1016/j.patcog.2020.107502,0031-3203,2020,107502,107,Pattern Recognition,Multi-Label classification of multi-modality skin lesion via hyper-connected convolutional neural network,article,BI2020107502,
"Defocus map estimation (DME) is very useful in many computer vision applications and has drawn much attention in recent years. Edge-based DME methods can generate sharp defocus discontinuities but usually suffer from textures of the input image. Region-based methods are free of textures but cannot catch the defocus discontinuities very well. In this paper, we propose a DME method combining edge-based and region-based methods together to keep their respective advantages while eliminating the shortcomings. The combination is achieved via regression tree fields (RTF). In an RTF, the input feature and the linear basis are of vital importance. For our RTF, they are obtained as follows. (i) Two orthogonal gradient operators with the corresponding subsets of Gabor filters are employed in localized 2D frequency analysis to generate accurate likelihood, and the first K highest local maximums of likelihood are sent to an RTF as input feature. (ii) At the same time, the input image is processed by three edge-based methods and the results serve as the linear basis of RTF. The experiments demonstrate that the proposed method outperforms state-of-the-art DME methods. Moreover, the proposed method can be readily applied to defocused image deblurring and defocus blur detection.","Defocus map estimation, Regression tree fields, Localized 2D frequency analysis",Shaojun Liu and Qingmin Liao and Jing-Hao Xue and Fei Zhou,https://www.sciencedirect.com/science/article/pii/S0031320320302880,https://doi.org/10.1016/j.patcog.2020.107485,0031-3203,2020,107485,107,Pattern Recognition,Defocus map estimation from a single image using improved likelihood feature and edge-based basis,article,LIU2020107485,
"Loss functions play a key role in training superior deep neural networks. In convolutional neural networks (CNNs), the popular cross entropy loss together with softmax does not explicitly guarantee minimization of intra-class variance or maximization of inter-class variance. In the early studies, there is no theoretical analysis and experiments explicitly indicating how to choose the number of units in fully connected layer. To help CNNs learn features more fast and discriminative, there are two contributions in this paper. First, we determine the minimum number of units in FC layer by rigorous theoretical analysis and extensive experiment, which reduces CNNsâ parameter memory and training time. Second, we propose a negative-focused weights-biased softmax (W-Softmax) loss to help CNNs learn more discriminative features. The proposed W-Softmax loss not only theoretically formulates the intra-class compactness and inter-class separability, but also can avoid overfitting by enlarging decision margins. Moreover, the size of decision margins can be flexibly controlled by adjusting a hyperparameter Î±. Extensive experimental results on several benchmark datasets show the superiority of W-Softmax in image classification tasks.","Classification, Softmax, CNNs, Fully connected layer units, Classifier weights",Xiaobin Li and Weiqiang Wang,https://www.sciencedirect.com/science/article/pii/S0031320320302089,https://doi.org/10.1016/j.patcog.2020.107405,0031-3203,2020,107405,107,Pattern Recognition,Learning discriminative features via weights-biased softmax loss,article,LI2020107405,
"Feature selection effectively reduces the dimensionality of data. For feature selection, rough set theory offers a systematic theoretical framework based on consistency measures, of which information entropy is one of the most important significance measures of attributes. However, an information-entropy-based significance measure is computationally expensive and requires repeated calculations. Although many accelerating strategies have been proposed thus far, there remains a bottleneck when using an information-entropy-based feature selection algorithm to handle large-scale datasets with high dimensions. In this study, we introduce a classified nested equivalence class (CNEC)-based approach to calculate the information-entropy-based significance for feature selection using rough set theory. The proposed method extracts knowledge of the reducts of a decision table to reduce the universe and construct CNECs. By exploring the properties of different types of CNECs, we can not only accelerate both outer and inner significance calculation by discarding useless CNECs but also effectively decrease the number of inner significance calculations by using one type of CNECs. The use of CNECs is shown to significantly enhance three representative entropy-based feature selection algorithms that use rough set theory. The feature subset selected by the CNEC-based algorithms is the same as that selected by algorithms using the original definition of information entropies. Experiments conducted using 31 datasets from multiple sources, such as the UCI repository and KDD Cup competition, including large-scale and high-dimensional datasets, confirm the efficiency and effectiveness of the proposed method.","Feature selection, Rough set theory, Attribute reduction, Information entropy",Jie Zhao and Jia-ming Liang and Zhen-ning Dong and De-yu Tang and Zhen Liu,https://www.sciencedirect.com/science/article/pii/S0031320320303204,https://doi.org/10.1016/j.patcog.2020.107517,0031-3203,2020,107517,107,Pattern Recognition,Accelerating information entropy-based feature selection using rough set theory with classified nested equivalence classes,article,ZHAO2020107517,
"Convolutional Neural Networks (CNNs), also known as deep learners have seen much success in the last few years due to the availability of large amounts of data and high-performance computational resources. A CNN can be trained effectively if large amounts of data are available as it enables a CNN to find the optimal set of features and weights that can achieve the highest generalization performance. However, due to the requirement of large data size, CNNs require a lot of resources for example running time and computational resources to achieve a reasonable performance. Additionally, unbalanced data makes it difficult to train a CNN effectively that can achieve good generalization performance. In order to alleviate these limitations, in this paper, we propose a novel ensemble of deep learners that learns by combining multiple deep learners trained on small strongly class associated input data effectively. We propose a novel methodology of generating random subspace through clustering input data and propose a measure which can classify each cluster as a strong data cluster and a balanced data cluster. A methodology is also proposed that balances all strong data clusters in the pool so that an architecturally simple CNN can be trained on all balanced data clusters simultaneously. Classification decisions on all trained CNNs are then fused through majority voting to generate class decisions of the ensemble. The performance of the proposed ensemble approach is evaluated on UCI benchmark datasets, and results are compared with existing state-of-the-art ensemble approaches. Significance testing was conducted to further validate the efficacy of the results and a significance test analysis is presented.","Deep learning, Ensemble classifier, Neural networks, Clustering",Zohaib Jan and Brijesh Verma,https://www.sciencedirect.com/science/article/pii/S0031320320302235,https://doi.org/10.1016/j.patcog.2020.107420,0031-3203,2020,107420,107,Pattern Recognition,Multiple strong and balanced cluster-based ensemble of deep learners,article,JAN2020107420,
"In this paper we propose to embed SMPL within a deep-based model to accurately estimate 3D pose and shape from a still RGB image. We use CNN-based 3D joint predictions as an intermediate representation to regress SMPL pose and shape parameters. Later, 3D joints are reconstructed again in the SMPL output. This module can be seen as an autoencoder where the encoder is a deep neural network and the decoder is SMPL model. We refer to this as SMPL reverse (SMPLR). By implementing SMPLR as an encoder-decoder we avoid the need of complex constraints on pose and shape. Furthermore, given that in-the-wild datasets usually lack accurate 3D annotations, it is desirable to lift 2D joints to 3D without pairing 3D annotations with RGB images. Therefore, we also propose a denoising autoencoder (DAE) module between CNN and SMPLR, able to lift 2D joints to 3D and partially recover from structured error. We evaluate our method on SURREAL and Human3.6M datasets, showing improvement over SMPL-based state-of-the-art alternatives by about 4 and 12Â mm, respectively.","Deep learning, 3D Human pose, Body shape, SMPL, Denoising autoencoder, Volumetric stack hourglass",Meysam Madadi and Hugo Bertiche and Sergio Escalera,https://www.sciencedirect.com/science/article/pii/S0031320320302752,https://doi.org/10.1016/j.patcog.2020.107472,0031-3203,2020,107472,106,Pattern Recognition,SMPLR: Deep learning based SMPL reverse for 3D human pose and shape recovery,article,MADADI2020107472,
"In recent years, fully convolutional neural network (FCN) has broken all records in various vision task. It also achieves great performance in salient object detection. However, most of the state-of-the-art methods have suffered from the challenge of precisely segmenting the entire salient object with uniform region and explicit boundary and effectively suppressing the backgrounds on complex images. There is still a large room for improvement over the FCN-based saliency detection approaches. In this paper, we propose an attention and boundary guided deep neural network for salient object detection to better locate and segment the salient objects with uniform interior and explicit boundary. A channel-wise attention module is utilized to emphasize the important regions, which selects the important feature channels and assigns large weights to them. A boundary information localization module is proposed for suppressing the irrelevant boundary information to better locate and explore the useful structure of objects. The proposed approach achieves state-of-the-art performance on four well-known benchmark datasets.","Salient object detection, Visual saliency, Feature learning, Fully convolutional neural network",Qing Zhang and Yanjiao Shi and Xueqin Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320302879,https://doi.org/10.1016/j.patcog.2020.107484,0031-3203,2020,107484,107,Pattern Recognition,Attention and boundary guided salient object detection,article,ZHANG2020107484,
"Mobile devices are becoming ubiquitous and being increasingly used for data-sensitive activities such as communication, personal media storage, and banking. The protection of such data commonly relies on passwords and biometric traits such as fingerprints. These methods perform the user authentication sporadically and often require action from the user, which may make them susceptible to spoofing attacks. This scenario can be mitigated if we bring to bear motion-sensing based methods for authentication, which operate continuously and without requiring user action, hence are harder to attack. Such methods could be used allied with traditional authentication methods or on their own. This paper explores this idea in a novel user-agnostic approach for identity verification based on motion traits acquired by mobile sensors. The proposed approach does not require user-specific training before deployment in mobile devices nor does it require any extra sensor in the device. This solution is capable of learning a user profiling manifold from a small user subset and extend it to unknown users. We validated the proposal on two public datasets. The reported experiments demonstrate remarkable results under a cross-dataset protocol and an open-set setup. Moreover, we performed several analyses aiming at answering critical questions of a biometric method and the presented solution.","User profiling, Motion sensor, Gait, Manifold learning, Open-set user profiling, Cross-dataset user profiling",Geise Santos and Paulo Henrique Pisani and Roberto Leyva and Chang-Tsun Li and Tiago Tavares and Anderson Rocha,https://www.sciencedirect.com/science/article/pii/S0031320320302119,https://doi.org/10.1016/j.patcog.2020.107408,0031-3203,2020,107408,106,Pattern Recognition,Manifold learning for user profiling and identity verification using motion sensors,article,SANTOS2020107408,
"Handwritten Chinese Character Recognition (HCCR) is a challenging topic in the field of pattern recognition due to large-scale character vocabulary, complex hierarchical structure, various writing styles, and scarce training samples. In this paper, we explored the hierarchical knowledge of Chinese characters and presented a novel zero-shot HCCR method. First, we handled the relations between the characters and their primitives, such as radicals and structures, to obtain a tree layout of primitives. Then, we presented a novel zero-shot hierarchical decomposition embedding method to encode the tree layout into a semantic vector. Next, we devised a Convolutional Neural Network (CNN) based framework to learn both radicals and structures of characters via the semantic vector. As different Chinese characters share some common radicals and structures, our method is able to recognize new categories without any labeled samples from them. Moreover, our method is effective in both traditional HCCR and zero-shot HCCR tasks. It achieves competitive performance on the traditional experiment setting and significantly surpasses the state-of-the-art methods on the zero-shot experiment setting.","Chinese character recognition, Radical analysis, Zero-shot learning, Label embedding",Zhong Cao and Jiang Lu and Sen Cui and Changshui Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320302910,https://doi.org/10.1016/j.patcog.2020.107488,0031-3203,2020,107488,107,Pattern Recognition,Zero-shot Handwritten Chinese Character Recognition with hierarchical decomposition embedding,article,CAO2020107488,
"Dynamic gesture recognition, which plays an essential role in human-computer interaction, has been widely investigated but not yet fully addressed. The challenge mainly lies in three folders: 1) to model both of the spatial appearance and the temporal evolution simultaneously; 2) to address the interference from the varied and complex background; 3) the requirement of real-time processing. In this paper, we address the above challenges by proposing a novel deep deformable 3D convolutional neural network for end-to-end learning, which not only gains impressive accuracy in challenging datasets but also can meet the requirement of the real-time processing. We propose three types of very deep 3D CNNs for gesture recognition, which can directly model the spatiotemporal information with their inherent hierarchical structure. To eliminate the background interference, a light-weight spatiotemporal deformable convolutional module is specially designed to augment the spatiotemporal sampling locations of the 3D convolution by learning additional offsets according to the preceding feature map. It can not only diversify the shape of the convolution kernel to better fit the appearance of the hands and arms, but also help the models pay more attention to the discriminative frames in the video sequence. The proposed method is evaluated on three challenging datasets, EgoGesture, Jester and Chalearn-IsoGD, and achieves the state-of-the-art performance on all of them. Our model ranked first on Jesterâs official leader-board until the submission time. The code and the trained models are released for better communication and future works11https://github.com/lshiwjx/deform_conv3d_pytorch_op.","Gesture recognition, Spatiotemporal deformable convolution, Spatiotemporal convolutional neural network",Yifan Zhang and Lei Shi and Yi Wu and Ke Cheng and Jian Cheng and Hanqing Lu,https://www.sciencedirect.com/science/article/pii/S0031320320302193,https://doi.org/10.1016/j.patcog.2020.107416,0031-3203,2020,107416,107,Pattern Recognition,Gesture recognition based on deep deformable 3D convolutional neural networks,article,ZHANG2020107416,
"Depth image super-resolution (DISR) is an effective solution to improve the quality of depth images captured by real world low-cost cameras. In this paper, we propose a multi-scale symmetric network with the correlation-controlled color guidance block (CCGB) for DISR. The proposed network consists of two multi-scale sub-networks to respectively provide guidance and estimate depth. A symmetric unit (SU), which is a mini-encoder-decoder structure with residual learning, is designed and used as a basic network atom. The encoder part in SU aims to extract essential features, while the decoder part works to restore edge details. The way the SU processes information matches well with the textureless and sharp-edge characteristics of depth images. The two sub-networks present a high-level symmetric structure connected by dense guidance links in between. Based on the correlation analyses between the two sub-networks, each guidance link will transfer information trough a CCGB designed to implement channel-wise re-weighting mechanism. The accurate color guidance from CCGB helps avoiding artifacts introduced by non-co-occurrence of depth discontinuities and color edges. Experimental results demonstrate the superiority of the proposed method over several state-of-the-art DISR works.","Depth image super-resolution, Deep convolutional neural network, Encoder-decoder structure, Color guidance, Channel correlation",Tao Li and Hongwei Lin and Xiucheng Dong and Xiaohua Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320303162,https://doi.org/10.1016/j.patcog.2020.107513,0031-3203,2020,107513,107,Pattern Recognition,Depth image super-resolution using correlation-controlled color guidance and multi-scale symmetric network,article,LI2020107513,
"Variational autoencoders (VAEs), as well as other generative models, have been shown to be efficient and accurate for capturing the latent structure of vast amounts of complex high-dimensional data. However, existing VAEs can still not directly handle data that are heterogenous (mixed continuous and discrete) or incomplete (with missing data at random), which is indeed common in real-world applications. In this paper, we propose a general framework to design VAEs suitable for fitting incomplete heterogenous data. The proposed HI-VAE includes likelihood models for real-valued, positive real valued, interval, categorical, ordinal and count data, and allows accurate estimation (and potentially imputation) of missing data. Furthermore, HI-VAE presents competitive predictive performance in supervised tasks, outperforming supervised models when trained on incomplete data.","Generative models, Variational autoencoders, Incomplete heterogenous data",Alfredo NazÃ¡bal and Pablo M. Olmos and Zoubin Ghahramani and Isabel Valera,https://www.sciencedirect.com/science/article/pii/S0031320320303046,https://doi.org/10.1016/j.patcog.2020.107501,0031-3203,2020,107501,107,Pattern Recognition,Handling incomplete heterogeneous data using VAEs,article,NAZABAL2020107501,
"In this paper, we design a simple yet powerful deep network architecture, U2-Net, for salient object detection (SOD). The architecture of our U2-Net is a two-level nested U-structure. The design has the following advantages: (1) it is able to capture more contextual information from different scales thanks to the mixture of receptive fields of different sizes in our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the whole architecture without significantly increasing the computational cost because of the pooling operations used in these RSU blocks. This architecture enables us to train a deep network from scratch without using backbones from image classification tasks. We instantiate two models of the proposed architecture, U2-Net (176.3Â MB, 30 FPS on GTX 1080Ti GPU) and U2-Netâ  (4.7Â MB, 40 FPS), to facilitate the usage in different environments. Both models achieve competitive performance on six SOD datasets. The code is available: https://github.com/NathanUA/U-2-Net.","Salient object detection, Convolutional neural network, Network architecture design, Nested U-structure, Multi-scale feature extraction",Xuebin Qin and Zichen Zhang and Chenyang Huang and Masood Dehghan and Osmar R. Zaiane and Martin Jagersand,https://www.sciencedirect.com/science/article/pii/S0031320320302077,https://doi.org/10.1016/j.patcog.2020.107404,0031-3203,2020,107404,106,Pattern Recognition,U2-Net: Going deeper with nested U-structure for salient object detection,article,QIN2020107404,
"Image super-resolution (SR) techniques with deep convolutional network (CNN) have achieved significant improvements compared to previous shallow-learning-based methods. Especially for dense connection based networks, these methods have yielded unprecedented achievements but bring the higher complexity and more parameters. To this end, this paper considers both reconstruction performance and efficiency, and advocates a novel hierarchical dense connection network (HDN) for image SR. First of all, we construct a hierarchical dense residual block (HDB) to promote the feature representation while saving the memory footprint with a hierarchical matrix structure design. In this way, it can provide additional interleaved pathways for information fusion and gradient optimization but with a shallower depth compare to the previous networks. In particular, a group of convolutional layers with small size (1Â ÃÂ 1) are embedded in HDB, releasing the computational burden and parameters by rescaling the feature dimensions. Furthermore, HDBs are connected to each other in a sharing manner, thereby allowing the network to fuse the features in different stages. At the final, the multi-scale features from these HDBs are integrated into global fusion module (GFM) for a global fusion and representation, and then the final profile-enriched residual map is obtained by realigning and sub-pixel upsampling the fusion maps. Extensive experimental results on benchmark datasets and really degraded images show that our model outperforms the state-of-the-art methods in terms of quantitative indicators and realistic visual effects, as well as enjoys a fast and accurate reconstruction.","Super-resolution, Hierarchical dense connection, Global feature fusion, Recursive network, Multi-scale",Kui Jiang and Zhongyuan Wang and Peng Yi and Junjun Jiang,https://www.sciencedirect.com/science/article/pii/S0031320320302788,https://doi.org/10.1016/j.patcog.2020.107475,0031-3203,2020,107475,107,Pattern Recognition,Hierarchical dense recursive network for image super-resolution,article,JIANG2020107475,
"Graph convolutional network (GCN) provides a powerful means for graph-based semi-supervised tasks. However, as a localized first-order approximation of spectral graph convolution, the classic GCN can not take full advantage of unlabeled data, especially when the unlabeled node is far from labeled ones. To capitalize on the information from unlabeled nodes to boost the training for GCN, we propose a novel framework named Self-Ensembling GCN (SEGCN), which marries GCN with Mean Teacher â a powerful self-ensemble learning mechanism for semi-supervised task. SEGCN contains a student model and a teacher model. As a student, it not only learns to correctly classify the labeled nodes, but also tries to be consistent with the teacher on unlabeled nodes in more challenging situations, such as a high dropout rate and graph corrosion. As a teacher, it averages the student model weights and generates more accurate predictions to lead the student. In such a mutual-promoting process, both labeled and unlabeled samples can be fully utilized for backpropagating effective gradients to train GCN. In a variety of semi-supervised classification benchmarks, i.e. Citeseer, Cora, Pubmed and NELL, we validate that the proposed method matches the state of the arts in the classification accuracy. The code is publicly available at https://github.com/RoyalVane/SEGCN.","Teacher-student models, Self-ensemble learning, Graph convolutional networks, Semi-supervised learning",Yawei Luo and Rongrong Ji and Tao Guan and Junqing Yu and Ping Liu and Yi Yang,https://www.sciencedirect.com/science/article/pii/S0031320320302545,https://doi.org/10.1016/j.patcog.2020.107451,0031-3203,2020,107451,106,Pattern Recognition,Every node counts: Self-ensembling graph convolutional networks for semi-supervised learning,article,LUO2020107451,
"Multi-video summarization is an effective tool for users to browse multiple videos. In this paper, multi-video summarization is formulated as a graph analysis problem and a dynamic graph convolutional network is proposed to measure the importance and relevance of each video shot in its own video as well as in the whole video collection. Two strategies are proposed to solve the inherent class imbalance problem of video summarization task. Moreover, we propose a diversity regularization to encourage the model to generate a diverse summary. Extensive experiments are conducted, and the comparisons are carried out with the state-of-the-art video summarization methods, the traditional and novel graph models. Our method achieves state-of-the-art performances on two standard video summarization datasets. The results demonstrate the effectiveness of our proposed model in generating a representative summary for multiple videos with good diversity.","Multi-video summarization, Graph convolutional network, Class imbalance problem",Jiaxin Wu and Sheng-hua Zhong and Yan Liu,https://www.sciencedirect.com/science/article/pii/S0031320320301850,https://doi.org/10.1016/j.patcog.2020.107382,0031-3203,2020,107382,107,Pattern Recognition,Dynamic graph convolutional network for multi-video summarization,article,WU2020107382,
"Deep neural network is a effective way for scene graph generation tasks. However, it also makes the scene graph generation models difficult to explain. For instance, the current standard metric cannot explain how capable neural network models are of capturing the semantics of relations. In this paper, we try to understand the semantics capturing capability of scene graph generation models based on three types of metrics: conformance recall, violation recall, and non-violation recall, which measure semantic properties of relations that are reflected by triples in scene graph generated by models. Evaluation of these metrics on three representative state-of-the-art scene graph generation models based on deep neural network in Visual Genome dataset shows that the proposed metrics can effectively explain the capability of models to capture different semantic properties and identify design problems in models. By extending the Visual Genome dataset with different sets of additional annotations, these metrics can also explaining whether the semantics capturing capability of deep neural network models can be improved by data enhancement.","Explanation, Metrics, Semantic property, Scene graph generation, Deep neural network",Jie Luo and Jia Zhao and Bin Wen and Yuhang Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320302302,https://doi.org/10.1016/j.patcog.2020.107427,0031-3203,2021,107427,110,Pattern Recognition,Explaining the semantics capturing capability of scene graph generation models,article,LUO2021107427,
"In this paper, we propose a spatial-temporal deformable networks approach to investigate both problems of face alignment in static images and face tracking in videos under unconstrained environments. Unlike conventional feature extractions which cannot explicitly exploit augmented spatial geometry for various facial shapes, in our approach, we propose a deformable hourglass networks (DHGN) method, which aims to learn a deformable mask to reduce the variances of facial deformation and extract attentional facial regions for robust feature representation. However, our DHGN is limited to extract only spatial appearance features from static facial images, which cannot explicitly exploit the temporal consistency information across consecutive frames in videos. For efficient temporal modeling, we further extend our DHGN to a temporal DHGNÂ (T-DHGN) paradigm particularly for video-based face alignment. To this end, our T-DHGN principally incorporates with a temporal relational reasoning module, so that the temporal order relationship among frames is encoded in the relational feature. By doing this, our T-DHGN reasons about the temporal offsets to select a subset of discriminative frames over time steps, thus allowing temporal consistency information memorized to flow across frames for stable landmark tracking in videos. Compared with most state-of-the-art methods, our approach achieves superior performance on folds of widely-evaluated benchmarking datasets. Code will be made publicly available upon publication.","Face alignment, Face tracking, Spatial transformer, Relational reasoning, Video analysis, Biometrics",Hongyu Zhu and Hao Liu and Congcong Zhu and Zongyong Deng and Xuehong Sun,https://www.sciencedirect.com/science/article/pii/S0031320320301576,https://doi.org/10.1016/j.patcog.2020.107354,0031-3203,2020,107354,107,Pattern Recognition,Learning spatial-temporal deformable networks for unconstrained face alignment and tracking in videos,article,ZHU2020107354,
"Measurements by many multi-sensor systems can be considered as point-clouds. One such system is the tracker for the PANDA experiment. Charged particles passing through the tracker produce patterns representing their paths. We present a new, graph-based, attribute-space morphological connected filter for reconstructing particle paths through such a detector. We introduce the concept of attribute-spaces and attribute-space connected filters on graphs, rather than binary images and show a new processing scheme to reduce the size of the memory required to store the attribute-space representations of binary images and graphs. The result is an O(Nlogâ(N)) algorithm with a total recognition error of approximately 0.10, a significant improvement compared to our previous state-of-the-art O(N2) algorithm with a total error of 0.17.","Graph-based image analysis, Attribute-space connectivity, Orientation-based segmentation, Irregular graph morphology, Graph morphology, Orientation-based path merging, Sub-atomic particle tracking, Straw tube tracker (STT)",M. Babai and N. Kalantar-Nayestanaki and J.G. Messchendorp and M.H.F. Wilkinson,https://www.sciencedirect.com/science/article/pii/S0031320320302703,https://doi.org/10.1016/j.patcog.2020.107467,0031-3203,2020,107467,106,Pattern Recognition,An efficient attribute-space connected filter on graphs to reconstruct paths in point-clouds,article,BABAI2020107467,
"We propose a method that use a convolutional neural network (CNN) to estimate human pose by analyzing the projection of the depth and ridge data, which represent local maxima in a distance transform map. To fully utilize the 3D information of depth points, we propose a method to project the depth and ridge data on various directions. The proposed projection method can reduce the 3D information loss, the ridge data can avoid joint drift, and the CNN increases localization accuracy. The proposed method proceeds as follows. (1) We use depth data to segment the human from the background and extract ridge data from human silhouettes. (2) We project the depth and ridge data onto XY, XZ, and ZY planes. (3) ResNet-101 accepts six projected images and use 1 Â ÃÂ  1 convolution layers to generate 2D heatmaps and offsets. (4) We generate 2D keypoints per plane by using the soft-argmax operation. (5) We obtain 3D joint positions by using the fully-connected layers. In experiments on the SMMC-10, EVAL, and ITOP datasets, the proposed method achieved the state-of-the-art pose estimation accuracies. The proposed method can eliminate the 3D information loss and drift of joint positions that can occur during estimation of human pose.","3D Human pose estimation, 3D Point projection, Ridge data",Yeonho Kim and Daijin Kim,https://www.sciencedirect.com/science/article/pii/S003132032030265X,https://doi.org/10.1016/j.patcog.2020.107462,0031-3203,2020,107462,106,Pattern Recognition,A CNN-based 3D human pose estimation based on projection of depth and ridge data,article,KIM2020107462,
"In this paper, we propose a novel deep learning-based framework for facial landmark detection. This framework takes as input face image returned by a face detector (Faster R-CNN) and generates as output a set of landmarks positions. Prior CNN-based methods often select randomly small local patches to predict an initial guess of landmarks locations. One issue with these local patches is that the adjacent landmarks might share the same regions due to the overlapping, thus, they might not convey precise information of each individual landmark. By contrast, our approach formulates this problem as a divide-conquer search for facial patches using CNN architecture in a hierarchy, where the input face image is recursively split into two cohesive non-overlapped subparts until each one contains only the region around the expected landmark. To attain better division of face topology, the search is carried out in a structured coarse-to-fine manner, where a learned hierarchical model of the face defining the granularity of each division level is introduced. We also propose a cascaded regressor to detect and refine the position of the individual landmark in each predicted non-overlapped patch. We adopt a carefully designed shallow CNN architecture so that to improve real-time performance. In addition, unlike previous cascaded methods, our regressor does not require auxiliary input such as initial landmarks locations. Extensive experiments on several challenging datasets (including MTFL, AFW, AFLW, COFW, 300W, and 300VW) show that our approach is particularly impressive in the unconstrained scenarios where it outperforms prior arts in both accuracy and efficiency.","Facial landmark detection, Convolutional neural network, Face landmark modeling, Hierarchical divider, Cascade detector",Rachida Hannane and Abdessamad Elboushaki and Karim Afdel,https://www.sciencedirect.com/science/article/pii/S0031320320303071,https://doi.org/10.1016/j.patcog.2020.107504,0031-3203,2020,107504,107,Pattern Recognition,A divide-and-conquer strategy for facial landmark detection using dual-task CNN architecture,article,HANNANE2020107504,
"Many applications in biomedical informatics deal with data in the tensor form. Traditional regression methods which take vectors as covariates may encounter difficulties in handling tensors due to their ultrahigh dimensionality and complex structure. In this paper, we introduce a novel sparse regularized Tucker tensor regression model to exploit the structure of tensor covariates and perform feature selection on tensor data. Based on Tucker decomposition of the regression coefficient tensor, we reduce the ultrahigh dimensionality to a manageable level. To make our model identifiable, we impose the orthonormality constraint on the factor matrices. Unlike previous tensor regression models that impose sparse penalty on the factor matrices of the coefficient tensor, our model directly imposes sparse penalty on the coefficient tensor to select the relevant features on tensor data. An efficient optimization algorithm based on alternating direction method of multiplier (ADMM) algorithm is designed to solve our proposed model. The performance of our model is evaluated on both synthetic and real genomic data. Experiment results on synthetic data demonstrate that our model could identify the true related signals more accurately than other state-of-the-art regression models. The analysis on genomic data of melanoma demonstrates that our model can achieve better prediction performance and identify markers with important implications. Our model and the associated studies can provide useful insights to disease or pathogenesis mechanisms, and will benefit further studies in variable selection.","Tensor regression, Tensor decomposition, Sparse penalty",Le Ou-Yang and Xiao-Fei Zhang and Hong Yan,https://www.sciencedirect.com/science/article/pii/S0031320320303198,https://doi.org/10.1016/j.patcog.2020.107516,0031-3203,2020,107516,107,Pattern Recognition,Sparse regularized low-rank tensor regression with applications in genomic data analysis,article,OUYANG2020107516,
"In multi-dimensional classification (MDC), each training example is represented by a single instance (feature vector) while associated with multiple class variables, each of which specifies its class membership w.r.t. one specific class space. Most existing MDC approaches try to model dependencies among class variables in output space when inducing predictive functions, while the potential usefulness of manipulating feature space hasnât been investigated. As a first attempt towards feature manipulation in input space for MDC, a simple yet effective approach named Kram is proposed which enriches the original feature space with augmented features based on kNN techniques. Specifically, simple counting statistics on the class membership of neighboring MDC examples as well as distance information between MDC examples and their k nearest neighbors are used to generate augmented feature vector. In this way, discriminative information from class space is expected to be brought into the feature space which would be helpful to the following MDC predictive model induction. To validate the effectiveness of the proposed feature augmentation techniques, comprehensive comparative studies are conducted over fifteen benchmark data sets. Compared to the original feature space, it is clearly shown that the kNN-augmented features generated by the proposed Kram approach can significantly improve generalization abilities of existing MDC approaches.","Machine learning, Multi-dimensional classification, Feature augmentation, Class dependencies",Bin-Bin Jia and Min-Ling Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320302260,https://doi.org/10.1016/j.patcog.2020.107423,0031-3203,2020,107423,106,Pattern Recognition,Multi-dimensional classification via kNN feature augmentation,article,JIA2020107423,
"We introduce an approach to divisive hierarchical clustering that is capable of identifying clusters in nonlinear manifolds. This approach uses the isometric mapping (Isomap) to recursively embed (subsets of) the data in one dimension, and then performs a binary partition designed to avoid the splitting of clusters. We provide a theoretical analysis of the conditions under which contiguous and high-density clusters in the original space are guaranteed to be separable in the one-dimensional embedding. To the best of our knowledge there is little prior work that studies this problem. Extensive experiments on simulated and real data sets show that hierarchical divisive clustering algorithms derived from this approach are effective.","Nonlinearity, Dimensionality reduction, Divisive hierarchical clustering, Manifold clustering",Sotiris Tasoulis and Nicos G. Pavlidis and Teemu Roos,https://www.sciencedirect.com/science/article/pii/S0031320320303113,https://doi.org/10.1016/j.patcog.2020.107508,0031-3203,2020,107508,107,Pattern Recognition,Nonlinear dimensionality reduction for clustering,article,TASOULIS2020107508,
"The decision-making ability of deep reinforcement learning has been proved successfully in a variety of fields. Here, we use this method for precise character detection by making tight bounding boxes around the Chinese characters in historical documents. An agent is trained to learn the control policy of fine-tuning a bounding box step-by-step through a Markov Decision Process. We introduce a novel fully convolutional network with position-sensitive Region-of-Interest (RoI) pooling (FCPN). The network receives character patches as input without fixed size, and it can fuse position information into the features of actions. Besides, we propose a dense reward function (DRF) that provides excellent rewards according to different actions and environment states, improving the decision-making ability of the agent. Our approach is designed as a universal method that can be applied to the output of all character-level or word-level text detectors to obtain more precise detection results. Application to the Tripitaka Koreana in Han (TKH) and Multiple Tripitaka in Han (MTH) datasets confirm the very promising performance of this method. In particular, our approach yields a significant improvement under a large Intersection over Union (IoU) of 0.8. The robustness and generality are also proved by experiments on the scene text datasets ICDAR2013 and ICDAR2015.","Deep reinforcement learning, Historical documents, Character detection, Reward function",Wu Sihang and Wang Jiapeng and Ma Weihong and Jin Lianwen,https://www.sciencedirect.com/science/article/pii/S003132032030306X,https://doi.org/10.1016/j.patcog.2020.107503,0031-3203,2020,107503,107,Pattern Recognition,Precise detection of Chinese characters in historical documents with deep reinforcement learning,article,SIHANG2020107503,
"Covariance descriptors (CovDs) for image set classification have been widely studied recently. Different from the conventional CovDs, which describe similarities between pixels at different locations, we focus more on similarities between regions that convey more comprehensive information. In this paper, we extract pixel-wise features of image regions and represent them by Gaussian models. We extend the conventional covariance computation onto a special type of Riemannian manifold, namely a Gaussian manifold, so that it is applicable to our image set data representation provided in terms of Gaussian models. We present two methods to calculate a Riemannian local difference vector on the Gaussian manifold (RieLDV-G) and generate our proposed Riemannian covariance descriptors (RieCovDs) using the resulting RieLDV-G. By measuring the recognition accuracy achieved on benchmarking datasets, we demonstrate experimentally the superior performance of our proposed RieCovDs descriptors, as compared with state-of-the-art methods. (The code is available at: https://github.com/Kai-Xuan/RiemannianCovDs)","Covariance descriptors, Riemannian local difference vector, Riemannian covariance descriptors, Image set classification",Kai-Xuan Chen and Jie-Yi Ren and Xiao-Jun Wu and Josef Kittler,https://www.sciencedirect.com/science/article/pii/S0031320320302661,https://doi.org/10.1016/j.patcog.2020.107463,0031-3203,2020,107463,107,Pattern Recognition,Covariance descriptors on a Gaussian manifold and their application to image set classification,article,CHEN2020107463,
"The label propagation algorithm is a well-known semi-supervised clustering method, which uses pre-given partial labels as constraints to predict the labels of unlabeled data. However, the algorithm has the following limitations: (1) it does not fully consider the misalignment between the pre-given labels and clustering labels, and (2) it only uses label information as clustering constraints. Real applications not only contain partial label information but pairwise constraints on a dataset. To overcome these deficiencies, a new version of the label propagation algorithm is proposed, which makes use of pairwise relations of labels as constraints to construct an optimization model for spreading labels. Experimental analysis was used to compare the proposed algorithm with 8 other semi-supervised clustering algorithms on 11 benchmark datasets. The experimental results demonstrated that the proposed algorithm is more effective than other algorithms.","Cluster analysis, Semi-supervised clustering, Label propagation algorithm, Pairwise constraint, Spectral clustering",Liang Bai and Junbin Wang and Jiye Liang and Hangyuan Du,https://www.sciencedirect.com/science/article/pii/S0031320320302144,https://doi.org/10.1016/j.patcog.2020.107411,0031-3203,2020,107411,106,Pattern Recognition,New label propagation algorithm with pairwise constraints,article,BAI2020107411,
"Vision-based crack detection is of crucial importance in various industries, and it is very challenging due to weak signals in noisy backgrounds. In this paper, we propose a novel hybrid approach for crack detection in raw images, which combines deep learning models and Bayesian probabilistic analysis for robust crack detection. First, we re-train a state-of-the-art object detector (e.g. a Faster R-CNN) to detect crack patches of suitable SNR (signal-noise-ratio). We design a semi-automatic method to generate ground truths of crack patches along crack lines for training. To further improve the accuracy of crack detections over the whole image, we propose a Bayesian integration algorithm to suppress false detections. Specifically, we use a deep CNN to recognize the orientation of the crack segment in each detected patch. Then, a Bayesian probability is computed on the accumulated evidence from detected adjacent patches within a neighborhood based on spatial proximity, orientation consistency and alignment consistency. The patch which lacks local supports is suppressed as false detection. An algorithm to learn the parameters of Bayesian integration is also derived. Extensive experiments and evaluations are performed on a new comprehensive dataset of crack images. The results show that our approach outperforms the state-of-the-art baseline approach on deep CNN classifier. Ablation experiments are also conducted to show the effectiveness of proposed techniques.","Crack detection, Defect detection, Object detection, Convolutional neural network, Faster R-CNN, Bayesian fusion",Fen Fang and Liyuan Li and Ying Gu and Hongyuan Zhu and Joo-Hwee Lim,https://www.sciencedirect.com/science/article/pii/S0031320320302776,https://doi.org/10.1016/j.patcog.2020.107474,0031-3203,2020,107474,107,Pattern Recognition,A novel hybrid approach for crack detection,article,FANG2020107474,
"Face re-identification (Re-ID) aims to track the same individuals over space and time with subtle identity class information in automatically detected face images captured by unconstrained surveillance camera views. Despite significant advances of face recognition systems for constrained social media facial images, face Re-ID is more challenging due to poor-quality surveillance face imagery data and remains under-studied. However, solving this problem enables a wide range of practical applications, ranging from law enforcement and information security to business, entertainment and e-commerce. To facilitate more studies on face Re-ID towards practical and robust solutions, a true large scale Surveillance Face Re-ID benchmark (SurvFace) is introduced, characterised by natively low-resolution, motion blur, uncontrolled poses, varying occlusion, poor illumination, and background clutters. This new benchmark is the largest and more importantly the only true surveillance face Re-ID dataset to our best knowledge, where facial images are captured and detected under realistic surveillance scenarios. We show that the current state-of-the-art FR methods are surprisingly poorfor face Re-ID. Besides, face Re-ID is generally more difficult in an open-set setting as naturally required in surveillance scenarios, owing to a large number of non-target people (distractors) appearing in open ended scenes. Moreover, the low-resolution problem inherent to surveillance facial imagery is investigated. Finally, we discuss open research problems that need to be solved in order to overcome the under-studied face Re-ID problem.","Face re-identification, Surveillance facial imagery, Low-resolution, Super-resolution, Open-set matching, Deep learning, Face recognition",Zhiyi Cheng and Xiatian Zhu and Shaogang Gong,https://www.sciencedirect.com/science/article/pii/S0031320320302259,https://doi.org/10.1016/j.patcog.2020.107422,0031-3203,2020,107422,107,Pattern Recognition,Face re-identification challenge: Are face recognition models good enough?,article,CHENG2020107422,
"Intuitively, all facial images of a person are located on or near a manifold in the high-dimensional image space, and the process of face recognition can be regarded as the recovery process of multiple low-dimensional manifolds. To preserve the manifold structure information of intra-class samples after dimensionality reduction, we proposed a patch-based multi-manifold orthogonal neighborhood-preserving discriminant analysis algorithm, namely ONPDA. From the perspective of path alignment, we consider the intra-class compactness, intra-class structure and inter-class separability simultaneously. Moreover, we infuse intra-class structure information described by the sample reconstruction into intra-class compactness loss, considering the compactness of two reconstruction groups instead of sample pairs in the same class. By analyzing the relationship between the projection direction and the maximum inter-class margin, we select the samples that should participate in the inter-class separability on the patch. Meanwhile, a fast orthogonalization method is performed to obtain the orthogonal projection matrix. Besides, we perform ONPDA in reproducing kernel Hilbert space which gives rise to nonlinear maps, resulting in the kernel ONPDA (KONPDA). Experimental results compared with some state-of-the-art methods on a toy dataset and several benchmark face image databases demonstrate the effectiveness of ONPDA and KONPDA.","Neighborhood preserving, Inter-class separability, Orthogonal projection, Patch alignment, Face recognition",Liangchen Hu and Wensheng Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320302533,https://doi.org/10.1016/j.patcog.2020.107450,0031-3203,2020,107450,106,Pattern Recognition,Orthogonal neighborhood preserving discriminant analysis with patch embedding for face recognition,article,HU2020107450,
"Classifying and rendering volumes of the structure are two essential goals of the visualization process. However, loss of some voxels can cause poor visualization results, such as small holes or non-smooth patches in visualized volumes. Beginning with the classified volumes, we propose a modified Allen-Cahn equation, which has the motion of mean curvature, to recover lost voxels and to fill holes. Consequently, a probability function can be obtained, which indicates the probability of each voxel being a volume voxel. Usually, the obtained probability function is smooth due to the motion of the mean curvature flow. Therefore visualization quality of volumes can be significantly improved. The equation is numerically computed by the unconditional stable operator splitting method with a large time step size. Thus the numerical scheme is fast and can be straightforwardly applied to GPU-accelerated DCT implementation that performs up to many times faster than CPU-only alternatives. Many experimental results have been performed to demonstrate the efficiency of the proposed method.","Volume rendering, Volume repairing, Allen-Cahn equation, Mean curvature flow",Yibao Li and Shouren Lan and Xin Liu and Bingheng Lu and Lisheng Wang,https://www.sciencedirect.com/science/article/pii/S0031320320302818,https://doi.org/10.1016/j.patcog.2020.107478,0031-3203,2020,107478,107,Pattern Recognition,An efficient volume repairing method by using a modified Allen-Cahn equation,article,LI2020107478,
"Earlier ellipse fitting methods often consider the algebraic and geometric forms of the ellipse. The work presented here makes use of an ensemble to provide better results. The method proposes a new ellipse parametrization based on the coordinates of both foci, and the distance between them and each point of the ellipse where the Euclidean norm is applied. Besides, a certain number of subsets are uniformly drawn without replacement from the overall training set which allows estimating the center of the distribution robustly by employing the L1 median of each estimated focus. An additional postprocessing stage is proposed to filter out the effect of bad fits. In order to evaluate the performance of this method, four different error measures were considered. Results show that our proposal outperforms all its competitors, especially when higher levels of outliers are presented. Several synthetic and real data tests were developed and confirmed such finding.","Ellipse fitting, Geometric curve fitting, Ensemble methods, Spatial median, Robust estimation",Karl Thurnhofer-Hemsi and Ezequiel LÃ³pez-Rubio and Elidia Beatriz BlÃ¡zquez-Parra and M. Carmen LadrÃ³n-de-Guevara-MuÃ±oz and Ãscar David de-CÃ³zar-Macias,https://www.sciencedirect.com/science/article/pii/S0031320320302090,https://doi.org/10.1016/j.patcog.2020.107406,0031-3203,2020,107406,106,Pattern Recognition,Ellipse fitting by spatial averaging of random ensembles,article,THURNHOFERHEMSI2020107406,
"The semi-supervised video anomaly detection assumes that only normal video clips are available for training. Therefore, the intuitive idea is either to learn a dictionary by sparse coding or to train encoding-decoding neural networks by minimizing the reconstruction errors. For the former, the optimization of sparse coefficients is extremely time-consuming. For the latter, this manner cannot guarantee that an abnormal data corresponds to a larger reconstruction error due to the strong generalization of neural networks. To remedy their weaknesses and leverage their strengths, we propose a Fast Sparse Coding Network (FSCN) based on High-level Features. First, we propose a two-stream neural network to extract Spatial-Temporal Fusion Features (STFF) in hidden layers. With the STFF at hand, we use a Fast Sparse Coding Network to build a normal dictionary. By leveraging the predictor to produce approximate sparse coefficients, our FSCN generates sparse coefficients within a forward pass, which is simple and computationally efficient. Compared with traditional sparse coding based methods, FSCN is hundreds of or even thousands of times faster at the test stage. Extensive experiments on benchmark datasets demonstrate that our method reaches the state-of-the-art level.11Code will be released at https://github.com/Roc-Ng/FSCN_AnomalyDetection.","Anomaly detection, Encoding-decoding networks, Sparse coding networks, Spatial-temporal information, Video representation",Peng Wu and Jing Liu and Mingming Li and Yujia Sun and Fang Shen,https://www.sciencedirect.com/science/article/pii/S0031320320303186,https://doi.org/10.1016/j.patcog.2020.107515,0031-3203,2020,107515,107,Pattern Recognition,Fast sparse coding networks for anomaly detection in videos,article,WU2020107515,
"Convolutional Neural Networks (CNNs) have performed extremely well on data represented by regularly arranged grids such as images. However, directly leveraging the classic convolution kernels or parameter sharing mechanisms on sparse 3D point clouds is inefficient due to their irregular and unordered nature. We propose a point attention network that learns rich local shape features and their contextual correlations for 3D point cloud semantic segmentation. Since the geometric distribution of the neighboring points is invariant to the point ordering, we propose a Local Attention-Edge Convolution (LAE-Conv) to construct a local graph based on the neighborhood points searched in multi-directions. We assign attention coefficients to each edge and then aggregate the point features as a weighted sum of its neighbors. The learned LAE-Conv layer features are then given to a point-wise spatial attention module to generate an interdependency matrix of all points regardless of their distances, which captures long-range spatial contextual features contributing to more precise semantic information. The proposed point attention network consists of an encoder and decoder which, together with the LAE-Conv layers and the point-wise spatial attention modules, make it an end-to-end trainable network for predicting dense labels for 3D point cloud segmentation. Experiments on challenging benchmarks of 3D point clouds show that our algorithm can perform at par or better than the existing state of the art methods.","Semantic segmentation, 3D point cloud, Point attention network, Deep learning",Mingtao Feng and Liang Zhang and Xuefei Lin and Syed Zulqarnain Gilani and Ajmal Mian,https://www.sciencedirect.com/science/article/pii/S0031320320302491,https://doi.org/10.1016/j.patcog.2020.107446,0031-3203,2020,107446,107,Pattern Recognition,Point attention network for semantic segmentation of 3D point clouds,article,FENG2020107446,
"Multitask (multi-target or multi-output) learning (MTL) deals with simultaneous prediction of several outputs. MTL approaches rely on the optimization of a joint score function over the targets. However, defining a joint score in global models is problematic when the target scales are different. To address such problems, single target (i.e. local) learning strategies are commonly employed. Here we propose alternative tree-based learning strategies to handle the issue with target scaling in global models, and to identify the learning order for chaining operations in local models. In the first proposal, the problems with target scaling are resolved using alternative splitting strategies which consider the learning tasks in a multi-objective optimization framework. The second proposal deals with the problem of ordering in the chaining strategies. We introduce an alternative estimation strategy, minimum error chain policy, that gradually expands the input space using the estimations that approximate to true characteristics of outputs, namely out-of-bag estimations in tree-based ensemble framework. Our experiments on benchmark datasets illustrate the success of the proposed multitask extension of trees compared to the decision trees with de facto design especially for datasets with large number of targets. In line with that, minimum error chain policy improves the performance of the state-of-the-art chaining policies.","Multitask learning, Multi-objective trees, Stacking, Classifier chains, Ensemble learning",Esra AdÄ±yeke and Mustafa GÃ¶kÃ§e BaydoÄan,https://www.sciencedirect.com/science/article/pii/S0031320320303101,https://doi.org/10.1016/j.patcog.2020.107507,0031-3203,2020,107507,107,Pattern Recognition,The benefits of target relations: A comparison of multitask extensions and classifier chains,article,ADIYEKE2020107507,
"One-class classification (OCC) is a classical problem in computer vision that can be described as the task of classifying outlier class samples (OC samples) from the OCC model trained on inlier class samples (IC samples) when datasets are highly biased toward one class due to the insufficient sample size of the other class. Currently, the adversarial learning OCC (ALOCC) method has been proven to significantly improve OCC performance. However, its drawbacks include instability issues and non-evident reconstruction between the IC and OC samples. Therefore, we propose multihead enhanced self-attention in the ALOCC network, thereby increasing the difference between the IC and OC samples and significantly increasing OCC accuracy compared with ALOCC accuracy. For training, we propose a new loss, called adversarial-balance loss, that effectively solves the training instability problem, further increasing OCC accuracy. The experiments show the effectiveness of the proposed method compared with state-of-art methods.","One-class classification, Multihead attention network, Adversarial-balance loss, Adversarial Learning, Multihead enhanced self-attention",Yingying Zhang and Yuxin Gong and Haogang Zhu and Xiao Bai and Wenzhong Tang,https://www.sciencedirect.com/science/article/pii/S0031320320302892,https://doi.org/10.1016/j.patcog.2020.107486,0031-3203,2020,107486,107,Pattern Recognition,Multi-head enhanced self-attention network for novelty detection,article,ZHANG2020107486,
"Human pose estimation is the task of localizing body key points from still images. As body key points are inter-connected, it is desirable to model the structural relationships between body key points to further improve the localization performance. In this paper, based on original graph convolutional networks, we propose a novel model, termed Pose Graph Convolutional Network (PGCN), to exploit these important relationships for pose estimation. Specifically, our model builds a directed graph between body key points according to the natural compositional model of a human body. Each node (key point) is represented by a 3-D tensor consisting of multiple feature maps, initially generated by our backbone network, to retain accurate spatial information. Furthermore, attention mechanism is presented to focus on crucial edges (structured information) between key points. PGCN is then learned to map the graph into a set of structure-aware key point representations which encode both structure of human body and appearance information of specific key points. Additionally, we propose two modules for PGCN, i.e., the Local PGCN (L-PGCN) module and Non-Local PGCN (NL-PGCN) module. The former utilizes spatial attention to capture the correlations between the local areas of adjacent key points to refine the location of key points. While the latter captures long-range relationships via non-local operation to associate the challenging key points. By equipping with these two modules, our PGCN can further improve localization performance. Experiments both on single- and multi-person estimation benchmark datasets show that our method consistently outperforms competing state-of-the-art methods.","Human pose estimation, Graph convolutional networks, Key points structural relations",Yanrui Bin and Zhao-Min Chen and Xiu-Shen Wei and Xinya Chen and Changxin Gao and Nong Sang,https://www.sciencedirect.com/science/article/pii/S0031320320302132,https://doi.org/10.1016/j.patcog.2020.107410,0031-3203,2020,107410,106,Pattern Recognition,Structure-aware human pose estimation with graph convolutional networks,article,BIN2020107410,
"As the Internet confronts the multimedia explosion, it becomes urgent to investigate personalized recommendation for alleviating information overload and improving usersâ experience. Most personalized recommendation approaches pay their attention to collaborative filtering over usersâ interactions, which suffers greatly from the highly sparse interactions. In image recommendation, visual correlations among images that users consumed provide a piece of intrinsic evidence to reveal usersâ interests. It inspires us to investigate image recommendation over the dense visual graph of images instead of the sparse user interaction graph. In this paper, we propose a semantic manifold modularization-based ranking (MMR) for image recommendation. MMR leverages the dense visual manifold to propagate usersâ historical records and infer user-image correlations for image recommendation. Especially, it constrains interest propagation within semantic visual compact groups by manifold modularization to make a tradeoff between usersâ personality and graph smoothness in propagation. Experimental results demonstrate that user-consumed visual correlations play actively to capture usersâ interests, and the proposed MMR can infer user-image correlations via visual manifold propagation for image recommendation.","Manifold propagation, Modularization, Image recommendation, User interest",Meng Jian and Jingjing Guo and Chenlin Zhang and Ting Jia and Lifang Wu and Xun Yang and Lina Huo,https://www.sciencedirect.com/science/article/pii/S0031320321002879,https://doi.org/10.1016/j.patcog.2021.108100,0031-3203,2021,108100,120,Pattern Recognition,Semantic manifold modularization-based ranking for image recommendation,article,JIAN2021108100,
"Diabetic retinopathy (DR) detection has attracted much attention recently, and the deep learning algorithms have gained traction in this area. At present, DR screening by deep learning algorithms is often based on single-view fundus images, which usually leads to an unsatisfactory accuracy of DR grading due to the incomplete lesion features. In this paper, we proposed a novel diabetic retinopathy detection convolutional network for automatic DR detection by integrating multi-view fundus images. Compared to existing single-view DCNN-based DR detection methods, the proposed method has the following advantages. First, our method fully utilizes the lesion features from the retina with a field-of-view around 120ââ150â. Second, by introducing the attention mechanisms, more attention will be paid on the influential view and the performance can be improved. Besides, we also assign large weights to important channels in the network for effective feature extraction. Experiments are conducted on our collected multi-view DR dataset contained 15,468 images, in which each eye sample provides four-view images. The experimental results indicate that using multi-view images is suitable for automatic DR detection and our proposed method is superior to other benchmarking methods.","Diabetic retinopathy (DR), Deep convolutional neural networks (DCNNs), Multi-view, Attention mechanisms, Classification",Xiaoling Luo and Zuhui Pu and Yong Xu and Wai Keung Wong and Jingyong Su and Xiaoyan Dou and Baikang Ye and Jiying Hu and Lisha Mou,https://www.sciencedirect.com/science/article/pii/S0031320321002910,https://doi.org/10.1016/j.patcog.2021.108104,0031-3203,2021,108104,120,Pattern Recognition,MVDRNet: Multi-view diabetic retinopathy detection by combining DCNNs and attention mechanisms,article,LUO2021108104,
"By sorting channel-minimized values in an ascending order, we individually put the values of several existing image dehazing priors on the curve of sorted values to propose a framework for unifying and understanding these priors. Then we propose a confidence ratio to specify the probability of each channel-minimized value within a range, and thus we can intuitively find a suitable point from the curve, which is actually defined as a novel prior. Although our novel prior and existing ones are perfectly unified under the same framework, our prior has an important advantage that it can freely control the suppression degree of outliers by directly adjusting the confidence ratio of channel-minimized values. In this way, we can remove influence of outliers in a controllable manner. To solve the problems caused by heterogeneity of pixel values and abrupt jumps of scene depths in hazy images, we adopt a regression method to adaptively learn the relationship between patch appearance and confidence ratios for all pixels. To further improve robustness, we use a Gaussian kernel to smooth the estimated confidence ratios for local consistency. Extensive experiments on both natural and synthetic images show that our confidence prior achieves significantly better performance than existing state-of-the-art methods.","Regression, Classification, Image dehazing, Confidence prior, Appearance feature",Feiniu Yuan and Yu Zhou and Xue Xia and Xueming Qian and Jian Huang,https://www.sciencedirect.com/science/article/pii/S0031320321002636,https://doi.org/10.1016/j.patcog.2021.108076,0031-3203,2021,108076,119,Pattern Recognition,A confidence prior for image dehazing,article,YUAN2021108076,
"Zero-shot learning (ZSL) aims to assign the category corresponding to the relevant semantic as the label of the unseen sample based on the relationship between the learned visual and semantic features. However, most typical ZSL models faced with the domain bias problem, which leads to unseen or test samples being easily misclassified into seen or training categories. To handle this problem, we propose a relation-based discriminative cooperation network (RDCN) model for ZSL in this work. The proposed model effectively utilize the robust metric space spanned by the cooperated semantics with the help of a set of relations. On the other hand, we devise the relation network to measure the relationship between the visual features and embedded semantics, and the validation information will guide the embedding module to learn more discriminative information. At last, the proposed RDCN model is validated on six benchmarks, and extensive experiments demonstrate the superiority of proposed method over most existing ZSL models on the traditional zero-shot setting and the more realistic generalized zero-shot setting.","Zero-shot learning, Bias, Discriminative, Relation",Yang Liu and Xinbo Gao and Quanxue Gao and Jungong Han and Ling Shao,https://www.sciencedirect.com/science/article/pii/S0031320321002119,https://doi.org/10.1016/j.patcog.2021.108024,0031-3203,2021,108024,118,Pattern Recognition,Relation-based Discriminative Cooperation Network for Zero-Shot Classification,article,LIU2021108024,
"The fast pandemics of coronavirus disease (COVID-19) has led to a devastating influence on global public health. In order to treat the disease, medical imaging emerges as a useful tool for diagnosis. However, the computed tomography (CT) diagnosis of COVID-19 requires expertsâ extensive clinical experience. Therefore, it is essential to achieve rapid and accurate segmentation and detection of COVID-19. This paper proposes a simple yet efficient and general-purpose network, called Sequential Region Generation Network (SRGNet), to jointly detect and segment the lesion areas of COVID-19. SRGNet can make full use of the supervised segmentation information and then outputs multi-scale segmentation predictions. Through this, high-quality lesion-areas suggestions can be generated on the predicted segmentation maps, reducing the diagnosis cost. Simultaneously, the detection results conversely refine the segmentation map by a post-processing procedure, which significantly improves the segmentation accuracy. The superiorities of our SRGNet over the state-of-the-art methods are validated through extensive experiments on the built COVID-19 database.","COVID-19, Segmentation, Detection, Context enhancement, Edge loss",Jipeng Wu and Haibo Xu and Shengchuan Zhang and Xi Li and Jie Chen and Jiawen Zheng and Yue Gao and Yonghong Tian and Yongsheng Liang and Rongrong Ji,https://www.sciencedirect.com/science/article/pii/S003132032100193X,https://doi.org/10.1016/j.patcog.2021.108006,0031-3203,2021,108006,118,Pattern Recognition,Joint segmentation and detection of COVID-19 via a sequential region generation network,article,WU2021108006,
"The Bayesian approach to feature extraction, known as factor analysis (FA), has been widely studied in machine learning to obtain a latent representation of the data. An adequate selection of the probabilities and priors of these bayesian models allows the model to better adapt to the data nature (i.e. heterogeneity, sparsity), obtaining a more representative latent space. The objective of this article is to propose a general FA framework capable of modelling any problem. To do so, we start from the Bayesian Inter-Battery Factor Analysis (BIBFA) model, enhancing it with new functionalities to be able to work with heterogeneous data, to include feature selection, and to handle missing values as well as semi-supervised problems. The performance of the proposed model, Sparse Semi-supervised Heterogeneous Interbattery Bayesian Analysis (SSHIBA), has been tested on different scenarios to evaluate each one of its novelties, showing not only a great versatility and an interpretability gain, but also outperforming most of the state-of-the-art algorithms.","Bayesian model, Canonical correlation analysis, Principal component analysis, Factor analysis, Feature selection, Semi-supervised, Multi-task",Carlos Sevilla-Salcedo and Vanessa GÃ³mez-Verdejo and Pablo M. Olmos,https://www.sciencedirect.com/science/article/pii/S0031320321003289,https://doi.org/10.1016/j.patcog.2021.108141,0031-3203,2021,108141,120,Pattern Recognition,Sparse semi-supervised heterogeneous interbattery bayesian analysis,article,SEVILLASALCEDO2021108141,
"Convolutional neural networks (CNNs) easily suffer from the over-fitting problem since they are often over-parameterized in the case of small training datasets. The conventional dropout that drops feature units randomly works well for fully connected networks, but fails to regularize CNNs well due to high spatial correlation of the intermediate features, which allows the dropped information to flow through the network, thus leading to the problem of under-dropping. To better regularize CNNs, some structural dropout methods such as SpatialDropout and DropBlock have been proposed by dropping feature units in continuous regions randomly. However, these methods may suffer from the over-dropping problem by discarding the critical discriminative features, thus limiting the performance of CNNs. To address these issues, we propose a novel structural dropout method, Correlation based Dropout (CorrDrop), to regularize CNNs by dropping feature units based on feature correlation. Unlike the previous dropout methods, our CorrDrop can focus on the discriminative information and drops features in a spatial-wise or channel-wise manner. Extensive experiments on different datasets, network architectures, and various tasks (e.g., image classification and object localization) demonstrate the superiority of our method over other methods.","Over-fitting, Regularization, Dropout, Convolutional neural networks",Yuyuan Zeng and Tao Dai and Bin Chen and Shu-Tao Xia and Jian Lu,https://www.sciencedirect.com/science/article/pii/S0031320321003046,https://doi.org/10.1016/j.patcog.2021.108117,0031-3203,2021,108117,120,Pattern Recognition,Correlation-based structural dropout for convolutional neural networks,article,ZENG2021108117,
"Most of word recognition systems rely on a pre-defined lexicon in aims to achieve high performance. Recently, the availability of training /testing data allows to include a huge number of words in the lexicon to recognize. However, this leads to high computation cost as the lexicon is grown. In addition, including more and more word-classes may lead to increase the burden on classification methods and degrade the recognition rate. In this work, we propose a holistic word descriptor for word lexicon reduction in Arabic handwritten documents. The proposed descriptor represents geometrical features of word shape through three main feature sets, defined from multi-scale convexity concavity analysis. The first two sets are dedicated to defined the number of peaks and their intensity levels of convexity/concavity peaks, respectively. In contrast, the last set is dedicated to define a region codes of the peaks by analyzing their regions according to their spatial information. Given a query word and lexicon(reference dataset), the lexicon reduction system is applied by first defining the holistic word descriptor for both query word and each word in the lexicon. The lexicon is then indexed according to its distances to the query word descriptor. Finally, the reduced lexicon is formulated from the first kth entries of the indexed lexicon. The proposed system has been evaluated under two well-known Arabic datasets, namely Ibn Sina and IFN/ENIT. Reported results show superior performance compared to prior art, with 93.7% and 91.2% reduction efficacy for Ibn Sina and IFN/ENIT, respectively.","Word descriptor, Local shape descriptor, Lexicon reduction, Multi-scale representation, Contour matching, Arabic handwritten documents,",Said Elaiwat,https://www.sciencedirect.com/science/article/pii/S0031320321002594,https://doi.org/10.1016/j.patcog.2021.108072,0031-3203,2021,108072,119,Pattern Recognition,Holistic word descriptor for lexicon reduction in handwritten arabic documents,article,ELAIWAT2021108072,
"To assess the contributions of the different feature channels of sensors, we introduce a novel multimodal fusion method and demonstrate its practical utility using LiDAR-camera fusion networks. Specifically, a channel attention module that can be easily added to a fusion segmentation network is proposed. In this module, we use the channel attention mechanism to obtain the cross-channel local interaction information, and the weights of feature channels are assigned to represent the contributions of different feature channels. To verify the effectiveness of the proposed method, we conduct experiments on two types of feature fusion with the KITTI benchmark and A2D2 dataset. Our model achieves precise edge segmentation, with a 5.59% gain in precision and a 2.12% gain in F2-score compared to the values of the original fusion method. We believe that we have introduced a new optimization idea for multimodal fusion.","LiDAR-camera fusion, Lane line segmentation, Channel attention mechanism, Multimodal fusion, Fusion information",Xinyu Zhang and Zhiwei Li and Xin Gao and Dafeng Jin and Jun Li,https://www.sciencedirect.com/science/article/pii/S0031320321002077,https://doi.org/10.1016/j.patcog.2021.108020,0031-3203,2021,108020,118,Pattern Recognition,Channel Attention in LiDAR-camera Fusion for Lane Line Segmentation,article,ZHANG2021108020,
"Video based visual question answering (V-VQA) remains challenging at the intersection of vision and language. In this paper, we propose a novel architecture, namely Generalized Pyramid Co-attention with Learnable Aggregation Net (GPC) to address two central problems: 1) how to deploy co-attention to V-VQA task considering the complex and diverse content of videos; and 2) how to aggregate the frame-level features (or word-level features) without destroying the feature distributions and temporal information. To solve the first problem, we propose a Generalized Pyramid Co-attention structure with a novel diversity learning module to explicitly encourage attention accuracy and diversity. And we first instantiate it into a Multi-path Pyramid Co-attention (MPC) to capture diverse feature. Then we find each attention branch of original co-attention mechanism does not interact with the others, which results in coarse attention maps. So we extend the MPC structure to a Cascaded Pyramid Transformer Co-attention (CPTC) module in which we replace co-attention with transformer co-attention. To solve the second problem, we propose a new learnable aggregation method with a set of evidence gates. It automatically aggregates adaptively-weighted frame-level features (or word-level features) to extract rich video (or question) context semantic information. With evidence gates, it then further chooses the most related signals representing the evidence information to predict the answer. Extensive validations on the two V-VQA datasets, TGIF-QA and TVQA show that both our proposed MPC and CPTC achieve the state-of-the-art performance and CPTC performs better under various settings and metrics. Code and model have been released at:https://github.com/lixiangpengcs/LAD-Net-for-VideoQA.","Video question answering, Diversity learning, Learnable aggregation, Cascaded pyramid transformer co-attention",Lianli Gao and Tangming Chen and Xiangpeng Li and Pengpeng Zeng and Lei Zhao and Yuan-Fang Li,https://www.sciencedirect.com/science/article/pii/S0031320321003320,https://doi.org/10.1016/j.patcog.2021.108145,0031-3203,2021,108145,120,Pattern Recognition,Generalized pyramid co-attention with learnable aggregation net for video question answering,article,GAO2021108145,
"In this paper, we propose a novel stroke constrained attention network (SCAN) which treats stroke as the basic unit for encoder-decoder based online handwritten mathematical expression recognition (HMER). Unlike previous methods which use trace points or image pixels as basic units, SCAN makes full use of stroke-level information for better alignment and representation. The proposed SCAN can be adopted in both single-modal (online or offline) and multi-modal HMER. For single-modal HMER, SCAN first employs a CNN-GRU encoder to extract point-level features from input traces in online mode and employs a CNN encoder to extract pixel-level features from input images in offline mode, then use stroke constrained information to convert them into online and offline stroke-level features. Using stroke-level features can explicitly group points or pixels belonging to the same stroke, therefore reduces the difficulty of symbol segmentation and recognition via the decoder with attention mechanism. For multi-modal HMER, other than fusing multi-modal information in decoder, SCAN can also fuse multi-modal information in encoder by utilizing the stroke based alignments between online and offline modalities. The encoder fusion is a better way for combining multi-modal information as it implements the information interaction one step before the decoder fusion so that the advantages of multiple modalities can be exploited earlier and more adequately. Besides, we propose an approach combining the encoder fusion and decoder fusion, namely encoder-decoder fusion, which can further improve the performance. Evaluated on a benchmark published by CROHME competition, the proposed SCAN achieves the state-of-the-art performance. Furthermore, by conducting experiments on an additional task: online handwritten Chinese character recognition (HCCR), we demonstrate the generality of our proposed method.","Stroke-level information, Multi-modal fusion, Encoder-decoder, Attention mechanism, Handwritten mathematical expression recognition",Jiaming Wang and Jun Du and Jianshu Zhang and Bin Wang and Bo Ren,https://www.sciencedirect.com/science/article/pii/S003132032100234X,https://doi.org/10.1016/j.patcog.2021.108047,0031-3203,2021,108047,119,Pattern Recognition,Stroke constrained attention network for online handwritten mathematical expression recognition,article,WANG2021108047,
"In real-world applications, large-scale unlabeled data usually becomes available gradually over time. Online learning is important to update models while preserving their historical knowledge. However, a time-varying distribution shift exists in incoming sequential data in online learning, resulting in a data cluster discrepancy between the incoming unlabeled data and older labeled data, which is a challenging situation for online learning. To address this issue, we propose an online deep transferable dictionary learning (ODTDL) method that simultaneously mitigates the data cluster discrepancy for incoming unlabeled data while preserving historical knowledge of older data in the dictionary. By forming a locally linear representation and association of incoming unlabeled data over a small amount of labeled data in a deep feature space, the proposed ODTDL method can reveal data cluster discrepancies. To implement this approach, we propose a two-level affiliation regularizer that both comprehensively reveals the local instance-level and global cluster-level affiliations and enables an off-the-shelf dictionary reconstruction error method to establish a knowledge transfer pipeline between the labeled and unlabeled data. For online learning, this approach further decomposes the knowledge transfer pipeline into batchwise transfer pipelines, thereby establishing batchwise transfer pipelines between labeled and unlabeled data. Finally, the proposed method is confirmed to be feasible in online semi-supervised learning (SSL) and online unsupervised domain adaptation (UDA) scenarios and demonstrates its superiority in the online setting.","Online transferable dictionary learning, Semi-supervised learning, Domain adaptation",Sheng Wu and Ancong Wu and Wei-Shi Zheng,https://www.sciencedirect.com/science/article/pii/S0031320321001941,https://doi.org/10.1016/j.patcog.2021.108007,0031-3203,2021,108007,118,Pattern Recognition,Online deep transferable dictionary learning,article,WU2021108007,
"We present an image inpainting approach to generate diverse high-quality inpainting results. Recent advances in deep adversarial networks have led to significant improvements in the challenging task of filling large holes in natural images. Although deep generative models can generate visually plausible structures and textures, most of them are not interpretable, making it difficult to control the inpainting output. In addition, deep generative models do not have capacity to produce diverse results for each input. To address such limitations, we design a novel free-form image inpainting framework with two sequential steps: the first step formulates the inpainting process as a regression problem and utilizes a U-Net-like convolutional neural network to map an input to a coarse inpainting output, and the second step utilizes the nearest neighbor based pixel-wise matching to map the coarse output to diverse high-quality outputs. The second step allows our approach to compose novel high-quality content by copy-pasting high-frequency missing information from different training exemplars. Experiments on multiple datasets, i.e., CelebA-HQ, AFHQ, and Paris StreetView, show that our approach is able to offer multiple natural outputs with higher diversity in a controllable manner.","Diverse image inpainting, Free-form mask, U-Net-like network, Nearest neighbors",Yuan Zeng and Yi Gong and Jin Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321002235,https://doi.org/10.1016/j.patcog.2021.108036,0031-3203,2021,108036,119,Pattern Recognition,Feature learning and patch matching for diverse image inpainting,article,ZENG2021108036,
"Model selection with cross validation (CV) is very popular in machine learning. However, CV with grid and other common search strategies cannot guarantee to find the model with minimum CV error, which is often the ultimate goal of model selection. Recently, various solution path algorithms have been proposed for several important learning algorithms including support vector classification, Lasso, and so on. However, they still do not guarantee to find the model with minimum CV error. In this paper, we first show that the solution paths produced by various algorithms have the property of piecewise linearity. Then, we prove that a large class of error (or loss) functions are piecewise constant, linear, or quadratic w.r.t. the regularization parameter, based on the solution path. Finally, we propose a new generalized error path algorithm (GEP), and prove that it will find the model with minimum CV error in a finite number of steps for the entire range of the regularization parameter. The experimental results on a variety of datasets not only confirm our theoretical findings, but also show that the best model with our GEP has better generalization error on the test data, compared to the grid search, manual search, and random search.","Cross validation, Error path, Solution path, Model selection",Bin Gu and Charles X. Ling,https://www.sciencedirect.com/science/article/pii/S0031320321002995,https://doi.org/10.1016/j.patcog.2021.108112,0031-3203,2021,108112,120,Pattern Recognition,Generalized error path algorithm,article,GU2021108112,
"Respiratory droplet propagation has been extensively explored with simulation and experimental methods. However, there still exists a huge gap between these methods, making automatic assessment of simulation quality quantitatively being a challenge. To address above problem, in this work, a triplet neural network framework with multi-scale CNN-BiLSTM network is developed. Firstly, Conditional Variational Auto-Encoder (CVAE) is utilized to generate multi-view simulations. Secondly, YOLOv3 is adopted to extract droplet regions of real image and simulation results. Then, a multi-scale CNN-BiLSTM network with attentive temporal pooling is designed to extract and aggregate temporal information across consecutive frames. Finally, all above networks are constructed into a triplet structure with triplet loss, and a regularization constraint being denoted as reconstruction term and prediction term is proposed. To demonstrate the performance of our approach, a new dataset is established including real sequences of cough droplets and simulation results. We validate the effectiveness and feasibility of our proposed framework using our dataset and two benchmarks, the PSB dataset and the ETH dataset, for 3D object retrieval. Our approach outperforms state-of-the-arts on our dataset and achieves comparative performance on PSB and ETH for 3D object retrieval, given quantitative quality assessment of simulation for droplet respiratory propagation automatically.","Simulation quality assessment, Respiratory droplet propagation, Triplet network, Multi-scale CNN-BiLSTM, Attentive temporal pooling",Jinlong Hu and Songhua Xu and Xiangdong Ding,https://www.sciencedirect.com/science/article/pii/S0031320321002478,https://doi.org/10.1016/j.patcog.2021.108060,0031-3203,2021,108060,119,Pattern Recognition,A Triplet network framework based automatic assessment of simulation quality for respiratory droplet propagation,article,HU2021108060,
"High-quality magnetic resonance (MR) image, i.e., with near isotropic voxel spacing, is desirable in various scenarios of medical image analysis. However, many MR images are acquired using good in-plane resolution but large spacing between slices in clinical practice. In this work, we propose a novel deep-learning-based super-resolution algorithm to generate high-resolution (HR) MR images of small slice spacing from low-resolution (LR) inputs of large slice spacing. Notice that real HR images are needed in most existing deep-learning-based methods to supervise the training, but in clinical scenarios, usually they will not be acquired. Therefore, our unique goal herein is to design and train the super-resolution network without real HR ground-truth. Specifically, two-staged training is used in our method. In the first stage, HR images of reduced slice spacing are synthesized from real LR images using variational auto-encoder (VAE). Although these synthesized HR images of reduced slice spacing are as realistic as possible, they may still suffer from unexpected morphing induced by VAE, implying that the synthesized HR images cannot be paired with the real LR images in terms of anatomical structure details. In the second stage, we degrade the synthesized HR images to generate corresponding LR-HR image pairs and train a super-resolution network based on these synthesized pairs. The underlying mechanism is that such a super-resolution network is less vulnerable to anatomical variability. Experiments on knee MR images successfully demonstrate the effectiveness of our proposed solution to reduce the slice spacing for better rendering.","Generative adversarial network, Magnetic resonance imaging, Super-resolution, Variational auto-encoder",Kai Xuan and Liping Si and Lichi Zhang and Zhong Xue and Yining Jiao and Weiwu Yao and Dinggang Shen and Dijia Wu and Qian Wang,https://www.sciencedirect.com/science/article/pii/S0031320321002909,https://doi.org/10.1016/j.patcog.2021.108103,0031-3203,2021,108103,120,Pattern Recognition,Reducing magnetic resonance image spacing by learning without ground-truth,article,XUAN2021108103,
"Deep learning techniques have shown their superior performance in dermatologist clinical inspection. Nevertheless, melanoma diagnosis is still a challenging task due to the difficulty of incorporating the useful dermatologist clinical knowledge into the learning process. In this paper, we propose a novel knowledge-aware deep framework that incorporates some clinical knowledge into collaborative learning of two important melanoma diagnosis tasks, i.e., skin lesion segmentation and melanoma recognition. Specifically, to exploit the knowledge of morphological expressions of the lesion region and also the periphery region for melanoma identification, a lesion-based pooling and shape extraction (LPSE) scheme is designed, which transfers the structure information obtained from skin lesion segmentation into melanoma recognition. Meanwhile, to pass the skin lesion diagnosis knowledge from melanoma recognition to skin lesion segmentation, an effective diagnosis guided feature fusion (DGFF) strategy is designed. Moreover, we propose a recursive mutual learning mechanism that further promotes the inter-task cooperation, and thus iteratively improves the joint learning capability of the model for both skin lesion segmentation and melanoma recognition. Experimental results on two publicly available skin lesion datasets show the effectiveness of the proposed method for melanoma analysis.","Melanoma diagnosis, Knowledge-aware deep framework, Lesion-based pooling and shape extraction, Diagnosis guided feature fusion, Recursive mutual learning",Xiaohong Wang and Xudong Jiang and Henghui Ding and Yuqian Zhao and Jun Liu,https://www.sciencedirect.com/science/article/pii/S0031320321002624,https://doi.org/10.1016/j.patcog.2021.108075,0031-3203,2021,108075,120,Pattern Recognition,Knowledge-aware deep framework for collaborative skin lesion segmentation and melanoma recognition,article,WANG2021108075,
"Semantic segmentation is a challenging and important task in computer vision. Convolutional neural networks (CNNs) have demonstrated their outstanding performances on such dense classification tasks. Most recent segmentation networks mainly focus on feature extraction for one single input image, while paying little attention to facilitating the segmentation by image manipulation or enhancement. In this paper, we design an enhancement-fusion network (EFNet), which aims at enhancing an input image for more diversified features to boost the following task of pixel-wise labeling. Specifically, the enhancement modules are trained to produce multiple enhanced images. Afterwards, the fusion module selectively attends on such images and fuses them to yield one new image. The proposed EFNet can be directly and flexibly integrated as an auxiliary network with state-of-the-art semantic segmentation networks, while maintaining the end-to-end training manner. Extensive results on benchmark datasets corroborate that the combination of the EFNet and the CNN-based semantic segmentation networks significantly improves the segmentation performance compared with the original segmentation networks.","image enhancement, feature fusion, semantic segmentation, CNN",Zhijie Wang and Ran Song and Peng Duan and Xiaolei Li,https://www.sciencedirect.com/science/article/pii/S0031320321002107,https://doi.org/10.1016/j.patcog.2021.108023,0031-3203,2021,108023,118,Pattern Recognition,EFNet: Enhancement-Fusion Network for Semantic Segmentation,article,WANG2021108023,
"Binarization is a well-known image processing task, whose objective is to separate the foreground of an image from the background. One of the many tasks for which it is useful is that of preprocessing document images in order to identify relevant information, such as text or symbols. The wide variety of document types, alphabets, and formats makes binarization challenging. There are multiple proposals with which to solve this problem, from classical manually-adjusted methods, to more recent approaches based on machine learning. The latter techniques require a large amount of training data in order to obtain good results; however, labeling a portion of each existing collection of documents is not feasible in practice. This is a common problem in supervised learning, which can be addressed by using the so-called Domain Adaptation (DA) techniques. These techniques take advantage of the knowledge learned in one domain, for which labeled data are available, to apply it to other domains for which there are no labeled data. This paper proposes a method that combines neural networks and DA in order to carry out unsupervised document binarization. However, when both the source and target domains are very similar, this adaptation could be detrimental. Our methodology, therefore, first measures the similarity between domains in an innovative manner in order to determine whether or not it is appropriate to apply the adaptation process. The results reported in the experimentation, when evaluating up to 20 possible combinations among five different domains, show that our proposal successfully deals with the binarization of new document domains without the need for labeled data.","Binarization, Machine learning, Domain adaptation, Adversarial training",Francisco J. Castellanos and Antonio-Javier Gallego and Jorge Calvo-Zaragoza,https://www.sciencedirect.com/science/article/pii/S0031320321002867,https://doi.org/10.1016/j.patcog.2021.108099,0031-3203,2021,108099,119,Pattern Recognition,Unsupervised neural domain adaptation for document image binarization,article,CASTELLANOS2021108099,
"Semantic segmentation has been used successfully as a complementary information source in pedestrian detection. However, it requires accurate pixel-level semantic segmentation annotations for training, but it is extremely time-consuming to obtain these. In this work, we solve this problem by using weak segmentation masks automatically generated by depth images. This enables joint semantic segmentation and pedestrian detection with only ground truth bounding boxes for training. We show that this joint training boosts the performance of the pedestrian detector. Moreover, we show that fusing the outputs of the classification network and the generated segmentation masks leads to a further detection performance improvement. Extensive experiments have been conducted on three RGBD pedestrian datasets to demonstrate the effectiveness of our proposed method. As a byproduct, we also obtain pedestrian segmentation results of good quality, without using pixel-level segmentation annotations during training.","Pedestrian detection, Semantic segmentation, Deep learning",Zhixin Guo and Wenzhi Liao and Yifan Xiao and Peter Veelaert and Wilfried Philips,https://www.sciencedirect.com/science/article/pii/S0031320321002508,https://doi.org/10.1016/j.patcog.2021.108063,0031-3203,2021,108063,119,Pattern Recognition,Weak segmentation supervised deep neural networks for pedestrian detection,article,GUO2021108063,
"The sudden outbreak andÂ uncontrolled spread ofÂ COVID-19 disease is one ofÂ theÂ most important global problems today. InÂ aÂ short period ofÂ time, it has led to theÂ development ofÂ many deep neural network models forÂ COVID-19 detection with modules forÂ explainability. InÂ this work, we carry out aÂ systematic analysis ofÂ various aspects ofÂ proposed models. Our analysis revealed numerous mistakes made at different stages ofÂ data acquisition, model development, andÂ explanation construction. InÂ this work, we overview theÂ approaches proposed inÂ theÂ surveyed Machine Learning articles andÂ indicate typical errors emerging from theÂ lack ofÂ deep understanding ofÂ theÂ radiography domain. We present theÂ perspective ofÂ both: experts inÂ theÂ field - radiologists andÂ deep learning engineers dealing with model explanations. The final result is aÂ proposed checklist with the minimum conditions to be met byÂ aÂ reliable COVID-19 diagnostic model.","COVID-19, Lungs, Computed tomography, X-ray, Explainable AI, Deep learning",Weronika Hryniewska and PrzemysÅaw BombiÅski and Patryk Szatkowski and Paulina Tomaszewska and Artur Przelaskowski and PrzemysÅaw Biecek,https://www.sciencedirect.com/science/article/pii/S0031320321002223,https://doi.org/10.1016/j.patcog.2021.108035,0031-3203,2021,108035,118,Pattern Recognition,Checklist for responsible deep learning modeling of medical images basedÂ on COVID-19 detection studies,article,HRYNIEWSKA2021108035,
"Monocular depth estimation has been gaining growing momentum in recent years. Despite significant advances of this task, due to the inherent difficulty of reliably capturing contextual cues from RGB images, it remains challenging to accurately predict depth in scenes with complicated and cluttered spatial arrangement of objects. Instead of naively utilizing the primary features in the single RGB image, in this paper we propose a hierarchical object relationship constrained network for monocular depth estimation, which could enable accurate and smooth depth prediction from monocular RGB image. The key idea of our method is to exploit object-centric hierarchical relationship as contextual constraints to compensate for the regularity of spatial depth changing. In particular, we design a semantics-guided CNN network to encode the original image into a global context feature map and encode the objectsâ relationship into a local relationship feature map simultaneously, so that we can leverage such effective and consolidated coding scheme over scenario samples to guide the depth prediction in a more accurate way. Benefiting from the local-to-global context constraints, our method can well respect the global depth changing and preserve the local depth details at the same time. In addition, our approach could make full use of the hierarchical semantic relationship across inner-object components and neighboring objects to define depth changing constraints. We conduct extensive experiments and make comprehensive evaluations on widely-used public datasets, and the experiments confirm that our method outperforms most state-of-the-art depth estimation methods in preserving the local details in depth.","Monocular Depth Estimation, Semantic Constraints, Hierarchical Object Relationship, Global and Local Context",Shuai Li and Jiaying Shi and Wenfeng Song and Aimin Hao and Hong Qin,https://www.sciencedirect.com/science/article/pii/S0031320321003034,https://doi.org/10.1016/j.patcog.2021.108116,0031-3203,2021,108116,120,Pattern Recognition,Hierarchical Object Relationship Constrained Monocular Depth Estimation.,article,LI2021108116,
"The occurrence of long-tailed distributions and unavailability of high-quality annotated images is a common phenomenon in medical datasets. The use of conventional Deep Learning techniques to obtain an unbiased model with high generalization accuracy for such datasets is a challenging task. Thus, we formulated a few-shot learning problem and presented a meta-learning-based âMetaMedâ approach. The model presented here can adapt to rare disease classes with the availability of few images, and less compute. MetaMed is validated on three publicly accessible medical datasets â Pap smear, BreakHis, and ISIC 2018. We used advanced image augmentation techniques like CutOut, MixUp, and CutMix to overcome the problem of over-fitting. Our approach has shown promising results on all the three datasets with an accuracy of more than 70%. Inclusion of advanced augmentation techniques regularizes the model and increases the generalization capability by Â 2â5%. Comparative analysis of MetaMed against transfer learning demonstrated that MetaMed classifies images with a higher confidence score and on average outperforms transfer learning for 3, 5, and 10-shot tasks for both 2-way and 3-way classification.","Few-shot learning, Meta-learning, Multi-shot learning, Medical image classification, Image augmentation, Histopathological image classification",Rishav Singh and Vandana Bharti and Vishal Purohit and Abhinav Kumar and Amit Kumar Singh and Sanjay Kumar Singh,https://www.sciencedirect.com/science/article/pii/S0031320321002983,https://doi.org/10.1016/j.patcog.2021.108111,0031-3203,2021,108111,120,Pattern Recognition,MetaMed: Few-shot medical image classification using gradient-based meta-learning,article,SINGH2021108111,
"Finding the multiple longest common subsequences (MLCS) among many long sequences (i.e., the large scale MLCS problem) has many important applications, such as gene alignment, disease diagnosis, and documents similarity check, etc. It is an NP-hard problem (Maier etÂ al., 1978). The key bottle neck of this problem is that the existing state-of-the-art algorithms must construct a huge graph (called direct acyclic graph, briefly DAG), and the computer usually has no enough space to store and handle this graph. Thus the existing algorithms cannot solve the large scale MLCS problem. In order to quickly solve the large-scale MLCS problem within limited computer resources, this paper therefore proposes a branch and bound irredundant graph algorithm called Big-MLCS, which constructs a much smaller DAG (called Small-DAG) than the existing algorithms do by a branch and bound method, and designs a new data structure to efficiently store and handle Small-DAG. By these schemes, Big-MLCS is more efficient than the existing algorithms. Also, we compare the proposed algorithm with two state-of-the-art algorithms through the experiments, and the results show that the proposed algorithm outperforms the compared algorithms and is more suitable to large-scale MLCS problems.","Multiple longest common subsequences, Small DAG, Branch and bound, Gene alignment",Chunyang Wang and Yuping Wang and Yiuming Cheung,https://www.sciencedirect.com/science/article/pii/S0031320321002466,https://doi.org/10.1016/j.patcog.2021.108059,0031-3203,2021,108059,119,Pattern Recognition,A branch and bound irredundant graph algorithm for large-scale MLCS problems,article,WANG2021108059,
"Point cloud is important for object detection and recognition. The main challenge of point cloud denoising is to preserve the geometric structures. Several state-of-the-art point cloud denoising methods focus only on analyzing local geometric information, which is sensitive to noise and outliers. In this paper, we propose a novel point cloud denoising algorithm based on the characteristics of non-local self-similarity. First, we present an adaptive curvature threshold to select the edge points and tune their corresponding normals, which can preserve the sharp details. Then we propose a structure-aware descriptor called projective height vector to capture the local height variations by normal height projection and the most similar non-local projective height vectors are grouped into a height matrix to enhance the structure representation. Moreover, the proposed structure descriptor is invariant with rigid transformation. Finally, an improved weighted nuclear norm minimization is proposed to optimize the height matrix and reconstruct a high-quality point cloud. Rather than treating each singular value independently, each component in our proposed weight definition connects with the most important components to preserve the major structural information. Experiments on synthetic and scanned point cloud datasets demonstrate that our algorithm outperforms state-of-the-art methods in terms of reconstruction accuracy and structure preservation.","Point cloud denoising, Adaptive curvature threshold, Structure-aware descriptor, Projective height vector, Improved weighted nuclear norm minimization",Yiyao Zhou and Rui Chen and Yiqiang Zhao and Xiding Ai and Guoqing Zhou,https://www.sciencedirect.com/science/article/pii/S0031320321003150,https://doi.org/10.1016/j.patcog.2021.108128,0031-3203,2021,108128,120,Pattern Recognition,Point cloud denoising using non-local collaborative projections,article,ZHOU2021108128,
"This paper aims to develop an automatic method to segment pulmonary parenchyma in chest CT images and analyze texture features from the segmented pulmonary parenchyma regions to assist radiologists in COVID-19 diagnosis. A new segmentation method, which integrates a three-dimensional (3D) V-Net with a shape deformation module implemented using a spatial transform network (STN), was proposed to segment pulmonary parenchyma in chest CT images. The 3D V-Net was adopted to perform an end-to-end lung extraction while the deformation module was utilized to refine the V-Net output according to the prior shape knowledge. The proposed segmentation method was validated against the manual annotation generated by experienced operators. The radiomic features measured from our segmentation results were further analyzed by sophisticated statistical models with high interpretability to discover significant independent features and detect COVID-19 infection. Experimental results demonstrated that compared with the manual annotation, the proposed segmentation method achieved a Dice similarity coefficient of 0.9796, a sensitivity of 0.9840, a specificity of 0.9954, and a mean surface distance error of 0.0318 mm. Furthermore, our COVID-19 classification model achieved an area under curve (AUC) of 0.9470, a sensitivity of 0.9670, and a specificity of 0.9270 when discriminating lung infection with COVID-19 from community-acquired pneumonia and healthy controls using statistically significant radiomic features. The significant features measured from our segmentation results agreed well with those from the manual annotation. Our approach has great promise for clinical use in facilitating automatic diagnosis of COVID-19 infection on chest CT images.","COVID-19, Chest CT, Pulmonary parenchyma segmentation, Deep learning, 3D V-Net",Chen Zhao and Yan Xu and Zhuo He and Jinshan Tang and Yijun Zhang and Jungang Han and Yuxin Shi and Weihua Zhou,https://www.sciencedirect.com/science/article/pii/S0031320321002582,https://doi.org/10.1016/j.patcog.2021.108071,0031-3203,2021,108071,119,Pattern Recognition,Lung segmentation and automatic detection of COVID-19 using radiomic features from chest CT images,article,ZHAO2021108071,
"Panorama creation is still challenging in consumer-level photography because of varying conditions of image capturing. A long-standing problem is the presence of artifacts caused by structure inconsistent image transitions. Since it is difficult to achieve perfect alignment in unconstrained shooting environment especially with parallax and object movements, image composition becomes a crucial step to produce artifact-free stitching results. Current energy-based seam-cutting image composition approaches are limited by the hand-crafted features, which are not discriminative and adaptive enough to robustly create structure consistent image transitions. In this paper, we present the first end-to-end deep learning framework named Edge Guided Composition Network (EGCNet) for the composition stage in image stitching. We cast the whole composition stage as an image blending problem, and aims to regress the blending weights to seamlessly produce the stitched image. To better preserve the structure consistency, we exploit perceptual edges to guide the network with additional geometric prior. Specifically, we introduce a perceptual edge branch to integrate edge features into the model and propose two edge-aware losses for edge guidance. Meanwhile, we gathered a general-purpose dataset for image stitching training and evaluation (namely, RISD). Extensive experiments demonstrate that our EGCNet produces plausible results with less running time, and outperforms traditional methods especially under the circumstances of parallax and object motions.","Image stitching, Composition, Deep learning, Structure consistency, Edge guidance",Qinyan Dai and Faming Fang and Juncheng Li and Guixu Zhang and Aimin Zhou,https://www.sciencedirect.com/science/article/pii/S0031320321002065,https://doi.org/10.1016/j.patcog.2021.108019,0031-3203,2021,108019,118,Pattern Recognition,Edge-guided Composition Network for Image Stitching,article,DAI2021108019,
"Automatic segmentation of lung opacification from computed tomography (CT) images shows excellent potential for quickly and accurately quantifying the infection of Coronavirus disease 2019 (COVID-19) and judging the disease development and treatment response. However, some challenges still exist, including the complexity and variability features of the opacity regions, the small difference between the infected and healthy tissues, and the noise of CT images. Due to limited medical resources, it is impractical to obtain a large amount of data in a short time, which further hinders the training of deep learning models. To answer these challenges, we proposed a novel spatial- and channel-wise coarse-to-fine attention network (SCOAT-Net), inspired by the biological vision mechanism, for the segmentation of COVID-19 lung opacification from CT images. With the UNet++ as basic structure, our SCOAT-Net introduces the specially designed spatial-wise and channel-wise attention modules, which serve to collaboratively boost the attention learning of the network and extract the efficient features of the infected opacification regions at the pixel and channel levels. Experiments show that our proposed SCOAT-Net achieves better results compared to several state-of-the-art image segmentation networks and has acceptable generalization ability.","COVID-19, Convolutional neural network, Segmentation, Lung opacification, Attention mechanism",Shixuan Zhao and Zhidan Li and Yang Chen and Wei Zhao and Xingzhi Xie and Jun Liu and Di Zhao and Yongjie Li,https://www.sciencedirect.com/science/article/pii/S003132032100296X,https://doi.org/10.1016/j.patcog.2021.108109,0031-3203,2021,108109,119,Pattern Recognition,SCOAT-Net: A novel network for segmenting COVID-19 lung opacification from CT images,article,ZHAO2021108109,
"We explore a new video segmentation task, named portrait video segmentation (PVS), which aims to automatically segment the dominant person throughout a given portrait video. To achieve accurate and temporal-coherent segmentation results, a feature reconstruction based PVS method is developed under the meta-learning framework. Due to the dramatic pose variation and severe occlusion in portrait videos, feature reconstruction using existing optical flow models usually suffers from severe ghosting effects in reconstructed features. We mitigate this issue by presenting a soft correspondence network (SCN), which learns to facilitate feature reconstruction in an unsupervised fashion by softly assigning each pixel displacement probabilities between portrait frames. Based on the proposed SCN, a novel portrait segmentation network (PSN) is further designed, which explores the reconstructed features through feature aggregation blocks (FABs), yielding more reliable segmentation results. To capture temporal and target-specific cues, the parameters of FABs are determined by a meta-updater network which is trained offline in the meta-level. In addition, we introduce a new PVS dataset with high-quality segmentation annotations. Experimental results clearly demonstrate the effectiveness of the proposed PVS method.","Portrait video segmentation, Meta-learning, Feature reconstruction, Feature aggregation block",Yifan Wang and Wenbo Zhang and Lijun Wang and Fenghua Yang and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320321003307,https://doi.org/10.1016/j.patcog.2021.108143,0031-3203,2021,108143,120,Pattern Recognition,Temporal consistent portrait video segmentation,article,WANG2021108143,
"This work presents a thorough review concerning recent studies and text generation advancements using Generative Adversarial Networks. The usage of adversarial learning for text generation is promising as it provides alternatives to generate the so-called ânaturalâ language. Nevertheless, adversarial text generation is not a simple task as its foremost architecture, the Generative Adversarial Networks, were designed to cope with continuous information (image) instead of discrete data (text). Thus, most works are based on three possible options, i.e., Gumbel-Softmax differentiation, Reinforcement Learning, and modified training objectives. All alternatives are reviewed in this survey as they present the most recent approaches for generating text using adversarial-based techniques. The selected works were taken from renowned databases, such as Science Direct, IEEEXplore, Springer, Association for Computing Machinery, and arXiv, whereas each selected work has been critically analyzed and assessed to present its objective, methodology, and experimental results.","Text generation, Generative adversarial Networks, Machine learning, Language modeling, Natural language processing",Gustavo H. {de Rosa} and JoÃ£o P. Papa,https://www.sciencedirect.com/science/article/pii/S0031320321002855,https://doi.org/10.1016/j.patcog.2021.108098,0031-3203,2021,108098,119,Pattern Recognition,A survey on text generation using generative adversarial networks,article,DEROSA2021108098,
"Multi-person pose estimation in unconstrained scenarios, with an unknown number of individuals, is a main step towards scene understanding and action recognition. Due to the recent advancements on the architecture of convolutional networks, body part detectors are now accurate and estimate poses in real-time for both single- and multi-person scenes. In contrast, assigning detected body parts to coherent human poses when there are multiple persons interacting is an arduous task. To name a few of the challenges that arise in such scenes: person-to-person occlusion, truncated body parts, and more sources for double counting. Recently, the community contributed towards solving most of them. Hence, it would be interesting to analyze and compile successful approaches from current literature into research trends, and identify possible gaps for future works. To the best of our knowledge, there is no up-to-date review on the main advancements in the field that target this particular set of challenges. This survey fills this gap by reviewing the main breakthroughs on multi-person pose estimation over the last decade and summarizing their impact on the state-of-the-art. Regarding scientific contributions, we propose a novel taxonomy that categorizes the reviewed methods according to their main contributions to the pose estimation pipeline, lists the main datasets and evaluation metrics to train new models, and provides insights on the best entries of publicly available benchmarks.","Human pose estimation, Multi-person pose estimation, Markerless body part detectors, Human pose tracking",Eduardo {Souza dos Reis} and Lucas Adams Seewald and Rodolfo Stoffel Antunes and Vinicius Facco Rodrigues and Rodrigo {da Rosa Righi} and Cristiano AndrÃ© {da Costa} and Luiz Gonzaga {da Silveira Jr.} and Bjoern Eskofier and Andreas Maier and Tim Horz and Rebecca Fahrig,https://www.sciencedirect.com/science/article/pii/S0031320321002338,https://doi.org/10.1016/j.patcog.2021.108046,0031-3203,2021,108046,118,Pattern Recognition,Monocular multi-person pose estimation: A survey,article,SOUZADOSREIS2021108046,
"Physical damages (such as torn-offs and scratches) are commonly seen in historical documents. Recognition of such damages is currently absent in digitization-and-information-extraction (DIE) systems but crucial for automatic document comprehension and exploitation. In this paper we propose a generic damage recognition (DR) method based on a joint global and local modeling of the text homogeneity (TH) pattern exhibited in document images. More specifically, a connected component (CC) based formulation is developed as a global homogeneity measure, where TH is characterized using a probabilistic graph model for a coarse recognition of damaged regions. A multi-resolution analysis (MRA) of TH is further developed for a granular within-CC recognition of damage pixels, where the disparity between damage and text pixels is characterized by exploiting neighborhood transitions. This enables the formulation of a local homogeneity measure, where the neighborhood transition around an individual pixel is modeled using the propagation of the approximation coefficients of a stationary wavelet transform (SWT). The proposed global and local homogeneity measures are integrated as a joint likelihood in a Bayesian model with a Markov random field (MRF) prior, where DR is formulated as a maximum a posterior (MAP) inference which is addressed using Markov Chain Monte Carlo (MCMC) sampling. The resulting algorithm is tested on a set of real-life historical newspaper images containing damages of varying size and shape. The performance of the algorithm is evaluated using both F-measures and the Intersection-over-Union (IoU) metric, where test results demonstrate the promising potential of the proposed method.","Damage recognition, Text homogeneity, Neighborhood transition, Propagation of wavelet approximation, Bayesian inference",Tan Lu and Ann Dooms,https://www.sciencedirect.com/science/article/pii/S0031320321002211,https://doi.org/10.1016/j.patcog.2021.108034,0031-3203,2021,108034,118,Pattern Recognition,Bayesian damage recognition in document images based on a joint global and local homogeneity model,article,LU2021108034,
"COVID-19, as an infectious disease, has shocked the world and still threatens the lives of billions of people. Early detection of COVID-19 patients is an important issue for treating and controlling the disease from spreading. In this paper, a new strategy for detecting COVID-19 infected patients will be introduced, which is called Distance Biased NaÃ¯ve Bayes (DBNB). The novelty of DBNB as a proposed classification strategy is concentrated in two contributions. The first is a new feature selection technique called Advanced Particle Swarm Optimization (APSO) which elects the most informative and significant features for diagnosing COVID-19 patients. APSO is a hybrid method based on both filter and wrapper methods to provide accurate and significant features for the next classification phase. The considered features are extracted from Laboratory findings for different cases of people, some of whom are COVID-19 infected while some are not. APSO consists of two sequential feature selection stages, namely; Initial Selection Stage (IS2) and Final Selection Stage (FS2). IS2 uses filter technique to quickly select the most important features for diagnosing COVID-19 patients while removing the redundant and ineffective ones. This behavior minimizes the computational cost in FS2, which is the next stage of APSO. FS2 uses Binary Particle Swarm Optimization (BPSO) as a wrapper method for accurate feature selection. The second contribution of this paper is a new classification model, which combines evidence from statistical and distance based classification models. The proposed classification technique avoids the problems of the traditional NB and consists of two modules; Weighted NaÃ¯ve Bayes Module (WNBM) and Distance Reinforcement Module (DRM). The proposed DBNB tries to accurately detect infected patients with the minimum time penalty based on the most effective features selected by APSO. DBNB has been compared with recent COVID-19 diagnose strategies. Experimental results have shown that DBNB outperforms recent COVID-19 diagnose strategies as it introduce the maximum accuracy with the minimum time penalty.","COVID-19, Classification, NB, Feature selection, Wrapper, Optimization, Particle swarm",Warda M. Shaban and Asmaa H. Rabie and Ahmed I. Saleh and M.A. Abo-Elsoud,https://www.sciencedirect.com/science/article/pii/S0031320321002971,https://doi.org/10.1016/j.patcog.2021.108110,0031-3203,2021,108110,119,Pattern Recognition,Accurate detection of COVID-19 patients based on distance biased NaÃ¯ve Bayes (DBNB) classification strategy,article,SHABAN2021108110,
"Most state-of-the-art feature selection methods tend to overlook the structural relationship between a pair of samples associated with each feature dimension, which may encapsulate useful information for refining the performance of feature selection. Moreover, they usually consider candidate feature relevancy equivalent to selected feature relevancy, and therefore, some less relevant features may be misinterpreted as salient features. To overcome these issues, we propose a new feature selection method based on graph-based feature representations and the Fused Lasso framework in this paper. Unlike state-of-the-art feature selection approaches, our method has two main advantages. First, it can accommodate structural relationship between a pair of samples through a graph-based feature representation. Second, our method can enhance the trade-off between the relevancy of each individual feature on the one hand and its redundancy between pairwise features on the other. This is achieved through the use of a Fused Lasso framework applied to features reordered on the basis of their relevance with respect to the target feature. To effectively solve the optimization problem, an iterative algorithm is developed to identify the most discriminative features. Experiments demonstrate that our proposed approach can outperform its competitors on benchmark datasets.","Feature selection, Structural relationship, Fused lasso, Graph-based feature selection, Sparse learning, Correlated feature group",Lixin Cui and Lu Bai and Yue Wang and Philip S. Yu and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320321002454,https://doi.org/10.1016/j.patcog.2021.108058,0031-3203,2021,108058,119,Pattern Recognition,Fused lasso for feature selection using structural information,article,CUI2021108058,
"Minutia Cylinder Code (MCC) is an effective, high-quality representation of local minutia structures. MCC templates demonstrate fast and excellent fingerprint matching performance, but if compromised, they can be reverse-engineered to retrieve minutia information. In this paper, we propose alignment-free cancelable MCC-based templates by exploiting the MCC feature extraction and representation. The core component of our design is a dynamic random key model, called Dyno-key model. The Dyno-key model dynamically extracts elements from MCCâs binary feature vectors based on randomly generated keys. Those extracted elements are discarded after the block-based logic operations so as to increase security. Leveling with the performance of the unprotected, reproduced MCC templates, the proposed method exhibits competitive performance in comparison with state-of-the-art cancelable fingerprint templates, as evaluated over seven public databases, FVC2002 DB1-DB3, FVC2004 DB1 and DB2, and FVC2006 DB2 and DB3. The proposed cancelable MCC-based templates satisfy all the requirements of biometric template protection.","Cancelable biometrics, Minutia cylinder code, Cancelable fingerprint templates, Biometric template protection, Alignment-free",Aseel Bedari and Song Wang and Wencheng Yang,https://www.sciencedirect.com/science/article/pii/S0031320321002612,https://doi.org/10.1016/j.patcog.2021.108074,0031-3203,2021,108074,119,Pattern Recognition,Design of cancelable MCC-based fingerprint templates using Dyno-key model,article,BEDARI2021108074,
"In this paper, we propose a general framework in continual learning for generative models: Feature-oriented Continual Learning (FoCL). Unlike previous works that aim to solve the catastrophic forgetting problem by introducing regularization in the parameter space or image space, FoCL imposes regularization in the feature space. We show in our experiments that FoCL has faster adaptation to distributional changes in sequentially arriving tasks, and achieves state-of-the-art performance for generative models in task incremental learning. We discuss choices of combined regularization spaces towards different use case scenarios for boosted performance, e.g., tasks that have high variability in the background. Finally, we introduce a forgetfulness measure that fairly evaluates the degree to which a model suffers from forgetting. Interestingly, the analysis of our proposed forgetfulness score also implies that FoCL tends to have a mitigated forgetting for future tasks.","Catastrophic forgetting, Continual learning, Generative models, Feature matching, Generative replay, Pseudo-rehearsal",Qicheng Lao and Mehrzad Mortazavi and Marzieh Tahaei and Francis Dutil and Thomas Fevens and Mohammad Havaei,https://www.sciencedirect.com/science/article/pii/S0031320321003149,https://doi.org/10.1016/j.patcog.2021.108127,0031-3203,2021,108127,120,Pattern Recognition,FoCL: Feature-oriented continual learning for generative models,article,LAO2021108127,
"Blind image deblurring is a fundamental and challenging computer vision problem, which aims to recover both the blur kernel and the latent sharp image from only a blurry observation. Despite the superiority of deep learning methods in image deblurring have displayed, there still exists a major challenge with various non-uniform motion blur. Previous methods simply take all the image features as the input to the decoder, which handles different degrees (e.g. large blur, small blur) simultaneously, leading to challenges for sharp image generation. To tackle the above problems, we present a deep two-branch network to deal with blurry images via a component divided module, which divides an image into two components based on the representation of blurry degree. Specifically, two component attentive blocks are employed to learn attention maps to exploit useful deblurring feature representations on both large and small blurry regions. Then, the blur-aware features are fed into two-branch reconstruction decoders respectively. In addition, a new feature fusion mechanism, orientation-based feature fusion, is proposed to merge sharp features of the two branches. Both qualitative and quantitative experimental results show that our method performs favorably against the state-of-the-art approaches.","Non-uiniform deblurring, Component divided, Attention mechanism",Pei Wang and Wei Sun and Qingsen Yan and Axi Niu and Rui Li and Yu Zhu and Jinqiu Sun and Yanning Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321002697,https://doi.org/10.1016/j.patcog.2021.108082,0031-3203,2021,108082,120,Pattern Recognition,Non-uniform motion deblurring with blurry component divided guidance,article,WANG2021108082,
Isolation Forest represents a variant of Random Forest largely and successfully employed for outlier detection. The main idea is that outliers are likely to get isolated in a tree after few splits. The anomaly score is therefore a function inversely related to the leaf depth. This paper proposes enhanced anomaly scores of the Isolation Forest by making two different contributions. The first consists in weighing the path traversed by an object to obtain a more informative anomaly score. The second contribution employs a different aggregation function to combine the tree scores. We thoroughly evaluate the proposed methodology by testing it on sixteen datasets.,"Anomaly detection, Isolation forest, Anomaly score, Outliers",Antonella Mensi and Manuele Bicego,https://www.sciencedirect.com/science/article/pii/S0031320321003022,https://doi.org/10.1016/j.patcog.2021.108115,0031-3203,2021,108115,120,Pattern Recognition,Enhanced anomaly scores for isolation forests,article,MENSI2021108115,
"The paper is devoted to two problems: (1) reinforcement of SVM algorithms, and (2) justification of memorization mechanisms for generalization. (1) Current SVM algorithm was designed for the case when the risk for the set of nonnegative slack variables is defined by l1 norm. In this paper, along with that classical l1 norm, we consider risks defined by l2 norm and lâ norm. Using these norms, we formulate several modifications of the existing SVM algorithm and show that the resulting modified SVM algorithms can improve (sometimes significantly) the classification performance. (2) Generalization ability of existing learning algorithms is usually explained by arguments involving uniform convergence of empirical losses to the corresponding expected losses over a given set of functions. However, along with bounds for uniform convergence of empirical losses to the expected losses, the VC theory also provides bounds for relative uniform convergence. These bounds lead to a more accurate estimate of the expected loss. Advanced methods of estimating of expected risk of error have to leverage these bounds, which also support mechanisms of training data memorization, which, as the paper demonstrates, can improve classification performance.","Support vector machine, classification, learning theory, VC dimension, kernel function, Reproducing Kernel Hilbert space",Vladimir Vapnik and Rauf Izmailov,https://www.sciencedirect.com/science/article/pii/S0031320321002053,https://doi.org/10.1016/j.patcog.2021.108018,0031-3203,2021,108018,119,Pattern Recognition,Reinforced SVM method and memorization mechanisms,article,VAPNIK2021108018,
"Existing image fusion methods pay little research attention to human visual characteristics. However, human visual characteristics play an important role in visual processing tasks. To solve this problem, we propose a cross-modal image fusion method that combines illuminance factors and attention mechanisms. Human visual characteristics are studied and simulated in cross-modal image fusion task. Firstly, in order to reject high and low-frequency mixing and reduce the halo effect, we perform cross-modal image multi-scale decomposition. Secondly, in order to remove highlights, the visual saliency map and the deep feature map are combined with the illuminance fusion factor to perform high-low frequency non-linear fusion. Thirdly, the feature maps are selected through a channel attention network to obtain the final fusion map. Finally, we validate our image fusion method on public datasets of infrared and visible images. The experimental results demonstrate the superiority of our fusion method under the complex illumination environment. In addition, the experimental results also demonstrate the effectiveness of our simulation of human visual characteristics.","Image fusion, Deep learning, Non-linear characteristic, Feature selection characteristic",Aiqing Fang and Xinbo Zhao and Jiaqi Yang and Yanning Zhang and Xiang Zheng,https://www.sciencedirect.com/science/article/pii/S0031320321002296,https://doi.org/10.1016/j.patcog.2021.108042,0031-3203,2021,108042,119,Pattern Recognition,Non-linear and selective fusion of cross-modal images,article,FANG2021108042,
"In classification problems, detecting a skew class has extensively been studied in the machine learning community. Traditional extreme learning machine (ELM) algorithm becomes biased towards the majority class due to imbalance learning. To handle this problem, several extensions of ELM have been proposed such as variances-constrained weighted ELM (VW-ELM) and class-specific kernelized ELM (CSKELM). Kernelized ELM (KELM) has a better generalization capability than traditional ELM. This work proposes novel minimum variance embedded-kernelized weighted extreme learning machine (MVKWELM) and minimum variance-embedded class-specific kernelized extreme learning machine (MVCSKELM) methods for handling the imbalanced classification problems more effectively. These methods constitute novel extensions of the VW-ELM and CSKELM classifiers respectively. This minimum variance-embedding enhances the generalization capability of the algorithm by minimizing the intra-class variance. MVCSKELM uses the advantages of both the minimum variance-embedding framework and the class-specific regularization parameters. The proposed MVCSKELM also has comparable computational complexity compared to kernelized weighted ELM (KWELM). The proposed MVCSKELM adopted class-specific regularization parameters, which are determined by using class distribution. The proposed works are evaluated using benchmark real-world imbalanced datasets downloaded from the KEEL dataset repository. The experimental results demonstrate that MVKWELM and MVCSKELM achieve superior performance in contrast to KELM, KWELM, CCR-KELM, CSKELM, RUSBoost, WKSMOTE, VW-ELM, and EasyEnsemble for imbalance learning.","Extreme learning machine, Minimum variance-embedded class-specific kernelized extreme learning machine, Class imbalance problem, Classification",Bhagat Singh Raghuwanshi and Sanyam Shukla,https://www.sciencedirect.com/science/article/pii/S0031320321002569,https://doi.org/10.1016/j.patcog.2021.108069,0031-3203,2021,108069,119,Pattern Recognition,Minimum variance-embedded kernelized extension of extreme learning machine for imbalance learning,article,RAGHUWANSHI2021108069,
"Face recognition under adverse conditions, such as low-resolution, difficult illumination, blur and noise remains a challenging task. Among existing face recognition methods, Gabor wavelet plays a significant role and has robust performance under adverse conditions since it models the visual cortices of mammalian brain. It has been demonstrated the subbands of Gabor Wavelet (GW) can be efficiently represented by a covariance matrix. However, because covariance matrix does not belong to Euclidean space, Euclidean-based measure such as 2-norm cannot be directly applied to covariance matrix, and more importantly, it is difficult to incorporate learning techniques for the covariance matrix to promote the performance of face recognition. To address this issue, we propose two promising methods by learning the Covariance Matrix of Gabor Wavelet (LCMoG). The first method, called LCMoG-CNN, uses a shallow Convolutional Neural Network (CNN) to project the covariance matrices of GW into a feature vector of Euclidean space; the second method, called LCMoG-LWPZ, uses matrix-logarithm to embed the covariance matrix in the linear space and then uses Whitening Principal Component Analysis (WPCA) to learn the face features from the embedded covariance matrix. The proposed methods are effective to extract the fine features from the face image and have better performance than Deep CNN (DCNN) for small-varying face pose . For the large-varying face pose , LCMoG features combining with DCNN feature can enhance the performance of face recognition. In the experiments, the proposed methods yield promising recognition & verification accuracies under adverse conditions.","Face recognition, Gabor wavelet, Covariance matrix, Convolutional neural network",Chaorong Li and Yuanyuan Huang and Wei Huang and Fengqing Qin,https://www.sciencedirect.com/science/article/pii/S0031320321002727,https://doi.org/10.1016/j.patcog.2021.108085,0031-3203,2021,108085,119,Pattern Recognition,Learning features from covariance matrix of gabor wavelet for face recognition under adverse conditions,article,LI2021108085,
"Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and ResNeXt-56 are severely over-parameterized, necessitating a consequent increase in the computational resources required for model training which scales exponentially for increments in model depth. In this paper, we propose an Entropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust and simple, yet effective in resolving the problem of over-parameterization with regards to network depth of CNN model. The EBCLE heuristic employs a priori knowledge of the entropic data distribution of input datasets to determine an upper bound for convolutional network depth, beyond which identity transformations are prevalent offering insignificant contributions for enhancing model performance. Restricting depth redundancies by forcing feature compression and abstraction restricts over-parameterization while decreasing training time by 24.99% - 78.59% without degradation in model performance. We present empirical evidence to emphasize the relative effectiveness of broader, yet shallower models trained using the EBCLE heuristic, which maintains or outperforms baseline classification accuracies of narrower yet deeper models. The EBCLE heuristic is architecturally agnostic and EBCLE based CNN models restrict depth redundancies resulting in enhanced utilization of the available computational resources. The proposed EBCLE heuristic is a compelling technique for researchers to analytically justify their HyperParameter (HP) choices for CNNs. Empirical validation of the EBCLE heuristic in training CNN models was established on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10, MNIST) and four network architectures (DenseNet, ResNet, ResNeXt and EfficientNet B0-B2) with appropriate statistical tests employed to infer any conclusive claims presented in this paper.","Convolutional neural networks (CNNs), Depth redundancy, Entropy, Feature compression, EBCLE",Nidhi Gowdra and Roopak Sinha and Stephen MacDonell and Wei Qi Yan,https://www.sciencedirect.com/science/article/pii/S0031320321002442,https://doi.org/10.1016/j.patcog.2021.108057,0031-3203,2021,108057,119,Pattern Recognition,Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic,article,GOWDRA2021108057,
"COVID-19 leads to radiological evidence of lower respiratory tract lesions, which support analysis to screen this disease using chest X-ray. In this scenario, deep learning techniques are applied to detect COVID-19 pneumonia in X-ray images, aiding a fast and precise diagnosis. Here, we investigate seven deep learning architectures associated with data augmentation and transfer learning techniques to detect different pneumonia types. We also propose an image resizing method with the maximum window function that preserves anatomical structures of the chest. The results are promising, reaching an accuracy of 99.8% considering COVID-19, normal, and viral and bacterial pneumonia classes. The differentiation between viral pneumonia and COVID-19 achieved an accuracy of 99.8%, and 99.9% of accuracy between COVID-19 and bacterial pneumonia. We also evaluated the impact of the proposed image resizing method on classification performance comparing with the bilinear interpolation; this pre-processing increased the classification rate regardless of the deep learning architectures used. We c ompared our results with ten related works in the state-of-the-art using eight sets of experiments, which showed that the proposed method outperformed them in most cases. Therefore, we demonstrate that deep learning models trained with pre-processed X-ray images could precisely assist the specialist in COVID-19 detection.","COVID-19, X-ray, Deep learning, Pre-processing",Pablo Vieira and Orrana Sousa and Deborah MagalhÃ£es and Ricardo RabÃªlo and Romuere Silva,https://www.sciencedirect.com/science/article/pii/S0031320321002685,https://doi.org/10.1016/j.patcog.2021.108081,0031-3203,2021,108081,119,Pattern Recognition,Detecting pulmonary diseases using deep features in X-ray images,article,VIEIRA2021108081,
"Since positive data vectors are often naturally generated in various real-life applications, positive vectors modeling has become an important research topic. In this article, we tackle the problem of modeling positive sequential vectors through continuous hidden Markov models (HMMs). Motivated by several recent studies in which the generalized inverted Dirichlet (GID) distribution has provided better performance than the Gaussian distribution for modeling positive data, instead of adopting Gaussian mixture models (GMM) as the emission density for conventional continuous HMMs, we theoretically propose a novel HMM by considering the mixture of GID distributions as the emission density. Moreover, to cope with high-dimensional data which may contain irrelevant features, an unsupervised localized feature selection method is incorporated with our model, which results in a unified framework that can simultaneously perform positive sequential data modeling and feature selection. To learn the proposed model, we develop a convergence-guaranteed algorithm based on variational Bayes. The advantages of our model are demonstrated through both simulated data sets and a real-life application about human action recognition.","Continuous hidden Markov models, Generalized inverted Dirichlet, Mixture models, Variational Bayes, Localized feature selection",Wentao Fan and Ru Wang and Nizar Bouguila,https://www.sciencedirect.com/science/article/pii/S0031320321002600,https://doi.org/10.1016/j.patcog.2021.108073,0031-3203,2021,108073,119,Pattern Recognition,Simultaneous positive sequential vectors modeling and unsupervised feature selection via continuous hidden Markov models,article,FAN2021108073,
"Most of the temporal data mining tasks require representations to capture important characteristics of time series. Representation learning is challenging when time series differ in distributional characteristics and/or show irregularities such as varying lengths and missing observations. Moreover, when time series are multivariate, interactions between variables should be modeled efficiently. This study proposes a unified, flexible time series representation learning framework for both univariate and multivariate time series called Rand-TS. Rand-TS models density characteristics of each time series as a time-varying Gaussian distribution using random decision trees and embeds density information into a sparse vector. Rand-TS can work with time series of various lengths and missing observations, furthermore, it allows using customized features. We illustrate the classification performance of Rand-TS on 113 univariate, 19 multivariate along with 15 univariate time series with varying lengths from UCR database. The results show that in addition to its flexibility, Rand-TS provides competitive classification performance.","Time series, Representation learning, Random trees, Classification",Berk GÃ¶rgÃ¼lÃ¼ and Mustafa GÃ¶kÃ§e BaydoÄan,https://www.sciencedirect.com/science/article/pii/S0031320321002843,https://doi.org/10.1016/j.patcog.2021.108097,0031-3203,2021,108097,120,Pattern Recognition,Randomized trees for time series representation and similarity,article,GORGULU2021108097,
"Polarization image fusion aims to integrate intensity and degree of linear polarization images into one with more details, which is beneficial to improve the ability of targets detection under complex background. The fusion strategies in conventional methods are designed in a hand-crafted way and not robust to different fusion tasks. In this paper, we propose a novel and deep network to address the polarization image fusion issue with self-learned strategy. The network consists of Encoder, Fusion, and Decoder layers. Feature maps extracted by Encoder are fused, then fed into Decoder to generate fused images. Besides, a novel loss function is adopted to train the network in an unsupervised way, without ground truth of fused images. To verify the advantage, the network trained on polarization images is also used to infrared and visible images fusion, and multi-focus image fusion. Experimental results showed that our method outperforms several state-of-the-art methods in terms of visual quality and quantitative measurement. The proposed fused method can be applied in the military and civilian fields such as camouflage and hidden targets detection, medical diagnosis, and environmental monitoring.","Image fusion, Encoder-Decoder, Polarization image, Unsupervised learning",Junchao Zhang and Jianbo Shao and Jianlai Chen and Degui Yang and Buge Liang,https://www.sciencedirect.com/science/article/pii/S0031320321002326,https://doi.org/10.1016/j.patcog.2021.108045,0031-3203,2021,108045,118,Pattern Recognition,Polarization image fusion with self-learned fusion strategy,article,ZHANG2021108045,
"kNN is a widely used machine learning algorithm in many different domains because of its fairly good performance in actual cases and its simplicity. This study aims to enhance the performance of kNN for imbalanced datasets, a topic that has been relatively ignored in kNN research. The proposed kNN algorithm, called normalized class coherence change-based k-nearest neighbor (NCC-NN) algorithm, determines the label of a test sample by computing the normalized class coherence changes at class and sample levels for every possible class and assigning the sample to the class with the maximum value. It considers the tendency that the minority classes usually show the lower-class coherence than the majority class. NCC-kNN also utilizes the adaptive k for the class coherence, which is calculated in a weighted manner to reduce the sensitivity to the selection of k. NCC-kNN was applied to 20 benchmark datasets with varying class imbalance and coherence, and its performance was compared with that of five kNN algorithms, SMOTE and MetaCost with standard kNN as a base classifier. The proposed NCC-kNN outperformed the other kNN algorithms in classification of imbalanced data, especially for imbalanced data with low positive class coherence.","NN, Nearest neighbor classification, Imbalanced data, Class coherence",Kyoungok Kim,https://www.sciencedirect.com/science/article/pii/S0031320321003137,https://doi.org/10.1016/j.patcog.2021.108126,0031-3203,2021,108126,120,Pattern Recognition,Normalized class coherence change-based kNN for classification of imbalanced data,article,KIM2021108126,
"Biometrics technology is one of the most important and effective solutions for personal authentication. In recent years, as one of the emerging biometrics technologies, dorsal hand vein (DHV) biometrics has received a lot of attention. In fact, DHV biometrics has been studied for more than 30 years, during which different problems related to DHV recognition have been addressed. In this paper, we conduct a comprehensive survey on the state-of-the-art in DHV biometrics. Nearly all important aspects of DHV biometrics have been summarized, including the developmental history, data acquisition, databases, preprocessing algorithms, feature extraction and matching algorithms, information fusion schemes and commercial products. We also discuss the challenges and future directions in DHV biometrics research.","Biometrics, Dorsal hand vein, Survey",Wei Jia and Wei Xia and Bob Zhang and Yang Zhao and Lunke Fei and Wenxiong Kang and Di Huang and Guodong Guo,https://www.sciencedirect.com/science/article/pii/S0031320321003095,https://doi.org/10.1016/j.patcog.2021.108122,0031-3203,2021,108122,120,Pattern Recognition,A survey on dorsal hand vein biometrics,article,JIA2021108122,
"Top-rank learning identifies a real-valued ranking function that will provide more absolute top samples. These are highly reliable positive samples that are ranked higher than the highest-ranked negative samples. Therefore, top-rank learning is useful for tasks that require reliable decisions. Additionally, it inherits the merits of the ranking functions, such as robustness to the unbalanced condition. However, conventional top-rank learning tasks are formulated as linear or kernel-based problems and are thus limited in coping with complicated tasks. In this study, we propose a Top-rank convolutional neural network (TopRank CNN) to realize top-rank learning with representation learning for complicated tasks. Given that the original objective function of top-rank learning suffers from overfitting, we employ the p-norm relaxation of the original loss function in the proposed method. We prove the usefulness of TopRank CNN experimentally with medical diagnosis tasks that require reliable decisions and robustness to the unbalanced condition.","Top-rank learning, Representation learning, Medical diagnosis",Yan Zheng and Yuchen Zheng and Daiki Suehiro and Seiichi Uchida,https://www.sciencedirect.com/science/article/pii/S0031320321003253,https://doi.org/10.1016/j.patcog.2021.108138,0031-3203,2021,108138,120,Pattern Recognition,Top-rank convolutional neural network and its application to medical image-based diagnosis,article,ZHENG2021108138,
"Deep learning based subspace clustering networks have been a significant technique for motion segmentation, unsupervised image segmentation, image representation and compression, and face clustering by separating the high-dimensional data points into their representative low-dimensional linear subspaces. Effective feature selection is critical to remove redundant samples and select the representative feature subset from high-dimensional data space; hence deriving the number of subspaces, their dimensions, data segmentation, and a basis for each subspace. The effective self-representative feature selection and emphasis by scaling the feature map in the learned embedded space is required for deep learning based subspace clustering to reduce the number of parameters and dimension of the self-representative layer. In this paper, we propose a self-representative feature extraction deep neural network for unsupervised subspace clustering to improve representativeness and clustering ability. The extensive relevant results on various data demonstrate that deep subspace clustering employing self-representative features from high-dimensional data can effectively reduce the dimension of the self-representative layer while improving performance.","Subspace clustering, Self-representation, Deep subspace clustering",Sangwon Baek and Gangjoon Yoon and Jinjoo Song and Sang Min Yoon,https://www.sciencedirect.com/science/article/pii/S0031320321002284,https://doi.org/10.1016/j.patcog.2021.108041,0031-3203,2021,108041,118,Pattern Recognition,Deep self-representative subspace clustering network,article,BAEK2021108041,
"Data imbalance remains one of the factors negatively affecting the performance of contemporary machine learning algorithms. One of the most common approaches to reducing the negative impact of data imbalance is preprocessing the original dataset with data-level strategies. In this paper we propose a unified framework for imbalanced data over- and undersampling. The proposed approach utilizes radial basis functions to preserve the original shape of the underlying class distributions during the resampling process. This is done by optimizing the positions of generated synthetic observations with respect to the proposed potential resemblance loss. The final Potential Anchoring algorithm combines over- and undersampling within the proposed framework. The results of the experiments conducted on 60 imbalanced datasets show outperformance of Potential Anchoring over state-of-the-art resampling algorithms, including previously proposed methods that utilize radial basis functions to model class potential. Furthermore, the results of the analysis based on the proposed data complexity index show that Potential Anchoring is particularly well suited for handling naturally complex (i.e. not affected by the presence of noise) datasets.","Machine learning, Classification, Imbalanced data, Oversampling, Undersampling, Radial basis functions",MichaÅ Koziarski,https://www.sciencedirect.com/science/article/pii/S0031320321003010,https://doi.org/10.1016/j.patcog.2021.108114,0031-3203,2021,108114,120,Pattern Recognition,Potential Anchoring for imbalanced data classification,article,KOZIARSKI2021108114,
"Hashing based cross-modal retrieval has recently made significant progress. But straightforward embedding data from different modalities involving rich semantics into a joint Hamming space will inevitably produce false codes due to the intrinsic modality discrepancy and noises. We present a novel deep Robust Multilevel Semantic Hashing (RMSH) for more accurate multi-label cross-modal retrieval. It seeks to preserve fine-grained similarity among data with rich semantics,i.e., multi-label, while explicitly require distances between dissimilar points to be larger than a specific value for strong robustness. For this, we give an effective bound of this value based on the information coding-theoretic analysis, and the above goals are embodied into a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via fusing multiple hash codes to explore seldom-seen semantics, alleviating the sparsity problem of similarity information. Experiments on three benchmarks show the validity of the derived bounds, and our method achieves state-of-the-art performance.","Hashing, Multi-label, Cross-modal retrieval, Deep learning",Ge Song and Xiaoyang Tan and Jun Zhao and Ming Yang,https://www.sciencedirect.com/science/article/pii/S0031320321002715,https://doi.org/10.1016/j.patcog.2021.108084,0031-3203,2021,108084,120,Pattern Recognition,Deep robust multilevel semantic hashing for multi-label cross-modal retrieval,article,SONG2021108084,
"Using medical images for disease identification is an important application in the medical field. Graph Convolutional Network (GCN) is proposed to model multi-relational image and generate more informative image representations. Recently, the relations between medical images are utilized to identify diseases. This paper proposes a Gated GCN with Attention Convolutional Binary Neural Tree (GGAC) for Multi-Relational Image Identifying Disease. GGAC extracts the discriminative features of the image, strengthen the ability to model medical images, understands images representation deeply and then well captures the multi-modal relation between images. Firstly, an Attention Convolutional Binary Neural Tree based on the attention mechanism is designed to extract the fine-grained features of the images, and use the attention conversion operation on the edge of the tree structure to enhance the networkâs acquisition of key image features. Secondly, a Gated GCN is proposed to improve GCN performance by solving the problem of the weight distribution of different neighbors in the same-order neighborhood. Thirdly, a GCN propagation rule is used to transfer messages in multi-relational Graph and then solves the message passing problem of high-dimensional feature data in GCN. Finally, we verify GGAC on a multi-relational graph constructed on the Chest X-rays14. It can be seen from the experiment that overfitting and underfitting can be solved to a certain extent through the extraction and inference of the features of the multi-relational graph, and then GGAC has better performance than the state-of-the-art methods, and keeps good in model complexity.","Multi-relational graph, Gated graph convolutional network, Identifying disease, Attention transformer",Bing Yang and Yan Kang and Lan Zhang and Hao Li,https://www.sciencedirect.com/science/article/pii/S0031320321003009,https://doi.org/10.1016/j.patcog.2021.108113,0031-3203,2021,108113,120,Pattern Recognition,GGAC: Multi-relational image gated GCN with attention convolutional binary neural tree for identifying disease with chest X-rays,article,YANG2021108113,
"Human action evaluation (HAE) involves judgments about the abnormality and quality of human actions. If performed effectively, HAE based on skeleton data can be used to monitor the outcomes of behavioral therapies for Alzheimerâs disease (AD). In this paper, we propose a two-task graph convolutional network (2T-GCN) to represent skeleton data for HAE tasks involving abnormality detection and quality evaluation. The network is first evaluated using the UI-PRMD dataset and demonstrates accurate abnormality detection. Regarding quality evaluation, in addition to laboratory-collected UI-PRMD data, we test the network on a set of real exercise data collected from patients with AD. A numerical score indicating the degree to which actions deviate from normal is taken to reflect the severity of AD; thus, we apply 2T-GCN to determine such scores. Experimental results show that numerical scores for certain exercises performed by patients with AD are consistent with their AD severity level as identified by clinical staff. This corroboration highlights the potential of our approach for monitoring AD and other neurodegenerative diseases.","Human action evaluation, Alzheimerâs disease, Graph neural network, Abnormality detection",Bruce X.B. Yu and Yan Liu and Keith C.C. Chan and Qintai Yang and Xiaoying Wang,https://www.sciencedirect.com/science/article/pii/S003132032100282X,https://doi.org/10.1016/j.patcog.2021.108095,0031-3203,2021,108095,119,Pattern Recognition,Skeleton-based human action evaluation using graph convolutional network for monitoring Alzheimerâs progression,article,YU2021108095,
"In many practical credit evaluation problems, a lot of manpower as well as financial and material resources are required to label samples. Therefore, in the process of labeling, only a small number of samples with category labels can be obtained to train classification models and a large number of customer samples is abandoned without category labels. To solve this problem, we introduce a semi-supervised support vector machine (SVM) technology and combines it with a multi-layer convolution kernel to construct a semi-supervised multi-layer convolution kernel SVM (SSMCK) for category customer credit assessment data sets. We first use a basic solution of the generalized differential operator to generate a base convolution kernel function in the H1 space, and then use the multi-layer strategy of deep learning to construct the multi-layer convolution kernel in the H2 and H3 space (called the family of multi-layer convolution kernel) by using the kernel functions in the H1 space. We further propose a semi-supervised multi-layer convolution kernel SVM algorithm based on the category center estimation and develop two novel SSMCK methods to improve the classification ability: the SSMCK based on multi-kernel learning (SSMCK-MKL) and the SSMCK based on alternative optimization (SSMCK-AO). Finally, experimental verification and analysis is carried out on three customer credit evaluation data sets. The results show that our methods outperforms or are comparable to some the state-of-the-art credit evaluation models.","Semi-supervised learning, SVM, Convolution kernel function, Random sampling, Multi-layer kernel",Lixiang Xu and Lixin Cui and Thomas Weise and Xinlu Li and Zhize Wu and Feiping Nie and Enhong Chen and Yuanyan Tang,https://www.sciencedirect.com/science/article/pii/S0031320321003125,https://doi.org/10.1016/j.patcog.2021.108125,0031-3203,2021,108125,120,Pattern Recognition,Semi-supervised multi-Layer convolution kernel learning in credit evaluation,article,XU2021108125,
"In this paper, we propose a new deep neural network classifier that simultaneously maximizes the inter-class separation and minimizes the intra-class variation by using the polyhedral conic classification function. The proposed method has one loss term that allows the margin maximization to maximize the inter-class separation and another loss term that controls the compactness of the class acceptance regions. Our proposed method has a nice geometric interpretation using polyhedral conic function geometry. We tested the proposed method on various visual classification problems including closed/open set recognition and anomaly detection. The experimental results show that the proposed method typically outperforms other state-of-the-art methods, and becomes a better choice compared to other tested methods especially for open set recognition type problems. The source code of the proposed method is available at https://github.com/bdrhn9/dc-epcc.","Polyhedral conic classifier, Deep learning, Open set recognition, Image classification, Anomaly detection",Hakan Cevikalp and Bedirhan Uzun and Okan KÃ¶pÃ¼klÃ¼ and Gurkan Ozturk,https://www.sciencedirect.com/science/article/pii/S0031320321002673,https://doi.org/10.1016/j.patcog.2021.108080,0031-3203,2021,108080,119,Pattern Recognition,Deep compact polyhedral conic classifier for open and closed set recognition,article,CEVIKALP2021108080,
"Fashion landmark estimation aims at locating functional key points of clothes, which has wide potential applications in electronic commerce. However, due to the occlusion and weak outline information, landmark estimation occurs outliers and duplicate detection problems. To alleviate these issues, we propose Position Constraint Loss (PCLoss) to constrain error landmark locations by utilizing the position relationship of landmarks. Specifically, PCLoss adds a regularization term for each landmark to regularize their relative positions, and it can be easily applied to both regression and heatmap based methods without extra computation during inference. Unlike existing approaches that propagate landmark information between feature layers by specific network structures, PCLoss introduces position relations of landmarks in the label space without modifying the network structure. In addition, we leverage the skeleton-like relation of clothing to further strengthen position constraints between landmarks. Extensive experimental results on DeepFashion, FLD and FashionAI demonstrate that our methods can effectively increase the performance of mainstream frameworks by a large margin. We also explore the effectiveness of PCLoss on human pose estimation task, and the experimental results on COCO 2017 prove the generality of our methods on other key point estimation tasks.","Fashion landmark estimation, Position constraint, Loss function, Skeleton-like characteristic",Meijia Song and Hong Liu and Wei Shi and Xia Li,https://www.sciencedirect.com/science/article/pii/S0031320321002156,https://doi.org/10.1016/j.patcog.2021.108028,0031-3203,2021,108028,118,Pattern Recognition,PCLoss: Fashion Landmark Estimation with Position Constraint Loss,article,SONG2021108028,
"This paper proposes a method to compress deep neural networks (DNNs) based on interpretability. For a trained DNN model, the activation maximization technique is first used to visualize every filter of the DNN model. Then, a single-layer filter pruning approach is introduced from what is learned by visualization. The entire DNN model is compressed layer by layer by using the single-layer filter pruning method in which the compression of the current layer is based on the compression of the preceding layers. Importantly, in addition to effective compression, the proposed method renders a better interpretation of the deep learning process. With a 60% compression rate of the VGG-16, our method achieves 0.8429 Top-1 accuracy under CIFAR-10, with a slight accuracy drop of only 0.0322, and the storage space of the model can be compressed to 9.42 Mb. For a modern DNN model such as ResNet50, our visualization-based filter pruning method is significantly better than other pruning strategies in different convolutional layers under different compression rates and the larger ImageNet dataset. After pruning, the computation cost and storage requirement of the DNN can be significantly reduced, which means that complex DNN models can be easily implemented in small mobile devices, thus enabling the efficient use of DNNs in the Internet of Things technologies.","Deep neural network (DNN), Convolutional neural network (CNN), Visualization, Compression",Kaixuan Yao and Feilong Cao and Yee Leung and Jiye Liang,https://www.sciencedirect.com/science/article/pii/S0031320321002430,https://doi.org/10.1016/j.patcog.2021.108056,0031-3203,2021,108056,119,Pattern Recognition,Deep neural network compression through interpretability-based filter pruning,article,YAO2021108056,
"This paper proposes a new principal component analysis method in the wavelet domain, which is useful for dimension reduction and feature extraction of multiple non-stationary time series. The proposed method is constructed using a novel combination of eigenanalysis and the local wavelet spectrum defined in the locally stationary wavelet process. Therefore, we can expect the proposed method to reflect a more generalized non-stationary time series beyond some limited types of signals that existing methods have performed. We investigate the theoretical results of estimated principal components and their loadings. The results of numerical examples, including the analysis of real seismic data and financial data, show the promising empirical properties of the proposed approach.","Principal component analysis, Non-stationary time series, Wavelet process, Feature extraction, Seismic data",Yaeji Lim and Junhyeon Kwon and Hee-Seok Oh,https://www.sciencedirect.com/science/article/pii/S0031320321002831,https://doi.org/10.1016/j.patcog.2021.108096,0031-3203,2021,108096,119,Pattern Recognition,Principal component analysis in the wavelet domain,article,LIM2021108096,
"Dynamic hand skeletons consisting of discrete spatial-temporal finger joint clouds effectively convey the intentions of communicators. Previous graph convolutional networks (GCNs) relying on human hand-crafted inductive biases have been quickly promoted for skeleton-based hand gesture recognition (SHGR). However, most existing graph constructions for GCN-based solutions are set manually, only considering the physical topology of the hand skeleton, and the fixed dependencies among hand joints may lead to suboptimal models. To enrich the local dependencies, we emphasize that hand skeletons can be seen from two views: explicit joint clouds and implicit skeleton topology. Starting from those two views of hand gestures, we attempt to introduce dynamics and diversities into the local neighborhood of the graph by dividing it into sets of physical neighbors, temporal neighbors and varying neighbors. Next, we systematically proceed with three innovations, including the novel edge-varying graph, normalized edge convolution operation, and zig-zag sampling strategy, to alleviate the challenges resulting from engineering practices. Finally, spatial-based GCNs called normalized edge convolutional networks are constructed for hand gesture recognition. Experiments on publicly available hand datasets show that our work is stable for performing state-of-the-art gesture recognition, and ablation experiments are also provided to validate each contribution.","Skeleton-based hand gesture recognition, Edge-varying graph, Normalized edge convolution, Zig-zag sampling strategy",Fangtai Guo and Zaixing He and Shuyou Zhang and Xinyue Zhao and Jinhui Fang and Jianrong Tan,https://www.sciencedirect.com/science/article/pii/S0031320321002314,https://doi.org/10.1016/j.patcog.2021.108044,0031-3203,2021,108044,118,Pattern Recognition,Normalized edge convolutional networks for skeleton-based hand gesture recognition,article,GUO2021108044,
"Human trajectory prediction is an important topic in several application domains, ranging from self-driving cars to environment design and planning, from socially-aware robots to intelligent tracking systems. This complex subject comes with different challenges, such as human-space interaction, human-human interaction, multimodality, and generalizability. Currently, these challenges, especially generalizability, have not been completely explored by state-of-the-art works. This work attempts to fill this gap by proposing and defining new methods and metrics to help understand trajectories. In particular, new deep learning models based on Long Short-Term Memory and Generative Adversarial Network architectures are used in both unimodal and multimodal contexts. These approaches are evaluated with new error metrics, which normalize some biases in standard metrics. Tests have been assessed using newly collected datasets characterized by a higher diversity and lower linearity than those used in state-of-the-art works. The results prove that the proposed models and datasets are comparable to and yield better generalizability than state-of-the-art works. Moreover, we also prove that our datasets better represent multimodal scenarios (allowing for multiple possible behaviors) and that human trajectories are moderately influenced by their spatial region and slightly influenced by their date and time.","Trajectory generation, Trajectory prediction, LSTM, GANs",Luca Rossi and Marina Paolanti and Roberto Pierdicca and Emanuele Frontoni,https://www.sciencedirect.com/science/article/pii/S003132032100323X,https://doi.org/10.1016/j.patcog.2021.108136,0031-3203,2021,108136,120,Pattern Recognition,Human trajectory prediction and generation using LSTM models and GANs,article,ROSSI2021108136,
"From a series of observations, we have inferred that human actions in videos are defined by a set of significant frames. In this paper, we propose a weakly-supervised temporal attention 3D network for human action recognition, called as TA3DNet, to accelerate 3D convolutional neural networks (3D CNNs) by temporally assigning different importance to each frame. First, we obtain short-term frames with long-term connection by regularly or randomly skipping frames to avoid temporal redundancy, and apply 3D convolutional layers to extract features for action recognition. Then, we apply a temporal attention module to assign different weights to each frame. We train the temporal attention module in a weakly-supervised manner that updates weights based on only class labels without event information and extra labels. Thus, TA3DNet reduces the number of input frames and constructs a lightweight network for action recognition. Experimental results demonstrate that TA3DNet achieves high performance on two challenging datasets (UCF101 and HMDB51) and outperforms state-of-the-art methods for action recognition.","Action recognition, Temporal attention, Convolutional neural network, Weakly-supervised learning, Video analysis, Video classification",Jonghyun Kim and Gen Li and Inyong Yun and Cheolkon Jung and Joongkyu Kim,https://www.sciencedirect.com/science/article/pii/S0031320321002557,https://doi.org/10.1016/j.patcog.2021.108068,0031-3203,2021,108068,119,Pattern Recognition,Weakly-supervised temporal attention 3D network for human action recognition,article,KIM2021108068,
"For a learning task, Gaussian process (GP) is interested in learning the statistical relationship between inputs and outputs, since it offers not only the prediction mean but also the associated variability. The vanilla GP however is hard to learn complicated distribution with the property of, e.g., heteroscedastic noise, multi-modality and non-stationarity, from massive data due to the Gaussian marginal and the cubic complexity. To this end, this article studies new scalable GP paradigms including the non-stationary heteroscedastic GP, the mixture of GPs and the latent GP, which introduce additional latent variables to modulate the outputs or inputs in order to learn richer, non-Gaussian statistical representation. Particularly, we resort to different variational inference strategies to arrive at analytical or tighter evidence lower bounds (ELBOs) of the marginal likelihood for efficient and effective model training. Extensive numerical experiments against state-of-the-art GP and neural network (NN) counterparts on various tasks verify the superiority of these scalable modulated GPs, especially the scalable latent GP, for learning diverse data distributions.","Gaussian process, Modulation, Scalability, Heteroscedastic noise, Multi-modality, Non-stationarity",Haitao Liu and Yew-Soon Ong and Xiaomo Jiang and Xiaofang Wang,https://www.sciencedirect.com/science/article/pii/S0031320321003083,https://doi.org/10.1016/j.patcog.2021.108121,0031-3203,2021,108121,120,Pattern Recognition,Modulating scalable Gaussian processes for expressive statistical learning,article,LIU2021108121,
"Multi-label feature selection is an efficient technique to alleviate the high dimensionality for multi-label learning. Existing multi-label feature selection methods based on information theory either deal with labels individually or treat all label relationships as redundancy. However, two important and being ignored issues are the different effects of label relationships and the dynamic changes of label relationships in measuring different candidate features. To address these issues, we first distinguish three types of label relationships: label independence, label redundancy and label supplementation. Second, we consider the changes of label relationships based on different features. By analyzing the differences and the changes of label relationships, two new methods named LSMFS and MLSMFS are proposed, which extracts all supplementary information and the maximum supplementary information of features for each label from other labels, respectively. Finally, experiments on fifteen benchmark multi-label data sets demonstrate the effectiveness of the proposed methods against nine other methods.","Multi-label learning, Multi-label feature selection, Information theory, Label relationships",Ping Zhang and Guixia Liu and Wanfu Gao and Jiazhi Song,https://www.sciencedirect.com/science/article/pii/S0031320321003241,https://doi.org/10.1016/j.patcog.2021.108137,0031-3203,2021,108137,120,Pattern Recognition,Multi-label feature selection considering label supplementation,article,ZHANG2021108137,
"In a previous study, we have explored how to decompose the global entropy of a network into edge components using a graph-spectral decomposition technique. Here, we develop this work in more depth to understand the role of edge entropy as an efficient and effective tool in analysing network structure. We use the edge entropy distribution as a network feature or characterisation and combine it with linear discriminant analysis to distinguish different types of network model and structure. Interpreting the normalised Laplacian matrix as the network Hamiltonian (or energy) operator, the network is assumed to be in thermodynamic equilibrium with a heat bath where the energy states correspond to the normalised Laplacian eigenvalues. To model the way in which particles occupy the energy states, we explore the use of three different spin-dependent statistical models to determine the thermodynamic entropy of the network. These are a) the classical spinless Maxwell-Boltzmann distribution, and two models based on quantum mechanical spin-statistics, namely b) the Bose-Einstein model for particles with integer spin, and c) the Fermi-Dirac model for particles with half-integer spin. By using the spectral decomposition of the Laplacian, we illustrate how to project out the edge-entropy components from the global network entropy. In this way, the detailed distribution of entropy across the edges of a network can be constructed. Compared to our previous study of the von Neumann edge entropy, where the edge entropy just depends on the degrees of the nodes forming an edge, in the case of the new statistical mechanical model, there is a more subtle dependence of the edge entropy on the structure of a network. We illustrate how this new edge entropy distribution can be used to more effectively identify variations in network structure, in particular for edges incorporating nodes of large degree. Numerical experiments on synthetic and real-world data-sets are presented to evaluate the qualitative and quantitative differences in performance.","Network edge entropy, Spin statistics, Partition function",Jianjia Wang and Richard C. Wilson and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320321002272,https://doi.org/10.1016/j.patcog.2021.108040,0031-3203,2021,108040,118,Pattern Recognition,Network edge entropy decomposition with spin statistics,article,WANG2021108040,
"COVID-19 is an infectious disease caused by a newly discovered type of coronavirus called SARS-CoV-2. Since the discovery of this disease in late 2019, COVID-19 has become a worldwide concern, mainly due to its high degree of contagion. As of April 2021, the number of confirmed cases of COVID-19 reported to the World Health Organization has already exceeded 135 million worldwide, while the number of deaths exceeds 2.9 million. Due to the impacts of the disease, efforts in the literature have intensified in terms of studying approaches aiming to detect COVID-19, with a focus on supporting and facilitating the process of disease diagnosis. This work proposes the application of texture descriptors based on phylogenetic relationships between species to characterize segmented CT volumes, and the subsequent classification of regions into COVID-19, solid lesion or healthy tissue. To evaluate our method, we use images from three different datasets. The results are promising, with an accuracy of 99.93%, a recall of 99.93%, a precision of 99.93%, an F1-score of 99.93%, and an AUC of 0.997. We present a robust, simple, and efficient method that can be easily applied to 2D and/or 3D images without limitations on their dimensionality.","COVID-19, Computed tomography, 3D texture analysis, Phylogenetic diversity",VitÃ³ria {de Carvalho Brito} and Patrick Ryan Sales {dos Santos} and Nonato Rodrigues {de Sales Carvalho} and Antonio Oseas {de Carvalho Filho},https://www.sciencedirect.com/science/article/pii/S0031320321002703,https://doi.org/10.1016/j.patcog.2021.108083,0031-3203,2021,108083,119,Pattern Recognition,COVID-index: A texture-based approach to classifying lung lesions based on CT images,article,DECARVALHOBRITO2021108083,
"Low-resolution (LR) face verification has received much attention because of its wide applicability in real scenarios, especially in long-distance surveillance. However, the poor quality and scarcity of training data make the accuracy far from satisfactory. In this paper, we propose an end-to-end LR face translation and verification framework to improve the generation quality of face images and face verification accuracy simultaneously. We design a dual domain adaptive structure to generate high-quality images. On one hand, the structure can reduce the domain gap between training data and test data. On the other hand, the structure preserves identity consistency and low-level attributes. Meanwhile, in order to make the whole model more robust, we treat the generated images of the target domain as an extension of the training data. We conduct extensive comparative experiments on multiple benchmark data sets. Experimental results verify that our method achieves improved results in high-quality face generation and LR face verification. In particular, our model DDAT reduces FID to 18.63 and 39.55 on the source and the target domain from 254.7 and 206.19 of the up-sampling results, respectively. Our method outperforms competing approaches by more than 10 percentage points in terms of face verification accuracy on multiple surveillance benchmarks.","Low-resolution face verification, Domain adaptation, Image translation, GAN",Qianfen Jiao and Rui Li and Wenming Cao and Jian Zhong and Si Wu and Hau-San Wong,https://www.sciencedirect.com/science/article/pii/S0031320321002946,https://doi.org/10.1016/j.patcog.2021.108107,0031-3203,2021,108107,120,Pattern Recognition,DDAT: Dual domain adaptive translation for low-resolution face verification in the wild,article,JIAO2021108107,
"Novel coronavirus 2019 (COVID-19) has spread rapidly around the world and is threatening the health and lives of people worldwide. Early detection of COVID-19 positive patients and timely isolation of the patients are essential to prevent its spread. Chest X-ray images of COVID-19 patients often show the characteristics of multifocality, bilateral hairy glass turbidity, patchy network turbidity, etc. It is crucial to design a method to automatically identify COVID-19 from chest X-ray images to help diagnosis and prognosis. Existing studies for the classification of COVID-19 rarely consider the role of attention mechanisms on the classification of chest X-ray images and fail to capture the cross-channel and cross-spatial interrelationships in multiple scopes. This paper proposes a multi-kernel-size spatial-channel attention method to detect COVID-19 from chest X-ray images. Our proposed method consists of three stages. The first stage is feature extraction. The second stage contains two parallel multi-kernel-size attention modules: multi-kernel-size spatial attention and multi-kernel-size channel attention. The two modules capture the cross-channel and cross-spatial interrelationships in multiple scopes using multiple 1D and 2D convolutional kernels of different sizes to obtain channel and spatial attention feature maps. The third stage is the classification module. We integrate the chest X-ray images from three public datasets: COVID-19 Chest X-ray Dataset Initiative, ActualMed COVID-19 Chest X-ray Dataset Initiative, and COVID-19 radiography database for evaluation. Experimental results demonstrate that the proposed method improves the performance of COVID-19 detection and achieves an accuracy of 98.2%.","Deep learning, Attention, Coronavirus, X-ray images, Multi-scale",Yuqi Fan and Jiahao Liu and Ruixuan Yao and Xiaohui Yuan,https://www.sciencedirect.com/science/article/pii/S0031320321002429,https://doi.org/10.1016/j.patcog.2021.108055,0031-3203,2021,108055,119,Pattern Recognition,COVID-19 Detection from X-ray Images using Multi-Kernel-Size Spatial-Channel Attention Network,article,FAN2021108055,
"Existing studies on eye center localization have mostly applied localization methods on faces viewed from frontal angles or faces with small yaw-rotation angles. This study proposed a novel eye center localization method that can effectively localize the eye centers under various situations on multiview faces with large yaw-rotation angles (from +67.5Â° to â67.5Â°). First, this study developed a multiview face detector that can be applied to large yaw angles and can flexibly detect and precisely capture the face region. The face detector can facilitate the generation of satisfactory results by a complete representation generative adversarial network (CR-GAN). Because a large yaw-rotation angle can substantially reduce the completeness of eye representation in an image or even cause the eyes to disappear within the face image, this study used a CR-GAN to produce frontal face images to solve the problem of incomplete representation in multiview face images. Furthermore, this study proposed a new iris-ripple filter to increase the accuracy and robustness of gradient localization. Finally, a new depth corresponding-points conversion method was proposed to automatically estimate the rotation variable between two faces and the conversion relationship between the eye centers. This method can effectively solve the problem of instability resulting from a CR-GAN during eye generation and can ensure localization accuracy for eyeballs with subtle changes. According to the experimental results, compared with other advanced methods, the proposed method exhibited higher accuracy and robustness in eye center localization when applied to images obtained from four databases that involved various challenging situations, including large yaw-rotation angle, illumination change, head pose change, gaze interaction, and complete occlusion of eyes.","Eye center localization, Multiview face, Face generation, Geometric transformation",Wei-Yen Hsu and Chi-Jui Chung,https://www.sciencedirect.com/science/article/pii/S003132032100265X,https://doi.org/10.1016/j.patcog.2021.108078,0031-3203,2021,108078,119,Pattern Recognition,A novel eye center localization method for multiview faces,article,HSU2021108078,
"Single-image deraining is a kind of computer vision task that aims to restore the image that be degraded by rain streaks, which motivates existing methods to either directly translate the rainy image to its clean one, or indirectly learn the rain residual based on the prior information. However, both methodologies harm the generalization ability due to the limited diversity of the training samples, comparing with the endless varieties of the real-world rainy images. Such fact inspires us to take the merit of meta-learning and propose a meta-learning based representation learning network to learn the transferable embeddings of the rainy/clean images, while their discrepancies are characterized by the relation vector, which is generated by the subsequent meta-learning based relation learning network. These networks are leveraged into the meta-learning based deraining network (MLDN) to enhance the generalization ability by removing the latent relation vector from the transferable embedding of the rainy image and generate high-quality deraining result. Superior performance is achieved by MLDN, which has averaged 4% better than the state-of-the-arts.","Meta-learning, Relation network, Single-image deraining, Representation learning",Xinjian Gao and Yang Wang and Jun Cheng and Mingliang Xu and Meng Wang,https://www.sciencedirect.com/science/article/pii/S0031320321003113,https://doi.org/10.1016/j.patcog.2021.108124,0031-3203,2021,108124,120,Pattern Recognition,Meta-learning based relation and representation learning networks for single-image deraining,article,GAO2021108124,
"Cancer is a cluster of diseases caused due to unusual cell growth. This paper aims to discover cancer prediction from the microarray gene expression data using the selected features. The metaheuristic search algorithms select the global and local optimal features using population and neighbourhood based algorithms. Although the ant colony optimization and genetic algorithm search for the global optimal features from the dataset entails enhanced classification, sometimes there occur some challenges in the selection of neighbourhood features. Against this background, two feature selection algorithms are proposed to hybridize tabu search, a neighbourhood based search algorithm with global optimal feature selection algorithm. Those are (1) Ant Colony Optimization and Tabu search with Fuzzy Rough set for Optimal feature selection (ACTFRO) algorithm, (2) Genetic algorithm and Tabu search with Fuzzy Rough set for Optimal feature selection (GATFRO) algorithm. The performance of proposed feature selection algorithms is assessed through a fuzzy rough nearest neighbour classifier using ten-fold cross validation. Four cancer medical datasets and one non-medical dataset are used to analyse the performance of the proposed algorithms in terms of classification accuracy, computation time, sensitivity, specificity, f-measure, receiver operation characteristics and positive predicted value. Results derived from the different performance metrics confirm that the proposed algorithms evidence effective global and local feature selection hybridization with improved results.","Ant Colony Optimization, Genetic Algorithm, Tabu Search, Fuzzy Rough set, Optimal feature selection",L. Meenachi and S. Ramakrishnan,https://www.sciencedirect.com/science/article/pii/S0031320321002661,https://doi.org/10.1016/j.patcog.2021.108079,0031-3203,2021,108079,119,Pattern Recognition,Metaheuristic Search Based Feature Selection Methods for Classification of Cancer,article,MEENACHI2021108079,
"Users are sometimes interested in specific segments of an untrimmed video when using the video search engine. Targeting at this demand, we explore a novel research topic of text query based video segment retrieval (VSR). Different from the conventional video retrieval task or localizing text descriptions in a single video, it requires the retrieval of the most relevant video from a large collection as well as localizing the start and end timestamps of a segment that matches the text query best from the video. A direct solution is to perform video-level matching first, and then apply description localization among such video candidates. Such two-stage based methods are not able to utilize complementary information of each stage, and are time-consuming in inference. In this paper, We propose VSRNet, an end-to-end framework that efficiently retrieves video at segment granularity with two branches. In the first branch, individual videos and texts are mapped to a common space for stand-alone ranking. In the second branch, we propose a supervised text-aligned attention mechanism and calculate the response of every frame to the text query, from which the frames with high scores are aggregated as segment proposals. Extensive experiments conducted on ActivityNet Captions and DiDeMo verify the effectiveness of our method and show that our solution significantly outperforms the state of the art.","Video segment retrieval, Video retrieval, Description localization",Xiao Sun and Xiang Long and Dongliang He and Shilei Wen and Zhouhui Lian,https://www.sciencedirect.com/science/article/pii/S0031320321002144,https://doi.org/10.1016/j.patcog.2021.108027,0031-3203,2021,108027,119,Pattern Recognition,VSRNet: End-to-end video segment retrieval with text query,article,SUN2021108027,
"Ultra-fine-grained visual categorization (ultra-FGVC) categorizes objects with more similar patterns between classes than those in fine-grained visual categorization (FGVC), e.g., where the spectrum of granularity significantly moves down from classifying species to classifying cultivars within the same species. It is considered as an open research problem mainly due to the following challenges. First, the inter-class differences among images are much smaller by level of orders (e.g., cultivars in the same species) than those in current FGVC tasks (e.g., species). Second, there is only a few samples per category, which is beyond the ability of most large training data favored convolutional neural network methods. To address these problems, we propose a novel random mask covariance network (MaskCOV), which integrates an auxiliary self-supervised learning module with a powerful in-image data augmentation scheme for the ultra-FGVC. Specifically, we first uniformly partition input images into patches and then augment data by randomly shuffling and masking these patches. On top of that, we introduce an auxiliary self-supervised learning module of predicting the spatial covariance context of these patches to increase discriminability of our network for classification. Very encouraging experimental results of the proposed method in comparison with the state-of-the-art benchmarks demonstrate its superiority and potential of MaskCOV concept, which pushes research boundary forward from the fine-grained to the ultra-fine-grained visual categorization.","Ultra-fine-grained visual categorization, Fine-grained visual categorization, Covariance matrix, Self-supervised learning",Xiaohan Yu and Yang Zhao and Yongsheng Gao and Shengwu Xiong,https://www.sciencedirect.com/science/article/pii/S0031320321002545,https://doi.org/10.1016/j.patcog.2021.108067,0031-3203,2021,108067,119,Pattern Recognition,MaskCOV: A random mask covariance network for ultra-fine-grained visual categorization,article,YU2021108067,
"Arbitrary-view human action recognition is still a big challenge due to the view changes. A possible solution is to enlarge the view range of action samples in the training set. Therefore, we propose a Two-Branch Novel-View action Generation approach based on auxiliary conditional GAN, which generates a novel-view action sample for arbitrary-view human action recognition. The generated sample enlarge the view range of action samples for training. Furthermore, to narrow the representation of actions in different views, we propose a view-domain generalization model that improves the recognition performance of arbitrary-view human action recognition. Our approach is evaluated on three large-scale RGB+D skeleton datasets including UESTC varying-view RGB+D dataset, NTU RGB+D 60, and NTU RGB+D 120 datasets, with two types of view-invariant evaluations, i.e., the cross-view, and arbitrary-view recognition. The proposed approach achieves outstanding performance in human action recognition.","Arbitrary-view action recognition, Novel-view action generation, View domain generalization",Kumie Gedamu and Yanli Ji and Yang Yang and LingLing Gao and Heng Tao Shen,https://www.sciencedirect.com/science/article/pii/S0031320321002302,https://doi.org/10.1016/j.patcog.2021.108043,0031-3203,2021,108043,118,Pattern Recognition,Arbitrary-view human action recognition via novel-view action generation,article,GEDAMU2021108043,
"Video object segmentation is one of the most practical computer vision tasks, especially in the unsupervised case, which has no manually labeled segmentation mask at the beginning of a video sequence. In this paper, we propose a new real-time unsupervised video object segmentation network. Based on the encoder-decoder framework, we present a Dynamic ASPP module and a RNN-Conv module. The former adds a dynamic selection mechanism into the Astrous Spatial Pyramid Pooling structure, and then the dilated convolutional kernels adaptively select appropriate features according to the scales by the channel attention mechanism. Compared with directly concatenating the dilated convolutional features, dynamically selecting feature maps reduces the amount of parameters and makes the module more efficient. The RNN-Conv module incorporates the RNN units with external convolutional blocks, aggregating the temporal features of a video sequence with the spatial information extracted by the convolutional network. We stack this module to extract deeper spatiotemporal features than the traditional RNN network. This module helps to avoid the gradient disappearance and explosion during network training. We test our network on the popular video object segmentation datasets. The experiment results demonstrate the effectiveness of our model.11Our code is available at https://github.com/Sanyuan-Zhao/Real-Time-and-Light-Weighted-UVOS","Unsupervised video object segmentation, Salient object detection",Zongji Zhao and Sanyuan Zhao and Jianbing Shen,https://www.sciencedirect.com/science/article/pii/S0031320321003071,https://doi.org/10.1016/j.patcog.2021.108120,0031-3203,2021,108120,120,Pattern Recognition,Real-time and light-weighted unsupervised video object segmentation network,article,ZHAO2021108120,
"The emergence of geometric deep learning as a novel framework to deal with graph-based representations has faded away traditional approaches in favor of completely new methodologies. In this paper, we propose a new framework able to combine the advances on deep metric learning with traditional approximations of the graph edit distance. Hence, we propose an efficient graph distance based on the novel field of geometric deep learning. Our method employs a message passing neural network to capture the graph structure, and thus, leveraging this information for its use on a distance computation. The performance of the proposed graph distance is validated on two different scenarios. On the one hand, in a graph retrieval of handwritten wordsÂ i.e.Â keyword spotting, showing its superior performance when compared with (approximate) graph edit distance benchmarks. On the other hand, demonstrating competitive results for graph similarity learning when compared with the current state-of-the-art on a recent benchmark dataset.","Graph neural networks, Graph edit distance, Geometric deep learning, Keyword spotting, Document image analysis",Pau Riba and Andreas Fischer and Josep LladÃ³s and Alicia FornÃ©s,https://www.sciencedirect.com/science/article/pii/S0031320321003198,https://doi.org/10.1016/j.patcog.2021.108132,0031-3203,2021,108132,120,Pattern Recognition,Learning graph edit distance by graph neural networks,article,RIBA2021108132,
"Graph learning (GL) can dynamically capture the distribution structure (graph structure) of data based on graph convolutional networks (GCN), and the learning quality of the graph structure directly influences GCN for semi-supervised classification. Most existing methods combine the computational layer and the related losses into GCN for exploring the global graph (measuring graph structure from all data samples) or local graph (measuring graph structure from local data samples). The global graph emphasizes the whole structure description of the inter-class data, while the local graph tends to the neighborhood structure representation of the intra-class data. However, it is difficult to simultaneously balance these learning process graphs for semi-supervised classification because of the interdependence of these graphs. To simulate the interdependence, deep graph learning (DGL) is proposed to find a better graph representation for semi-supervised classification. DGL can not only learn the global structure by the previous layer metric computation updating, but also mine the local structure by next layer local weight reassignment. Furthermore, DGL can fuse the different structures by dynamically encoding the interdependence of these structures, and deeply mine the relationship of the different structures by hierarchical progressive learning to improve the performance of semi-supervised classification. Experiments demonstrate that the DGL outperforms state-of-the-art methods on three benchmark datasets (Citeseer, Cora, and Pubmed) for citation networks and two benchmark datasets (MNIST and Cifar10) for images.","Graph learning, Graph convolutional networks, Semi-supervised classification",Guangfeng Lin and Xiaobing Kang and Kaiyang Liao and Fan Zhao and Yajun Chen,https://www.sciencedirect.com/science/article/pii/S0031320321002260,https://doi.org/10.1016/j.patcog.2021.108039,0031-3203,2021,108039,118,Pattern Recognition,Deep graph learning for semi-supervised classification,article,LIN2021108039,
"Visual Question Answering (VQA) has emerged as a Visual Turing Test to validate the reasoning ability of AI agents. The pivot to existing VQA models is the joint embedding that is learned by combining the visual features from an image and the semantic features from a given question. Consequently, a large body of literature has focused on developing complex joint embedding strategies coupled with visual attention mechanisms to effectively capture the interplay between these two modalities. However, modelling the visual and semantic features in a high dimensional (joint embedding) space is computationally expensive, and more complex models often result in trivial improvements in the VQA accuracy. In this work, we systematically study the trade-off between the model complexity and the performance on the VQA task. VQA models have a diverse architecture comprising of pre-processing, feature extraction, multimodal fusion, attention and final classification stages. We specifically focus on the effect of âmulti-modal fusionâ in VQA models that is typically the most expensive step in a VQA pipeline. Our thorough experimental evaluation leads us to three proposals, one optimized for minimal complexity, one for balanced complexity-accuracy and the last one for state-of-the-art VQA performance.","Visual question answering, Visual feature extraction, Language features, Multi-modal fusion, Speed-accuracy trade-off",Moshiur Farazi and Salman Khan and Nick Barnes,https://www.sciencedirect.com/science/article/pii/S0031320321002934,https://doi.org/10.1016/j.patcog.2021.108106,0031-3203,2021,108106,120,Pattern Recognition,Accuracy vs. complexity: A trade-off in visual question answering models,article,FARAZI2021108106,
"Handwritten signature verification is a widely used biometric for person identity authentication in document forensics. Despite the tremendous efforts in past research, offline signature verification still remains a challenge, particularly in discriminating between genuine signatures and skilled forgeries, because the difference of appearance between genuine and skilled forgery may be smaller than that between genuine ones. This challenge is even more critical in writer-independent scenario, where each writer has very few samples for training. This paper proposes a region based Deep Convolutional Siamese Network using metric learning method, which is applicable to both writer-dependent (WD) and writer-independent (WI) scenario. For representing minute but discriminative details, a Mutual Signature DenseNet (MSDN) is designed to extract features and learn the similarity measure from local regions instead of whole signature images. Based on local regions comparison, the similarity scores of multiple regions are fused for final decision of verification. In experiments on public datasets CEDAR and GPDS, the proposed method achieved state-of-the-art performance of 6.74% EER and 8.24% EER in WI scenario, respectively, and 1.67% EER and 1.65% EER in WD scenario, respectively.","Signature verification, Convolutional siamese network, Deep metric learning, Region fusion",Li Liu and Linlin Huang and Fei Yin and Youbin Chen,https://www.sciencedirect.com/science/article/pii/S0031320321001965,https://doi.org/10.1016/j.patcog.2021.108009,0031-3203,2021,108009,118,Pattern Recognition,Offline signature verification using a region based deep metric learning network,article,LIU2021108009,
"Negative stress, or distress, represents a serious problem in advanced societies given its adverse consequences for health. Many studies have focused on the detection of distress from physiological signals such as the electroencephalogram (EEG). To this respect, the combination of regularity-based quadratic sample entropy (QSampEn) and symbolic amplitude-aware permutation entropy (AAPE) has reported valuable outcomes in distress recognition. In the present work, the recently introduced symbolic metric called dispersion entropy (DispEn) is applied for the first time to the same problem. Statistically significant results reported by the single metric have demonstrated its capability for calm and distress detection. Furthermore, relevant differences have been found between the combination of QSampEn with either AAPE or DispEn, finding that the assessment of ordinal and dispersion patterns leads to distinct and complementary outcomes. Finally, the combination of the three entropy metrics has considerably overcome the results ever reported by other indices in similar studies.","Electroencephalography, Distress, Dispersion patterns, Nonlinear analysis",Beatriz GarcÃ­a-MartÃ­nez and Antonio FernÃ¡ndez-Caballero and RaÃºl Alcaraz and Arturo MartÃ­nez-Rodrigo,https://www.sciencedirect.com/science/article/pii/S0031320321002818,https://doi.org/10.1016/j.patcog.2021.108094,0031-3203,2021,108094,119,Pattern Recognition,Assessment of dispersion patterns for negative stress detection from electroencephalographic signals,article,GARCIAMARTINEZ2021108094,
"Financial markets are time-evolving complex systems containing different financial entities, such as banks, corporations and institutions that interact through transactions and respond to external economic and political events. They can be conveniently represented as a network structure. In this paper, we analyse the unweighted and weighted market networks from a statistical mechanical perspective. In particular, we propose a novel thermodynamic analogy to characterise the dynamic structural properties of time-evolving networks. The intricate pattern of edge connections in the network is modelled by using a heat bath analogy in which particles occupy the energy states according to the Boltzmann distribution. According to this analogy the occupation of the energy states is determined by the temperature of the heat bath, and the spectrum of energy states of the network is determined by the number of nodes and edges. For unweighted networks, the binary representation of the elements in the adjacency matrix can be modelled as a statistical ensemble, using the corresponding partition function to compute thermodynamic network characterisations. For weighted networks, on the other hand, the derived thermodynamic quantities together with their distribution of fluctuations identify the salient structure in the network evolution. We conduct experiments on time-evolving stock exchanges using data for the S&P500 Index Stock Exchanges over the past decade. The thermodynamic characterisations provide an excellent framework to identify epochs in which there is significant variance in network structure during financial crises induced by economic and political events.","Stock market networks, Thermodynamic characterisations, Statistical mechanics",Jianjia Wang and Xingchen Guo and Weimin Li and Xing Wu and Zhihong Zhang and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320321003101,https://doi.org/10.1016/j.patcog.2021.108123,0031-3203,2021,108123,120,Pattern Recognition,Statistical mechanical analysis for unweighted and weighted stock market networks,article,WANG2021108123,
"Graph-based Semi-Supervised Learning (GSSL) methods aim to classify unlabeled data by learning the graph structure and labeled data jointly. In this work, we propose a simple GSSL approach, which can deal with various degrees of class imbalance in given datasets. The key idea is to estimate the class proportion of input data in order to enhance the discriminative power of learned smooth classification function on the graph. Moreover, it has interesting connections to the regularization framework, the Markov stability for graph partition and the group inverse of normalized Laplacain matrix. For classification problems, experimental results demonstrate our approach can achieve promising performance on several datasets with varying class imbalance.","Graph-based semi-supervised learning, Class imbalance, Markov stability, Group inverse",Jianjin Deng and Jin-Gang Yu,https://www.sciencedirect.com/science/article/pii/S0031320321002132,https://doi.org/10.1016/j.patcog.2021.108026,0031-3203,2021,108026,118,Pattern Recognition,A simple graph-based semi-supervised learning approach for imbalanced classification,article,DENG2021108026,
"Objects from one category may be drawn from different distributions due to diverse illuminations, backgrounds, and camera viewpoints. Traditional object detection methods generally perform poorly due to the domain shift. To address this problem, we propose to train a domain-adaptive scene-specific pedestrian detector in an unsupervised manner. A generic detector is transferred to different target domains from one labeled source domain dataset without human-annotated target samples. Specifically, we first extend the generic detector to a dual-boundary classifier and collect hard samples as unlabeled target samples according to the detection confidence. Then, we propose a cycle semantic transfer network to align the instance-level and class-level distributions between the source domain and target domain and automatically label the hard samples. The initial generic detector is then re-trained by these labeled hard samples and specialized to a target scene. This process can be conveniently extended to different surveillance scenarios and generate specific detectors under various static camera viewpoints. Moreover, to reduce the impact of mislabeled hard samples on the generic detector, an online gradual optimization algorithm is proposed to iteratively update the generic model, thereby obtaining an optimized process that is insensitive to individual mislabeled target samples. Extensive experiments show that even if the target domain is not manually annotated, the proposed self-learning method demonstrates the effectiveness of pedestrian detection in various domain shift scenarios, and it outperforms existing scene-specific pedestrian detection methods and some classic supervised methods.","Scene-specific pedestrian detection, Domain adaptation, Unsupervised learning",Quanzheng Mou and Longsheng Wei and Conghao Wang and Dapeng Luo and Songze He and Jing Zhang and Huimin Xu and Chen Luo and Changxin Gao,https://www.sciencedirect.com/science/article/pii/S0031320321002259,https://doi.org/10.1016/j.patcog.2021.108038,0031-3203,2021,108038,118,Pattern Recognition,Unsupervised domain-adaptive scene-specific pedestrian detection for static video surveillance,article,MOU2021108038,
"Generally, ensemble selection techniques are split into two categories: dynamic and static. Static ensemble selection selects a fixed subset of the original ensemble which improves the space complexity but is not flexible to each test instance. Dynamic ensemble selection selects base learners on-the-fly according to each test instance but it does not significantly improve the complexity. Currently, there is no ensemble selection technique that is robust to the test instances as well as improves space complexity. To narrow this gap, we propose a novel static ensemble selection method, called Ensemble Selection with Joint Spectral Clustering and Structural Sparsity. This method integrates spectral clustering and structural sparsity into a joint framework whose ensemble selection result is robust to test instances and consumes less space. Using 25 datasets from KEEL and UCI, we demonstrate the effectiveness of our proposed algorithm and its promising performance compared to that of other state-of-the-art algorithms.","Ensemble selection, Structural sparsity, Unsupervised selection, Spectral clustering, Dynamic and static, Robustness",Zhenlei Wang and Suyun Zhao and Zheng Li and Hong Chen and Cuiping Li and Yufeng Shen,https://www.sciencedirect.com/science/article/pii/S003132032100248X,https://doi.org/10.1016/j.patcog.2021.108061,0031-3203,2021,108061,119,Pattern Recognition,Ensemble selection with joint spectral clustering and structural sparsity,article,WANG2021108061,
"Traditional pattern recognition problems are usually accomplished through two successive stages of representation and classification, the generalization ability and stability are difficult to guarantee for small samples and category imbalance. For tackling these problems, an unlabeled data-driven representation learning classification (RLC) fused model is constructed by integrating representation learning and classification into one model, rather than simple putting the two stages together. The RLC fused model mainly focuses on interactive iteratively optimizing representation learning and classification in a model, guiding and reinforcing each other. Under the framework of RLC, a deep nonnegative matrix factorization (NMF) is adopted for representation learning by complementing the advantages of NMF and deep learning, and avoiding complex network structure and parameter modulation. The framework is called deep NMF-RLC fusion model, which can achieve good performance for binary classification even the simplest linear regression classifier is used. The model explores useful information embedded in unlabeled data, and is suitable for small training samples and unbalanced classification. The performance of the proposed framework is verified on genetic-based tumor recognition, which contains all three stages of early diagnosis, tumor type recognition and postoperative metastasis. Experiments show that, compared with the published state-of-the-art methods and results, there are significant improvements in classification accuracy, specificity and sensitivity.","Unlabeled data, Deep representation learning, Non-negative matrix factorization, Tumor recognition",Xiaohui Yang and Wenming Wu and Licheng Jiao and Changzhe Jiao and Zhicheng Jiao,https://www.sciencedirect.com/science/article/pii/S0031320321002533,https://doi.org/10.1016/j.patcog.2021.108066,0031-3203,2021,108066,119,Pattern Recognition,A deep fusion framework for unlabeled data-driven tumor recognition,article,YANG2021108066,
"In this paper, a critical points based descriptor for 3D objects recognition is presented. It is based on the topological invariant provided by the critical points of the 3D object. The critical points and the links between them are represented by a size function resulting from a measure function that captures the surface displacement along the 3D object, and that encompasses invariance to affine transformations, articulations and torsions. In order to tackle the problems of partial matching of the 3D objects, a well-suited metric learning method is used to weight the matchings according to their relevance. The proposed methodâs performance was validated by different collections of 3D objects. The obtained scores are favorably comparable to the related work.","Object description, Object recognition, Object categorization, Shape classification",Mohammed Ayoub {Alaoui Mhamdi} and Djemel Ziou,https://www.sciencedirect.com/science/article/pii/S0031320321003186,https://doi.org/10.1016/j.patcog.2021.108131,0031-3203,2021,108131,120,Pattern Recognition,3D object recognition through a size function resulting from an invariant topological feature,article,ALAOUIMHAMDI2021108131,
"In this paper, we propose a mismatch removal method, which mines consistent image feature correspondences using co-occurrence statistics. The proposed method relies on a co-occurrence matrix that counts the number of pixel value pairs co-occurring within the images. Specifically, we propose to integrate the co-occurrence statistics with local spatial information, to preserve the consensus of neighborhood elements. Then, a new measure based on co-occurrence statistics is defined for correspondence similarity, to preserve the consensus of neighborhood topology. After that, with the consensus of neighborhood elements and neighborhood topology, the mismatch removal problem is formulated into a mathematical model, which has a closed-form solution. Extensive experiments show that the proposed method is able to achieve superior or competitive performance on matching accuracy over several state-of-the-art competing methods. In addition, we further exploit the consensus of neighborhood elements and neighborhood topology to propose a novel guided sampling method, which can significantly improve the quality of sampling minimal subsets over state-of-the-arts for two-view geometric model fitting.","Feature matching, Geometric model fitting, Co-occurrence statistics, Guided sampling",Guobao Xiao and Shiping Wang and Han Wang and Jiayi Ma,https://www.sciencedirect.com/science/article/pii/S0031320321002491,https://doi.org/10.1016/j.patcog.2021.108062,0031-3203,2021,108062,119,Pattern Recognition,Mining consistent correspondences using co-occurrence statistics,article,XIAO2021108062,
"Pan-sharpening applies details injection to fuse a multispectral (MS) image with its corresponding panchromatic (PAN) image to produce a synthetic image. Theoretically, the synthetic image's spectral resolution should equal that of the MS image and its spatial resolution is the same as that of the PAN image. However, for existing pan-sharpening methods, the trade-off between the spectral and intensity information in the process of details injection is insufficient, resulting in spatial or spectral distortion of the fused image. In this paper we propose a novel pan-sharpening algorithm based on multi-objective decision for multi-band remote sensing images to improve the quality of the fused image. The proposed method focuses on developing a parametric model from a multi-objective perspective to simultaneously maximize the quality of all the pixels in the fused image. We introduce a details injection approach to enhance the edge and texture of the MS image. We design an efficient spectral fidelity fusion model based on the injected details using spectral modulation to pan-sharpen the MS image. We provide an algorithm based on multi-objective decision to solve this model. The main advantage of the proposed method is that it can provide effective spectral modulation to eliminate the adverse effects of details injection. We conduct experiments on simulated and real satellite image datasets to evaluate the proposed method. The results show that our method achieves superior performance to other state-of-the-art methods.","Pan-sharpening, Details injection, Multi-objective decision, Spectral and intensity modulation",Lei Wu and Yunqiang Yin and Xunyan Jiang and T.C.E. Cheng,https://www.sciencedirect.com/science/article/pii/S0031320321002090,https://doi.org/10.1016/j.patcog.2021.108022,0031-3203,2021,108022,118,Pattern Recognition,Pan-sharpening based on multi-objective decision for multi-band remote sensing images,article,WU2021108022,
"Face photo-sketch synthesis aims to generate face sketches from real photos and vice versa. It can be abstracted as a constrained quantization problem. Although many efforts have been dedicated to this problem, it is still a challenging task to synthesize detail-preserving photos or sketches due to the significant differences between face sketch (drawn by people) and photo (taken by cameras) domains. In this paper, we propose a novel Identity-sensitive Generative Adversarial Network (IsGAN) to address it. Our key insight is to formalize face photo-sketch synthesis as a special case of image-to-image translation and propose to embed identity information through adversarial learning. In particular, an adversarial architecture is used to capture the differences between the two domains, and a new network loss, namely, identity recognition loss is introduced to preserve the detailed identifiable information, which is crucial for photo-sketch synthesis. In addition, to enforce structural consistency during generation, a cyclic-synthesized loss is applied between the generated image of one domain and cycled image of another. The experiments on the CUFS and CUFSF datasets suggest that our model achieves state-of-the-art performance in both qualitative and quantitative measures.","Face photo-sketch synthesis, Image-to-image translation, Generative adversarial networks, Convolutional neural network, Face recognition",Lan Yan and Wenbo Zheng and Chao Gou and Fei-Yue Wang,https://www.sciencedirect.com/science/article/pii/S0031320321002648,https://doi.org/10.1016/j.patcog.2021.108077,0031-3203,2021,108077,119,Pattern Recognition,IsGAN: Identity-sensitive generative adversarial network for face photo-sketch synthesis,article,YAN2021108077,
"How to extract effective expression representations that invariant to the identity-specific attributes is a long-lasting problem for facial expression recognition (FER). Most of the previous methods process the RGB images of a sequence, while we argue that the off-the-shelf and valuable expression-related muscle movement is already embedded in the compression format. In this paper, we target to explore the inter-subject variations eliminated facial expression representation in the compressed video domain. In the up to two orders of magnitude compressed domain, we can explicitly infer the expression from the residual frames and possibly extract identity factors from the I frame with a pre-trained face recognition network. By enforcing the marginal independence of them, the expression feature is expected to be purer for the expression and be robust to identity shifts. Specifically, we propose a novel collaborative min-min game for mutual information (MI) minimization in latent space. We do not need the identity label or multiple expression samples from the same person for identity elimination. Moreover, when the apex frame is annotated in the dataset, the complementary constraint can be further added to regularize the feature-level game. In testing, only the compressed residual frames are required to achieve expression prediction. Our solution can achieve comparable or better performance than the recent decoded image-based methods on the typical FER benchmarks with about 3 times faster inference.","Facial expression recognition, Mutual information, Disentangled representation, Compressed video",Xiaofeng Liu and Linghao Jin and Xu Han and Jane You,https://www.sciencedirect.com/science/article/pii/S0031320321002922,https://doi.org/10.1016/j.patcog.2021.108105,0031-3203,2021,108105,119,Pattern Recognition,Mutual information regularized identity-aware facial expression recognition in compressed video,article,LIU2021108105,
"Fuzzy c-means (FCM) clustering had been extended for handling multi-view data with collaborative idea. However, these collaborative multi-view FCM treats multi-view data under equal importance of feature components. In general, different features should take different weights for clustering real multi-view data. In this paper, we propose a novel multi-view FCM (MVFCM) clustering algorithm with view and feature weights based on collaborative learning, called collaborative feature-weighted MVFCM (Co-FW-MVFCM). The Co-FW-MVFCM contains a two-step schema that includes a local step and a collaborative step. The local step is a single-view partition process to produce local partition clustering in each view, and the collaborative step is sharing information of their memberships between different views. These two steps are then continuing by an aggregation way to get a global result after collaboration. Furthermore, the embedded feature-weighted procedure in Co-FW-MVFCM can give feature reduction to exclude redundant/irrelevant feature components during clustering processes. Experiments with several data sets demonstrate that the proposed Co-FW-MVFCM algorithm can completely identify irrelevant feature components in each view and that, additionally, it can improve the performance of the algorithm. Comparisons of Co-FW-MVFCM with some existing MVFCM algorithms are made and also demonstrated the effectiveness and usefulness of the proposed Co-FW-MVFCM clustering algorithm.","Clustering, Fuzzy c-means (FCM), Multi-view FCM (MVFCM), Collaborative learning, Feature weights, Collaborative feature-weighted MVFCM (Co-FW-MVFCM), Feature reduction",Miin-Shen Yang and Kristina P. Sinaga,https://www.sciencedirect.com/science/article/pii/S003132032100251X,https://doi.org/10.1016/j.patcog.2021.108064,0031-3203,2021,108064,119,Pattern Recognition,Collaborative feature-weighted multi-view fuzzy c-means clustering,article,YANG2021108064,
"Although modern object detectors rely heavily on a significant amount of training data, humans can easily detect novel objects using a few training examples. The mechanism of the human visual system is to interpret spatial relationships among various objects and this process enables us to exploit contextual information by considering the co-occurrence of objects. Thus, we propose a spatial reasoning framework that detects novel objects with only a few training examples in a context. We infer geometric relatedness between novel and base RoIs (Region-of-Interests) to enhance the feature representation of novel categories using an object detector well trained on base categories. We employ a graph convolutional network as the RoIs and their relatedness are defined as nodes and edges, respectively. Furthermore, we present spatial data augmentation to overcome the few-shot environment where all objects and bounding boxes in an image are resized randomly. Using the PASCAL VOC and MS COCO datasets, we demonstrate that the proposed method significantly outperforms the state-of-the-art methods and verify its efficacy through extensive ablation studies.","Few-shot learning, Object detection, Transfer learning, Visual reasoning, Data augmentation",Geonuk Kim and Hong-Gyu Jung and Seong-Whan Lee,https://www.sciencedirect.com/science/article/pii/S0031320321003058,https://doi.org/10.1016/j.patcog.2021.108118,0031-3203,2021,108118,120,Pattern Recognition,Spatial reasoning for few-shot object detection,article,KIM2021108118,
"There are relatively few works dealing with conformal prediction for multi-task learning issues, and this is particularly true for multi-target regression. This paper focuses on the problem of providing valid (i.e., frequency calibrated) multi-variate predictions. To do so, we propose to use copula functions for inductive conformal prediction, and illustrate our proposal by applying it to deep neural networks and random forests. We show that the proposed method ensures efficiency and validity for multi-target regression problems on various data sets.","Inductive conformal prediction, Copula functions, Multi-target regression, Deep neural networks, Random forests",Soundouss Messoudi and SÃ©bastien Destercke and Sylvain Rousseau,https://www.sciencedirect.com/science/article/pii/S0031320321002880,https://doi.org/10.1016/j.patcog.2021.108101,0031-3203,2021,108101,120,Pattern Recognition,Copula-based conformal prediction for multi-target regression,article,MESSOUDI2021108101,
"Image dehazing is very important for many computer vision tasks. However, typical CNN-based methods learn a direct mapping from a hazy image to a clear image, ignoring relevant haze priors and multi-level features. In this paper, a new Visual Attention Dehazing Network (VADN) with multi-level refinement and fusion is proposed, which leverages a haze attention map as a haze relevant prior and learns complementary haze information among multi-level features. The VADN contains a feature extraction network, a recurrent refinement network and an encoder-decoder network. The feature extraction network captures the multi-level features. The recurrent refinement network generates and refines the haze attention map by taking low-level features and high-level features as inputs alternatively. Then, the haze attention map is injected into the encoder-decoder network to obtain the clear image with the help of complementary information learned from informative multi-level features. The experimental results demonstrate that the average PSNR of VADN is 32.50 dB which outperforms most state-of-the-art methods by up to 5.14 dB. Besides, the run time of VADN is 0.067 s, only 55% of the run time spent by the recent enhanced pix2pix dehazing network.","image dehazing, attention mechanism, multi-level features, recurrent network",Shibai Yin and Xiaolong Yang and Yibin Wang and Yee-Hong Yang,https://www.sciencedirect.com/science/article/pii/S0031320321002089,https://doi.org/10.1016/j.patcog.2021.108021,0031-3203,2021,108021,118,Pattern Recognition,Visual Attention Dehazing Network with Multi-level Features Refinement and Fusion,article,YIN2021108021,
"This paper focuses on the re-assembly of an archaeological artifact, given images of its fragments. This problem can be considered as a special challenging case of puzzle solving. The restricted case of re-assembly of a natural image from square pieces has been investigated extensively and was shown to be a difficult problem in its own right. Likewise, the case of matching âcleanâ 2D polygons/splines based solely on their geometric properties has been studied. But what if these ideal conditions do not hold? This is the problem addressed in the paper. Three unique characteristics of archaeological fragments make puzzle solving extremely difficult: (1) The fragments are of general shape; (2) They are abraded, especially at the boundaries (where the strongest cues for matching should exist); and (3) The domain of valid transformations between the pieces is continuous. The key contribution of this paper is a fully-automatic and general algorithm that addresses puzzle solving in this intriguing domain. We show that our approach manages to correctly reassemble dozens of broken artifacts and frescoes.","Re-assembly, Computer vision, Computer graphics",Niv Derech and Ayellet Tal and Ilan Shimshoni,https://www.sciencedirect.com/science/article/pii/S0031320321002521,https://doi.org/10.1016/j.patcog.2021.108065,0031-3203,2021,108065,119,Pattern Recognition,Solving archaeological puzzles,article,DERECH2021108065,
"Crowd flow describes the elementary group behavior. Dynamics behind group behavior can help to identify abnormalities in flows. Quantifying flow dynamics can be challenging. In this paper, an algorithm has been proposed to describe groupsâ movements in crowded scenarios by analyzing videos. A force model has been proposed based on the active Langevin equation, where the motion points are assumed to behave similarly to active colloidal particles in fluids. The force model is further augmented with computer-vision techniques to segment linear and non-linear flows. The evaluation of the proposed spatio-temporal flow segmentation scheme has been carried out with public datasets. Experiments reveal that the proposed system can segment the flows with lesser errors than existing methods. The segmentation accuracy and Normalized Mutual Information (NMI) have improved by 10% as compared to existing flow segmentation algorithms.","Visual surveillance, Active Langevin equation, Crowd analysis, Human flow segmentation, Dense crowd",Shreetam Behera and Debi Prosad Dogra and Malay Kumar Bandyopadhyay and Partha Pratim Roy,https://www.sciencedirect.com/science/article/pii/S0031320321002247,https://doi.org/10.1016/j.patcog.2021.108037,0031-3203,2021,108037,119,Pattern Recognition,Understanding crowd flow patterns using active-Langevin model,article,BEHERA2021108037,
"Recently, Self-Expressive-based Subspace Clustering (SESC) has been widely applied in pattern clustering and machine learning as it aims to learn a representation that can faithfully reflect the correlation between data points. However, most existing SESC methods directly use the original data as the dictionary, which miss the intrinsic structure (e.g., low-rank and nonlinear) of the real-word data. To address this problem, we propose a novel Projection Low-Rank Subspace Clustering (PLRSC) method by integrating feature extraction and subspace clustering into a unified framework. In particular, PLRSC learns a projection transformation to extract the low-dimensional features and utilizes a low-rank regularizer to ensure the informative and important structures of the extracted features. The extracted low-rank features effectively enhance the self-expressive property of the dictionary. Furthermore, we extend PLRSC to a nonlinear version (i.e., NPLRSC) by integrating a nonlinear activator into the projection transformation. NPLRSC cannot only effectively extract features but also guarantee the data structure of the extracted features. The corresponding optimization problem is solved by the Alternating Direction Method (ADM), and we also prove that the algorithm converges to a stationary point. Experimental results on the real-world datasets validate the superior of our model over the existing subspace clustering methods.","Subspace clustering, Low-rank, Feature extraction, Block diagonal representation",Yesong Xu and Shuo Chen and Jun Li and Lei Luo and Jian Yang,https://www.sciencedirect.com/science/article/pii/S0031320321003290,https://doi.org/10.1016/j.patcog.2021.108142,0031-3203,2021,108142,120,Pattern Recognition,Learnable low-rank latent dictionary for subspace clustering,article,XU2021108142,
"Training a supernet using a copy of shared weights has become a popular approach to speed up neural architecture search (NAS). However, it is difficult for supernet to accurately evaluate on a large-scale search space due to high weight coupling in weight-sharing setting. To address this, we present a shrinking-and-expanding supernet that decouples the shared parameters by reducing the degree of weight sharing, avoiding unstable and inaccurate performance estimation as in previous methods. Specifically, we propose a new shrinking strategy that progressively simplifies the original search space by discarding unpromising operators in a smart way. Based on this, we further present an expanding strategy by appropriately increasing parameters of the shrunk supernet. We provide comprehensive evidences showing that, in weight-sharing supernet, the proposed method SE-NAS brings more accurate and more stable performance estimation. Experimental results on ImageNet dataset indicate that SE-NAS achieves higher Top-1 accuracy than its counterparts under the same complexity constraint and search space. The ablation study is presented to further understand SE-NAS.","Neural architecture search, Supernet, Search space shrinking",Yiming Hu and Xingang Wang and Lujun Li and Qingyi Gu,https://www.sciencedirect.com/science/article/pii/S0031320321002120,https://doi.org/10.1016/j.patcog.2021.108025,0031-3203,2021,108025,118,Pattern Recognition,Improving One-Shot NAS with Shrinking-and-Expanding Supernet,article,HU2021108025,
"How to identify the most influential nodes in a network for the maximization of influence spread is a great challenge. Known methods like k-shell decomposition determine core nodes who individually might be the most influential spreaders for the spreading originating in a single origin. However, these techniques are not suitable for determining multiple origins that together lead to the most effective spreading. The reason is that core nodes are often found to be located closely to each other, which results in large overlapping regions rather than spreading far across the network. In this paper, we propose a new algorithm, called community-based k-shell decomposition, by which a network can be viewed as multiple hierarchically ordered structures each branching off from the innermost shell to the periphery shell. To alleviate the overlap problem, our algorithm pursues a greedy strategy that preferably selects core nodes from different communities in the network, thus maximizing the joint influence of multiple origins. We systematically evaluate our algorithm against competing algorithms on multiple networks with varying network characteristics, and find that our algorithm outperforms other algorithms on networks that exhibit community structures, and the stronger communities, the better performance.","Influential spreader, Community-based -shell decomposition, Linear threshold model",Peng Gang Sun and Qiguang Miao and Steffen Staab,https://www.sciencedirect.com/science/article/pii/S0031320321003174,https://doi.org/10.1016/j.patcog.2021.108130,0031-3203,2021,108130,120,Pattern Recognition,Community-based k-shell decomposition for identifying influential spreaders,article,SUN2021108130,
"Deep neural networks (DNNs) have become popular for medical image analysis tasks like cancer diagnosis and lesion detection. However, a recent study demonstrates that medical deep learning systems can be compromised by carefully-engineered adversarial examples/attacks with small imperceptible perturbations. This raises safety concerns about the deployment of these systems in clinical settings. In this paper, we provide a deeper understanding of adversarial examples in the context of medical images. We find that medical DNN models can be more vulnerable to adversarial attacks compared to models for natural images, according to two different viewpoints. Surprisingly, we also find that medical adversarial attacks can be easily detected, i.e., simple detectors can achieve over 98% detection AUC against state-of-the-art attacks, due to fundamental feature differences compared to normal examples. We believe these findings may be a useful basis to approach the design of more explainable and secure medical deep learning systems.","Adversarial attack, Adversarial example detection, Medical image analysis, Deep learning",Xingjun Ma and Yuhao Niu and Lin Gu and Yisen Wang and Yitian Zhao and James Bailey and Feng Lu,https://www.sciencedirect.com/science/article/pii/S0031320320301357,https://doi.org/10.1016/j.patcog.2020.107332,0031-3203,2021,107332,110,Pattern Recognition,Understanding adversarial attacks on deep learning based medical image analysis systems,article,MA2021107332,
"Unsupervised semantic hashing should in principle keep the semantics among samples consistent with the intrinsic geometric structures of the dataset. In this paper, we propose a novel multiple stage unsupervised hashing method, named âUnsupervised Hashing based on the Recovery of Subspace Structuresâ (RSSH) for image retrieval. Specifically, we firstly adapt the Low-rank Representation (LRR) model into a new variant which treats the real-world data as samples drawn from a union of several low-rank subspaces. Then, the pairwise similarities are represented in a space-and-time saving manner based on the learned low-rank correlation matrix of the modified LRR. Next, the challenging discrete graph hashing is employed for binary hashing codes. Notably, we convert the original graph hashing model into an optimization-friendly formalization, which is addressed with efficient closed-form solutions for its subproblems. Finally, the devised linear hash functions are fast achieved for out-of-samples. Retrieval experiments on four image datasets testify the superiority of RSSH to several state-of-the-art hashing models. Besides, itâs worth mentioning that RSSH, a shallow model, significantly outperforms two recently proposed unsupervised deep hashing methods, which further confirms its effectiveness.","Semantic hashing, Subspace learning, Low-rank representation, Discrete optimization",Zhibao Tian and Hui Zhang and Yong Chen and Dell Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320300662,https://doi.org/10.1016/j.patcog.2020.107261,0031-3203,2020,107261,103,Pattern Recognition,Unsupervised hashing based on the recovery of subspace structures,article,TIAN2020107261,
"With the development of social media and smartphones, people share their daily lives via a large number of images, but the convince also raises a problem of privacy leakage. Therefore, effective methods are needed to infer the privacy risk of images and identify images that may disclose privacy. Several works have tried to solve this problem with deep learning models. However, we know little about how the models infer the privacy label of an image, thus it is not easy to understand why the image may disclose privacy. Inspired by recent research on graph neural networks, we introduce prior knowledge to the deep models to make the inference more explainable. We propose the Graph-based neural networks for Image Privacy (GIP) to infer the privacy risk of images. The GIP mainly focuses on objects in an image, and the knowledge graph is extracted from the objects in the dataset without reliance on extra knowledge. Experimental results show that the GIP achieves higher performance compared with the object-based methods and comparable performance even compared with the multi-modal fusion method. The results show that the introduction of the knowledge graph not only makes the deep model more explainable but also makes better use of the information of objects provided by the images. Combing the knowledge graph with deep learning is a promising way to help protect image privacy that is worth exploring.","Image privacy protection, Graph neural networks, Image classification",Guang Yang and Juan Cao and Zhineng Chen and Junbo Guo and Jintao Li,https://www.sciencedirect.com/science/article/pii/S0031320320301631,https://doi.org/10.1016/j.patcog.2020.107360,0031-3203,2020,107360,105,Pattern Recognition,Graph-based neural networks for explainable image privacy inference,article,YANG2020107360,
"We consider the multi-view data completion problem, i.e., to complete a matrix U=[U1|U2] where the ranks of U, U1, and U2 are given. In particular, we investigate the fundamental conditions on the sampling pattern, i.e., locations of the sampled entries for finite completability of such a multi-view data given the corresponding rank constraints. We provide a geometric analysis on the manifold structure for multi-view data to incorporate more than one rank constraint. We derive a probabilistic condition in terms of the number of samples per column that guarantees finite completability with high probability. Finally, we derive the guarantees for unique completability. Numerical results demonstrate reduced sampling complexity when the multi-view structure is taken into account as compared to when only low-rank structure of individual views is taken into account. Then, we propose an apporach using Newtonâs method to almost achieve these information-theoretic bounds for mulit-view data retrieval by taking advantage of the rank decomposition and the analysis in this work.","Multi-view learning, Low-rank matrix completion, Sampling pattern, Sampling rate, Non-convex optimization, Rank decomposition",Morteza Ashraphijuo and Xiaodong Wang and Vaneet Aggarwal,https://www.sciencedirect.com/science/article/pii/S0031320320301114,https://doi.org/10.1016/j.patcog.2020.107307,0031-3203,2020,107307,103,Pattern Recognition,Fundamental sampling patterns for low-rank multi-view data completion,article,ASHRAPHIJUO2020107307,
"In this paper, we propose a fast and reliable neural network-based algorithm for fingerprint minutiae extraction. In particular, our algorithm involves a two-stage process: in the first stage, a network generates candidate patches in which minutiae may exist; in the second stage, another network extracts minutiae from every patch.These two networks share a common part to reduce the running time. Moreover, we analyze the properties of fingerprint images and propose a principle for designing efficient networks for minutiae extraction. For efficiency, our algorithm extracts minutiae directly from raw fingerprint images, without traditional pre-processes. Another benefit of this design is that the networks only require datasets with minutiae labels for training. On the public fingerprint datasets (FVC 2002 and 2004), our algorithm requires 26Â ms on average to extract minutiae from one fingerprint on a single GPU. Compared with other neural network-based algorithms, our algorithm runs approximately 10 times faster and does not lose substantial accuracy.","Minutiae extraction, Neural network, Fingerprint, Biometric technology",Baicun Zhou and Congying Han and Yonghong Liu and Tiande Guo and Jin Qin,https://www.sciencedirect.com/science/article/pii/S0031320320300789,https://doi.org/10.1016/j.patcog.2020.107273,0031-3203,2020,107273,103,Pattern Recognition,Fast minutiae extractor using neural network,article,ZHOU2020107273,
"Salient object detection (SOD) aims to precisely segment out the most attractive areas in a single image. With the rapid development of deep learning, much effort has been paid to learn an effective representation for SOD from bottom-up or top-down pathways. However, they fail to precisely separate out the whole salient object with fine boundaries due to the repeated subsampling operations such as pooling and striding leading to the loss of fine structures and spatial details. To address these issues, in this paper, we propose a residual refinement network with semantic context features for SOD. First, we design an encoder-decoder structure with side-connections to capture the sharper object boundaries, which can not only gradually recover the spatial details in each feature map from top to down, but also enhance the features at all scales with high-level semantic context information. The semantic context enhanced features are further strengthen by using a set of atrous convolutional filters with multiple atrous rates to encode multi-scale context information. Finally, using the side-output features as input, we develop a recurrent residual module to gradually learn to recover the missing boundary details in the previous coarsely predicted saliency map in a coarse-to-fine manner. Extensive evaluations on six popular SOD benchmark datasets demonstrate leading performance of the proposed approach compared with state-of-the-art methods. Especially, our approach runs in real-time at a speed of 29Â fps.","Salient object detection, Convolutional neural networks, Deep learning, Residual learning",Tengpeng Li and Huihui Song and Kaihua Zhang and Qingshan Liu,https://www.sciencedirect.com/science/article/pii/S0031320320301758,https://doi.org/10.1016/j.patcog.2020.107372,0031-3203,2020,107372,105,Pattern Recognition,Learning residual refinement network with semantic context representation for real-time saliency object detection,article,LI2020107372,
"Large-scale face identification or 1-to-N matching where N is huge, plays a vital role in biometrics and surveillance. The system demands accurate and speedy matching where compact facial feature representation and a simple matcher are favored. On the other hand, most research considers closed-set identification that assumes that all identities of probe samples are enclosed in the gallery. On the contrary, open-set identification expects that some probe identities are not known to the system. This setup poses an additional challenge, where the system should be able to reject those probes that correspond to unknown identities. In this paper, we address the large-scale open-set face identification problem with a compact facial representation that is based on the index-of-maximum (IoM) hashing, which was designed for biometric template protection. To be specific, the existing random IoM hashing is advanced to a data-driven based hashing technique, where the hashed face code can be made compact and matching can be easily performed by the Hamming distance, which can offer highly efficient matching. Furthermore, since IoM hashing transforms the original facial features non-invertibly, the privacy of users can also be preserved. Along with IoM hashed face code, we explore several fusion strategies to address the open-set face identification problem. The comprehensive evaluations are carried out with three large-scale unconstrained face datasets, namely LFW, VGG2 and IJB-C.","Secure open-set face identification, Index-of-max hashing, Fusion, Privacy",Xingbo Dong and Soohyung Kim and Zhe Jin and Jung Yeon Hwang and Sangrae Cho and Andrew Beng Jin Teoh,https://www.sciencedirect.com/science/article/pii/S0031320320300820,https://doi.org/10.1016/j.patcog.2020.107277,0031-3203,2020,107277,103,Pattern Recognition,Open-set face identification with index-of-max hashing by learning,article,DONG2020107277,
"Pedestrian detection has been one of the key technologies in computer vision for autonomous driving in underground mines. However, such pedestrian detection is easily affected by complex environmental factors, such as uneven light, dense dust and cable interference. Recently, the problem of pedestrian detection is solved as an object detection task, which has achieved significant advances with the framework of deep neural networks. In this paper, we propose a novel parallel feature transfer network based detector called PftNet that achieves better efficiency than one-stage methods and maintains comparable accuracy of two-stage methods. PftNet consists of two interconnected modules, i.e., the pedestrian identification module and the pedestrian location module. The former aims to roughly adjust the location and size of the anchor box, filter out the negative anchor box, and provide better initialization for the regression. The latter enables PftNet to adapt to different scales and aspect ratios of objects and further improves the regression accuracy. Meanwhile, a feature transfer block compromising gated units is well designed to transmit the pedestrian characteristics between two modules. Extensive experiments on self-annotated underground dataset as well as INRIA and ETH datasets show that PftNet achieves state-of-the-art detection efficiency with high accuracy, which is significant to realizing unmanned driving systems in mines.","Pedestrian detection, Underground mine, Deep learning network, Parallel feature transfer, Gated unit, Unmanned driving",Xing Wei and Haitao Zhang and Shaofan Liu and Yang Lu,https://www.sciencedirect.com/science/article/pii/S0031320320300029,https://doi.org/10.1016/j.patcog.2020.107195,0031-3203,2020,107195,103,Pattern Recognition,Pedestrian detection in underground mines via parallel feature transfer network,article,WEI2020107195,
"In recent years, visual-textual matching has been widely studied in the intersection of computer vision and natural language processing communities. A feasible scheme for learning discriminative representations is leveraging hierarchical features to align both modalities at multiple semantic levels. However, most existing approaches rely on pre-trained object detectors or semantic parsers to generate multi-level representations, whose performance is overly dependent on the extra supervision and thereby leads to its vulnerability. In this paper, we introduce a Stacked Squeeze-and-Excitation Recurrent Residual Network (SER2-Net) for visual-textual matching. Firstly, an efficient multi-level representation module is presented to produce a series of semantically discriminative features without the aid of extra supervision, which is built by stacking the squeeze-and-excitation recurrent residual (SER2) learning components. Specifically, SER2 incorporates the residual learning and inverse recurrent connection into the squeeze-and-excitation learning block, which allows for utilizing complementary current information and residual information to improve the modality-specific representation ability. Besides, to capture the implicit correlations contained among multi-level features, we propose a novel objective namely Cross-modal Semantic Discrepancy (CMSD) loss, which is characterized by exploiting the interdependency among different semantic levels to narrow the cross-modal distribution discrepancy. Extensive experiments on two benchmark datasets validate the superiority of our model, which compares favorably with the state-of-the-art approaches.","Vision and language, Cross-modal retrieval, Visual-Semantic embedding",Haoran Wang and Zhong Ji and Zhigang Lin and Yanwei Pang and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S003132032030162X,https://doi.org/10.1016/j.patcog.2020.107359,0031-3203,2020,107359,105,Pattern Recognition,Stacked squeeze-and-excitation recurrent residual network for visual-semantic matching,article,WANG2020107359,
"Clustering is a research problem based on the data's proximity relationship which is not made full use of by all the existing algorithms. In this paper, we present a novel two-stage LG framework consisting of the proposed Local Energy Gradient Oppression (LEGO) and the Guide Point Assignation (GPA) strategies which are closely related to the data pointsâ proximity relations. In the LG framework, it is crucial to locate the appropriate centers for the subsequent data label assignment, and therefore we introduce the nuclear model viewing the dataset as a collection of charged particles, which is the basis of LEGO, and the points with local maximum potential energy are ascertained as the cluster centers. Besides, the GPA strategy innovatively adopts the idea that the cluster center actively selects data points as the same cluster, enabling the LG framework still to be effective when dealing with datasets of arbitrary shape distribution. Superiorities of the proposed framework and the two strategies are demonstrated on four synthetic datasets and three real-world faces image datasets in terms of two clustering performance metrics.","Clustering, Proximity relation, Local energy, Guide point, Face clustering",Hui Qv and Jihao Yin and Xiaoyan Luo,https://www.sciencedirect.com/science/article/pii/S0031320320300704,https://doi.org/10.1016/j.patcog.2020.107265,0031-3203,2020,107265,103,Pattern Recognition,LG: A clustering framework supported by point proximity relations,article,QV2020107265,
"Beneficial from Fully Convolutional Neural Networks (FCNs), saliency detection methods have achieved promising results. However, it is still challenging to learn effective features for detecting salient objects in complicated scenarios, in which i) non-salient regions may have âsalient-likeâ appearance; ii) the salient objects may have different-looking regions. To handle these complex scenarios, we propose a Feature Guide Network which exploits the nature of low-level and high-level features to i) make foreground and background regions more distinct and suppress the non-salient regions which have âsalient-likeâ appearance; ii) assign foreground label to different-looking salient regions. Furthermore, we utilize a Multi-scale Feature Extraction Module (MFEM) for each level of abstraction to obtain multi-scale contextual information. Finally, we design a loss function which outperforms the widely used Cross-entropy loss. By adopting four different pre-trained models as the backbone, we prove that our method is very general with respect to the choice of the backbone model. Experiments on six challenging datasets demonstrate that our method achieves the state-of-the-art performance in terms of different evaluation metrics. Additionally, our approach contains fewer parameters than the existing ones, does not need any post-processing, and runs fast at a real-time speed of 28 FPS when processing a 480Â ÃÂ 480 image.","Saliency detection, Fully convolutional neural networks, Attention guidance",Sina Mohammadi and Mehrdad Noori and Ali Bahri and Sina {Ghofrani Majelan} and Mohammad Havaei,https://www.sciencedirect.com/science/article/pii/S0031320320301072,https://doi.org/10.1016/j.patcog.2020.107303,0031-3203,2020,107303,103,Pattern Recognition,CAGNet: Content-Aware Guidance for Salient Object Detection,article,MOHAMMADI2020107303,
"Recently, different studies have demonstrated the use of co-clustering, a data mining technique which simultaneously produces row-clusters of observations and column-clusters of features. The present work introduces a novel co-clustering model to easily summarize textual data in a document-term format. In addition to highlighting homogeneous co-clusters as other existing algorithms do we also distinguish noisy co-clusters from significant co-clusters, which is particularly useful for sparse document-term matrices. Furthermore, our model proposes a structure among the significant co-clusters, thus providing improved interpretability to users. The approach proposed contends with state-of-the-art methods for document and term clustering and offers user-friendly results. The model relies on the Poisson distribution and on a constrained version of the Latent Block Model, which is a probabilistic approach for co-clustering. A Stochastic Expectation-Maximization algorithm is proposed to run the modelâs inference as well as a model selection criterion to choose the number of co-clusters. Both simulated and real data sets illustrate the efficiency of this model by its ability to easily identify relevant co-clusters.","Co-Clustering, Document-term matrix, Latent block model",Margot Selosse and Julien Jacques and Christophe Biernacki,https://www.sciencedirect.com/science/article/pii/S0031320320301199,https://doi.org/10.1016/j.patcog.2020.107315,0031-3203,2020,107315,103,Pattern Recognition,Textual data summarization using the Self-Organized Co-Clustering model,article,SELOSSE2020107315,
"The generative adversarial network (GAN) is composed of a generator and a discriminator where the generator is trained to transform random latent vectors to valid samples from a distribution and the discriminator is trained to separate such âfakeâ examples from true examples of the distribution, which in turn forces the generator to generate better fakes. The bidirectional GAN (BiGAN) also has an encoder working in the inverse direction of the generator to produce the latent space vector for a given example. This added encoder allows defining auxiliary reconstruction losses as hints for a better generator. On five widely-used data sets, we showed that BiGANs trained with the Wasserstein loss and augmented with hints learn better generators in terms of image generation quality and diversity, as measured numerically by the 1-nearest neighbor test, FrÃ©chet inception distance, and reconstruction error, and qualitatively by visually analyzing the generated samples.","Generative Modeling, Generative Adversarial Networks, Unsupervised Learning, Autoencoders, Neural Networks, Deep Learning",Uras Mutlu and Ethem AlpaydÄ±n,https://www.sciencedirect.com/science/article/pii/S0031320320301230,https://doi.org/10.1016/j.patcog.2020.107320,0031-3203,2020,107320,103,Pattern Recognition,Training bidirectional generative adversarial networks with hints,article,MUTLU2020107320,
"Multi-label feature selection plays an indispensable role in multi-label learning, which eliminates irrelevant and redundant features while retaining relevant features. Most of existing multi-label feature selection methods employ two strategies to construct feature selection models: extracting label correlations to guide feature selection process and maintaining the consistency between the feature matrix and the reduced low-dimensional feature matrix. However, the data information is described by two data matrices: the feature matrix and the label matrix. Previous methods devote attention to either of the two data matrices. To address this issue, we propose a novel feature selection method named Feature Selection considering Shared Common Mode between features and labels (SCMFS). First, we utilize Coupled Matrix Factorization (CMF) to extract the shared common mode between the feature matrix and the label matrix, considering the comprehensive data information in the two matrices. Additionally, Non-negative Matrix Factorization (NMF) is adopted to enhance the interpretability for feature selection. Extensive experiments are implemented on fifteen real-world benchmark data sets for multiple evaluation metrics, the experimental results demonstrate the classification superiority of the proposed method.","Feature selection, Multi-label learning, Coupled matrix factorization, Non-negative matrix factorization, Classification",Liang Hu and Yonghao Li and Wanfu Gao and Ping Zhang and Juncheng Hu,https://www.sciencedirect.com/science/article/pii/S0031320320301473,https://doi.org/10.1016/j.patcog.2020.107344,0031-3203,2020,107344,104,Pattern Recognition,Multi-label feature selection with shared common mode,article,HU2020107344,
"A standard word embedding algorithm, such as âword2vecâ, embeds each word as a dense vector of a preset dimensionality, the components of which are learned by maximizing the likelihood of predicting the context around it. However, as an inherent linguistic phenomenon, it is evident that there is a varying degree of difficulty in identifying words from their contexts. This suggests that a variable granularity in word vector representation may be useful to obtain sparser and more compressed word representations, requiring less storage space. To that end, in this paper, we propose a word vector training algorithm that uses a variable number of components to represent words. Given a text collection of documents, our algorithm, similar to the skip-gram approach of word2vec, learns to predict the context of a word given the current instance of a word. However, in contrast to skip-gram, which uses a static number of dimensions for each word vector, we propose to dynamically increase the dimensionality as a stochastic function of the prediction error. Our experiments with standard test collections demonstrate that our word representation method is able to achieve comparable (and sometimes even better) effectiveness than skip-gram word2vec, using a significantly smaller number of parameters (achieving compression ratio of around 65%).","Word embedding, Compression and sparsity, Lexical semantics",Debasis Ganguly,https://www.sciencedirect.com/science/article/pii/S0031320320301102,https://doi.org/10.1016/j.patcog.2020.107306,0031-3203,2020,107306,103,Pattern Recognition,Learning variable-length representation of words,article,GANGULY2020107306,
"We present a novel 2D shape decomposition algorithm via a recursive partitioning process. Starting with the contour points of a shape, we repeatedly separate the points into two parts by spectral clustering, until the stopping condition is met. Motivated by the fact that the points in a convex part are mutually visible, we regard the visibility matrix of points as the affinity matrix of spectral clustering to obtain a near-convex decomposition. Additionally, we present an efficient stopping rule to avoid over-segmentation on the shape branches. The stopping criterion is based on a novel shape signature called visible protrusion strength which can be used to measure the segmentability of a sub-shape. Finally, we demonstrate the efficiency of our algorithm on a variety of publicly available shapes, and provide qualitative and quantitative comparisons with state-of-art approaches.","Convex decomposition, Visibility, Shape signature, Spectral graph cut",Zhiyang Li and Jia Hu and Milos Stojmenovic and Zhaobin Liu and Weijiang Liu,https://www.sciencedirect.com/science/article/pii/S0031320320301746,https://doi.org/10.1016/j.patcog.2020.107371,0031-3203,2020,107371,105,Pattern Recognition,Revisiting spectral clustering for near-convex decomposition of 2D shape,article,LI2020107371,
"We propose an end-to-end ensemble method for person re-identification (ReID) to address the problem of overfitting in discriminative models. These models are known to converge easily, but they are biased to the training data in general and may produce a high model variance, which is known as overfitting. The ReID task is more prone to this problem due to the large discrepancy between training and test distributions. To address this problem, our proposed ensemble learning framework produces several diverse and accurate base learners in a single DenseNet. Since most of the costly dense blocks are shared, our method is computationally efficient, which makes it favorable compared to the conventional ensemble models. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art results. Noticeable performance improvements, especially on relatively small datasets, indicate that the proposed method deals with the overfitting problem effectively.","Deep networks, Ensemble learning, Person re-identification",Ayse Serbetci and Yusuf Sinan Akgul,https://www.sciencedirect.com/science/article/pii/S0031320320301229,https://doi.org/10.1016/j.patcog.2020.107319,0031-3203,2020,107319,104,Pattern Recognition,End-to-end training of CNN ensembles for person re-identification,article,SERBETCI2020107319,
"In this work, with the help of the rescaled Hinge loss, we propose a twin support vector regression (TSVR) model that is robust to noise. The corresponding optimization problem turns out to be non-convex with smooth l2 regularizer. To solve the problem efficiently, we convert it to its dual form, thereby transforming it into a convex optimization problem. An algorithm, named Res-TSVR, is provided to solve the formulated dual problem. The proof of the convergence of the algorithm is given. It is shown that the maximum number of iterations to achieve an Îµ-precision solution to the dual problem is O(log(1Îµ)). We conduct a set of numerical experiments to compare the proposed method with the recently proposed robust approaches of TSVR and the standard SVR. Experimental results reveal that the proposed approach outperforms other robust methods of TSVR in terms of generalization performance and robustness to noise with comparable training time. This claim is based on the experiments performed using seven real-world data sets and three synthetic data sets.","Twin support vector regression, Correntropy, Gaussian noise, Outliers, Linear kernel, Non-linear kernels, Res-TSVR",Manisha Singla and Debdas Ghosh and K.K. Shukla and Witold Pedrycz,https://www.sciencedirect.com/science/article/pii/S0031320320301989,https://doi.org/10.1016/j.patcog.2020.107395,0031-3203,2020,107395,105,Pattern Recognition,Robust twin support vector regression based on rescaled Hinge loss,article,SINGLA2020107395,
"In this paper, we propose a novel Deep Kinship Matching and Recognition (DKMR) framework for multi-person kinship matching and recognition, which is a complicated and challenging task with little previous literature. Compared with most existing kinship understanding methods that mainly work on matching kinship in pairwise face images, we target at recognizing the exact kinship in nuclear family photos consisting of multiple persons. The proposed DKMR framework contains three modules. Firstly, we design a deep kinship matching model (termed DKM-TRL) to predict kin-or-not scores by integrating the triple ranking loss into a Siamese CNN model. Secondly, we develop a deep kinship recognition model (named DKR-GA) to predict the exact kinship categories, in which gender and relative age attributes are utilized to learn more discriminative representations. Thirdly, based on the outputs of DKM-TRL and DKR-GA, we propose a reasoning conditional random field (R-CRF) model to infer the corresponding optimal family tree by exploiting the common kinship knowledge of a nuclear family. To evaluate the effectiveness of our DKMR framework, we conduct extensive experiments and the results show that it can gain superior performance on Group-Face dataset, TSKinFace dataset and FIW dataset over state-of-the-arts.","Kinship matching and recognition, Deep learning, R-CRF Algorithm",Mengyin Wang and Xiangbo Shu and Jiashi Feng and Xun Wang and Jinhui Tang,https://www.sciencedirect.com/science/article/pii/S003132032030145X,https://doi.org/10.1016/j.patcog.2020.107342,0031-3203,2020,107342,105,Pattern Recognition,Deep multi-person kinship matching and recognition for family photos,article,WANG2020107342,
"Network embedding plays a pivotal role in network analysis, due to the capability of encoding each node to a low-dimensional dense feature vector. However, most existing network embedding approaches only focus on preserving structural information in the network. The text features and category attributes of nodes are ignored, which are important to network analysis. In this paper, we propose an innovative semi-supervised network embedding (SNE) model integrating structural information, text features and category attributes into embedding vectors simultaneously. Specifically, we design a structure preserving module and a text representation module to capture the global structural information and the text features separately. Meanwhile, a label indicator matrix and a supervised loss are proposed for preserving category information and mapping nodes in the same class closer. We utilize stacked auto-encoders to explore the highly nonlinear characteristics of the network. By optimizing the reconstruction loss and the designed supervised loss jointly in the proposed semi-supervised model, the embedding vectors are finally learned. Extensive experiments on real-world datasets demonstrate that our method is superior to the state-of-the-art baselines in a variety of tasks, including visualization, node classification and clustering.","Network embedding, Structure preserving, Text representation, Stacked auto-encoders",Maoguo Gong and Chuanyu Yao and Yu Xie and Mingliang Xu,https://www.sciencedirect.com/science/article/pii/S0031320320301503,https://doi.org/10.1016/j.patcog.2020.107347,0031-3203,2020,107347,104,Pattern Recognition,Semi-supervised network embedding with text information,article,GONG2020107347,
"Three-dimensional feature descriptors play an important role in 3D computer vision because they are widely employed in many 3D perception applications to extract point correspondences between two point clouds. However, most existing description methods suffer from either weak robustness, low descriptiveness, or costly computation. Thus, a 3D local feature descriptor named Histograms of Point Pair Features (HoPPF) is proposed in this paper, and it is aimed at robust representation, high descriptiveness, and efficient computation. First, we propose a novel method to redirect surface normals and use the Poisson-disk sampling strategy to solve the problem of data redundancy in data pre-processing. Second, a new technique is applied to divide the local point pair set of each keypoint into eight regions. Then, the distribution of local point pairs of each region is used to construct the corresponding sub-features. Finally, the proposed HoPPF is generated by concatenating all sub-features into a vector. The performance of the HoPPF method is rigorously evaluated on several standard datasets. The results of the experiments and comparisons with other state-of-the-art methods validate the superiority of the HoPPF descriptor in term of robustness, descriptiveness, and efficiency. Moreover, the proposed technique for division of point pair sets is used to modify the other typical point-pair-based descriptor (i.e., PFH) to show its generalization ability. The proposed HoPPF is also applied to object recognition on real datasets captured by different devices (e.g., Kinect and LiDAR) to verify the feasibility of this method for 3D vision applications.","Local feature descriptor, 3D representation, Feature matching, Shape retrieval, Object recognition",Huan Zhao and Minjie Tang and Han Ding,https://www.sciencedirect.com/science/article/pii/S0031320320300777,https://doi.org/10.1016/j.patcog.2020.107272,0031-3203,2020,107272,103,Pattern Recognition,HoPPF: A novel local surface descriptor for 3D object recognition,article,ZHAO2020107272,
"Driving Maneuver Early DetectionÂ (DMED) is particularly useful for many applications of intelligent vehicle systems, including driver warning and collision avoidance systems. In this paper, we introduce a robust DMED model, denoted as University of Michigan DearbornÂ (UMD)-DMED, developed using innovative features and deep learning techniques. The UMD-DMED model contains three major computational components, distance based representation of driving context, combined vehicle trajectory features and visual features, and a Long Short-Term MemoryÂ (LSTM)-based neural network that captures temporal dependencies of driving maneuvers. To properly evaluate the performances of UMD-DMED, we developed two DMED systems based on the UMD-DMED model, one system is based on partially observed evidence of maneuver events, and another on features observed ahead of the time that driving maneuvers take place. We conducted the extensive experiments using a data set containing 1078 maneuver events extracted from 37 hours of real world driving trips. The results demonstrate that the UMD-DMED model is capable of learning the latent features of five different classes of driving maneuvers, i.e. left turn, right turn, left lane change, right lane change, driving straight. Comparing to four different state-of-the-art DMED systems, the UMD-DMED achieved better detection performances in both, the detection based on partial observations of driver maneuvering, and based on driving context observed ahead-of-time.","Driving maneuver early detection, Deep neural networks, Sequence learning, Advanced driver assistance systems, Computer vision",Xishuai Peng and Yi Lu Murphey and Ruirui Liu and Yuanxiang Li,https://www.sciencedirect.com/science/article/pii/S0031320320300819,https://doi.org/10.1016/j.patcog.2020.107276,0031-3203,2020,107276,103,Pattern Recognition,Driving maneuver early detection via sequence learning from vehicle signals and video images,article,PENG2020107276,
"In this paper, we address the problem of depth recovery from a sequence of multi-focus images, known as shape-from-focus (SFF). The conventional SFF techniques typically exhibit poor performance over textureless regions, and it is difficult to preserve depth edges and fine details while maintaining spatial consistency. Therefore, we propose an SFF depth recovery framework composed of depth reconstruction and refinement processes. We first formulate the depth reconstruction as a maximum a posterior (MAP) estimation problem with the inclusion of matting Laplacian prior. The nonlocal principle is adopted in matting Laplacian matrix construction to preserve depth edges and fine details. As the nonlocal principle breaks the spatial consistency, the reconstructed depth image is spatially inconsistent and suffers from the texture-copy artifacts. To smooth the noise and suppress the texture-copy artifacts, a closed-form edge-preserving depth refinement is proposed, which is formulated as a MAP estimation problem using Markov random fields (MRFs). Experimental results over synthetic and real scene datasets demonstrate the superiority of our algorithm in terms of robustness, and the ability to preserve edges and fine details while maintaining spatial consistency compared to existing approaches.","Shape from focus, Depth reconstruction, Matting Laplacian, Image denoising, Markov random field, Edge-preserving",Zhiqiang Ma and Dongjoon Kim and Yeong-Gil Shin,https://www.sciencedirect.com/science/article/pii/S0031320320301060,https://doi.org/10.1016/j.patcog.2020.107302,0031-3203,2020,107302,103,Pattern Recognition,Shape-from-focus reconstruction using nonlocal matting Laplacian prior followed by MRF-based refinement,article,MA2020107302,
"Image to image translation achieves superior performance with the advent of generative adversarial networks. In this paper, we propose a Simplified Unsupervised Image Translation (SUIT) model for domain adaptation on semantic segmentation. We adopt adversarial training for superior image generation, and design a novel semantic-content loss to enhance visual appearance preservation. Thus, the high-fidelity generated images with target-style can help the model generalize to the target domain. Besides, the semantic-content loss contains two components, which focus on label- and content-consistency, respectively. Both of them can be derived from existing modules of SUIT, which makes it simple yet suitable for domain adaptation on semantic segmentation tasks. Meanwhile, since the transformation network (generator) is decoupled from the segmentation network, the former can be easily transplanted to other semantic segmentation models. Extensive experimental results demonstrate that these translated images within SUIT can significantly improve performance of the model on the target domain, and our model with FCN8s-VGG16 architecture achieves around 13 percentage points improvement in terms of mIoU on multiple semantic segmentation adaptation benchmarks.","Domain adaptation, Image segmentation, Image translation",Rui Li and Wenming Cao and Qianfen Jiao and Si Wu and Hau-San Wong,https://www.sciencedirect.com/science/article/pii/S0031320320301461,https://doi.org/10.1016/j.patcog.2020.107343,0031-3203,2020,107343,105,Pattern Recognition,Simplified unsupervised image translation for semantic segmentation adaptation,article,LI2020107343,
"The first attention model in the computer science community is proposed in 1998. In the following years, human attention has been intensively studied. However, these studies mainly refer human attention as the image regions that draw the attention of a human (outside the image) who is looking at the image. In this paper, we infer the attention of a human inside a third-person view video where the human is doing a task, and define human attention as attentional objects that coincide with the task the human is doing. To infer human attention, we propose a deep neural network model that fuses both low-level human pose cue and high-level task encoding cue. Due to the lack of appropriate public datasets for studying this problem, we newly collect a video dataset in complex Virtual-Reality (VR) scenes. In the experiments, we widely compare our method with three other methods on this VR dataset. In addition, we re-annotate a public real dataset and conduct the extensional experiments on this real dataset. The experiment results validate the effectiveness of our method.","Human attention, Deep neural network, Attentional objects",Zhixiong Nan and Tianmin Shu and Ran Gong and Shu Wang and Ping Wei and Song-Chun Zhu and Nanning Zheng,https://www.sciencedirect.com/science/article/pii/S0031320320301187,https://doi.org/10.1016/j.patcog.2020.107314,0031-3203,2020,107314,103,Pattern Recognition,Learning to infer human attention in daily activities,article,NAN2020107314,
"Equipped with powerful convolutional neural networks (CNNs), generative models have achieved tremendous success in various vision applications. However, deep generative networks suffer from high computational and memory costs in both model training and deployment. While many efforts have been devoted to accelerate discriminative models by quantization, effectively reducing the costs for deep generative models is more challenging and remains unexplored. In this work, we investigate applying quantization technology to deep generative models. We find that keeping as much information as possible for quantized activations is key to obtain high-quality generative models. With this in mind, we propose Deep Quantization Generative Networks (DQGNs) to effectively accelerate and compress deep generative networks. By expanding the dimensions of the quantization basis space, DQGNs can achieve lower quantization error and are highly adaptive to complex data distributions. Various experiments on two powerful frameworks (i.e., variational auto-encoders, and generative adversarial networks) and two practical applications (i.e., style transfer, and super-resolution) demonstrate our findings and the effectiveness of our proposed approach.","Compression, Acceleration, Generative models, Network quantization",Diwen Wan and Fumin Shen and Li Liu and Fan Zhu and Lei Huang and Mengyang Yu and Heng Tao Shen and Ling Shao,https://www.sciencedirect.com/science/article/pii/S0031320320301412,https://doi.org/10.1016/j.patcog.2020.107338,0031-3203,2020,107338,105,Pattern Recognition,Deep quantization generative networks,article,WAN2020107338,
"In the process of exploring the world, the curiosity constantly drives humans to cognize new things. Supposing you are a zoologist, for a presented animal image, you can recognize it immediately if you know its class. Otherwise, you would more likely attempt to cognize it by exploiting the side-information (e.g., semantic information, etc.) you have accumulated. Inspired by this, this paper decomposes the generalized zero-shot learning (G-ZSL) task into an open set recognition (OSR) task and a zero-shot learning (ZSL) task, where OSR recognizes seen classes (if we have seen (or known) them) and rejects unseen classes (if we have never seen (or known) them before), while ZSL identifies the unseen classes rejected by the former. Simultaneously, without violating OSRâs assumptions (only known class knowledge is available in training), we also first attempt to explore a new generalized open set recognition (G-OSR) by introducing the accumulated side-information from known classes to OSR. For G-ZSL, such a decomposition effectively solves the class overfitting problem with easily misclassifying unseen classes as seen classes. The problem is ubiquitous in most existing G-ZSL methods. On the other hand, for G-OSR, introducing such semantic information of known classes not only improves the recognition performance but also endows OSR with the cognitive ability of unknown classes. Specifically, a visual and semantic prototypes-jointly guided convolutional neural network (VSG-CNN) is proposed to fulfill these two tasks (G-ZSL and G-OSR) in a unified end-to-end learning framework. Extensive experiments on benchmark datasets demonstrate the advantages of our learning framework.","Convolutional prototype learning, Generalized zero-shot Learning, Open set recognition",Chuanxing Geng and Lue Tao and Songcan Chen,https://www.sciencedirect.com/science/article/pii/S0031320320300686,https://doi.org/10.1016/j.patcog.2020.107263,0031-3203,2020,107263,102,Pattern Recognition,Guided CNN for generalized zero-shot and open-set recognition using visual and semantic prototypes,article,GENG2020107263,
"Image classifiers based on deep neural networks show severe vulnerability when facing adversarial examples crafted on purpose. Designing more effective and efficient adversarial attacks is attracting considerable interest due to its potential contribution to interpretability of deep learning and validation of neural networksâ robustness. However, current iterative attacks use a fixed step size for each noise-adding step, making further investigation into the effect of variable step size on model robustness ripe for exploration. We prove that if the upper bound of noise added to the original image is fixed, the attack effect can be improved if the step size is positively correlated with the gradient obtained at each step by querying the target model. In this paper, we propose Ada-FGSM (Adaptive FGSM), a new iterative attack that adaptively allocates step size of noises according to gradient information at each step. Improvement of success rate and accuracy decrease measured on ImageNet with multiple models emphasizes the validity of our method. We analyze the process of iterative attack by visualizing their trajectory and gradient contour, and further explain the vulnerability of deep neural networks to variable step size adversarial examples.","Adversarial example, Adversarial attack, Image classification",Yucheng Shi and Yahong Han and Quanxin Zhang and Xiaohui Kuang,https://www.sciencedirect.com/science/article/pii/S0031320320301138,https://doi.org/10.1016/j.patcog.2020.107309,0031-3203,2020,107309,105,Pattern Recognition,Adaptive iterative attack towards explainable adversarial robustness,article,SHI2020107309,
"Tracking movement of insects in a social group (such as ants) is challenging, because the individuals are not only similar in appearance but also likely to perform intensive body contact and sudden movement adjustment (start/stop, direction changes). To address this challenge, we introduce an online multi-object tracking framework that combines both the motion and appearance information of ants. We obtain the appearance descriptors by using the ResNet model for offline training on a small (N=50) sample dataset. For online association, a cosine similarity metric computes the matching degree between historical appearance sequences of the trajectory and the current detection. We validate our method in both indoor (lab setup) and outdoor video sequences. The results show that our model obtains 99.3%Â Â Â±Â Â 0.5% MOTA and 91.9%Â Â Â±Â Â 2.1% MOTP across 24,050 testing samples in five indoor sequences, with real-time tracking performance. In an outdoor sequence, we achieve 99.3% MOTA and 92.9% MOTP across 22,041 testing samples. The datasets and code are made publicly available for future research in relevant domains.","Ant tracking, ResNet model, Mahalanobis distance, Appearance descriptors",Xiaoyan Cao and Shihui Guo and Juncong Lin and Wenshu Zhang and Minghong Liao,https://www.sciencedirect.com/science/article/pii/S003132032030039X,https://doi.org/10.1016/j.patcog.2020.107233,0031-3203,2020,107233,103,Pattern Recognition,"Online tracking of ants based on deep association metrics: method, dataset and evaluation",article,CAO2020107233,
"the GrabCut can effectively extract the foreground according to features in a cartoon image; however, the performance is not so effective for a real image, because the feature extraction is independent of segmentation. To improve segmentation performance, this paper proposes an improved GrabCut which combines the segmentation and multiscale feature extraction into a unified model. In this model, the segmentation relies on multiscale features, and the multiscale features depend on multiscale decomposition. A novel total variation regularization is proposed in multiscale decomposition to preserve edges and remove the region inhomogeneity, by which the generalization of features for segmentation is improved. The features obtained by the multiscale decomposition are integrated into the segmentation process, and the foreground can be easily extracted from a proper scale. Experimental results indicate that, compared to the existing GrabCut and improved techniques, this method provides competitive performance in terms of the segmentation accuracy, while being insensitive to inhomogeneity.","Image segmentation, GrabCut, Multiscale feature, Total variation regularization, Inhomogeneity",Kun He and Dan Wang and Miao Tong and Zhijuan Zhu,https://www.sciencedirect.com/science/article/pii/S0031320320300960,https://doi.org/10.1016/j.patcog.2020.107292,0031-3203,2020,107292,103,Pattern Recognition,An improved GrabCut on multiscale features,article,HE2020107292,
"First-person interaction recognition is a challenging task because of unstable video conditions resulting from the camera wearerâs movement. For human interaction recognition from a first-person viewpoint, this paper proposes a three-stream fusion network with two main parts: three-stream architecture and three-stream correlation fusion. The three-stream architecture captures the characteristics of the target appearance, target motion, and camera ego-motion. Meanwhile the three-stream correlation fusion combines the feature map of each of the three streams to consider the correlations among the target appearance, target motion, and camera ego-motion. The fused feature vector is robust to the camera movement and compensates for the noise of the camera ego-motion. Short-term intervals are modeled using the fused feature vector, and a long short-term memory (LSTM) model considers the temporal dynamics of the video. We evaluated the proposed method on two public benchmark datasets to validate the effectiveness of our approach. The experimental results show that the proposed fusion method successfully generated a discriminative feature vector, and our network outperformed all competing activity recognition methods in first-person videos where considerable camera ego-motion occurs.","First-person vision, First-person interaction recognition, Three-stream fusion network, Three-stream correlation fusion, Camera ego-motion",Ye-Ji Kim and Dong-Gyu Lee and Seong-Whan Lee,https://www.sciencedirect.com/science/article/pii/S0031320320300844,https://doi.org/10.1016/j.patcog.2020.107279,0031-3203,2020,107279,103,Pattern Recognition,Three-stream fusion network for first-person interaction recognition,article,KIM2020107279,
"With the breakthroughs in general action understanding, it has become an inevitable trend to analyze the actions in finer granularity. However, related researches have been largely hindered by the lack of fine-grained datasets and the difficulty of capturing subtle differences between fine-grained actions that are highly similar overall. In this paper, we address the above challenges by constructing a fine-grained action dataset, i.e., Figure Skating, which can be used for end-to-end network training and presenting a framework for the joint optimization of classification and similarity constraints. We propose to incorporate the triplet loss into the training of Convolutional Neural Network, which learns a mapping from fine-grained actions to a compact Euclidean space where distances directly correspond to a measure of action similarity. Triplet loss compels actions of distinct classes to have larger distances than actions of the same class. Besides, to boost the discrimination of the fine-grained actions, we further propose a temporal variance embedding network (TVENet) embedding temporal context variances into the feature embeddings during the joint network training. The experimental results on Figure Skating dataset, HMDB51 dataset as well as UCF101 dataset demonstrate the effectiveness of TVENet representation for fine-grained action search.","Fine-grained action representation, temporal variance embedding network (TVENet), joint optimization, temporal triplet loss, action search",Tingting Han and Hongxun Yao and Wenlong Xie and Xiaoshuai Sun and Sicheng Zhao and Jun Yu,https://www.sciencedirect.com/science/article/pii/S0031320320300728,https://doi.org/10.1016/j.patcog.2020.107267,0031-3203,2020,107267,103,Pattern Recognition,TVENet: Temporal variance embedding network for fine-grained action representation,article,HAN2020107267,
"This paper addresses the person search task, which is a computer vision technology that finds the location of a pedestrian and retrieves it in a video taken by a single camera or multiple cameras. This task is much more challenging than the conventional settings for person re-identification or pedestrian detection since the search is susceptible to factors such as different resolutions, similar pedestrians, lighting, viewing angles and occlusion. Moreover, the person search task is a typical big data-small sample problem because each pedestrian only has a few images. It is difficult for the model to learn the discriminant features of pedestrians with a small quantity of pedestrian data. This paper proposes a framework for person search that uses the original training set without collecting extra data by implementing a generative adversarial network (GAN) to generate unlabeled samples. We propose a deep complementary classifier for pedestrian detection to leverage complementary object regions for pedestrian/non-pedestrian classification. In the re-identification section, we propose a center-constrained triplet loss that avoids the complicated triplet selection of the triplet loss and simultaneously pushes away all the distances of rather similar negative centers and the positive center. Experiments show that the GAN-generated data can effectively help to improve the discriminating ability of the CNN model. On the two large-scale datasets, CUHK-SYSU and PRW, we achieve a performance improvement over the baseline CNN. We additionally apply the proposed center-constrained triplet loss and complementary classifiers in the training model, and we achieve mAP improvements over the original method of +1.9% on CUHK-SYSU and +2.5% on PRW.","Person search, Re-Identification, Pedestrian detection",Rui Yao and Cunyuan Gao and Shixiong Xia and Jiaqi Zhao and Yong Zhou and Fuyuan Hu,https://www.sciencedirect.com/science/article/pii/S0031320320301539,https://doi.org/10.1016/j.patcog.2020.107350,0031-3203,2020,107350,104,Pattern Recognition,GAN-based person search via deep complementary classifier with center-constrained Triplet loss,article,YAO2020107350,
"Chinese characters have a valuable property, this is, numerous Chinese characters are composed of a compact set of fundamental and structural radicals. This paper introduces a radical analysis network (RAN) that makes full use of this valuable property to implement radical-based Chinese character recognition. The proposed RAN employs an attention mechanism to extract radicals from Chinese characters and to detect spatial structures among the radicals. Then, the decoder in RAN generates a hierarchical composition of Chinese characters based on the knowledge of the extracted radicals and their internal structures. The method of treating a Chinese character as a composition of radicals rather than as a single character category is a human-like method that can reduce the size of the vocabulary, ignore redundant information among similar characters and enable the system to recognize unseen Chinese character categories, i.e., zero-shot learning. Through experiments, we assess the practicality of RAN for recognizing Chinese characters in natural scenes. Furthermore, a RAN framework can be proposed for scene text recognition with the extension of a dense recurrent neural network (denseRNN) encoder, a multihead coverage attention model and HSV representations. The proposed approach achieved the best performance in the ICPR MTWI 2018 competition.","Radical, Attention, Chinese character, Few-/zero-shot learning",Jianshu Zhang and Jun Du and Lirong Dai,https://www.sciencedirect.com/science/article/pii/S0031320320301096,https://doi.org/10.1016/j.patcog.2020.107305,0031-3203,2020,107305,103,Pattern Recognition,Radical analysis network for learning hierarchies of Chinese characters,article,ZHANG2020107305,
"Recent progress on one-stage detectors focuses on improving the quality of bounding boxes, while they pay less attention to the classification head. In this work, we focus on investigating the influence of the classification head. To understand the behavior of the classifier in one-stage detectors, we resort to the methods of the Explainable deep learning area. We visualize its learned representations via activation maps and analyze its robustness to image scene context. Based on the analysis, we observe that the classifier limits the performance of the detector due to its limited receptive field and the lack of object locations. Then, we design a simple but efficient location-aware multi-dilation module (LAMD) to enhance the weak classifier. We conduct extensive experiments on the COCO benchmark to validate the effectiveness of LAMD. The results suggest that our LAMD can achieve consistent improvements and leads to robust detection across various one-stage detectors with different backbones.","Object detetion, Classification, Localization, Feature visualization, Receptive field",Qiang Chen and Peisong Wang and Anda Cheng and Wanguo Wang and Yifan Zhang and Jian Cheng,https://www.sciencedirect.com/science/article/pii/S0031320320301370,https://doi.org/10.1016/j.patcog.2020.107334,0031-3203,2020,107334,105,Pattern Recognition,Robust one-stage object detection with location-aware classifiers,article,CHEN2020107334,
"Matching partially overlapping point sets is a challenging problem in computer vision. To achieve this goal, we model point matching as a mixed linear assignmentÂ -Â least square problem. By eliminating the transformation variable, we reduce the minimization problem to a concave optimization problem with the property that the objective function can be converted into a form with few nonlinear terms. We then use a heuristic variant of the branch-and-bound algorithm for optimization where convergence of the upper bound is used as the stopping criterion. We also propose a new lower bounding scheme which involves solving a k-cardinality linear assignment problem. Two cases of transformations, transformation output being linear with respect to parameters and 2D/3D similarity transformations, are discussed, resulting in ability to handle unknown arbitrary translation and similarity, respectively. Experimental results demonstrate better robustness of the algorithm over state-of-the-art methods.","Concave optimization, Point matching, Branch-and-bound, Linear assignment, Global optimization",Wei Lian and Lei Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320301254,https://doi.org/10.1016/j.patcog.2020.107322,0031-3203,2020,107322,103,Pattern Recognition,A concave optimization algorithm for matching partially overlapping point sets,article,LIAN2020107322,
"Image and video cosegmentation is a newly emerging and rapidly progressing area, which aims at delineating common objects at pixel-level from a group of images or a set of videos. Plenty of related works have been published and implemented in varied applications, but there lacks a systematic survey on both image and video cosegmentation. This paper provides a comprehensive overview including the existing methods, applications, and challenges. Specifically, different cosegmentation problem settings are described, the formulation details of the methods are summarized and their potential applications are listed. Moreover, the benchmark datasets and standard evaluation metrics are also given; and the future directions and unsolved challenges are discussed.","Image cosegmentation, Video cosegmentation",Yan Ren and Adams Wai Kin Kong and Licheng Jiao,https://www.sciencedirect.com/science/article/pii/S0031320320301011,https://doi.org/10.1016/j.patcog.2020.107297,0031-3203,2020,107297,103,Pattern Recognition,"A survey on image and video cosegmentation: Methods, challenges and analyses",article,REN2020107297,
"End-to-end training of a deep CNN-Based model for salient object detection usually requires a huge number of training samples with pixel-level annotations, which are costly and time-consuming to obtain. In this paper, we propose an approach that can utilize large amounts of web data for learning a deep salient object detection model. With thousands of images collected from the Web, we first employ several bottom-up saliency detection techniques to generate salient object masks for all images, and then use a novel quality evaluation method to pick out a subset of images with reliable masks for training. After that, we develop a self-training approach to boost the performance of our initial network, which iterates between the network training process and the training set updating process. Importantly, different from existing webly-supervised or weakly-supervised methods, our approach is able to automatically select reliable images for network training without requiring any human intervention (e.g., dividing images into different difficulty levels). Results of extensive experiments on several widely-used benchmarks demonstrate that our method has achieved state-of-the-art performance. It significantly outperforms existing unsupervised and weakly-supervised salient object detection methods, and achieves competitive or even better performance than fully supervised approaches.","Salient object detection, Webly-supervised learning, Deep learning",Ao Luo and Xin Li and Fan Yang and Zhicheng Jiao and Hong Cheng,https://www.sciencedirect.com/science/article/pii/S0031320320301126,https://doi.org/10.1016/j.patcog.2020.107308,0031-3203,2020,107308,103,Pattern Recognition,Webly-supervised learning for salient object detection,article,LUO2020107308,
"Image representation is the key factor influencing the accuracy of remote sensing image segmentation. Traditional algorithms rely on the pixel-wise characteristics exhibited in the feature space. They introduce spatial information by establishing the connections between neighboring pixels in the neighborhood system. But the spectral-spatial features cannot be well expressed. In this paper, a Riemannian manifold space is introduced to express the contextual information by jointly mapping the spectral features of a pixel and its neighboring ones on to it. To benefit from the expression ability and geometric properties of the Riemannian manifold, a data submanifold and a parameter submanifold are established to depict the characteristics of the detected image and all possible segmentation results. On the parameter submanifold, only points representing objects of the current segmentation are active. Then distance between a point on the data submanifold and an active point on the parameter submanifold is measured by a geodesic-kernel function. Consequently, four geodesic-kernel function-based manifold projection criteria are proposed to explore the complementation between features expressed in different feature spaces. Experiments on synthetic and real remote sensing images demonstrated that the proposed geodesic-kernel function-based manifold projection algorithm outperforms traditional ones and features expressed in different feature spaces did contain some complementary information.","Remote sensing, Image segmentation, Riemannian manifold, Manifold projection, Kernel function",Xuemei Zhao and Haijian Wang and Jun Wu and Yu Li and Shijie Zhao,https://www.sciencedirect.com/science/article/pii/S0031320320301369,https://doi.org/10.1016/j.patcog.2020.107333,0031-3203,2020,107333,104,Pattern Recognition,Remote sensing image segmentation using geodesic-kernel functions and multi-feature spaces,article,ZHAO2020107333,
"Single-modality human action recognition on RGB or skeleton has been extensively studied. Each of these two modalities has its own advantages as well as limitations, because they depict action from different perspectives. The feature of different modalities can complement each other for describing actions. Therefore, it is meaningful to fuse these two modalities using their complementarity for action recognition. However, existing multimodal methods fail to fully exploit the complementarity of RGB and skeleton modalities. In this paper, we propose a Skeleton-Guided Multimodal Network (SGM-Net) for human action recognition. The proposed method takes full use of the complementarity of these two modalities at semantic feature level. From the technical perspective, we introduce a guided block, the key component of SGM-Net. It enables skeleton feature to guide on RGB feature, so that the important RGB information strongly related to the action is enhanced. Moreover, in the guided block, two schemes of correlation operation are explored. We perform a series of ablation experiments to verify the effectiveness of the guided block. The experimental results show that our approach achieves state-of-the-art performance over the existing methods on NTU and Sub-JHMDB datasets.","Action recognition, multi-modality, skeleton-guided",Jianan Li and Xuemei Xie and Qingzhe Pan and Yuhan Cao and Zhifu Zhao and Guangming Shi,https://www.sciencedirect.com/science/article/pii/S003132032030159X,https://doi.org/10.1016/j.patcog.2020.107356,0031-3203,2020,107356,104,Pattern Recognition,SGM-Net: Skeleton-guided multimodal network for action recognition,article,LI2020107356,
"Recognising and locating image patches or sets of image features is an important task underlying much work in computer vision. Traditionally this has been accomplished using template matching. However, template matching is notoriously brittle in the face of changes in appearance caused by, for example, variations in viewpoint, partial occlusion, and non-rigid deformations. This article tests a method of template matching that is more tolerant to such changes in appearance and that can, therefore, more accurately identify image patches. In traditional template matching the comparison between a template and the image is independent of the other templates. In contrast, the method advocated here takes into account the evidence provided by the image for the template at each location and the full range of alternative explanations represented by the same template at other locations and by other templates. Specifically, the proposed method of template matching is performed using a form of probabilistic inference known as âexplaining awayâ. The algorithm used to implement explaining away has previously been used to simulate several neurobiological mechanisms, and been applied to image contour detection and pattern recognition tasks. Here it is applied for the first time to image patch matching, and is shown to produce superior results in comparison to the current state-of-the-art methods.","Template matching, Feature detection, Image matching, Image registration, Correspondence problem, Multi-view vision",M.W. Spratling,https://www.sciencedirect.com/science/article/pii/S0031320320301400,https://doi.org/10.1016/j.patcog.2020.107337,0031-3203,2020,107337,104,Pattern Recognition,Explaining away results in accurate and tolerant template matching,article,SPRATLING2020107337,
"In this work, we introduce an anisotropic minimal path model based on a new Riemannian tensor integrating the crossing-adaptive anisotropic radius-lifted tensor field and the front freezing indicator by appearance and path features. The non-local path feature only can be obtained during the geodesic distance computation process by the fast marching method. The predefined criterion derived from path feature is able to steer the front evolution by freezing the point causing high bending of the geodesic to solve the shortcut problem. We performed qualitative and quantitative experiments on synthetic and real images (including retinal vessels, rivers and roads) and compare with the minimal path models with classical anisotropic Riemannian metric and dynamic isotropic metric, which demonstrated the proposed method can detect desired targets from complex tubular tree structures.","Minimal path model, Anisotropy enhancement, Riemannian metric, Path feature, Tubular structures",Li Liu and Da Chen and Laurent D. Cohen and Jiasong Wu and Michel Paques and Huazhong Shu,https://www.sciencedirect.com/science/article/pii/S0031320320301527,https://doi.org/10.1016/j.patcog.2020.107349,0031-3203,2020,107349,104,Pattern Recognition,Anisotropic tubular minimal path model with fast marching front freezing scheme,article,LIU2020107349,
"This paper proposes a strategy for the automatic registration of soccer images on a model of the field of play. First, a robust and efficient preprocessing is applied to discard the areas of the image that do not belong to the field of play and eliminate most edge points that are not part of the line marks. Then, a novel probabilistic decision tree is used to identify the most probable classification for the set of all the straight lines in the image. Additionally, the line surrounding the center circle is also modeled for providing results when only few straight lines are visible. Finally, a three-step analysis stage is applied to ensure the validity of the results. To assess its quality, the strategy has been tested on several sequences corresponding to three stadiums with different characteristics. The results obtained have shown that the registration is successful in most images (94%).","Soccer, Football, Field of play, Playing field, Registration, Segmentation, Line segment detection, Line classification, Validation",Carlos Cuevas and Daniel QuilÃ³n and Narciso GarcÃ­a,https://www.sciencedirect.com/science/article/pii/S0031320320300832,https://doi.org/10.1016/j.patcog.2020.107278,0031-3203,2020,107278,103,Pattern Recognition,Automatic soccer field of play registration,article,CUEVAS2020107278,
"Saliency detection has made remarkable progress along with the development of deep learning. While how to integrate the low-level intrinsic context with high-level semantic information to keep the object boundary sharp and restrain the background noise is still a challenging problem. Many attempts on network structures and refinement strategies have been explored, such as using Conditional Random Field (CRF) to improve the accuracy of saliency map, but it is independent from the deep network and cannot be trained end-to-end. To tackle this issue, we propose a novel Deep Conditional Random Field network (DCRF) to take both deep feature and neighbor information into consideration. First, Multi-scale Feature Extraction Module (MFEM) is adopted to capture the low level texture and high level semantic features, multi-stacks of deconvolution layers are employed to improve the spatial resolution of deep layers. Then we employ Backward Optimization Module (BOM) to guide shallower layers by high-level location and shape information derived from deeper layers, which intrinsically enhance the representational capacity of low-level features. Finally, a Deep Conditional Random Field Module (DCRFM) with unary and pairwise potentials is designed to concentrate on spatial neighbor relations to obtain a compact and uniformed saliency map. Extensive experimental results on 5 datasets in terms of 6 evaluation metrics demonstrate that the proposed method achieves state-of-the-art performance.","Saliency detection, Conditional random field, Convolutional neural network",Wenliang Qiu and Xinbo Gao and Bing Han,https://www.sciencedirect.com/science/article/pii/S0031320320300716,https://doi.org/10.1016/j.patcog.2020.107266,0031-3203,2020,107266,103,Pattern Recognition,Saliency detection using a deep conditional random field network,article,QIU2020107266,
"We focus on an image classification task in which only several unlabeled images per class are available for learning and low computational complexity is required. We recall the state-of-the-art methods that are used to solve the task: autoencoder-based approaches and manifold-decomposition-based approaches. Next, we introduce our proposed method, which is based on a combination of the F-transform and (kernel) principal component analysis. F-transform significantly reduces the computation time of PCA and increases the robustness of PCA to translation, while PCA proposes more descriptive features. This combination performs 3D reduction: the F-transform reduces dimensionality over a single 2D image, while PCA reduces dimensionality through the whole set of processed images. Based on the benchmark results, our method may outperform deep-learning-based methods in limited settings. For completeness, we also address other image resampling algorithms that can be used instead of the F-transform, and we find that the F-transform is the most suitable.","Unsupervised learning, Dimensionality reduction, PCA, F-transform, Image classification, Autoencoder",Petr Hurtik and Vojtech Molek and Irina Perfilieva,https://www.sciencedirect.com/science/article/pii/S0031320320300959,https://doi.org/10.1016/j.patcog.2020.107291,0031-3203,2020,107291,103,Pattern Recognition,Novel dimensionality reduction approach for unsupervised learning on small datasets,article,HURTIK2020107291,
"Accurately detecting scene text is a challenging task due to perspective distortion, scale variance, varied orientations, uneven illumination. Among them, scale variance has always been a core issue and generally involves two types: various size and diverse aspect ratios of the text regions. In contrast to most existing approaches focusing on addressing one type of scale variance, this paper presents a novel inside-to-outside supervision network (IOS-Net) that can well tackle both two. Specifically, we design a hierarchical supervision module (HSM), which consists of a new inception unit with parallel asymmetric convolution and a skip-layer fusion structure. Inside the HSM, we introduce hierarchical supervision into the new inception unit to effectively capture the texts with diverse aspect ratios. Outside the HSM, we adopt multiple-scale supervision on the stacked HSMs to accurately detect the texts with various sizes. Moreover, a position-sensitive segmentation is used to enhance the representation of difficult text objects and the discrimination of adjacent ones. The proposed method achieves state-of-the-art performance on representative public benchmarks, reaching 86% F-score and 11.5 frames per second (FPS) on the ICDAR 2015 incidental text dataset, 47% F-score and 16.1 FPS on the COCO-Text dataset, 69% F-score and 11.7 FPS on the ICDAR 2013 video text dataset.","Text detection, Various sizes, Diverse aspect ratios, Inside-to-outside supervision, Position-sensitive segmentation",Yuanqiang Cai and Weiqiang Wang and Yuting Chen and Qixiang Ye,https://www.sciencedirect.com/science/article/pii/S0031320320301084,https://doi.org/10.1016/j.patcog.2020.107304,0031-3203,2020,107304,103,Pattern Recognition,IOS-Net: An inside-to-outside supervision network for scale robust text detection in the wild,article,CAI2020107304,
"We propose a new point set registration method based on mixture framework and variational inference. A three-phase registration strategy (TRS) is proposed to automatically process point set registration problem in different cases. A Gaussian variational mixture model (GVMM) with isotropic and anisotropic components under the variational inference framework is designed to weaken the effect of outliers. The Dirichlet distribution is applied to govern the mixture proportion of Gaussian components and then distinguishes missing points. We test the performance of our method in contour registration, Graffiti images, retinal images, remote sensing images and 3D human motion, and compare with six state-of-the-art methods. Our method shows favorable performances in most scenarios.","Point set registration, Image registration, Gaussian variational mixture model, Variational inference",Xinke Ma and Shijin Xu and Jie Zhou and Qinglu Yang and Yang Yang and Kun Yang and Sim Heng Ong,https://www.sciencedirect.com/science/article/pii/S0031320320301485,https://doi.org/10.1016/j.patcog.2020.107345,0031-3203,2020,107345,104,Pattern Recognition,Point set registration with mixture framework and variational inference,article,MA2020107345,
"Depth map upsampling will unavoidably smoothen the edges leading to blurry results on the depth boundaries, especially at large upscaling factors. Given that edges represent the most important cue in addressing the task of depth upsampling, we propose a novel depth upsampling framework based on deep edge-aware learning. Unlike existing CNN-based approaches that directly predict depth values from low resolution (LR) depth input, our framework firstly learns edge information of depth boundaries from the known LR depth map and its corresponding high resolution (HR) color image as reconstruction cues. Then, two depth restoration modules, i.e., a fast depth filling strategy and a cascaded restoration network, are proposed to recover HR depth map by leveraging the predicted edge map and the HR color image. Extensive comparisons on both edge inference and depth upsampling under noisy and noiseless cases demonstrate the superiority of the proposed approaches.","Upsampling, CNN, Edge-aware, Depth map",Zhihui Wang and Xinchen Ye and Baoli Sun and Jingyu Yang and Rui Xu and Haojie Li,https://www.sciencedirect.com/science/article/pii/S0031320320300790,https://doi.org/10.1016/j.patcog.2020.107274,0031-3203,2020,107274,103,Pattern Recognition,Depth upsampling based on deep edge-aware learning,article,WANG2020107274,
"Recently, graph convolutional networks (GCNs) have achieved state-of-the-art results for skeleton based action recognition by expanding convolutional neural networks (CNNs) to graphs. However, due to the lack of effective feature aggregation method, e.g. max pooling in CNN, existing GCN-based methods only learn local information among adjacent joints and are hard to obtain high-level interaction features, such as interactions between five parts of human body. Moreover, subtle differences of confusing actions often hide in specific channels of key jointsâ features, this kind of discriminative information is rarely exploited in previous methods. In this paper, we propose a novel graph convolutional network with structure based graph pooling (SGP) scheme and joint-wise channel attention (JCA) modules. The SGP scheme pools the human skeleton graph according to the prior knowledge of human bodyâs typology. This pooling scheme not only leads to more global representations but also reduces the amount of parameters and computation cost. The JCA module learns to selectively focus on discriminative joints of skeleton and pays different levels of attention to different channels. This novel attention mechanism enhance the modelâs ability to classify confusing actions. We evaluate our SGP scheme and JCA module on three most challenging skeleton based action recognition datasets: NTU-RGB+D, Kinetics-M, and SYSU-3D. Our method outperforms the state-of-art methods on three benchmarks.","Graph convolutional network, Structure graph pooling, Joint-wise channel attention",Yuxin Chen and Gaoqun Ma and Chunfeng Yuan and Bing Li and Hui Zhang and Fangshi Wang and Weiming Hu,https://www.sciencedirect.com/science/article/pii/S0031320320301242,https://doi.org/10.1016/j.patcog.2020.107321,0031-3203,2020,107321,103,Pattern Recognition,Graph convolutional network with structure pooling and joint-wise channel attention for action recognition,article,CHEN2020107321,
"Low-rank matrix factorization (LRMF) has received much popularity owing to its successful applications in both computer vision and data mining. By assuming noise to come from a Gaussian, Laplace or mixture of Gaussian distributions, significant efforts have been made on optimizing the (weighted) L1 or L2-norm loss between an observed matrix and its bilinear factorization. However, the type of noise distribution is generally unknown in real applications and inappropriate assumptions will inevitably deteriorate the behavior of LRMF. On the other hand, real data are often corrupted by skew rather than symmetric noise. To tackle this problem, this paper presents a novel LRMF model called AQ-LRMF by modeling noise with a mixture of asymmetric Laplace distributions. An efficient algorithm based on the expectation-maximization (EM) algorithm is also offered to estimate the parameters involved in AQ-LRMF. The AQ-LRMF model possesses the advantage that it can approximate noise well no matter whether the real noise is symmetric or skew. The core idea of AQ-LRMF lies in solving a weighted L1 problem with weights being learned from data. The experiments conducted on synthetic and real data sets show that AQ-LRMF outperforms several state-of-the-art techniques. Furthermore, AQ-LRMF also has the superiority over the other algorithms in terms of capturing local structural information contained in real images.","Low-rank matrix factorization, Mixture of asymmetric Laplace distributions, Expectation maximization algorithm, Skew noise",Shuang Xu and Chunxia Zhang and Jiangshe Zhang,https://www.sciencedirect.com/science/article/pii/S003132032030114X,https://doi.org/10.1016/j.patcog.2020.107310,0031-3203,2020,107310,103,Pattern Recognition,Adaptive quantile low-rank matrix factorization,article,XU2020107310,
"In this paper, we aim to improve the performance of semantic image segmentation in a semi-supervised setting where training is performed with a reduced set of annotated images and additional non-annotated images. We present a method based on an ensemble of deep segmentation models. Models are trained on subsets of the annotated data and use non-annotated images to exchange information with each other, similar to co-training. Diversity across models is enforced with the use of adversarial samples. We demonstrate the potential of our method on three challenging image segmentation problems, and illustrate its ability to share information between simultaneously trained models, while preserving their diversity. Results indicate clear advantages in terms of performance compared to recently proposed semi-supervised methods for segmentation.","Deep learning, Semi-supervised learning, Ensemble learning, Co-training, Image segmentation",Jizong Peng and Guillermo Estrada and Marco Pedersoli and Christian Desrosiers,https://www.sciencedirect.com/science/article/pii/S0031320320300741,https://doi.org/10.1016/j.patcog.2020.107269,0031-3203,2020,107269,107,Pattern Recognition,Deep co-training for semi-supervised image segmentation,article,PENG2020107269,
"Recently, there have been various saliency detection methods proposed for still images based on deep learning techniques. However, the research on saliency detection for video sequences is still limited. In this study, we introduce a novel deep learning framework of saliency detection for video sequences, namely Deep Video Saliency Network (DevsNet). DevsNet mainly consists of two components: 3D Convolutional Network (3D-ConvNet) and Bidirectional Convolutional Long-Short Term Memory Network (B-ConvLSTM). 3D-ConvNet is constructed to learn short-term spatiotemporal information and the long-term spatiotemporal features are learned by B-ConvLSTM. The designed B-ConvLSTM can extract the temporal information not just from the previous video frames but also from the next frames, which demonstrates that the proposed model considers both the forward and backward temporal information. By combining the short-term and long-term spatiotemporal cues, the proposed DevsNet can extract saliency information for video sequences effectively and efficiently. Extensive experiments have been conducted to show that the proposed model can obtain better performance in spatiotemporal saliency prediction than the state-of-the-art models.","Video saliency detection, Spatiotemporal saliency, 3D convolution network (3D-ConvNet), Bidirectional convolutional long-short term memory network (B-ConvLSTM)",Yuming Fang and Chi Zhang and Xiongkuo Min and Hanqin Huang and Yugen Yi and Guangtao Zhai and Chia-Wen Lin,https://www.sciencedirect.com/science/article/pii/S0031320320300984,https://doi.org/10.1016/j.patcog.2020.107294,0031-3203,2020,107294,103,Pattern Recognition,DevsNet: Deep Video Saliency Network using Short-term and Long-term Cues,article,FANG2020107294,
"Gait recognition invariant to carried objects (COs) is very difficult in a real-life scene because the COs can have various shapes and sizes, in addition to unpredictable carrying locations (e.g., front, back, and side, or multiple locations). Therefore, in this paper, we propose a robust method for gait recognition against various COs by reconstructing a gait template without COs. A straightforward approach is to directly generate a gait template without COs given a gait template with COs as the input using a conventional generative adversarial network. There is, however, a potential risk of unnecessarily altering parts that were originally unaffected by COs (e.g., leg parts for a person carrying a backpack). Because we do not want to touch such unaffected parts in the original template, we first estimate a gait template without COs, and then blend it with the original template by an estimated alpha matte that indicates the blending parameters. We then create an alpha-blended template from the original template and the generated template without COs based on the estimated alpha matte. We use two independent generators to estimate the alpha matte and the generated template without COs. Finally, we feed the alpha-blended gait template into a state-of-the-art discrimination network for gait recognition. The experimental results on three publicly available gait databases with real-life COs demonstrate the state-of-the-art performance of the proposed method.","Alpha blending, Generative adversarial network, Gait recognition, Carried objects",Xiang Li and Yasushi Makihara and Chi Xu and Yasushi Yagi and Mingwu Ren,https://www.sciencedirect.com/science/article/pii/S0031320320301795,https://doi.org/10.1016/j.patcog.2020.107376,0031-3203,2020,107376,105,Pattern Recognition,Gait recognition invariant to carried objects using alpha blending generative adversarial networks,article,LI2020107376,
"Orthogonal moments enable computer-based systems to discriminate between similar objects. Mathematicians proved that the orthogonal polynomials of fractional-orders outperformed their corresponding counterparts in representing the fine details of a given function. In this work, novel orthogonal fractional-order Legendre-Fourier moments are proposed for pattern recognition applications. The basis functions of these moments are defined and the essential mathematical equations for the recurrence relations, orthogonality and the similarity transformations (rotation and scaling) are derived. The proposed new fractional-order moments are tested where their performance is compared with the existing orthogonal quaternion, multi-channel and fractional moments. New descriptors were found to be superior to the existing ones in terms of accuracy, stability, noise resistance, invariance to similarity transformations, recognition rates and computational times.","Color image descriptors, Pattern recognition, Rotation invariance, Fractional-order moments, Legendre-Fourier moments",Khalid M Hosny and Mohamed M Darwish and Tarek Aboelenen,https://www.sciencedirect.com/science/article/pii/S0031320320301278,https://doi.org/10.1016/j.patcog.2020.107324,0031-3203,2020,107324,103,Pattern Recognition,New fractional-order Legendre-Fourier moments for pattern recognition applications,article,HOSNY2020107324,
"The connectionist temporal classification (CTC) enables end-to-end sequence learning by maximizing the probability of correctly recognizing sequences during training. The outputs of a CTC-trained model tend to form a series of spikes separated by strongly predicted blanks, know as the spiky problem. To figure out the reason for it, we reinterpret the CTC training process as an iterative fitting task that is based on frame-wise cross-entropy loss. It offers us an intuitive way to compare target probabilities with model outputs for each iteration, and explain how the model outputs gradually turns spiky. Inspired by it, we put forward two ways to modify the CTC training. The experiments demonstrate that our method can well solve the spiky problem and moreover, lead to faster convergence over various training settings. Beside this, the reinterpretation of CTC, as a brand new perspective, may be potentially useful in other situations. The code is publicly available at https://github.com/hzli-ucas/caffe/tree/ctc.",Connectionist temporal classification (CTC),Hongzhu Li and Weiqiang Wang,https://www.sciencedirect.com/science/article/pii/S0031320320301953,https://doi.org/10.1016/j.patcog.2020.107392,0031-3203,2020,107392,105,Pattern Recognition,Reinterpreting CTC training as iterative fitting,article,LI2020107392,
"Video salient object detection is a challenging and important problem in computer vision domain. In recent years, deep-learning based methods have contributed to significant improvements in this domain. This paper provides an overview of recent developments in this domain and compares the corresponding methods up to date, including 1) Classification of the state-of-the-art methods and their frameworks; 2) summary of the benchmark datasets and commonly used evaluation metrics; 3) experimental comparison of the performances of the state-of-the-art methods; 4) suggestions of some promising future works for unsolved challenges.","Deep-learning, Salient object detection, Video",Qiong Wang and Lu Zhang and Yan Li and Kidiyo Kpalma,https://www.sciencedirect.com/science/article/pii/S0031320320301436,https://doi.org/10.1016/j.patcog.2020.107340,0031-3203,2020,107340,104,Pattern Recognition,Overview of deep-learning based methods for salient object detection in videos,article,WANG2020107340,
"Since the frames of a video are inherently contiguous, information redundancy is ubiquitous. Unlike previous works densely process each frame of a video, in this paper we present a novel method to focus on efficient feature propagation across frames to tackle the challenging video semantic segmentation task. Firstly, we propose a Light, Efficient and Real-time network (denoted as LERNet) as a strong backbone network for per-frame processing. Then we mine rich features within a key frame and propagate the across-frame consistency information by calculating a temporal holistic attention with the following non-key frame. Each element of the attention matrix represents the global correlation between pixels of a non-key frame and the previous key frame. Concretely, we propose a brand-new attention module to capture the spatial consistency on low-level features along temporal dimension. Then we employ the attention weights as a spatial transition guidance for directly generating high-level features of the current non-key frame from the weighted corresponding key frame. Finally, we efficiently fuse the hierarchical features of the non-key frame and obtain the final segmentation result. Extensive experiments on two popular datasets, i.e. the CityScapes and the CamVid, demonstrate that the proposed approach achieves a remarkable balance between inference speed and accuracy.","Real-time, Attention mechanism, Feature propagation, Video semantic segmentation",Junrong Wu and Zongzheng Wen and Sanyuan Zhao and Kele Huang,https://www.sciencedirect.com/science/article/pii/S003132032030073X,https://doi.org/10.1016/j.patcog.2020.107268,0031-3203,2020,107268,104,Pattern Recognition,Video semantic segmentation via feature propagation with holistic attention,article,WU2020107268,
"Deep Learning (DL) has provided powerful tools for visual information analysis. For example, Convolutional Neural Networks (CNNs) are excelling in complex and challenging image analysis tasks by extracting meaningful feature vectors with high discriminative power. However, these powerful feature vectors are crushed through the pooling layers of the network, that usually implement the pooling operation in a less sophisticated manner. This can lead to significant information loss, especially in cases where the informative content of the data is sequentially distributed over the spatial or temporal dimension, e.g., videos, which often require extracting fine-grained temporal information. A novel stateful recurrent pooling approach, that can overcome the aforementioned limitations, is proposed in this paper. The proposed method is inspired by the well-known Bag-of-Features (BoF) model, but employs a stateful trainable recurrent quantizer, instead of plain static quantization, allowing for efficiently processing sequential data and encoding both their temporal, as well as their spatial aspects. The effectiveness of the proposed Recurrent BoF model to enclose spatio-temporal information compared to other competitive methods is demonstrated using six different datasets and two different tasks.","Bag-of-Features, Recurrent neural networks, Pooling operators, Activity recognition",Marios Krestenitis and Nikolaos Passalis and Alexandros Iosifidis and Moncef Gabbouj and Anastasios Tefas,https://www.sciencedirect.com/science/article/pii/S0031320320301837,https://doi.org/10.1016/j.patcog.2020.107380,0031-3203,2020,107380,106,Pattern Recognition,Recurrent bag-of-features for visual information analysis,article,KRESTENITIS2020107380,
"Nowadays, prototype-based binary quantization (PBQ) is a promising solution for the approximate nearest neighbor search problem, which simultaneously preserves the affinity structures of prototypes in both Euclidean space as well as those of their codes in binary space. To learn longer binary codes, space decomposition based on product quantization is usually adopted. In practice, we find that the scale between Euclidean distance and Hamming distance usually varies across these decomposed subspaces, which degenerates the performance of PBQ based methods. We make an attempt to balance the scale of these subspaces via a joint optimization problem in the classic PBQ model, and present both an iterative and alternate algorithm for optimization. We conducted experiments on 6 public databases, and demonstrated that our scale balancing based methods SKMH and SABQ outperform state-of-the-art hashing methods including popular prototype-based binary quantization methods, with up to 81.62% relative performance gains when learning 256-bit binary codes.","Approximate nearest neighbor search, High-dimensional vectors, Prototype-based binary quantization",Zhiyang Li and Wenyu Qu and Yuan Cao and Heng Qi and Milos Stojmenovic and Jia Hu,https://www.sciencedirect.com/science/article/pii/S0031320320302120,https://doi.org/10.1016/j.patcog.2020.107409,0031-3203,2020,107409,106,Pattern Recognition,Scale balance for prototype-based binary quantization,article,LI2020107409,
"Face alignment has gained great popularity in computer vision due to its wide-spread applications. In this paper, we propose a novel learning architecture, i.e., heterogenous output regression network (HORNet), for face alignment, which directly predicts facial landmarks from images. HORNet is based on kernel approximations and establishes a new compact multi-layer architecture. A nonlinear layer with cosine activations disentangles nonlinear relationships between representations of images and shapes of facial landmarks. A linear layer with identity activations explicitly encodes landmark correlations by low-rank learning via matrix elastic nets. HORNet is highly flexible and can work either with pre-built feature representations or with convolutional architectures for end-to-end learning. HORNet leverages the strengths of both kernel methods in modeling nonlinearities and of neural networks in structural prediction. This combination renders it effective and efficient for direct face alignment. Extensive experiments on five in-the-wild datasets show that HORNet delivers high performance and consistently exceeds state-of-the-art methods.","Direct face alignment, Multi-output regression network, Random Fourier features",Xiantong Zhen and Mengyang Yu and Zehao Xiao and Lei Zhang and Ling Shao,https://www.sciencedirect.com/science/article/pii/S0031320320301151,https://doi.org/10.1016/j.patcog.2020.107311,0031-3203,2020,107311,105,Pattern Recognition,Heterogenous output regression network for direct face alignment,article,ZHEN2020107311,
"Electron tomographic reconstruction is a method for obtaining a three-dimensional image of a specimen with a series of two dimensional microscope images taken from different viewing angles. Filtered backprojection, one of the most popular tomographic reconstruction methods, does not work well under the existence of image noises and missing wedges. This paper presents a new approach to largely mitigate the effect of noises and missing wedges. We propose a novel filtered backprojection that optimizes the filter of the backprojection operator in terms of a reconstruction error. This data-dependent filter adaptively chooses the spectral domains of signals and noises, suppressing the noise frequency bands, so it is very effective in denoising. We also propose the new filtered backprojection embedded within the simultaneous iterative reconstruction iteration for mitigating the effect of missing wedges. Our numerical study is presented to show the performance gain of the proposed approach over the state-of-the-art.","Tomographic reconstruction, Filtered backprojection, Filter optimization, Filtered backprojection within SIRT",Chen Mu and Chiwoo Park,https://www.sciencedirect.com/science/article/pii/S0031320320300583,https://doi.org/10.1016/j.patcog.2020.107253,0031-3203,2020,107253,102,Pattern Recognition,Sparse filtered SIRT for electron tomography,article,MU2020107253,
"The promising achievement of sparse ranking in image-based recognition gives rise to a number of development on person re-identification (Re-ID) which aims to reconstruct the probe as a linear combination of few atoms/images from an over-complete dictionary/gallery. However, most of the existing sparse ranking based Re-ID methods lack considering the geometric relationships between probe, gallery, and cross-modal images of the same person in multi-shot Re-ID. In this paper, we propose a novel joint graph regularized dictionary learning and sparse ranking method for multi-modal multi-shot person Re-ID. First, we explore the probe-based geometrical structure by enforcing the smoothness between the codings/coefficients, which refers to the multi-shot images from the same person in probe. Second, we explore the gallery-based geometrical structure among gallery images, which encourages the multi-shot images from the same person in the gallery making similar contributions while reconstructing a certain probe image. Third, we explore the cross-modal geometrical structure by enforcing the smoothness between the cross-modal images and thus extend our model for the multi-modal case. Finally, we design an APG based optimization to solve the problem. Comprehensive experiments on benchmark datasets demonstrate the superior performance of the proposed model. The code is available at https://github.com/ttaalle/Lhc.","Person re-identification, Sparse ranking, Joint graph regularization",Aihua Zheng and Hongchao Li and Bo Jiang and Wei-Shi Zheng and Bin Luo,https://www.sciencedirect.com/science/article/pii/S0031320320301552,https://doi.org/10.1016/j.patcog.2020.107352,0031-3203,2020,107352,104,Pattern Recognition,Joint graph regularized dictionary learning and sparse ranking for multi-modal multi-shot person re-identification,article,ZHENG2020107352,
"Image corners have been widely used in various computer vision tasks. Current multi-scale analysis based corner detectors do not make full use of the multi-scale and multi-directional structural information. This degrades their detection accuracy and capability of refining corners. In this work, an improved shearlet transform with a flexible number of directions and a reasonable support is proposed to extract accurate multi-scale and multi-directional structural information from images. To make full use of the structural information from the improved shearlets, a novel multi-directional structure tensor is constructed for corner detection, and a multi-scale corner measurement function is proposed to remove false candidate corners. Experimental results demonstrate that the proposed corner detector performs better than existing corner and interest point detectors in terms of detection accuracy, localization accuracy, and robustness to affine transformations, illumination changes, noise, viewpoint changes, etc. It has a great potential for extension as a descriptor and for applications in computer vision tasks.","Shearlet transform, Multi-directional structure tensor, Corner detection",Mingzhe Wang and Weichuan Zhang and Changming Sun and Arcot Sowmya,https://www.sciencedirect.com/science/article/pii/S0031320320301035,https://doi.org/10.1016/j.patcog.2020.107299,0031-3203,2020,107299,103,Pattern Recognition,Corner detection based on shearlet transform and multi-directional structure tensor,article,WANG2020107299,
"Skeleton-based action recognition is an increasing attentioned task that analyses spatial configuration and temporal dynamics of a human action from skeleton data, which has been widely applied in intelligent video surveillance and human-computer interaction. How to design an effective framework to learn discriminative spatial and temporal characteristics for skeleton-based action recognition is still a challenging problem. The shape and motion representations of skeleton sequences are the direct embodiment of spatial and temporal characteristics respectively, which can well address for human action description. In this work, we propose an original unified framework to learn comprehensive shape and motion representations from skeleton sequences by using Geometric Algebra. We firstly construct skeleton sequence space as a subset of Geometric Algebra to represent each skeleton sequence along both the spatial and temporal dimensions. Then rotor-based view transformation method is proposed to eliminate the effect of viewpoint variation, which remains the relative spatio-temporal relations among skeleton frames in a sequence. We also construct spatio-temporal view invariant model (STVIM) to collectively integrate spatial configuration and temporal dynamics of skeleton joints and bones. In STVIM, skeleton sequence shape and motion representations which mutually compensate are jointly learned to describe skeleton-based actions comprehensively. Furthermore, a selected multi-stream Convolutional Neural Network is employed to extract and fuse deep features from mapping images of the learned representations for skeleton-based action recognition. Experimental results on NTU RGB+D, Northwestern-UCLA and UTD-MHAD datasets consistently verify the effectiveness of our proposed method and the superior performance over state-of-the-art competitors.","Human action recognition, Skeleton sequence, Representation learning, View invariant, Geometric Algebra",Yanshan Li and Rongjie Xia and Xing Liu,https://www.sciencedirect.com/science/article/pii/S0031320320300972,https://doi.org/10.1016/j.patcog.2020.107293,0031-3203,2020,107293,103,Pattern Recognition,Learning shape and motion representations for view invariant skeleton-based action recognition,article,LI2020107293,
"This paper details a novel optical flow-based structure from motion (SfM) approach for the reconstruction of surfaces with few textures using video sequences acquired under strong illumination changes. An original image search and grouping strategy allows to reconstruct each 3D scene point using a large set of 2D homologous points extracted from a reference image and its superimposed images acquired from different viewpoints. A variational optical flow scheme with a descriptor-based data term leads to a robust, accurate and dense homologous point determination between the image pairs. Thus, contrary to classical SfM usable for textured scenes, the proposed dense point cloud reconstruction algorithm requires neither a feature point tracking method nor any multi-view stereo technique. The performance of the proposed SfM approach is assessed on phantoms with known ground truth and on very complex patient data of various medical examinations and image modalities.","3D Image mosaicing, Structure-from-Motion (SfM), Dense optical flow, Endoscopy, Dermatology",Tan-Binh Phan and Dinh-Hoan Trinh and Didier Wolf and Christian Daul,https://www.sciencedirect.com/science/article/pii/S0031320320301941,https://doi.org/10.1016/j.patcog.2020.107391,0031-3203,2020,107391,105,Pattern Recognition,Optical flow-based structure-from-motion for the reconstruction of epithelial surfaces,article,PHAN2020107391,
"Existing tracking-by-detection approaches build trackers on binary classifiers. Despite achieving state-of-the-art performance on tracking benchmarks, these trackers pay limited attention to data imbalance issue, e.g, positive and negative, easy and hard. In this paper, we demonstrate that separately learning feature embeddings corresponding to negative samples with different semantic characteristics is effective in reducing the background diversity to handle the imbalance between positive and negative samples, which facilitates background awareness of classifiers. Specifically, we propose a negative sample embedding combination network, which helps to learn several sub-embeddings and combine them to build a robust classifier. In addition, we propose a weighted-gradient loss to handle the imbalance between easy and hard samples. The gradient contribution of each sample to model training is dynamically weighted according to the gradient distribution, which prevents easy samples from overwhelming model training. Extensive experiments on benchmarks demonstrate that our tracker performs favorably against state-of-the-art algorithms.","Visual tracking, Data imbalance, Embedding combination, Weighted-gradient loss",Jin Feng and Peng Xu and Shi Pu and Kaili Zhao and Honggang Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320301424,https://doi.org/10.1016/j.patcog.2020.107339,0031-3203,2020,107339,104,Pattern Recognition,Robust visual tracking by embedding combination and weighted-gradient optimization,article,FENG2020107339,
"The paper presents aÂ new theory of invariants to Gaussian blur. Unlike earlier methods, the blur kernel may be arbitrary oriented, scaled and elongated. Such blurring is aÂ semi-group action in the image space, where the orbits are classes of blur-equivalent images. We propose aÂ non-linear projection operator which extracts blur-insensitive component of the image. The invariants are then formally defined as moments of this component but can be computed directly from the blurred image without an explicit construction of the projections. Image description by the new invariants does not require any prior knowledge of the blur kernel parameters and does not include any deconvolution. The invariance property could be extended also to linear transformation of the image coordinates and combined affine-blur invariants can be constructed. Experimental comparison to three other blur-invariant methods is given. Potential applications of the new invariants are in blur/position invariant image recognition and in robust template matching.","Gaussian blur, Semi-group, Projection operator, Blur invariants, Image moments, Affine transformation, Combined invariants",Jitka KostkovÃ¡ and Jan Flusser and MatÄj LÃ©bl and Matteo Pedone,https://www.sciencedirect.com/science/article/pii/S0031320320300698,https://doi.org/10.1016/j.patcog.2020.107264,0031-3203,2020,107264,103,Pattern Recognition,Handling Gaussian blur without deconvolution,article,KOSTKOVA2020107264,
"The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected.","Binary neural network, Deep learning, Model compression, Network quantization, Model acceleration",Haotong Qin and Ruihao Gong and Xianglong Liu and Xiao Bai and Jingkuan Song and Nicu Sebe,https://www.sciencedirect.com/science/article/pii/S0031320320300856,https://doi.org/10.1016/j.patcog.2020.107281,0031-3203,2020,107281,105,Pattern Recognition,Binary neural networks: A survey,article,QIN2020107281,
"Electroencephalography (EEG) topographical representation (ETR) can monitor regional brain activities and is emerging as a successful technique for causally exploring cortical mechanisms and connections. However, it is a challenge to find a robust method supporting high-dimensional EEG data with low signal-to-noise ratios from multiple objects and multiple channels. To address this issue, a new ETR energy calculation method for learning the EEG patterns of brain activities using a convolutional neural network is reported. It is able to customize temporal ETR training and recognize multiple objects within a common learning model. Specifically, an open-access dataset from the 2008 Brain-Computer Interface (BCI) Competition IV-2a is used for classification of five classes containing four Motor Imagery actions and one relax action. The proposed classification framework outperforms the best state-of-the-art classification method by 10.11% in average subject accuracy. Furthermore, by studying the ETR parameter optimization, a user interface for BCI applications is obtained and a real-time method implemented.","Motor imagery, Electroencephalography topographical representation, Convolutional neural network, Machine learning, Signal pre-processing",Meiyan Xu and Junfeng Yao and Zhihong Zhang and Rui Li and Baorong Yang and Chunyan Li and Jun Li and Junsong Zhang,https://www.sciencedirect.com/science/article/pii/S003132032030193X,https://doi.org/10.1016/j.patcog.2020.107390,0031-3203,2020,107390,105,Pattern Recognition,Learning EEG topographical representation for classification via convolutional neural network,article,XU2020107390,
"In this paper, an effective novel approach for dimensionality reduction of labeled proportional data is proposed. By avoiding formulating an eigenvalue problem and constructing a neighborhood graph, the introduced method mitigates some of the major problems from which the well-known algorithms in this category suffer. These disadvantages include problem handling multi-modal or sparse data as well as curse of dimensionality. The devised method transfers the data from high-dimensional space into low-dimensional space using a linear transform which is optimized using an information theoretic measure. To find this projection, a novel approach has been adopted in which projected data are transfered into the low-dimensional space first, and a mixture of distributions is estimated using the projected data for each class separately. In the next step, the distance between the estimated distributions is used as a measure of separation for data classes, and a heuristic search is carried on to find the optimal projection. The effectiveness of the proposed algorithm is demonstrated using different datasets in different scenarios in comparison with other well-known algorithms in the literature.","Dimensionality reduction, Feature extraction",Walid Masoudimansour and Nizar Bouguila,https://www.sciencedirect.com/science/article/pii/S0031320320301825,https://doi.org/10.1016/j.patcog.2020.107379,0031-3203,2020,107379,105,Pattern Recognition,Supervised dimensionality reduction of proportional data using mixture estimation,article,MASOUDIMANSOUR2020107379,
"Large-scale datasets play a fundamental role in training deep learning models. However, dataset collection is difficult in domains that involve sensitive information. Collaborative learning techniques provide a privacy-preserving solution, by enabling training over a number of private datasets that are not shared by their owners. However, recently, it has been shown that the existing collaborative learning frameworks are vulnerable to an active adversary that runs a generative adversarial network (GAN) attack. In this work, we propose a novel classification model that is resilient against such attacks by design. More specifically, we introduce a key-based classification model and a principled training scheme that protects class scores by using class-specific private keys, which effectively hide the information necessary for a GAN attack. We additionally show how to utilize high dimensional keys to improve the robustness against attacks without increasing the model complexity. Our detailed experiments demonstrate the effectiveness of the proposed technique. Source code will be made available at https://github.com/mbsariyildiz/key-protected-classification.","Privacy-preserving machine learning, collaborative learning, classification, generative adversarial networks",Mert Bulent Sariyildiz and Ramazan Gokberk Cinbis and Erman Ayday,https://www.sciencedirect.com/science/article/pii/S0031320320301308,https://doi.org/10.1016/j.patcog.2020.107327,0031-3203,2020,107327,104,Pattern Recognition,Key protected classification for collaborative learning,article,SARIYILDIZ2020107327,
"Cross-modal retrieval aims to realize accurate and flexible retrieval across different modalities of data, e.g., image and text, which has achieved significant progress in recent years, especially since generative adversarial networks (GAN) were used. However, there still exists much room for improvement. How to jointly extract and utilize both the modality-specific (complementarity) and modality-shared (correlation) features effectively has not been well studied. In this paper, we propose an approach named Modality-Specific and Shared Generative Adversarial Network (MS2GAN) for cross-modal retrieval. The network architecture consists of two sub-networks that aim to learn modality-specific features for each modality, followed by a common sub-network that aims to learn the modality-shared features for each modality. Network training is guided by the adversarial scheme between the generative and discriminative models. The generative model learns to predict the semantic labels of features, model the inter- and intra-modal similarity with label information, and ensure the difference between the modality-specific and modality-shared features, while the discriminative model learns to classify the modality of features. The learned modality-specific and shared feature representations are jointly used for retrieval. Experiments on three widely used benchmark multi-modal datasets demonstrate that MS2GAN can outperform state-of-the-art related works.","Cross-modal retrieval, Generative adversarial networks (GAN), Modality-specific feature learning, Modality-shared feature learning",Fei Wu and Xiao-Yuan Jing and Zhiyong Wu and Yimu Ji and Xiwei Dong and Xiaokai Luo and Qinghua Huang and Ruchuan Wang,https://www.sciencedirect.com/science/article/pii/S0031320320301382,https://doi.org/10.1016/j.patcog.2020.107335,0031-3203,2020,107335,104,Pattern Recognition,Modality-specific and shared generative adversarial network for cross-modal retrieval,article,WU2020107335,
"Image-text matching has drawn much attention recently with the rapid growth of multi-modal data. Many effective approaches have been proposed to solve this challenging problem, but limited effort has been devoted to re-ranking methods. Compared with the uni-modal re-ranking methods, modality heterogeneity is the major difficulty when designing a re-ranking method in the cross-modal field, which mainly lies in two aspects of different visual and textual feature spaces and different distributions in inverse directions. In this paper, we propose a heuristic re-ranking method called Adaptive Metric Fusion (AMF) for image-text matching. The method can obtain a better metric by adaptively fusing metrics based on two modules: 1) Cross-modal Reciprocal Encoding, which considers ranks in inverse directions to comprehensively evaluate a metric. The sentence retrieval and image retrieval have different distribution characteristics and galleries in different modalities, thus it is necessary to exploit them simultaneously for appropriate metric fusion. 2) Query Replacement Gap, which quantifies the gap between cross-modal and uni-modal similarities to alleviate the influence of different visual and textual feature spaces on the fused metric. The proposed re-ranking method can be implemented in an unsupervised way without requiring any human interaction or annotated data, and can be easily applied to any initial ranking result. Extensive experiments and analysis validate the effectiveness of our method on the large-scale MS-COCO and Flickr30K datasets.","Image-text matching, Re-ranking method, Adaptive metric fusion",Kai Niu and Yan Huang and Liang Wang,https://www.sciencedirect.com/science/article/pii/S0031320320301540,https://doi.org/10.1016/j.patcog.2020.107351,0031-3203,2020,107351,104,Pattern Recognition,Re-ranking image-text matching by adaptive metric fusion,article,NIU2020107351,
"To improve on the robustness of traditional machine learning approaches, emphasis has recently shifted to the integration of such methods with Deep Learning techniques. However, the classification problems, complexity and inconsistency in several spectral classifiers developed for hyperspectral images are some reasons warranting further research. This study investigates the application of Deep Support Vector Machine (DSVM) for hyperspectral image classification. Two hyperspectral images, Indian Pines and University of Pavia are used as tentative test beds for the experiment. The DSVM is implemented with four kernel functions: Exponential Radial Basis Function (ERBF), Gaussian Radial Basis Function (GRBF), neural and polynomial. Stand-alone Support Vector Machines form the interconnecting weights of the entire network. The network is trained with one hundred input datasets, and the interconnecting weights of the network are initialised using the regularisation parameter of the model. Numerical results show that the classification accuracies of the DSVM for Indian Pines and University of Pavia based on each DSVM kernel functions are: ERBF (98.87%, 98.16%), GRBF (98.90%, 98.47%), neural (98.41%, 97.27%), and polynomial (99.24%, 98.79%). By comparing the DSVM algorithm against well-known classifiers, Support Vector Machine (SVM), Deep Neural Network (DNN), Gaussian Mixture Model (GMM), K Nearest Neighbour (KNN), and K Means (KM) classifiers, the mean classification accuracies for Indian Pines and University of Pavia are: DSVM (98.86%, 98.17%), SVM (76.03%, 73.52%), DNN (94.45%, 93.79%), GMM (76.82%, 78.35%), KNN (76.87%, 78.80%), and KM (21.65%, 18.18%). These results indicate that the DSVM outperformed the other classification algorithms. The high accuracy obtained with the DSVM validates its efficacy as state-of-the-art algorithm for hyperspectral image classification.","Remote sensing, Hyperspectral image, Deep support vector machine, Image classification",Onuwa Okwuashi and Christopher E. Ndehedehe,https://www.sciencedirect.com/science/article/pii/S0031320320301023,https://doi.org/10.1016/j.patcog.2020.107298,0031-3203,2020,107298,103,Pattern Recognition,Deep support vector machine for hyperspectral image classification,article,OKWUASHI2020107298,
"Unsupervised feature selection methods try to select features which can well preserve the intrinsic structure of data. To represent such structure, conventional methods construct various graphs from data. In most cases, those different graphs often contain some consensus and complementary information. To make full use of such information, we construct multiple base graphs and learn an adaptive consensus graph from these base graphs for feature selection. In our method, we integrate the multiple graph learning and the feature selection into a unified framework, which can jointly characterize the structure of the data and select the features to preserve such structure. The underlying optimization problem is hard to solve, and we solve it via a block coordinate descent schema, whose convergence is guaranteed. The extensive experiments well demonstrate the effectiveness of our proposed framework.","Feature selection, Multiple graph learning, Consensus learning",Peng Zhou and Liang Du and Xuejun Li and Yi-Dong Shen and Yuhua Qian,https://www.sciencedirect.com/science/article/pii/S0031320320301783,https://doi.org/10.1016/j.patcog.2020.107375,0031-3203,2020,107375,105,Pattern Recognition,Unsupervised feature selection with adaptive multiple graph learning,article,ZHOU2020107375,
"Conventional minutia-based fingerprint recognition requires a complicated geometric matching and hard to be adopted in the bit-string based cancellable biometrics or bio-encryption, as the minutia data representing a fingerprint image is geometrical, unordered and variable in size. In this paper, we propose a new method to represent a fingerprint image by an ordered and fixed-length bit-string to cope with those difficulties with providing a faster matching, compressibility and improved accuracy performance as well. Firstly, we devised a novel minutia-based local structure modeled by a mixture of 2D elliptical Gaussian functions to represent a minutia in the image pixel space. Then, each local structure was mapped to a point in a Euclidean space by normalizing the local structure by the number of minutiae in it. This simple yet crucial computation for converting the image space to the Euclidean-space enabled the fast dissimilarity computation of two local structures and all followed processes in our proposed method. A complementary texture-based local structure to the minutia-based local structure was also introduced, whereby both were compressed via principal component analysis and fused in the compressed Euclidean space. The fused local structures were then converted to a K-bit ordered string using the K-means clustering algorithm. This chain of computations with the sole use of Euclidean distance was vital for speedy and discriminative bit-string conversion. The accuracy was further improved by the finger-specific bit-training algorithm, in which two criteria were leveraged to select the useful bit positions for matching. Experiments were performed on Fingerprint Verification Competition (FVC) databases for comparisons with the existing techniques to show the superiority of the proposed method.","Bit-oriented, Fingerprint, Clustering, Fixed-length bit-string representation, Bit-training, Minutiae-based local structure",Jun Beom Kho and Andrew B.J. Teoh and Wonjune Lee and Jaihie Kim,https://www.sciencedirect.com/science/article/pii/S0031320320301266,https://doi.org/10.1016/j.patcog.2020.107323,0031-3203,2020,107323,103,Pattern Recognition,Bit-string representation of a fingerprint image by normalized local structures,article,KHO2020107323,
"Developing tools to understand and visualize lifestyle is of high interest when addressing the improvement of habits and well-being of people. Routine, defined as the usual things that a person does daily, helps describe the individualsâ lifestyle. With this paper, we are the first ones to address the development of novel tools for automatic discovery of routine days of an individual from his/her egocentric images. In the proposed model, sequences of images are firstly characterized by semantic labels detected by pre-trained CNNs. Then, these features are organized in temporal-semantic documents to later be embedded into a topic models space. Finally, Dynamic-Time-Warping and Spectral-Clustering methods are used for final day routine/non-routine discrimination. Moreover, we introduce a new EgoRoutine-dataset, a collection of 104 egocentric days with more than 100.000 images recorded by 7 users. Results show that routine can be discovered and behavioural patterns can be observed.","Routine, Egocentric vision, Lifestyle, Behaviour analysis, Topic modelling",Estefania Talavera and Carolin Wuerich and Nicolai Petkov and Petia Radeva,https://www.sciencedirect.com/science/article/pii/S0031320320301333,https://doi.org/10.1016/j.patcog.2020.107330,0031-3203,2020,107330,104,Pattern Recognition,Topic modelling for routine discovery from egocentric photo-streams,article,TALAVERA2020107330,
"An u-shapelet is a sub-sequence of a time series used for the clustering of time series datasets. The purpose of this paper is to discover u-shapelets on uncertain time series. To achieve this goal, we propose a dissimilarity score called FOTS whose computation is based on the eigenvector decomposition and the comparison of the autocorrelation matrices of the time series. This score is robust to the presence of uncertainty; it is not very sensitive to transient changes; it allows capturing complex relationships between time series such as oscillations and trends, and it is also well adapted to the comparison of short time series. The FOTS score is used with the Scalable Unsupervised Shapelet Discovery algorithm for the clustering of 63 datasets, and it has shown a substantial improvement in the quality of the clustering with respect to the Rand Index. This work defines a novel framework for the clustering of uncertain time series.","Clustering, UShapelet, Correlation, Time series",Vanel Steve {Siyou Fotso} and Engelbert {Mephu Nguifo} and Philippe Vaslin,https://www.sciencedirect.com/science/article/pii/S0031320320301059,https://doi.org/10.1016/j.patcog.2020.107301,0031-3203,2020,107301,103,Pattern Recognition,Frobenius correlation based u-shapelets discovery for time series clustering,article,SIYOUFOTSO2020107301,
"Detection and localization of abnormal behaviors in surveillance videos of crowded scenes is challenging, where high-density people and various objects performing highly unpredictable motions lead to severe occlusions, making object segmentation and tracking extremely difficult. We associate the optical flows between multiple frames to capture short-term trajectories and introduce the histogram-based shape descriptor to describe such short-term trajectories, which reflects faithfully the motion trend and details in local patches. Furthermore, we propose a method to detect anomalies over time and space by judging whether the similarities between the testing sample and the retrieved K-NN samples follow the pattern distribution of homogeneous intra-class similarities, which is unsupervised one-class learning requiring no clustering nor prior assumption. Such a scheme can adapt to the whole scene, since the probability is used to judge and the calculation of probability is not affected by motion distortions arising from perspective distortion, which gains advantage over the existing solutions. We conduct experiments on real-world surveillance videos, and the results demonstrate that the proposed method can reliably detect and locate the abnormal events in video sequences, outperforming the state-of-the-art approaches.","Abnormal activity, Anomaly detection, Anomaly localization, Shape description, -NN similarity-based outlier detection",Xinfeng Zhang and Su Yang and Jiulong Zhang and Weishan Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320301977,https://doi.org/10.1016/j.patcog.2020.107394,0031-3203,2020,107394,105,Pattern Recognition,Video anomaly detection and localization using motion-field shape description and homogeneity testing,article,ZHANG2020107394,
"Dense indoor scene modeling from 2D images has been bottlenecked due to the absence of depth information and cluttered occlusions. We present an automatic indoor scene modeling approach using deep features from neural networks. Given a single RGB image, our method simultaneously recovers semantic contents, 3D geometry and object relationship by reasoning indoor environment context. Particularly, we design a shallow-to-deep architecture on the basis of convolutional networks for semantic scene understanding and modeling. It involves multi-level convolutional networks to parse indoor semantics/geometry into non-relational and relational knowledge. Non-relational knowledge extracted from shallow-end networks (e.g. room layout, object geometry) is fed forward into deeper levels to parse relational semantics (e.g. support relationship). A Relation Network is proposed to infer the support relationship between objects. All the structured semantics and geometry above are assembled to guide a global optimization for 3D scene modeling. Qualitative and quantitative analysis demonstrates the feasibility of our method in understanding and modeling semantics-enriched indoor scenes by evaluating the performance of reconstruction accuracy, computation performance and scene complexity.","Scene understanding, Image-based modeling, Semantic modeling, Relational reasoning",Yinyu Nie and Shihui Guo and Jian Chang and Xiaoguang Han and Jiahui Huang and Shi-Min Hu and Jian Jun Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320300765,https://doi.org/10.1016/j.patcog.2020.107271,0031-3203,2020,107271,103,Pattern Recognition,Shallow2Deep: Indoor scene modeling by single image understanding,article,NIE2020107271,
"Zero Shot Learning (ZSL) aims to learn projective functions on labeled seen data and transfer the learned functions to unseen classes by discovering their relationship with semantic embeddings. However, the mapping process often suffers from the domain shift problem caused by only using the labeled seen data. In this paper, we propose a novel explainable Deep Transductive Network (DTN) for the task of Generalized ZSL (GZSL) by training on both labeled seen data and unlabeled unseen data, with subsequent testing on both seen classes and unseen classes. The proposed network exploits a KL Divergence constraint to iteratively refine the probability of classifying unlabeled instances by learning from their high confidence assignments with the assistance of an auxiliary target distribution. Besides, to avoid the meaningless ascription assumption of unseen data on GZSL, we also propose an experimental paradigm by splitting the unseen data into two equivalent parts for training and testing respectively. Extensive experiments and detailed analysis demonstrate that our DTN can efficiently handle the problems and achieve the state-of-the-art performance on four popular datasets.","Generalized zero shot learning (GZSL), Transductive ZSL, KL Divergence, Deep transductive network (DTN)",Haofeng Zhang and Li Liu and Yang Long and Zheng Zhang and Ling Shao,https://www.sciencedirect.com/science/article/pii/S0031320320301734,https://doi.org/10.1016/j.patcog.2020.107370,0031-3203,2020,107370,105,Pattern Recognition,Deep transductive network for generalized zero shot learning,article,ZHANG2020107370,
"Biclustering can be defined as the simultaneous clustering of rows and columns in a data matrix and it has been recently applied to many scientific scenarios such as bioinformatics, text analysis and computer vision to name a few. In this paper we propose a novel biclustering approach, that is based on the concept of dominant-set clustering and extends such algorithm to the biclustering problem. In more detail, we propose a novel encoding of the biclustering problem as a graph so to use the dominant set concept to analyse rows and columns simultaneously. Moreover, we extend the Dominant Set Biclustering approach to facilitate the insertion of prior knowledge that may be available on the domain. We evaluated the proposed approach on a synthetic benchmark and on two computer vision tasks: multiple structure recovery and region-based correspondence. The empirical evaluation shows that the method achieves promising results that are comparable to the state-of-the-art and that outperforms competitors in various cases.","Biclustering, Dominant set, Replicator dynamics, Prior knowledge",M. Denitto and M. Bicego and A. Farinelli and S. Vascon and M. Pelillo,https://www.sciencedirect.com/science/article/pii/S0031320320301217,https://doi.org/10.1016/j.patcog.2020.107318,0031-3203,2020,107318,104,Pattern Recognition,Biclustering with dominant sets,article,DENITTO2020107318,
"Conditional Random Fields (CRFs) have been widely adopted in conjunction with Fully Convolutional Networks (FCNs) to model and integrate contextual information in the semantic segmentation procedure. In contrast to existing approaches applying CRFs in parallel or in cascade with FCNs, we propose a new paradigm to incorporate CRFs deeper inside the architecture of FCNs to model the context exhibited within the middle layers of an FCN. We approximate the mean-field inference process of a dense CRF as a multi-dimensional Gated Recurrent Unit (GRU) layer, termed CRF-GRU layer, effectively extracting intermediate context within an FCN. More importantly, multiple CRF-GRU layers can be injected into an FCN to model hierarchical contexts presented in multiple middle layers, showing competitive results on the PASCAL VOC 2012 and PASCAL-Context datasets. Secondly, we contribute a new approach to automatically learn, from the training data, the optimal segmentation architecture of the FCN with multiple CRF-GRU layers injected. The proposed approach relies on Genetic Evolution Strategies to allow the existing architecture to iteratively evolve towards higher accuracy instances. The discovered network not only outperforms state-of-the-art segmentation techniques, but also provides exciting new insights into the design of the segmentation networks.","Semantic segmentation, Context modeling, Network evolution, Conditional random field, Probabilistic graphical models",Kien Nguyen and Clinton Fookes and Sridha Sridharan,https://www.sciencedirect.com/science/article/pii/S0031320320301618,https://doi.org/10.1016/j.patcog.2020.107358,0031-3203,2020,107358,105,Pattern Recognition,Context from within: Hierarchical context modeling for semantic segmentation,article,NGUYEN2020107358,
"Image saliency is determined by spatial semantic features, while video saliency is affected by multiple factors such as spatial and temporal information. Since human eyes stay extremely short on each frame, the dynamic salient area is more focused and concentrated on one salient object. In order to better simulate the human visual attention mechanism in dynamic scenes, we propose a key salient object re-augmentation method (KSORA) based on the guidance of both bottom-up weighted features and top-down semantic knowledge. The bottom-up feature weighting strategy effectively eliminates noisy and redundancy, and provide accurate local spatiotemporal features for saliency inference. The top-down key object enhancement strategy ranks salient candidates based on global statistical knowledge, so as to explicitly enhance the saliency proportion of the key object. The fusion of the local weighted spatiotemporal features and the global key object augmentation features not only ensures spatiotemporal consistency, but also facilitates obtaining more concentrated salient prediction. Results on three large datasets validate that our proposed method has the capability of improving the detection accuracy in complex scenes.","Video saliency detection, Ranking saliency, Semantical guidance",Zheng Wang and Ziqi Zhou and Huchuan Lu and Jianmin Jiang,https://www.sciencedirect.com/science/article/pii/S0031320320300807,https://doi.org/10.1016/j.patcog.2020.107275,0031-3203,2020,107275,103,Pattern Recognition,Global and local sensitivity guided key salient object re-augmentation for video saliency detection,article,WANG2020107275,
"Decision trees are commonly used for learning and extracting classification rules from data. The fuzzy rule based decision tree (FRDT) is very representative owing to its better robustness and generalization. However, FRDT cannot work well on the analysis of large-scale data sets. One solution for this problem is parallel computing. A proved effective parallel computing model is Map-Reduce. Ensemble learning is an effective strategy which can significantly improve the generalization ability of machine learning systems. The objective of this paper is to develop a fuzzy rule-base based decision tree on the strategies of parallel computing and ensemble learning. First, we implement a parallel fusing fuzzy rule based classification system via Map-Reduce (MR-FFRCS) to display how to extract fuzzy rules from data in parallel and how to evaluate the fuzzy rules in an ensemble learning way. Then, taking MR-FFRCS as a fundamental module, we propose a parallel fuzzy rule-base based decision tree (MR-FRBDT) to improve the original FRDT algorithm. The experimental studies mainly focus on feasibility and parallelism. Compared with FRDT on 23 UCI benchmark data sets, the proposed MR-FRBDT algorithm with fewer parameters is effective and has the ability to handle large-scale data sets. Furthermore, some numerical experiments conducted on several large-scale data sets verify the parallel performance on reducing computing time and avoiding memory restrictions.","Parallel computing, Fuzzy classifier, Decision trees, Fuzzy rules, Map-Reduce",Yashuang Mu and Xiaodong Liu and Lidong Wang and Juxiang Zhou,https://www.sciencedirect.com/science/article/pii/S0031320320301291,https://doi.org/10.1016/j.patcog.2020.107326,0031-3203,2020,107326,103,Pattern Recognition,A parallel fuzzy rule-base based decision tree in the framework of map-reduce,article,MU2020107326,
"Most existing sparse representation-based (SR) fusion methods consider the local information of each image patch independently during fusion. Some spatial artifacts are easily introduced to the fused image. A sliding window technology is often employed by these methods to overcome this issue. However, this comes at the cost of high computational complexity. Alternatively, we come up with a novel multi-focus image fusion method that takes full consideration of the strong correlations among spatially adjacent image patches with NO need for a sliding window. To this end, a non-negative SR model with local consistency constraint (CNNSR) on the representation coefficients is first constructed to encode each image patch. Then a patch-level consistency rectification strategy is presented to merge the input image patches, by which the spatial artifacts in the fused images are greatly reduced. As well, a compact non-negative dictionary is constructed for the CNNSR model. Experimental results demonstrate that the proposed fusion method outperforms some state-of-the art methods. Moreover, the proposed method is computationally efficient, thereby facilitating real-world applications.","Multi-focus image fusion, Non-negative sparse representation, Compact non-negative dictionary construction, Patch-level consistency rectification, High computational efficiency",Qiang Zhang and Guanghe Li and Yunfeng Cao and Jungong Han,https://www.sciencedirect.com/science/article/pii/S003132032030128X,https://doi.org/10.1016/j.patcog.2020.107325,0031-3203,2020,107325,104,Pattern Recognition,Multi-focus image fusion based on non-negative sparse representation and patch-level consistency rectification,article,ZHANG2020107325,
"Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned problem. We observe that the scaling based weight space symmetry (SBWSS) in rectified nonlinear network will cause this negative effect. Therefore, we propose to constrain the incoming weights of each neuron to be unit-norm, which is formulated as an optimization problem over the Oblique manifold. A simple yet efficient method referred to as projection based weight normalization (PBWN) is also developed to solve this problem. This proposed method has the property of regularization and collaborates well with the commonly used batch normalization technique. We conduct comprehensive experiments on several widely-used image datasets including CIFAR-10, CIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art neural networks. The experimental results show that our method is able to improve the performance of different architectures consistently. We also apply our method to Ladder network for semi-supervised learning on permutation invariant MNIST dataset, and our method achievers the state-of-the-art methods: we obtain test errors as 2.52%, 1.06%, and 0.91% with only 20, 50, and 100 labeled samples, respectively.","Deep learning, Weight normalization, Oblique manifold, Image classification",Lei Huang and Xianglong Liu and Jie Qin and Fan Zhu and Li Liu and Ling Shao,https://www.sciencedirect.com/science/article/pii/S0031320320301175,https://doi.org/10.1016/j.patcog.2020.107317,0031-3203,2020,107317,105,Pattern Recognition,Projection based weight normalization: Efficient method for optimization on oblique manifold in DNNs,article,HUANG2020107317,
"Matrix factorization has been utilized for the task of cross-view hashing, where basis functions are learned to map data from different views to the same hamming embedding. It is possible that the basis functions between the hamming embedding and the original data matrix contain rather complex hierarchical information, which existing work can not capture. In addition, previous work employs relaxation technique in the matrix factorization based hashing which may lead to large quantization error. To address these issues, this paper presents a novel Supervised Discrete Deep Matrix Factorization (SDDMF) for cross-view hashing. We introduce deep matrix factorization so that SDDMF is able to learn a set of hierarchical basis functions and unified binary codes from different views. In addition, a classification error term is incorporated into the objective to learn discriminative binary codes. We then employ a linearization technique to directly optimize the discrete constraints which can significantly reduce the quantization error. Experimental results on three standard datasets with image-text modalities verify that SDDMF significantly outperforms several state-of-the-art methods.","Matrix factorization, Cross-view hashing, Similarity search",Yingjun Xiong and Yan Xu and Xin Shu,https://www.sciencedirect.com/science/article/pii/S0031320320300753,https://doi.org/10.1016/j.patcog.2020.107270,0031-3203,2020,107270,103,Pattern Recognition,Cross-view hashing via supervised deep discrete matrix factorization,article,XIONG2020107270,
"The paper addresses two issues relative to machine learning on 2DÂ +Â X data volumes, where 2D refers to image observation and X denotes a variable that can be associated with time, depth, wavelength, etc. The first issue addressed is conditioning these structured volumes for compatibility with respect to convolutional neural networks operating on 2D image file formats. The second issue is associated with sensitive action detection in the â2DÂ +Â Timeâ case (video clips and image time series). For the data conditioning issue, the paper first highlights that referring 2D spatial convolution to its 1D Hilbert based instance is highly accurate for information compressibility upon tight frames of convolutional networks. As a consequence of this compressibility, the paper proposes converting the 2DÂ +Â X data volume into a single meta-image file format, prior to machine learning frameworks. This conversion is such that any 2D frame of the 2DÂ +Â X data is reshaped as a 1D array indexed by a Hilbert space-filling curve and the third variable X of the initial file format becomes the second variable in the meta-image format. For the sensitive action recognition issue, the paper provides: (i) a 3 category video database involving non-violent, moderate and extreme violence actions; (ii) the conversion of this database into a timed meta-image database from the 2DÂ +Â Time to 2D conditioning stage described above and (iii) outstanding 2-level and 3-level violence classification results from deep convolutional neural networks operating on meta-image databases.","Data conditioning, Video analysis, Deep learning, Convolution frames, Hilbert space-filling curve, Action recognition, Violence detection",Abdourrahmane Mahamane Atto and Alexandre Benoit and Patrick Lambert,https://www.sciencedirect.com/science/article/pii/S0031320320301564,https://doi.org/10.1016/j.patcog.2020.107353,0031-3203,2020,107353,104,Pattern Recognition,Timed-image based deep learning for action recognition in video sequences,article,ATTO2020107353,
"Image matching aims to find a similar area of the small image in the large image, which is one of the key steps in image fusion and vision-based navigation; however, most matching methods perform poorly when the images to be matched are blurred. Traditional approaches for blurred image matching usually follow a two-stage framework - first resorting to image deblurring and then performing image matching with the recovered image. However, the matching accuracy of these methods often suffers greatly from the deficiency of image deblurring. Recently, a joint image deblurring and matching method that utilizes the sparse representation prior to exploit the correlation between deblurring and matching was proposed to address this problem and found to obtain a higher matching accuracy. Yet, that technique is not efficient when the image is seriously blurred, and the methodâs time complexity is excessive. In this paper, we propose a joint image deblurring and matching approach with a feature-based sparse representation prior. Our approach utilizes two-directional two-dimensional (2D)2PCA to extract feature vectors from images and obtains a sparse representation prior in a robust feature space rather than the original pixel space, thus mitigating the influence of image blur. Moreover, the reduction in the feature dimension can also increase the computational efficiency. Extensive experiments show that our approach significantly outperforms state-of-the-art approaches in terms of both accuracy and speed.","Blurred image matching, Joint image deblurring and matching, Sparse representation priorsparse, (2)PCA feature",Juncai Peng and Yuanjie Shao and Nong Sang and Changxin Gao,https://www.sciencedirect.com/science/article/pii/S0031320320301047,https://doi.org/10.1016/j.patcog.2020.107300,0031-3203,2020,107300,103,Pattern Recognition,Joint image deblurring and matching with feature-based sparse representation prior,article,PENG2020107300,
"We study the problem of clustering validation, i.e., clustering evaluation without knowledge of ground-truth labels, for the increasingly-popular framework known as subspace clustering. Existing clustering quality metrics (CQMs) rely heavily on a notion of distance between points, but common metrics fail to capture the geometry of subspace clustering. We propose a novel point-to-point pseudometric for points lying on a union of subspaces and show how this allows for the application of existing CQMs to the subspace clustering problem. We provide theoretical and empirical justification for the proposed point-to-point distance, and then demonstrate on a number of common benchmark datasets that our proposed methods generally outperform existing graph-based CQMs in terms of choosing the best clustering and the number of clusters.","Subspace clustering, Clustering validation, Union of subspaces",John Lipor and Laura Balzano,https://www.sciencedirect.com/science/article/pii/S003132032030131X,https://doi.org/10.1016/j.patcog.2020.107328,0031-3203,2020,107328,104,Pattern Recognition,Clustering quality metrics for subspace clustering,article,LIPOR2020107328,
"We develop a Learning Direct Optimization (LiDO) method for the refinement of a latent variable model that describes input image x. Our goal is to explain a single image x with an interpretable 3D computer graphics model having scene graph latent variables z (such as object appearance, camera position). Given a current estimate of z we can render a prediction of the image g(z), which can be compared to the image x. The standard way to proceed is then to measure the error E(x, g(z)) between the two, and use an optimizer to minimize the error. However, it is unknown which error measure E would be most effective for simultaneously addressing issues such as misaligned objects, occlusions, textures, etc. In contrast, the LiDO approach trains a Prediction Network to predict an update directly to correct z, rather than minimizing the error with respect to z. Experiments show that LiDO converges rapidly as it does not need to perform a search on the error landscape, produces better solutions than error-based competitors, and is able to handle the mismatch between the data and the fitted scene model. We apply LiDO to a realistic synthetic dataset, and show that the method also transfers to work well with real images.","Computer vision, Scene understanding, 3D Reconstruction, Inverse graphics, Object recognition, Scene graph, Analysis-by-synthesis, Graphics",Lukasz Romaszko and Christopher K.I. Williams and John Winn,https://www.sciencedirect.com/science/article/pii/S0031320320301722,https://doi.org/10.1016/j.patcog.2020.107369,0031-3203,2020,107369,105,Pattern Recognition,Learning Direct Optimization for scene understanding,article,ROMASZKO2020107369,
"Head pose estimation plays a vital role in various applications, e.g., driver-assistance systems, human-computer interaction, virtual reality technology, and so on. We propose a novel geometry-based method for accurately estimating the head pose from a single 2D face image at a very low computational cost. Specifically, the rectangular coordinates of only four non-coplanar feature points from a predefined 3D facial model as well as the corresponding ones automatically/manually extracted from a 2D face image are first normalized to exclude the effect of external factors (i.e., scale factor and translation parameters). Then, the four normalized 3D feature points are represented in spherical coordinates with reference to the uniquely determined sphere by themselves. Due to the spherical parametrization, the coordinates of feature points can then be morphed along all the three directions in the rectangular coordinates effectively. Finally, the rotation matrix indicating the head pose is obtained by minimizing the Euclidean distance between the normalized 2D feature points and the 2D re-projections of the morphed 3D feature points. Comprehensive experimental results over two popular datasets, i.e., Pointingâ04 and Biwi Kinect, demonstrate that the proposed method can estimate head poses with higher accuracy and lower run time than state-of-the-art geometry-based methods. Even compared with start-of-the-art learning-based methods or geometry-based methods with additional depth information, our method still produces comparable performance.","Head pose estimation, Spherical parameterization, 3D facial model",Hui Yuan and Mengyu Li and Junhui Hou and Jimin Xiao,https://www.sciencedirect.com/science/article/pii/S0031320320301205,https://doi.org/10.1016/j.patcog.2020.107316,0031-3203,2020,107316,103,Pattern Recognition,Single image-based head pose estimation with spherical parametrization and 3D morphing,article,YUAN2020107316,
"Recently, electroencephalogram (EEG) signal presents a great potential for a new biometric system to deal with a cognitive task. Several studies defined the EEG with uniqueness features, universality, and natural robustness that can be used as a new track to prevent spoofing attacks. The EEG signals are the graphical recording of the brain electrical activities which can be measured by placing electrodes (channels) in various positions of the scalp. With a large number of channels, some channels have very important information for biometric system while others not. The channel selection problem has been recently formulated as an optimisation problem and solved by optimisation techniques. This paper proposes hybrid optimisation techniques based on binary flower pollination algorithm (FPA) and Î²-hill climbing (called FPAÎ²-hc) for selecting the most relative EEG channels (i.e., features) that come up with efficient accuracy rate of personal identification. Each EEG signals with three different groups of EEG channels have been utilized (i.e., time domain, frequency domain, and time-frequency domain). The FPAÎ²-hc is measured using a standard EEG signal dataset, namely, EEG motor movement/imagery dataset with a real world data taken from 109 persons each with 14 different cognitive tasks using 64 channels. To evaluate the performance of the FPAÎ²-hc, five measurement criteria are considered:accuracy (Acc), (ii) sensitivity (Sen), (iii) F-score (F_s), (v) specificity (Spe), and (iv) number of channels selected (No. Ch). The proposed method is able to identify the personals with high Acc, Sen., F_s, Spe, and less number of channels selected. Interestingly, the experimental results suggest that FPAÎ²-hc is able to reduce the number of channels with accuracy rate up to 96% using time-frequency domain features. For comparative evaluation, the proposed method is able to achieve results better than those produced by binary-FPA-OPF method using the same EEG motor movement/imagery datasets. In a nutshell, the proposed method can be very beneficial for effective use of EEG signals in biometric applications.","EEG, Biometric, Channel selection, Flower pollination algorithm, -hill climbing",Zaid Abdi Alkareem Alyasseri and Ahamad Tajudin Khader and Mohammed Azmi Al-Betar and Osama Ahmad Alomari,https://www.sciencedirect.com/science/article/pii/S0031320320301965,https://doi.org/10.1016/j.patcog.2020.107393,0031-3203,2020,107393,105,Pattern Recognition,Person identification using EEG channel selection with hybrid flower pollination algorithm,article,ALYASSERI2020107393,
"Although Convolutional Neural Networks (CNNs) have made substantial improvements in many computer vision tasks, there remains room for improvements in image-based action recognition due to the limited capability to exploit the body structure information.In this work, we propose a unified deep model to explicitly explore body structure information and fuse multiple body structure cues for robust action recognition in images.In order to fully explore the body structure information, we design the Body Structure Exploration sub-network.It generates two novel body structure cues, Structural Body Parts and Limb Angle Descriptor, which capture structure information of human bodies from the global and local perspectives respectively. And then, we design the Action Classification sub-network to fuse the predictions from multiple body structure cues to obtain precise results. Moreover, we integrate the two sub-networks into a unified model by sharing the bottom convolutional layers, which improves the computational efficiency in both training and testing stages. We comprehensively evaluate our network on the challenging image-based human action datasets, Pascal VOC 2012 Action and Stanford40. Our approach achieves 93.5% and 93.8% mAP respectively, which outperforms all recent approaches in this field.","Image-based action recognition, Convolutional neural network, Body structure cues",Yang Li and Kan Li and Xinxin Wang,https://www.sciencedirect.com/science/article/pii/S0031320320301448,https://doi.org/10.1016/j.patcog.2020.107341,0031-3203,2020,107341,104,Pattern Recognition,Recognizing actions in images by fusing multiple body structure cues,article,LI2020107341,
"Research on brain biometrics using electroencephalographic (EEG) signals has received increasing attentions in recent years. In particular, it has been recognized that the brain functional connectivity reflects individual variability. However, many questions need to be answered before we can properly use distinctive characteristics of brain connectivity for biometric applications. This paper proposes a graph-based method for EEG biometric identification. It consists of a network estimation module to generate brain connectivity networks and a graph analysis module to generate topological features based on brain networks. Specifically, we investigate seven different connectivity metrics for the network estimation module, each of which is characterized by a certain signal interaction mechanism, defining a peculiar subjective brain network. A new connectivity metric is proposed based on the algorithmic complexity of EEG signals from a information-theoretic perspective. Meanwhile, six nodal features and six global features are proposed and studied for the graph analysis module. A comprehensive evaluation is carried out to assess the impact of different connectivity metrics, graph features, and EEG frequency bands on biometric identification performance. The results demonstrate that the graph-based method proposed in this study is effective in improving the recognition rate and inter-state stability of EEG-based biometric identification systems. Our findings about the network patterns and graph features bring a further understanding of distinctiveness of humansâ EEG functional connectivity and provide useful guidance for the design of graph-based EEG biometric systems.","EEG biometrics, Brain functional connectivity, Person identification",Min Wang and Jiankun Hu and Hussein A. Abbass,https://www.sciencedirect.com/science/article/pii/S0031320320301849,https://doi.org/10.1016/j.patcog.2020.107381,0031-3203,2020,107381,105,Pattern Recognition,BrainPrint: EEG biometric identification based on analyzing brain connectivity graphs,article,WANG2020107381,
"In this paper, we introduce a method of exploring temporal information for estimating human poses in videos. The current state-of-the-art methods utilizing temporal information can be categorized into two major branches. The first category is a model-based method that captures the temporal information entirely by using a learnable function such as RNN or 3D convolution. However, these methods are limited in exploring temporal consistency, which is essential for estimating human joint positions in videos. The second category is the posterior enhancement method, where an independent post-processing step (e.g., using optical flow) is applied to enhance the prediction. However, operations such as optical flow estimation can be susceptible to the occlusion and motion blur problems, which will adversely affect the final performance. We propose a novel Temporal Consistency Exploration (TCE) module to address both shortcomings. Compared to previous approaches, the TCE module is more efficient as it captures the temporal consistency at the feature level without having to post-process and calculate extra optical flow. Further, to capture the rich spatial context in video data, we design a multi-scale TCE to explore the time consistency information at multi-scale spatial levels. Finally, a video-based pose estimation network is designed, which is based on the encoder-decoder architecture and extended with the powerful multi-scale TCE module. We comprehensively evaluate the proposed model on two video datasets, Sub-JHMDB and Penn, and our model achieves state-of-the-art performance on both datasets.","Video-based pose estimation, Convolution neural network, Temporal information",Yang Li and Kan Li and Xinxin Wang and Richard Yi Da Xu,https://www.sciencedirect.com/science/article/pii/S0031320320300637,https://doi.org/10.1016/j.patcog.2020.107258,0031-3203,2020,107258,103,Pattern Recognition,Exploring temporal consistency for human pose estimation in videos,article,LI2020107258,
"Sparsity preserving projection (SPP), as a widely used linear unsupervised dimensionality reduction (DR) method, is designed to preserve the sparse reconstructive relationship of the raw data. SPP constructs an affinity weight matrix by solving a sparse representation model which does not need any parameters. Moreover, the obtained projection may contain some discriminating information even if no prior knowledge is provided. Although SPP may be more conveniently used in practice due to these advantages, it still suffers from the so-called small-sample-size problem as may other DR methods do. To solve this problem, we propose an exponential sparsity preserving projection (ESPP) by using matrix exponential, and present two efficiently numerical methods for solving the corresponding large-scale matrix exponential eigenvalue problem. ESPP avoids the singularity of the coefficient matrices, and obtains more valuable information for the SPP. Image recognition experiments are conducted on several real-world image databases and the experimental results illustrate the outperformances of ESPP.","Sparsity preserving projection, Dimensionality reduction, Small-sample-size problem, Matrix exponential, Image recognition",Wei Wei and Hua Dai and Wei-tai Liang,https://www.sciencedirect.com/science/article/pii/S0031320320301606,https://doi.org/10.1016/j.patcog.2020.107357,0031-3203,2020,107357,104,Pattern Recognition,Exponential sparsity preserving projection with applications to image recognition,article,WEI2020107357,
"The current deep learning based spatio-temporal action localization methods that using motion information (predominated is optical flow) obtain the state-of-the-art performance. However, since the optical flow is pre-computed, leading to these methods face two problems â the computational efficiency is low and the whole network is not end-to-end trainable. We propose a novel spatio-temporal action localization approach with an integrated optical flow sub-network to address these two issues. Specifically, our designed flow subnet can estimate optical flow efficiently and accurately by using multiple consecutive RGB frames rather than two adjacent frames in a deep network, simultaneously, action localization is implemented in the same network interactive with flow computation end-to-end. To faster the speed, we exploit a neural network based feature fusion method in a pyramid hierarchical manner. It fuses spatial and temporal features at different granularities via combination function (i.e. concatenation) and point-wise convolution to obtain multiscale spatio-temporal action features. Experimental results on three publicly available datasets, e.g. UCF101-24, JHMDB and AVA show that with both RGB appearance and optical flow cues, the proposed method gets the state-of-the-art performance in both efficiency and accuracy. Noticeably, it gets a significant improvement on efficiency. Compared to the currently most efficient method, it is 1.9 times faster in the running speed and 1.3% video-mAP more accurate on the UCF101-24. Our proposed method reaches real-time computation for the first time (up to 38 FPS).","Spatio-Temporal Action Localization, Real-time Computation, Optical Flow Sub-network, Pyramid Hierarchical Fusion",Dejun Zhang and Linchao He and Zhigang Tu and Shifu Zhang and Fei Han and Boxiong Yang,https://www.sciencedirect.com/science/article/pii/S0031320320301163,https://doi.org/10.1016/j.patcog.2020.107312,0031-3203,2020,107312,103,Pattern Recognition,Learning motion representation for real-time spatio-temporal action localization,article,ZHANG2020107312,
"Many convolutional neural network (CNN)-based approaches for stereoscopic salient object detection involve fusing either low-level or high-level features from the color and disparity channels. The former method generally produces incomplete objects, whereas the latter tends to blur object boundaries. In this paper, a coupled CNN (CoCNN) is proposed to fuse color and disparity features from low to high layers in a unified deep model. It consists of three parts: two parallel multilinear span networks, a cascaded span network and a conditional random field module. We first apply the multilinear span network to compute multiscale saliency predictions based on RGB and disparity individually. Each prediction, learned under separate supervision, utilizes the multilevel features extracted by the multilinear span network. Second, a proposed cascaded span network, under deep supervision, is designed as a coupling unit to fuse the two feature streams at each scale and integrate all fused features in a supervised manner to construct a saliency map. Finally, we formulate a constraint in the form of a conditional random field model to refine the saliency map based on the a priori assumption that objects with similar saliency values have similar colors and disparities. Experiments conducted on two commonly used datasets demonstrate that the proposed method outperforms previous state-of-the-art methods.","Coupled CNN, Cascaded span network, Stereoscopic images, Salient object detection",Fangfang Liang and Lijuan Duan and Wei Ma and Yuanhua Qiao and Zhi Cai and Jun Miao and Qixiang Ye,https://www.sciencedirect.com/science/article/pii/S0031320320301321,https://doi.org/10.1016/j.patcog.2020.107329,0031-3203,2020,107329,104,Pattern Recognition,CoCNN: RGB-D deep fusion for stereoscopic salient object detection,article,LIANG2020107329,
"As a popular living fingerprint feature, sweat pore has been adopted to build robust high resolution automated fingerprint recognition systems (AFRSs). Pore matching is an important step in high resolution fingerprint recognition. This paper proposes a novel pore matching method with high recognition accuracy. The method mainly solves the pore representation problem in the state-of-the-art direct pore matching method. By making full use of the diversity and large quantities of sweat pores on fingerprints, deep convolutional networks are carefully designed to learn a deep feature (denoted as DeepPoreID) for each pore. The inter-class difference and intra-class similarity of pore patch pairs can be well solved using deep learning. The DeepPoreID is then used to describe the local feature for each pore and finally integrated into the classical direct pore matching method. More specifically, pore patches, which are cropped from both Query and Template fingerprint images, are imported into the well-trained networks to generate DeepPoreID for pore representation. The similarity between those DeepPoreIDs are then obtained by calculating the Euclidian Distance between them. Subsequently, one-to-many coarse pore correspondences are established via comparing their similarity. Finally, classical Weighted RANdom SAmple Consensus (WRANSAC) is employed to pick true pore correspondences from coarse ones. The experiments carried on the two public high resolution fingerprint database have shown the effectiveness of the proposed DeepPoreID, especially for fingerprint matching with small image size. Meanwhile, better recognition accuracy is achieved by the proposed method when compared with the existing state-of-the-art methods. About 35% rise in equal error rate (EER) and about 30% rise in FMR1000 when compared with the best result evaluated on the database with image size of 320Â ÃÂ 240Â pixels.","Fingerprint recognition, Pore representation, Direct pore matching, Convolutional neural networks",Feng Liu and Yuanhao Zhao and Guojie Liu and Linlin Shen,https://www.sciencedirect.com/science/article/pii/S0031320320300145,https://doi.org/10.1016/j.patcog.2020.107208,0031-3203,2020,107208,102,Pattern Recognition,Fingerprint pore matching using deep features,article,LIU2020107208,
"In real world, most of the tracking methods suffer from uncertain motion, which may make the tracker failure because of a local search window with the motion smooth assumption. To address this problem, a novel tracking framework based on convolutional net with semantics estimation and region proposals is proposed. Firstly, we present a semantics object proposals generation strategy, including category-level semantics proposals, one-object-level semantics estimation and semantics-contextual proposals generation, to obtain a few of high-quality object-oriented proposals covering uncertain motion. Secondly, combining the globally sparse semantics region proposals prediction and correlation filter prediction, a hybrid semantics tracking algorithm is proposed, which obtains a coarse object location by the decision of multiple response maps. Finally, we learn and train independent correlation filter to estimate the scale of target for a higher tracking accuracy. Extensive experiments on two visual tracking benchmarks and results demonstrate our method achieves state-of-the-art performance.","Correlation filter, Semantics estimation, Visual tracking, Region proposals, Contextual information",Huanlong Zhang and Jian Chen and Guohao Nie and Shiqiang Hu,https://www.sciencedirect.com/science/article/pii/S0031320320300388,https://doi.org/10.1016/j.patcog.2020.107232,0031-3203,2020,107232,102,Pattern Recognition,Uncertain motion tracking based on convolutional net with semantics estimation and region proposals,article,ZHANG2020107232,
"Existing video question answering methods answer given questions based on short video snippets. The underlying assumption is that the visual content indicating the ground truth answer ubiquitously exists in the snippet. It might be problematic for long video applications, since involving large numbers of answer-irrelevant snippets will dramatically degenerate the performance. To deal with this issue, we focus on a rarely investigated but practically important problem, namely long video QA, by predicting answers directly from long videos rather than manually pre-extracted short video snippets. We accordingly propose a Matching-guided Attention Model (MAM) which jointly extracts question-related video snippets and predicts answers in a unified framework. To localize questions accurately and efficiently, we calculate corresponding matching scores and boundary regression results for candidate video snippet proposals generated by sliding windows of limited granularity. Guided by the matching scores, the model pays different attention to the extracted video snippet proposals for each question. Finally, we use the attended visual features along with the question to predict the answer in a classification manner. A key obstacle to training our model is that publicly available video QA datasets only contain short videos especially designed for short video QA. Thus, we generate two new datasets for this task on the top of TACoS Multi-level dataset and MSR-VTT dataset by generating QA pairs from the video captions, called TACoS-QA and MSR-VTT-QA. Experimental results show the effectiveness of our proposed method on both datasets by comparing with two short video QA methods and a baseline method.","Long video QA, Matching-guided attention",Weining Wang and Yan Huang and Liang Wang,https://www.sciencedirect.com/science/article/pii/S0031320320300546,https://doi.org/10.1016/j.patcog.2020.107248,0031-3203,2020,107248,102,Pattern Recognition,Long video question answering: A Matching-guided Attention Model,article,WANG2020107248,
"Recent visual object tracking methods have witnessed a continuous improvement in the state-of-the-art with the development of efficient discriminative correlation filters (DCF) and robust deep neural network features. Despite the outstanding performance achieved by the above combination, existing advanced trackers suffer from the burden of high computational complexity of the deep feature extraction and online model learning. We propose an accelerated ADMM optimisation method obtained by adding a momentum to the optimisation sequence iterates, and by relaxing the impact of the error between DCF parameters and their norm. The proposed optimisation method is applied to an innovative formulation of the DCF design, which seeks the most discriminative spatially regularised feature channels. A further speed up is achieved by an adaptive initialisation of the filter optimisation process. The significantly increased convergence of the DCF filter is demonstrated by establishing the optimisation process equivalence with a continuous dynamical system for which the convergence properties can readily be derived. The experimental results obtained on several well-known benchmarking datasets demonstrate the efficiency and robustness of the proposed ACFT method, with a tracking accuracy comparable to the start-of-the-art trackers.","Visual object tracking, Discriminative correlation filters, Accelerated optimisation, Alternating direction method of multipliers",Tianyang Xu and Zhen-Hua Feng and Xiao-Jun Wu and Josef Kittler,https://www.sciencedirect.com/science/article/pii/S0031320319304728,https://doi.org/10.1016/j.patcog.2019.107172,0031-3203,2020,107172,102,Pattern Recognition,An accelerated correlation filter tracker,article,XU2020107172,
"Generative models can be used for a wide range of tasks, and have the appealing ability to learn from both labelled and unlabelled data. In contrast, discriminative models cannot learn from unlabelled data, but tend to outperform their generative counterparts in supervised tasks. We develop a framework to jointly train deep generative and discriminative models, enjoying the benefits of both. The framework allows models to learn from labelled and unlabelled data, as well as naturally account for uncertainty in predictive distributions, providing the first Bayesian approach to semi-supervised learning with deep generative models. We demonstrate that our blended discriminative and generative models outperform purely generative models in both predictive performance and uncertainty calibration in a number of semi-supervised learning tasks.","Probabilistic models, Semi-supervised learning, Variational autoencoders, Predictive uncertainty",Jonathan Gordon and JosÃ© Miguel HernÃ¡ndez-Lobato,https://www.sciencedirect.com/science/article/pii/S003132031930456X,https://doi.org/10.1016/j.patcog.2019.107156,0031-3203,2020,107156,100,Pattern Recognition,Combining deep generative and discriminative models for Bayesian semi-supervised learning,article,GORDON2020107156,
"Nowadays, there is a huge amount of Historical Arabic Documents (HAD) in the national libraries and archives around the world. Analyzing this type of data manually is a difficult and costly task. Thus, an automatic process is required to exploit these documents more rapidly. Processing historical documents is a recent research subject that has seen a remarkable growth in the last years. Processing Historical Arabic Documents is a particularly challenging problem. First, due to complicated nature of Arabic script compared to other scripts and second because the documents are ancient. This paper focuses on this difficult problem and provides a comprehensive survey of existing research work. First, we describe in detail the challenges making the automatic processing of Historical Arabic Documents a difficult task. Second, we classify this task into four applications of automatic processing of HAD: i) Analyze the document to extract the main text ii) Identify the writer of the document iii) Recognize some words or parts of the document in a reference dataset andiv) Retrieve and extract specific data from the document. For each application, existing approaches are surveyed and qualitatively described. Finally, we focus on available datasets and describe how they can be used in each application.","Historical Arabic Documents, Writer identification, Data retrieval, Text analysis, Text recognition, Survey on Historical Arabic Documents",Mohamed {Ibn Khedher} and Houda Jmila and Mounim A. El-Yacoubi,https://www.sciencedirect.com/science/article/pii/S0031320319304455,https://doi.org/10.1016/j.patcog.2019.107144,0031-3203,2020,107144,100,Pattern Recognition,Automatic processing of Historical Arabic Documents: A comprehensive Survey,article,IBNKHEDHER2020107144,
"Existing deep neural network based image super-resolution (SR) methods are mostly designed for non-blind cases, where the blur kernel used to generate the low-resolution (LR) images is assumed to be known and fixed. However, this assumption does not hold in many real scenarios. Motivated by the observation that SR of LR images generated by different blur kernels are essentially different but also correlated, we propose a mixture model of deep networks, which is capable of clustering SR tasks of different blur kernels into a set of groups. Each group is composed of correlated SR tasks with similar blur kernels and can be effectively handled by a combination of specific networks in the mixture model. To achieve automatic SR tasks clustering and network selection, we model the blur kernel with a latent variable, which is inferred from the input image by an encoder network. Since the ground-truth of the latent variable is unknown in the training stage, we initialize the encoder network by pre-training it on the blur kernel classification task to avoid trivial solutions. To jointly train the mixture model and the encoder network, we further derive a lower bound of the likelihood function, which circumvents the intractability in direct maximum likelihood estimation. Extensive evaluations are performed on benchmark data sets and validate the effectiveness of the proposed method.","Blind super-resolution, Mixture of networks, Blur kernels, Lower bound, Latent variables",Yifan Wang and Lijun Wang and Hongyu Wang and Peihua Li and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320319304698,https://doi.org/10.1016/j.patcog.2019.107169,0031-3203,2020,107169,102,Pattern Recognition,Blind single image super-resolution with a mixture of deep networks,article,WANG2020107169,
"Cylinder detection is an important step in reverse engineering of industrial sites, as such environments often contain a large number of cylindrical pipes and tanks. However, existing techniques for cylinder detection require the specification of several parameters which are difficult to adjust because their values depend on the noise level of the input point cloud. Also, these solutions often expect the cylinders to be either parallel or perpendicular to the ground. We present a cylinder-detection technique that is robust to noise, contains parameters which require little to no fine-tuning, and can handle cylinders with arbitrary orientations. Our approach is based on a robust linear-time circle-detection algorithm that naturally discards outliers, allowing our technique to handle datasets with various density and noise levels while using a set of default parameter values. It works by projecting the point cloud onto a set of directions over the unit hemisphere and detecting circular projections formed by samples defining connected components in 3D. The extracted cylindrical surfaces are obtained by fitting a cylinder to each connected component. We compared our technique against the state-of-the-art methods on both synthetic and real datasets containing various densities and noise levels, and show that it outperforms existing techniques in terms of accuracy and robustness to noise, while still maintaining a competitive running time.","Cylinder detection, Unorganized point clouds, Reverse engineering, Industrial sites",Abner M.C. AraÃºjo and Manuel M. Oliveira,https://www.sciencedirect.com/science/article/pii/S0031320319304613,https://doi.org/10.1016/j.patcog.2019.107161,0031-3203,2020,107161,100,Pattern Recognition,Connectivity-based cylinder detection in unorganized point clouds,article,ARAUJO2020107161,
"Structural spatial relations between image components are fundamental in the human perception of image similarity, and constitute a challenging topic in the domain of image analysis. By definition, some specific relations are ambiguous and difficult to formalize precisely by humans. In this work, we deal with the issue of evaluating complex spatial configurations, where objects can surround each other, potentially with multiple levels of depth. Based on a recently introduced spatial relation called enlacement, which generalizes the idea of surrounding for arbitrary objects, we propose a fuzzy landscape model that allows both to visualize and evaluate this relation directly in the image space, following different directions. Experiments on several characteristic examples highlight the interest and the behavior of this approach, allowing for rich interpretations of these complex spatial configurations.","Evaluation of spatial relations, Fuzzy directional landscapes, Enlacement, Surrounding",MichaÃ«l ClÃ©ment and Camille Kurtz and Laurent Wendling,https://www.sciencedirect.com/science/article/pii/S0031320319304856,https://doi.org/10.1016/j.patcog.2019.107185,0031-3203,2020,107185,101,Pattern Recognition,Fuzzy directional enlacement landscapes for the evaluation of complex spatial relations,article,CLEMENT2020107185,
"Most Correlation Filter (CF)-based tracking methods can hardly handle occlusion or severe deformation, due to the lack of effective utilization of previous target information. To overcome this, we propose a novel Transfer Learning-based Discriminative Correlation Filter (TLDCF), which extracts knowledge from multiple previous tracking tasks and applies the knowledge for a new tracking task through Instance-Transfer Learning (ITL) and Probability-Transfer Learning (PTL). ITL applies knowledge of Gaussian Mixture Modelling (GMM) target representations and multi-channel filters learned in previous frames to directly train a new correlation filter. This improves the robustness of tracker for heavy occlusion and large appearance variations. Meanwhile, PTL encodes the spatio-temporal relationship predicted by Kalman Filter (KF) into a shared Gaussian prior to suppress huge location drift caused by similar targets. For optimization, we develop an efficient Alternating Direction Method of Multipliers (ADMM) based algorithm to calculate CFs on each independent channel in real time. Extensive experiments on OTB-2013 and OTB-2015 datasets well demonstrate the effectiveness of the proposed method. In particular, our method improves AUC score of the two datasets by 5.5% and 3.9% respectively compared to baseline, and achieves competitive performance against recent state-of-the-art deep trackers.","Visual tracking, Discriminative correlation filter, Instance-Transfer, Probability-Transfer",Bo Huang and Tingfa Xu and Jianan Li and Ziyi Shen and Yiwen Chen,https://www.sciencedirect.com/science/article/pii/S0031320319304571,https://doi.org/10.1016/j.patcog.2019.107157,0031-3203,2020,107157,100,Pattern Recognition,Transfer learning-based discriminative correlation filter for visual tracking,article,HUANG2020107157,
"Person search aims to locate the target person matching a given query from a list of unconstrained whole images. It is a challenging task due to the unavailable bounding boxes of pedestrians, limited samples for each labeled identity and large amount of unlabeled persons in existing datasets. To address these issues, we propose a novel end-to-end learning framework for person search. The proposed framework settles pedestrian detection and person re-identification concurrently. To achieve the goal of co-learning and utilize the information of unlabeled persons, a novel yet extremely efficient Dynamic Imposter based Online Instance Matching (DI-OIM) loss is formulated. The DI-OIM loss is inspired by the observation that pedestrians appearing in the same image obviously have different identities. Thus we assign the unlabeled persons with dynamic pseudo-labels. The pseudo-labeled persons along with the labeled persons can be used to learn powerful feature representations. Experiments on CUHK-SYSU and PRW datasets demonstrate that our method outperforms other state-of-the-art algorithms. Moreover, it is superior and efficient in terms of memory capacity comparing with existing methods.","Person search, Pedestrian detection, Person re-identification, Dynamic pseudo-label",Ju Dai and Pingping Zhang and Huchuan Lu and Hongyu Wang,https://www.sciencedirect.com/science/article/pii/S0031320319304212,https://doi.org/10.1016/j.patcog.2019.107120,0031-3203,2020,107120,100,Pattern Recognition,Dynamic imposter based online instance matching for person search,article,DAI2020107120,
"Associative classifiers are one of the most efficient classifiers for large datasets. However, they are unsuitable to be directly used in large-scale data problems. Associative classifiers discover frequent/rare rules or both in order to produce an efficient classifier. Discovery rules need to explore a large solution space in a well-organized manner; hence, learning of the associative classification methods of large datasets is not suitable on large-scale datasets because of memory and time-complexity constraints. The proposed method, CARs-Lands, presents an efficient distributed associative classifier. In CARs-Lands, first, a modified dataset is generated. This new dataset has sub-datasets that are completely appropriate to produce classification association rules (CARs) in a parallel manner. The produced dataset by CARs-Lands contains two types of instances: main instances and neighbor instances. Main instances can be either real instances of training dataset or meta-instances, which are not in the training dataset; each main instance has several neighbor instances from the training dataset, which together form a sub-dataset. These sub-datasets are used for parallel local association rule mining. In CARs-Lands, local association rules lead to more accurate prediction, because each test instance is classified by the association rules of their nearest neighbors in the training datasets. The proposed approach is evaluated in terms of accuracy on six real-world large-scale datasets against five recent and well-known methods. Experiment results show that the proposed classification method has high prediction accuracy and is highly competitive when compared to other classification methods.","Classification association rules (CARs), Associative classifier, Big data, Large-scale datasets, Evolutionary algorithms",Mehrdad Almasi and Mohammad {Saniee Abadeh},https://www.sciencedirect.com/science/article/pii/S0031320319304297,https://doi.org/10.1016/j.patcog.2019.107128,0031-3203,2020,107128,100,Pattern Recognition,CARs-Lands: An associative classifier for large-scale datasets,article,ALMASI2020107128,
"Synthesizing talking face from text and audio is increasingly becoming a direction in human-machine and face-to-face interactions. Although progress has been made, several existing methods either have unsatisfactory co-articulation modeling effects or ignore relations between adjacent inputs. Moreover, some of these methods often train models on shaky head videos or utilize linear-based face parameterization strategies, which further decrease synthesized quality. To address the above issues, this study proposes a sequence-to-sequence convolutional neural network to automatically synthesize talking face video with accurate lip sync. First, an advanced landmark location pipeline is used to accurately locate the facial landmarks, which can effectively reduce landmark shake. Then, a part-based autoencoder is presented to encode face images into a low-dimensional space and obtain compact representations. A sequence-to-sequence network is also presented to encode the relation of neighboring frames with multiple loss functions, and talking faces are synthesized through a reconstruction strategy with a decoder. Experiments on two public audio-visual datasets and a new dataset called CCTV news demonstrate the effectiveness of the proposed method against other state-of-the-art methods.","Convolutional neural network, Autoencoder, Regression, Face landmark, Face tracking, Lip sync, Video, Audio",Na Liu and Tao Zhou and Yunfeng Ji and Ziyi Zhao and Lihong Wan,https://www.sciencedirect.com/science/article/pii/S0031320320300376,https://doi.org/10.1016/j.patcog.2020.107231,0031-3203,2020,107231,102,Pattern Recognition,Synthesizing Talking Faces from Text and Audio: An Autoencoder and Sequence-to-Sequence Convolutional Neural Network,article,LIU2020107231,
"Alzheimer's disease (AD) is an irreversible and progressive neurodegenerative disease. The close AD monitoring of this disease is essential for the patient treatment plan adjustment. For AD monitoring, clinical score prediction via neuroimaging data is highly desirable since it is able to reveal the disease status, adequately. For this task, most previous studies are focused on a single time point without considering relationship between neuroimaging data (e.g., Magnetic Resonance Imaging (MRI)) and clinical scores at multiple time points. Differing from these studies, we propose to build a framework based on longitudinal multiple time points data to predict clinical scores. Specifically, the proposed framework consists of three parts, feature selection based on correntropy regularized joint learning, feature encoding based on deep polynomial network, and ensemble learning for regression via the support vector regression method. Two scenarios are designed for scores prediction. Namely, scenario 1 uses the baseline data to achieve the longitudinal scores prediction, while scenario 2 utilizes all the previous time points data to obtain the predicted scores at the next time point, which can improve the score prediction's accuracy. Meanwhile, the missing clinical scores at longitudinal multiple time points are imputated to solve the incompleteness of the data. Extensive experiments on the public database of Alzheimer's Disease Neuroimaging Initiative (ADNI) demonstrate that our proposed framework can effectively reveal the relationship between clinical score and MRI data and outperforms the state-of-the-art methods in scores prediction.","Alzheimer's disease, Longitudinal scores prediction, Joint learning, Correntropy, Deep polynomial network",Baiying Lei and Mengya Yang and Peng Yang and Feng Zhou and Wen Hou and Wenbin Zou and Xia Li and Tianfu Wang and Xiaohua Xiao and Shuqiang Wang,https://www.sciencedirect.com/science/article/pii/S0031320320300534,https://doi.org/10.1016/j.patcog.2020.107247,0031-3203,2020,107247,102,Pattern Recognition,Deep and joint learning of longitudinal data for Alzheimer's disease prediction,article,LEI2020107247,
"Deep neural networks and Multiple Kernel Learning are representation learning methodologies of widespread use and increasing success. While the former aims at learning representations through a hierarchy of features of increasing complexity, the latter provides a principled approach for the combination of base representations. In this paper, we introduce a general framework in which the internal representations computed by a deep neural network are optimally combined by means of Multiple Kernel Learning. The resulting ensemble methodology is instantiated for Multi-layer Perceptrons architectures (both fully trained and with random-weights), and for Convolutional Neural Networks. Experimental results on several benchmark datasets concretely show the advantages and potentialities of the proposed approach.","Deep neural networks, Deep learning, Multiple kernel learning, Ensemble learning",Ivano Lauriola and Claudio Gallicchio and Fabio Aiolli,https://www.sciencedirect.com/science/article/pii/S0031320320300017,https://doi.org/10.1016/j.patcog.2020.107194,0031-3203,2020,107194,101,Pattern Recognition,Enhancing deep neural networks via multiple kernel learning,article,LAURIOLA2020107194,
"Deformable brain MR image registration is challenging due to large inter-subject anatomical variation. For example, the highly complex cortical folding pattern makes it hard to accurately align corresponding cortical structures of individual images. In this paper, we propose a novel deep learning way to simplify the difficult registration problem of brain MR images. Specifically, we train a morphological simplification network (MS-Net), which can generate a simple image with less anatomical details based on the complex input. With MS-Net, the complexity of the fixed image or the moving image under registration can be reduced gradually, thus building an individual (simplification) trajectory represented by MS-Net outputs. Since the generated images at the ends of the two trajectories (of the fixed and moving images) are so simple and very similar in appearance, they are easy to register. Thus, the two trajectories can act as a bridge to link the fixed and the moving images, and guide their registration. Our experiments show that the proposed method can achieve highly accurate registration performance on different datasets (i.e., NIREP, LPBA, IBSR, CUMC, and MGH). Moreover, the method can be also easily transferred across diverse image datasets and obtain superior accuracy on surface alignment. We propose MS-Net as a powerful and flexible tool to simplify brain MR images and their registration. To our knowledge, this is the first work to simplify brain MR image registration by deep learning, instead of estimating deformation field directly.","Deformable image registration, Deep learning, Anatomical complexity",Dongming Wei and Lichi Zhang and Zhengwang Wu and Xiaohuan Cao and Gang Li and Dinggang Shen and Qian Wang,https://www.sciencedirect.com/science/article/pii/S0031320319304716,https://doi.org/10.1016/j.patcog.2019.107171,0031-3203,2020,107171,100,Pattern Recognition,Deep morphological simplification network (MS-Net) for guided registration of brain magnetic resonance images,article,WEI2020107171,
"Reliable and fast traffic sign recognition (TSR) that locates the traffic sign from an image and then estimates its category is a crucial perception function for Advanced Driver Assistance Systems (ADAS) of autonomous vehicles. Most of the popular deep convolutional neural networks (DCNNs) based TSR techniques advocate discriminative feature learning for traffic signs against their appearance variability. However, such feature learning scheme may suffer from the diversity of traffic signs categories, especially when samples within each category are limited for model training (i.e., few-shot learning). Here, we present a generative feature learning based TSR network with well generalization capacity and high computational efficiency. Instead of relying on large amounts of supervision to learn discriminative features, our method devotes to learn common but unique properties of class-specific traffic signs with few training samples. Specifically, we combine clustering inductive bias with a random neural network, and then exploit computational advantages offered by a fast random projection algorithm. Experiments on two TSR benchmarks illustrate that our method achieves comparable or higher recognition accuracy than state-of-the-art DCNN-based methods with less training data and inference time consumption.","Traffic sign recognition, Few-shot learning, Clustering, Randomization,",Shichao Zhou and Chenwei Deng and Zhengquan Piao and Baojun Zhao,https://www.sciencedirect.com/science/article/pii/S0031320319304601,https://doi.org/10.1016/j.patcog.2019.107160,0031-3203,2020,107160,100,Pattern Recognition,Few-shot traffic sign recognition with clustering inductive bias and random neural network,article,ZHOU2020107160,
"Scene discrepancy between the left and right views presents more challenges for image quality assessment (IQA) of stereoscopic images as opposed to monocular ones. Existing no-reference stereoscopic IQA (NR-SIQA) metrics cannot achieve a good performance on asymmetrically distorted stereoscopic images. In this paper, we propose an NR-SIQA index that first addresses scene discrepancy by means of image registration. It then uses a registered distortion representation based on the left and registered right views to represent the distortion in the stereoscopic image. Because different distortion types influence image quality differently, a multi-task convolutional neural network (CNN) is employed to learn image quality prediction and distortion-type identification simultaneously. We first design a one-column multi-task CNN model, that learns from the registered distortion representation. Then, we extend the one-column model to a three-column model, which also learns from the left and right views. Our experimental results validate the effectiveness of the proposed registered distortion representation and multi-task CNN architecture. The proposed one- and three-column models outperform the state-of-the-art NR-SIQA metrics, especially for asymmetrically distorted stereoscopic images.","No-reference stereoscopic image quality assessment, Multi-task learning, Convolutional neural network, Image registration",Yiqing Shi and Wenzhong Guo and Yuzhen Niu and Jiamei Zhan,https://www.sciencedirect.com/science/article/pii/S0031320319304686,https://doi.org/10.1016/j.patcog.2019.107168,0031-3203,2020,107168,100,Pattern Recognition,No-reference stereoscopic image quality assessment using a multi-task CNN and registered distortion representation,article,SHI2020107168,
"The lack of interpretability of existing CNN-based hand detection methods makes it difficult to understand the rationale behind their predictions. In this paper, we propose a novel neural network model, which introduces interpretability into hand detection for the first time. The main improvements include: (1) Detect hands at pixel level to explain what pixels are the basis for its decision and improve transparency of the model. (2) The explainable Highlight Feature Fusion block highlights distinctive features among multiple layers and learns discriminative ones to gain robust performance. (3) We introduce a transparent representation, the rotation map, to learn rotation features instead of complex and non-transparent rotation and derotation layers. (4) Auxiliary supervision accelerates the training process, which saves more than 10Â h in our experiments. Experimental results on the VIVA and Oxford hand detection and tracking datasets show competitive accuracy of our method compared with state-of-the-art methods with higher speed. Models and code are available: https://isrc.iscas.ac.cn/gitlab/research/pr2020-phdn.","Interpretability, Hand detection, Pixel level, Explainable representation, Rotation map",Dan Liu and Libo Zhang and Tiejian Luo and Lili Tao and Yanjun Wu,https://www.sciencedirect.com/science/article/pii/S0031320320300091,https://doi.org/10.1016/j.patcog.2020.107202,0031-3203,2020,107202,105,Pattern Recognition,Towards interpretable and robust hand detection via pixel-wise prediction,article,LIU2020107202,
"Deep learning (DL) models, e.g., state-of-the-art convolutional neural networks (CNNs), have been widely applied into security sensitivity tasks, such as face payment, security monitoring, automated driving, etc. Then their vulnerability analysis is an emergent topic, especially for black-box attacks, where adversaries do not know the model internal architectures or training parameters. In this paper, two types of ensemble-based black-box attack strategies, selective cascade ensemble strategy (SCES) and stack parallel ensemble strategy (SPES), are proposed to explore the vulnerability of DL system and potential factors that contribute to the high-efficiency attacks are explored. SCES adopts a boosting structure of ensemble learning and SPES employs a bagging structure. Moreover, two pairwise and non-pairwise diversity measures are adopted to examine the relationship between the diversity in substitutes ensembles and transferability of generated adversarial examples. Experimental results show that proposed ensemble adversarial black-box attack strategies can successfully attack the DL system with some defense mechanism, such as adversarial training and ensemble adversarial training. The experimental results also show the greater the diversity in substitute ensembles enables stronger transferability.","Black-box attack, Vulnerability, Ensemble adversarial attack, Diversity, Transferability",Jie Hang and Keji Han and Hui Chen and Yun Li,https://www.sciencedirect.com/science/article/pii/S0031320319304844,https://doi.org/10.1016/j.patcog.2019.107184,0031-3203,2020,107184,101,Pattern Recognition,Ensemble adversarial black-box attacks against deep learning systems,article,HANG2020107184,
"Different convolutional layers in an explainable CNN usually encode different kinds of semantic information for an image, thus the feature fusion approaches like SSD, DSSD, and FPN are widely employed to enhance the detection performance by integrating different results based on multiple convolutional layers. However, the typical fusion approaches first need to independently detect objects based on one convolutional layer before fusion, and this single layer may exist noises or be irrelevant to objects, resulting in detection failure. To tackle the above problem, this paper proposes âGated CNNâ (short for âG-CNNâ) to introduce a âgateâ structure to integrate multiple convolutional layers for object detection. Injected by multi-scale feature layers, a gate employs several filters to extract useful information and block noises by executing one more convolutional or deconvolutional operation simultaneously, thus a gate-based feature layer is more effective and efficient as compared to the convolutional one. Besides, G-CNN employs a detector with two branches to predict the locations and categories of objects, respectively, as well as an inter-class loss to help detectors learn discrepant information among categories. Therefore, the learned detectors could better differentiate similar objects of different categories. Extensive experiments are conducted on two image datasets (PASCAL VOC and COCO), and the results demonstrate that G-CNN outperforms the state-of-the-art approaches, with a mAP of 40.9% at 10.6 FPS.","Gated CNN, object detection, multi-scale feature layers, explainable CNN",Jin Yuan and Heng-Chang Xiong and Yi Xiao and Weili Guan and Meng Wang and Richang Hong and Zhi-Yong Li,https://www.sciencedirect.com/science/article/pii/S0031320319304327,https://doi.org/10.1016/j.patcog.2019.107131,0031-3203,2020,107131,105,Pattern Recognition,Gated CNN: Integrating multi-scale feature layers for object detection,article,YUAN2020107131,
"Learning robust representations for applications with multiple modalities of input can have a significant impact on improving performance. Traditional representation learning methods rely on projecting the input modalities to a common subspace to maximize agreement amongst the modalities for a particular task. We propose a novel approach to representation learning that uses a latent representation decoder to reconstruct the target modality and thereby employ the target modality purely as a supervision signal for discovering correlations between the modalities. Through cross modality supervision, we demonstrate that the learnt representation is able to improve upon the performance of the task of facial action unit (AU) recognition over modality specific representations and even their fused counterparts. As an extension, we explore a new transfer learning technique to adapt the learnt representation to the target domain. We also present a shared representation based feature fusion methodology to improve the performance of any multi-modal system. Our experiments on three AU recognition datasets - MMSE, BP4D and DISFA, show strong performance gains producing state-of-the-art results in spite of the absence of data from a modality.","Feature fusion, Feature fine-tuning, Facial action unit recognition, Deep fusion, Multi-Modal representation learning",Nishant Sankaran and Deen Dayal Mohan and Nagashri N. Lakshminarayana and Srirangaraj Setlur and Venu Govindaraju,https://www.sciencedirect.com/science/article/pii/S0031320319304285,https://doi.org/10.1016/j.patcog.2019.107127,0031-3203,2020,107127,102,Pattern Recognition,Domain adaptive representation learning for facial action unit recognition,article,SANKARAN2020107127,
"The previous matrix regression based methods mainly focus on designing a robust error term to characterize the occlusion and illumination changes. In actually, it is very challenging to give a strong model for solving the original images directly since the images contains rich and complex structure information. To address this problem, we aim to simplify the complex images and propose a simple and robust matrix regression based classification model. In our method, we firstly employ the local gradient distribution to decompose the image into a series of gradient images (LID for short). Each gradient image reveals the local structure information in different gradient orientations. Subsequently, we consider each gradient image as the diagonal block element and construct the diagonal block matrix for image representation. Nuclear norm based matrix regression model (NMR) is then applied to complete the classification tasks. The proposed model can be called ID-NMR for short. We further design a fast ADMM optimization algorithm to solve the proposed ID-NMR due to the fact that the big diagonal block matrix will increase the computational load. Experimental results show that the proposed method performs favorably compared with state-of-the-art regression based classification methods.","Matrix regression, Low rank, Image decomposition, Pattern classification",Jianjun Qian and Jian Yang and Yong Xu and Jin Xie and Zhihui Lai and Bob Zhang,https://www.sciencedirect.com/science/article/pii/S003132032030008X,https://doi.org/10.1016/j.patcog.2020.107204,0031-3203,2020,107204,102,Pattern Recognition,Image decomposition based matrix regression with applications to robust face recognition,article,QIAN2020107204,
"Identity switches caused by inter-object interactions remain a critical problem for multi-player tracking in real-world sports video analysis. Existing approaches utilizing the appearance model is difficult to associate detections and preserve identities due to the similar appearance of players in the same team. Instead of the appearance model, we propose a distinguishable deep representation for player identity in this paper. A robust multi-player tracker incorporating with deep player identification is further developed to produce identity-coherent trajectories. The framework consists of three parts: (1) the core component, a Deep Player Identification (DeepPlayer) model that provides an adequate discriminative feature through the coarse-to-fine jersey number recognition and the pose-guided partial feature embedding; (2) an Individual Probability Occupancy Map (IPOM) model for players 3D localization with ID; and (3) a K-Shortest Path with ID (KSP-ID) model that links nodes in the flow graph by a proposed player ID correlation coefficient. With the distinguishable identity, the performance of tracking is improved. Experiment results illustrate that our framework handles the identity switches effectively, and outperforms state-of-the-art trackers on the sports video benchmarks.","Identity switch, Multi-target multi-camera tracking, Object detection, Player identification, CNN",Ruiheng Zhang and Lingxiang Wu and Yukun Yang and Wanneng Wu and Yueqiang Chen and Min Xu,https://www.sciencedirect.com/science/article/pii/S0031320320300650,https://doi.org/10.1016/j.patcog.2020.107260,0031-3203,2020,107260,102,Pattern Recognition,Multi-camera multi-player tracking with deep player identification in sports video,article,ZHANG2020107260,
"Multi-view clustering is a hot research topic in machine learning and pattern recognition, however, it remains high computational complexity when clustering multi-view data sets. Although a number of approaches have been proposed to accelerate the computational efficiency, most of them do not consider the data duality between features and samples. In this paper, we propose a novel co-clustering approach termed as Fast Multi-view Bilateral K-meansÂ (FMVBKM), which can implement clustering task on row and column of the input data matrix, simultaneously. Specifically, FMVBKM applies the relaxed K-means clustering technique to multi-view data clustering. In addition, to decrease information loss in matrix factorization, we further introduce a new co-clustering method named as Fast Multi-view Matrix Tri-FactorizationÂ (FMVMTF). Extensive experimental results on six benchmark data sets show that the proposed two approaches not only have comparable clustering performance but also present the high computational efficiency, in comparison with state-of-the-art multi-view clustering methods.","Co-clustering, Multi-view data, Matrix factorization, Auto-weighted",Feiping Nie and Shaojun Shi and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320320300133,https://doi.org/10.1016/j.patcog.2020.107207,0031-3203,2020,107207,102,Pattern Recognition,Auto-weighted multi-view co-clustering via fast matrix factorization,article,NIE2020107207,
"Single image rain removal plays an important role in numerous multimedia applications. Existing algorithms usually tackle the deraining problem by the way of signal removal, which lead to over-smoothness and generate unexpected artifacts in de-rained images. This paper addresses the deraining problem from a completely different perspective of feature-wise disentanglement, and introduces the interactions and constraints between two disentangled latent spaces. Specifically, we propose an Asynchronous Interactive Generative Adversarial Network (AI-GAN) to progressively disentangle the rainy image into background and rain spaces in feature level through a two-branch structure. Each branch employs a two-stage synthesis strategy and interacts asynchronously by exchanging feed-forward information and sharing feedback gradients, achieving complementary adversarial optimization. This âadversarialâ is not only the âadversarialâ between the generator and the discriminator, but also means that the two generators are entangled, and interact with each other in the optimization process. Extensive experimental results demonstrate that AI-GAN outperforms state-of-the-art deraining methods and benefits various typical multimedia applications such as Image/Video Coding, Action Recognition, and Person Re-identification.","Feature-wise disentanglement, Asynchronous and interactive, Single image deraining, Complementary adversarial training",Xin Jin and Zhibo Chen and Weiping Li,https://www.sciencedirect.com/science/article/pii/S0031320319304443,https://doi.org/10.1016/j.patcog.2019.107143,0031-3203,2020,107143,100,Pattern Recognition,AI-GAN: Asynchronous interactive generative adversarial network for single image rain removal,article,JIN2020107143,
"Low-dimensional and compact representation of time series data is of importance for mining and storage. In practice, time series data are vulnerable to various temporal transformations, such as shift and temporal scaling, however, which are unavoidable in the process of data collection. If a learning algorithm directly calculates the difference between such transformed data based on Euclidean distance, the measurement cannot faithfully reflect the similarity and hence could not learn the underlying discriminative features. In order to solve this problem, we develop a novel subspace learning algorithm based on dynamic time warping (DTW) distance which is an elastic distance defined in a DTW space. The algorithm aims to minimize the reconstruction error in the DTW space. However, since DTW space is a semi-pseudo metric space, it is difficult to generalize common subspace learning algorithms for such semi-pseudo metric spaces. In this work, we introduce warp operators with which DTW reconstruction error can be approximated by reconstruction error between transformed series and their reconstructions in a subspace. The warp operators align time series data with their linear representations in the DTW space, which is in particular important for misaligned time series, so that the subspace can be learned to obtain an intrinsic basis (dictionary) for the representation of the data. The warp operators and the subspace are optimized alternatively until reaching equilibrium. Experiments results show that the proposed algorithm outperforms traditional subspace learning algorithms and temporal transform-invariance based methods (including SIDL, Kernel PCA, and SPMC et. al), and obtains competitive results with the state-of-the-art algorithms, such as BOSS algorithm.","Invariant subspace learning, Dynamic time warping (DTW), Time series, Dictionary learning",Huiqi Deng and Weifu Chen and Qi Shen and Andy J. Ma and Pong C. Yuen and Guocan Feng,https://www.sciencedirect.com/science/article/pii/S0031320320300169,https://doi.org/10.1016/j.patcog.2020.107210,0031-3203,2020,107210,102,Pattern Recognition,Invariant subspace learning for time series data based on dynamic time warping distance,article,DENG2020107210,
"Image dehazing is a very important pre-processing step to many computer vision tasks such as object recognition and tracking. However, it is a challenging problem because the physical parameters of imaging, e.g. the depth information of scene pixels and the attenuation model, are usually unknown. Based on a physical model, different methods have been proposed to recover these parameters. Existing convolutional neural networks (CNNs) based methods try to solve the image dehazing problem using an end-to-end network to learn a direct mapping between a hazy image and its corresponding clear image. But the representational ability, spatial variant ability and dehazing capability of these network models are hindered by treating all the spatial and channel-wise features indiscriminately. Hence, we propose an end-to-end dehazing network with a parallel spatial/channel-wise attention block for capturing more informative spatial and channel-wise features respectively. Specifically, based on the encoder-decoder framework with a pyramid pooling operation, a novel parallel spatial/channel-wise attention block is proposed and applied to the end of the encoder for guiding the decoder to reconstruct better clear images. In the spatial/channel-wise attention block, the spatial attention module and the channel-wise attention module are connected in parallel, where the spatial attention module highlights important spatial positions of features. Meanwhile, the channel-wise module exploits inter-dependencies among the channel-wise features. Extensive experiments demonstrate that our network with a parallel spatial /channel-wise attention block can achieve better accuracy and visual results over state-of-the-art methods.","Image dehazing, Spatial attention mechanism, Channel-wise attention mechanism, Encoder-decoder, CNN",Shibai Yin and Yibin Wang and Yee-Hong Yang,https://www.sciencedirect.com/science/article/pii/S0031320320300601,https://doi.org/10.1016/j.patcog.2020.107255,0031-3203,2020,107255,102,Pattern Recognition,A novel image-dehazing network with a parallel attention block,article,YIN2020107255,
"Considering the proliferation of extremely high-dimensional data in many domains including computer vision and healthcare applications such as computer-aided diagnosis (CAD), advanced techniques for reducing data dimensionality and identifying the most relevant features for a given classification task such as distinguishing between healthy and disordered brain states are needed. Despite the existence of many works on boosting the classification accuracy using a particular feature selection (FS) method, choosing the best one from a large pool of existing FS techniques for boosting feature reproducibility within a dataset of interest remains a formidable challenge to tackle. Notably, a good performance of a particular FS method does not necessarily imply that the experiment is reproducible and that the features identified are optimal for the entirety of the samples. Essentially, this paper presents the first attempt to address the following challenge: âGiven a set of different feature selection methods {FS1,â¯,FSK}, and a dataset of interest, how to identify the most reproducible and âtrustworthyâ connectomic features that would produce reliable biomarkers capable of accurately differentiate between two specific conditions?â To this aim, we propose FS-Select framework which explores the relationships among the different FS methods using a multi-graph architecture based on feature reproducibility power, average accuracy, and feature stability of each FS method. By extracting the âcentralâ graph node, we identify the most reliable and reproducible FS method for the target brain state classification task along with the most discriminative features fingerprinting these brain states. To evaluate the reproducibility power of FS-Select, we perturbed the training set by using different cross-validation strategies on a multi-view small-scale connectomic dataset (late mild cognitive impairment vs Alzheimerâs disease) and large-scale dataset including autistic vs healthy subjects. Our experiments revealed reproducible connectional features fingerprinting disordered brain states.","Feature selection methods, Multi-graph topological analysis, Feature reproducibility, Biomarker discovery, Morphological brain network, Neurological disorders, Connectomics, Cross-validation",Nicolas Georges and Islem Mhiri and Islem Rekik,https://www.sciencedirect.com/science/article/pii/S0031320319304832,https://doi.org/10.1016/j.patcog.2019.107183,0031-3203,2020,107183,101,Pattern Recognition,Identifying the best data-driven feature selection method for boosting reproducibility in classification tasks,article,GEORGES2020107183,
"In this paper, we address the fusion of image and point cloud data for road detection. To take advantage of both deep network and multi-modal data fusion, we propose an end-to-end road segmentation network called SPSTFN (Spatial Propagation and Spatial Transformation Fusion Network). Our method considers the model-level fusion and dual-view fusion in the network simultaneously for the first time. Specifically, the proposed SPSTFN contains three parts: the point cloud branch, the image branch, and the fusion block. Firstly, we design a simple but efficient lightweight network to handle the unordered and sparse point cloud to obtain a coarse representation of the road area. Then, an equal-resolution convolutional block is adopted to capture the low-level features of the image which are used to produce the heat diffusion coefficients of the joint anisotropic diffusion based spatial propagation model. Thirdly, we conduct the diffusion process on the coarse representation under the guidance of the learned low-level image features, both in the perspective and bird views, via the spatial transformation in the network. Finally, the diffusion results of the two views are then integrated to generate the final refined representation of the road area. The proposed fusion method is totally data-driven and parameter-free, and the whole fusion network can be trained with the standard BP (Back Propagation) algorithm. Without any additional process steps and pre-training, the proposed method obtains competitive results on the KITTI Road Benchmark.","Road detection, Fusion, Spatial propagation, Spatial transformation, Joint anisotropic diffusion",Fei Yang and Huan Wang and Zhong Jin,https://www.sciencedirect.com/science/article/pii/S003132031930442X,https://doi.org/10.1016/j.patcog.2019.107141,0031-3203,2020,107141,100,Pattern Recognition,A fusion network for road detection via spatial propagation and spatial transformation,article,YANG2020107141,
"Deep facial attribute prediction has received considerable attention with a wide range of real-world applications in the past few years. Existing works almost extract abstract global features at high levels of deep neural networks to make predictions. However, local features at low levels, which contain detailed local attribute information, are not well exploited. In this paper, we propose a novel Bi-directional Ladder Attentive Network (BLAN) to learn hierarchical representations, covering the correlations between feature hierarchies and attribute characteristics. BLAN adopts layer-wise bi-directional connections based on the autoencoder framework from low to high levels. In this way, hierarchical features with local and global attribute characteristics could be correspondingly interweaved at each level via multiple designed Residual Dual Attention Modules (RDAMs). Besides, we derive a Local Mutual Information Maximization (LMIM) loss to further incorporate the locality of facial attributes to high-level representations at each hierarchy. Multiple attribute classifiers receive hierarchical representations to produce local and global decisions, followed by a proposed adaptive score fusion module to merge these decisions for yielding the final prediction result. Extensive experiments on two facial attribute datasets, CelebA and LFWA, demonstrate that our BLAN outperforms state-of-the-art methods.","Deep facial attribute prediction, Bi-directional ladder attentive network (BLAN), Residual dual attention module (RDAM), Local mutual information maximization (LMIM), Adaptive score fusion",Xin Zheng and Huaibo Huang and Yanqing Guo and Bo Wang and Ran He,https://www.sciencedirect.com/science/article/pii/S0031320319304558,https://doi.org/10.1016/j.patcog.2019.107155,0031-3203,2020,107155,100,Pattern Recognition,BLAN: Bi-directional ladder attentive network for facial attribute prediction,article,ZHENG2020107155,
"Imposing the 1-Lipschitz constraint is a problem of key importance in the training of Generative Adversarial Networks (GANs), which has been proved to productively improve stability of GAN training. Although some interesting alternative methods have been proposed to enforce the 1-Lipschitz property, these existing approaches (e.g., weight clipping, gradient penalty (GP), and spectral normalization (SN)) are only partially successful. In this paper, we propose a novel method, which we refer to as spectral bounding (SB) to strictly enforce the 1-Lipschitz constraint. Our method adopts very cost-effective terms of both 1-norm and â-norm, and yet allows us to efficiently approximate the upper bound of spectral norms. In this way, our method provide important insights to the relationship between an alternative of strictly satisfying the Lipschitz property and explainable training stability improvements of GAN. Our proposed method thus significantly enhances the stability of GAN training and the quality of generated images. Extensive experiments are conducted, showing that the proposed method outperforms GP and SN on both CIFAR-10 and ILSVRC2015 (ImagetNet) dataset in terms of the standard inception score.","Generative adversarial networks, 1-Lipschitz constraint, Spectral bounding, Image generation",Zhihong Zhang and Yangbin Zeng and Lu Bai and Yiqun Hu and Meihong Wu and Shuai Wang and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320319304790,https://doi.org/10.1016/j.patcog.2019.107179,0031-3203,2020,107179,105,Pattern Recognition,Spectral bounding: Strictly satisfying the 1-Lipschitz property for generative adversarial networks,article,ZHANG2020107179,
"Business information networks involve diverse users and rich content and have emerged as important platforms for enabling business intelligence and business decision making. A key step in an organizations business intelligence process is to cluster users with similar interests into social audiences and discover the roles they play within a business network. In this article, we propose a novel machine-learning approach, called CBIN, that co-clusters business information networks to discover and understand these audiences. The CBIN framework is based on co-factorization. The audience clusters are discovered from a combination of network structures and rich contextual information, such as node interactions and node-content correlations. Since what defines an audience cluster is data-driven, plus they often overlap, pre-determining the number of clusters is usually very difficult. Therefore, we have based CBIN on an overlapping clustering paradigm with a hold-out strategy to discover the optimal number of clusters given the underlying data. Experiments validate an outstanding performance by CBIN compared to other state-of-the-art algorithms on 13 real-world enterprise datasets.","Machine learning, Clustering, Business information networks, Social networks",Yu Zheng and Ruiqi Hu and Sai-fu Fung and Celina Yu and Guodong Long and Ting Guo and Shirui Pan,https://www.sciencedirect.com/science/article/pii/S0031320319304273,https://doi.org/10.1016/j.patcog.2019.107126,0031-3203,2020,107126,100,Pattern Recognition,Clustering social audiences in business information networks,article,ZHENG2020107126,
"In this paper, we propose an efficient algorithm, the ACC-REG, to automatically extract intrinsic key characteristics on hippocampal mesh surfaces and hence compute an accurate registration mapping between them. Given a pair of hippocampal surface mesh, the proposed algorithm constructs the eigen-graphs, an intrinsic feature on the surface, on each surface as its representative. The eigen-graphs are then calibrated along the longitudinal direction of the hippocampal surfaces. Accurately corresponded intrinsic characteristics on each hippocampus can thus be extracted. As a result, the two surfaces can be registered with improved accuracy and low computation cost. Experiments on ADNI data demonstrate the effectiveness of the proposed ACC-REG model over existing methods.","Feature extraction, Feature correspondence, Surface registration, Surface deformation, Eigen-graph",Hei Long CHAN and Tsz Chun YAM and Lok Ming LUI,https://www.sciencedirect.com/science/article/pii/S0031320319304431,https://doi.org/10.1016/j.patcog.2019.107142,0031-3203,2020,107142,103,Pattern Recognition,Automatic characteristic-calibrated registration (ACC-REG): Hippocampal surface registration using eigen-graphs,article,CHAN2020107142,
"Feature coding is a key component of the bag of visual words (BoVW) model, which is designed to improve image classification and retrieval performance. In the feature coding process, each feature of an image is nonlinearly mapped via a dictionary of visual words to form a high-dimensional sparse vector. Inspired by the well-known locality-constrained linear coding (LLC), we present a locality-constrained affine subspace coding (LASC) method to address the limitation whereby LLC fails to consider the local geometric structure around visual words. LASC is distinguished from all the other coding methods since it constructs a dictionary consisting of an ensemble of affine subspaces. As such, the local geometric structure of a manifold is explicitly modeled by such a dictionary. In the process of coding, each feature is linearly decomposed and weighted to form the first-order LASC vector with respect to its top-k neighboring subspaces. To further boost performance, we propose the second-order LASC vector based on information geometry. We use the proposed coding method to perform both image classification and image retrieval tasks and the experimental results show that the method achieves superior or competitive performance in comparison to state-of-the-art methods.","Bag of visual words, Locality-constrained affine subspace coding, Image classification, Image retrieval",Bingbing Zhang and Qilong Wang and Xiaoxiao Lu and Fasheng Wang and Peihua Li,https://www.sciencedirect.com/science/article/pii/S0031320319304674,https://doi.org/10.1016/j.patcog.2019.107167,0031-3203,2020,107167,100,Pattern Recognition,Locality-constrained affine subspace coding for image classification and retrieval,article,ZHANG2020107167,
"Indices quantifying the performance of classifiers under class-imbalance, often suffer from distortions depending on the constitution of the test set or the class-specific classification accuracy, creating difficulties in assessing the merit of the classifier. We identify two fundamental conditions that a performance index must satisfy to be respectively resilient to altering number of testing instances from each class and the number of classes in the test set. In light of these conditions, under the effect of class imbalance, we theoretically analyze four indices commonly used for evaluating binary classifiers and five popular indices for multi-class classifiers. For indices violating any of the conditions, we also suggest remedial modification and normalization. We further investigate the capability of the indices to retain information about the classification performance over all the classes, even when the classifier exhibits extreme performance on some classes. Simulation studies are performed on high dimensional deep representations of subset of the ImageNet dataset using four state-of-the-art classifiers tailored for handling class imbalance. Finally, based on our theoretical findings and empirical evidence, we recommend the appropriate indices that should be used to evaluate the performance of classifiers in presence of class-imbalance.","Imbalanced classification, Performance evaluation indices, Precision, Recall, GMean, Area under the curve",Sankha Subhra Mullick and Shounak Datta and Sourish Gunesh Dhekane and Swagatam Das,https://www.sciencedirect.com/science/article/pii/S0031320320300042,https://doi.org/10.1016/j.patcog.2020.107197,0031-3203,2020,107197,102,Pattern Recognition,Appropriateness of performance indices for imbalanced data classification: An analysis,article,MULLICK2020107197,
"Data imbalance remains one of the most widespread problems affecting contemporary machine learning. The negative effect data imbalance can have on the traditional learning algorithms is most severe in combination with other dataset difficulty factors, such as small disjuncts, presence of outliers and insufficient number of training observations. Aforementioned difficulty factors can also limit the applicability of some of the methods of dealing with data imbalance, in particular the neighborhood-based oversampling algorithms based on SMOTE. Radial-Based Oversampling (RBO) was previously proposed to mitigate some of the limitations of the neighborhood-based methods. In this paper we examine the possibility of utilizing the concept of mutual class potential, used to guide the oversampling process in RBO, in the undersampling procedure. Conducted computational complexity analysis indicates a significantly reduced time complexity of the proposed Radial-Based Undersampling algorithm, and the results of the performed experimental study indicate its usefulness, especially on difficult datasets.","Machine learning, Classification, Imbalanced data, Undersampling, Radial basis functions",MichaÅ Koziarski,https://www.sciencedirect.com/science/article/pii/S0031320320300674,https://doi.org/10.1016/j.patcog.2020.107262,0031-3203,2020,107262,102,Pattern Recognition,Radial-Based Undersampling for imbalanced data classification,article,KOZIARSKI2020107262,
"Computer-aided lesion detection (CAD) techniques, which provide potential for automatic early screening of retinal pathologies, are widely studied in retinal image analysis. While many CAD approaches based on lesion samples or lesion features can well detect pre-defined lesion types, it remains challenging to detect various abnormal regions (namely abnormalities) from retinal images. In this paper, we try to identify diverse abnormalities from a retinal test image by finely learning its individualized retinal background (IRB) on which retinal lesions superimpose. 3150 normal retinal images are collected as the priors for IRB learning. A preprocessing step is applied to all retinal images for spatial, scale and color normalization. Retinal blood vessels, which have individual variations in different images, are particularly suppressed from all images. A multi-scale sparse coding based learning (MSSCL) algorithm and a repeated learning strategy are proposed for finely learning the IRB. By the MSSCL algorithm, a background space is constructed by sparsely encoding the test image in a multi-scale manner using the dictionary learned from normal retinal images, which will contain more complete IRB information than any single-scale coding result. From the background space, the IRB can be well learned by low-rank approximation and thus different salient lesions can be separated and detected. The MSSCL algorithm will be iteratively repeated on the modified test image in which the detected salient lesions are suppressed, so as to further improve the accuracy of the IRB and suppress lesions in the IRB. Consequently, a high-accuracy IRB can be learned and thus both salient lesions and weak lesions that have low contrasts with the background can be clearly separated. The effectiveness and contributions of the proposed method are validated by experiments over different clinical data-sets and comparisons with the state-of-the-art CAD methods.","Retinal abnormality detection, Retinal lesion detection, Computer-aided detection, Dictionary learning, Background learning, Retinal image reading",Benzhi Chen and Lisheng Wang and Xiuying Wang and Jian Sun and Yijie Huang and Dagan Feng and Zongben Xu,https://www.sciencedirect.com/science/article/pii/S0031320320300157,https://doi.org/10.1016/j.patcog.2020.107209,0031-3203,2020,107209,102,Pattern Recognition,Abnormality detection in retinal image by individualized background learning,article,CHEN2020107209,
"Face photo-sketch synthesis and recognition has many applications in digital entertainment and law enforcement. Recently, generative adversarial networks (GANs) based methods have significantly improved the quality of image synthesis, but they have not explicitly considered the purpose of recognition. In this paper, we first propose an Identity-Aware CycleGAN (IACycleGAN) model that applies a new perceptual loss to supervise the image generation network. It improves CycleGAN on photo-sketch synthesis by paying more attention to the synthesis of key facial regions, such as eyes and nose, which are important for identity recognition. Furthermore, we develop a mutual optimization procedure between the synthesis model and the recognition model, which iteratively synthesizes better images by IACycleGAN and enhances the recognition model by the triplet loss of the generated and real samples. Extensive experiments are performed on both photo-to-sketch and sketch-to-photo tasks using the widely used CUFS and CUFSF databases. The results show that the proposed method performs better than several state-of-the-art methods in terms of both synthetic image quality and photo-sketch recognition accuracy.","Convolutional neural network, Generative adversarial network, Photo-sketch synthesis, Photo-sketch recognition, Identity-aware training",Yuke Fang and Weihong Deng and Junping Du and Jiani Hu,https://www.sciencedirect.com/science/article/pii/S0031320320300558,https://doi.org/10.1016/j.patcog.2020.107249,0031-3203,2020,107249,102,Pattern Recognition,Identity-aware CycleGAN for face photo-sketch synthesis and recognition,article,FANG2020107249,
"The aim of the robust tensor completion problem for third-order tensors is to recover a low-rank tensor from incomplete and/or corrupted observations. In this paper, we develop a patched-tubes unitary transform method for robust tensor completion. The proposed method is to extract similar patched-tubes to form a third-order sub-tensor, and then a transformed tensor singular value decomposition is employed to recover such low-rank incomplete and/or corrupted sub-tensor. Here the unitary transform matrix for transformed tensor singular value decomposition is constructed by using left singular vectors of the unfolding matrix arising from such sub-tensor. Moreover, we establish the perturbation results of the transformed tensor singular value decomposition for patched-tubes tensor completion. Extensive numerical experiments on hyperspectral, video and face data sets are presented to demonstrate the superior performance of the proposed patched-tubes unitary transform method over testing state-of-the-art robust tensor completion methods.","Unitary transform, Patched-tubes, Robust tensor completion, Transformed tensor singular value decomposition",Michael K. Ng and Xiongjun Zhang and Xi-Le Zhao,https://www.sciencedirect.com/science/article/pii/S0031320319304819,https://doi.org/10.1016/j.patcog.2019.107181,0031-3203,2020,107181,100,Pattern Recognition,Patched-tube unitary transform for robust tensor completion,article,NG2020107181,
"Accurate pedestrian orientation estimation of autonomous driving helps the ego vehicle obtain the intentions of pedestrians in the related environment, which are the base of safety measures such as collision avoidance and prewarning. However, because of relatively small sizes and high-level deformation of pedestrians, common pedestrian orientation estimation models fail to extract sufficient and comprehensive information from them, thus having their performance restricted, especially monocular ones which fail to obtain depth information of objects and related environment. In this paper, a novel monocular pedestrian orientation estimation model, called FFNet, is proposed. Apart from camera captures, the model adds the 2D and 3D dimensions of pedestrians as two other inputs according to the logic relationship between orientation and them. The 2D and 3D dimensions of pedestrians are determined from the camera captures and further utilized through two feedforward links connected to the orientation estimator. The feedforward links strengthen the logicality and interpretability of the network structure of the proposed model. Experiments show that the proposed model has at least 1.72% AOS increase than most state-of-the-art models after identical training processes. The model also has competitive results in orientation estimation evaluation on KITTI dataset.","Information feedforward, Logic relationship, Monocular vision, Orientation estimation",Chenchen Zhao and Yeqiang Qian and Ming Yang,https://www.sciencedirect.com/science/article/pii/S0031320319304820,https://doi.org/10.1016/j.patcog.2019.107182,0031-3203,2020,107182,100,Pattern Recognition,Monocular pedestrian orientation estimation based on deep 2D-3D feedforward,article,ZHAO2020107182,
"Sparse representation-based classification (SRC) has been shown to achieve a high level of accuracy in face recognition (FR). However, matching faces captured in unconstrained video against a gallery with a single reference facial still per individual typically yields low accuracy. For improved robustness to intra-class variations, SRC techniques for FR have recently been extended to incorporate variational information from an external generic set into an auxiliary dictionary. Despite their success in handling linear variations, non-linear variations (e.g., pose and expressions) between probe and reference facial images cannot be accurately reconstructed with a linear combination of images in the gallery and auxiliary dictionaries because they do not share the same type of variations. In order to account for non-linear variations due to pose, a paired sparse representation model is introduced allowing for joint use of variational information and synthetic face images. The proposed model, called synthetic plus variational model, reconstructs a probe image by jointly using (1) a variational dictionary and (2) a gallery dictionary augmented with a set of synthetic images generated over a wide diversity of pose angles. The augmented gallery dictionary is then encouraged to pair the same sparsity pattern with the variational dictionary for similar pose angles by solving a newly formulated simultaneous sparsity-based optimization problem. Experimental results obtained on Chokepoint and COX-S2V datasets, using different face representations, indicate that the proposed approach can outperform state-of-the-art SRC-based methods for still-to-video FR with a single sample per person.","Face recognition, Sparse representation-based classification, Face synthesis, Generic learning, Simultaneous sparsity, Video surveillance",Fania Mokhayeri and Eric Granger,https://www.sciencedirect.com/science/article/pii/S0031320319304303,https://doi.org/10.1016/j.patcog.2019.107129,0031-3203,2020,107129,100,Pattern Recognition,A paired sparse representation model for robust face recognition from a single sample,article,MOKHAYERI2020107129,
"Background subtraction is needed to extract foreground information from a video sequence for further processing in many applications, such as surveillance tracking. However, due to the presence of a dynamic background and noise, extracting foreground accurately from a video sequence remains challenging. A novel projection method, namely Principal Mode Component Analysis (PMCA), is proposed to capture the most repetitive patterns of a video sequence, which is one of the key characteristics of the video background. The patterns are captured by applying the bootstrapping method together with the statistic mode measure. The bootstrapping method can model the distribution of almost any statistic of the dynamic background and complicated noise. This is different from current methods, which restrict the distribution to a closed-form function. We introduce a mathematical relaxation that can formulate the statistical mode measure for a continuous video data. A fast exhaustive search method is proposed to find the global optimal solution for the PMCA. This fast method adopts a simplification procedure that makes the optimization procedure independent of the video size. The proposed method is computationally much more traceable than existing ones. We compare the proposed method with 10 different methods, including several state-of-the-art techniques, for 19 different real-world video sequences from two popular datasets. Experiment results show that the proposed method performs the best in 16 cases and second best in 2 cases.","Background modeling, Video surveillance, Principal Component analysis, Statistical mode",Benson S.Y. Lam and Amanda M.Y. Chu and H. Yan,https://www.sciencedirect.com/science/article/pii/S0031320319304546,https://doi.org/10.1016/j.patcog.2019.107153,0031-3203,2020,107153,100,Pattern Recognition,Statistical bootstrap-based principal mode component analysis for dynamic background subtraction,article,LAM2020107153,
"Domain adaptation (DA) and domain generalization (DG) have emerged as a solution to the domain shift problem where the distribution of the source and target data is different. The task of DG is more challenging than DA as the target data is totally unseen during the training phase in DG scenarios. The current state-of-the-art employs adversarial techniques, however, these are rarely considered for the DG problem. Furthermore, these approaches do not consider correlation alignment which has been proven highly beneficial for minimizing domain discrepancy. In this paper, we propose a correlation-aware adversarial DA and DG framework where the features of the source and target data are minimized using correlation alignment along with adversarial learning. Incorporating the correlation alignment module along with adversarial learning helps to achieve a more domain agnostic model due to the improved ability to reduce domain discrepancy with unlabeled target data more effectively. Experiments on benchmark datasets serve as evidence that our proposed method yields improved state-of-the-art performance.","Domain adaptation, Domain generalization, Correlation-alignment, Adversarial learning",Mohammad Mahfujur Rahman and Clinton Fookes and Mahsa Baktashmotlagh and Sridha Sridharan,https://www.sciencedirect.com/science/article/pii/S003132031930425X,https://doi.org/10.1016/j.patcog.2019.107124,0031-3203,2020,107124,100,Pattern Recognition,Correlation-aware adversarial domain adaptation and generalization,article,RAHMAN2020107124,
"Despite the great success in the computer vision field, visual tracking is still a challenging task. The main obstacle is that the target object often suffers from interference, such as occlusion. As most Siamese network-based trackers mainly sample image patches of target objects for training, the tracking algorithm lacks sufficient information about the surrounding environment. Besides, many Siamese network-based tracking algorithms build a regression only with the target object samples without considering the relationship between target and background, which may deteriorate the performance of trackers. In this paper, we propose a metric correlation Siamese network and multi-class negative sampling tracking method. For the first time, we explore a sampling approach that includes three different kinds of negative samples: virtual negative samples for pre-learning the potential occlusion situation, boundary negative samples to cope with potential tracking drift, and context negative samples to cope with potential incorrect positioning. With the three kinds of negative samples, we also propose a metric correlation method to train a correlation filter that contains metric information for better discrimination. Furthermore, we design a Siamese network-based architecture to embed the metric correlation filter module mentioned above in order to benefit from the powerful representation ability of deep learning. Extensive experiments on challenging OTB100 and VOT2017 datasets demonstrate the competitive performance of the proposed algorithm performs favorably compared with state-of-the-art approaches.","Visual tracking, Siamese network, Metric correlation filter, Multi-class negative samples",Yafu Xiao and Jing Li and Bo Du and Jia Wu and Jun Chang and Wenfan Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319304704,https://doi.org/10.1016/j.patcog.2019.107170,0031-3203,2020,107170,100,Pattern Recognition,MeMu: Metric correlation Siamese network and multi-class negative sampling for visual tracking,article,XIAO2020107170,
"Age estimation of unknown persons is a challenging pattern analysis task due to the lack of training data and various ageing mechanisms for different individuals. Label distribution learning-based methods usually make distribution assumptions to simplify age estimation. However, since different genders, races and/or any other characteristics may influence facial ageing, age-label distributions are often complicated and difficult to model parametrically. In this paper, we propose a label refinery network (LRN) with two concurrent processes: label distribution refinement and slack regression refinement. The label refinery network aims to learn age-label distributions progressively in an iterative manner. In this way, we can adaptively obtain the specific age-label distributions for different facial images without making strong assumptions on the fixed distribution formulations. To further utilize the correlations among age labels, we propose a slack regression refinery to convert the age-label regression model into an age-interval regression model. Extensive experiments on three popular datasets, namely, MORPH Album2, ChaLearn15 and MegaAge-Asian, demonstrate the superiority of our method.","Age estimation, Deep learning, Convolutional neural networks, Label distribution learning",Peipei Li and Yibo Hu and Xiang Wu and Ran He and Zhenan Sun,https://www.sciencedirect.com/science/article/pii/S0031320319304789,https://doi.org/10.1016/j.patcog.2019.107178,0031-3203,2020,107178,100,Pattern Recognition,Deep label refinement for age estimation,article,LI2020107178,
"In the field of machine learning, it is still a critical issue to identify and supervise the learned representation without manually intervening or intuition assistance to extract useful knowledge or serve for the downstream tasks. In this work, we focus on supervising the influential factors extracted by the variational autoencoder (VAE). The VAE is proposed to learn independent low dimension representation while facing the problem that sometimes pre-set factors are ignored. We argue that the mutual information of the input and each learned factor of the representation plays a necessary indicator of discovering the influential factors. We find the VAE objective inclines to induce mutual information sparsity in factor dimension over the data intrinsic dimension and therefore result in some non-influential factors whose function on data reconstruction could be ignored. We show mutual information also influences the lower bound of VAEâs reconstruction error and downstream classification task. To make such indicator applicable, we design an algorithm for calculating the mutual information for VAE and prove its consistency. Experimental results on MNIST, CelebA and DEAP datasets show that mutual information can help determine influential factors, of which some are interpretable and can be used to further generation and classification tasks, and help discover the variant that connects with emotion on DEAP dataset.","Variational autoencoder, Mutual information, Generative model",Shiqi Liu and Jingxin Liu and Qian Zhao and Xiangyong Cao and Huibin Li and Deyu Meng and Hongying Meng and Sheng Liu,https://www.sciencedirect.com/science/article/pii/S0031320319304662,https://doi.org/10.1016/j.patcog.2019.107166,0031-3203,2020,107166,100,Pattern Recognition,Discovering influential factors in variational autoencoders,article,LIU2020107166,
"Unconstrained face recognition still remains a challenging task due to various factors such as pose, expression, illumination, partial occlusion, etc. In particular, the most significant appearance variations are stemmed from poses which leads to severe performance degeneration. In this paper, we propose a novel Deformable Face Net (DFN) to handle the pose variations for face recognition. The deformable convolution module attempts to simultaneously learn face recognition oriented alignment and identity-preserving feature extraction. The displacement consistency loss (DCL) is proposed as a regularization term to enforce the learnt displacement fields for aligning faces to be locally consistent both in the orientation and amplitude since faces possess strong structure. Moreover, the identity consistency loss (ICL) and the pose-triplet loss (PTL) are designed to minimize the intra-class feature variation caused by different poses and maximize the inter-class feature distance under the same poses. The proposed DFN can effectively handle pose invariant face recognition (PIFR). Extensive experiments show that the proposed DFN outperforms the state-of-the-art methods, especially on the datasets with large poses.","Pose-invariant face recognition, Displacement consistency loss, Pose-triplet loss",Mingjie He and Jie Zhang and Shiguang Shan and Meina Kan and Xilin Chen,https://www.sciencedirect.com/science/article/pii/S0031320319304145,https://doi.org/10.1016/j.patcog.2019.107113,0031-3203,2020,107113,100,Pattern Recognition,Deformable face net for pose invariant face recognition,article,HE2020107113,
"Graphs are an invaluable modelling tool in many domains, but visualising large graphs in their entirety can be difficult. Hierarchical graph visualisation â recursively clustering a graphâs nodes to view it at a higher level of abstraction â has thus become popular. However, this can hide important information that a user needs to understand a graphâs topology, e.g. nodesâ neighbourhoods. TugGraph addressed this by âseparating outâ a given nodeâs neighbours from their hierarchy ancestors to visualise them independently. Its original implementation was straightforward, but copied parts of the hierarchy, making it slow and memory-hungry. An optimised later version, which we refer to as FastTug, avoided this, but at a cost in clarity. Optimising TugGraph without sacrificing clarity is difficult because of the need to keep every hierarchy node connected, a common challenge for graph hierarchy editing algorithms. Recently, this problem has been addressed by âzippingâ algorithms, multi-level split/merge algorithms that preserve hierarchy node connectedness and can be built upon for higher-level editing. In this paper, we generalise the original unzipping algorithms to implement SimpleTug, a simple, modular version of TugGraph that is easy to understand and implement, and even faster and more memory-efficient than FastTug. We formally prove its equivalence to FastTug, and show how both can be parallelised. Using our millipede hierarchical image segmentation system, we show experimentally that both the serial and parallel versions of SimpleTug are around 25% faster than their FastTug counterparts, whilst using considerably less memory. Finally, we discuss the interesting theoretical connections between TugGraph and zipping, and suggest ideas for further work.","Graph visualisation, TugGraph, Cluster hierarchy, Zipping algorithms",S. Golodetz and A. Arnab and I.D. Voiculescu and S.A. Cameron,https://www.sciencedirect.com/science/article/pii/S0031320320300625,https://doi.org/10.1016/j.patcog.2020.107257,0031-3203,2020,107257,103,Pattern Recognition,Simplifying TugGraph using zipping algorithms,article,GOLODETZ2020107257,
"Fingerprint image enhancement is one of the fundamental modules in an automated fingerprint recognition system (AFRS). While the performance of AFRS advances with sophisticated fingerprint matching algorithms, poor fingerprint image quality remains a major issue to achieve accurate fingerprint recognition. In this paper, we present a multi-task convolutional neural network (CNN) based method to recover fingerprint ridge structures from corrupted fingerprint images. By learning from the noises and corruptions caused by various undesirable conditions of finger and sensor, the proposed CNN model consists of two streams that reconstruct the fingerprint image and orientation field simultaneously. The enhanced fingerprint is further refined using the orientation field information. Moreover, we create a deliberately corrupted fingerprint image dataset associated with ground truth images to facilitate the supervised learning of the proposed CNN model. Experimental results show significant improvement on both image quality and fingerprint matching accuracy after applying the proposed fingerprint image enhancement technique to several well-known fingerprint datasets.","Fingerprint image enhancement, Fingerprint recognition, Convolutional neural networks, Multi-task learning",Wei Jing Wong and Shang-Hong Lai,https://www.sciencedirect.com/science/article/pii/S0031320320300108,https://doi.org/10.1016/j.patcog.2020.107203,0031-3203,2020,107203,101,Pattern Recognition,Multi-task CNN for restoring corrupted fingerprint images1,article,WONG2020107203,
"Text detection is a prerequisite of text recognition, and multi-oriented text detection is a hot topic recently. The existing multi-oriented text detection methods fall short when facing two issues: 1) text scales change in a wide range, and 2) there exists the foreground-background class imbalance. In this paper, we propose a scale-robust deep multi-oriented text-detection model, which not only has the efficiency of the one-stage deep detection model, but also has the comparable accuracy of the two-stage deep text-detection model. We design the feature refining block to fuse multi-scale context features for the purpose of keeping text detection in a higher-resolution feature map. Moreover, in order to mitigate the foreground-background class imbalance, Focal Loss is adopted to up weight the hard-classified samples. Our method is implemented on four benchmark text datasets: ICDAR2013, ICDAR2015, COCO-Text and MSRA-TD500. The experimental results demonstrate that our method is superior to the existing one-stage deep text-detection models and comparable to the state-of-the-art text detection methods.","Scene text detection, One-stage, Scale robust",Yuqiang Zheng and Yuan Xie and Yanyun Qu and Xiaodong Yang and Cuihua Li and Yan Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319304807,https://doi.org/10.1016/j.patcog.2019.107180,0031-3203,2020,107180,102,Pattern Recognition,Scale robust deep oriented-text detection network,article,ZHENG2020107180,
"In this paper, we propose a Contextual Deconvolution Network (CDN) and focus on context association in decoder network. Specifically, in upsampling path, we introduce two types of contextual modules to model the interdependencies of features in channel and spatial dimensions respectively. The channel contextual module captures image-level semantic information by aggregating the feature maps across spatial dimensions, and clarifies global ambiguity of features. Meanwhile, the spatial contextual module obtains patch-level semantic context by learning a spatial weight map, and enhance the feature discrimination. We embed the two contextual modules into individual components of the decoder network, thus improving the representation power and gaining more precise segment results. Thorough evaluations are performed on four challenging datasets, i.e., PASCAL VOC 2012, ADE20K, PASCAL-Context and Cityscapes dataset. Our approach achieves competitive performance with state-of-the-art models on PASCAL VOC 2012,ADE20K and Cityscapes dataset, and new state-of-the-art performance on PASCAL-Context dataset.","Semantic segmentation, Deconvolution network, Channel contextual module, Spatial contextual module",Jun Fu and Jing Liu and Yong Li and Yongjun Bao and Weipeng Yan and Zhiwei Fang and Hanqing Lu,https://www.sciencedirect.com/science/article/pii/S0031320319304534,https://doi.org/10.1016/j.patcog.2019.107152,0031-3203,2020,107152,101,Pattern Recognition,Contextual deconvolution network for semantic segmentation,article,FU2020107152,
"Moments, as a popular class of the global invariant image descriptors, have been widely used in image analysis, pattern recognition and computer vision applications. Exponent-Fourier moments (EFMs) are a new set of orthogonal moments based on exponential functions, which are suitable for image analysis and rotation invariant pattern recognition. However, EFMs lack natively the scaling-invariant property. In addition, they always suffer from high time complexity, numerical instability, and reconstruction error, especially for higher order of moments. In this paper, we introduce a class of scaling and rotation-invariant orthogonal moments, named Log-Polar Exponent-Fourier moments (LPEFMs), by extending the classical EFMs to the log-polar coordinates. Firstly, we redefined the EFMsâ basis functions in log-polar domain instead of Cartesian/polar coordinate domain in order to obtain the scaling-invariant property. Then, we develop a new framework for computing the LPEFMs by using pseudo-polar Fourier transform and frequency domain interpolation, which result in better image representation capability, numerical stability, and computational speed. Compared with the classical EFMs, the proposed LPEFMs have four advantages, scaling invariance, speed, accuracy and stability. Theoretical analysis and simulation results are provided to validate the proposed image moment and to compare its performance with previous works.","Exponent-Fourier moments, Log-polar coordinates, Pseudo-polar Fourier transform, Frequency domain interpolation, Scaling and rotation-invariant",Hong-ying YANG and Shu-ren QI and Chao WANG and Si-bo YANG and Xiang-yang WANG,https://www.sciencedirect.com/science/article/pii/S0031320319304777,https://doi.org/10.1016/j.patcog.2019.107177,0031-3203,2020,107177,101,Pattern Recognition,Image analysis by log-polar Exponent-Fourier moments,article,YANG2020107177,
"This research paper introduces a new approach for human activity recognition from UAV-captured video sequences. The proposed approach involves two phases: an offline phase and an inference phase. A scene stabilization step is performed together with these two phases. The offline phase aims to generate the human/non-human model as well as a human activity model using a convolutional neural network. The inference phase makes use of the already generated models in order to detect humans and recognize their activities. Our main contribution lies in adapting the convolutional neural networks, normally dedicated to the classification task, to detect humans. In addition, the classification of human activities is carried out according to two scenarios: An instant classification of video frames and an entire classification of the video sequences. Relying on an experimental evaluation of the proposed methods for human detection and human activity classification on the UCF-ARG dataset, we validated not only these contributions but also the performance of our methods compared to the existing ones.","Scene stabilization, Human detection, Human activity recognition, Deep learning, Convolutional neural networks, UAV",Hazar Mliki and Fatma Bouhlel and Mohamed Hammami,https://www.sciencedirect.com/science/article/pii/S0031320319304418,https://doi.org/10.1016/j.patcog.2019.107140,0031-3203,2020,107140,100,Pattern Recognition,Human activity recognition from UAV-captured video sequences,article,MLIKI2020107140,
"Fine-grained Sketch-based Image Retrieval (FG-SBIR), which utilizes hand-drawn sketches to search the target object images, has recently drawn much attention. It is a challenging task because sketches and images belong to different modalities and sketches are highly abstract and ambiguous. Existing solutions to this problem either focus on visual comparisons between sketches and images and ignore the multimodal characteristics of annotated images, or treat the retrieval as a one-time process. In this paper, we formulate FG-SBIR as a coarse-to-fine process, and propose a Deep Cascaded Cross-modal Ranking Model (DCCRM) that can exploit all the beneficial multimodal information in sketches and annotated images and improve both the retrieval efficiency and the top-K ranked effectiveness. Our goal concentrates on constructing deep representations for sketches, images, and descriptions, and learning the optimized deep correlations across such different domains. Thus for a given query sketch, its relevant images with fine-grained instance-level similarities in a specific category can be returned, and the strict requirement of the instance-level retrieval for FG-SBIR is satisfied. Very positive results have been obtained in our experiments by using a large quantity of public data.","Fine-grained Sketch-based Image Retrieval (FG-SBIR), Deep Cascaded Cross-modal Correlation Learning, Deep Multimodal Representation, Deep Multimodal Embedding, Deep Triplet Ranking",Yanfei Wang and Fei Huang and Yuejie Zhang and Rui Feng and Tao Zhang and Weiguo Fan,https://www.sciencedirect.com/science/article/pii/S0031320319304492,https://doi.org/10.1016/j.patcog.2019.107148,0031-3203,2020,107148,100,Pattern Recognition,Deep cascaded cross-modal correlation learning for fine-grained sketch-based image retrieval,article,WANG2020107148,
"A common approach to recognition of objects in cluttered scenes is to generate hypotheses about objects present in the scene by matching local descriptors of point features. These hypotheses are then evaluated by measuring how well they explain a particular part of the scene. In this paper, we investigate an alternative approach, which is based on alignment of convex hulls of segments detected in a depth image with convex hulls of target 3D object models or their parts. This alignment is performed using the Convex Template Instance descriptor. This descriptor was originally proposed for fruit recognition and classification of segmented objects. We have adapted this approach to recognize objects in complex scenes. Furthermore, we propose a novel three-level hypothesis evaluation strategy which can be used to achieve highly efficient object recognition. The proposed approach is evaluated by comparison with nine state-of-the-art approaches using three challenging benchmark datasets.","Object recognition, Shape instance detection, Depth image analysis, Convex hull, Shape alignment",Robert Cupec and Ivan VidoviÄ and Damir Filko and Petra ÄuroviÄ,https://www.sciencedirect.com/science/article/pii/S0031320320300066,https://doi.org/10.1016/j.patcog.2020.107199,0031-3203,2020,107199,102,Pattern Recognition,Object recognition based on convex hull alignment,article,CUPEC2020107199,
"This paper presents a high discriminative texture analysis method based on the fusion of complex networks and randomized neural networks. In this approach, the input image is modeled as a complex network and its topological properties as well as the image pixels are used to train randomized neural networks to create a signature that represents the deep characteristics of the texture. The results obtained surpassed the accuracy of many methods available in the literature. This performance demonstrates that our proposed approach opens a promising source of research, which consists of exploring the synergy of neural networks and complex networks in the texture analysis field.","Randomized neural networks, Complex networks, Texture analysis, Feature extraction",Lucas C. Ribas and Jarbas Joaci de Mesquita {SÃ¡ Junior} and Leonardo F.â¯S. Scabini and Odemir M. Bruno,https://www.sciencedirect.com/science/article/pii/S0031320319304893,https://doi.org/10.1016/j.patcog.2019.107189,0031-3203,2020,107189,103,Pattern Recognition,Fusion of complex networks and randomized neural networks for texture analysis,article,RIBAS2020107189,
"Learning robust representations that allow to reliably establish relations between images is of paramount importance for virtually all of computer vision. Annotating the quadratic number of pairwise relations between training images is simply not feasible, while unsupervised inference is prone to noise, thus leaving the vast majority of these relations to be unreliable. To nevertheless find those relations which can be reliably utilized for learning, we follow a divide-and-conquer strategy: We find reliable similarities by extracting compact groups of images and reliable dissimilarities by partitioning these groups into subsets, converting the complicated overall problem into few reliable local subproblems. For each of the subsets we obtain a representation by learning a mapping to a target feature space so that their reliable relations are kept. Transitivity relations between the subsets are then exploited to consolidate the local solutions into a concerted global representation. While iterating between grouping, partitioning, and learning, we can successively use more and more reliable relations which, in turn, improves our image representation. In experiments, our approach shows state-of-the-art performance on unsupervised classification on ImageNet with 46.0% and competes favorably on different transfer learning tasks on PASCAL VOC.","Unsupervised learning, Visual representation learning, Unsupervised image classification, Mining reliable relations, Divide-and-conquer",Timo Milbich and Omair Ghori and Ferran Diego and BjÃ¶rn Ommer,https://www.sciencedirect.com/science/article/pii/S003132031930408X,https://doi.org/10.1016/j.patcog.2019.107107,0031-3203,2020,107107,102,Pattern Recognition,Unsupervised representation learning by discovering reliable image relations,article,MILBICH2020107107,
"Heterogeneous domain adaptation (HDA) aims to leverage knowledge from a source domain for helping learn an accurate model in a heterogeneous target domain. HDA is exceedingly challenging since the feature spaces of domains are distinct. To tackle this issue, we propose a unified learning framework called Discriminative Distribution Alignment (DDA) for deriving a domain-invariant subspace. The proposed DDA can simultaneously match the discriminative directions of domains, align the distributions across domains, and enhance the separability of data during adaptation. To achieve this, DDA trains an adaptive classifier by both reducing the distribution divergence and enlarging distances between class centroids. Based on the proposed DDA framework, we further develop two methods, by embedding the cross-entropy loss and squared loss into this framework, respectively. We conduct experiments on the tasks of categorization across domains and modalities. Experimental results clearly demonstrate that the proposed DDA outperforms several state-of-the-art models.","Heterogeneous domain adaptation, Subspace learning, Classifier adaptation, Distribution alignment, Discriminative embedding",Yuan Yao and Yu Zhang and Xutao Li and Yunming Ye,https://www.sciencedirect.com/science/article/pii/S0031320319304650,https://doi.org/10.1016/j.patcog.2019.107165,0031-3203,2020,107165,101,Pattern Recognition,Discriminative distribution alignment: A unified framework for heterogeneous domain adaptation,article,YAO2020107165,
"Scene recognition is currently one of the top-challenging research fields in computer vision. This may be due to the ambiguity between classes: images of several scene classes may share similar objects, which causes confusion among them. The problem is aggravated when images of a particular scene class are notably different. Convolutional Neural Networks (CNNs) have significantly boosted performance in scene recognition, albeit it is still far below from other recognition tasks (e.g., object or image recognition). In this paper, we describe a novel approach for scene recognition based on an end-to-end multi-modal CNN that combines image and context information by means of an attention module. Context information, in the shape of a semantic segmentation, is used to gate features extracted from the RGB image by leveraging on information encoded in the semantic representation: the set of scene objects and stuff, and their relative locations. This gating process reinforces the learning of indicative scene content and enhances scene disambiguation by refocusing the receptive fields of the CNN towards them. Experimental results on three publicly available datasets show that the proposed approach outperforms every other state-of-the-art method while significantly reducing the number of network parameters. All the code and data used along this paper is available at: https://github.com/vpulab/Semantic-Aware-Scene-Recognition","Scene recognition, Deep learning, Convolutional neural networks, Semantic segmentation",Alejandro LÃ³pez-Cifuentes and Marcos Escudero-ViÃ±olo and JesÃºs BescÃ³s and Ãlvaro GarcÃ­a-MartÃ­n,https://www.sciencedirect.com/science/article/pii/S0031320320300613,https://doi.org/10.1016/j.patcog.2020.107256,0031-3203,2020,107256,102,Pattern Recognition,Semantic-aware scene recognition,article,LOPEZCIFUENTES2020107256,
"In keeping with recent developments in artificial intelligence in the era of big data, there is a demand for online signature verification systems that operate at high speeds, provide a high level of security, and allow high tolerances while achieving sufficient performance. In response to these needs, the present study proposes a novel, single-template strategy using a mean template set and weighted multiple dynamic time warping (DTW) distances for a function-based approach to online signature verification. Specifically, to obtain an effective mean template for each feature while reflecting intra-user variability between all the reference samples, we adopt a novel time-series averaging method based on Euclidean barycenter-based DTW barycenter averaging. Then, by using the mean template set, we calculate multiple DTW distances from multivariate time series based on dependent and independent warping. Finally, to boost the discriminative power, we apply a weighting scheme using a gradient boosting model to efficiently combine the multiple DTW distances. Experimental results using the common SVC2004 Task1/Task2 and MCYT-100 signature datasets confirm that the proposed method is effective for online signature verification.","Biometrics, Forensics, Signature verification, Template matching, Variable importance, Gradient boosting, Dynamic time warping (DTW), Euclidean barycenter-based DTW barycenter averaging (EB-DBA)",Manabu Okawa,https://www.sciencedirect.com/science/article/pii/S0031320320300339,https://doi.org/10.1016/j.patcog.2020.107227,0031-3203,2020,107227,102,Pattern Recognition,Online signature verification using single-template matching with time-series averaging and gradient boosting,article,OKAWA2020107227,
"There are two different approaches to macro-averaging F measure for multi-label classification. The first encloses averaging F measure over all classes, which makes it easy to optimize. The second, extensively investigated in this paper, comprises the F measure of macro precision and recall calculation. We examine and compare these two measures when applied to different multi-label datasets. To optimize the performance measure, we adopt a widely known and proven modular approach. Classifiers sort the instances in descending order, according to a real-valued score of belonging to a corresponding class. After that, thresholds are selected so as to optimize the performance measure. If the number of classes is sufficiently large and the second alternative of macro-averaging F measure is employed, then it becomes non-trivial to define the optimal number of instances to assign to each class. Cyclic optimization procedure is widely used for threshold optimization although it results in a maximum in a special coordinate-wise sense. For a micro averaged F measure, such a coordinate-wise optimum is a maximum in the conventional sense of this term but it is not true for the F measure of macro precision and recall, which is shown by a counterexample. We reduce the problem of selecting the optimal threshold for each class to the problem of obtaining a fixed point of a specifically introduced transformation of a unit square. The suggested algorithm lets us localize all possible coordinate-wise maximums and detect the optimal among them. The approach is applied to datasets from diverse application domains.","Macro-averaged F measure, Multi-label classification, Optimal threshold selection, Fixed point method",Anna Berger and Sergey Guda,https://www.sciencedirect.com/science/article/pii/S003132032030056X,https://doi.org/10.1016/j.patcog.2020.107250,0031-3203,2020,107250,102,Pattern Recognition,Threshold optimization for F measure of macro-averaged precision and recall,article,BERGER2020107250,
"We consider the problem of recovering a circular arrangement of data instances with respect to some proximity measure, such that nearby instances are more similar. Applications of this problem, also referred to as circular seriation, can be found in various disciplines such as genome sequencing, data visualization and exploratory data analysis. Circular seriation can be expressed as a quadratic assignment problem, which is in general an intractable problem. Spectral-based approaches can be used to find approximate solutions, but are shown to perform well only for a specific class of data matrices. We propose a bilevel optimization framework where we employ a spherical embedding approach together with a spectral method for circular ordering in order to recover circular arrangements of the embedded data. Experiments on real and synthetic datasets demonstrate the competitive performance of the proposed method.","Combinatorial data analysis, Data sequencing, Circular seriation, Quadratic assignment problem, Spherical embeddings",Xenophon Evangelopoulos and Austin J. Brockmeier and Tingting Mu and John Y. Goulermas,https://www.sciencedirect.com/science/article/pii/S003132031930490X,https://doi.org/10.1016/j.patcog.2019.107192,0031-3203,2020,107192,103,Pattern Recognition,Circular object arrangement using spherical embeddings,article,EVANGELOPOULOS2020107192,
"Training deep convolutional neural networks (CNNs) often requires high computational cost and a large number of learnable parameters. To overcome this limitation, one solution is computing predefined convolution kernels from training data. In this paper, we propose a novel three-stage approach for filter learning alternatively. It learns filters in multiple structures including standard filters, channel-wise filters and point-wise filters which are inspired from variations of CNNsâ convolution operations. By analyzing the linear combination between learned filters and original convolution kernels in pre-trained CNNs, the reconstruction error is minimized to determine the most representative filters from the filter bank. These filters are used to build a network followed by HOG-based feature extraction for feature representation. The proposed approach shows competitive performance on color face recognition compared with other deep CNNs-based methods. Besides, it provides a perspective of interpreting CNNs by introducing the concepts of advanced convolutional layers to unsupervised filter learning.","Deep eigen-filters, Convolution kernels, Face recognition, Convolutional neural networks, Feature representation",Ming Zhang and Sheheryar Khan and Hong Yan,https://www.sciencedirect.com/science/article/pii/S0031320319304765,https://doi.org/10.1016/j.patcog.2019.107176,0031-3203,2020,107176,100,Pattern Recognition,Deep eigen-filters for face recognition: Feature representation via unsupervised multi-structure filter learning,article,ZHANG2020107176,
"Hashing methods perform the efficient nearest neighbor search by mapping high-dimensional data to binary codes. Compared to projection-based hashing methods, hashing methods that adopt the clustering technique can encode the complex relationship of the data into binary codes. However, their search performance is affected by the boundary of the cluster. Two similar data points may be assigned to two different clusters and then encoded into two much different binary codes. In this paper, we propose a new hashing method based on the clustering technique and it can alleviate the effect from the cluster boundary. It is from an observation that the relative positions of any two close data points to each cluster center are close. An alternating optimization is developed to simultaneously discover the cluster structures of the data and learn the hash functions to preserve the relative positions of the data to each cluster center. To integrate the information in each cluster, the corresponding binary code of each data point is obtained by concatenating the substrings learnt by the hash functions in each cluster. The experiments show that our method is competitive to or better than the state-of-the-art hashing methods.","Unsupervised hashing, Approximate nearest neighbor search, Clustering",Zhenyu Weng and Yuesheng Zhu,https://www.sciencedirect.com/science/article/pii/S0031320319304522,https://doi.org/10.1016/j.patcog.2019.107151,0031-3203,2020,107151,100,Pattern Recognition,Concatenation hashing: A relative position preserving method for learning binary codes,article,WENG2020107151,
"Simultaneous Localization and Mapping is the process of simultaneously creating a map of the environment while navigating in it. Most of the SLAM approaches use natural features (e.g. keypoints) that are unstable over time, repetitive in many cases or their number insufficient for a robust tracking (e.g. in indoor buildings). Other researchers, on the other hand, have proposed the use of artificial landmarks, such as squared fiducial markers, placed in the environment to help tracking and relocalization. This paper proposes a novel SLAM approach by fusing natural and artificial landmarks in order to achieve long-term robust tracking in many scenarios. Our method has been compared to the start-of-the-art methods ORB-SLAM2 [1], LDSO [2] and SPM-SLAM [3] in the public datasets Kitti [4], Euroc-MAV [5], TUM [6] and SPM [3], obtaining better precision, robustness and speed. Our tests also show that the combination of markers and keypoints achieves better accuracy than each one of them independently.","SLAM, KeyPoints, Fiducial Markers, Marker Mapping, ArUco",Rafael MuÃ±oz-Salinas and R. Medina-Carnicer,https://www.sciencedirect.com/science/article/pii/S0031320319304923,https://doi.org/10.1016/j.patcog.2019.107193,0031-3203,2020,107193,101,Pattern Recognition,UcoSLAM: Simultaneous localization and mapping by fusion of keypoints and squared planar markers,article,MUNOZSALINAS2020107193,
"This paper presents a new video super-resolution (SR) method that can generate high-quality and temporally coherent high-resolution (HR) videos. Starting from the traditional sparse reconstruction framework that works well for image SR, we improve it significantly from the following aspects to obtain an effect video SR method. Firstly, to enhance the temporal coherence between adjacent HR frames, once a HR frame is estimated, we use it to guide the sparse reconstruction of the next low-resolution frame. Secondly, instead of using just hand-craft features, we further incorporate deep features generated by VGG16 into our sparse reconstruction based video SR method. Thirdly, we constantly update the dictionary, which is the core of the sparse reconstruction, by making use of the previously estimated HR frame. Finally, after the HR video is reconstructed, we use a joint bilateral filter to post-process it to remove artifacts and transfer image details. Experiments demonstrate that the proposed four strategies effectively improve our final results. In most of the experiments of this paper, our results are better than those produced by the latest deep learning based approaches.","Video super resolution, Sparse representation, Deep features, Temporal coherence",Qiuxia Lai and Yongwei Nie and Hanqiu Sun and Qiang Xu and Zhensong Zhang and Mingyu Xiao,https://www.sciencedirect.com/science/article/pii/S0031320319304406,https://doi.org/10.1016/j.patcog.2019.107139,0031-3203,2020,107139,100,Pattern Recognition,Video super-resolution via pre-frame constrained and deep-feature enhanced sparse reconstruction,article,LAI2020107139,
"This paper introduces a robust method for semi-supervised training of deep neural networks for multi-label image classification. To this end, a ramp loss is utilized since it is more robust against noisy and incomplete image labels compared to the classic hinge loss. The proposed method allows for learning from both labeled and unlabeled data in a semi-supervised setting. This is achieved by propagating labels from the labeled images to their unlabeled neighbors in the feature space. Using a robust loss function becomes crucial here, as the initial label propagations may include many errors, which degrades the performance of non-robust loss functions. In contrast, the proposed robust ramp loss restricts extreme penalties from the samples with incorrect labels, and the label assignment improves in each iteration and contributes to the learning process. The proposed method achieves state-of-the-art results in semi-supervised learning experiments on the CIFAR-10 and STL-10 datasets, and comparable results to the state-of the-art in supervised learning experiments on the NUS-WIDE and MS-COCO datasets. Experimental results also verify that our proposed method is more robust against noisy image labels as expected.","Multi-label classification, Semi-supervised learning, Ramp loss, Image classification, Deep learning",Hakan Cevikalp and Burak Benligiray and Omer Nezih Gerek,https://www.sciencedirect.com/science/article/pii/S0031320319304649,https://doi.org/10.1016/j.patcog.2019.107164,0031-3203,2020,107164,100,Pattern Recognition,Semi-supervised robust deep neural networks for multi-label image classification,article,CEVIKALP2020107164,
"A complex multiscale network named complex Contourlet convolutional neural network (complex Contourlet-CNN) is proposed for polarimetric synthetic aperture radar (PolSAR) image classification in this paper. In order to make full use of the phase information of PolSAR image, we redefine the conventional operations of CNN in complex field, and the data sets and parameters are always expressed through the complex matrixes in complex CNN. Moreover, the multiscale deep Contourlet filter banks are constructed to extract more robust discriminative features with multidirection, multiscale, and multiresolution properties, which can improve the performance of complex CNN by replacing filters of the first complex convolutional layer with the multiscale deep Contourlet filter banks. The Contourlet transform is used for helping the complex CNN network to capture abstract features in a certain direction and frequency band. Furthermore, the proposed network based on the multiscale geometric properties of Contourlet transform can retrieve the information in the region and direction corresponding to the extracted features. Experiments on different spatial resolutions and land coverings of Flevoland, San Francisco Bay, and Germany PolSAR images show that less training data is required and the performance of the proposed explainable deep learning method is comparable to that of the existing state-of-the-art methods.","Complex Contourlet-CNN, Multiscale deep Contourlet filter banks, Polarimetric SARimage classification",Lingling Li and Liyuan Ma and Licheng Jiao and Fang Liu and Qigong Sun and Jin Zhao,https://www.sciencedirect.com/science/article/pii/S003132031930411X,https://doi.org/10.1016/j.patcog.2019.107110,0031-3203,2020,107110,100,Pattern Recognition,Complex Contourlet-CNN for polarimetric SAR image classification,article,LI2020107110,
"Visual ego-motion estimation is one of the longstanding problems which estimates the movement of cameras from images. Learning based ego-motion estimation methods have seen an increasing attention since its desirable properties of robustness to image noise and camera calibration independence. In this work, we propose a data-driven approach of learning based visual ego-motion estimation for a monocular camera. We use an end-to-end learning approach in allowing the model to learn a map from input image pairs to the corresponding ego-motion, which is parameterized as 6-DoF transformation matrix. We introduce a two-module Long-term Recurrent Convolutional Neural Networks called PoseConvGRU. The feature-encoding module encodes the short-term motion feature in an image pair, while the memory-propagating module captures the long-term motion feature in the consecutive image pairs. The visual memory is implemented with convolutional gated recurrent units, which allows propagating information over time. At each time step, two consecutive RGB images are stacked together to form a 6-channel tensor for feature-encoding module to learn how to extract motion information and estimate poses. The sequence of output maps is then passed through the memory-propagating module to generate the relative transformation pose of each image pair. In addition, we have designed a series of data augmentation methods to avoid the overfitting problem and improve the performance of the model when facing challengeable scenarios such as high-speed or reverse driving. We evaluate the performance of our proposed approach on the KITTI Visual Odometry benchmark and Malaga 2013 Dataset. The experiments show a competitive performance of the proposed method to the state-of-the-art monocular geometric and learning methods and encourage further exploration of learning-based methods for the purpose of estimating camera ego-motion even though geometrical methods demonstrate promising results.","Ego-motion, Pose estimation, Deep learning, Recurrent Convolutional Neural Networks, Data augmentation",Guangyao Zhai and Liang Liu and Linjian Zhang and Yong Liu and Yunliang Jiang,https://www.sciencedirect.com/science/article/pii/S003132031930487X,https://doi.org/10.1016/j.patcog.2019.107187,0031-3203,2020,107187,102,Pattern Recognition,PoseConvGRU: A Monocular Approach for Visual Ego-motion Estimation by Learning,article,ZHAI2020107187,
"Electrocardiogram (ECG) biometrics has recently received considerable attention and is considered to be a promising biometric trait. Although some promising results on ECG biometrics have been reported, it is still challenging to perform this technique robustly and precisely. To address these issues, this paper presents a novel ECG biometrics framework: Multi-Scale Differential Feature for ECG biometrics with Collective Matrix Factorization (CMF). First, we extract the Multi-Scale Differential Feature (MSDF) from the one-dimensional ECG signal and then fuse MSDF with 1DMRLBP to generate the MSDF-1DMRLBP, which acts as the base feature of the ECG signal. Second, to extract discriminative information from the intermediate base features, we leverage the CMF technique to generate the final robust ECG representations by simultaneously embedding MSDF-1DMRLBP and label information. Consequently, the final robust features could preserve the intra-subject and inter-subject similarities. Extensive experiments are conducted on four ECG databases, and the results demonstrate that the proposed method can outperform the state-of-the-art in terms of both accuracy and efficiency.","ECG biometrics, Multi-scale differential feature, Collective matrix factorization, Feature learning",Kuikui Wang and Gongping Yang and Yuwen Huang and Yilong Yin,https://www.sciencedirect.com/science/article/pii/S0031320320300170,https://doi.org/10.1016/j.patcog.2020.107211,0031-3203,2020,107211,102,Pattern Recognition,Multi-scale differential feature for ECG biometrics with collective matrix factorization,article,WANG2020107211,
"This paper proposes an automated visual classification framework in which a novel analysis method (LSTMS-B) of EEG signals guides the selection of multiple networks that leads to the improvement of classification performance. The method, called LSTMS-B, combines deep learning and ensemble learning to extract the category-dependent representations of EEG signals. Specifically, it introduces Swish activation function into traditional LSTM which reduces the effect of vanishing gradient and optimize the training process. Besides, the Bagging theory is applied to increase the generalization. The LSTMS-B method reaches the average precision of 97.13% for learning EEG visual presentations, which greatly outperforms traditional LSTM network and other contrast models. Then, to verify its application value, a ResNet-based regression is trained using original images and relevant EEG representations learned before. We use the output of the regression as the features to classify the images, and finally obtain the average classification accuracy of 90.16%.","Ensemble deep learning, Bagging algorithm, EEG, Automated visual classification",Xiao Zheng and Wanzhong Chen and Yang You and Yun Jiang and Mingyang Li and Tao Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319304480,https://doi.org/10.1016/j.patcog.2019.107147,0031-3203,2020,107147,102,Pattern Recognition,Ensemble deep learning for automated visual classification using EEG signals,article,ZHENG2020107147,
"Dynamic magnetic resonance imaging (DMRI) often requires a long time for measurement acquisition, and it is a crucial problem about the enhancement of reconstruction quality from a limited set of under-samples. The low-rank plus sparse decomposition model, which is also called robust principal component analysis (RPCA), is widely used for reconstruction of DMRI data in the model-based way. In this paper, considering that DMRI data are naturally in tensor form with block-wise smoothness, we propose a smooth robust tensor principal component analysis (SRTPCA) method for DMRI reconstruction. Compared with classical RPCA approaches, the low rank and sparsity terms are extended to tensor versions to fully exploit the spatial and temporal data structures. Moreover, a tensor total variation regularization term is used to encourage the multi-dimensional block-wise smoothness for the reconstructed DMRI data. The relaxed convex optimization model can be divided into several sub-problems by the alternating direction method of multipliers. Numerical experiments on cardiac perfusion and cine datasets demonstrate that the proposed SRTPCA method outperforms the state-of-the-art ones in terms of recovery accuracy.","Robust tensor principal component analysis, Compressed sensing, Low rank tensor approximation, Tensor total variation, Dynamic magnetic resonance imaging",Yipeng Liu and Tengteng Liu and Jiani Liu and Ce Zhu,https://www.sciencedirect.com/science/article/pii/S0031320320300571,https://doi.org/10.1016/j.patcog.2020.107252,0031-3203,2020,107252,102,Pattern Recognition,Smooth robust tensor principal component analysis for compressed sensing of dynamic MRI,article,LIU2020107252,
"Detecting anomalies in the data is a common machine learning task, with numerous applications in the sciences and industry. In practice, it is not always sufficient to reach high detection accuracy, one would also like to be able to understand why a given data point has been predicted to be anomalous. We propose a principled approach for one-class SVMs (OC-SVM), that draws on the novel insight that these models can be rewritten as distance/pooling neural networks. This âneuralizationâ step lets us apply deep Taylor decomposition (DTD), a methodology that leverages the model structure in order to quickly and reliably explain decisions in terms of input features. The proposed method (called âOC-DTDâ) is applicable to a number of common distance-based kernel functions, and it outperforms baselines such as sensitivity analysis, distance to nearest neighbor, or edge detection.","Outlier detection, Explainable machine learning, Deep Taylor decomposition, Kernel machines, Unsupervised learning",Jacob Kauffmann and Klaus-Robert MÃ¼ller and GrÃ©goire Montavon,https://www.sciencedirect.com/science/article/pii/S0031320320300054,https://doi.org/10.1016/j.patcog.2020.107198,0031-3203,2020,107198,101,Pattern Recognition,Towards explaining anomalies: A deep Taylor decomposition of one-class models,article,KAUFFMANN2020107198,
"Sliding window and candidate sampling are two widely used search strategies for visual object tracking, but they are far behind real-time. By treating the tracking problem as a three-step decision-making process, a novel tracking network, which explores only three small subsets of candidate regions, is developed to achieve faster (real-time) localization of the target object along the frames in a video. A convolutional neural network agent is formulated to interact with a video over time, and two action-value functions are exploited to learn a favorable policy off-line to determine the best action for visual object tracking. Our model is trained in a collaborative learning way by using action classification and cumulative reward approximation in reinforcement learning. We have evaluated our proposed tracker against a number of state-of-the-art ones over three popular tracking benchmarks including OTB-2013, OTB-2015, and VOT2017. The experimental results have demonstrated that our proposed method can achieve very competitive performance on real-time object tracking.","Object tracking, Deep Q-learning, Action search network",Zhu Teng and Baopeng Zhang and Jianping Fan,https://www.sciencedirect.com/science/article/pii/S0031320319304881,https://doi.org/10.1016/j.patcog.2019.107188,0031-3203,2020,107188,101,Pattern Recognition,Three-step action search networks with deep Q-learning for real-time object tracking,article,TENG2020107188,
"Mathematical morphology provides powerful nonlinear operators for a variety of image processing tasks such as filtering, segmentation, and edge detection. In this paper, we propose a way to use these nonlinear operators in an end-to-end deep learning framework and illustrate them on different applications. We demonstrate on various examples that new layers making use of the morphological non-linearities are complementary to convolution layers. These new layers can be used to integrate the non-linear operations and pooling into a joint operation. We finally enhance results obtained in boundary detection using this new family of layers with just 0.01% of the parameters of competing state-of-the-art methods.","Mathematical Morphology, Deep learning, Edges detection, Denoising",Gianni Franchi and Amin Fehri and Angela Yao,https://www.sciencedirect.com/science/article/pii/S0031320320300522,https://doi.org/10.1016/j.patcog.2020.107246,0031-3203,2020,107246,102,Pattern Recognition,Deep morphological networks,article,FRANCHI2020107246,
"With the success of deep learning in the field of computer vision, object recognition has made important breakthroughs, and its recognition accuracy has been drastically improved. However, the performance of scene recognition is still not sufficient to some extent because of complex configurations. Over the past several years, scene recognition algorithms have undergone important evolution as a result of the development of machine learning and Deep Convolutional Neural Networks (DCNN). This paper reviews many of the most popular and effective approaches to scene recognition, which is expected to create benefits for future research and practical applications. We seek to establish relationships among different algorithms and determine the critical components that lead to remarkable performance. Through the analysis of some representative schemes, motivation and insights are identified, which will help to facilitate the design of better recognition architectures. In addition, current available scene datasets and benchmarks are presented for evaluation and comparison. Finally, potential problems and promising directions are highlighted.","Scene recognition, Patch feature encoding, Spatial layout pattern learning, Discriminative region detection, Convolutional neural networks, Deep learning",Lin Xie and Feifei Lee and Li Liu and Koji Kotani and Qiu Chen,https://www.sciencedirect.com/science/article/pii/S003132032030011X,https://doi.org/10.1016/j.patcog.2020.107205,0031-3203,2020,107205,102,Pattern Recognition,Scene recognition: A comprehensive survey,article,XIE2020107205,
"This paper proposes an innovative object detector by leveraging deep features learned in high-level layers. Compared with features produced in earlier layers, the deep features are better at expressing semantic and contextual information. The proposed deep feature learning scheme shifts the focus from concrete features with details to abstract ones with semantic information. It considers not only individual objects and local contexts but also their relationships by building a multi-scale deep feature learning network (MDFN). MDFN efficiently detects the objects by introducing information square and cubic inception modules into the high-level layers, which employs parameter-sharing to enhance the computational efficiency. MDFN provides a multi-scale object detector by integrating multi-box, multi-scale and multi-level technologies. Although MDFN employs a simple framework with a relatively small base network (VGG-16), it achieves better or competitive detection results than those with a macro hierarchical structure that is either very deep or very wide for stronger ability of feature extraction. The proposed technique is evaluated extensively on KITTI, PASCAL VOC, and COCO datasets, which achieves the best results on KITTI and leading performance on PASCAL VOC and COCO. This study reveals that deep features provide prominent semantic information and a variety of contextual contents, which contribute to its superior performance in detecting small or occluded objects. In addition, the MDFN model is computationally efficient, making a good trade-off between the accuracy and speed.","Deep feature learning, Multi-scale, Semantic and contextual information, Small and occluded objects",Wenchi Ma and Yuanwei Wu and Feng Cen and Guanghui Wang,https://www.sciencedirect.com/science/article/pii/S0031320319304509,https://doi.org/10.1016/j.patcog.2019.107149,0031-3203,2020,107149,100,Pattern Recognition,MDFN: Multi-scale deep feature learning network for object detection,article,MA2020107149,
"The past decade has witnessed the success of transformed domain methods for image saliency prediction. However, it is intractable to develop a transformed domain method for video saliency prediction, due to the limited choices on spatio-temporal transforms. In this paper, we propose learning the transform from training data, rather than the predefined transform in the existing methods. Specifically, we develop a novel deep Complex-valued network with learnable Transform (DeepCT) for video saliency prediction. The architecture of DeepCT includes the Complex-valued Transform Module (CTM), inverse CTM (iCTM) and Complex-valued Stacked Convolutional Long Short-Term Memory network (CS-ConvLSTM). In the CTM and iCTM, multi-scale pyramid structures are introduced, as we find that transforms at multiple receptive scales can improve the accuracy of saliency prediction. To make the CTM and iCTM âinvertibleâ, we further propose the cycle consistency loss in training DeepCT, which is composed of frame reconstruction loss and complex feature reconstruction loss. Additionally, the CS-ConvLSTM is developed to learn the temporal saliency transition across video frames. Finally, the experimental results show that our DeepCT method outperforms other 13 state-of-the-art methods for video saliency prediction.","Saliency prediction, Complex-valued network, Learnable transform, Convolutional LSTM",Lai Jiang and Mai Xu and Shanyi Zhang and Leonid Sigal,https://www.sciencedirect.com/science/article/pii/S0031320320300406,https://doi.org/10.1016/j.patcog.2020.107234,0031-3203,2020,107234,102,Pattern Recognition,DeepCT: A novel deep complex-valued network with learnable transform for video saliency prediction,article,JIANG2020107234,
"Calligraphy imitation (CI) from a handful of target handwriting samples is such a challenging task that most of the existing writing style analysis or handwriting generation methods do not exhibit satisfactory performance. In this paper, we propose a novel multi-module framework to address the problem of CI. Firstly, we utilized a deep convolution neural network (CNN) to extract personalized calligraphical features. Then we built a calligraphy-clustering attention module and a mata-style matrix (msM) to compute an embedding of calligraphy. The structure of conditional gated recurrent unit (cGRU) is then improved to predict the probabilistic density of pen tip movement displacement by dual condition inputs. Finally, we generated personalized handwriting stroke sequences through iterative sampling with Gaussian mixture model (GMM). Experiments on public online handwriting databases verify that the proposed method could achieve satisfactory performance; the generated samples achieved high similarities with original handwriting examples.","Calligraphy imitation, Attention, Mata-style matrix, Condition gated recurrent unit",Bocheng Zhao and Jianhua Tao and Minghao Yang and Zhengkun Tian and Cunhang Fan and Ye Bai,https://www.sciencedirect.com/science/article/pii/S0031320319303814,https://doi.org/10.1016/j.patcog.2019.107080,0031-3203,2020,107080,104,Pattern Recognition,Deep imitator: Handwriting calligraphy imitation via deep attention networks,article,ZHAO2020107080,
"We study the problem of unsupervised domain adaptive re-identification (re-ID) which is an active topic in computer vision but lacks a theoretical foundation. We first extend existing unsupervised domain adaptive classification theories to re-ID tasks. Concretely, we introduce some assumptions on the extracted feature space and then derive several loss functions guided by these assumptions. To optimize them, a novel self-training scheme for unsupervised domain adaptive re-ID tasks is proposed. It iteratively makes guesses for unlabeled target data based on an encoder and trains the encoder based on the guessed labels. Extensive experiments on unsupervised domain adaptive person re-ID and vehicle re-ID tasks with comparisons to the state-of-the-arts confirm the effectiveness of the proposed theories and self-training framework. Our code is available on https://github.com/LcDog/DomainAdaptiveReID.","Person re-identification, Unsupervised domain adaptation",Liangchen Song and Cheng Wang and Lefei Zhang and Bo Du and Qian Zhang and Chang Huang and Xinggang Wang,https://www.sciencedirect.com/science/article/pii/S003132031930473X,https://doi.org/10.1016/j.patcog.2019.107173,0031-3203,2020,107173,102,Pattern Recognition,Unsupervised domain adaptive re-identification: Theory and practice,article,SONG2020107173,
"This work describes an investigation into the relationship between the visual impressions of a number of facial images each described by a set of parameters related to the position and size of discrete components: the eyes, the nose and the lips. Observations were made by members of two ethnic groups, Chinese and Pakistani, and the images comprised female faces from the same two groups. The observers used 16 impression scales to assess each image and a forced-choice scaling technique. From the factor analysis, the results showed that three visual impressions, attractive, feminine and mature, can well represent all the 16 scales. In the second experiment, the observers assessed visual impressions of more images using only these three impressions. Data are presented relating to the differences between the observations for the various facial locations, as well as between observers from different ethnic groups. Models have been developed that describe the data and their predictions outperformed traditional models, i.e.symmetry, golden ratio and neoclassical canons. The differences between the results of the two ethnic groups were found to be small; there were however, some significant differences in the responses to different facial features.","Visual impression, Facial features, Symmetry, Golden ratio, Neoclassical canons, Ethnic group difference, , ,",Muhammad Farhan Mughal and Ming Ronnier Luo and Michael Pointer,https://www.sciencedirect.com/science/article/pii/S0031320320300649,https://doi.org/10.1016/j.patcog.2020.107259,0031-3203,2020,107259,103,Pattern Recognition,Modelling visual impressions for Chinese and Pakistani ethnic groups,article,MUGHAL2020107259,
"Learning from imbalanced data is a challenging problem in many real-world machine learning applications due in part to the bias of performance in most classification systems. This bias may exist due to three reasons: (1) Classification systems are often optimized and compared using performance measurements that are unsuitable for imbalance problems; (2) most learning algorithms are designed and tested on a fixed imbalance level of data, which may differ from operational scenarios; (3) the preference of correct classification of classes is different from one application to another. This paper investigates specialized performance evaluation metrics and tools for imbalance problem, including scalar metrics that assume a given operating condition (skew level and relative preference of classes), and global evaluation curves or metrics that consider a range of operating conditions. We focus on the case in which the scalar metric F-measure is preferred over other scalar metrics, and propose a new global evaluation space for the F-measure that is analogous to the cost curves for expected cost. In this space, a classifier is represented as a curve that shows its performance over all of its decision thresholds and a range of possible imbalance levels for the desired preference of true positive rate to precision. Curves obtained in the F-measure space are compared to those of existing spaces (ROC, precision-recall and cost) and analogously to cost curves. The proposed F-measure space allows to visualize and compare classifiersâ performance under different operating conditions more easily than in ROC and precision-recall spaces. This space allows us to set the optimal decision threshold of a soft classifier and to select the best classifier among a group. This space also allows to empirically improve the performance obtained with ensemble learning methods specialized for class imbalance, by selecting and combining the base classifiers for ensembles using a modified version of the iterative Boolean combination algorithm that is optimized using the F-measure instead of AUC. Experiments on a real-world dataset for video face recognition show the advantages of evaluating and comparing different classifiers in the F-measure space versus ROC, precision-recall, and cost spaces. In addition, it is shown that the performance evaluated using the F-measure of Bagging ensemble method can improve considerably by using the modified iterative Boolean combination algorithm.","Pattern classification, Class imbalance, Performance metrics, F-measure, Visualization tools, Video face recognition",Roghayeh Soleymani and Eric Granger and Giorgio Fumera,https://www.sciencedirect.com/science/article/pii/S0031320319304479,https://doi.org/10.1016/j.patcog.2019.107146,0031-3203,2020,107146,100,Pattern Recognition,F-measure curves: A tool to visualize classifier performance under imbalance,article,SOLEYMANI2020107146,
"In most of the real world datasets, there is an imbalance in the number of samples belonging to different classes. Various pattern classification problems such as fault or disease detection involve class imbalanced data. The support vector machine (SVM) classifier becomes biased towards the majority class due to class imbalance. Moreover, in the existing SVM based techniques for class imbalance, there is no information about the distribution of data. Motivated by the idea of prior information about data distribution, a reduced universum twin support vector machine for class imbalance learning (RUTSVM-CIL) is proposed in this paper. For the first time, universum learning is incorporated with SVM to solve the problem of class imbalance. Oversampling and undersampling of data is performed to remove the imbalance in the classes. The universum data points are used to give prior information about the data. To reduce the computation time of our universum based algorithm, we use a small sized rectangular kernel matrix. The reduced kernel matrix needs less storage space, and thus applicable for large scale imbalanced datasets. Comprehensive experimentation is performed on various synthetic, real world and large scale imbalanced datasets. In comparison to the existing approaches for class imbalance, the proposed RUTSVM-CIL gives better generalization performance for most of the benchmark datasets. Also, the computation cost of RUTSVM-CIL is very less, making it suitable for real world applications.","Universum, Rectangular kernel, Class imbalance, Imbalance ratio, Twin support vector machine",B. Richhariya and M. Tanveer,https://www.sciencedirect.com/science/article/pii/S0031320319304510,https://doi.org/10.1016/j.patcog.2019.107150,0031-3203,2020,107150,102,Pattern Recognition,A reduced universum twin support vector machine for class imbalance learning,article,RICHHARIYA2020107150,
"Previous spectral clustering methods sequentially conduct three steps, i.e.,Â similarity matrix learning from original data, spectral representation learning, and K-means clustering on spectral representation, respectively, to difficultly output robust clustering result even though each of three steps achieves individual optimization. The reason is that each goal of former two steps is not focused on achieving optimal clustering result. Moreover, original data usually contains noise to affect the clustering result, as well as has high-dimensional representation to easily result in the curse of dimensionality. In this paper, we propose a deep spectral clustering method which embeds four parts (i.e.,Â similarity matrix learning, spectral representation learning, optimized K-means clustering, and transformation matrix learning) in a unified framework with the following advantages: 1) similarity matrix is obtained from the low-dimensional feature space of original data where the influence of both noise and high-dimensional data are considered; 2) optimized K-means clustering rotates original result of K-means clustering to search optimized clustering hyperplane which partitions data points into clusters; and 3) each of four parts is iteratively updated so that the clustering result is obtained based on the feedback of other three parts. As a result, our proposed framework develops a two-task deep clustering model with linear activation functions to output effective clustering result. Experimental results on real data sets show the effectiveness of our method in terms of four clustering evaluation metrics, compared to state-of-the-art clustering methods.","Similarity matrix learning, Spectral clustering, One-step clustering, Alternating direction method of multipliers",Xiaofeng Zhu and Yonghua Zhu and Wei Zheng,https://www.sciencedirect.com/science/article/pii/S0031320319304753,https://doi.org/10.1016/j.patcog.2019.107175,0031-3203,2020,107175,105,Pattern Recognition,Spectral rotation for deep one-step clustering,article,ZHU2020107175,
"In this work, we focus on solving four standard inverse problems in imaging â denoising, deblurring, super-resolution, and reconstruction. All these problems are usually associated with image acquisition. Traditionally, signal processing techniques are used to solve such problems. However, such techniques are computationally expensive. In many applications today, when the requirement is to compute on the edge, such expensive inversion techniques are not viable solutions. Thus, one resorts to machine learning based solutions. In the past few years, variants of neural networks were developed to âlearnâ the inversion operation. Although the learning was computationally expensive, the learnt model was light-weight and portable; suitable for deployment on leaner platforms. This work is based on the same concept, however, instead of using neural networks we treat inversion as a domain adaptation problem â a transfer from corrupted space to clean space. Our work is based on the developing field of coupled representation learning. We propose a deep version of the well known coupled dictionary learning technique; but instead of learning a single layer of dictionary, we learn multiple layers. Experimental results on standard datasets against state-of-the-art solutions for each problem, show that our method yields the best results both in terms of accuracy and speed.","Dictionary learning, Deep learning, Domain adaptation, Inverse problems",Vanika Singhal and Angshul Majumdar,https://www.sciencedirect.com/science/article/pii/S0031320319304637,https://doi.org/10.1016/j.patcog.2019.107163,0031-3203,2020,107163,100,Pattern Recognition,A domain adaptation approach to solve inverse problems in imaging via coupled deep dictionary learning,article,SINGHAL2020107163,
"Recent works have shown that deep networks can be trained for optical flow estimation without supervision. Based on the photometric constancy assumption, most of these methods adopt the reconstruction loss as the supervision by point-based backward warping. Inspired by the traditional patch matching based approaches, we propose a patch-based consistency to improve the vanilla unsupervised learning method Ren etÂ al. [1]. Instead of only comparing the corresponding pixel intensity, we locate the correspondence by using the image patches with census transform, which is more robust for the illumination variation and occlusion. Moreover, a novel parallel branch is devised to estimate a soft occlusion mask jointly in an unsupervised way. The mask is adopted to weight our patch-based consistency loss to alleviate the influence of the occlusion. The plenty of experiments have been implemented on Flying Chairs, KITTI and MPI-Sintel benchmarks. The results show that our method is efficient and outperforms the peer unsupervised learning methods that are using the FlowNet-liked network.","Patch consistency, Optical flow estimation, Occlusion estimation, Unsupervised learning, Deep learning",Zhe Ren and Junchi Yan and Xiaokang Yang and Alan Yuille and Hongyuan Zha,https://www.sciencedirect.com/science/article/pii/S0031320319304911,https://doi.org/10.1016/j.patcog.2019.107191,0031-3203,2020,107191,103,Pattern Recognition,Unsupervised learning of optical flow with patch consistency and occlusion estimation,article,REN2020107191,
"Density-based clustering has several desirable properties, such as the abilities to handle and identify noise samples, discover clusters of arbitrary shapes, and automatically discover of the number of clusters. Identifying the core samples within the dense regions of a dataset is a significant step of the density-based clustering algorithm. Unlike many other algorithms that estimate the density of each samples using different kinds of density estimators and then choose core samples based on a threshold, in this paper, we present a novel approach for identifying local high-density samples utilizing the inherent properties of the nearest neighbor graph (NNG). After using the density estimator to filter noise samples, the proposed algorithm ADBSCAN in which âAâ stands for âAdaptiveâ performs a DBSCAN-like clustering process. The experimental results on artificial and real-world datasets have demonstrated the significant performance improvement over existing density-based clustering algorithms.","Density-based clustering, Nearest neighbor graph, DBSCAN",Hao Li and Xiaojie Liu and Tao Li and Rundong Gan,https://www.sciencedirect.com/science/article/pii/S0031320320300121,https://doi.org/10.1016/j.patcog.2020.107206,0031-3203,2020,107206,102,Pattern Recognition,A novel density-based clustering algorithm using nearest neighbor graph,article,LI2020107206,
"Many of the state-of-the-art methods can only localize scene texts with rotated rectangle boundaries, which may result in incorrect rectification of the detected scene texts and erroneous elimination of proposals or detections during non-maximum suppression (NMS). A few existing methods that can detect scene texts with quadrilateral boundaries, are just based on one-stage architectures or sliding windows scanning and thus have sub-optimal performance. To address these problems, we propose an end-to-end two-stage network architecture for scene text detection, which can accurately localize scene texts with quadrilateral boundaries. At the first stage, we propose a quadrilateral region proposal network (QRPN) for generating quadrilateral proposals, based on a newly proposed quadrilateral regression algorithm. At the second stage, we introduce a novel weighted RoI pooling module with learned weight masks to pool the features, and then classify the proposals and refine their shapes with the proposed quadrilateral regression algorithm again. Specially, during training, we adopt a dual-branch structure of detection heads, that is, jointly train the quadrilateral detection head and an additional rotated rectangle detection head. Furthermore, we develop an accelerated NMS algorithm with O(nlogn) complexity, for redundant quadrilateral text proposals and detections eliminating during the first and the second stage, respectively. Experiments on several challenging benchmarks demonstrate the superior performance of the proposed method, which achieves state-of-the-art results on widely used benchmarks ICDAR 2017 MLT, RCTW, and ICDAR 2015 Incidental Scene Text benchmark.","Scene text detection, Deep learning, Quadrilateral regression",Siwei Wang and Yudong Liu and Zheqi He and Yongtao Wang and Zhi Tang,https://www.sciencedirect.com/science/article/pii/S0031320320300364,https://doi.org/10.1016/j.patcog.2020.107230,0031-3203,2020,107230,102,Pattern Recognition,A quadrilateral scene text detector with two-stage network architecture,article,WANG2020107230,
"Long short-term memory (LSTM) is a type of recurrent neural networks that is efficient for encoding spatio-temporal features in dynamic sequences. Recent work has shown that the LSTM retains information related to the mode of variation in the input dynamic sequence which reduces the discriminability of the encoded features. To encode features robust to unseen modes of variation, we devise an LSTM adaptation named attentive mode variational LSTM. The proposed attentive mode variational LSTM utilizes the concept of attention to separate the input dynamic sequence into two parts: (1) task-relevant dynamic sequence features and (2) task-irrelevant static sequence features. The task-relevant dynamic features are used to encode and emphasize the dynamics in the input sequence. The task-irrelevant static sequence features are utilized to encode the mode of variation in the input dynamic sequence. Finally, the attentive mode variational LSTM suppresses the effect of mode variation with a shared output gate and results in a spatio-temporal feature robust to unseen variations. The effectiveness of the proposed attentive mode variational LSTM has been verified using two tasks: facial expression recognition and human action recognition. Comprehensive and extensive experiments have verified that the proposed method encodes spatio-temporal features robust to variations unseen during the training.","Long short-term memory, Recurrent neural networks, Attention, Robust features, Modes of variation, Facial expression recognition, Human action recognition",Wissam J. Baddar and Yong Man Ro,https://www.sciencedirect.com/science/article/pii/S0031320319304595,https://doi.org/10.1016/j.patcog.2019.107159,0031-3203,2020,107159,100,Pattern Recognition,Encoding features robust to unseen modes of variation with attentive long short-term memory,article,BADDAR2020107159,
"Hyper-parameter optimization is a process to find suitable hyper-parameters for predictive models. It typically incurs highly demanding computational costs due to the need of the time-consuming model training process to determine the effectiveness of each set of candidate hyper-parameter values. A priori, there is no guarantee that hyper-parameter optimization leads to improved performance. In this work, we propose a framework to address the problem of whether one should apply hyper-parameter optimization or use the default hyper-parameter settings for traditional classification algorithms. We implemented a prototype of the framework, which we use a basis for a three-fold evaluation with 486 datasets and 4 algorithms. The results indicate that our framework is effective at supporting modeling tasks in avoiding adverse effects of using ineffective optimizations. The results also demonstrate that incrementally adding training datasets improves the predictive performance of framework instantiations and hence enables âlife-long learning.â","Hyper-parameter optimization, Framework, Bayesian optimization, Machine learning, Incremental learning",Ngoc Tran and Jean-Guy Schneider and Ingo Weber and A.K. Qin,https://www.sciencedirect.com/science/article/pii/S0031320320300510,https://doi.org/10.1016/j.patcog.2020.107245,0031-3203,2020,107245,103,Pattern Recognition,Hyper-parameter optimization in classification: To-do or not-to-do,article,TRAN2020107245,
"Blind or No reference quality evaluation is a challenging issue since it is done without access to the original content. In this work, we propose a method based on deep learning for the mesh visual quality assessment without reference. For a given 3D model, we first compute its mesh saliency. Then, we extract views from the 3D mesh and the corresponding mesh saliency. After that, the views are split into small patches that are filtered using a saliency threshold. Only the salient patches are selected and used as input data. After that, three pre-trained deep convolutional neural networks are employed for feature learning: VGG, AlexNet, and ResNet. Each network is fine-tuned and produces a feature vector. The Compact Multi-linear Pooling (CMP) is used afterward to fuse the retrieved vectors into a global feature representation. Finally, fully connected layers followed by a regression module are used to estimate the quality score. Extensive experiments are executed on four mesh quality datasets and comparisons with existing methods demonstrate the effectiveness of our method in terms of correlation with subjective scores.","Blind mesh quality assessment, Convolutional neural network, Fine-tuning, Compact multi-linear pooling, Visual saliency",Ilyass Abouelaziz and Aladine Chetouani and Mohammed {El Hassouni} and Longin Jan Latecki and Hocine Cherifi,https://www.sciencedirect.com/science/article/pii/S0031320319304741,https://doi.org/10.1016/j.patcog.2019.107174,0031-3203,2020,107174,100,Pattern Recognition,No-reference mesh visual quality assessment via ensemble of convolutional neural networks and compact multi-linear pooling,article,ABOUELAZIZ2020107174,
"The boosting on the need of security notably increased the amount of possible facial recognition applications, especially due to the success of the Internet of Things (IoT) paradigm. However, although handcrafted and deep learning-inspired facial features reached a significant level of compactness and expressive power, the facial recognition performance still suffers from intra-class variations such as ageing, facial expressions, lighting changes, and pose. These variations cannot be captured in a single acquisition and require multiple acquisitions of long duration, which are expensive and need a high level of collaboration from the users. Among others, self-update algorithms have been proposed in order to mitigate these problems. Self-updating aims to add novel templates to the usersâ gallery among the inputs submitted during system operations. Consequently, computational complexity and storage space tend to be among the critical requirements of these algorithms. The present paper deals with the above problems by a novel template-based self-update algorithm, able to keep over time the expressive power of a limited set of templates stored in the system database. The rationale behind the proposed approach is in the working hypothesis that a dominating mode characterises the featuresâ distribution given the client. Therefore, the key point is to select the best templates around that mode. We propose two methods, which are tested on systems based on handcrafted features and deep-learning-inspired autoencoders at the state-of-the-art. Three benchmark data sets are used. Experimental results confirm that, by effective and compact feature sets which can support our working hypothesis, the proposed classification-selection approaches overcome the problem of manual updating and, in case, stringent computational requirements.","Self-update, Face recognition, Adaptive systems",Giulia OrrÃ¹ and Gian Luca Marcialis and Fabio Roli,https://www.sciencedirect.com/science/article/pii/S0031320319304224,https://doi.org/10.1016/j.patcog.2019.107121,0031-3203,2020,107121,100,Pattern Recognition,A novel classification-selection approach for the self updating of template-based face recognition systems,article,ORRU2020107121,
"In this paper, a new submodule clustering method for imaging (2-D) data is proposed. Unlike most existing clustering methods that first convert such data into vectors as preprocessing, the proposed method arranges the data samples as lateral slices of a third-order tensor. Our algorithm is based on the union-of-free-submodules model and the samples are represented using t-product in the third-order tensor space. First, we impose a low-rank constraint on the representation tensor to capture the principle information of data. By incorporating manifold regularization into the tensor factorization, the proposed method explicitly exploits the local manifold structure of data. Meanwhile, a segmentation dependent term is employed to integrate the two pipeline steps of affinity learning and spectral clustering into a unified optimization framework. The proposed method can be efficiently solved based on the alternating direction method of multipliers and spectral clustering. Finally, a nonlinear extension is proposed to handle data drawn from a mixture of nonlinear manifolds. Extensive experimental results on five real-world image datasets confirm the effectiveness of the proposed methods.","Clustering, Kernel methods, Manifold regularization, Submodule clustering, Tensor nuclear norm, Union of free submodules",Tong Wu,https://www.sciencedirect.com/science/article/pii/S0031320319304467,https://doi.org/10.1016/j.patcog.2019.107145,0031-3203,2020,107145,100,Pattern Recognition,Graph regularized low-rank representation for submodule clustering,article,WU2020107145,
"Local reference frames (LRFs) have been widely used for 3D local surface description. In this work, we propose a repeatable LRF with strong robustness to different nuisances. Different from existing LRF methods, the proposed LRF uses a part of neighboring points within the support region to calculate the z-axis, and performs an effective feature transformation on the neighboring points to define the x-axis. Specifically, feature transformation is applied to the data on a projection plane based on three point distribution characteristics via weighted strategies. These characteristics include the z-height, the distance to the center and the average length to 1-ring neighbors, covariance analysis is then applied to the transformed points to obtain the eigenvector with the largest eigenvalue, which points towards the maximum variance direction. Using a sign disambiguation technique, the modified eigenvector is used to define the final x-axis. Furthermore, a scale strategy is proposed to improve the robustness of the LRF with respect to mesh decimation. The proposed LRF was rigorously tested on six public benchmark datasets consisting of three different application contexts, i.e., 3D shape retrieval, 3D object recognition and registration. Experiments show that our method achieves significantly higher repeatability and stronger robustness than the state-of-the-arts under Gaussian noise, shot noise and mesh resolution variation. Finally, the descriptor matching results on four typical datasets further demonstrate the effectiveness of our LRF.","Local reference frame, 3D Surface description, Feature transformation, Local characteristics, Scale strategy",Sheng Ao and Yulan Guo and Jindong Tian and Yong Tian and Dong Li,https://www.sciencedirect.com/science/article/pii/S0031320319304868,https://doi.org/10.1016/j.patcog.2019.107186,0031-3203,2020,107186,100,Pattern Recognition,A repeatable and robust local reference frame for 3D surface matching,article,AO2020107186,
"Estimating pixel-wise surface normal from a single image is a challenging task but offers great values to computer vision and robotics applications. By using the spectrally and spatially variant illumination, multispectral photometric stereo can produce pixel-wise surface normal from just one image. But multispectral photometric stereo methods may encounter the tangle problem of illumination, surface reflectance and camera response, which lead to an under-determined system. Existing approaches rely on either extra depth information or material calibration strategies, assuming a Lambertian surface condition which limits their application in practical systems. Previous learning-based methods employ fully-connected or CNN architectures to estimate surface normal. Compared with fully-connected framework, CNN takes advantage of the information embedded in the neighborhood of a surface point, but losing high-frequency surface normal details. In this paper, we present a new method that addresses this task by designing two stacked deep network. We first apply a CNN-based structural cue network to approximate coarse surface normal on small patches. Then, we use a pixel level fully-connected photometric cue network to further refine surface normal details and correct errors from the first step. The fused network is robust to non-Lambertian surfaces and complex illumination environments, such as ambient light and variant light directions. Experimental results show that our dual-cue fused network outperforms existing methods.","Multispectral photometric stereo, Normal estimation, Deep neural networks, Networks fusion",Yakun Ju and Xinghui Dong and Yingyu Wang and Lin Qi and Junyu Dong,https://www.sciencedirect.com/science/article/pii/S0031320319304625,https://doi.org/10.1016/j.patcog.2019.107162,0031-3203,2020,107162,100,Pattern Recognition,A dual-cue network for multispectral photometric stereo,article,JU2020107162,
"Band selection plays an important role in hyperspectral imaging for reducing the data and improving the efficiency of data acquisition and analysis whilst significantly lowering the cost of the imaging system. Without the category labels, it is challenging to select an effective and low-redundancy band subset. In this paper, a new unsupervised band selection algorithm is proposed based on a new band search criterion and an improved Determinantal Point Processes (DPP). First, to preserve the original information of hyperspectral image, a novel band search criterion is designed for searching the bands with high information entropy and low noise. Unfortunately, finding the optimal solution based on the search criteria to select a low-redundancy band subset is a NP-hard problem. To solve this problem, we consider the correlation of bands from both original hyperspectral image and its spatial information to construct a double-graph model to describe the relationship between spectral bands. Besides, an improved DPP algorithm is proposed for the approximate search of a low-redundancy band subset from the double-graph model. Experiment results on several well-known datasets show that the proposed optical band selection algorithm achieves better performance than many other state-of-the-art methods.","Hyperspectral images (HSI), Unsupervised band selection, Maximum information and minimum noise (MIMN) criterion, Determinantal point processes (DPP)",Weizhao Chen and Zhijing Yang and Jinchang Ren and Jiangzhong Cao and Nian Cai and Huimin Zhao and Peter Yuen,https://www.sciencedirect.com/science/article/pii/S0031320320300194,https://doi.org/10.1016/j.patcog.2020.107213,0031-3203,2020,107213,102,Pattern Recognition,MIMN-DPP: Maximum-information and minimum-noise determinantal point processes for unsupervised hyperspectral band selection,article,CHEN2020107213,
"Domain adaptation studies how to build a robust model to solve pattern recognition problems when training in a source domain while testing in a related but different target domain. The existing methods focus on how to shorten the distance between the two domains, however, they have limited considerations on the preservation of data structures. In this paper, we propose a novel model for unsupervised domain adaptation. For the reduction of domain discrepancy, we propose modified Aâdistance, which is computationally fast and can be optimized using gradient information. Moreover, in order to capture the internal structures of target samples, within-domain normalization based sparse filtering is raised, which proved to be more powerful for domain adaptation. Extensive experiments compared to both shallow and deep methods demonstrate the effectiveness of our approach.","Domain adaptation, distance, Sparse filtering",Chao Han and Yu Lei and Yu Xie and Deyun Zhou and Maoguo Gong,https://www.sciencedirect.com/science/article/pii/S0031320320300595,https://doi.org/10.1016/j.patcog.2020.107254,0031-3203,2020,107254,104,Pattern Recognition,Visual domain adaptation based on modified Aâdistance and sparse filtering,article,HAN2020107254,
"Estrogen and progesterone receptors serve as an important predictive and prognostic biomarkers for breast cancer immunohistological analysis. For breast cancer prognosis, pathologists manually compute the score based on the visual expression and the number of immunopositive and immunonegative nuclei. This manual scoring technique is time-consuming, cumbersome, expensive, error-prone, and susceptible to intra- and interobserver ambiguities. To solve these issues, we proposed a deep neural network (i.e., HscoreNet), which consists of three parts, i.e., encoder, decoder, and scoring layer. A total of 600 (300 ER and 300 PR) regions of interest at 40Â ÃÂ  magnification from 100 histologically confirmed slides were used in this study. The size of each region of interest was 2048 Â ÃÂ  1536 pixels (width Â ÃÂ  height). The encoder layer has been used to transform input pixels into a lower-dimensional representation, whereas the decoder reconstructs the output of the encoder through minimization of a cost function. The decoder generates an image that only contains immunopositive and immunonegative nuclei. The output of the decoder is fed to the input of the scoring layer. This layer computes the Histo-score or H-score based on the staining intensity, the color expression, and the number of immunopositive and immunonegative nuclei. Pathologists compute this score to subcategorize the cancer grades and to decide proper treatment procedures. Our proposed approach is affordable, accurate, and fast. We achieved excellent performance, with 95.87% precision and 94.53% classification accuracy. Our proposed approach streamlines the human error-prone and time-consuming process. This methodology can also be used for other types of histology and immunohistology image segmentation and scoring.","Breast, Estrogen, Progesterone, Encoder, Decoder",Monjoy Saha and Indu Arun and Rosina Ahmed and Sanjoy Chatterjee and Chandan Chakraborty,https://www.sciencedirect.com/science/article/pii/S0031320320300078,https://doi.org/10.1016/j.patcog.2020.107200,0031-3203,2020,107200,102,Pattern Recognition,HscoreNet: A Deep network for estrogen and progesterone scoring using breast IHC images,article,SAHA2020107200,
"The segmentation of video, or separating out objects in the foreground, is an important application of pattern recognition and computer vision. Segmentation errors in pattern recognition approaches mainly come from difficulties in selecting maximally informative frames for learning. In this paper, we develop an approach to video segmentation that relies on temporal features by modeling the uncertainty of the distribution of different feature mask forms. We use those uncertainty values for unsupervised active learning. We evaluate our approach on the DAVIS16 annotated video data set and Shining3D dental video data set, and the results show our approach to be more accurate than other video segmentation approaches.","Video segmentation, Deep learning, Computer vision",Yan Tian and Guohua Cheng and Judith Gelernter and Shihao Yu and Chao Song and Bailin Yang,https://www.sciencedirect.com/science/article/pii/S0031320319304583,https://doi.org/10.1016/j.patcog.2019.107158,0031-3203,2020,107158,100,Pattern Recognition,Joint temporal context exploitation and active learning for video segmentation,article,TIAN2020107158,
"Efficient model inference is an important and practical issue in the deployment of deep neural networks on resource constraint platforms. Network quantization addresses this problem effectively by leveraging low-bit representation and arithmetic that could be conducted on dedicated embedded systems. In the previous works, the parameter bitwidth is set homogeneously and there is a trade-off between superior performance and aggressive compression. Actually, the stacked network layers, which are generally regarded as hierarchical feature extractors, contribute diversely to the overall performance. For a well-trained neural network, the feature distributions of different categories are organized gradually as the network propagates forward. Hence the capability requirement on the subsequent feature extractors is reduced. It indicates that the neurons in posterior layers could be assigned with lower bitwidth for quantized neural networks. Based on this observation, a simple yet effective mixed-precision quantized neural network with progressively decreasing bitwidth is proposed to improve the trade-off between accuracy and compression. Extensive experiments on typical network architectures and benchmark datasets demonstrate that the proposed method could achieve better or comparable results while reducing the memory space for quantized parameters by more than 25% in comparison with the homogeneous counterparts. In addition, the results also demonstrate that the higher-precision bottom layers could boost the 1-bit network performance appreciably due to a better preservation of the original image information while the lower-precision posterior layers contribute to the regularization of kâbit networks.","Model compression, Quantized neural networks, Mixed-precision",Tianshu Chu and Qin Luo and Jie Yang and Xiaolin Huang,https://www.sciencedirect.com/science/article/pii/S0031320320304507,https://doi.org/10.1016/j.patcog.2020.107647,0031-3203,2021,107647,111,Pattern Recognition,Mixed-precision quantized neural networks with progressively decreasing bitwidth,article,CHU2021107647,
"Nonnegative matrix factorization (NMF) is a powerful dimension reduction method, and has received increasing attention in various practical applications. However, most traditional NMF based algorithms are sensitive to noisy data, or fail to fully utilize the limited supervised information. In this paper, a novel robust semi-supervised NMF method, namely correntropy based semi-supervised NMF (CSNMF), is proposed to solve these issues. Specifically, CSNMF adopts a correntropy based loss function instead of the squared Euclidean distance (SED) in constrained NMF to suppress the influence of non-Gaussian noise or outliers contaminated in real world data, and simultaneously uses two types of supervised information, i.e., the pointwise and pairwise constraints, to obtain the discriminative data representation. The proposed method is analyzed in terms of convergence, robustness and computational complexity. The relationships between CSNMF and several previous NMF based methods are also discussed. Extensive experimental results show the effectiveness and robustness of CSNMF in image clustering tasks, compared with several state-of-the-art methods.","Nonnegative matrix factorization, Supervised information, Correntropy, Outliers, Image clustering",Siyuan Peng and Wee Ser and Badong Chen and Zhiping Lin,https://www.sciencedirect.com/science/article/pii/S0031320320304866,https://doi.org/10.1016/j.patcog.2020.107683,0031-3203,2021,107683,111,Pattern Recognition,Robust semi-supervised nonnegative matrix factorization for image clustering,article,PENG2021107683,
"High-frequency oscillations (HFOs) are spontaneous electroencephalogram patterns that have been regarded as potential biomarkers of epileptic seizure onset zones (SOZs). Accurately detected HFOs are used to localize SOZs, which is crucial for the presurgical assessment. Since the visual marking of HFOs is time-consuming, a method is desirable to automatically detect HFOs for localizing SOZs in clinical practice. However, the existing methods cannot obtain satisfactory performance, which are not suitable for clinical application. In order to solve this problem, we present a new localization method for epileptic SOZs in this study. Firstly, a threshold method is used to detect events of interest (EoIs). Secondly, a time-frequency analysis method is adopted to acquire channels of interest (CoIs) by calculating the average power of EoIs on each channel. Then, the k-medoids clustering method is employed to detect HFOs of CoIs. Finally, the concentrations of detected HFOs are used to localize SOZs. The superiority of our localization method is demonstrated by comparing its sensitivity and specificity with some existing methods.","Epilepsy, Seizure onset zones, High-frequency oscillations, Time-frequency analysis, Clustering analysis",Min Wu and Ting Wan and Xiongbo Wan and Zelin Fang and Yuxiao Du,https://www.sciencedirect.com/science/article/pii/S0031320320304908,https://doi.org/10.1016/j.patcog.2020.107687,0031-3203,2021,107687,111,Pattern Recognition,A new localization method for epileptic seizure onset zones based on time-frequency and clustering analysis,article,WU2021107687,
"Low-rank representation (LRR), which is a powerful method to find the low-dimensional subspace structure embedded in high-dimensional data spaces, has been used in both unsupervised learning and semi-supervised classification. LRR aims at finding the lowest rank representation that can express each data sample as linear combination of other samples. However, this method doesnât consider the geometrical structure of the data. Thus the similarity and local structure might be lost in the process of learning. Motivated by this, a novel hierarchical weighted low-rank representation (HWLRR) is proposed in this paper. In the new algorithm, a hierarchical weighted matrix is defined to find more samples that may belong to the same subspace using affinity propagation. By taking advantage of the affinity propagation, our proposed method can preserve both local and global structure of the whole dataset. The experimental results on both unsupervised learning and semi-supervised classification demonstrate the superiority of our proposed method.","Low-rank representation, Clustering, Semi-supervised learning, Similarity graph construction",Zhiqiang Fu and Yao Zhao and Dongxia Chang and Yiming Wang,https://www.sciencedirect.com/science/article/pii/S0031320320305392,https://doi.org/10.1016/j.patcog.2020.107736,0031-3203,2021,107736,112,Pattern Recognition,A hierarchical weighted low-rank representation for image clustering and classification,article,FU2021107736,
"Online action detection aims to detect a current action from an untrimmed, streaming video, where only current and past frames are available. Recent methods for online action detection have focused on how to model discriminative representations from temporally partial information. However, they overlook the fact that the input video contains background as well as actions. To overcome this problem, in this paper, we propose a novel approach, named Temporal Filtering Network, to distinguish between relevant and irrelevant information from a partially observed, untrimmed video. Specifically, we present a filtering module to learn relevance scores indicating how relevant the information is to a current action. Our filtering module emphasizes the relevant information to a current action, while it filters out the information of background and unrelated actions. We conduct extensive experiments on THUMOS-14 and TVSeries datasets. On these datasets, the proposed method outperforms state-of-the-art methods by a large margin. We also show the effectiveness of the filtering module through comprehensive ablation studies.","Online action detection, Temporal filtering networks, Filter modules, TFN",Hyunjun Eun and Jinyoung Moon and Jongyoul Park and Chanho Jung and Changick Kim,https://www.sciencedirect.com/science/article/pii/S0031320320304982,https://doi.org/10.1016/j.patcog.2020.107695,0031-3203,2021,107695,111,Pattern Recognition,Temporal filtering networks for online action detection,article,EUN2021107695,
"Salient object detection (SOD) has gained tremendous attention in the field of computer vision. Multi-modal SOD based on the complementary information from RGB images and depth maps has shown remarkable success, making RGB-D saliency detection an active research topic. In this paper, we propose a novel multi-modal enhancement and fusion network (EF-Net) for effective RGB-D saliency detection. Specifically, we first utilize a color hint map module with RGB images to predict a hint map, which encodes the coarse information of salient objects. The resulting hint map is then utilized to enhance the depth map with our depth enhancement module, which suppresses the noise and sharpens the object boundary. Finally, we propose an effective layer-wise aggregation module to fuse the features extracted from the enhanced depth maps and RGB images for the accurate detection of salient objects. Our EF-Net utilizes an enhancement-and-fusion framework for saliency detection, which makes full use of the information from RGB images and depth maps. In addition, our depth enhancement module effectively resolves the low-quality issue of depth maps, which boosts the saliency detection performance remarkably. Extensive experiments on five widely-used benchmark datasets demonstrate that our method outperforms 12 state-of-the-art RGB-D saliency detection approaches in terms of five key evaluation metrics.","Salient object detection, RGB-D image, Depth enhancement, Feature fusion",Qian Chen and Keren Fu and Ze Liu and Geng Chen and Hongwei Du and Bensheng Qiu and Ling Shao,https://www.sciencedirect.com/science/article/pii/S0031320320305434,https://doi.org/10.1016/j.patcog.2020.107740,0031-3203,2021,107740,112,Pattern Recognition,EF-Net: A novel enhancement and fusion network for RGB-D saliency detection,article,CHEN2021107740,
"Recent years have witnessed a trend that uses image representation models, including sparse representation (SR), low-rank representation (LRR) and their variants for multi-focus image fusion. Despite the thrilling preliminary results, existing methods conduct the fusion patch by patch, leading to insufficient consideration of the spatial consistency among the image patches within a local region or an object. As a result, not only the spatial artifacts are easily introduced to the fused image but also the âjaggedâ artifacts frequently arise on the boundaries between the focused regions and the de-focused regions, which is an inherent problem in these patch-based fusion methods.Aiming to address the above problems, we propose, in this paper,a new multi-focus image fusion method integrating super-pixel clustering and a unified LRR (ULRR) model. The entire algorithm is carried out in three steps. In the first step, the source image is segmented into a few super-pixels with irregular sizes, rather than patches with regular sizes, to diminish the âjaggedâ artifacts and meanwhile to preserve the boundaries of objects on the fused image. Secondly, a super-pixel clustering-based fusion strategy is employed to further reduce the spatial artifacts in the fused images. This is achieved by using a proposed ULRR model, which imposes the low-rank constraints onto each super-pixel cluster.Thisis apparently more reasonable for those images with complicated scenes. Moreover, a Laplacianregularization term is incorporated in the proposed ULRR model to ensure the spatial consistency among the super-pixels with the same cluster. Finally, a measure of focus for each super-pixel is defined to seek the focused as well as de-focused regions in thesource image via jointly using representation coefficients and sparse errors derived from the proposed ULRR model. Extensive experiments have been conducted and the results demonstrate the superiorities of the proposed fusion method in diminishing the spatial artifactsin the fused image and the âjaggedâ boundary artifacts between the focused and de-focused regions, compared to the state-of-the-art fusion algorithms.","Multi-focus image fusion, Super-pixel clustering, Unified low-rank representation, Spatial consistency",Qiang Zhang and Fan Wang and Yongjiang Luo and Jungong Han,https://www.sciencedirect.com/science/article/pii/S0031320320305550,https://doi.org/10.1016/j.patcog.2020.107752,0031-3203,2021,107752,113,Pattern Recognition,Exploring a unified low rank representation for multi-focus image fusion,article,ZHANG2021107752,
"Various AI functionalities such as pattern recognition and prediction can effectively be used to diagnose (recognize) and predict coronavirus disease 2019 (COVID-19) infections and propose timely response (remedial action) to minimize the spread and impact of the virus. Motivated by this, an AI system based on deep meta learning has been proposed in this research to accelerate analysis of chest X-ray (CXR) images in automatic detection of COVID-19 cases. We present a synergistic approach to integrate contrastive learning with a fine-tuned pre-trained ConvNet encoder to capture unbiased feature representations and leverage a Siamese network for final classification of COVID-19 cases. We validate the effectiveness of our proposed model using two publicly available datasets comprising images from normal, COVID-19 and other pneumonia infected categories. Our model achieves 95.6% accuracy and AUC of 0.97 in diagnosing COVID-19 from CXR images even with a limited number of training samples.","COVID-19 diagnosis, Multi-shot learning, Contrastive loss, CXR images, Siamese network",Mohammad Shorfuzzaman and M. Shamim Hossain,https://www.sciencedirect.com/science/article/pii/S0031320320305033,https://doi.org/10.1016/j.patcog.2020.107700,0031-3203,2021,107700,113,Pattern Recognition,MetaCOVID: A Siamese neural network framework with contrastive loss for n-shot diagnosis of COVID-19 patients,article,SHORFUZZAMAN2021107700,
"The deep learning feature is the best for face recognition nowadays, but its performance exhibits unsatisfactorily under severe illumination variations. The main reason is that the deep learning feature was trained by the internet face images with variations of large pose/expression and slight/moderate illumination, which cannot well tackle severe illumination variations. Inspired by the fact that the deep learning feature can cope well with slight/moderate varying illumination, this paper proposes an illumination recovery model to transform severe varying illumination to slight/moderate varying illumination. The illumination recovery model enables the illumination of the severe illumination variation image close to that of the reference image with slight/moderate varying illumination. The reference image generated from the severe illumination variation image is termed as the generated reference image (GRI), which is obtained by normalizing singular values of the logarithm version of the severe illumination variation image to have unit L2-norm. The gradient descent algorithm is employed to address the proposed illumination recovery model, to obtain the generated reference image based illumination recovery image (GRIR). GRIR preserves better face inherent information than GRI such as the face color. Experimental results indicate that the proposed GRIR can efficiently improve the performance of the deep learning feature under severe illumination variations.","Severe illumination variations, Face recognition, Illumination recovery model, Deep learning feature",Chang-Hui Hu and Jian Yu and Fei Wu and Yang Zhang and Xiao-Yuan Jing and Xiao-Bo Lu and Pan Liu,https://www.sciencedirect.com/science/article/pii/S0031320320305276,https://doi.org/10.1016/j.patcog.2020.107724,0031-3203,2021,107724,111,Pattern Recognition,Face illumination recovery for the deep learning feature under severe illumination variations,article,HU2021107724,
"Mostly, shape from focus (SFF) methods do not consider any prior to extend the accuracy of the depth map. Ultimately, even the improved depth map might lack the accurate structure of the object. While reviewing the guided filters, it has been observed that SFF has not been considered as an application. In this study, we not only suggest to apply guided filtering for depth enhancement but also provide a comparative analysis of recently proposed guided filters for SFF framework in a systematic way. In addition, a set of potential guidance maps has been suggested and the performance of these guidance maps has been evaluated. The improved performance of guided filters has been ranked against the depth maps of synthetic and real image sequences where the corresponding scenes have diverse range of geometrical complexities. It has been observed that guided image filtering is effective in improving the initial depth maps in SFF.","Shape from focus (SFF), Focus measure, Guided image filtering, Depth map",Usman Ali and Ik Hyun Lee and Muhammad Tariq Mahmood,https://www.sciencedirect.com/science/article/pii/S0031320320304738,https://doi.org/10.1016/j.patcog.2020.107670,0031-3203,2021,107670,111,Pattern Recognition,Guided image filtering in shape-from-focus: A comparative analysis,article,ALI2021107670,
"Convolutional neural networks (CNNs) have shown unprecedented success in object representation and detection. Nevertheless, CNNs lack the capability to model context dependencies among objects, which are crucial for salient object detection. As the long short-term memory (LSTM) is advantageous in propagating information, in this paper, we propose two variant LSTM units for the exploration of contextual dependencies. By incorporating these units, we present a context-aware network (CAN) to detect salient objects in RGB-D images. The proposed model consists of three components: feature extraction, context fusion of multiple modalities and context-dependent deconvolution. The first component is responsible for extracting hierarchical features in color and depth images using CNNs, respectively. The second component fuses high-level features by a variant LSTM to model multi-modal spatial dependencies in contexts. The third component, embedded with another variant LSTM, models local hierarchical context dependencies of the fused features at multi-scales. Experimental results on two public benchmark datasets show that the proposed CAN can achieve state-of-the-art performance for RGB-D stereoscopic salient object detection.","Stereoscopic saliency analysis, 3D images, Multi-modal context fusion, Context-dependent deconvolution",Fangfang Liang and Lijuan Duan and Wei Ma and Yuanhua Qiao and Jun Miao and Qixiang Ye,https://www.sciencedirect.com/science/article/pii/S0031320320304337,https://doi.org/10.1016/j.patcog.2020.107630,0031-3203,2021,107630,111,Pattern Recognition,Context-aware network for RGB-D salient object detection,article,LIANG2021107630,
"Many trackers which divide the tracking process into two stages have recently been proposed to solve the problem of long-term tracking. Their outstanding performance makes them become one of the mainstream algorithms of long-term tracking. To further improve the performance of two-stage tracking algorithms, some improvements are proposed in this paper. (a) A hard negative mining method is proposed. It can optimize the training process of the verification network and bridge the gap between the two sub-networks. (b) The architecture of the verification network is designed as a Siamese structure; therefore, the semantic ambiguity in classification can be alleviated. Extensive experiments performed on benchmarks demonstrate that the proposed approach significantly outperforms the state-of-the-art methods, yielding 7% relative gain in the VOT2018-LT dataset and 14.2% relative gain in the OxUvA dataset.","Long-term object trakcing, Siamese network, Deep learning",Shiyu Xuan and Shengyang Li and Zifei Zhao and Longxuan Kou and Zhuang Zhou and Gui-Song Xia,https://www.sciencedirect.com/science/article/pii/S003132032030501X,https://doi.org/10.1016/j.patcog.2020.107698,0031-3203,2021,107698,112,Pattern Recognition,Siamese networks with distractor-reduction method for long-term visual object tracking,article,XUAN2021107698,
"Person re-identification is challenging with low-resolution query and high-resolution gallery images. To address the resolution mismatch, many methods perform super-resolution (SR) on low-resolution queries with specifying a single scale factor. However, using a single SR module, whichever scale factor is specified, always brings both advantages and drawbacks in recovering and identifying identity information. A larger scale factor recovers more details but produces excessive artifacts, while a smaller one is on the contrary. To exploit their complementary property for more robust recovery and identification, we propose the Adaptive Person Super-Resolution (APSR) model. APSR jointly trains and fuses multiple SR modules based on their generated visual contents, to fully compensate and learn the complementary identity features in an end-to-end manner. To improve the robustness to artifacts during fusion, our model further learns informative features by online dividing and integrating the generated body regions. Extensive experiments verify the effectiveness of our method with state-of-the-art performances.","Person re-identification, Super-resolution, Body regions, Adaptive feature integration",Ke Han and Yan Huang and Chunfeng Song and Liang Wang and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S0031320320304854,https://doi.org/10.1016/j.patcog.2020.107682,0031-3203,2021,107682,114,Pattern Recognition,Adaptive super-resolution for person re-identification with low-resolution images,article,HAN2021107682,
"In this paper, we introduce a new evidential clustering algorithm based on finding the belief-peaks and disjoint neighborhoods, called BPDNEC. The basic idea of BPDNEC is that each cluster center has the highest possibility of becoming a cluster center among its neighborhood and neighborhoods of those cluster centers are disjoint in vector space. Such possibility is measured by the belief notion in framework of evidence theory. By solving an equation related to neighborhood size, the size of such disjoint neighborhoods is determined and those objects having highest belief among their neighborhoods are automatically detected as cluster centers. Finally, a credal partition is created by minimizing an objective function concerning dissimilarity matrix of data objects. Experimental results show that BPDNEC can automatically detect cluster centers and derive an appropriate credal partition for both object data and proximity data. Simulations on synthetic and real-world datasets validate the conclusions.","Evidential clustering, Belief-peaks, Disjoint neighborhood, Proximity data",Chaoyu Gong and Zhi-gang Su and Pei-hong Wang and Qian Wang,https://www.sciencedirect.com/science/article/pii/S0031320320305549,https://doi.org/10.1016/j.patcog.2020.107751,0031-3203,2021,107751,113,Pattern Recognition,An evidential clustering algorithm by finding belief-peaks and disjoint neighborhoods,article,GONG2021107751,
"Neurophysiological evidence demonstrates that classical receptive field responses in the primary visual cortex can be modulated by the non-classical receptive field. Although models based on the non-classical receptive field have been proposed, which has not employed the two following characteristics: dynamic regulation of the receptive field under external stimuli and depth determination by binocular cells. We propose a model that includes processing of gray-scale images and depth images. Luminance of grayscale image response was preprocessed using weighted least squares filtering; luminance of grayscale image and disparity information of depth image responses were obtained by V1 neuron responses. The receptive field size of each pixel used to calculate the luminance and disparity response is determined by the depth information. Luminance and disparity information responses were combined based on the optimal orientation, and robust contour maps were developed. Experimental results show that the proposed contour detection model outperforms biologically inspired models.","Contour detection, Non-classical receptive field, Depth information, Dynamic property",Qing Zhang and Chuan Lin and Fuzhang Li,https://www.sciencedirect.com/science/article/pii/S003132032030460X,https://doi.org/10.1016/j.patcog.2020.107657,0031-3203,2021,107657,110,Pattern Recognition,Application of binocular disparity and receptive field dynamics: A biologically-inspired model for contour detection,article,ZHANG2021107657,
"Cancelable biometrics is an important biometric template protection technique. However, many existing cancelable fingerprint templates suffer post-transformation performance deterioration and the attacks via record multiplicity (ARM). In this paper, we design alignment-free cancelable fingerprint templates with dual protection, which is composed of the window-shift-XOR model and the partial discrete wavelet transform. The former defuses the ARM threat and is combined with the latter to provide dual protection and enhance matching performance. The designed cancelable templates meet the requirements of non-invertibility, diversity and revocability and demonstrate superior recognition accuracy, when evaluated over public databases; for example, the Equal Error Rate of the proposed method in the lost-key scenario under the 1vs1 protocol is 0% for both FVC2002 DB1 and DB2, 1.63% for FVC2002 DB3, 7.35% for FVC2004 DB1 and 4.69% for FVC2004DB2.","Cancelable biometrics, Alignment-free, Cancelable fingerprint templates, Discrete wavelet transform, Attacks via record multiplicity",Muhammad Shahzad and Song Wang and Guang Deng and Wencheng Yang,https://www.sciencedirect.com/science/article/pii/S0031320320305380,https://doi.org/10.1016/j.patcog.2020.107735,0031-3203,2021,107735,111,Pattern Recognition,Alignment-free cancelable fingerprint templates with dual protection,article,SHAHZAD2021107735,
"Facial Expression Recognition (FER) is a challenging yet important research topic owing to its significance with respect to its academic and commercial potentials. In this work, we propose an oriented attention pseudo-siamese network that takes advantage of global and local facial information for high accurate FER. Our network consists of two branches, a maintenance branch that consisted of several convolutional blocks to take advantage of high-level semantic features, and an attention branch that possesses a UNet-like architecture to obtain local highlight information. Specifically, we first input the face image into the maintenance branch. For the attention branch, we calculate the correlation coefficient between a face and its sub-regions. Next, we construct a weighted mask by correlating the facial landmarks and the correlation coefficients. Then, the weighted mask is sent to the attention branch. Finally, the two branches are fused to output the classification results. As such, a direction-dependent attention mechanism is established to remedy the limitation of insufficient utilization of local information. With the help of our attention mechanism, our network not only grabs a global picture but can also concentrate on important local areas. Experiments are carried out on 4 leading facial expression datasets. Our method has achieved a very appealing performance compared to other state-of-the-art methods.","Facial expression recognition, Weighted mask, Attention, Oriented gradient",Zhengning Wang and Fanwei Zeng and Shuaicheng Liu and Bing Zeng,https://www.sciencedirect.com/science/article/pii/S0031320320304970,https://doi.org/10.1016/j.patcog.2020.107694,0031-3203,2021,107694,112,Pattern Recognition,OAENet: Oriented attention ensemble for accurate facial expression recognition,article,WANG2021107694,
"Gaussian discriminant analysis is a popular classification model, that in the precise case can produce unreliable predictions in case of high uncertainty (e.g., due to scarce or noisy data). While imprecise probability theory offers a nice theoretical framework to solve such issues, it has not been yet applied to Gaussian discriminant analysis. This work remedies this, by proposing a new Gaussian discriminant analysis based on robust Bayesian analysis and near-ignorance priors. The model delivers cautiouspredictions, in form of set-valued class, in case of limited or imperfect available information. We present and discuss results of experimentation on real and synthetic datasets, where for this latter we corrupt the test instance to see how our approach reacts to non i.i.d. samples. Experiments show that including an imprecise component in the Gaussian discriminant analysis produces reasonably cautious predictions, and that set-valued predictions correspond to instances for which the precise model performs poorly.","Discriminant analysis, Robust Bayesian, Classification, Near-ignorance",Yonatan Carlos {Carranza AlarcÃ³n} and SÃ©bastien Destercke,https://www.sciencedirect.com/science/article/pii/S0031320320305422,https://doi.org/10.1016/j.patcog.2020.107739,0031-3203,2021,107739,112,Pattern Recognition,Imprecise Gaussian discriminant classification,article,CARRANZAALARCON2021107739,
"With recent advances in sensor technology, multivariate time series data are becoming extremely large with sophisticated but insightful inter-variable dependency patterns. Mining contrast dependency patterns in controlled experiments can help quantify the differences between control and experimental time series, however, overwhelms practitionersâ capability. Existing methods suffer from determining whether the differences are caused by the intervention or by different states. We propose a novel Contrast Pattern Mining (CPM) framework to find the intervention-related differences by jointly determining and characterizing the dynamic states in both time series via multivariate Gaussian distributions. Under the CPM framework, we not only propose a new covariance-based contrast pattern model, but also integrate our previous proposed partial correlation-based model as a special case. An efficient generic algorithm is developed to optimize various CPM models by adjusting one of the sub-routines. Comprehensive experiments are conducted to analyze the effectiveness, scalability, utility, and interpretability of the proposed framework.","Contrast pattern, Feature dependency, Controlled experiment, Driving behavior, Multivariate time series",Qingzhe Li and Liang Zhao and Yi-Ching Lee and Avesta Sassan and Jessica Lin,https://www.sciencedirect.com/science/article/pii/S0031320320305148,https://doi.org/10.1016/j.patcog.2020.107711,0031-3203,2021,107711,112,Pattern Recognition,CPM: A general feature dependency pattern mining framework for contrast multivariate time series,article,LI2021107711,
"To meet the recent demands for automated security systems, this study proposes a novel single-template strategy that uses mean templates and local stability-weighted dynamic time warping (LS-DTW) to simultaneously improve the speed and accuracy of online signature verification. Specifically, we adopt a recent time-series averaging method, called Euclidean barycenter-based DTW barycenter averaging (EB-DBA), to obtain an effective mean template set for each feature while preserving intra-user variability among reference samples. We then estimate the local stability of the mean template set by using direct matching points that represent stable signature regions in the DTW warping paths between the mean template set and the references. Subsequently, we boost the discriminative power in the verification phase using the LS-DTW distance measure that incorporates the local stability sequence as the weights for the DTW cost function between the mean template set and a query signature. Finally, we use the public SVC2004 Task2/MCYT-100 online signature datasets and the recent 3DAirSig in-air signature dataset to conduct experiments, whose results confirm the effectiveness of the proposed method in both the random- and skilled-forgery scenarios.","Biometrics, Signature verification, In-air signature, Time-series analysis, Dynamic time warping (DTW), Euclidean barycenter-based DTW barycenter averaging (EB-DBA), Local stability-weighted DTW (LS-DTW)",Manabu Okawa,https://www.sciencedirect.com/science/article/pii/S0031320320305021,https://doi.org/10.1016/j.patcog.2020.107699,0031-3203,2021,107699,112,Pattern Recognition,Time-series averaging and local stability-weighted dynamic time warping for online signature verification,article,OKAWA2021107699,
"Data reduction is becoming increasingly relevant due to the enormous amounts of data that are constantly being produced in many fields of research. Instance selection is one of the most widely used methods for this task. At the same time, most recent pattern recognition problems involve highly complex datasets with a large number of possible explanatory variables. For many reasons, this abundance of variables significantly hinders classification and recognition tasks. There are efficiency issues, too, because the speed of many classification algorithms is greatly improved when the complexity of the data is reduced. Thus, feature selection is also a widely used method for data reduction and for gaining an understanding of feature information. Although most methods address instance and feature selection separately, the two problems are interwoven, and benefits are expected from performing these two tasks jointly. However, few algorithms have been proposed for simultaneously addressing the tasks of instance and feature selection. Furthermore, most of those methods are based on complex heuristics that are very difficult to scale up even to moderately large datasets. This paper proposes a new algorithm for dealing with many instances and many features simultaneously by performing joint instance and feature selection using a simple heuristic search and several scaling-up mechanisms that can be successfully applied to datasets with millions of features and instances. In the proposed method, a forward selection search is performed in the feature space jointly with the application of standard instance selection in a constructive subspace built stepwise. Several simplifications are adopted in the search to obtain a scalable method. An extensive comparison using 95 large datasets shows the usefulness of our method and its ability to deal with millions of instances and features simultaneously. The method is able to obtain better classification performance results than state-of-the-art approaches while achieving considerable data reduction.","Instance selection, Feature selection, Evolutionary algorithms, nearest neighbor rule",NicolÃ¡s GarcÃ­a-Pedrajas and Juan A. Romero {del Castillo} and Gonzalo Cerruela-GarcÃ­a,https://www.sciencedirect.com/science/article/pii/S0031320320305264,https://doi.org/10.1016/j.patcog.2020.107723,0031-3203,2021,107723,111,Pattern Recognition,SI(FS)2: Fast simultaneous instance and feature selection for datasets with many features,article,GARCIAPEDRAJAS2021107723,
"Contextual information, defined in terms of the proximity of feature vectors in a feature space, has been successfully used in the construction of search services. These search systems aim to exploit such information to effectively improve ranking results, by taking into account the manifold distribution of features usually encoded. In this paper, a novel unsupervised manifold learning is proposed through a similarity representation based on ranking references. A breadth-first tree is used to represent similarity information given by ranking references and is exploited to discovery underlying similarity relationships. As a result, a more effective similarity measure is computed, which leads to more relevant objects in the returned ranked lists of search sessions. Several experiments conducted on eight public datasets, commonly used for image retrieval benchmarking, demonstrated that the proposed method achieves very high effectiveness results, which are comparable or superior to the ones produced by state-of-the-art approaches.","Content-based image retrieval, Unsupervised manifold learning, Tree representation, Ranking references",Daniel Carlos GuimarÃ£es Pedronette and Lucas Pascotti Valem and Ricardo da S. Torres,https://www.sciencedirect.com/science/article/pii/S0031320320304696,https://doi.org/10.1016/j.patcog.2020.107666,0031-3203,2021,107666,111,Pattern Recognition,A BFS-Tree of ranking references for unsupervised manifold learning,article,PEDRONETTE2021107666,
"Nonlinear dimensionality reduction is an active area of research. In this paper, we present a thematically different approach to detect a low-dimensional manifold that lies within a set of bounds derived from a given point cloud. A matrix representing distances on a low-dimensional manifold is low-rank, and our method is based on current low-rank Matrix Completion (MC) techniques for recovering a partially observed matrix from fully observed entries. MC methods are currently used to solve challenging real-world problems such as image inpainting and recommender systems. Our MC scheme utilizes efficient optimization techniques that employ a nuclear norm convex relaxation as a surrogate for non-convex and discontinuous rank minimization. The method theoretically guarantees on detection of low-dimensional embeddings and is robust to non-uniformity in the sampling of the manifold. We validate the performance of this approach using both a theoretical analysis as well as synthetic and real-world benchmark datasets.","Manifold, Low-rank matrix completion, Positive semi-definite, Truncated nuclear norm, Gramian",Kelum Gajamannage and Randy Paffenroth,https://www.sciencedirect.com/science/article/pii/S0031320320304647,https://doi.org/10.1016/j.patcog.2020.107661,0031-3203,2021,107661,111,Pattern Recognition,Bounded manifold completion,article,GAJAMANNAGE2021107661,
"Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.","Graph learning, Hypergraph learning, Graph neural networks, Semi-supervised learning",Song Bai and Feihu Zhang and Philip H.S. Torr,https://www.sciencedirect.com/science/article/pii/S0031320320304404,https://doi.org/10.1016/j.patcog.2020.107637,0031-3203,2021,107637,110,Pattern Recognition,Hypergraph convolution and hypergraph attention,article,BAI2021107637,
"Video summarization is an effective way to facilitate video searching and browsing. Most of existing systems employ encoder-decoder based recurrent neural networks, which fail to explicitly diversify the system-generated summary frames while requiring intensive computations. In this paper, we propose an efficient convolutional neural network architecture for video SUMmarization via Global Diverse Attention called SUM-GDA, which adapts attention mechanism in a global perspective to consider pairwise temporal relations of video frames. Particularly, the GDA module has two advantages: (1) it models the relations within paired frames as well as the relations among all pairs, thus capturing the global attention across all frames of one video; (2) it reflects the importance of each frame to the whole video, leading to diverse attention on these frames. Thus, SUM-GDA is beneficial for generating diverse frames to form satisfactory video summary. Extensive experiments on three data sets, i.e., SumMe, TVSum, and VTW, have demonstrated that SUM-GDA and its extension outperform other competing state-of-the-art methods with remarkable improvements. In addition, the proposed models can be run in parallel with significantly less computational costs, which helps the deployment in highly demanding applications.","Global diverse attention, Pairwise temporal relation, Video summarization, Convolutional neural networks",Ping Li and Qinghao Ye and Luming Zhang and Li Yuan and Xianghua Xu and Ling Shao,https://www.sciencedirect.com/science/article/pii/S0031320320304805,https://doi.org/10.1016/j.patcog.2020.107677,0031-3203,2021,107677,111,Pattern Recognition,Exploring global diverse attention via pairwise temporal relation for video summarization,article,LI2021107677,
"As data always lie on a lower-dimensional space, feature selection has become an important step in computer vision, machine learning and data mining. Due to the lack of class information, the performance of unsupervised feature selection depends on how to characterize and preserve the manifold structure among data. In this paper, we propose a novel unsupervised feature selection framework, named as joint adaptive manifold and embedding learning for unsupervised feature selectionÂ (JAMEL). It iteratively and adaptively learns lower-dimensional embeddings for data to preserve the manifold structure among data, regresses data to embeddings to measure the importance of features, and learns the manifold structure among data according to the data density in the intrinsic space, where the redundant and noisy features are eliminated. In addition, we present an efficient algorithm to solve the proposed problem, together with the convergence analysis. Finally, the evaluation results with the tasks of k-means, spectral clustering and nearest neighbor classification using the selected features on 12 datasets show the effectiveness and efficiency of our approach.","Unsupervised feature selection, Manifold learning, Embedding learning, Sparse learning",Jian-Sheng Wu and Meng-Xiao Song and Weidong Min and Jian-Huang Lai and Wei-Shi Zheng,https://www.sciencedirect.com/science/article/pii/S0031320320305458,https://doi.org/10.1016/j.patcog.2020.107742,0031-3203,2021,107742,112,Pattern Recognition,Joint adaptive manifold and embedding learning for unsupervised feature selection,article,WU2021107742,
"Top-Hat transformation is an essential technology in the field of infrared small target detection. Many modified Top-Hat transformation methods have been proposed based on the different structure of structural elements. However, these methods are still hard to handle the dim targets and complex background. It can be summarized as two reasons, one is that the structural elements cannot suppress the background adaptively due to the fixed value of structural elements in image. Another is that simple structural element cannot utilize the local feature for target enhancement. To overcome these two limitations, a special ring Top-Hat transformation based on M-estimator and local entropy is proposed in this paper. First, an adaptive ring structural element based on M-estimator is used to suppress the complex background. Second, a novel local entropy is proposed to weight structural element for capturing local feature and target enhancement. Finally, a comparison experiment based on massive infrared image data (more than 500 infrared target images) is done. And the results demonstrate that the proposed algorithm acquires better performance compared with some recent methods.","Infrared small target detection, Top-hat transformation, M-estimator, Local entropy",Lizhen Deng and Jieke Zhang and Guoxia Xu and Hu Zhu,https://www.sciencedirect.com/science/article/pii/S003132032030532X,https://doi.org/10.1016/j.patcog.2020.107729,0031-3203,2021,107729,112,Pattern Recognition,Infrared small target detection via adaptive M-estimator ring top-hat transformation,article,DENG2021107729,
"Handwritten signature verification is used to verify the identity of individuals through recognizing their signatures. Adversarial examples can induce misclassification, hence posing a severe threat to signature verification. At present, a variety of adversarial example attacks have been developed for image classification, but they are not that useful for attacking signature verification due to two main reasons. First, adversarial perturbations are likely to be imposed on the background of signature images, making them perceptible to human eyes. Second, perfect knowledge about signature verification systems is actually unavailable to attackers. Therefore, how to generate effective and stealthy signature adversarial examples is still an open issue. To shed insights on this challenging problem, we propose the first black-box adversarial example attack against handwritten signature verification in this paper. Our method has two key designs. First, its perturbations are intentionally restricted to the foreground (i.e., strokes) of signature images, which reduces the risk of being recognized by humans. Second, a gradient-free method is developed to achieve the desired perturbations through iteratively updating their positions and optimizing their intensity. Extensive experiments confirm the three advantages of our method. First, the adversarial perturbations generated by our method are almost invisible, while those generated by existing methods are more well-marked. Second, our method defeats the state-of-the-art signature verification method with a surprisingly high success rate of 92.1%. Last, our method breaks through the defense of background cleaning, although this defense can deactivate almost all the existing adversarial example attacks towards signature verification.","Handwritten signature verification, Adversarial example, Black-box attack",Haoyang Li and Heng Li and Hansong Zhang and Wei Yuan,https://www.sciencedirect.com/science/article/pii/S0031320320304921,https://doi.org/10.1016/j.patcog.2020.107689,0031-3203,2021,107689,111,Pattern Recognition,Black-box attack against handwritten signature verification with region-restricted adversarial perturbations,article,LI2021107689,
"Discriminative feature learning is critical for pedestrian re-identification. Previous part-based methods mainly focus on the region of specific predefined semantics to learn local representations, ignoring the influence of posture changes, and the learning efficiency and robustness in complex scenes are poor. In this paper, a hypergraph video pedestrian re-identification method based on posture structure relationships and action constraint(PA-HVPReid) is proposed, which aims to make full use of pedestrian walking postures to obtain more discriminative features. The pose structure relationship feature solves the problem that the pooling operation destroys the feature structure relationship. This paper uses a graph convolution network (GCN) to preserve the structure relationship presented by the image feature map. The input of the GCN is the region at the joint points of the pedestrian to be detected, and the output is the color feature that retains the structural relationship. The structural relationship hypergraph is formed according to the structural relationship between the joint point regions. The action hypergraph can be constructed by constraining the action information between the joint point regions. The saliency score of the joint point region is calculated from the posture structure hypergraph and the action hypergraph. We convert the saliency score into a probability distribution problem and propose a relative entropy loss function based on regional saliency to measure the similarity of the two probability distributions. Experimental results show that the performance of our method is better than the existing method on three data sets.","Pedestrian re-identification, Structural relationship, Action hypergraph, Saliency score",Xiaoqiang Hu and Dan Wei and Ziyang Wang and Jianglin Shen and Hongjuan Ren,https://www.sciencedirect.com/science/article/pii/S003132032030491X,https://doi.org/10.1016/j.patcog.2020.107688,0031-3203,2021,107688,111,Pattern Recognition,Hypergraph video pedestrian re-identification based on posture structure relationship and action constraints,article,HU2021107688,
"In recent years, Linear Discriminant Analysis (LDA) has seen huge adoption in data mining applications. Due to its globality, it is incompetent to handle multimodal data. Besides, most of LDAâs variants learn the projection matrix based on the pre-defined similarity matrix, which is easily affected by noisy and irrelevant features. To address above two issues, a novel local structured feature learning with Dynamic Maximum Entropy Graph (DMEG) method is developed which firstly develops a more discriminative LDA with whitening constraint that can minimize the within-class scatter while keeping the total samples scatter unchanged simultaneously. Second, for exploring the local structure of data, the â0-norm constraint is imposed on similarity matrix to ensure the k connectivity on graph. More importantly, proposed model learns the similarity and projection matrix simultaneously to ensure that the neighborships can be found in the optimal subspace where the noise have been removed already. Moreover, a maximum entropy regularization is employed to reinforce the discriminability of graph and avoid the trivial solution. Last but not least, an efficient iterative optimization algorithm is provided to optimize proposed model with a NP-hard constraint. Extensive experiments conducted on synthetic and several real-world data sets demonstrate the efficiency in classification task and robustness to noise of proposed method.","Supervised dimensionality reduction, Local structured feature learning, â-Norm constraint optimization, Dynamic maximum entropy graph",Zheng Wang and Feiping Nie and Rong Wang and Hui Yang and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320320304763,https://doi.org/10.1016/j.patcog.2020.107673,0031-3203,2021,107673,111,Pattern Recognition,Local structured feature learning with dynamic maximum entropy graph,article,WANG2021107673,
"For one given scene, multimodal data are acquired from multiple sensors. They share some similarities across the sensor types (redundant part of the information, also called coupling part) and they also provide modality-specific information (dissimilarities across the sensors, also called decoupling part). Additional critical knowledge about the scene can hence be extracted, which is not extractable from each modality alone. For the processing of multimodal data, we propose in this paper a model to simultaneously learn the underlying low-dimensional manifold in each modality, and locally align these manifolds across different modalities. For each pair of modalities we first build a common manifold that represents the corresponding (redundant) part of information, ignoring non-corresponding (modality specific) parts. We propose a semi-supervised learning model, using a limited amount of prior knowledge about the coupling and decoupling components of the different modalities. We propose a localized version of Laplacian eigenmaps technique specifically designed to handle multimodal manifold learning, in which the ideas of local patching of the manifolds, also known as manifold charting, is combined with the joint spectral analysis of the graph Laplacians of the different modalities. The limited given supervised information is then extending on the manifold of each modality. The idea of functional mapping is finally used to align the different manifolds across modalities. The evaluation of the proposed model using synthetic and real-world multimodal problems shows promising results, compared to several related techniques.","Semi-supervised learning, Multimodal data, Functional map, Manifold learning, Data fusion, Hyperspectral images",Ali Pournemat and Peyman Adibi and Jocelyn Chanussot,https://www.sciencedirect.com/science/article/pii/S0031320320304489,https://doi.org/10.1016/j.patcog.2020.107645,0031-3203,2021,107645,111,Pattern Recognition,Semisupervised charting for spectral multimodal manifold learning and alignment,article,POURNEMAT2021107645,
"Averaging GPS trajectories is needed in applications such as automatic generation of road network and finding representative movement patterns. We organized a challenge where participants submitted proposals to solve the averaging problem. In this paper, we review the proposals and evaluate their performance. We present a synthesis of the submitted methods and develop a new baseline composed of the well-performing components. The new baseline outperforms all existing averaging methods. All datasets, submissions and evaluations can be accessed on the competition webpage: http://cs.uef.fi/sipu/segments.","Gps trajectories, Trajectory averaging, Time series, Representative segment",Pasi FrÃ¤nti and Radu Mariescu-Istodor,https://www.sciencedirect.com/science/article/pii/S0031320320305331,https://doi.org/10.1016/j.patcog.2020.107730,0031-3203,2021,107730,112,Pattern Recognition,Averaging GPS segments competition 2019,article,FRANTI2021107730,
"Parameters Selection Problem (PSP) is a relevant and complex optimization issue in Support Vector Machine (SVM) and Support Vector Regression (SVR), looking for obtaining an optimal set of hyperparameters. In our case, the optimization problem is addressed to obtain models that minimize the number of support vectors and maximize generalization capacity. However, to obtain accurate and low complexity solutions, defining an adequate kernel function and the SVM/SVRâs hyperparameters are necessary, which currently represents a relevant research topic. To tackle this problem, this work proposes a multi-objective metaheuristic named Adaptive Parameter control with Mutant Tournament Multi-Objective Differential Evolution (APMT-MODE). Its performance is first tested in a series of benchmarks for classification and regression problems using simple kernels such as Gaussian and polynomial kernels. In both cases, the APMT-MODE is able to yield more precise and more straightforward solutions using simple kernels. Then, the approach is used on a real case study to create a welding bead depth and width SVR models for a Gas Metal Arc Welding (GMAW) process. Additionally, a study on kernel functions was developed in terms of computational effort, aiming to assess its performance for embedded systems applications.","Support vector machines, Parameters selection problem, Multi-objective optimization, Differential evolution, Adaptive parameters strategy",Carlos Eduardo da Silva Santos and Renato Coral Sampaio and Leandro dos Santos Coelho and Guillermo Alvarez Bestard and Carlos Humberto Llanos,https://www.sciencedirect.com/science/article/pii/S0031320320304520,https://doi.org/10.1016/j.patcog.2020.107649,0031-3203,2021,107649,110,Pattern Recognition,Multi-objective adaptive differential evolution for SVM/SVR hyperparameters selection,article,SANTOS2021107649,
"In this paper, we address one specific video retrieval problem in terms of human face. Given one query in forms of either a frame or a sequence from a person, we search the database and return the most relevant face videos, i.e., ones have the same class label with the query. Such problem is very challenging due to the large intra-class variations and the high request on the efficiency of video representations in terms of both time and space. To handle such challenges, this paper proposes a novel Deep Video Code (DVC) method which encodes video faces into compact binary codes. Specifically, we devise an end-to-end convolutional neural network (CNN) framework that takes face videos as training inputs, models each of them as a unified representation by temporal feature pooling operation, and finally projects the high-dimensional representations of both frames and videos into Hamming space to generate binary codes. In such Hamming space, distance of dissimilar pairs is larger than that of similar pairs by a margin. To this end, a novel bounded triplet hashing loss is elaborately designed, which takes all dissimilar pairs into consideration for each anchor point in a mini-batch, and the optimization of the loss function is smoother and more stable. Extensive experiments on challenging video face databases and general image/video datasets with comparison to the state-of-the-arts verify the effectiveness of our method in different kinds of retrieval scenarios.","Face video retrieval, Temporal feature pooling, Bounded triplet loss, Deep video code, Hash learning",Shishi Qiao and Ruiping Wang and Shiguang Shan and Xilin Chen,https://www.sciencedirect.com/science/article/pii/S0031320320305574,https://doi.org/10.1016/j.patcog.2020.107754,0031-3203,2021,107754,113,Pattern Recognition,Deep video code for efficient face video retrieval,article,QIAO2021107754,
"Video captioning aims at translating from a sequence of video frames into a sequence of words with the encoder-decoder framework. Hence, it is critical to align these two different sequences. Most existing methods exploit soft-attention (temporal attention) mechanism to align target words with corresponding frames, where the relevance of them merely depends on the previously generated words (i.e., language context). As we know, however, there is an inherent gap between vision and language, and most of the words in a caption belong to non-visual words (e.g. âaâ, âisâ, and âinâ). Hence, merely with the guidance of the language context, existing temporal attention-based methods cannot exactly align target words with corresponding frames. In order to address this problem, we first introduce pre-detected visual tags from the video to bridge the gap between vision and language. The reason is that visual tags not only belong to textual modality, but also can convey visual information. Then, we present a Textual-Temporal Attention Model (TTA) to exactly align the target words with corresponding frames. The experimental results show that our proposed method outperforms the state-of-the-art methods on two well known datasets, i.e., MSVD and MSR-VTT. 11Our code is available at https://github.com/tuyunbin/Enhancing-the-Alignment-between-Target-Words-and-Corresponding-Frames-for-Video-Captioning","Video captioning, Alignment, Visual tags, Textual-temporal attention",Yunbin Tu and Chang Zhou and Junjun Guo and Shengxiang Gao and Zhengtao Yu,https://www.sciencedirect.com/science/article/pii/S0031320320305057,https://doi.org/10.1016/j.patcog.2020.107702,0031-3203,2021,107702,111,Pattern Recognition,Enhancing the alignment between target words and corresponding frames for video captioning,article,TU2021107702,
"Scene parsing is essential for many high-level AI applications, such as intelligent vehicles and traffic surveillance. In this work, we propose a highly efficient and powerful deep convolutional neural network, namely Efficient Knowledge Enhanced Network (EKENet), for parsing scenes in real-time. Unlike most existing approaches that compromise efficiency for the sake of high accuracy, EKENet achieves an ideal trade-off between the two. Our EKENet is built upon a novel building block, namely Efficient Dual Abstraction (EDA) block, which employs an efficiently parallel convolution structure for extracting spatial features and modeling cross-channel correlations in a dual fashion. Additionally, a novel light-weight Encoding-Enhancing (EE) module is designed to enhance our EKENet, which can efficiently encode high-level knowledge extracted from top layers to guide the learning of low-level features from bottom layers. Extensive experiments on challenging benchmarks, Cityscapes and CamVid datasets, demonstrate that EKENet achieves the new state-of-the-art performance in terms of speed and accuracy tradeoff.","Scene parsing, Real-time method, Deep learning",Ao Luo and Fan Yang and Xin Li and Rui Huang and Hong Cheng,https://www.sciencedirect.com/science/article/pii/S003132032030474X,https://doi.org/10.1016/j.patcog.2020.107671,0031-3203,2021,107671,111,Pattern Recognition,EKENet: Efficient knowledge enhanced network for real-time scene parsing,article,LUO2021107671,
"In this paper, we propose a speed-up approach for subclass discriminant analysis and formulate a novel efficient multi-view solution to it. The speed-up approach is developed based on graph embedding and spectral regression approaches that involve eigendecomposition of the corresponding Laplacian matrix and regression to its eigenvectors. We show that by exploiting the structure of the between-class Laplacian matrix, the eigendecomposition step can be substituted with a much faster process. Furthermore, we formulate a novel criterion for multi-view subclass discriminant analysis and show that an efficient solution to it can be obtained in a similar manner to the single-view case. We evaluate the proposed methods on nine single-view and nine multi-view datasets and compare them with related existing approaches. Experimental results show that the proposed solutions achieve competitive performance, often outperforming the existing methods. At the same time, they significantly decrease the training time.","Subclass discriminant analysis, Spectral regression, Multi-view learning, Kernel regression, Subspace learning",Kateryna Chumachenko and Jenni Raitoharju and Alexandros Iosifidis and Moncef Gabbouj,https://www.sciencedirect.com/science/article/pii/S0031320320304635,https://doi.org/10.1016/j.patcog.2020.107660,0031-3203,2021,107660,111,Pattern Recognition,Speed-up and multi-view extensions to subclass discriminant analysis,article,CHUMACHENKO2021107660,
"We introduce a new arbitrary-shaped text detection approach named ReLaText by formulating text detection as a visual relationship detection problem. To demonstrate the effectiveness of this new formulation, we start from using a âlinkâ relationship to address the challenging text-line grouping problem firstly. The key idea is to decompose text detection into two subproblems, namely detection of text primitives and prediction of link relationships between nearby text primitive pairs. Specifically, an anchor-free region proposal network based text detector is first used to detect text primitives of different scales from different feature maps of a feature pyramid network, from which a text primitive graph is constructed by linking each pair of nearby text primitives detected from a same feature map with an edge. Then, a Graph Convolutional Network (GCN) based link relationship prediction module is used to prune wrongly-linked edges in the text primitive graph to generate a number of disjoint subgraphs, each representing a detected text instance. As GCN can effectively leverage context information to improve link prediction accuracy, our GCN based text-line grouping approach can achieve better text detection accuracy than previous text-line grouping methods, especially when dealing with text instances with large inter-character or very small inter-line spacing. Consequently, the proposed ReLaText achieves state-of-the-art performance on five public text detection benchmarks, namely RCTW-17, MSRA-TD500, Total-Text, CTW1500 and DAST1500.","Arbitrary-Shaped text detection, Graph convolutional network, Link prediction, Visual relationship detection",Chixiang Ma and Lei Sun and Zhuoyao Zhong and Qiang Huo,https://www.sciencedirect.com/science/article/pii/S0031320320304878,https://doi.org/10.1016/j.patcog.2020.107684,0031-3203,2021,107684,111,Pattern Recognition,ReLaText: Exploiting visual relationships for arbitrary-shaped scene text detection with graph convolutional networks,article,MA2021107684,
"Clustering is a powerful tool in exploratory data analysis that partitions a set of objects into clusters with the goal of maximizing the similarity of objects within each cluster. Due to the tendency of clustering algorithms to find suboptimal partitions of data, the approximation method Simulated Annealing (SA) has been used to search for near-optimal partitions. However, existing SA-based partitional clustering algorithms still settle to local optima. We propose a new SA-based clustering algorithm, the Simulated Annealing with Gaussian Mutation and Distortion Equalization algorithm (SAGMDE), which uses two perturbation methods to allow for both large and small perturbations in solutions. Our experiments on a diverse collection of data sets show that SAGMDE performs more consistently and yields better results than existing SA clustering algorithms in terms of cluster quality while maintaining a reasonable runtime. Finally, we use generative art as a visualization tool to compare various partitional clustering algorithms.","Partitional clustering, Simulated annealing, Sum of squared error criterion, -means",Julian Lee and David Perkins,https://www.sciencedirect.com/science/article/pii/S0031320320305161,https://doi.org/10.1016/j.patcog.2020.107713,0031-3203,2021,107713,112,Pattern Recognition,A simulated annealing algorithm with a dual perturbation method for clustering,article,LEE2021107713,
"A view-graph is vital for both the accuracy and robustness of structure-from-motion (SfM). Conventional matrix decomposition techniques treat all edges of view-graph equally; hence, many edge outliers are produced in matching pairs with fewer feature matches. To address this problem, we propose an incremental framework for view-graph construction, where the robustness of matched pairs that have a larger number of feature matches is propagated to their connected images. Given pairwise feature matches, a verified maximum spanning tree (VMST) is first constructed; for each edge in the VMST, we perform a local reconstruction and register its visible cameras. Based on the local reconstruction, pairwise relative geometries are computed and some new epipolar edges are produced. In this way, these newly computed edges inherit the robustness and accuracy of VMST, and by embedding them into VMST, our view-graph is constructed. We feed our view-graph into a standard SfM pipeline and compare this newly formed system with many of state-of-the-art SfM methods. The experimental results demonstrate that our view-graph provides a better foundation for conventional SfM systems, and enables them to reconstruct both general and ambiguous images.","Structure-from-motion, View-graph construction, Epipolar geometry computation",Hainan Cui and Tianxin Shi and Jun Zhang and Pengfei Xu and Yiping Meng and Shuhan Shen,https://www.sciencedirect.com/science/article/pii/S003132032030515X,https://doi.org/10.1016/j.patcog.2020.107712,0031-3203,2021,107712,114,Pattern Recognition,View-graph construction framework for robust and efficient structure-from-motion,article,CUI2021107712,
"Hierarchical classification is imperative in that almost all objects are described in hierarchical semantics. If a classification method enables incremental class learning to learn new objects online, it will be practically used for real-time applications. In this sense, we propose online incremental hierarchical classification resonance network (OIHCRN) that enables online incremental class learning in hierarchical classification. OIHCRN has a structure that grows horizontally and vertically online according to object classes, so that a newly added object can be classified. By the proposed process of scale-preserving projection and prior label appending, OIHCRN reflects the class dependency between class levels and simultaneously normalizes the input vector online. Additionally, to reduce the model complexity and improve performance, two auxiliary strategies, named OIHCRN with class END and OIHCRN with differentiated class labels, are introduced. To demonstrate the effectiveness of OIHCRNs, experiments are carried out for benchmark datasets and then for a multimedia recommendation system.","Hierarchical classification, Incremental class learning, Blocking method, Human-Computer/robot interaction, Multimedia recommendation",Ju-Youn Park and Jong-Hwan Kim,https://www.sciencedirect.com/science/article/pii/S0031320320304751,https://doi.org/10.1016/j.patcog.2020.107672,0031-3203,2021,107672,111,Pattern Recognition,Online incremental hierarchical classification resonance network,article,PARK2021107672,
"One-class anomaly detection approaches are particularly appealing for use in face presentation attack detection (PAD), especially in an unseen attack scenario, where the system is exposed to novel types of attacks. This work builds upon an anomaly-based formulation of the problem and analyses the merits of deploying client-specific information for face spoofing detection. We propose training one-class client-specific classifiers (both generative and discriminative) using representations obtained from pre-trained deep Convolutional Neural Networks (CNN). In order to incorporate client-specific information, a distinct threshold is set for each client based on subject-specific score distributions, which is then used for decision making at the test time. Through extensive experiments using different one-class systems, it is shown that the use of client-specific information in a one-class anomaly detection formulation (both in model construction as well as decision boundary selection) improves the performance significantly. We also show that anomaly-based solutions have the capacity to perform as well or better than two-class approaches in the unseen attack scenarios. Moreover, it is shown that CNN features obtained from models trained for face recognition appear to discard discriminative traits for spoofing detection and are less capable for PAD compared to the CNNs trained for a generic object recognition task.","Anomaly detection, Biometrics, Client-specific information, Deep convolutional neural networks, Face spoofing detection",Soroush Fatemifar and Shervin Rahimzadeh Arashloo and Muhammad Awais and Josef Kittler,https://www.sciencedirect.com/science/article/pii/S0031320320304994,https://doi.org/10.1016/j.patcog.2020.107696,0031-3203,2021,107696,112,Pattern Recognition,Client-specific anomaly detection for face presentation attack detection,article,FATEMIFAR2021107696,
"Obtaining an optimal tradeoff between accuracy and efficiency in ellipse detection is a significant challenge. In this paper, we propose a fast, high-precision ellipse detection method that utilizes arc selection and grouping strategies to significantly reduce the computation amount. A fast corner detection algorithm is also proposed. In the proposed method, to generate ellipse candidates comprehensively, both grouped and ungrouped-salient arcs are fitted. Further, the salient ellipse candidates are selected as final detections that are subject to the selection strategy, which realizes both validation and de-redundancy (clustering) functions. A complexity analysis of the method revealed that the detection time is linearly related to the number of edge points. The results of extensive experiments conducted on three public datasets demonstrate that the proposed method is approximately 75% faster than state-of-the-art methods with comparable or higher precision, and its detection time is less than 30Â ms in most cases.","Ellipse detection, Gaussian filter, Corner detection, Arc matching, Saliency score",Zepeng Wang and Derong Chen and Jiulu Gong and Changyuan Wang,https://www.sciencedirect.com/science/article/pii/S0031320320305446,https://doi.org/10.1016/j.patcog.2020.107741,0031-3203,2021,107741,111,Pattern Recognition,Fast high-precision ellipse detection method,article,WANG2021107741,
"One primary challenge of face recognition is that the performance is seriously affected by varying illumination. Multi-spectral imaging can capture face images in the visible spectrum and beyond, which is deemed to be an effective technology in response to this challenge. For current multi-spectral imaging-based face recognition methods, how to fully explore the discriminant and correlation features from both the intra-spectrum and inter-spectrum aspects with only a limited number of multi-spectral samples for model training has not been well studied. To address this problem, in this paper, we propose a novel face recognition approach named Spectrum-aware Discriminative Deep Learning (SDDL). To take full advantage of the multi-spectral training samples, we build a discriminative multi-spectral network (DMN) and take face sample pairs as the input of the network. By jointly considering the spectrum and the class label information, SDDL trains the network for projecting samples pairs into a discriminant feature subspace, on which the intrinsic relationship including the intra- and inter-spectrum discrimination and the inter-spectrum correlation among face samples is well discovered. The proposed approach is evaluated on three widely used datasets HK PolyU, CMU, and UWA. Extensive experimental results demonstrate the superiority of SDDL over state-of-the-art competing methods.","Deep feature learning, Inter-spectrum correlation, Intra- and inter-spectrum discrimination, Multi-spectral face recognition",Fei Wu and Xiao-Yuan Jing and Yujian Feng and Yi-mu Ji and Ruchuan Wang,https://www.sciencedirect.com/science/article/pii/S0031320320304350,https://doi.org/10.1016/j.patcog.2020.107632,0031-3203,2021,107632,111,Pattern Recognition,Spectrum-aware discriminative deep feature learning for multi-spectral face recognition,article,WU2021107632,
"We propose an improved version of the SMO algorithm for training classification and regression SVMs, based on a Conjugate Descent procedure. This new approach only involves a modest increase on the computational cost of each iteration but, in turn, usually results in a substantial decrease in the number of iterations required to converge to a given precision. Besides, we prove convergence of the iterates of this new Conjugate SMO as well as a linear rate when the kernel matrix is positive definite. We have implemented Conjugate SMO within the LIBSVM library and show experimentally that it is faster for many hyper-parameter configurations, being often a better option than second order SMO when performing a grid-search for SVM tuning.","SVM, Conjugate gradient, SMO",Alberto Torres-BarrÃ¡n and Carlos M. AlaÃ­z and JosÃ© R. Dorronsoro,https://www.sciencedirect.com/science/article/pii/S0031320320304477,https://doi.org/10.1016/j.patcog.2020.107644,0031-3203,2021,107644,111,Pattern Recognition,Faster SVM training via conjugate SMO,article,TORRESBARRAN2021107644,
"The emergence of large-scale data demands new regression models with multi-dimensional coefficient arrays, known as tensor regression models. The recently proposed tensor ring decomposition has interesting properties of enhanced representation and compression capability, cyclic permutation invariance and balanced tensor ring rank, which may lead to efficient computation and fewer parameters in regression problems. In this paper, a generally multi-linear tensor-on-tensor regression model is proposed that the coefficient array has a low-rank tensor ring structure, which is termed tensor ring ridge regression (TRRR). Two optimization models are developed for the TRRR problem and solved by different algorithms: the tensor factorization based one is solved by alternating least squares algorithm, and accelerated by a fast network contraction, while the rank minimization based one is addressed by the alternating direction method of multipliers algorithm. Comparative experiments, including Spatio-temporal forecasting tasks and 3D reconstruction of human motion capture data from its temporally synchronized video sequences, demonstrate the enhanced performance of our algorithms over existing state-of-the-art ones, especially in terms of training time.","Multilinear regression, Ridge regression, Tensor ring decomposition",Jiani Liu and Ce Zhu and Zhen Long and Huyan Huang and Yipeng Liu,https://www.sciencedirect.com/science/article/pii/S0031320320305562,https://doi.org/10.1016/j.patcog.2020.107753,0031-3203,2021,107753,113,Pattern Recognition,Low-rank tensor ring learning for multi-linear regression,article,LIU2021107753,
"For facial expression recognition, the sparseness constraints of the features or weights can improve the generalization ability of a deep network. However, the optimization of the hyper-parameters in fusing different sparseness strategies demands much computation, when the traditional gradient-based algorithms are used. In this work, an iterative framework with surrogate network is proposed for the optimization of hyper-parameters in fusing different sparseness strategies. In each iteration, a network with significantly smaller model complexity is fitted to the original large network based on four Euclidean losses, where the hyper-parameters are optimized with heuristic optimizers. Since the surrogate network uses the same deep metrics and embeds the same hyper-parameters as the original network, the optimized hyper-parameters are then used for the training of the original deep network in the next iteration. While the performance of the proposed algorithm is justified with a tiny model, i.e. LeNet on the FER2013 database, our approach achieved competitive performances on six publicly available expression datasets, i.e., FER2013, CK+, Oulu-CASIA, MMI, AFEW and AffectNet.","Expression recognition, Deep sparseness strategies, Hyper-parameter optimization, Surrogate network, Heuristic optimizer",Weicheng Xie and Wenting Chen and Linlin Shen and Jinming Duan and Meng Yang,https://www.sciencedirect.com/science/article/pii/S0031320320305045,https://doi.org/10.1016/j.patcog.2020.107701,0031-3203,2021,107701,111,Pattern Recognition,Surrogate network-based sparseness hyper-parameter optimization for deep expression recognition,article,XIE2021107701,
"3D feature description is one of the central techniques that rely on point clouds since a lot of point cloud processing techniques apply the point-to-point correspondences that are achieved via feature descriptors as input data. The feature descriptor encodes the information of the underlying surface around the feature point so as to make a local surface distinguished from another. The focus of the existing descriptors is accumulating the geometric or topological measurements into histograms or encoding the 2D images that are acquired by rotationally projecting the 3D local surfaces onto 2D planes. Histograms can hardly deal with three or more dimensional information, and the rotational projection operation does bring much unnecessary intermediate computations. To overcome these limitations, in this article, a descriptor named Kernel Density Descriptor (KDD) has been presented. One core contribution of this method is to encode the information of the whole 3D space around the feature point via kernel density estimation, and another is providing the strategy for selecting different matching metrics for datasets with diverse levels of resolution qualities. We compare KDD against several representative descriptors on publicly available datasets, the experimental results demonstrate that the KDD descriptor achieves a satisfactory and balanced performance in terms of descriptiveness, robustness, and compactness, furthermore, the comparisons validate the overall superiority of our method. The benefits and applicability on object registration and recognition and 3D object reconstruction are demonstrated by the favorable results that are obtained for both public datastes and the real-world point clouds of Terracotta fragments.","3D feature descriptor, Kernel density estimation, Point cloud registration, KL divergence",Yuhe Zhang and Chunhui Li and Bao Guo and Chenhao Guo and Shunli Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320304945,https://doi.org/10.1016/j.patcog.2020.107691,0031-3203,2021,107691,111,Pattern Recognition,KDD: A kernel density based descriptor for 3D point clouds,article,ZHANG2021107691,
"The binary partition tree (BPT) allows for the hierarchical representation of images in a multiscale way, by providing a tree of nodes corresponding to image regions. In particular, cuts of a BPT can be interpreted as segmentations of the associated image. Building the BPT of an image then constitutes a relevant preliminary step for optimization-based segmentation methods. A wide literature has been devoted to the construction of BPTs, and their involvement in such segmentation tasks. Comparatively, there exist few works dedicated to evaluate the quality of BPTs, i.e. their ability to allow further segmentation methods to compute good results. We propose such a framework for evaluating the quality of a BPT with respect to the object segmentation problem, i.e. the segmentation of one or several objects from an image. This framework is supervised, since the notion of segmentation quality is not only depending on the application but also on the userâs objectives, expressed via the chosen ground-truth and quality metric. We develop two sides within this framework. First, we propose an intrinsic quality analysis, that relies on the structural coherence of the BPT with respect to ground-truth. More precisely, we evaluate to what extent the BPT structure is well-matching such examples, in a set / combinatorial fashion. Second, we propose an extrinsic analysis, by allowing the user to assess the quality of a BPT based on chosen metrics that correspond to the desired properties of the subsequent segmentation. In particular, we evaluate to what extent a BPT can provide good results with respect to such metrics whereas handling the trade-off with the cardinality of the cuts.","Binary partition tree, Object segmentation, Hierarchical image model, Supervised quality evaluation, Mathematical morphology",Jimmy Francky Randrianasoa and Pierre Cettour-Janet and Camille Kurtz and Ãric Desjardin and Pierre GanÃ§arski and Nathalie Bednarek and FranÃ§ois Rousseau and Nicolas Passat,https://www.sciencedirect.com/science/article/pii/S0031320320304702,https://doi.org/10.1016/j.patcog.2020.107667,0031-3203,2021,107667,111,Pattern Recognition,Supervised quality evaluation of binary partition trees for object segmentation,article,RANDRIANASOA2021107667,
"Traditional partition-based clustering algorithms, hard or fuzzy version of C-means, could not deal with high-dimensional data sets effectively as redundant features may impact the computation of distances and local spatial structures among patterns are rarely considered. High dimensionality of space gives rise to so-called concentration effect that is detrimental. In this paper, a novel locality preserving based fuzzy C-means (LPFCM) clustering method and its optimization are presented. An orthogonally projected space, which preserves the locality of structural properties, can be generated in LPFCM, thus enhancing the capability of fuzzy C-means (FCM) for handling high-dimensional data. It is the first time to introduce projection techniques to the FCM optimization objective function directly, and the ideas of fuzzy clustering, geometric structure preservation and feature extraction are seamlessly integrated. LPFCM is also regarded as a unified model that combines two separate stages of spectral clustering. Experimental results on some benchmark data sets show the effectiveness of LPFCM in comparison with FCM and some state-of-the-art methods.","Fuzzy C-means, Locality preserving projections, Clustering, Projection-based spatial transformation",Jie Zhou and Witold Pedrycz and Xiaodong Yue and Can Gao and Zhihui Lai and Jun Wan,https://www.sciencedirect.com/science/article/pii/S0031320320305513,https://doi.org/10.1016/j.patcog.2020.107748,0031-3203,2021,107748,113,Pattern Recognition,Projected fuzzy C-means clustering with locality preservation,article,ZHOU2021107748,
"In multi-label learning, the issue of missing labels brings a major challenge. Many methods attempt to recovery missing labels by exploiting low-rank structure of label matrix. However, these methods just utilize global low-rank label structure, ignore both local low-rank label structures and label discriminant information to some extent, leaving room for further performance improvement. In this paper, we develop a simple yet effective discriminant multi-label learning (DM2L) method for multi-label learning with missing labels. Specifically, we impose the low-rank structures on all the predictions of instances from the same labels (local shrinking of rank), and a maximally separated structure (high-rank structure) on the predictions of instances from different labels (global expanding of rank). In this way, these imposed low-rank structures can help modeling both local and global low-rank label structures, while the imposed high-rank structure can help providing more underlying discriminability. Our subsequent theoretical analysis also supports these intuitions. In addition, we provide a nonlinear extension via using kernel trick to enhance DM2L and establish a concave-convex objective to learn these models. Compared to the other methods, our method involves the fewest assumptions and only one hyper-parameter. Even so, extensive experiments show that our method still outperforms the state-of-the-art methods.","Multi-label learning, Missing labels, Local low-rank label structure, Global low-rank label structure, Label discriminant information",Zhongchen Ma and Songcan Chen,https://www.sciencedirect.com/science/article/pii/S0031320320304787,https://doi.org/10.1016/j.patcog.2020.107675,0031-3203,2021,107675,111,Pattern Recognition,"Expand globally, shrink locally: Discriminant multi-label learning with missing labels",article,MA2021107675,
"While complex-valued transforms have been widely used in image processing and have their deep connections to biological vision systems, complex-valued convolutional neural networks (CNNs) have not seen their applications in image recovery. This paper aims at investigating the potentials of complex-valued CNNs for image denoising. A CNN is developed for image denoising with its key mathematical operations defined in the complex number field to exploit the merits of complex-valued operations, including the compactness of convolution given by the tensor product of 1D complex-valued filters, the nonlinear activation on phase, and the noise robustness of residual blocks. The experimental results show that, the proposed complex-valued denoising CNN performs competitively against existing state-of-the-art real-valued denoising CNNs, with better robustness to possible inconsistencies of noise models between training samples and test images. The results also suggest that complex-valued CNNs provide another promising deep-learning-based approach to image denoising and other image recovery tasks.","Complex-valued operations, Convolutional neural network, Image denoising, Deep learning",Yuhui Quan and Yixin Chen and Yizhen Shao and Huan Teng and Yong Xu and Hui Ji,https://www.sciencedirect.com/science/article/pii/S0031320320304428,https://doi.org/10.1016/j.patcog.2020.107639,0031-3203,2021,107639,111,Pattern Recognition,Image denoising using complex-valued deep CNN,article,QUAN2021107639,
"Recent advanced trackers, consisting of discriminative classification component and dedicated bounding box estimation, have achieved improved performance in the visual tracking community. The most essential factor for the development is the utilization of different Convolutional Neural Networks (CNNs), which significantly improves the model capacity via offline trained deep feature representations. Though powerful deep structures emphasize more semantic appearance through high dimensional latent variables, how to achieve effective feature adaptation in the online tracking stage has not been sufficiently considered yet. To this end, we argue the necessity of exploring hierarchical and complementary appearance descriptors from different convolutional layers to achieve online tracking adaptation. Therefore, in this paper, we propose an adaptive feature fusion mechanism, which can balance the detection granularities from shallow to deep convolutional layers. To be specific, the correlation between template and instance is employed to generate adaptive weights to achieve advanced saliency and discrimination. In addition, considering temporal appearance variation, the projection matrix for the multi-channel inputs is jointly updated with the correlation classifier to further enhance the robustness. The experimental results on four recent benchmarks, i.e., OTB-2015, VOT2018, LaSOT and TrackingNet, demonstrate the effectiveness and robustness of the proposed method, with superior performance compared to the state-of-the-art approaches.","Visual tracking, Deep neural network, Feature fusion, Online adaptation",Shaochuan Zhao and Tianyang Xu and Xiao-Jun Wu and Xue-Feng Zhu,https://www.sciencedirect.com/science/article/pii/S0031320320304829,https://doi.org/10.1016/j.patcog.2020.107679,0031-3203,2021,107679,111,Pattern Recognition,Adaptive feature fusion for visual object tracking,article,ZHAO2021107679,
"Hyper-Spectral Image (HSI) classification is an important task because of its wide range of applications. With the remarkable success from the Convolutional Neural Network (CNN), the performance of HSI classification has been significantly improved. However, two main challenges remained. One is that the samples of HSI have dramatic intra-class diversity and inter-class similarity, and the conventional cross-entropy loss is not good enough to learn discriminative features. The other is that the number of the training samples is so limited that the network is easy to overfit. To address the first challenge, we develop an improved triplet loss in order to make samples from the same class close to each other and make samples from different classes further apart. The proposed loss function considers all the possible positive pairs and negative pairs in a training batch, filters many trivial pairs, and prevents the impact of the outliers at the same time. To deal with the second challenge, we design an appropriate network architecture with less learnable parameters. We train the designed network based on the proposed loss with randomly initialized network weights using only hundreds of training samples, and attain quite good results. The experimental results show that the proposed method significantly surpasses other state-of-the-art methods, especially with less training samples. Furthermore, being less complex, the training process only takes a few minutes on a single GPU, which is faster than other state-of-the-art CNN-based methods.","Hyper-spectral image classification, Convolutional neural network, Triplet loss, Discriminative learning, Metric learning",Ke-Kun Huang and Chuan-Xian Ren and Hui Liu and Zhao-Rong Lai and Yu-Feng Yu and Dao-Qing Dai,https://www.sciencedirect.com/science/article/pii/S0031320320305471,https://doi.org/10.1016/j.patcog.2020.107744,0031-3203,2021,107744,112,Pattern Recognition,Hyperspectral image classification via discriminative convolutional neural network with an improved triplet loss,article,HUANG2021107744,
"Recently, finger-based multimodal biometrics, due to its high security and stability, has received considerable attention compared with unimodal biometrics. However, existing multimodal finger feature extraction approaches separately extract the features of different modalities, at the same time ignoring correlations among these different modalities. Furthermore, most of the conventional finger feature representation approaches are hand-crafted by design, which require strong prior knowledge. It is therefore very important to explore and develop a suitable feature representation and fusion strategy for multimodal biometrics recognition. In this paper, we proposed a joint discriminative feature learning (JDFL) framework for multimodal finger recognition by combining finger vein (FV) and finger knuckle print (FKP) patterns. For the FV and FKP images, we first established the informative dominant direction vector by convoluting a bank of Gabor filters and the original finger image. Then, we developed a simple yet effective feature learning algorithm, which simultaneously maximized the distance of between-class samples and minimized the distance of within-class samples, as well as maximized the correlation among inter-modality samples of the within-class. Finally, we integrated the block-wise histograms of the learned feature maps together for multimodal finger fusion recognition. Experimental results demonstrated that the proposed approach has a better recognition performance than state-of-the-art finger recognition methods.","Multimodal biometrics, Feature fusion, Inter-modality, Joint feature learning",Shuyi Li and Bob Zhang and Lunke Fei and Shuping Zhao,https://www.sciencedirect.com/science/article/pii/S0031320320305070,https://doi.org/10.1016/j.patcog.2020.107704,0031-3203,2021,107704,111,Pattern Recognition,Joint discriminative feature learning for multimodal finger recognition,article,LI2021107704,
"Cross-modal hashing similarity retrieval plays dual roles across various applications including search engines and autopilot systems. More generally, these methods also known to reduce the computation and memory storage in a training scheme. The key limitation of current methods are that: (i) they relax the discrete constrains to solve the optimization problem which may defeat the model purpose, (ii) projecting heterogenous data into a latent space may encourage to loss the diverse representations in such data, (iii) transforming real-valued data point to the binary codes always resulting in a loss of information and producing the suboptimal continuous latent space. In this paper, we propose a novel framework to project the original data points from different modalities into its own low-dimensional latent space and finds the cluster centroid points in its a low-dimensional space, using Cluster-wise Unsupervised Hashing (CUH). In particular, the proposed clustering scheme aims to jointly learns the compact hash codes and the corresponding linear hash functions. A discrete optimization framework is developed to learn the unified binary codes across modalities under of the guidance cluster-wise code-prototypes. Extensive experiments over multiple datasets demonstrate the effectiveness of our proposed model in comparison with the state-of-the-art in unsupervised cross-modal hashing tasks.","Cross-modal similarity retrieval, Multi-view clustering, The cluster-wise code-prototypes, Cross-modal hashing,",Lu Wang and Jie Yang and Masoumeh Zareapoor and Zhonglong Zheng,https://www.sciencedirect.com/science/article/pii/S0031320320305355,https://doi.org/10.1016/j.patcog.2020.107732,0031-3203,2021,107732,111,Pattern Recognition,Cluster-wise unsupervised hashing for cross-modal similarity search,article,WANG2021107732,
"Aiming at solving the problem of image size limiting in the traditional Random Projection (RP) algorithm, a novel Tighter Random Projection (TRP), which combines the scheme with Minimal Intra-class Variance (TRP-MIV) for hyperspectral remote sensing image classification is proposed. First, a new tighter dimensional boundary for expanding image size with the TRP-MIV matrix selected by multiple sampling for improving the class separability is defined to reduce dimension. Then the proposed algorithm is implemented, which integrates TRP-MIV for dimensionality reduction and Minimum Distance (MD) classifier for image classification. Finally, the image size and dimensionality reduction are evaluated by the number of spectral pixels under different theorems, and the spectral difference before and after dimensionality reduction, respectively. Classification performance is evaluated by kappa coefficient, Overall Accuracy (OA), Average Accuracy (AA), Average Precision Rate (APR) and running time. Classification results are obtained from the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) scanner and the Reflective Optics System Imaging Spectrometer (ROSIS) scanner, which indicate that the proposed algorithm is efficient and promising.","Random projection, Dimensionality reduction, Image size, Minimum distance classifier, Hyperspectral remote sensing image classification",Quanhua Zhao and Shuhan Jia and Yu Li,https://www.sciencedirect.com/science/article/pii/S0031320320304386,https://doi.org/10.1016/j.patcog.2020.107635,0031-3203,2021,107635,111,Pattern Recognition,Hyperspectral remote sensing image classification based on tighter random projection with minimal intra-class variance algorithm,article,ZHAO2021107635,
"Accurate boundary segmentation in medical images is significant yet challenging due to large variation of shape, size and appearance within intra- and inter- samples. In this paper, we present a novel deep model termed as Crossover-Net for robust segmentation in medical images. The proposed model is inspired by an interesting observation â the features learned from horizontal and vertical directions can provide informative and complement contextual information to enhance discriminative ability between different tissues. Specifically, we first originally propose a cross-shaped patch, namely crossover-patch which consists of a pair of (orthogonal and overlapping) vertical and horizontal patches. Then, we develop our Crossover-Net to learn the vertical and horizontal crossover relation according to the proposed crossover-patches. To train our model end-to-end, we design a novel loss function to (1) impose the consistency on overlapping region of vertical and horizontal patches and (2) preserve the diversity on their non-overlapping regions. We have extensively evaluated our method on CT kidney tumor, MR cardiac, and X-ray breast mass segmentation tasks, showing promising results compared with the current state-of-the-art methods. The code is available at https://github.com/Qianyu1226/Crossover-Net.","Convolutional neural network, Non-elongated tissue, Crossover-Net, Image segmentation, Crossover-patch",Qian Yu and Yang Gao and Yefeng Zheng and Jianbing Zhu and Yakang Dai and Yinghuan Shi,https://www.sciencedirect.com/science/article/pii/S0031320320305598,https://doi.org/10.1016/j.patcog.2020.107756,0031-3203,2021,107756,113,Pattern Recognition,Crossover-Net: Leveraging vertical-horizontal crossover relation for robust medical image segmentation,article,YU2021107756,
"Many research topics present very high dimensional data. Because of the heavy execution times and large memory requirements, many machine learning methods have difficulty in processing these data. In this paper, we propose a new unsupervised feature selection method considering the pairwise dependence of features (feature dependency-based unsupervised feature selection, or DUFS). To avoid selecting redundant features, the proposed method calculates the dependence among features and applies this information to a regression-based unsupervised feature selection process. We can select small feature set with the dependence among features by eliminating redundant features. To consider the dependence among features, we used mutual information widely used in supervised feature selection area. To our best knowledge, it is the first study to consider the pairwise dependence of features in the unsupervised feature selection method. Experimental results for six data sets demonstrate that the proposed method outperforms existing state-of-the-art unsupervised feature selection methods in most cases.","Unsupervised feature selection, Feature dependency, Feature redundancy, Joint entropy,  regularization",Hyunki Lim and Dae-Won Kim,https://www.sciencedirect.com/science/article/pii/S0031320320304660,https://doi.org/10.1016/j.patcog.2020.107663,0031-3203,2021,107663,111,Pattern Recognition,Pairwise dependence-based unsupervised feature selection,article,LIM2021107663,
"Ensemble learning has been widely applied to both batch data classification and streaming data classification. For the latter setting, most existing ensemble systems are homogenous, which means they are generated from only one type of learning model. In contrast, by combining several types of different learning models, a heterogeneous ensemble system can achieve greater diversity among its members, which helps to improve its performance. Although heterogeneous ensemble systems have achieved many successes in the batch classification setting, it is not trivial to extend them directly to the data stream setting. In this study, we propose a novel HEterogeneous Ensemble Selection (HEES) method, which dynamically selects an appropriate subset of base classifiers to predict data under the stream setting. We are inspired by the observation that a well-chosen subset of good base classifiers may outperform the whole ensemble system. Here, we define a good candidate as one that expresses not only high predictive performance but also high confidence in its prediction. Our selection process is thus divided into two sub-processes: accurate-candidate selection and confident-candidate selection. We define an accurate candidate in the stream context as a base classifier with high accuracy over the current concept, while a confident candidate as one with a confidence score higher than a certain threshold. In the first sub-process, we employ the prequential accuracy to estimate the performance of a base classifier at a specific time, while in the latter sub-process, we propose a new measure to quantify the predictive confidence and provide a method to learn the threshold incrementally. The final ensemble is formed by taking the intersection of the sets of confident classifiers and accurate classifiers. Experiments on a wide range of data streams show that the proposed method achieves competitive performance with lower running time in comparison to the state-of-the-art online ensemble methods.","Data streams, Heterogeneous ensembles, Ensemble selection",Anh Vu Luong and Tien Thanh Nguyen and Alan Wee-Chung Liew and Shilin Wang,https://www.sciencedirect.com/science/article/pii/S003132032030546X,https://doi.org/10.1016/j.patcog.2020.107743,0031-3203,2021,107743,112,Pattern Recognition,Heterogeneous ensemble selection for evolving data streams,article,LUONG2021107743,
"The acquisition of point clouds with a 3D scanner often yields large-scale, irregular, and unordered raw data, which hinders the classification of objects from these data. Some studies have introduced a method of applying the point clouds to convolutional neural networks (CNNs). This is achieved after preprocessing the volume metrics or multi-view images. However, this method has a limited resolution and a low classification accuracy in comparison to heavy computation in object classification. In this paper, DenX-Conv is proposed to improve the accuracy of object classification while securing the connectivity of points from the raw point cloud. DenX-Conv can extract effective local geometric features by finding the neighbor connectivity based on the geometric topology information of the points. In addition, stable feature learning is made possible by applying a densely connected network to PointCNN's Ï-Conv. Application of DenX-Conv to the ModelNet40 dataset resulted in a classification accuracy of 92.5%.","Convolutional neural networks, Delaunay triangulation, Dense connectivity, Neighbor connectivity, Point clouds classification",Jinwon Lee and Sang-Uk Cheon and Jeongsam Yang,https://www.sciencedirect.com/science/article/pii/S0031320320305112,https://doi.org/10.1016/j.patcog.2020.107708,0031-3203,2021,107708,112,Pattern Recognition,Connectivity-based convolutional neural network for classifying point clouds,article,LEE2021107708,
"Unsupervised domain adaptation is an effective approach to solve the problem of dataset bias. However, most existing unsupervised domain adaptation methods assume that the geometry structures of data distributions are similar in the source and target domains. This assumption is invalid in many practical applications, because the training and test datasets usually differ in the variability modes and/or variation degrees. This paper handles the problem of inconsistent geometries by aligning both data representations and geometries. To overcome the lack of target labels in aligning geometries, this paper proposes learning the adaptive geometry that is derived from the domain-shared label space. Source and target geometries are aligned by constraining them with the unified criteria of the adaptive geometry. Combining the adaptive geometry learning and adversarial learning techniques, we develop a geometry-aware dual-stream network to learn the geometry-aligned representations. Experimental results show that our method achieves good performance on cross-dataset recognition tasks.","Domain adaptation, Manifold structure, Distribution alignment",Baoyao Yang and Pong C. Yuen,https://www.sciencedirect.com/science/article/pii/S0031320320304416,https://doi.org/10.1016/j.patcog.2020.107638,0031-3203,2021,107638,110,Pattern Recognition,Learning adaptive geometry for unsupervised domain adaptation,article,YANG2021107638,
"Similar to binary and multi-class classifiers, one-class classifiers have to face the difficulty of âcurse of dimensionalityâ when they are applied to deal with high-dimensional samples. As an efficient dimensionality reduction method, sparse coding tries to learn a set of over-complete bases to represent the given samples. It can effectively overcome the âcurse of dimensionalityâ problem. However, the traditional sparse coding only fit for tackling Gaussian noise. When the noise within the given set of samples obey non-Gaussian distribution, the conventional sparse coding cannot obtain accurate coefficient vectors. To make sparse coding more fit for dealing with non-Gaussian noise and enhance the sparseness of the obtained coefficient vectors, correntropy is utilized to substitute its reconstruction error term and logarithmic penalty function is introduced as its regularization term. Furthermore, the obtained sparse coefficient vectors are used as the input vectors for one-class support vector machine (OCSVM). Experimental results on twenty UCI benchmark data sets and one handwritten digit data set demonstrate that the proposed method achieves better anti-noise and generalization abilities in comparison with its related approaches.","Sparse coding, One-class classification, Logarithmic penalty function, Correntropy, One-class support vector machine",Hong-Jie Xing and Ya-Jie Liu and Zi-Chuan He,https://www.sciencedirect.com/science/article/pii/S003132032030488X,https://doi.org/10.1016/j.patcog.2020.107685,0031-3203,2021,107685,111,Pattern Recognition,Robust sparse coding for one-class classification based on correntropy and logarithmic penalty function,article,XING2021107685,
"The learning model has been popular recently due to its promising results in various image classification tasks. Many existing learning methods, especially the deep learning methods, need a large amount of training data to achieve a high accuracy of classification. Conversely, only provided with a small-size dataset, some dictionary learning (DL) methods can achieve a perfect performance on a image classification task and hence still get a lot of attention. Among these DL methods, DL based feature learning methods are the mainstream for image classification in recent years, however, most of these methods have trained a classifier independently from dictionary learning. Therefore, the features extracted by the learned dictionary may not be very proper to perform classification for the classifier. Inspired by the feedback mechanism in cybernetics, this paper proposes a novel discriminative DL framework, named support vector machines (SVMs) multi-class loss feedback based discriminative dictionary learning (SMLFDL) that learns a discriminative dictionary while training SVMs to make the features extracted by the learned dictionary and SVMs better matched with each other. Because of integrating dictionary learning and SVMs training into a unified learning framework and good exactness of the looped multi-class loss term formulated from the feedback viewpoint for the classification scheme, better classification performance can be achieved. Experimental results on several widely used image databases show that SMLFDL can achieve a competitive performance with other state-of-the-art dictionary learning methods.","Dictionary learning, Feature representation, Feature learning, Feedback learning, Image classification",Bao-Qing Yang and Xin-Ping Guan and Jun-Wu Zhu and Chao-Chen Gu and Kai-Jie Wu and Jia-Jie Xu,https://www.sciencedirect.com/science/article/pii/S0031320320304933,https://doi.org/10.1016/j.patcog.2020.107690,0031-3203,2021,107690,112,Pattern Recognition,SVMs multi-class loss feedback based discriminative dictionary learning for image classification,article,YANG2021107690,
"Support vector data description (SVDD) is a popular anomaly detection technique. The computation of the SVDD classifier requires a kernel function, for which the Gaussian kernel is a common choice. The Gaussian kernel has a bandwidth parameter, and it is important to set the value of this parameter correctly to ensure good results. A small bandwidth leads to overfitting, and the resulting SVDD classifier overestimates the number of anomalies, whereas a large bandwidth leads to underfitting and an inability to detect many anomalies. In this paper, we present a new, unsupervised method for selecting the Gaussian kernel bandwidth. Our method exploits a low-rank representation of the kernel matrix to suggest a kernel bandwidth value. Our new technique is competitive with the current state of the art for low-dimensional data and performs extremely well for many classes of high-dimensional data. This method is also applicable to one-class support vector machines (OCSVM).","Support vector data description, SVDD, One-class support vector machines, OCSVM, Gaussian kernel, Automatic tuning, Gaussian kernel bandwidth",Arin Chaudhuri and Carol Sadek and Deovrat Kakde and Haoyu Wang and Wenhao Hu and Hansi Jiang and Seunghyun Kong and Yuwei Liao and Sergiy Peredriy,https://www.sciencedirect.com/science/article/pii/S0031320320304659,https://doi.org/10.1016/j.patcog.2020.107662,0031-3203,2021,107662,111,Pattern Recognition,The trace kernel bandwidth criterion for support vector data description,article,CHAUDHURI2021107662,
"Inspired by the temporal subspace clustering (TSC) method and low-rank matrix approximation constraint, a new model is proposed termed as temporal plus low-rank subspace clustering (TLRSC) by utilizing both the local and global structural information. On one hand, to solve the drawback that the nuclear norm-based constraint usually results in a suboptimal solution, we incorporate certain nonconvex surrogates into our model, which approximates the low-rank constraint closely and holds the potential for the convexity of the whole cost function. On the other hand, to ensure fast convergence, we propose an efficient iteratively reweighted singular value minimization (IRSVD) algorithm under the majorization-minimization framework. Moreover, we show that for the weighted low-rank constraint, a cutoff can be derived to automatically threshold the singular values computed from the proximal operator. This guarantees the thresholding operation can be reduced to that of two smaller matrices. Accordingly, an efficient singular value thresholding scheme is proposed for acceleration. Comprehensive experiments are conducted on several public available datasets for quantitative evaluation. Results demonstrate the efficacy and efficiency of TLRSC compared with several state-of-the-art methods.","Subspace clustering, Majorization-minimization, Nonconvex surrogate function, Iteratively reweighted norm minimization, Extrapolation technique, Randomized singular value decomposition",Jianwei Zheng and Ping Yang and Guojiang Shen and Shengyong Chen and Wei Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320304817,https://doi.org/10.1016/j.patcog.2020.107678,0031-3203,2021,107678,111,Pattern Recognition,Enhanced low-rank constraint for temporal subspace clustering and its acceleration scheme,article,ZHENG2021107678,
"Generative Adversarial Networks (GANs) have been widely used to generate realistic-looking instances. However, training robust GAN is a non-trivial task due to the problem of mode collapse. Although many GAN variants are proposed to overcome this problem, they have limitations. Those existing studies either generate identical instances or result in negative gradients during training. In this paper, we propose a new approach to training GAN to overcome mode collapse by employing a set of generators, an encoder and a discriminator. A new minimax formula is proposed to simultaneously train all components in a similar spirit to vanilla GAN. The orthogonal vector strategy is employed to guide multiple generators to learn different information in a complementary manner. In this way, we term our approach Multi-Generator Orthogonal GAN (MGO-GAN). Specifically, the synthetic data produced by those generators are fed into the encoder to obtain feature vectors. The orthogonal value is calculated between any two feature vectors, which loyally reflects the correlation between vectors. Such a correlation indicates how different information has been learnt by generators. The lower the orthogonal value is, the more different information the generators learn. We minimize the orthogonal value along with minimizing the generator loss through back-propagation in the training of GAN. The orthogonal value is integrated with the original generator loss to jointly update the corresponding generatorâs parameters. We conduct extensive experiments utilizing MNIST, CIFAR10 and CelebA datasets to demonstrate the significant performance improvement of MGO-GAN in terms of generated data quality and diversity at different resolutions.","GANs, Mode collapse, Multiple generators, Orthogonal vectors, Minimax formula",Wei Li and Li Fan and Zhenyu Wang and Chao Ma and Xiaohui Cui,https://www.sciencedirect.com/science/article/pii/S0031320320304490,https://doi.org/10.1016/j.patcog.2020.107646,0031-3203,2021,107646,110,Pattern Recognition,Tackling mode collapse in multi-generator GANs with orthogonal vectors,article,LI2021107646,
"Comprehensive and accurate eye modeling is crucial to a variety of applications, including human-computer interaction, assistive technologies, and medical diagnosis. However, most studies focus on the localization of one or two components of eyes, such as pupil or iris, lacking a comprehensive eye model. We propose to model an eye image by a set of parametric curves. The set of curves are plotted on an eye image to form a Contour-Eye image. A deep neural network is trained to evaluate the fitness of the Contour-Eye image. Then an evolutionary process is conducted to search the best fitting curve set, guided by the trained deep neural network. Finally, an accurate eye model with optimized parametric curves is obtained. For the algorithm evaluation, a finely annotated eye dataset denoted as FAED-50 is established by us, which contains 2,498 eye images from 50 subjects. The experimental results on the FAED-50 and the relabeled CASIA datasets and comparison with the state-of-the-art methods demonstrate the effectiveness and accuracy of the proposed parametric model.","Parametric eye modeling, Deep neural network, Evolutionary search, Fitness evaluation",Yang Zheng and Hong Fu and Ruimin Li and Tai-Chiu Hsung and Zongxi Song and Desheng Wen,https://www.sciencedirect.com/science/article/pii/S0031320320305586,https://doi.org/10.1016/j.patcog.2020.107755,0031-3203,2021,107755,113,Pattern Recognition,Deep neural network oriented evolutionary parametric eye modeling,article,ZHENG2021107755,
"In this paper an algorithm for multicamera pedestrian detection is proposed. The first stage of this work is based on the probabilistic occupancy map framework, in which the ground plane is discretized into a grid and the likelihood of pedestrian presence at each location is estimated by comparing a rectangle, of the average size of the pedestrians standing there, with the foreground silhouettes in all camera views. In the second stage, where we borrowed the idea from the Quine-McCluskey method for logic function minimization, essential candidates are initially identified, each of which covers at least a significant part of the foreground that is not covered by the other candidates. Then non-essential candidates are selected to cover the remaining foregrounds by following an iterative process, which alternates between merging redundant candidates and finding emerging essential candidates. Experiments on benchmark video datasets have demonstrated the improved performance of this algorithm in comparison with some benchmark non-deep or deep multicamera/monocular algorithms for pedestrian detection.","Pedestrian detection, Multicamera, Homography, Logic minimization, Video surveillance",Yuyao Yan and Ming Xu and Jeremy S. Smith and Mo Shen and Jin Xi,https://www.sciencedirect.com/science/article/pii/S0031320320305069,https://doi.org/10.1016/j.patcog.2020.107703,0031-3203,2021,107703,112,Pattern Recognition,Multicamera pedestrian detection using logic minimization,article,YAN2021107703,
"In this paper, a new discriminative dictionary learning algorithm is introduced. An entropy based criterion is embedded into the objective function to enforce a proper structure for the dictionary items when decomposing signals of different classes. The proposed criterion influences the dictionary items to participate in the decomposition of a smaller number of classes as possible. Unlike the other methods, columns of the dictionary are not restricted to have pre-assigned labels and they are free to be representative of any class or to share features of several classes. The number of shared and discriminative items along with the number of dictionary items for each specific class is learned dynamically during the optimization process, depending on the complexity of the classification task and the distribution of different classes. The experimental results demonstrate that the proposed entropy based dictionary learning (EDL) algorithm outperforms other discriminative dictionary learning methods using several real-world image datasets.","Discriminative dictionary learning, Entropy, Entropy based dictionary learning, Image classification, Sparse representation, Supervised dictionary learning",Arash Abdi and Mohammad Rahmati and Mohammad M. Ebadzadeh,https://www.sciencedirect.com/science/article/pii/S0031320320304374,https://doi.org/10.1016/j.patcog.2020.107634,0031-3203,2021,107634,110,Pattern Recognition,Entropy based dictionary learning for image classification,article,ABDI2021107634,
"History shows that the infectious disease (COVID-19) can stun the world quickly, causing massive losses to health, resulting in a profound impact on the lives of billions of people, from both a safety and an economic perspective, for controlling the COVID-19 pandemic. The best strategy is to provide early intervention to stop the spread of the disease. In general, Computer Tomography (CT) is used to detect tumors in pneumonia, lungs, tuberculosis, emphysema, or other pleura (the membrane covering the lungs) diseases. Disadvantages of CT imaging system are: inferior soft tissue contrast compared to MRI as it is X-ray-based Radiation exposure. Lung CT image segmentation is a necessary initial step for lung image analysis. The main challenges of segmentation algorithms exaggerated due to intensity in-homogeneity, presence of artifacts, and closeness in the gray level of different soft tissue. The goal of this paper is to design and evaluate an automatic tool for automatic COVID-19 Lung Infection segmentation and measurement using chest CT images. The extensive computer simulations show better efficiency and flexibility of this end-to-end learning approach on CT image segmentation with image enhancement comparing to the state of the art segmentation approaches, namely GraphCut, Medical Image Segmentation (MIS), and Watershed. Experiments performed on COVID-CT-Dataset containing (275) CT scans that are positive for COVID-19 and new data acquired from the EL-BAYANE center for Radiology and Medical Imaging. The means of statistical measures obtained using the accuracy, sensitivity, F-measure, precision, MCC, Dice, Jacquard, and specificity are 0.98, 0.73, 0.71, 0.73, 0.71, 0.71, 0.57, 0.99 respectively; which is better than methods mentioned above. The achieved results prove that the proposed approach is more robust, accurate, and straightforward.","Corona-virus Ddisease (COVID-19), Computer-Aaided Ddetection (CAD), COVID-19 lesion, Segmentation, Color-mapping, 3D Visualization",Adel Oulefki and Sos Agaian and Thaweesak Trongtirakul and Azzeddine {Kassah Laouar},https://www.sciencedirect.com/science/article/pii/S0031320320305501,https://doi.org/10.1016/j.patcog.2020.107747,0031-3203,2021,107747,114,Pattern Recognition,Automatic COVID-19 lung infected region segmentation and measurement using CT-scans images,article,OULEFKI2021107747,
"Naive Bayes (NB) continues to be one of the top 10 data mining algorithms, but its conditional independence assumption rarely holds true in real-world applications. Therefore, many different categories of improved approaches, including attribute weighting and instance weighting, have been proposed to alleviate this assumption. However, few of these approaches simultaneously pay attention to attribute weighting and instance weighting. In this study, we propose a new improved model called attribute and instance weighted naive Bayes (AIWNB), which combines attribute weighting with instance weighting into one uniform framework. In AIWNB, the attribute weights are incorporated into the naive Bayesian classification formula, and then the prior and conditional probabilities are estimated using instance weighted training data. To learn instance weights, we single out an eager approach and a lazy approach, and thus two different versions are created, which we denote as AIWNBE and AIWNBL, respectively. Extensive experimental results show that both AIWNBE and AIWNBL significantly outperform NB and all the other existing state-of-the-art competitors.","Naive Bayes, Attribute weighting, Instance weighting, Eager learning, Lazy learning",Huan Zhang and Liangxiao Jiang and Liangjun Yu,https://www.sciencedirect.com/science/article/pii/S0031320320304775,https://doi.org/10.1016/j.patcog.2020.107674,0031-3203,2021,107674,111,Pattern Recognition,Attribute and instance weighted naive Bayes,article,ZHANG2021107674,
"Scene graph parsing has become a new challenge in the field of image understanding and pattern recognition in recent years. It captures objects and their relationships, and provides a structured representation of the visual scene. Among the three types of high-level relationships of scene graphs, semantic relationships, which contain the global understanding of the scene, are the core and the most valuable, while geometric and possessive relationships contain local and limited information. However, semantic relationships have the characteristics of multiple types and fewer instances, leading to a low recognition rate of most semantic relationships by existing detectors. To address this issue, this paper proposes a new architecture, the graphical focal network, which uses a decision-level global detector to capture the dependencies between object and relationship local detectors. We construct a graphical focal loss, which overcomes the lack of semantic relationship instances by adjusting the proportion of relationship loss based on the degree of relationship rarity and learning difficulty, and improves the stability of key object recognition by adjusting the proportion of object loss based on the degree of node connectivity and the value of neighborhood relationships. The proposed relative depth encoding module and regional layout encoding module, respectively, introduce relative depth information and more effective geometric layout information between objects, thereby further improving the performance. Experiments using the Visual Genome benchmark show that our method outperforms the most advanced competitors in two types of performance metrics. For semantic types, the recognition rate of our method is 2.0 times that of the baseline.","Semantic relationship, Graphical focus, Scene graph, Class imbalance, Image understanding",Junjie Jiang and Zaixing He and Shuyou Zhang and Xinyue Zhao and Jianrong Tan,https://www.sciencedirect.com/science/article/pii/S0031320320305100,https://doi.org/10.1016/j.patcog.2020.107707,0031-3203,2021,107707,112,Pattern Recognition,Learning to transfer focus of graph neural network for scene graph parsing,article,JIANG2021107707,
"Temporal action localization has been a hot topic in video analyzation. In this paper, we propose a novel method called deep snippet selective network (DSSN) to address two key problems in weak supervision for temporal action localization, which are separability and integrality. Specifically, we employ two erasing branches to ensure the integrality, which can force the network to select other complementary snippets by erasing the most discriminative snippets. It is worth mentioning that a ternary mask is utilized to provide erasing branches with a background prior to enhance the separability of the model. Besides, we design a background suppression branch to further reduce the effect of background snippets. Extensive experiments on dataset THUMOSâ14 and ActivityNet show the effectiveness of our method.","Weak supervision, Temporal action localization, Erasing branches, Ternary mask, Background suppression branch",Yongxin Ge and Xiaolei Qin and Dan Yang and Martin Jagersand,https://www.sciencedirect.com/science/article/pii/S0031320320304891,https://doi.org/10.1016/j.patcog.2020.107686,0031-3203,2021,107686,110,Pattern Recognition,Deep snippet selective network for weakly supervised temporal action localization,article,GE2021107686,
"Recently, we have proposed a novel physically-inspired method, called the Nearest Descent (ND), which plays the role of organizing all the samples into an effective Graph, called the in-tree. Due to its effective characteristics, this in-tree proves very suitable for data clustering. Nevertheless, this in-tree-based clustering still has some non-trivial limitations in terms of robustness, capability, etc. In this study, we first propose a distance-ensemble-based framework for the in-tree-based clustering, which proves a very convenient way to overcome the robustness limitation in our previous in-tree-based clustering. To enhance the capability of the in-tree-based clustering in handling extremely linearly-inseparable clusters, we kernelize the proposed ensemble-based clustering via the so-called kernel trick. As a result, the improved in-tree-based clustering method achieves high robustness and accuracy on diverse challenging synthetic and real-world datasets, showing a certain degree of practical value.","In-tree, Distance ensemble, Kernelization, Clustering",Teng Qiu and Yongjie Li,https://www.sciencedirect.com/science/article/pii/S0031320320305343,https://doi.org/10.1016/j.patcog.2020.107731,0031-3203,2021,107731,112,Pattern Recognition,Enhancing in-tree-based clustering via distance ensemble and kernelization,article,QIU2021107731,
"In this paper, we focus on event discovery by utilizing data distributed in multiple media domains, such as news media and social media. To this end, we propose an in-domain and cross-domain Laplacian regularization (ICLR) model, which can learn effective data representation for both textual news reports contributed by journalists in news media domain, and image posts shared by amateur users in social media domain. The achieved data representation can be used by classification and clustering strategies for existing and new event discovery, respectively. More specifically, ICLR constructs respective Laplacian regularization terms considering the property of inter-domain and intra-domain label consistency, which can be optimized by employing an alternating optimization strategy with theoretical guarantee for convergence. In particular, we collect and release a multi-domain and multimodal dataset for evaluations and public use.","Data representation learning, Event detection, Social media, Multi-modality data",Zhenguo Yang and Qing Li and Haoran Xie and Qi Wang and Wenyin Liu,https://www.sciencedirect.com/science/article/pii/S003132032030443X,https://doi.org/10.1016/j.patcog.2020.107640,0031-3203,2021,107640,110,Pattern Recognition,Learning representation from multiple media domains for enhanced event discovery,article,YANG2021107640,
"This paper presents a novel method for matching line segments between stereo images. Given the fundamental matrix, the local homography can be over determined with pairwise line segment candidates. We exploit this constraint to initialize the candidate and construct the novel homography graph. Because the constraint between the node is based on the epipolar geometry, the homography graph is invariant to the local projective transformation. We employ the reweighted random walk on the graph to rank the candidate, then, we propose the constrained-greedy algorithm to obtain the reliable match. To the best of our knowledge, this is the first study to embed the epipolar geometry into the graph matching theory for the line segment matching. When evaluated on the 32 image patches, our method outperformed the state of the art methods, especially in the scenes of the wide baseline, steep viewpoint changes and dense line segments. The proposed algorithm is available at https://github.com/weidong-whu/line-match-RRW.","Line segment matching, Epipolar geometry, Reweighted random walks, Graph matching",Dong Wei and Yongjun Zhang and Chang Li,https://www.sciencedirect.com/science/article/pii/S0031320320304969,https://doi.org/10.1016/j.patcog.2020.107693,0031-3203,2021,107693,111,Pattern Recognition,Robust line segment matching via reweighted random walks on the homography graph,article,WEI2021107693,
"A vital aspect of the classification based model construction process is the calibration of the scoring function. One of the weaknesses of the calibration process is that it does not take into account the information about the relative positions of the recognized objects in the feature space. To alleviate this limitation, in this paper, we propose a novel concept of calculating a scoring function based on the distance of the object from the decision boundary and its distance to the class centroid. An important property is that the proposed score function has the same nature for all linear base classifiers, which means that outputs of these classifiers are equally represented and have the same meaning. The proposed approach is compared with other ensemble algorithms and experiments on multiple Keel datasets demonstrate the effectiveness of our method. To discuss the results of our experiments, we use multiple classification performance measures and statistical analysis.","Linear classifier, Potential function, Ensemble of classifiers, Score function",Pawel Trajdos and Robert Burduk,https://www.sciencedirect.com/science/article/pii/S0031320320304842,https://doi.org/10.1016/j.patcog.2020.107681,0031-3203,2021,107681,111,Pattern Recognition,Linear classifier combination via multiple potential functions,article,TRAJDOS2021107681,
"Occlusions handling poses a significant challenge to many computer vision and pattern recognition applications. Recently, Synthetic Aperture Imaging (SAI), which uses more than two cameras, is widely applied to reconstruct occluded objects in complex scenes. However, it usually fails in cases of heavy occlusions, in particular, when the occluded information is not captured by any of the camera views. Hence, it is a challenging task to generate a realistic all-in-focus synthetic aperture image which shows a completely occluded object. In this paper, semantic inpainting using a Generative Adversarial Network (GAN) is proposed to address the above-mentioned problem. The proposed method first computes a synthetic aperture image of the occluded objects using a labeling method, and an alpha matte of the partially occluded objects. Then, it uses energy minimization to reconstruct the background by focusing on the background depth of each camera. Finally, the occluded regions of the synthesized image are semantically inpainted using a GAN and the results are composited with the reconstructed background to generate a realistic all-in-focus image. The experimental results demonstrate that the proposed method can handle heavy occlusions and can produce better all-in-focus images than other state-of-the-art methods. Compared with traditional labeling methods, our method can quickly generate label for occlusion without introducing noise. To the best of our knowledge, our method is the first to address missing information caused by heavy occlusions in SAI using a GAN.","Synthetic aperture imaging, Occlusions handling, Image inpainting",Zhao Pei and Min Jin and Yanning Zhang and Miao Ma and Yee-Hong Yang,https://www.sciencedirect.com/science/article/pii/S0031320320304726,https://doi.org/10.1016/j.patcog.2020.107669,0031-3203,2021,107669,111,Pattern Recognition,All-in-focus synthetic aperture imaging using generative adversarial network-based semantic inpainting,article,PEI2021107669,
"Visual anomaly detection addresses the problem of classification or localization of regions in an image that deviate from their normal appearance. A popular approach trains an auto-encoder on anomaly-free images and performs anomaly detection by calculating the difference between the input and the reconstructed image. This approach assumes that the auto-encoder will be unable to accurately reconstruct anomalous regions. But in practice neural networks generalize well even to anomalies and reconstruct them sufficiently well, thus reducing the detection capabilities. Accurate reconstruction is far less likely if the anomaly pixels were not visible to the auto-encoder. We thus cast anomaly detection as a self-supervised reconstruction-by-inpainting problem. Our approach (RIAD) randomly removes partial image regions and reconstructs the image from partial inpaintings, thus addressing the drawbacks of auto-enocoding methods. RIAD is extensively evaluated on several benchmarks and sets a new state-of-the art on a recent highly challenging anomaly detection benchmark.","Anomaly detection, Video anomaly detection, Inpainting, CNN",Vitjan Zavrtanik and Matej Kristan and Danijel SkoÄaj,https://www.sciencedirect.com/science/article/pii/S0031320320305094,https://doi.org/10.1016/j.patcog.2020.107706,0031-3203,2021,107706,112,Pattern Recognition,Reconstruction by inpainting for visual anomaly detection,article,ZAVRTANIK2021107706,
"In recent years, Discriminative Correlation Filter (DCF) based tracking methods have achieved impressive performance in visual tracking. However, their excellent performance usually comes at the cost of sacrificing the computational speed. Furthermore, training correlation filters using high dimensional raw features may introduce the risk of severe over-fitting. To address the above issues, we propose Spatio-Temporal adaptive and Channel selective Correlation Filters (STCCF) for robust tracking. Specifically, we first select a set of target-specific features from high dimensional features via an effective channel selective scheme based on the Taylor expansion. Then, we reformulate the filter learning problem from ridge regression to elastic net regression to adaptively select the discriminative features inside the target bounding box at the spatial level. Moreover, we constrain the filters to be adaptive across temporal frames by learning a transformation matrix from the initial filters to the previous filters. In particular, with a specific spatio-temporal-channel constraint, STCCF can not only alleviate the over-fitting problem and reduce the computational cost, but also enhance the discriminability and interpretability of the learned filters. The proposed STCCF can be optimized by using a few iterations of Alternating Direction Method of Multipliers (ADMM). Experiments on six challenging datasets show that STCCF can achieve promising performance with fast running speed.","Visual tracking, Correlation filter, Filter compression, Elastic net regression, Filter transformation",Yanjie Liang and Yi Liu and Yan Yan and Liming Zhang and Hanzi Wang,https://www.sciencedirect.com/science/article/pii/S0031320320305410,https://doi.org/10.1016/j.patcog.2020.107738,0031-3203,2021,107738,112,Pattern Recognition,Robust visual tracking via spatio-temporal adaptive and channel selective correlation filters,article,LIANG2021107738,
A novel approach relying on the notion of mixture models is proposed for modeling and clustering directed weighted networks. The developed methodology can be used in a variety of settings including multilayer networks. Computational issues associated with the developed procedure are effectively addressed by the use of MCMC techniques. The utility of the methodology is illustrated on a set of experiments as well as applications to real-life data containing export trade amounts for European countries.,"Model-based clustering, Directed network, Weighted network, Multilayer network, MCMC",Volodymyr Melnykov and Shuchismita Sarkar and Yana Melnykov,https://www.sciencedirect.com/science/article/pii/S0031320320304441,https://doi.org/10.1016/j.patcog.2020.107641,0031-3203,2021,107641,112,Pattern Recognition,On finite mixture modeling and model-based clustering of directed weighted multilayer networks,article,MELNYKOV2021107641,
"Human behaviours consist different types of motion; we show how they can be disambiguated into their components in a richer way than that currently possible. Studies on optical flow have concentrated on motion alone without the higher order components: snap, jerk and acceleration. We are the first to show how the acceleration, jerk, snap and their constituent parts can be obtained from image sequences, and can be deployed for analysis, especially of behaviour. We demonstrate the estimation of acceleration in sport, human motion, traffic and in scenes of violent behaviour to demonstrate the wide potential for application of analysis of acceleration. Determining higher order components is suited to the analysis of scenes which contain them: higher order motion is innate to scenes containing acts of violent behaviour, but it is not just for behaviour which contains quickly changing movement: human gait contains acceleration though approaches have yet to consider radial and tangential acceleration, since they concentrate on motion alone. The analysis of synthetic and real-world images illustrates the ability of higher order motion to discriminate different objects under different motion. Then the new approaches are applied in heel strike detection in the analysis of human gait. These results demonstrate that the new approach is ready for developing new applications in behaviour recognition and provides a new basis for future research and applications of higher-order motion analysis.","Motion analysis, Higher-order motion, Acceleration, Jerk, Snap",Yan Sun and Jonathon S. Hare and Mark S. Nixon,https://www.sciencedirect.com/science/article/pii/S0031320320305136,https://doi.org/10.1016/j.patcog.2020.107710,0031-3203,2021,107710,112,Pattern Recognition,On parameterizing higher-order motion for behaviour recognition,article,SUN2021107710,
"Cross-modal retrieval aims at retrieving relevant points across different modalities, such as retrieving images via texts. One key challenge of cross-modal retrieval is narrowing the heterogeneous gap across diverse modalities. To overcome this challenge, we propose a novel method termed as Cross-modal discriminant Adversarial NetworkÂ (CAN). Taking bi-modal data as a showcase, CAN consists of two parallel modality-specific generators, two modality-specific discriminators, and a Cross-modal Discriminant MechanismÂ (CDM). To be specific, the generators project diverse modalities into a latent cross-modal discriminant space. Meanwhile, the discriminators compete against the generators to alleviate the heterogeneous discrepancy in this space, i.e., the generators try to generate unified features to confuse the discriminators, and the discriminators aim to classify the generated results. To further remove the redundancy and preserve the discrimination, we propose CDM to project the generated results into a single common space, accompanying with a novel eigenvalue-based loss. Thanks to the eigenvalue-based loss, CDM could push as much discriminative power as possible into all latent directions. To demonstrate the effectiveness of our CAN, comprehensive experiments are conducted on four multimedia datasets comparing with 15 state-of-the-art approaches.","Adversarial learning, Cross-modal representation learning, Cross-modal retrieval, Discriminant adversarial network, Cross-modal discriminant mechanism, Latent common space",Peng Hu and Xi Peng and Hongyuan Zhu and Jie Lin and Liangli Zhen and Wei Wang and Dezhong Peng,https://www.sciencedirect.com/science/article/pii/S0031320320305379,https://doi.org/10.1016/j.patcog.2020.107734,0031-3203,2021,107734,112,Pattern Recognition,Cross-modal discriminant adversarial network,article,HU2021107734,
"The distillation technique helps transform cumbersome neural networks into compact networks so that models can be deployed on alternative hardware devices. The main advantage of distillation-based approaches include a simple training process, supported by most off-the-shelf deep learning software and no special hardware requirements. In this paper, we propose a guideline for distilling the architecture and knowledge of pretrained standard CNNs. The proposed algorithm is first verified on a large-scale task: offline handwritten Chinese text recognition (HCTR). Compared with the CNN in the state-of-the-art system, the reconstructed compact CNN can reduce the computational cost by >10Ãand the model size by >8Ãwith negligible accuracy loss. Then, by conducting experiments on two additional classification task datasets: Chinese Text in the Wild (CTW) and MNIST, we demonstrate that the proposed approach can also be successfully applied on mainstream backbone networks.","Convolutional neural network, Acceleration and compression, Architecture and knowledge distillation, Offline handwritten Chinese text recognition",Zi-Rui Wang and Jun Du,https://www.sciencedirect.com/science/article/pii/S0031320320305252,https://doi.org/10.1016/j.patcog.2020.107722,0031-3203,2021,107722,111,Pattern Recognition,Joint architecture and knowledge distillation in CNN for Chinese text recognition,article,WANG2021107722,
"In this work, we propose a new approach to detect anomalous graphs in a stream of directed and labeled heterogeneous edges. The stream consists of a sequence of edges derived from different graphs. Each of these dynamic graphs represents the evolution of a specific activity in a monitored system whose events are acquired in real-time. Our approach is based on graph clustering and uses a simple graph embedding based on substructures and graph edit distance. Our graph representation is flexible and updates incrementally the graph vectors as soon as a new edge arrives. This allows the detection of anomalies in real-time which is an important requirement for sensitive applications such as cyber-security. Our implementation results prove the effectiveness of our approach in terms of accuracy of detection and time processing.","Graph anomaly detection, Graph stream, Graph embedding, Graph edit distance",Abd Errahmane Kiouche and Sofiane Lagraa and Karima Amrouche and Hamida Seba,https://www.sciencedirect.com/science/article/pii/S0031320320305495,https://doi.org/10.1016/j.patcog.2020.107746,0031-3203,2021,107746,112,Pattern Recognition,A simple graph embedding for anomaly detection in a stream of heterogeneous labeled graphs,article,KIOUCHE2021107746,
"Feature matching plays a very important role in many computer vision and pattern recognition tasks. The spatial neighborhood relationship (representing the topological structures of some key feature points of an image scene) is generally well preserved between two feature points of an image pair. Several mismatch-removing methods that maintain the local neighborhood structures of potential true matches have been proposed. Defining local neighborhood structures is a crucial issue in the feature matching problem. In this paper, we propose a robust and efficient topological structure measurement called top K rank preservation (TopKRP) for mismatch removal from given putative point set. We transform feature points from the feature space to the ranking list space. Thus, the topological structure similarity of two feature points can be simply calculated by comparing their ranking lists, which are measured by the top K ranking similarity based on the spatial Euclidean distance as well as the angle correlation. TopKRP is validated on 10 public image pairs with typical scenes and 2 artificially established datasets, namely, MI52 and RS153. Experimental results demonstrate that the proposed approach outperforms several state-of-the-art feature matching methods, especially when the number of mismatches is large.","Feature matching, Mismatch removal, Top  rank similarity, Local neighborhood structure",Junjun Jiang and Qing Ma and Xingyu Jiang and Jiayi Ma,https://www.sciencedirect.com/science/article/pii/S0031320320304684,https://doi.org/10.1016/j.patcog.2020.107665,0031-3203,2021,107665,111,Pattern Recognition,Ranking list preservation for feature matching,article,JIANG2021107665,
"Scene text with an irregular layout is difficult to recognize. To this end, a Sequential Transformation Attention-based Network (STAN), which comprises a sequential transformation network and an attention-based recognition network, is proposed for general scene text recognition. The sequential transformation network rectifies irregular text by decomposing the task into a series of patch-wise basic transformations, followed by a grid projection submodule to smooth the junction between neighboring patches. The entire rectification process is able to be trained in an end-to-end weakly supervised manner, requiring only images and their corresponding groundtruth text. Based on the rectified images, an attention-based recognition network is employed to predict a character sequence. Experiments on several benchmarks demonstrate the state-of-the-art performance of STAN on both regular and irregular text.","Scene text recognition, Scene text rectification, Optical character recognition, Deep learning",Qingxiang Lin and Canjie Luo and Lianwen Jin and Songxuan Lai,https://www.sciencedirect.com/science/article/pii/S0031320320304957,https://doi.org/10.1016/j.patcog.2020.107692,0031-3203,2021,107692,111,Pattern Recognition,STAN: A sequential transformation attention-based network for scene text recognition,article,LIN2021107692,
"Due to the difficulty in acquiring massive task-specific occluded images, the classification of occluded images with deep convolutional neural networks (CNNs) remains highly challenging. To alleviate the dependency on large-scale occluded image datasets, we propose a novel approach to improve the classification accuracy of occluded images by fine-tuning the pre-trained models with a set of augmented deep feature vectors (DFVs). The set of augmented DFVs is composed of original DFVs and pseudo-DFVs. The pseudo-DFVs are generated by randomly adding difference vectors (DVs), extracted from a small set of clean and occluded image pairs, to the real DFVs. In the fine-tuning, the back-propagation is conducted on the DFV data flow to update the network parameters. The experiments on various datasets and network structures show that the deep feature augmentation significantly improves the classification accuracy of occluded images without a noticeable influence on the performance of clean images. Specifically, on the ILSVRC2012 dataset with synthetic occluded images, the proposed approach achieves 11.21% and 9.14% average increases in classification accuracy for the ResNet50 networks fine-tuned on the occlusion-exclusive and occlusion-inclusive training sets, respectively.","Deep feature augmentation, Image occlusion, Convolutional neural networks, Image classification",Feng Cen and Xiaoyu Zhao and Wuzhuang Li and Guanghui Wang,https://www.sciencedirect.com/science/article/pii/S0031320320305409,https://doi.org/10.1016/j.patcog.2020.107737,0031-3203,2021,107737,111,Pattern Recognition,Deep feature augmentation for occluded image classification,article,CEN2021107737,
"Subspace clustering methods have been extensively studied in recent years. For 2-dimensional (2D) data, existing subspace clustering methods usually convert 2D examples to vectors, which severely damages inherent structural information and relationships of the original data. In this paper, we propose a novel subspace clustering method, named KTRR, for 2D data. The KTRR provides us with a way to learn the most representative 2D features from 2D data in learning data representation. In particular, the KTRR performs 2D feature learning and low-dimensional representation construction simultaneously, which renders the two tasks to mutually enhance each other. 2D kernel is introduced to the KTRR, which renders the KTRR to have enhanced capability of capturing nonlinear relationships from data. An efficient algorithm is developed for its optimization with provable decreasing and convergent property in objective value. Extensive experimental results confirm the effectiveness and efficiency of our method.","Subspace clustering, Ridge regression, 2-dimensional, Kernel",Chong Peng and Qian Zhang and Zhao Kang and Chenglizhao Chen and Qiang Cheng,https://www.sciencedirect.com/science/article/pii/S0031320320305525,https://doi.org/10.1016/j.patcog.2020.107749,0031-3203,2021,107749,113,Pattern Recognition,Kernel two-dimensional ridge regression for subspace clustering,article,PENG2021107749,
"Feature selection is a key step in many machine learning tasks. A majority of the existing methods of feature selection address the problem by devising some scoring function while treating the features independently, thereby overlooking their interdependencies. We leverage the scale invariance property of copula to construct a greedy, supervised feature selection algorithm that maximizes the feature relevance while minimizing the redundant information content. Multivariate copula is used in the proposed copula Based Feature Selection (CBFS) to discover the dependence structure between features. The incorporation of copula-based multivariate dependency in the formulation of mutual information helps avoid averaging over multiple instances of bivariate dependencies, thus eliminating the average estimation error introduced when bivariate dependency is used between a pair of feature variables. Under a controlled setting, our algorithm outperformed the existing best practice methods in warding off the noise in data. On several real and synthetic datasets, the proposed algorithm performed competitively in maximizing classification accuracy. CBFS also outperforms the other methods in terms of its noise tolerance property.","Copula, Feature selection, Mutual information, Stability, Classification accuracy",Snehalika Lall and Debajyoti Sinha and Abhik Ghosh and Debarka Sengupta and Sanghamitra Bandyopadhyay,https://www.sciencedirect.com/science/article/pii/S0031320320305008,https://doi.org/10.1016/j.patcog.2020.107697,0031-3203,2021,107697,112,Pattern Recognition,Stable feature selection using copula based mutual information,article,LALL2021107697,
"In a real-world scenario, an object is easily considered as features combined by multiple views in reality. Thus, multiview features can be encoded into a unified and discriminative framework to achieve satisfactory clustering performance. An increasing number of algorithms have been proposed for multiview data clustering. However, existing multiview methods have several drawbacks. First, most multiview algorithms focus only on origin data in high dimension directly without the intrinsic structure in the relative low-dimensional subspace. Spectral and manifold-based methods ignore pseudo-information that can be extracted from the optimization process. Thus, we design an unsupervised nonnegative matrix factorization (NMF)-based method called discriminative multiview subspace matrix factorization (DMSMF) for clustering. We provide the following contributions. (1) We extend linear discriminant analysis and NMF to a multiview version and connect them to a unified framework to learn in the discriminant subspace. (2) We propose a multiview manifold regularization term and discriminant multiview manifold regularization term that instruct the regularization term to discriminate different classes and obtain the geometry st ructure from the low-dimensional subspace. (3) We design an effective optimization algorithm with proven convergence to obtain an optimal solution procedure for the complex model. Adequate experiments are conducted on multiple benchmark datasets. Finally, we demonstrate that our model is superior to other comparable multiview data clustering algorithms.","Dimension reduction, Multiview, Clustering, Machine learning",Jiaqi Ma and Yipeng Zhang and Lefei Zhang,https://www.sciencedirect.com/science/article/pii/S0031320320304799,https://doi.org/10.1016/j.patcog.2020.107676,0031-3203,2021,107676,111,Pattern Recognition,Discriminative subspace matrix factorization for multiview data clustering,article,MA2021107676,
"Graph kernels are applied heavily for the classification of structured data. In this paper, we propose a deep RÃ©nyi entropy graph kernel for this purpose. We gauge the deep information through a family of h-layer expansion subgraphs rooted at a vertex, and define a h-layer depth-based second-order RÃ©nyi entropy representation for each vertex. The second-order RÃ©nyi entropy representation is used together with Euclidean distance to build a deep second-order RÃ©nyi entropy graph kernel (SREGK). For graphs with n vertices, the time complexity for our kernel is O(n3). This low-order polynomial complexity enables our subgraph kernels to easily scale up to graphs of reasonably large sizes and thus overcome the size limits arising in state-of-the-art graph kernels. Experimental results on fourteen real world graph datasets are shown to demonstrate the overall superior performance of our approach over a number of state-of-the-art methods.","Shannon entropy, RÃ©nyi entropy, Deep representation, Graph kernel, Graph classification",Lixiang Xu and Lu Bai and Xiaoyi Jiang and Ming Tan and Daoqiang Zhang and Bin Luo,https://www.sciencedirect.com/science/article/pii/S0031320320304714,https://doi.org/10.1016/j.patcog.2020.107668,0031-3203,2021,107668,111,Pattern Recognition,Deep RÃ©nyi entropy graph kernel,article,XU2021107668,
"We consider the problem of segmenting an image into superpixels in the context of k-means clustering, in which we wish to decompose an image into local, homogeneous regions corresponding to the underlying objects. Our novel approach builds upon the widely used Simple Linear Iterative Clustering (SLIC), and incorporate a measure of objectsâ structure based on the spectral residual of an image. Based on this combination, we propose a modified initialisation scheme and search metric, which keeps fine-details. This combination leads to better adherence to object boundaries, while preventing unnecessary segmentation of large, uniform areas, and remaining computationally tractable in comparison to other methods. We demonstrate through numerical and visual experiments that our approach outperforms the state-of-the-art techniques.","Superpixels, K-means, Spectral residual, Segmentation",Jianchao Zhang and Angelica I. Aviles-Rivero and Daniel Heydecker and Xiaosheng Zhuang and Raymond Chan and Carola-Bibiane SchÃ¶nlieb,https://www.sciencedirect.com/science/article/pii/S0031320320305082,https://doi.org/10.1016/j.patcog.2020.107705,0031-3203,2021,107705,112,Pattern Recognition,Dynamic spectral residual superpixels,article,ZHANG2021107705,
"Unmanned aerial vehicles (UAVs) have been used in a wide range of applications and become an increasingly important radar target. To better model radar data and to tackle the curse of dimensionality, a three-step classification framework is proposed for UAV detection. First we propose to utilize the greedy subspace clustering to handle potential outliers and the complex sample distribution of radar data. Parameters of the resulting multi-Gaussian model, especially the covariance matrices, could not be reliably estimated due to insufficient training samples and the high dimensionality. Thus, in the second step, a multi-Gaussian subspace reliability analysis is proposed to handle the unreliable feature dimensions of these covariance matrices. To address the challenges of classifying samples using the complex multi-Gaussian model and to fuse the distances of a sample to different clusters at different dimensionalities, a subspace-fusion scheme is proposed in the third step. The proposed approach is validated on a large benchmark dataset, which significantly outperforms the state-of-the-art approaches.","Radar UAV detection, Micro-Doppler signature, Greedy subspace clustering, Multi-Gaussian subspace reliability analysis, Subspace fusion",Jianfeng Ren and Xudong Jiang,https://www.sciencedirect.com/science/article/pii/S0031320320305124,https://doi.org/10.1016/j.patcog.2020.107709,0031-3203,2021,107709,111,Pattern Recognition,A three-step classification framework to handle complex data distribution for radar UAV detection,article,REN2021107709,
"Null space Linear Discriminant Analysis (NLDA) was proposed twenty years ago to overcome the singularity problem of LDA in practical applications. With two decades of technique development, many Discriminative Dimensionality Reduction (DDR) methods that outperform NLDA have been proposed. This paper provides new insight into NLDA and illustrates that NLDA is much more powerful after solving its inherent problem. The main problem of NLDA is the intrinsic overfitting problem. An ideal NLDA model is proposed to analyze its overfitting problem. Based on the ideal NLDA model, a more reasonable Representative NLDA (RNLDA) method is proposed to prevent overfitting. Two simple but efficient RNLDA algorithms are proposed to implement the RNLDA method with a theoretical proof. This study theoretically analyzed and indicated that applying the classical but simple hold-out pretraining method can automatically set the only parameter to achieve high performance. Extensive experiments with eight databases demonstrate the superior performance of the RNLDA method over state-of-the-art DDR methods.","Linear discriminant analysis, Dimensionality reduction, Feature selection, Null space, Overfitting, Singularity problem",Zaixing He and Mengtian Wu and Xinyue Zhao and Shuyou Zhang and Jianrong Tan,https://www.sciencedirect.com/science/article/pii/S0031320320304672,https://doi.org/10.1016/j.patcog.2020.107664,0031-3203,2021,107664,111,Pattern Recognition,Representative null space LDA for discriminative dimensionality reduction,article,HE2021107664,
"As for a biometric key, key management and biometric data security are both important. Existing bio-key generation methods are usually based on the biometric templates or features directly, it may expose userâs biometric data and will further make the biometric data permanently unusable for his secure identification recognitions. In this paper, a fingerprint bio-key generation approach using the feature distance is proposed. We utilize the relative distances among userâs fingerprint minutiae to generate a unique bio-key. Such bio-key is determinable and recoverable via the generation interval scheme. In addition, we use a two-layer error correcting technique to guarantee a better reliability during the data transmission. The experimental results positively show that our approach can ensure higher security of the bio-key and guarantee a good key regeneration rate. Besides, the storage of the original bio-key or any fingerprint template is unnecessary.","Feature distance, Generated interval, Bio-key, Biometric, Code",Peiyi Wang and Lin You and Gengran Hu and Liqin Hu and Zhihua Jian and Chaoping Xing,https://www.sciencedirect.com/science/article/pii/S0031320320305367,https://doi.org/10.1016/j.patcog.2020.107733,0031-3203,2021,107733,111,Pattern Recognition,Biometric key generation based on generated intervals and two-layer error correcting technique,article,WANG2021107733,
"Inspired by the human way of place understanding, we present a novel indoor place perception network to overcome: 1). the simplicity of existing methods that only use the image features of object regions to recognize the indoor place, 2). insufficient consideration of the semantic information about object attributes and states. By utilizing multi-modal information containing the image and natural language, the proposed method can comprehensively express the attributes, state, and relationships of objects which are beneficial for indoor place understanding and recognition. Specifically, we first present a natural language generation framework based on a Convolution Neural Network (CNN) and Long Short-Term Memory (LSTM) to imitate the process of place understanding. Next, a Convolutional Auto-Encoder (CAE) and a mixed CNN-LSTM are proposed to extract image features and semantic features, respectively. Then, two different fusion strategies, namely feature-level fusion and object-level fusion, are designed to integrate different types of features and features from different objects. The category of the indoor place is finally recognized based on fused information. Comprehensive experiments are conducted on public datasets, and the results verify the effectiveness of the proposed place perception method based on linguistic cues.","Indoor place perception, CNN, LSTM, Convolutional auto-encoder, Natural language",Pei Li and Xinde Li and Xianghui Li and Hong Pan and M.O. Khyam and Md. Noor-A-Rahim and Shuzhi Sam Ge,https://www.sciencedirect.com/science/article/pii/S0031320320304830,https://doi.org/10.1016/j.patcog.2020.107680,0031-3203,2021,107680,110,Pattern Recognition,Place perception from the fusion of different image representation,article,LI2021107680,
"Graph representation learning is of paramount importance for a variety of graph analytical tasks, ranging from node classification to community detection. Recently, graph convolutional networks (GCNs) have been successfully applied for graph representation learning. These GCNs generate node representation by aggregating features from the neighborhoods, which follows the âneighborhood aggregationâ scheme. In spite of having achieved promising performance on various tasks, existing GCN-based models have difficulty in well capturing complicated non-linearity of graph data. In this paper, we first theoretically prove that coefficients of the neighborhood interacting terms are relatively small in current models, which explains why GCNs barely outperforms linear models. Then, in order to better capture the complicated non-linearity of graph data, we present a novel GraphAIR framework which models the neighborhood interaction in addition to neighborhood aggregation. Comprehensive experiments conducted on benchmark tasks including node classification and link prediction using public datasets demonstrate the effectiveness of the proposed method.","Graph representation learning, Neighborhood aggregation, Graph neural networks, Neighborhood interaction, Node classification, Link prediction",Fenyu Hu and Yanqiao Zhu and Shu Wu and Weiran Huang and Liang Wang and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S0031320320305483,https://doi.org/10.1016/j.patcog.2020.107745,0031-3203,2021,107745,112,Pattern Recognition,GraphAIR: Graph representation learning with neighborhood aggregation and interaction,article,HU2021107745,
"Lane detection is an application of environmental perception, which aims to detect lane areas or lane lines by camera or lidar. In recent years, gratifying progress has been made in detection accuracy. To the best of our knowledge, this paper is the first attempt to make a comprehensive review of vision-based lane detection methods. First, we introduce the background of lane detection, including traditional lane detection methods and related deep learning methods. Second, we group the existing lane detection methods into two categories: two-step and one-step methods. Around the above summary, we introduce lane detection methods from the following two perspectives: (1) network architectures, including classification and object detection-based methods, end-to-end image-segmentation based methods, and some optimization strategies; (2) related loss functions. For each method, its contributions and weaknesses are introduced. Then, a brief comparison of representative methods is presented. Finally, we conclude this survey with some current challenges, such as expensive computation and the lack of generalization. And we point out some directions to be further explored in the future, that is, semi-supervised learning, meta-learning and neural architecture search, etc.","Lane detection, Deep learning, Semantic segmentation, Instance segmentation",Jigang Tang and Songbin Li and Peng Liu,https://www.sciencedirect.com/science/article/pii/S003132032030426X,https://doi.org/10.1016/j.patcog.2020.107623,0031-3203,2021,107623,111,Pattern Recognition,A review of lane detection methods based on deep learning,article,TANG2021107623,
"Camouflaged Object Detection (COD) aims to detect objects with similar patterns (e.g., texture, intensity, colour, etc) to their surroundings, and recently has attracted growing research interest. As camouflaged objects often present very ambiguous boundaries, how to determine object locations as well as their weak boundaries is challenging and also the key to this task. Inspired by the biological visual perception process when a human observer discovers camouflaged objects, this paper proposes a novel edge-based reversible re-calibration network called ERRNet. Our model is characterized by two innovative designs, namely Selective Edge Aggregation (SEA) and Reversible Re-calibration Unit (RRU), which aim to model the visual perception behaviour and achieve effective edge prior and cross-comparison between potential camouflaged regions and background. More importantly, RRU incorporates diverse priors with more comprehensive information comparing to existing COD models. Experimental results show that ERRNetÂ outperforms existing cutting-edge baselines on three COD datasets and five medical image segmentation datasets. Especially, compared with the existing top-1 model SINet, ERRNetÂ significantly improves the performance by â¼6% (mean E-measure) with notably high speed (79.3Â FPS), showing that ERRNetÂ could be a general and robust solution for the COD task.","Camouflaged Object Detection, Reversible Re-calibration Unit, Selective Edge Aggregation, NGES Priors",Ge-Peng Ji and Lei Zhu and Mingchen Zhuge and Keren Fu,https://www.sciencedirect.com/science/article/pii/S0031320321005902,https://doi.org/10.1016/j.patcog.2021.108414,0031-3203,2022,108414,123,Pattern Recognition,Fast Camouflaged Object Detection via Edge-based Reversible Re-calibration Network,article,JI2022108414,
"Texture can be defined as the change of image intensity that forms repetitive patterns resulting from the physical properties of an objectâs roughness or differences in a reflection on the surface. Considering that texture forms a system of patterns in a non-deterministic way, biodiversity concepts can help its characterization from an image. This paper proposes a novel approach to quantify such a complex system of diverse patterns through species diversity, richness, and taxonomic distinctiveness. The proposed approach considers each image channel as a species ecosystem and computes species diversity and richness as well as taxonomic measures to describe the texture. Furthermore, the proposed approach takes advantage of ecological patternsâ invariance characteristics to build a permutation, rotation, and translation invariant descriptor. Experimental results on three datasets of natural texture images and two datasets of histopathological images have shown that the proposed texture descriptor has advantages over several texture descriptors and deep methods.","Pattern recognition, Texture characterization and classification, Species richness, Taxonomic distinctiveness, Phylogenetic indices, Species abundance",Steve Tsham Mpinda Ataky and Alessandro {Lameiras Koerich},https://www.sciencedirect.com/science/article/pii/S0031320321005628,https://doi.org/10.1016/j.patcog.2021.108382,0031-3203,2022,108382,123,Pattern Recognition,A novel bio-inspired texture descriptor based on biodiversity and taxonomic measures,article,ATAKY2022108382,
"Quantitative trading takes advantage of mathematical functions for automatically making stock or futures trading decisions. Specifically, various trading strategies that proposed by human-experts are associated with weight hyper-parameters to determine the probability of selecting a specific strategy according to market conditions. Prior work manually adjusting the weight hyper-parameters is error-prone, because the essential advantage of quantitative trading, i.e., automation, is lost. In this paper, we propose a dynamic parameter tuning algorithm, i.e., TradeBot, based on bandit learning for quantitative trading. We consider sequentially selecting hyper-parameters of rules for trading as a bandit game, where a set of hyper-parameters of trading rule is considered as an action. A novel reward-agnostic Upper Confidence Bound bandit method is proposed to solve the automatically trading problem with a reward function estimated by inverse reinforcement learning. Experimental results on China Commodity Futures Market Data show state-of-the-art performance. To our best knowledge, this is one of the first work deployed in the online trading system via reinforcement learning, in published literature.","High-Frequency trading, Hyper-parameter optimization, Multi-armed bandit learning, Inverse reinforcement learning",Weipeng Zhang and Lu Wang and Liang Xie and Ke Feng and Xiang Liu,https://www.sciencedirect.com/science/article/pii/S003132032100666X,https://doi.org/10.1016/j.patcog.2021.108490,0031-3203,2022,108490,124,Pattern Recognition,TradeBot: Bandit learning for hyper-parameters optimization of high frequency trading strategy,article,ZHANG2022108490,
"Face masks have become one of the main methods for reducing the transmission of COVID-19. This makes face recognition (FR) a challenging task because masks hide several discriminative features of faces. Moreover, face presentation attack detection (PAD) is crucial to ensure the security of FR systems. In contrast to the growing number of masked FR studies, the impact of face masked attacks on PAD has not been explored. Therefore, we present novel attacks with real face masks placed on presentations and attacks with subjects wearing masks to reflect the current real-world situation. Furthermore, this study investigates the effect of masked attacks on PAD performance by using seven state-of-the-art PAD algorithms under different experimental settings. We also evaluate the vulnerability of FR systems to masked attacks. The experiments show that real masked attacks pose a serious threat to the operation and security of FR systems.","Face presentation attack detection, COVID-19, Masked face, Face recognition, Biometric security",Meiling Fang and Naser Damer and Florian Kirchbuchner and Arjan Kuijper,https://www.sciencedirect.com/science/article/pii/S0031320321005744,https://doi.org/10.1016/j.patcog.2021.108398,0031-3203,2022,108398,123,Pattern Recognition,Real masks and spoof faces: On the masked face presentation attack detection,article,FANG2022108398,
"Despite the great breakthroughs that deep convolutional neural networks (DCNNs) have achieved on image representation learning in recent years, they lack the ability to extract invariant information from images. On the other hand, several traditional feature extractors like Gabor filters are widely used for invariant information learning from images. In this paper, we propose a new class of DCNNs named adaptive Gabor convolutional networks (AGCNs). In the AGCNs, the convolutional kernels are adaptively multiplied by Gabor filters to construct the Gabor convolutional filters (GCFs), while the parameters in the Gabor functions (i.e., scale and orientation) are learned alongside those in the convolutional kernels. In addition, the GCFs can be regenerated after updating the Gabor filters and convolutional kernels. We evaluate the performance of the proposed AGCNs on image classification using five benchmark image datasets, i.e., MNIST and its rotated version, SVHN, CIFAR-10, CINIC-10, and DogsVSCats. Experimental results show that the AGCNs are robust to spatial transformations and have achieved higher accuracy compared with the DCNNs and other state-of-the-art deep networks. Moreover, the GCFs can be easily embedded into any classical DCNN models (e.g., ResNet) and require fewer parameters than the corresponding DCNNs.","Gabor filters, Deep convolutional neural networks, Invariant information, Gabor convolutional filters, Image classification",Ye Yuan and Li-Na Wang and Guoqiang Zhong and Wei Gao and Wencong Jiao and Junyu Dong and Biao Shen and Dongdong Xia and Wei Xiang,https://www.sciencedirect.com/science/article/pii/S0031320321006713,https://doi.org/10.1016/j.patcog.2021.108495,0031-3203,2022,108495,124,Pattern Recognition,Adaptive Gabor convolutional networks,article,YUAN2022108495,
"Linear Discriminant Analysis (LDA) assumes that all samples from the same class are independently and identically distributed (i.i.d.). LDA may fail in the cases where the assumption does not hold. Particularly when a class contains several clusters (or subclasses), LDA cannot correctly depict the internal structure as the scatter matrices that LDA relies on are defined at the class level. In order to mitigate the problem, this paper proposes a neighborhood linear discriminant analysis (nLDA) in which the scatter matrices are defined on a neighborhood consisting of reverse nearest neighbors. Thus, the new discriminator does not need an i.i.d. assumption. In addition, the neighborhood can be naturally regarded as the smallest subclass, for which it is easier to be obtained than subclass without resorting to any clustering algorithms. The projected directions are sought to make sure that the within-neighborhood scatter as small as possible and the between-neighborhood scatter as large as possible, simultaneously. The experimental results show that nLDA performs significantly better than previous discriminators, such as LDA, LFDA, ccLDA, LM-NNDA, and l2,1-RLDA.","Linear discriminant analysis, Reverse nearest neighbors, Neighborhood linear discriminant analysis, Multimodal class",Fa Zhu and Junbin Gao and Jian Yang and Ning Ye,https://www.sciencedirect.com/science/article/pii/S0031320321005987,https://doi.org/10.1016/j.patcog.2021.108422,0031-3203,2022,108422,123,Pattern Recognition,Neighborhood linear discriminant analysis,article,ZHU2022108422,
"Label noise significantly degrades the generalization ability of deep models in applications. Effective strategies and approaches (e.g., re-weighting or loss correction) are designed to alleviate the negative impact of label noise when training a neural network. Those existing works usually rely on the pre-specified architecture and manually tuning the additional hyper-parameters. In this paper, we propose warped probabilistic inference (WarPI) to achieve adaptively rectifying the training procedure for the classification network within the meta-learning scenario. In contrast to the deterministic models, WarPI is formulated as a hierarchical probabilistic model by learning an amortization meta-network, which can resolve sample ambiguity and be therefore more robust to serious label noise. Unlike the existing approximated weighting function of directly generating weight values from losses, our meta-network is learned to estimate a rectifying vector from the input of the logits and labels, which has the capability of leveraging sufficient information lying in them. The procedure provides an effective way to rectify the learning procedure for the classification network, demonstrating a significant improvement of the generalization ability. Besides, modeling the rectifying vector as a latent variable and learning the meta-network can be seamlessly integrated into the SGD optimization of the classification network. We evaluate WarPI on four benchmarks of robust learning with noisy labels and achieve the new state-of-the-art under variant noise types. Extensive study and analysis also demonstrate the effectiveness of our model.","Label noise, Meta-learning, Probabilistic model, Robust learning",Haoliang Sun and Chenhui Guo and Qi Wei and Zhongyi Han and Yilong Yin,https://www.sciencedirect.com/science/article/pii/S0031320321006439,https://doi.org/10.1016/j.patcog.2021.108467,0031-3203,2022,108467,124,Pattern Recognition,Learning to rectify for robust learning with noisy labels,article,SUN2022108467,
"It is time to stop neglecting the text around your world. In VQA, the surrounding text helps humans to understand complete visual scenes and reason question semantics efficiently. Here, we address the challenging Text-based Visual Question Answering (TextVQA) problem, which requires a model to answer the VQA questions with text reading ability. Existing TextVQA methods mainly focus on the latent relationships between detected object instances and scene texts with the given question, but ignore spatial location relationships and complex relational semantics between visual object instances and OCR texts (e.g. the A of B on C). To deal with these challenges, we propose a novel Text-Instance Graph (TIG) network for TextVQA. The TIG builds an OCR-OBJ graph for overlapping relationships modeling, where each node of graph is updated by utilizing relative objects or OCR texts. To deal with the question with complex logic, we propose a dynamic OCR-OBJ graph network to extend the perception space of graph nodes, which grasps the information of non-directly adjacent node features. Considering a scene about âthe brand of the computer on the tableâ, the model would build correlations between âbrandâ and âtableâ using âthe computerâ node as the intermediate node. Extensive experiments on three benchmarks demonstrate the effectiveness and superiority of the proposed method. In addition, our TIG achieves 0.505 ANLS on ST-VQA challenge leaderboard and sets a new state-of-the-art.","Text-based visual question answering, Spatial overlapping, Text-Instance graph, Copy mechanism",Xiangpeng Li and Bo Wu and Jingkuan Song and Lianli Gao and Pengpeng Zeng and Chuang Gan,https://www.sciencedirect.com/science/article/pii/S0031320321006312,https://doi.org/10.1016/j.patcog.2021.108455,0031-3203,2022,108455,124,Pattern Recognition,Text-instance graph: Exploring the relational semantics for text-based visual question answering,article,LI2022108455,
"In this work, we propose a new data visualization and clustering technique for discovering discriminative structures in high-dimensional data. This technique, referred to as cPCA++, is motivated by the fact that the interesting features of a âtargetâ dataset may be obscured by high variance components during traditional PCA. By analyzing what is referred to as a âbackgroundâ dataset (i.e., one that exhibits the high variance principal components but not the interesting structures), our technique is capable of efficiently highlighting the structures that are unique to the âtargetâ dataset. Similar to another recently proposed algorithm called âcontrastive PCAâ (cPCA), the proposed cPCA++ method identifies important dataset-specific patterns that are not detected by traditional PCA in a wide variety of settings. However, unlike cPCA, the proposed cPCA++ method does not require a parameter sweep, and as a result, it is significantly more efficient. Several experiments were conducted in order to compare the proposed method to state-of-the-art methods. These experiments show that the proposed method achieves performance that is similar to or better than that of the other methods, while being more efficient.","PCA, Contrastive PCA, Feature learning, Dimensionality reduction",Ronald Salloum and C.-C. Jay Kuo,https://www.sciencedirect.com/science/article/pii/S0031320321005586,https://doi.org/10.1016/j.patcog.2021.108378,0031-3203,2022,108378,124,Pattern Recognition,cPCA++: An efficient method for contrastive feature learning,article,SALLOUM2022108378,
"Breast cancer is one of the most common forms of cancer among women worldwide. The development of computer-aided diagnosis (CAD) technology based on ultrasound imaging to promote the diagnosis of breast lesions has attracted the attention of researchers and deep learning is a popular and effective method. However, most of the deep learning based CAD methods neglect the relationship between two vision tasks tumor region segmentation and classification. In this paper, taking into account some prior knowledges of medicine, we propose a novel segmentation-to-classification scheme by adding the segmentation-based attention (SBA) information to the deep convolution network (DCNN) for breast tumors classification. A segmentation network is trained to generate tumor segmentation enhancement images. Then two parallel networks extract features for the original images and segmentation enhanced images and one channel attention based feature aggregation network is to automatically integrate the features extracted from two feature networks to improve the performance of recognizing malignant tumors in the breast ultrasound images. To validate our method, experiments have been conducted on breast ultrasound datasets. The classification results of our method have been compared with those obtained by eleven existing approaches. The experimental results show that the proposed method achieves the highest Accuracy (90.78%), Sensitivity (91.18%), Specificity (90.44%), F1-score (91.46%), and AUC (0.9549).","Computer-aided diagnosis, Breast ultrasound, Deep convolution neural network, Feature combination",Yaozhong Luo and Qinghua Huang and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320321006038,https://doi.org/10.1016/j.patcog.2021.108427,0031-3203,2022,108427,124,Pattern Recognition,Segmentation information with attention integration for classification of breast tumor in ultrasound image,article,LUO2022108427,
"The person representation problem is a critical bottleneck in the player identification task. However, the current approaches for player identification utilizing the entire image features only are not sufficient to preserve identities due to the reliance on visible visual representations. In this paper, we propose a novel player representation method using a graph-powered pose representation to resolve this bottleneck problem. Our framework consists of three modules: (i.) a novel pose-guided representation module that is able to capture the pose changes dynamically and their associated effects; (ii.) a pose-guided graph embedding module using both the image deep features and the pose structure information for a better player representation inference; (iii.) an identification module as a player classifier. Experiment results on the real-world sport game scenarios demonstrate that our method achieves state-of-the-art identification performance, together with a better player representation.","Graph representation learning, Graph embedding, Pre-trained model, Player identification",Tao Feng and Kaifan Ji and Ang Bian and Chang Liu and Jianzhou Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321006798,https://doi.org/10.1016/j.patcog.2021.108503,0031-3203,2022,108503,124,Pattern Recognition,Identifying players in broadcast videos using graph convolutional network,article,FENG2022108503,
"This paper focuses on high-transferable adversarial attacks on detectors, which are hard to attack in a black-box manner, because of their multiple-output characteristics and the diversity across architectures. To pursue a high attack transferability, one plausible way is to find a common property across detectors, which facilitates the discovery of common weaknesses. We are the first to suggest that the relevance map from interpreters for detectors is such a property. Based on it, we design a Relevance Attack on Detectors (RAD), which achieves a state-of-the-art transferability, exceeding existing results by above 20%. On MS COCO, the detection mAPs for all 8 black-box architectures are more than halved and the segmentation mAPs are also significantly influenced. Given the great transferability of RAD, we generate the first adversarial dataset for object detection and instance segmentation, i.e., Adversarial Objects in COntext (AOCO), which helps to quickly evaluate and improve the robustness of detectors.","Adversarial attack, Attack transferability, Black-box attack, Relevance map, Interpreters, Object detection",Sizhe Chen and Fan He and Xiaolin Huang and Kun Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321006671,https://doi.org/10.1016/j.patcog.2021.108491,0031-3203,2022,108491,124,Pattern Recognition,Relevance attack on detectors,article,CHEN2022108491,
"This paper considers to jointly tackle the highly correlated tasks of estimating 3D human body poses and predicting future 3D motions from RGB image sequences. Based on Lie algebra pose representation, a novel self-projection mechanism is proposed that naturally preserves human motion kinematics. This is further facilitated by a sequence-to-sequence multi-task architecture based on an encoder-decoder topology, which enables us to tap into the common ground shared by both tasks. Finally, a global refinement module is proposed to boost the performance of our framework. The effectiveness of our approach, called PoseMoNet, is demonstrated by ablation tests and empirical evaluations on Human3.6M and HumanEva-I benchmark, where competitive performance is obtained comparing to the state-of-the-arts.","Pose estimation, Motion prediction, Multitask learning",Ji Yang and Youdong Ma and Xinxin Zuo and Sen Wang and Minglun Gong and Li Cheng,https://www.sciencedirect.com/science/article/pii/S0031320321006154,https://doi.org/10.1016/j.patcog.2021.108439,0031-3203,2022,108439,124,Pattern Recognition,3D pose estimation and future motion prediction from 2D images,article,YANG2022108439,
"In this work we present a framework of designing iterative techniques for image deblurring in inverse problem. The new framework is based on two observations about existing methods. We used Landweber method as the basis to develop and present the new framework but note that the framework is applicable to other iterative techniques. First, we observed that the iterative steps of Landweber method consist of a constant term, which is a low-pass filtered version of the already blurry observation. We proposed a modification to use the observed image directly. Second, we observed that Landweber method uses an estimate of the true image as the starting point. This estimate, however, does not get updated over iterations. We proposed a modification that updates this estimate as the iterative process progresses. We integrated the two modifications into one framework of iteratively deblurring images. Finally, we tested the new method and compared its performance with several existing techniques, including Landweber method, Van Cittert method, GMRES (generalized minimal residual method), and LSQR (least square), to demonstrate its superior performance in image deblurring.","Continuous forward model update, GMRES, Inverse problem, Image deblurring, Iterative algorithms, Landweber method, Least square method, Van Cittert method",Min Zhang and Geoffrey S. Young and Yanmei Tie and Xianfeng Gu and Xiaoyin Xu,https://www.sciencedirect.com/science/article/pii/S0031320321006397,https://doi.org/10.1016/j.patcog.2021.108463,0031-3203,2022,108463,124,Pattern Recognition,A new framework of designing iterative techniques for image deblurring,article,ZHANG2022108463,
"A major challenge in scene understanding is the handling of spatial relations between objects or object parts. Several descriptors dedicated to this task already exist, such as the force histogram which is a typical example of relative position descriptor. By computing the interaction between two objects for a given force in all the directions, it gives a good overview of the configuration, and it has useful properties that can make it invariant to the 2D viewpoint. Considering that using complementary forces (negative for repulsion, positive for attraction) should improve the description of complex spatial configurations, we propose to extend the force histogram to a panel of forces so as to make it a more complete descriptor. This gives a 2D descriptor that we called â(discrete) Force Bannerâ and which can be used as input of a classical Convolutional Neural Network (CNN), benefiting from their powerful performances, and reduced into more compact spatial features to use them in another system. As an illustration of its ability to describe spatial configurations, we used it to solve a classification problem aiming to discriminate simple spatial relations, but with variable configuration complexities. Experimental results obtained on datasets of synthetic and natural images with various shapes highlight the interest of this approach, in particular for complex spatial configurations.","Image analysis, Spatial relations, Relative position, Features and descriptors, Force histogram, Scene understanding",Robin DelÃ©arde and Camille Kurtz and Laurent Wendling,https://www.sciencedirect.com/science/article/pii/S0031320321005860,https://doi.org/10.1016/j.patcog.2021.108410,0031-3203,2022,108410,123,Pattern Recognition,Description and recognition of complex spatial configurations of object pairs with Force Banner 2D features,article,DELEARDE2022108410,
"Pose guided person image generation means to generate a photo-realistic person image conditioned on an input person image and a desired pose. This task requires spatial manipulation of the source image according to the target pose. However, convolutional neural networks (CNNs) are inherently limited to geometric transformations due to the fixed geometric structures in their building modules, i.e., convolution, pooling and unpooling, which cannot handle large motion and occlusions caused by large pose transform. This paper introduces a novel two-stream context-aware appearance transfer network to address these challenges. It is a three-stage architecture consisting of a source stream and a target stream. Each stage features an appearance transfer module, a multi-scale context module and two-stream feature fusion modules. The appearance transfer module handles large motion by finding the dense correspondence between the two-stream feature maps and then transferring the appearance information from the source stream to the target stream. The multi-scale context module handles occlusion via contextual modeling, which is achieved by atrous convolutions of different sampling rates. Both quantitative and qualitative results indicate the proposed network can effectively handle challenging cases of large pose transform while retaining the appearance details. Compared with state-of-the-art approaches, it achieves comparable or superior performance using much fewer parameters while being significantly faster.","Person image generation, Appearance transfer, Multi-scale context, Efficient image generation",Chengkang Shen and Peiyan Wang and Wei Tang,https://www.sciencedirect.com/science/article/pii/S0031320321006270,https://doi.org/10.1016/j.patcog.2021.108451,0031-3203,2022,108451,124,Pattern Recognition,Exploiting appearance transfer and multi-scale context for efficient person image generation,article,SHEN2022108451,
"Surface defect segmentation is very important for the quality inspection of industrial production and is an important pattern recognition problem. Although deep learning (DL) has achieved remarkable results in surface defect segmentation, most of these results have been obtained by using massive images with pixel-level annotations, which are difficult to obtain at industrial sites. This paper proposes a weakly supervised defect segmentation method based on the dynamic templates generated by an improved cycle-consistent generative adversarial network (CycleGAN) trained by image-level annotations. To generate better templates for defects with weak signals, we propose a defect attention module by applying the defect residual for the discriminator to strengthen the elimination of defect regions and suppress changes in the background. A defect cycle-consistent loss is designed by adding structural similarity (SSIM) to the original L1 loss to include the grayscale and structural features; the proposed loss can better model the inner structure of defects. After obtaining the defect-free template, a defect segmentation map can easily be obtained through a simple image comparison and threshold segmentation. Experiments show that the proposed method is both efficient and effective, significantly outperforms other weakly supervised methods, and achieves performance that is comparable or even superior to that of supervised methods on three industrial datasets (intersection over union (IoU) on the DAGM 2007, KSD and CCSD datasets of 78.28%, 59.43%,and 68.83%, respectively). The proposed method can also be employed as a semiautomatic annotation tool combined with active learning.","Weakly supervised learning, Defect detection, Image segmentation, Generative adversarial network (GAN), Attention model",Shuanlong Niu and Bin Li and Xinggang Wang and Songping He and Yaru Peng,https://www.sciencedirect.com/science/article/pii/S0031320321005616,https://doi.org/10.1016/j.patcog.2021.108396,0031-3203,2022,108396,123,Pattern Recognition,Defect attention template generation cycleGAN for weakly supervised surface defect segmentation,article,NIU2022108396,
"Joint image denoising algorithms use the structures of the guidance image as a prior to restore the noisy target image. While the provided guidance images are helpful to improve the denoising performance, the denoised edges are most likely to be blurred especially when the edges of the guidance image are weak or inexistent. To address this weakness, this paper proposes a new gradient-direction-based joint image denoising method in which the absolute cosine value of the angle between two gradient vectors of the guidance image and those of the image to recover is employed as the parallel measurement to ensure that the gradient directions of the denoised image are approximately the same as or opposite to those of the guidance image. Besides, a new edge-preserving regularization term is developed to alleviate the effects of the unreliable prior information from guidance image. To simplify the resultant complex nonconvex and nonlinear fractional model, the logarithm function is employed to convert the multiplication operation into addition operation. Then, we construct the surrogate function for the logarithmic term of l2-norm, and separate the variables to transform the objective function into convex one with high numerical stability while retaining high efficiency. Finally, the optimal solutions can be obtained by directly minimizing the convex functions. Experimental results on public datasets and from nine benchmark methods consistently demonstrate the effectiveness of the proposed method both visually and quantitatively.","Joint image denoising, Gradient direction, Majorization minimization, Nonlinear optimization, Nonconvex optimization",Pengliang Li and Junli Liang and Miaohua Zhang and Wen Fan and Guoyang Yu,https://www.sciencedirect.com/science/article/pii/S0031320321006828,https://doi.org/10.1016/j.patcog.2021.108506,0031-3203,2022,108506,125,Pattern Recognition,Joint image denoising with gradient direction and edge-preserving regularization,article,LI2022108506,
"Detection and recognition of scene texts of arbitrary shapes remain a grand challenge due to the super-rich text shape variation in text line orientations, lengths, curvatures, etc. This paper presents a mask-guided multi-task network that detects and rectifies scene texts of arbitrary shapes reliably. Three types of keypoints are detected which specify the centre line and so the shape of text instances accurately. In addition, four types of keypoint links are detected of which the horizontal links associate the detected keypoints of each text instance and the vertical links predict a pair of landmark points (for each keypoint) along the upper and lower text boundary, respectively. Scene texts can be located and rectified by linking up the associated landmark points (giving localization polygon boxes) and transforming the polygon boxes via thin plate spline, respectively. Extensive experiments over several public datasets show that the use of text keypoints is tolerant to the variation in text orientations, lengths, and curvatures, and it achieves competitive scene text detection and rectification performance as compared with state-of-the-art methods.","Scene text detection, Scene text recognition, Deep learning, Neural network",Chuhui Xue and Shijian Lu and Steven Hoi,https://www.sciencedirect.com/science/article/pii/S0031320321006701,https://doi.org/10.1016/j.patcog.2021.108494,0031-3203,2022,108494,124,Pattern Recognition,Detection and rectification of arbitrary shaped scene texts by using text keypoints and links,article,XUE2022108494,
"In this era of active development of autonomous vehicles, it becomes crucial to provide driving systems with the capacity to explain their decisions. In this work, we focus on generating high-level driving explanations as the vehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep architecture which explains the behavior of a trajectory prediction model. Supervised by annotations of human driving decisions justifications, BEEF learns to fuse features from multiple levels. Leveraging recent advances in the multi-modal fusion literature, BEEF is carefully designed to model the correlations between high-level decisions features and mid-level perceptual features. The flexibility and efficiency of our approach are validated with extensive experiments on the HDD and BDD-X datasets.","Explainable self-driving, Multi-level fusion, Cause classification, Natural language explanations, HDD, BDD-X",HÃ©di Ben-Younes and Ãloi Zablocki and Patrick PÃ©rez and Matthieu Cord,https://www.sciencedirect.com/science/article/pii/S0031320321005975,https://doi.org/10.1016/j.patcog.2021.108421,0031-3203,2022,108421,123,Pattern Recognition,Driving behavior explanation with multi-level fusion,article,BENYOUNES2022108421,
"Recent advances in Deep Reinforcement Learning (DRL) demonstrates the potential for solving Combinatorial Optimization (CO) problems. DRL shows advantages over traditional methods both on scalability and computation efficiency. However, the DRL problems transformed from CO problems usually have a huge state space, and the main challenge of solving them has changed from high computation complexity to high sample complexity. Credit assignment determines the contribution of each internal decision to the final success or failure, and it has been shown to be effective in reducing the sample complexity of the training process. In this paper, we resort to a model-based reinforcement learning method to assign credits for model-free DRL methods. Since heuristic methods plays an important role on state-of-the-art solutions for CO problems, we propose using a model to represent those heuristic knowledge and derive the credit assignment from the model. This model-based credit assignment can facilitate the model-free DRL to perform a more effective exploration, and the data collected by the model-free DRL refines the model continuously as the training progresses. Extensive experiments on various CO problems with different settings show that our framework outperforms previous state-of-the-art methods on performance and training efficiency.","Combinatorial optimization, Reinforcement learning, Credit assignment",Dong Yan and Jiayi Weng and Shiyu Huang and Chongxuan Li and Yichi Zhou and Hang Su and Jun Zhu,https://www.sciencedirect.com/science/article/pii/S0031320321006427,https://doi.org/10.1016/j.patcog.2021.108466,0031-3203,2022,108466,124,Pattern Recognition,Deep reinforcement learning with credit assignment for combinatorial optimization,article,YAN2022108466,
"As a sub-branch of behavioral biometrics, online signature verification systems deal with unique signing characteristics, which could be better differentiated by extraction of habitual singing styles instead of geometric features in case of perfect forgery. Even if the signatures are geometrically identical, speed and frequency components of the signing process might significantly vary. Therefore, a novel framework is introduced as a new signature verification protocol for touchscreen devices using barcodes containing the dominant frequency component of the speed signals. A special interface is designed as signature tracker to extract the displacement data sampled from the signing process. The speed signals are interpolated from the displacement data and the frequency components of the signals are computed by scalograms analysis governed by continuous wavelet transformations (CWT). The signature barcodes are generated as 4-scale scalograms and classified by support vector machines (SVM). Among several compatible wavelets, Gaussian derivative wavelet is selected for generating scalograms and the results of the process are calculated as 2.25% FAR, 2.75% FRR and 2.81%EER for our dataset. The framework is also tested with SVC2004 data that we achieved 0% FAR, 9.33% FRR and 8%EER, also with SUSIG-Visual, SUSIG-Blind, MOBISIG databases and we reached between 1.22%-3.62% average EERs, which are competitive among the relevant results. Given the promising outcomes, the signature barcoding is very reliable method which could be executed by a simple touchscreen interface collecting the barcodes for storing and benchmarking when needed.","Barcode, Biometrics, Online signature verification, Scalogram, Frequency, SVM",Orcan Alpar,https://www.sciencedirect.com/science/article/pii/S0031320321006026,https://doi.org/10.1016/j.patcog.2021.108426,0031-3203,2022,108426,124,Pattern Recognition,Signature barcodes for online verification,article,ALPAR2022108426,
"Siamese networks have achieved great success in visual tracking with the advantages of speed and accuracy. However, how to track an object precisely and robustly still remains challenging. One reason is that multiple types of features are required to achieve good precision and robustness, which are unattainable by a single training phase. Moreover, Siamese networks usually struggle with online adaption problem. In this paper, we present a novel two-stage aware attentional Siamese network for tracking (Ta-ASiam). Concretely, we first propose a position-aware and an appearance-aware training strategy to optimize different layers of Siamese network. By introducing diverse training patterns, two types of required features can be captured simultaneously. Then, following the rule of feature distribution, an effective feature selection module is constructed by combining both channel and spatial attention networks to adapt to rapid appearance changes of the object. Extensive experiments on various latest benchmarks have well demonstrated the effectiveness of our method, which significantly outperforms state-of-the-art trackers.","Visual tracking, Siamese network, Feature learning, Attention network",Xinglong Sun and Guangliang Han and Lihong Guo and Hang Yang and Xiaotian Wu and Qingqing Li,https://www.sciencedirect.com/science/article/pii/S0031320321006786,https://doi.org/10.1016/j.patcog.2021.108502,0031-3203,2022,108502,124,Pattern Recognition,Two-stage aware attentional Siamese network for visual tracking,article,SUN2022108502,
"Supervised subspace projection technology is a major method for dimensionality reduction in pattern recognition. At present, most supervised subspace projection algorithms are derived from the multi-dimensional extended version of Fisher linear discriminant analysis (FDA), also known as Multi-dimensional Fisher discriminant analysis (MD-FDA). However, MD-FDA needs to be improved further because the projection vectors in the noise-subspace cannot be sorted and the ill-condition of the within-class scatter matrix may cause severe numerical instabilities. Generalized discriminant component analysis (GDCA), the generalization of MD-FDA, together with its kernelization forms are proposed and correspondingly rigorous mathematical proofs are detailed in this paper. By virtue of 5 validation data sets derived from UCI Machine Learning Repository and our laboratory, the theoretical validity and technical advantages of GDCA as well as its kernelization forms are verified, and the effectiveness of the newly proposed method is demonstrated in comparison with 36 kinds of state-of-the-art dimensionality reduction algorithms.","Dimensionality reduction, Subspace projection, Generalized discriminant component analysis, Pattern recognition",Ruixu Zhou and Wensheng Gao and Dengwei Ding and Weidong Liu,https://www.sciencedirect.com/science/article/pii/S0031320321006269,https://doi.org/10.1016/j.patcog.2021.108450,0031-3203,2022,108450,124,Pattern Recognition,Supervised dimensionality reduction technology of generalized discriminant component analysis and its kernelization forms,article,ZHOU2022108450,
"Point clouds-based networks have achieved great attention in 3D object classification, segmentation, and indoor scene semantic parsing, but its application to 3D face recognition is still underdeveloped owing to two main reasons: lack of large-scale 3D facial data and absence of deep neural network that can directly extract discriminative face representations from point clouds. To address these two problems, a PointNet++ based network is proposed in this paper to extract face features directly from point clouds facial scans and a statistical 3D Morphable Model based 3D face synthesizing strategy is established to generate large-scale unreal facial scans to train the proposed network from scratch. A curvature-aware point sampling technique is proposed to hierarchically down-sample feature-sensitive points which are crucial to pass and aggregate discriminative facial features deeply. In addition, a novel 3D face transfer learning method is proposed to ease the domain discrepancy between synthetic and âin-the-wildâ faces. Experimental results on two public 3D face benchmarks show that the network trained only on synthesized data can also be well generalized to âin-the-wildâ 3D face recognition. Our method achieves the state-of-the-art results by achieving an overall rank-1 identification rate of 99.46% and 99.65% on FRGCv2 and Bosphorus, respectively. Further, we evaluate on a self-collected dataset to demonstrate the robustness and application potential of our method.","3D face recognition, Learning from synthetic, Curvature-aware point sampling, Transfer learning",Ziyu Zhang and Feipeng Da and Yi Yu,https://www.sciencedirect.com/science/article/pii/S0031320321005732,https://doi.org/10.1016/j.patcog.2021.108394,0031-3203,2022,108394,123,Pattern Recognition,Learning directly from synthetic point clouds for âin-the-wildâ 3D face recognition,article,ZHANG2022108394,
"Person re-identification (Re-ID) aims to search for the same pedestrian in different cameras, which is a crucial research direction in pattern recognition. Recent deep learning methods have advanced the development of Re-ID. However, the existing approaches easily result in performance degradation in the case of larger scene data because they do not adequately consider the spatial dependencies of both the inter-image and the intra-image. The paper proposes a novel Spatial-Driven Network (SDN) to learn particularly discriminative features with abundant semantic information from both the inter-image and the intra-image dependencies for person Re-ID. Firstly, we design a global-correlation attention module to capture the inter-image dependencies among a series of different pedestrian images. Secondly, we present a local-correlation attention module to compute the intra-image dependencies from any pair of pixels within each pedestrian image. Furthermore, we propose a specific network integration mechanism, which carefully combines the above two complementary modules to match well the solution of the spatial dependency problem. We implement numerous experiments to assess the proposed SDN on mainstream person Re-ID databases. The results demonstrate that the proposed SDN outperforms most of the state-of-the-art methods in typical key criteria.","Person re-identification, Spatial dependencies, Recurrent neural network, Deep learning",Tongzhen Si and Fazhi He and Haoran Wu and Yansong Duan,https://www.sciencedirect.com/science/article/pii/S0031320321006385,https://doi.org/10.1016/j.patcog.2021.108462,0031-3203,2022,108462,124,Pattern Recognition,Spatial-driven features based on image dependencies for person re-identification,article,SI2022108462,
"Using the face as a biometric identity trait is motivated by the contactless nature of the capture process and the high accuracy of the recognition algorithms. After the current COVID-19 pandemic, wearing a face mask has been imposed in public places to keep the pandemic under control. However, face occlusion due to wearing a mask presents an emerging challenge for face recognition systems. In this paper, we present a solution to improve masked face recognition performance. Specifically, we propose the Embedding Unmasking Model (EUM) operated on top of existing face recognition models. We also propose a novel loss function, the Self-restrained Triplet (SRT), which enabled the EUM to produce embeddings similar to these of unmasked faces of the same identities. The achieved evaluation results on three face recognition models, two real masked datasets, and two synthetically generated masked face datasets proved that our proposed approach significantly improves the performance in most experimental settings.","COVID-19, Biometric recognition, Identity verification, Masked face recognition",Fadi Boutros and Naser Damer and Florian Kirchbuchner and Arjan Kuijper,https://www.sciencedirect.com/science/article/pii/S003132032100649X,https://doi.org/10.1016/j.patcog.2021.108473,0031-3203,2022,108473,124,Pattern Recognition,Self-restrained triplet loss for accurate masked face recognition,article,BOUTROS2022108473,
"Conventional sampling techniques fall short of selecting representatives that encode the underlying conformation of non-linear manifolds. The problem is exacerbated if the data is contaminated with gross sparse corruptions. In this paper, we present a data selection approach, dubbed MoSSaRT, which draws robust and descriptive sketches of grossly corrupted manifold structures. Built upon an explicit randomized transformation, we obtain a judiciously designed representation of the data relations, which facilitates a versatile selection approach accounting for robustness to gross corruption, descriptiveness and novelty of the chosen representatives, simultaneously. Our model lends itself to a convex formulation with an efficient parallelizable algorithm, which coupled with our randomized matrix structures gives rise to a highly scalable implementation. Theoretical analysis guarantees probabilistic convergence of the approximate function to the desired objective function and reveals insightful geometrical characterization of the chosen representatives. Finally, MoSSaRT substantially outperforms the state-of-the-art algorithms as demonstrated by experiments conducted on both real and synthetic data.","Representative selection, Gross sparse corruption, Manifold learning, Reproducing kernel Hilbert spaces",Mahlagha Sedghi and Michael Georgiopoulos and George K. Atia,https://www.sciencedirect.com/science/article/pii/S0031320321006300,https://doi.org/10.1016/j.patcog.2021.108454,0031-3203,2022,108454,124,Pattern Recognition,Sketches by MoSSaRT: Representative selection from manifolds with gross sparse corruptions,article,SEDGHI2022108454,
"Images taken in the rain usually have poor visual quality, which may cause difficulties for vision-based analysis systems. The research aims to recover clean image content from a single rainy image by removing rain components without introducing any artifacts. Existing rain removal methods often model the rain component as noise, but it obviously has clear patterns instead of random noise. Motivated by this, we raise the idea to build modules to capture rain patterns for de-raining. A Rain-Component-Aware (RCA) network is proposed to capture the characteristics of the rain. We then integrate it into an image-conditioned generative adversarial network (image-cGAN) as a RCA loss to guide the generation of rainless images. This results in the proposed two-branch cGAN, where one branch aims at improving the image visual quality after de-raining, and the other aims at extracting rain patterns so that the rain could be effectively removed. To better capture the spatial relationship of different objects within an image, we incorporate the capsule structure in both generator and discriminator of cGAN, which further improves the quality of generated images. The proposed approach is hence named as RCA-cGAN. Benefited by the RCA loss based two-branch optimization and the capsule structure, RCA-cGAN achieves good de-raining effect. Extensive experimental results on several benchmark datasets show that the RCA network is effective to capture rain patterns and the proposed approach could produce much better de-raining images in terms of both subjective visual quality inspection and objective quantitative assessment.","De-raining, Capsule, Generative adversarial network, Rain-component-aware network",Fei Yang and Jianfeng Ren and Zheng Lu and Jialu Zhang and Qian Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321005574,https://doi.org/10.1016/j.patcog.2021.108377,0031-3203,2022,108377,123,Pattern Recognition,Rain-component-aware capsule-GAN for single image de-raining,article,YANG2022108377,
"Human-Object Interaction (HOI) detection aims to detect visual relations between humans and objects in images. One significant problem of HOI detection is that non-interactive human-object pair can be easily mis-grouped and misclassified as an action, especially when the humans are close and performing similar actions in the scene. To address the mis-grouping problem, we propose a spatial enhancement approach to enforce fine-level spatial constraints in two directions between human body parts and object parts. At inference, we propose a human-object regrouping approach for object-exclusive actions by considering the object-exclusive property of the interactive object, where the target object should not be shared by more than one human. By suppressing non-interactive pairs, our approach can decrease the false positives. Experiments on V-COCO and HICO-DET datasets demonstrate our approach is more robust compared to the existing methods under the presence of multiple humans and objects in the scene.","Human-object interaction detection, Two-direction spatial enhancement, Exclusive object prior, Mis-grouped human-object pairs, Non-interactive suppression",Lu Liu and Robby T. Tan,https://www.sciencedirect.com/science/article/pii/S0031320321006142,https://doi.org/10.1016/j.patcog.2021.108438,0031-3203,2022,108438,124,Pattern Recognition,Human object interaction detection using two-direction spatial enhancement and exclusive object prior,article,LIU2022108438,
"Most modern face completion approaches adopt an autoencoder or its variants to restore missing regions in face images. Encoders are often utilized to learn powerful representations that play an important role in meeting the challenges of sophisticated learning tasks. Specifically, various kinds of masks are often presented in face images in the wild, forming complex patterns, especially in this hard period of COVID-19. Itâs difficult for encoders to capture such powerful representations under this complex situation. To address this challenge, we propose a self-supervised Siamese inference network to improve the generalization and robustness of encoders. It can encode contextual semantics from full-resolution images and obtain more discriminative representations. To deal with geometric variations of face images, a dense correspondence field is integrated into the network. We further propose a multi-scale decoder with a novel dual attention fusion module (DAF), which can combine the restored and known regions in an adaptive manner. This multi-scale architecture is beneficial for the decoder to utilize discriminative representations learned from encoders into images. Extensive experiments clearly demonstrate that the proposed approach not only achieves more appealing results compared with state-of-the-art methods but also improves the performance of masked face recognition dramatically.","Face completion, Unsupervised learning, Attention mechanism, 3D Face analysis",Xin Ma and Xiaoqiang Zhou and Huaibo Huang and Gengyun Jia and Zhenhua Chai and Xiaolin Wei,https://www.sciencedirect.com/science/article/pii/S0031320321006415,https://doi.org/10.1016/j.patcog.2021.108465,0031-3203,2022,108465,124,Pattern Recognition,Contrastive attention network with dense field estimation for face completion,article,MA2022108465,
"Feature matching, which refers to determining reliable correspondences between two sets of feature points, is a fundamental component of numerous visual tasks. This paper proposes a novel method, termed as guided neighborhood affine subspace embedding (NASE), to eliminate false matches from the given tentative feature matches. Its essential philosophy is to preserve the underlying intrinsic manifold of potential true matches. Specifically, we aim to approximate the manifold of an inlier with an affine subspace fitted on its neighbors by imposing a motion-consistency constraint. Considering that the âcorresponding manifoldâ of inliers may be biased by gross outliers, we introduce a density-based seed point selection strategy for neighborhood refinement. Based on the above two strategies, we further formulate the general feature matching problem into a mathematical optimization model and deduce a closed-form solution with linearithmic time complexity (i.e., O(NlogN)) for mismatch removal. Additionally, we devise a multi-scale strategy for neighborhood construction, making our method more robust to various degradations. Extensive experiments on general feature matching, fundamental matrix estimation, and loop closure detection demonstrate the clear superiority of NASE over the state-of-the-arts.","Feature matching, Image correspondence, Neighborhood affine subspace, Multi-scale, Outlier, Mismatch removal",Zizhuo Li and Yong Ma and Xiaoguang Mei and Jun Huang and Jiayi Ma,https://www.sciencedirect.com/science/article/pii/S0031320321006658,https://doi.org/10.1016/j.patcog.2021.108489,0031-3203,2022,108489,124,Pattern Recognition,Guided neighborhood affine subspace embedding for feature matching,article,LI2022108489,
"An interesting development in automatic visual recognition has been the emergence of tasks where it is not possible to assign objective labels to images, yet still feasible to collect annotations that reflect human judgements about them. Machine learning-based predictors for these tasks rely on supervised training that models the behavior of the annotators, i.e., what would the average personâs judgement be for an image? A key open question for this type of work, especially for applications where inconsistency with human behavior can lead to ethical lapses, is how to evaluate the epistemic uncertainty of trained predictors, i.e., the uncertainty that comes from the predictorâs model. We propose a Bayesian framework for evaluating black box predictors in this regime, agnostic to the predictorâs internal structure. The framework specifies how to estimate the epistemic uncertainty that comes from the predictor with respect to human labels by approximating a conditional distribution and producing a credible interval for the predictions and their measures of performance. The framework is successfully applied to four image classification tasks that use subjective human judgements: facial beauty assessment, social attribute assignment, apparent age estimation, and ambiguous scene labeling.","Uncertainty estimation, Epistemic uncertainty, Supervised learning, Bayesian inference, Bayesian modeling",Derek S. Prijatelj and Mel McCurrie and Samuel E. Anthony and Walter J. Scheirer,https://www.sciencedirect.com/science/article/pii/S0031320321005604,https://doi.org/10.1016/j.patcog.2021.108395,0031-3203,2022,108395,123,Pattern Recognition,A Bayesian evaluation framework for subjectivelyÂ annotatedÂ visualÂ recognitionÂ tasks,article,PRIJATELJ2022108395,
"To achieve high-resolution segmentation results, typical semantic segmentation models often require high-resolution inputs. However, high-resolution inputs inevitably bring high cost on computation, which limits its application seriously in realistic scenarios. To address the problem, we propose to predict a high-resolution semantic segmentation result with a degraded low-resolution image as input, which is called super-resolution semantic segmentation in this paper. We further propose a Relation Calibrating Network (RCNet) for this task. Specifically, we propose two modules, namely Relation Upsampling Module (RUM) and Feature Calibrating Module (FCM). In RUM, the input feature map generates the relation map of pixels in low-resolution, which is then gradually upsampled to high-resolution. Meanwhile, FCM takes the input feature map and the relation map from RUM as inputs, gradually calibrating the feature. Finally, the last FCM outputs the high-resolution segmentation results. We conduct extensive experiments to verify the effectiveness of our method. Specially, we achieve a comparable segmentation result (from 70.01% to 70.90%) with only 1/4 of the computational cost (from 1107.57 to 255.72 GFLOPs) based on FCN on Cityscapes dataset.","Image semantic segmentation, Super-resolution semantic segmentation, Relation calibrating",Jie Jiang and Jing Liu and Jun Fu and Weining Wang and Hanqing Lu,https://www.sciencedirect.com/science/article/pii/S0031320321006774,https://doi.org/10.1016/j.patcog.2021.108501,0031-3203,2022,108501,124,Pattern Recognition,Super-resolution semantic segmentation with relation calibrating network,article,JIANG2022108501,
"Hierarchical classification is significant for big data, where the original task is divided into several sub-tasks to provide multi-granularity predictions based on a tree-shape label structure. Obviously, these sub-tasks are highly correlated: results of the coarser-grained sub-tasks can reduce the candidates for the fine-grained sub-tasks, while results of the fine-grained sub-tasks provide attributes describing the coarser-grained classes. A human can integrate feedbacks from all the related sub-tasks instead of considering each sub-task independently. Therefore, we propose a deep collaborative multi-task network for hierarchical image classification. Specifically, we first extract the relationship matrix between every two sub-tasks defined by the hierarchical label structure. Then, the information of each sub-task is broadcasted to all the related sub-tasks through the relationship matrix. Finally, to combine this information, a novel fusion function based on the task evaluation and the decision uncertainty is designed. Extensive experimental results demonstrate that our model can achieve state-of-the-art performance.","Hierarchical image classification, Deep multi-task network, Collaborative learning, Decision uncertainty evaluation",Yu Zhou and Xiaoni Li and Yucan Zhou and Yu Wang and Qinghua Hu and Weiping Wang,https://www.sciencedirect.com/science/article/pii/S0031320321006257,https://doi.org/10.1016/j.patcog.2021.108449,0031-3203,2022,108449,124,Pattern Recognition,Deep collaborative multi-task network: A human decision process inspired model for hierarchical image classification,article,ZHOU2022108449,
"Accurate segmentation of the brain into gray matter, white matter, and cerebrospinal fluid using magnetic resonance (MR) imaging is critical for visualization and quantification of brain anatomy. Compared to 3T MR images, 7T MR images exhibit higher tissue contrast that is contributive to accurate tissue delineation for training segmentation models. In this paper, we propose a cascaded nested network (CaNes-Net) for segmentation of 3T brain MR images, trained by tissue labels delineated from the corresponding 7T images. We first train a nested network (Nes-Net) for a rough segmentation. The second Nes-Net uses tissue-specific geodesic distance maps as contextual information to refine the segmentation. This process is iterated to build CaNes-Net with a cascade of Nes-Net modules to gradually refine the segmentation. To alleviate the misalignment between 3T and corresponding 7T MR images, we incorporate a correlation coefficient map to allow well-aligned voxels to play a more important role in supervising the training process. We compared CaNes-Net with SPM and FSL tools, as well as four deep learning models on 18 adult subjects and the ADNI dataset. Our results indicate that CaNes-Net reduces segmentation errors caused by the misalignment and improves segmentation accuracy substantially over the competing methods.","Brain segmentation, Cascaded nested network, Deep learning, Magnetic resonance imaging",Jie Wei and Zhengwang Wu and Li Wang and Toan Duc Bui and Liangqiong Qu and Pew-Thian Yap and Yong Xia and Gang Li and Dinggang Shen,https://www.sciencedirect.com/science/article/pii/S0031320321005963,https://doi.org/10.1016/j.patcog.2021.108420,0031-3203,2022,108420,124,Pattern Recognition,A cascaded nested network for 3T brain MR image segmentation guided by 7T labeling,article,WEI2022108420,
"The objective of entity alignment is to judge whether entities refer to the same object in the real world. Methods for entity alignment can be grossly divided into two groups: conventional symbol-based entity alignment methods and embedding-based entity alignment methods. Both groups of methods have advantages and disadvantages (which are detailed in Section 1). Therefore, combining the advantages of both methods might be a promising strategy. However, to the best of our knowledge, only the RTEA algorithm that was proposed in our previous conference paper (Proceeding of Pacific Rim International Conference on Artificial Intelligence, pp. 162â175, 2019) utilizes this strategy for entity alignment. This manuscript is an extended version of that conference paper, in which an improved algorithm, namely, ESEA (combining embedding-based and symbol-based methods for entity alignment), is proposed based on the following steps. First, a novel method for combining embedding models with symbol-based models is proposed. Entities with high vector similarities are obtained through a hybrid embedding model, and the final aligned entity pairs are calculated via symbol-based methods. Second, a series of symbol-based methods, instead of only the edit distance method in the original version, are combined with embedding-based methods for relation alignment. Third, we combine symbol-based and embedding-based methods in a more complicated framework with the objective of better exploiting the advantages of both methods. The experimental results on real-world datasets demonstrate that the proposed method outperformed several state-of-the-art embedding-based entity alignment approaches and outperformed our previous RTEA method.","Entity alignment, Knowledge graph embedding, String Similarity",Tingting Jiang and Chenyang Bu and Yi Zhu and Xindong Wu,https://www.sciencedirect.com/science/article/pii/S0031320321006099,https://doi.org/10.1016/j.patcog.2021.108433,0031-3203,2022,108433,124,Pattern Recognition,Combining embedding-based and symbol-based methods for entity alignment,article,JIANG2022108433,
"Detecting rotated faces is a challenging task with images from uncontrolled environments. The use of deep convolutional neural networks have greatly improved detection performance, but these methods still do not fully exploit face structure information. This leaves faces with more extreme rotation angles undetectable. In this paper, we present a novel Multi-Task Collaboration Network (MTCNet) for rotation-invariance face detection that fully uses facial landmarks to improve the detection performance by means of collaboration between face detection and face alignment. Differing from previous methods that predict rotation angles in a single step, MTCNet employs a cascaded architecture with three stages to predict faces with gradually decreasing rotation-in-plane ranges in a coarse-to-fine process. Accurate facial landmarks further facilitate face detection. We also introduce a new training loss by integrating the geometric angle into the penalization process, which is much more reasonable than measuring the differences of training samples roughly. Our approach also explores contextual information to distinguish challenging faces from unconstrained scenarios. Extensive experimental results were conducted to demonstrate the effectiveness of MTCNet on both the multiple orientation and rotation datasets. Empirical studies show that MTCNet achieves results competitive with state-of-the-art face detectors while being time-efficient.","Rotation-invariant face detection, Face alignment, Multi-task learning",Lifang Zhou and Hui Zhao and Jiaxu Leng,https://www.sciencedirect.com/science/article/pii/S0031320321006014,https://doi.org/10.1016/j.patcog.2021.108425,0031-3203,2022,108425,124,Pattern Recognition,MTCNet: Multi-task collaboration network for rotation-invariance face detection,article,ZHOU2022108425,
"Increasing object detectors reveal the importance of feature representation in improving detection performance. Currently, feature enhancement mainly focuses on Feature Pyramid Network (FPN) as well as Region-of-Interest (RoI) feature fusion in two-stage object detectors. Based on this, we propose Adaptive Region-aware Feature Enhancement method including Adaptive Region-aware FPN (AR-FPN) and Adaptive Region-aware RoI Feature Fusion (AR-RFF) modules. Specifically, AR-FPN aims to capture position-sensitive map for each level to enhance the pixel-wise interest degree and make the differences among levels more distinctive. AR-RFF focuses on obtaining distinguishable RoI features by introducing adaptive region information and eliminating scale inconsistency between the refined and original features. Extensive experiments show that our method acquires 1.7% AP higher at least and strong generalization capability compared to others.","Object detection, Feature enhancement, Adaptive region-aware FPN, Adaptive region-aware RoI feature fusion",Zhongjie Fan and Qiong Liu,https://www.sciencedirect.com/science/article/pii/S0031320321006130,https://doi.org/10.1016/j.patcog.2021.108437,0031-3203,2022,108437,124,Pattern Recognition,Adaptive region-aware feature enhancement for object detection,article,FAN2022108437,
"Weakly supervised semantic segmentation task aims to learn a segmentation model with only image-level annotations. Existing methods generally refine the initial seeds to obtain pseudo labels for training a fully supervised model. In recent years, some affinity-based methods perform well in this task. However, most of these methods only focus on the localization information from class activation map, while ignoring rule-based appearance information. In this paper, we find that the superpixel guidance is helpful for mining semantic affinities between pixels because pixels belonging to the same superpixel often have the same class label. As such, we propose a Superpixel Guided Weakly Segmentation framework, which alternately learns two modules to fuse superpixel information and localization information. The semantic segmentation results are more consistent with the imageâs local and global consistency through our framework. Experiments show that the proposed method achieves state-of-the-art performance, with mIoU at 70.5% on the PASCAL VOC 2012 test set and mIoU at 34.4% on the MS-COCO 2014 val set.","Weakly supervised condition, Semantic segmentation, Pixel-level affinity, Superpixel",Sheng Yi and Huimin Ma and Xiang Wang and Tianyu Hu and Xi Li and Yu Wang,https://www.sciencedirect.com/science/article/pii/S0031320321006804,https://doi.org/10.1016/j.patcog.2021.108504,0031-3203,2022,108504,124,Pattern Recognition,Weakly-supervised semantic segmentation with superpixel guided local and global consistency,article,YI2022108504,
"It is well recognized that the contextual information of surrounding objects is beneficial for object detection. Such contextual information can often be obtained from long-range dependencies. This paper proposes a sparse attention block to capture long-range dependencies in an efficient way. Unlike the conventional non-local block, which generates a dense attention map to characterize the dependency between any two positions of the input feature map, our sparse attention block samples the most representative positions for contextual information aggregation. After searching for local peaks in a heat map of the given input feature map, it adaptively selects a sparse set of positions to represent the relationship between query and key elements. With the obtained sparse positions, our sparse attention block can well model long-range dependencies, and greatly improve the object detection performance at the additional cost of <2% GPU memory and computation of the conventional non-local block. This sparse attention block can be easily plugged into various object detection frameworks, such as Faster R-CNN, RetinaNet and Mask R-CNN. Experiments on COCO benchmark confirm that our sparse attention block can boost the detection accuracy with significant gains ranging from 1.4% to 1.9% and negligible overhead of computation and memory usage.","Object detection, Self-attention, Convolution neural network",Chunlin Chen and Jun Yu and Qiang Ling,https://www.sciencedirect.com/science/article/pii/S003132032100594X,https://doi.org/10.1016/j.patcog.2021.108418,0031-3203,2022,108418,124,Pattern Recognition,Sparse attention block: Aggregating contextual information for object detection,article,CHEN2022108418,
"The chips contained in any electronic device are manufactured over circular silicon wafers, which are monitored by inspection machines at different production stages. Inspection machines detect and locate any defect within the wafer and return a Wafer Defect Map (WDM), i.e., a list of the coordinates where defects lie, which can be considered a huge, sparse, and binary image. In normal conditions, wafers exhibit a small number of randomly distributed defects, while defects grouped in specific patterns might indicate known or novel categories of failures in the production line. Needless to say, a primary concern of semiconductor industries is to identify these patterns and intervene as soon as possible to restore normal production conditions. Here we address WDM monitoring as an open-set recognition problem, where the aim is to classify WDM in known categories and promptly detect novel patterns. In particular, we propose a comprehensive pipeline for wafer monitoring based on a Submanifold Sparse Convolutional Network, a deep architecture designed to process sparse data at an arbitrary resolution, which is trained on the known classes. To detect novelties, we define an outlier detector based on a Gaussian Mixture Model fitted on the latent representation of the classifier. Our experiments on a real dataset of WDMs show that directly processing full-resolution WDMs by Submanifold Sparse Convolutions yields superior classification performance on known classes than traditional Convolutional Neural Networks, which require a preliminary binning to reduce the size of the binary images representing WDMs. Moreover, our solution outperforms state-of-the-art open-set recognition solutions in novelty detection.","Pattern classification, Open-set recognition, Sparse convolutions, Quality inspection, Wafer monitoring",Luca Frittoli and Diego Carrera and Beatrice Rossi and Pasqualina Fragneto and Giacomo Boracchi,https://www.sciencedirect.com/science/article/pii/S0031320321006646,https://doi.org/10.1016/j.patcog.2021.108488,0031-3203,2022,108488,124,Pattern Recognition,Deep open-set recognition for silicon wafer production monitoring,article,FRITTOLI2022108488,
"The existing domain adaptive object detection methods often need to carry a large number of source domain samples for domain adaptation, which is not realistic due to GPU limitations, privacy and physical memory in practical applications. To solve this problem, we propose a source data-free domain adaptive object detection method. Only unlabeled target domain data is used to optimize the source domain model so that it can work better in the target domain. Our method takes Faster R-CNN as baseline. Specifically, we first construct global class prototypes which will be updated in batch iteratively. Then based on the global class prototypes, more accurate pseudo-labels are generated for training the target model. In this way, the source and target domains are also implicitly aligned. Our contributions are 1) a prototype guided domain adaptation method which uses prototypes to mine the semantic category information without accessing the source dataset; 2) a scheme of iteratively updating global class prototype which can handle the class and sample imbalances in the training procedure and 3) a more accurate pseudo-label generation method combining semantic information and image information. On multiple public domain adaptive scenarios, our method achieves the state-of-the-art results in terms of accuracy compared with the Faster R-CNN model and some domain adaptive methods with source datasets.","Source data-free, Object detection, Domain adaptation, Transfer learning",Lin Xiong and Mao Ye and Dan Zhang and Yan Gan and Yiguang Liu,https://www.sciencedirect.com/science/article/pii/S0031320321006129,https://doi.org/10.1016/j.patcog.2021.108436,0031-3203,2022,108436,124,Pattern Recognition,Source data-free domain adaptation for a faster R-CNN,article,XIONG2022108436,
"Adversarial examples have been shown to be a severe threat to deep neural networks (DNNs). One of the most effective adversarial defense methods is adversarial training (AT) through minimizing the adversarial risk Radv, which encourages both the benign example x and its adversarially perturbed neighborhoods within the âp-ball to be predicted as the ground-truth label. In this paper, we propose a novel defense method, the robust training (RT), by jointly minimizing two separated risks (i.e., Rstand and Rrob), which are with respect to the benign example and its neighborhoods, respectively. The motivation is to explicitly and jointly enhance the accuracy and the adversarial robustness. We prove that Radv is upper-bounded by Rstand+Rrob, which implies that RT has similar effect as AT. Intuitively, minimizing the standard risk enforces the benign example to be correctly predicted, while the robust risk minimization encourages the predictions of the neighbor examples to be consistent with the prediction of the benign example. Besides, since Rrob is independent of the ground-truth label, RT is naturally extended to the semi-supervised mode (i.e., SRT), to further enhance its effectiveness. Moreover, we extend the âp-bounded neighborhood to a general case, which covers different types of perturbations, such as the pixel-wise (i.e., x+Î´) or the spatial perturbation (i.e., Ax+b). Extensive experiments on benchmark datasets not only verify the superiority of the proposed SRT to state-of-the-art methods for defending pixel-wise or spatial perturbations separately but also demonstrate its robustness to both perturbations simultaneously. Our work may shed the light on the understanding of universal model robustness and the potential of unlabeled samples. The code for reproducing main results is available at https://github.com/THUYimingLi/Semi-supervised_Robust_Training.","Adversarial Defense, Adversarial Learning, Semi-supervised Learning, AI Security, Deep Learning, Classification",Yiming Li and Baoyuan Wu and Yan Feng and Yanbo Fan and Yong Jiang and Zhifeng Li and Shu-Tao Xia,https://www.sciencedirect.com/science/article/pii/S0031320321006488,https://doi.org/10.1016/j.patcog.2021.108472,0031-3203,2022,108472,124,Pattern Recognition,Semi-supervised robust training with generalized perturbed neighborhood,article,LI2022108472,
"An Orthogonal Least Squares (OLS) based feature selection method is proposed for both binomial and multinomial classification. The novel Squared Orthogonal Correlation Coefficient (SOCC) is defined based on Error Reduction Ratio (ERR) in OLS and used as the feature ranking criterion. The equivalence between the canonical correlation coefficient, Fisherâs criterion, and the sum of the SOCCs is revealed, which unveils the statistical implication of ERR in OLS for the first time. It is also shown that the OLS based feature selection method has speed advantages when applied for greedy search. The proposed method is comprehensively compared with the mutual information based feature selection methods and the embedded methods using both synthetic and real world datasets. The results show that the proposed method is always in the top 5 among the 12 candidate methods. Besides, the proposed method can be directly applied to continuous features without discretisation, which is another significant advantage over mutual information based methods.","Feature selection, Orthogonal least squares, Canonical correlation analysis, Linear discriminant analysis, Multi-label, Multivariate time series, Feature interaction",Sikai Zhang and Zi-Qiang Lang,https://www.sciencedirect.com/science/article/pii/S0031320321005951,https://doi.org/10.1016/j.patcog.2021.108419,0031-3203,2022,108419,123,Pattern Recognition,Orthogonal least squares based fast feature selection for linear classification,article,ZHANG2022108419,
"Characterizing signal dynamics with network approaches have attracted significant attention in nonlinear time series analysis. Among these approaches, ordinal networks have received great interest for their simplicity and computational efficiency. But most studies mainly use the topological structure of ordinal network to characterize time series while the underlying information in the transition probabilities remain insufficiently concerned. In this paper, the authors introduce an ordinal network-based complexity-entropy curve to fill this gap. The numerical results show that this curve has a great discriminating power for signals with different dynamics, outperforming the recently proposed global node entropy. In the empirical application on stock indices, these curves distinguish stock market with different market development and further identify the impact of the 2008 global financial crisis on stock market dynamics. In the analysis of geomagnetic activity, these curves detect the dynamical change in Earths magnetic field caused by the geomagnetic storm.","Ordinal network, Signal processing, Symbolic patterns, Tsallis -entropy, Novelty detection",Kun Peng and Pengjian Shang,https://www.sciencedirect.com/science/article/pii/S0031320321006403,https://doi.org/10.1016/j.patcog.2021.108464,0031-3203,2022,108464,124,Pattern Recognition,Characterizing ordinal network of time series based on complexity-entropy curve,article,PENG2022108464,
"Although Generative Adversarial Networks (GAN) have shown remarkable performance in image generation, there exist some challenges in instability and convergence speed. During the training, the results of some models display the imbalances of quality within a generated image, in which some defective parts appear compared with other regions. Different from general single global optimization methods, we introduce an adaptive global and local bilevel optimization model (GL-GAN). The model achieves the generation of high-resolution images in a complementary and promoting way, where global optimization is to optimize the whole images and local is only to optimize the low-quality areas. Based on DCGAN, GL-GAN is able to effectively avoid the nature of imbalance by local bilevel optimization, which is accomplished by first locating low-quality areas and then optimizing them. Moreover, through feature map cues from discriminator output, we propose the adaptive local and global optimization method (Ada-OP) for interactive optimization and observe that it boosts the convergence speed. Compared with the current GAN methods, our model has shown impressive performance on CelebA, Oxford Flowers, CelebA-HQ and LSUN datasets.","Generative adversarial networks (GAN), Global and local bilevel optimization, Ada-OP, Image generation",Ying Liu and Heng Fan and Xiaohui Yuan and Jinhai Xiang,https://www.sciencedirect.com/science/article/pii/S0031320321005550,https://doi.org/10.1016/j.patcog.2021.108375,0031-3203,2022,108375,123,Pattern Recognition,GL-GAN: Adaptive global and local bilevel optimization for generative adversarial network,article,LIU2022108375,
"Forecasting human motion from a sequence of human poses is an important problem in the fields of computer vision and robotics. Most previous approaches merely consider learning the temporal dynamics of body joints or joint angles, while neglect derivatives of body joints (i.e., pose velocities) which could reasonably reduce noise impact and improve stability. To exploit the benefits of pose velocities, we propose the velocity-to-velocity learning paradigm for human motion prediction which attempts to directly build the sequence-to-sequence model in the velocity space. Two variant architectures based on recurrent encoder-decoder networks are introduced under this paradigm. Considering human motion as kinematics of rigid bodies, joint angles which denote transformation are the computations of inverse kinematics. Accordingly, a novel loss function in terms of rotation matrices is designed during training for human motion prediction through a rotation matrix transformation (RMT) layer. Finally, we present an effective training algorithm which exploits sequence transformation to improve model generalization. Our approaches substantially outperform state-of-the-art approaches on two large-scale datasets, Human3.6M and CMU Motion Capture, for both short-term prediction and long-term prediction. In particular, our model can competently forecast human-like and meaningful poses up to 1000 milliseconds. The code is available on GitHub: https://github.com/hongsong-wang/RNN_based_human_motion_prediction.","Human motion prediction, Action anticipation",Hongsong Wang and Liang Wang and Jiashi Feng and Daquan Zhou,https://www.sciencedirect.com/science/article/pii/S0031320321006002,https://doi.org/10.1016/j.patcog.2021.108424,0031-3203,2022,108424,124,Pattern Recognition,Velocity-to-velocity human motion forecasting,article,WANG2022108424,
"This study proposes a contrastive convolutional auto-encoder (contrastive CAE), a combined architecture of an auto-encoder and contrastive loss, to identify individuals with suspected COVID-19 infection using heart-rate data from participants with multiple sclerosis (MS) in the ongoing RADAR-CNS mHealth research project. Heart-rate data was remotely collected using a Fitbit wristband. COVID-19 infection was either confirmed through a positive swab test, or inferred through a self-reported set of recognised symptoms of the virus. The contrastive CAE outperforms a conventional convolutional neural network (CNN), a long short-term memory (LSTM) model, and a convolutional auto-encoder without contrastive loss (CAE). On a test set of 19 participants with MS with reported symptoms of COVID-19, each one paired with a participant with MS with no COVID-19 symptoms, the contrastive CAE achieves an unweighted average recall of 95.3%, a sensitivity of 100% and a specificity of 90.6%, an area under the receiver operating characteristic curve (AUC-ROC) of 0.944, indicating a maximum successful detection of symptoms in the given heart rate measurement period, whilst at the same time keeping a low false alarm rate.","COVID-19, Respiratory tract infection, Anomaly detection, Contrastive learning, Convolutional auto-encoder",Shuo Liu and Jing Han and Estela Laporta Puyal and Spyridon Kontaxis and Shaoxiong Sun and Patrick Locatelli and Judith Dineley and Florian B. Pokorny and Gloria Dalla Costa and Letizia Leocani and Ana Isabel Guerrero and Carlos Nos and Ana Zabalza and Per Soelberg SÃ¸rensen and Mathias Buron and Melinda Magyari and Yatharth Ranjan and Zulqarnain Rashid and Pauline Conde and Callum Stewart and Amos A Folarin and Richard JB Dobson and Raquel BailÃ³n and Srinivasan Vairavan and Nicholas Cummins and Vaibhav A Narayan and Matthew Hotopf and Giancarlo Comi and BjÃ¶rn Schuller and RADAR-CNS Consortium,https://www.sciencedirect.com/science/article/pii/S0031320321005793,https://doi.org/10.1016/j.patcog.2021.108403,0031-3203,2022,108403,123,Pattern Recognition,Fitbeat: COVID-19 estimation based on wristband heart rate using a contrastive convolutional auto-encoder,article,LIU2022108403,
"In recent years, person re-identification, which learns discriminative features for the specific person retrieval problem across non-overlapping cameras, has attracted extensive attention. One of the main challenges in person re-identification with deep neural networks is the design of the loss function, which plays a vital role in improving the discrimination of the learned features. However, most existing models utilize the hand-designed loss functions, which are usually sub-optimal and time-consuming. The search spaces of the two existing AutoML-based methods are either too complicated or too simple to include various forms of loss functions. In order to solve the irrationality of the above search spaces, in this paper, we propose a method of AutoML for loss function search named LFS-ReID for person ReID in the framework of the margin-based softmax loss function. Specifically, we first analyze the margin-based softmax loss function and conclude four key properties. Then we carefully design a sampling distribution based on the non-independent truncated Gaussian distributions to sample the loss function, which conforms to the above four properties. Finally, a method based on reinforcement learning is adopted to optimize the sampling distribution dynamically. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on four commonly used datasets.","Person re-identification, Margin-based softmax loss, Loss function search, AutoML",Hongyang Gu and Jianmin Li and Guangyuan Fu and Min Yue and Jun Zhu,https://www.sciencedirect.com/science/article/pii/S0031320321006087,https://doi.org/10.1016/j.patcog.2021.108432,0031-3203,2022,108432,124,Pattern Recognition,Loss function search for person re-identification,article,GU2022108432,
"How to effectively fuse inter- and intra-frame spatio-temporal information plays a key role in video super-resolution (VSR). Most existing works rely heavily on the accuracy of motion estimation and compensation for spatio-temporal feature alignment. However, they cannot perform well when suffering from large-scale and complex motions. To this end, this paper introduces an efficient and effective Interlaced Sparse Sinkhorn Matching (ISSM) network for VSR, which aligns supporting frames with the reference one in the feature space by learning optimal matching between image regions across frames. Specifically, the ISSM divides the input dense affinity matrix into two sparse block matrixes: one can match long-distance regions while the other can match short-distance regions, and then we leverage an efficient Sinkhorn method on each block to learn optimal matching. Moreover, we insert a residual atrous spatial pyramid pooling module before the ISSM, which can flexibly generate multi-scale features frame by frame to capture the multi-scale context information in images. The aligned features of each adjacent frame are then fed to a bidirectional temporal fusion module to capture the rich temporal information. Finally, the fused features are sent into a frame-wise dynamic reconstruction network to produce an HR frame. Extensive evaluations on three benchmark datasets demonstrate the superiority of our method over the state-of-the-art methods in terms of PSNR and SSIM.","Video super-resolution, Multi-scale feature, Interlaced sparse sinkhorn attention, Bidirectional fusion, Dynamic reconstruction",Huihui Song and Yutong Jin and Yong Cheng and Bo Liu and Dong Liu and Qingshan Liu,https://www.sciencedirect.com/science/article/pii/S0031320321006518,https://doi.org/10.1016/j.patcog.2021.108475,0031-3203,2022,108475,124,Pattern Recognition,Learning interlaced sparse Sinkhorn matching network for video super-resolution,article,SONG2022108475,
"Previous research on Facial Expression Recognition (FER) assisted by facial landmarks mainly focused on single-task learning or hard-parameter sharing based multi-task learning. However, soft-parameter sharing based methods have not been explored in this area. Therefore, this paper adopts Facial Landmark Detection (FLD) as the auxiliary task and explores new multi-task learning strategies for FER. First, three classical multi-task structures, including Hard-Parameter Sharing (HPS), Cross-Stitch Network (CSN), and Partially Shared Multi-task Convolutional Neural Network (PS-MCNN), are used to verify the advantages of multi-task learning for FER. Then, we propose a new end-to-end Co-attentive Multi-task Convolutional Neural Network (CMCNN), which is composed of the Channel Co-Attention Module (CCAM) and the Spatial Co-Attention Module (SCAM). Functionally, the CCAM generates the channel co-attention scores by capturing the inter-dependencies of different channels between FER and FLD tasks. The SCAM combines the max- and average-pooling operations to formulate the spatial co-attention scores. Finally, we conduct extensive experiments on four widely used benchmark facial expression databases, including RAF, SFEW2, CK+, and Oulu-CASIA. Extensive experimental results show that our approach achieves better performance than single-task and multi-task baselines, fully validating multi-task learningâs effectiveness and generalizability11Codes and detailed instructions can be available at https://github.com/thuiar/cmcnn..","Facial expression recognition, Facial landmarks detection, Multi-task learning",Wenmeng Yu and Hua Xu,https://www.sciencedirect.com/science/article/pii/S003132032100577X,https://doi.org/10.1016/j.patcog.2021.108401,0031-3203,2022,108401,123,Pattern Recognition,Co-attentive multi-task convolutional neural network for facial expression recognition,article,YU2022108401,
"Scale inconsistency is a widely encountered issue in multi-output learning problems. Specifically, target sets with multiple real valued or a mixture of categorical and real valued variables require addressing the scale differences to obtain predictive models with sufficiently good performance. Data transformation techniques are often employed to solve that problem. However, these operations are susceptible to different shortcomings such as changing the statistical properties of the data and increase the computational burden. Scale differences also pose problem in semi-supervised learning (SSL) models as they require processing of unsupervised information where distance measures are commonly employed. Classical distance metrics can be criticized as they lose efficiency when variables exhibit type or scale differences, too. Besides, in higher dimensions distance metrics cause problems due to loss of discriminative power. This paper introduces alternative semi-supervised tree-based strategies that are robust to scale differences both in terms of feature and target variables. We propose use of a scale-invariant proximity measure by means of tree-based ensembles to preserve the original characteristics of the data. We update classical tree derivation procedure to a multi-criteria form to resolve scale inconsistencies. We define proximity based clustering indicators and extend the supervised model with unsupervised criteria. Our experiments show that proposed method significantly outperforms its benchmark learning model that is predictive clustering trees.","Semi-supervised learning, Multi-task learning, Multi-objective trees, Ensemble learning, Totally randomized trees",Esra AdÄ±yeke and Mustafa GÃ¶kÃ§e BaydoÄan,https://www.sciencedirect.com/science/article/pii/S0031320321005549,https://doi.org/10.1016/j.patcog.2021.108393,0031-3203,2022,108393,123,Pattern Recognition,Semi-supervised extensions of multi-task tree ensembles,article,ADIYEKE2022108393,
"Deep clustering aims to promote clustering tasks by combining deep learning and clustering together to learn the clustering-oriented representation, and many approaches have shown their validity. However, the feature learning modules in existing methods hardly learn a discriminative representation. In addition, the label assignment mechanism becomes inefficient when dealing with some hard samples. To address these issues, a new joint optimization clustering framework is proposed through introducing the contractive representation in feature learning and utilizing focal loss in the clustering layer. The contractive penalty term added in feature learning would cause the local feature space contraction, resulting in learning more discriminative features. To our certain knowledge, this is also the first work to utilize the focal loss to improve the label assignment in deep clustering method. Moreover, the construction of the joint optimization framework enables the proposed method to learn feature representation and label assignment simultaneously in an end-to-end way. Finally, we comprehensively compare with some state-of-the-art clustering approaches on several clustering tasks to demonstrate the effectiveness of the proposed method.","Unsupervised learning, Clustering, Contractive feature representation, Focal loss, Auto-encoder",Jinyu Cai and Shiping Wang and Chaoyang Xu and Wenzhong Guo,https://www.sciencedirect.com/science/article/pii/S0031320321005665,https://doi.org/10.1016/j.patcog.2021.108386,0031-3203,2022,108386,123,Pattern Recognition,Unsupervised deep clustering via contractive feature representation and focal loss,article,CAI2022108386,
"Deep neural networks based purely on attention have been successful across several domains, relying on minimal architectural priors from the designer. In Human Action Recognition (HAR), attention mechanisms have been primarily adopted on top of standard convolutional or recurrent layers, improving the overall generalization capability. In this work, we introduce Action Transformer (AcT), a simple, fully, self-attentional architecture that consistently outperforms more elaborated networks that mix convolutional, recurrent, and attentive layers. In order to limit computational and energy requests, building on previous human action recognition research, the proposed approach exploits 2D pose representations over small temporal windows, providing a low latency solution for accurate and effective real-time performance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as an attempt to build a formal training and evaluation benchmark for real-time, short-time HAR. The proposed methodology was extensively tested on MPOSE2021 and compared to several state-of-the-art architectures, proving the effectiveness of the AcT model and laying the foundations for future work on HAR.","Human action recognition, Deep learning, Computer vision, Transformer",Vittorio Mazzia and Simone Angarano and Francesco Salvetti and Federico Angelini and Marcello Chiaberge,https://www.sciencedirect.com/science/article/pii/S0031320321006634,https://doi.org/10.1016/j.patcog.2021.108487,0031-3203,2022,108487,124,Pattern Recognition,Action Transformer: A self-attention model for short-time pose-based human action recognition,article,MAZZIA2022108487,
"We indirectly predict a class by deriving user-defined (i.e., existing) attributes (UA) from an image in generalized zero-shot learning (GZSL). High-quality attributes are essential for GZSL, but the existing UAs are sometimes not discriminative. We observe that the hidden units at each layer in a convolutional neural network (CNN) contain highly discriminative semantic information across a range of objects, parts, scenes, textures, materials, and color. The semantic information in CNN features is similar to the attributes that can distinguish each class. Motivated by this observation, we employ CNN features like novel class representative semantic data, i.e., deep attribute (DA). Precisely, we propose three objective functions (e.g., compatible, discriminative, and intra-independent) to inject the fundamental properties into the generated DA. We substantially outperform the state-of-the-art approaches on four challenging GZSL datasets, including CUB, FLO, AWA1, and SUN. Furthermore, the existing UA and our proposed DA are complementary and can be combined to enhance performance further.","Generalized zero-shot learning, Deep attribute, Discriminative latent attribute",Hoseong Kim and Jewook Lee and Hyeran Byun,https://www.sciencedirect.com/science/article/pii/S0031320321006117,https://doi.org/10.1016/j.patcog.2021.108435,0031-3203,2022,108435,124,Pattern Recognition,Discriminative deep attributes for generalized zero-shot learning,article,KIM2022108435,
"Scene classification is an important basis for many modern intelligent applications, however the performance of pattern recognition or deep learning-based methods are still not sufficient since complicated structure and context of scene images. In this paper, we propose a novel fusion framework of adaptive nonnegative feature fusion (AdaNFF) for scene classification. The AdaNFF integrates nonnegative matrix factorization, adaptive feature fusion and feature fusion boosting into an end-to-end process. Firstly, feature fusion is known as a general strategy to strengthen weak features, and we observe that pixel values and most hand-craft features of the scene image are naturally nonnegative. Therefore we are motivated to build a fusion method based on nonnegative matrix factorization, which can preserve features nonnegative properties and improve their representation performance. Secondly, with the results of fused single or multiple features fusion, we develop an adaptive feature fusion and boosting algorithm to improve the efficiency of image features. Finally, a normalized l2-norm classifier and a deep-learning like multilayer perceptron (MLP) classifier are trained to predict label of scene image. Under this framework, there are two versions of the proposed feature fusion method for nonnegative single-feature fusion and multi-feature fusion. All methods were validated on scene classification benchmarks. Experiment results suggest that the proposed methods can deal with multi-class scene problems and achieve remarkable classification performance.","Scene classification, Adaptive feature fusion, Nonnegative matrix factorization, Feature fusion boosting",Zhiyuan Zou and Weibin Liu and Weiwei Xing,https://www.sciencedirect.com/science/article/pii/S0031320321005781,https://doi.org/10.1016/j.patcog.2021.108402,0031-3203,2022,108402,123,Pattern Recognition,AdaNFF: A new method for adaptive nonnegative multi-feature fusion to scene classification,article,ZOU2022108402,
"There is an urgent need for automated methods to assist accurate and effective assessment of COVID-19. Radiology and nucleic acid test (NAT) are complementary COVID-19 diagnosis methods. In this paper, we present an end-to-end multitask learning (MTL) framework (COVID-MTL) that is capable of automated and simultaneous detection (against both radiology and NAT) and severity assessment of COVID-19. COVID-MTL learns different COVID-19 tasks in parallel through our novel random-weighted loss function, which assigns learning weights under Dirichlet distribution to prevent task dominance; our new 3D real-time augmentation algorithm (Shift3D) introduces space variances for 3D CNN components by shifting low-level feature representations of volumetric inputs in three dimensions; thereby, the MTL framework is able to accelerate convergence and improve joint learning performance compared to single-task models. By only using chest CT scans, COVID-MTL was trained on 930 CT scans and tested on separate 399 cases. COVID-MTL achieved AUCs of 0.939 and 0.846, and accuracies of 90.23% and 79.20% for detection of COVID-19 against radiology and NAT, respectively, which outperformed the state-of-the-art models. Meanwhile, COVID-MTL yielded AUC of 0.800Â Â±Â 0.020 and 0.813Â Â±Â 0.021 (with transfer learning) for classifying control/suspected, mild/regular, and severe/critically-ill cases. To decipher the recognition mechanism, we also identified high-throughput lung features that were significantly related (P < 0.001) to the positivity and severity of COVID-19.","COVID-19, Multitask learning, 3D CNNs, Diagnosis, Severity assessment, Deep learning, Computer tomography",Guoqing Bao and Huai Chen and Tongliang Liu and Guanzhong Gong and Yong Yin and Lisheng Wang and Xiuying Wang,https://www.sciencedirect.com/science/article/pii/S0031320321006750,https://doi.org/10.1016/j.patcog.2021.108499,0031-3203,2022,108499,124,Pattern Recognition,COVID-MTL: Multitask learning with Shift3D and random-weighted loss for COVID-19 diagnosis and severity assessment,article,BAO2022108499,
"A weakly-supervised learning framework named as complementary-label learning has been proposed recently, where each sample is equipped with a single complementary label that denotes one of the classes the sample does not belong to. However, the existing complementary-label learning methods cannot learn from the easily accessible unlabeled samples and samples with multiple complementary labels, which are more informative. In this paper, to remove these limitations, we propose the novel multi-complementary and unlabeled learning framework that allows unbiased estimation of classification risk from samples with any number of complementary labels and unlabeled samples, for arbitrary loss functions and models. We first give an unbiased estimator of the classification risk from samples with multiple complementary labels, and then further improve the estimator by incorporating unlabeled samples into the risk formulation. The estimation error bounds show that the proposed methods are in the optimal parametric convergence rate. We also propose a risk correction scheme for alleviating over-fitting caused by negative empirical risk. Finally, the experiments on both linear and deep models show the effectiveness of our proposed methods.","Multi-complementary, Unlabeled learning, Empirical risk minimization, Unbiased estimator, Classification",Yuzhou Cao and Shuqi Liu and Yitian Xu,https://www.sciencedirect.com/science/article/pii/S0031320321006233,https://doi.org/10.1016/j.patcog.2021.108447,0031-3203,2022,108447,124,Pattern Recognition,Multi-complementary and unlabeled learning for arbitrary losses and models,article,CAO2022108447,
"Studying on the microstructural evolution of cement paste during hydration is of considerable significance for understanding its mechanism and designing such material in cement industry. With the use of microtomography and image registration, the four-dimensional (4D) microstructure of cement paste can be captured, thereby assisting material scientists in studying the hydration process in situ. However, as a challenging task, the construction of high-quality 4D microstructural image is remarkably impeded by image size, isotropy, and homogeneity. This paper proposes an image processing framework to construct 4D high quality microstructural image rapidly for cement hydration. This framework improves and accelerates microstructural image registration and enhancement by using bias field correction, temporal intensity calibration and fast image registration. Additionally, a partial information registration method adopting partial information on the spatial and phased scales, is proposed to improve the registration speed and accuracy. Furthermore, a multi-factor multi-layer particle swarm optimization is proposed to improve the optimization in registration. Experimental results indicate that the 4D high quality microstructural image can be constructed rapidly with promising precision.","Cement hydration, Rapid image construction, Image registration, Particle swarm optimization, Microstructural temporal image sequences",Liangliang Zhang and Lin Wang and Bo Yang and Sijie Niu and Yamin Han and Sung-Kwun Oh,https://www.sciencedirect.com/science/article/pii/S0031320321006476,https://doi.org/10.1016/j.patcog.2021.108471,0031-3203,2022,108471,124,Pattern Recognition,Rapid construction of 4D high-quality microstructural image for cement hydration using partial information registration,article,ZHANG2022108471,
"This paper introduces a new DBSCAN-based method for boundary detection and plane segmentation for 3D point clouds. The proposed method is based on candidate samples selection in 3D space and plane validity detection via revising the classical DBSCAN clustering algorithm to obtain a valid fitting plane. Technically, a coplanar threshold is designed as an additional clustering condition to group 3D points whose distances to the fitting plane satisfy the constraint of the threshold as one cluster. The threshold value is automatically adjusted to fit the local distribution of samples in the input dataset, which is free of parameter tuning. Planar objects can be detected by the proposed method since a cluster contains only data points belonging to one plane, and the boundaries among different planes can be correctly detected. Experimental evaluations are performed on both synthetic and real point cloud datasets. Results show that the proposed approach is effective for planar segmentation and high-quality segmentation of intersection boundaries.","3D point cloud, Plane segmentation, Boundary detection, DBSCAN",Hui Chen and Man Liang and Wanquan Liu and Weina Wang and Peter Xiaoping Liu,https://www.sciencedirect.com/science/article/pii/S0031320321006075,https://doi.org/10.1016/j.patcog.2021.108431,0031-3203,2022,108431,124,Pattern Recognition,An approach to boundary detection for 3D point clouds based on DBSCAN clustering,article,CHEN2022108431,
"Graph convolutional network is now an effective tool to deal with non-Euclidean data, such as social behavior analysis, molecular structure analysis, and skeleton-based action recognition. Graph convolutional kernel is one of the most significant factors in graph convolutional networks to extract nodesâ feature, and some variants of it have achieved highly satisfactory performance theoretically and experimentally. However, there was limited research about how exactly different graph structures influence the performance of these kernels. Some existing methods used an adaptive convolutional kernel to deal with a given graph structure, which still not explore the internal reasons. In this paper, we start from theoretical analysis of the spectral graph and study the properties of existing graph convolutional kernels, revealing the self-smoothing phenomenon and its effect in specific structured graphs. After that, we propose the Poisson kernel that can avoid self-smoothing without training any adaptive kernel. Experimental results demonstrate that our Poisson kernel not only works well on the benchmark datasets where state-of-the-art methods work fine, but also is evidently superior to them in synthetic datasets.","Graph convolutional kernel, Graph convolutional network, Graph neural network, Graph structure, Self-smoothing",Ziqing Yang and Shoudong Han and Jun Zhao,https://www.sciencedirect.com/science/article/pii/S0031320321006191,https://doi.org/10.1016/j.patcog.2021.108443,0031-3203,2022,108443,124,Pattern Recognition,Poisson kernel: Avoiding self-smoothing in graph convolutional networks,article,YANG2022108443,
"Every day, large-scale data are continuously generated on social media as streams, such as Twitter, which inform us about all events around the world in real-time. Notably, Twitter is one of the effective platforms to update countries leaders and scientists during the coronavirus (COVID-19) pandemic. Other people have also used this platform to post their concerns about the spread of this virus and a rapid increase of death cases globally. The aim of this work is to detect anomalous events associated with COVID-19 from Twitter. To this end, we propose a distributed Directed Acyclic Graph topology framework to aggregate and process large-scale real-time tweets related to COVID-19. The core of our system is a novel lightweight algorithm that can automatically detect anomaly events. In addition, our system can also identify, cluster, and visualize important keywords in tweets. On 18 August 2020, our model detected the highest anomaly since many tweets mentioned the casualtiesâ updates and the debates on the pandemic that day. We obtained the three most commonly listed terms on Twitter: âcovidâ, âdeathâ, and âTrumpâ (21,566, 11,779, and 4761 occurrences, respectively), with the highest TF-IDF score for these terms: âpeopleâ (0.63637), âschoolâ (0.5921407) and âvirusâ (0.57385). From our clustering result, the word âdeathâ, âcoronaâ, and âcaseâ are grouped into one cluster, where the word âpandemicâ, âschoolâ, and âpresidentâ are grouped as another cluster. These terms were located near each other on vector space so that they were clustered, indicating peopleâs most concerned topics on Twitter.","Anomaly detection, Big data, COVID-19, Directed acyclic graph, Event stream",Bakhtiar Amen and Syahirul Faiz and Thanh-Toan Do,https://www.sciencedirect.com/science/article/pii/S003132032100580X,https://doi.org/10.1016/j.patcog.2021.108404,0031-3203,2022,108404,123,Pattern Recognition,Big data directed acyclic graph model for real-time COVID-19 twitter stream detection,article,AMEN2022108404,
"Although deep learning-based stereo matching approaches have achieved excellent performance in recent years, it is still a non-trivial task to estimate the uncertainty of the produced disparity map. In this paper, we propose a novel approach to estimate both aleatoric and epistemic uncertainties for stereo matching in an end-to-end way. We introduce an evidential distribution, named Normal Inverse-Gamma (NIG) distribution, whose parameters can be used to calculate the uncertainty. Instead of directly regressed from aggregated features, the uncertainty parameters are predicted for each potential disparity and then averaged via the guidance of matching probability distribution. Furthermore, considering the sparsity of ground truth in real scene datasets, we design two additional losses. The first one tries to enlarge uncertainty on incorrect predictions, so uncertainty becomes more sensitive to erroneous regions. The second one enforces the smoothness of the uncertainty in the regions with smooth disparity. Most stereo matching models, such as PSM-Net, GA-Net, and AA-Net, can be easily integrated with our approach. Experiments on multiple benchmark datasets show that our method improves stereo matching results. We prove that both aleatoric and epistemic uncertainties are well-calibrated with incorrect predictions. Particularly, our method can capture increased epistemic uncertainty on out-of-distribution data, making it effective to prevent a system from potential fatal consequences. Code is available at https://github.com/Dawnstar8411/StereoMatching-Uncertainty.","Stereo matching, Uncertainty estimation, Evidential deep learning",Chen Wang and Xiang Wang and Jiawei Zhang and Liang Zhang and Xiao Bai and Xin Ning and Jun Zhou and Edwin Hancock,https://www.sciencedirect.com/science/article/pii/S0031320321006749,https://doi.org/10.1016/j.patcog.2021.108498,0031-3203,2022,108498,124,Pattern Recognition,Uncertainty estimation for stereo matching based on evidential deep learning,article,WANG2022108498,
"In the field of multimodal segmentation, the correlation between different modalities can be considered for improving the segmentation results. Considering the correlation between different MR modalities, in this paper, we propose a multi-modality segmentation network guided by a novel tri-attention fusion. Our network includes N model-independent encoding paths with N image sources, a tri-attention fusion block, a dual-attention fusion block, and a decoding path. The model independent encoding paths can capture modality-specific features from the N modalities. Considering that not all the features extracted from the encoders are useful for segmentation, we propose to use dual attention based fusion to re-weight the features along the modality and space paths, which can suppress less informative features and emphasize the useful ones for each modality at different positions. Since there exists a strong correlation between different modalities, based on the dual attention fusion block, we propose a correlation attention module to form the tri-attention fusion block. In the correlation attention module, a correlation description block is first used to learn the correlation between modalities and then a constraint based on the correlation is used to guide the network to learn the latent correlated features which are more relevant for segmentation. Finally, the obtained fused feature representation is projected by the decoder to obtain the segmentation results. Our experiment results tested on BraTS 2018 dataset for brain tumor segmentation demonstrate the effectiveness of our proposed method.","Multi-modality fusion, Correlation, Brain tumor segmentation, Deep learning",Tongxue Zhou and Su Ruan and Pierre Vera and StÃ©phane Canu,https://www.sciencedirect.com/science/article/pii/S0031320321005938,https://doi.org/10.1016/j.patcog.2021.108417,0031-3203,2022,108417,124,Pattern Recognition,A Tri-Attention fusion guided multi-modal segmentation network,article,ZHOU2022108417,
"Domain adaptation is proposed to generalize learning machines and address performance degradation of models that are trained from one specific source domain but applied to novel target domains. Existing domain adaptation methods focus on transferring holistic features whose discriminability is generally tailored to be source-specific and inferiorly generic to be transferable. As a result, standard domain adaptation on holistic features usually damages feature structures, especially local feature statistics, and deteriorates the learned discriminability. To alleviate this issue, we propose to transfer primitive local feature patterns, whose discriminability are shown to be inherently more sharable, and perform hierarchical feature adaptation. Concretely, we first learn a cluster of domain-shared local feature patterns and partition the feature space into cells. Local features are adaptively aggregated inside each cell to obtain cell features, which are further integrated into holistic features. To achieve fine-grained adaptations, we simultaneously perform alignment on local features, cell features and holistic features, within which process the local and cell features are aligned independently inside each cell to maintain the learned local structures and prevent negative transfer. Experimenting on typical one-to-one unsupervised domain adaptation for both image classification and action recognition tasks, partial domain adaptation, and domain-agnostic adaptation, we show that the proposed method achieves more reliable feature transfer by consistently outperforming state-of-the-art models and the learned domain-invariant features generalize well to novel domains.","Domain adaptation, Local feature patterns, Adversarial learning, Hierarchical alignment",Jun Wen and Junsong Yuan and Qian Zheng and Risheng Liu and Zhefeng Gong and Nenggan Zheng,https://www.sciencedirect.com/science/article/pii/S003132032100621X,https://doi.org/10.1016/j.patcog.2021.108445,0031-3203,2022,108445,124,Pattern Recognition,Hierarchical domain adaptation with local feature patterns,article,WEN2022108445,
"In this study, we present a robust and efficient fingerprint image restoration algorithm using the nonlocal CahnâHilliard (CH) equation, which was proposed for modeling the microphase separation of diblock copolymers. We take a small local region embedding the damaged domain and solve the nonlocal CH equation to restore the fingerprint image. A GaussâSeidel type iterative method, which is efficient and simple to implement, is used. The proposed method has the advantage in that the pixel values in the damaged fingerprint domain can be obtained using the image information from the outside of the damaged fingerprint region. Fingerprint restoration based on adjacent pixel information can ensure the accuracy of the fingerprint information with a low computational cost. Computational experiments demonstrated the superior performance of the proposed fingerprint restoration algorithm.","Fingerprint restoration, Diblock copolymer, Nonlocal CahnâHilliard equation",Yibao Li and Qing Xia and Chaeyoung Lee and Sangkwon Kim and Junseok Kim,https://www.sciencedirect.com/science/article/pii/S0031320321005811,https://doi.org/10.1016/j.patcog.2021.108405,0031-3203,2022,108405,123,Pattern Recognition,A robust and efficient fingerprint image restoration method based on a phase-field model,article,LI2022108405,
"Existing works on Automated Machine Learning (AutoML) are mainly based on predefined search space. This paper seeks synergetic automation of two ingredients, i.e., search space and search strategies. Specifically, we formulate the automation of search space and search strategies as a combinatorial optimization problem. Our empirical study on many architecture benchmarks shows that identifying the suitable search space exerts more effect than choosing a sophisticated search strategy. Motivated by this, we attempt to leverage a machine learning method to solve the discrete optimization problem, and thus develop a Layered Architecture Search Tree (LArST) approach to synergize these two components. In addition, we use a probe model-based method to extract dataset-wise features, i.e., meta-features, which is able to facilitate the estimation of proper search space and search strategy for a given task. Experimental results show the efficacy of our approach under different search mechanisms and various datasets and hardware platforms.","AutoML, Search space selection, Combinatorial optimization for AutoML",Chao Xue and Mengting Hu and Xueqi Huang and Chun-Guang Li,https://www.sciencedirect.com/science/article/pii/S0031320321006506,https://doi.org/10.1016/j.patcog.2021.108474,0031-3203,2022,108474,124,Pattern Recognition,Automated search space and search strategy selection for AutoML,article,XUE2022108474,
"Inspired by the great success of deep neural networks in image classification, recent works use Recurrence Plots (RP) to encode time series as images for classification. RP provide rich texture information and construct long-term time correlations, which are effective supplements to the networks. However, RP cannot handle the scale and length variability of sequences. Moreover, RP have serious tendency confusion problem. They cannot represent the upward and downward trends of sequences effectively. In addition to the defects of RP, existing time series classification (TSC) networks cannot adapt to the various scales of discriminative regions of time series effectively. To tackle these problems, this paper proposes a method, named MSRP-IFCN. It is composed of two submodules, the Multi-scale Signed RP (MSRP) and the Inception Fully Convolutional Network (IFCN). MSRP are proposed to handle the defects of RP. They comprise three components, namely the multi-scale RP, the asymmetric RP and the signed RP. We first use the multi-scale RP to enrich the scales of images. Then, the asymmetric RP are constructed to represent long sequences. Finally, the signed RP images are obtained by multiplying the designed sign masks to remove the tendency confusion. Besides, IFCN is proposed to enhance the existing TSC networks in multi-scale feature extraction. By introducing the modified Inception modules, IFCN obtains extensive receptive fields and better extracts multi-scale features from the MSRP images. Experimental results on 85 UCR datasets indicate the superior performance of MSRP-IFCN. The visualization results further demonstrate the effectiveness of our method.","Time series classification, Multi-scale, Signed, Recurrence plots, Inception network",Ye Zhang and Yi Hou and Kewei OuYang and Shilin Zhou,https://www.sciencedirect.com/science/article/pii/S0031320321005653,https://doi.org/10.1016/j.patcog.2021.108385,0031-3203,2022,108385,123,Pattern Recognition,Multi-scale signed recurrence plot based time series classification using inception architectural networks,article,ZHANG2022108385,
"Capsule Network (CapsNet) achieves great improvements in recognizing pose and deformation through a novel encoding mode. However, it carries a large number of parameters, leading to the challenge of heavy memory and computational cost. To solve this problem, we propose sparse CapsNet with an explicit regularizer in this paper. To our knowledge, itâs the first work that utilizes sparse optimization to compress CapsNet. Specifically, to reduce unnecessary weight parameters, we first introduce the component-wise absolute value regularizer into the objective function of CapsNet based on zero-means Laplacian prior. Then, to reduce the computational cost and speed up CapsNet, the weight parameters are further grouped by 2D filters and sparsified by 1-norm regularization. To train our model efficiently, a new stochastic proximal gradient algorithm, which has analytical solutions at each iteration, is presented. Extensive numerical experiments on four commonly used datasets validate the effectiveness and efficiency of the proposed method.","Capsule network, Model compression, Sparse regularization, Proximal gradient descent",Ruiyang Shi and Lingfeng Niu and Ruizhi Zhou,https://www.sciencedirect.com/science/article/pii/S0031320321006622,https://doi.org/10.1016/j.patcog.2021.108486,0031-3203,2022,108486,124,Pattern Recognition,Sparse CapsNet with explicit regularizer,article,SHI2022108486,
"Because of the tumor with infiltrative growth, the glioma boundary is usually fused with the brain tissue, which leads to the failure of accurately segmenting the brain tumor structure through single-modal images. The multi-modal ones are relatively complemented to the inherent heterogeneity and external boundary, which provide complementary features and outlines. Besides, it can retain the structural characteristics of brain diseases from multi angles. However, due to the particularity of multi-modal medical image sampling that increases uneven data density and dense structural vascular tumor mitosis, the glioma may have atypical boundary fuzzy and more noise. To solve this problem, in this paper, the dual-path network based on multi-modal feature fusion (MFF-DNet) is proposed. Firstly, the proposed network uses different kernels multiplexing methods to realize the combination of the large-scale perceptual domain and the non-linear mapping features, which effectively enhances the coherence of information flow. Then, the over-lapping frequency and the vanishing gradient phenomenon are reduced by the residual connection and the dense connection, which alleviate the mutual influence of multi-modal channels. Finally, a dual-path model based on the DenseNet network and the feature pyramid networks (FPN) is established to realize the fusion of low-level, middle-level, and high-level features. Besides, it increases the diversification of glioma non-linear structural features and improves the segmentation precision. A large number of ablation experiments show the effectiveness of the proposed model. The precision of the whole brain tumor and the core tumor can reach 0.92 and 0.90, respectively.","Brain tumor segmentation, Deep learning, Dual-path model, Magnetic resonance imaging, Multi-modal images",Lingling Fang and Xin Wang,https://www.sciencedirect.com/science/article/pii/S0031320321006105,https://doi.org/10.1016/j.patcog.2021.108434,0031-3203,2022,108434,124,Pattern Recognition,Brain tumor segmentation based on the dual-path network of multi-modal MRI images,article,FANG2022108434,
"Assessing if an image comes from a specific device is fundamental in many application scenarios. The most promising techniques to solve this problem rely on the Photo Response Non Uniformity (PRNU), a unique trace left during image acquisition. A PRNU fingerprint is computed from several images of a given device, then it is compared with the probe residual noise by means of correlation. However, such a comparison requires that PRNUs are synchronized: even small image transformations can spoil this task. Most of the attempts to solve the registration problem rely on time consuming brute-force search, which is prone to missing detections and false positives. In this paper, the problem is addressed from a computer vision perspective, exploiting recent image registration techniques based on deep learning, and focusing on scaling and rotation transformations. Experiments show that the proposed method is both more accurate and faster than state-of-the-art approaches.","Image forensics, PRNU, Deep learning, CNN, Rotation, Scale",Marco Fanfani and Alessandro Piva and Carlo Colombo,https://www.sciencedirect.com/science/article/pii/S0031320321005896,https://doi.org/10.1016/j.patcog.2021.108413,0031-3203,2022,108413,124,Pattern Recognition,PRNU registration under scale and rotation transform based on convolutional neural networks,article,FANFANI2022108413,
"Face photo-sketch synthesis refers transforming a face image between photo domain and sketch domain. It plays a crucial role in law enforcement and digital entertainment. A great deal of efforts have been devoted on face photo-sketch synthesis. However, limited by the weak identity supervision, existing methods mostly yield indistinct details or great deformation, resulting in poor perceptual appearance or low recognition accuracy. In the past several years, face identification achieved great progress, which represents the face images much more precisely than before. Considering the face image translation is also a type of face image re-representation, we attempt to introduce face recognition models to improve the synthesis performance. First, we applied existing synthesis models to augment the training set. Then, we proposed a full-scale identity supervision method to reduce redundant information introduced by these pseudo samples and take the valid information to enhance the intra-class variations. The proposed framework consists of two sub-networks: cross-domain translation (CT) network and intra-domain adaptation (IA) network. The CT network translates the input image from source domain to latent image of target domain, which overcomes the great gap between two domains with less structural deformation. The IA network adapts the perceptual appearance of latent image to target image by adversarial learning. Experimental results on CUHK Face Sketch Database and CUHK Face Sketch FERET Database demonstrate the proposed method preserved best perceptual appearance and more distinct details with less deformation.","Face photo-sketch synthesis, Identity supervision, Cross-domain translation, Intra-domain adaptation",Bing Cao and Nannan Wang and Jie Li and Qinghua Hu and Xinbo Gao,https://www.sciencedirect.com/science/article/pii/S0031320321006221,https://doi.org/10.1016/j.patcog.2021.108446,0031-3203,2022,108446,124,Pattern Recognition,Face photo-sketch synthesis via full-scale identity supervision,article,CAO2022108446,
"Food computing has recently attracted considerable research attention due to its significance for health risk analysis. In the literature, the majority of research efforts are dedicated to food recognition. Relatively few works are conducted for food counting and segmentation, which are essential for portion size estimation. This paper presents a deep neural network, named SibNet, for simultaneous counting and extraction of food instances from an image. The problem is challenging due to varying size and shape of food as well as arbitrary viewing angle of camera, not to mention that food instances often occlude each other. SibNet is novel for proposal of learning seed map to minimize the overlap between instances. The map facilitates counting and can be completed as an instance segmentation map that depicts the arbitrary shape and size of individual instance under occlusion. To this end, a novel sibling relation sub-network is proposed for pixel connectivity analysis. Along with this paper, three new datasets covering Western, Chinese and Japanese food are also constructed for performance evaluation. The three datasets and SibNet source code are publicly available.","Food counting, Food instance segmentation",Huu-Thanh Nguyen and Chong-Wah Ngo and Wing-Kwong Chan,https://www.sciencedirect.com/science/article/pii/S0031320321006464,https://doi.org/10.1016/j.patcog.2021.108470,0031-3203,2022,108470,124,Pattern Recognition,SibNet: Food instance counting and segmentation,article,NGUYEN2022108470,
"Domain adaptation aims at transferring knowledge from labeled source domain to unlabeled target domain. Current advances primarily concern single source domain and neglect the setting of multiple source domains. Previous unsupervised multi-source domain adaptation (MDA) algorithms only consider domain-level alignment, while neglecting the category-level information among multiple domains and the instance variations inside each domain. This paper introduces a Two-Way alignment framework for MDA (TWMDA), which considers both domain-level and category-level alignments, and addresses the instance variations. We first align the target and multiple sources on the domain-level by an adversarial learning process. To circumvent the drawbacks of adversarial learning, we further reduce the domain gap on the category-level by minimizing the distance between the category prototypes and unlabeled target instances. To address the instance variations, we design an instance weighting strategy for diverse source instances. The effectiveness of TWMDA is demonstrated on three benchmark datasets for image classification.","Domain adaptation, Feature extraction, Category prototype, Adversarial training, Instance weighting",Yong-Hui Liu and Chuan-Xian Ren,https://www.sciencedirect.com/science/article/pii/S0031320321006063,https://doi.org/10.1016/j.patcog.2021.108430,0031-3203,2022,108430,124,Pattern Recognition,A Two-Way alignment approach for unsupervised multi-Source domain adaptation,article,LIU2022108430,
"To ensure the operational safety and reliability, fault recognition of complex systems is becoming an essential process in industrial systems. However, the existing recognition methods mainly focus on common faults with enough data, which ignore that many faults are lack of samples in engineering practice. Transfer learning can be helpful, but irrelevant knowledge transfer can cause performance degradation, especially in complex systems. To address the above problem, a hierarchy guided transfer learning framework (HGTL) is proposed in this paper for fault recognition with few-shot samples. Firstly, we fuse domain knowledge, label semantics and inter-class distance to calculate the affinity between categories, based on which a category hierarchical tree is constructed by hierarchical clustering. Then, guided by the hierarchical structure, the samples in most similar majority classes are selected from the source domain to pre-train the hierarchical feature learning network (HFN) and extract the transferable fault information. For the fault knowledge extracted from the child nodes of one parent node are similar and can be transferred with each other, so the trained HFN can extract better features of few samples classes with the help of the information from similar faults, and used to address few-shot fault recognition problems. Finally, a dataset of a nuclear power system with 65 categories and the widely used Tennessee Eastman dataset are analyzed respectively via the proposed method, as well as state-of-the-art recognition methods for comparison. The experimental results demonstrate the effectiveness and superiority of the proposed method in fault recognition with few-shot problem.","Transfer learning, Fault recognition, Few-shot problem, Hierarchical category structure, Complex systems",Hao Chen and Ruonan Liu and Zongxia Xie and Qinghua Hu and Jianhua Dai and Junhai Zhai,https://www.sciencedirect.com/science/article/pii/S003132032100563X,https://doi.org/10.1016/j.patcog.2021.108383,0031-3203,2022,108383,123,Pattern Recognition,Majorities help minorities: Hierarchical structure guided transfer learning for few-shot fault recognition,article,CHEN2022108383,
"Stroke extraction and matching are critical for structural interpretation based applications of handwritten Chinese characters, such as Chinese character education and calligraphy analysis. Stroke extraction from offline handwritten Chinese characters is difficult because of the missing of temporal information, the multi-stroke structures and the distortion of handwritten shapes. In this paper, we propose a comprehensive scheme for solving the stroke extraction problem for handwritten Chinese characters. The method consists of three main steps: (1) fully convolutional network (FCN) based skeletonization; (2) query pixel guided stroke extraction; (3) model-based stroke matching. Specifically, based on a recently proposed architecture of FCN, both the stroke skeletons and cross regions are firstly extracted from the character image by the proposed SkeNet and CrossNet, respectively. Stroke extraction is solved by simulating the human perception that once given a certain pixel from non-cross region of a stroke, the whole stroke containing the pixel can be traced. To realize this idea, we formulate stroke extraction as a problem of pairing and connecting skeleton-wise stroke segments which are adjacent to the same cross region, where the pairing consistency between stroke segments is measured using a PathNetÂ [1]. To reduce the ambiguity of stroke extraction, the extracted candidate strokes are matched with a character model consisting of standard strokes by tree search to identify the correct strokes. For verifying the effectiveness of the proposed method, we train and test our models on character images with stroke segmentation annotations generated from the online handwriting datasets CASIA-OLHWDB and ICDAR13-Online, as well as a dataset of Regularly-Written online handwritten characters (RW-OLHWDB). The experimental results demonstrate the effectiveness of the proposed method and provide several benchmarks. Particularly, the precisions of stroke extraction for ICDAR13-Online and RW-OLHWDB are 89.0% and 94.9%, respectively.","Stroke extraction, Conditional fully convolutional network, PathNet, Stroke matching, Tree search",Tie-Qiang Wang and Xiaoyi Jiang and Cheng-Lin Liu,https://www.sciencedirect.com/science/article/pii/S0031320321005926,https://doi.org/10.1016/j.patcog.2021.108416,0031-3203,2022,108416,123,Pattern Recognition,Query Pixel Guided Stroke Extraction with Model-Based Matching for Offline Handwritten Chinese Characters,article,WANG2022108416,
"The semantic segmentation of building facades is critical for various construction applications, such as urban building reconstruction and damage assessments. As there is a lack of 3D point cloud datasets related to fine-grained building facades, in this work we construct the first large-scale point cloud benchmark dataset for building facade semantic segmentation. In terms of the characteristics of building facade dataset, the existing methods of semantic segmentation cannot fully mine the local neighborhood information of point clouds; therefore, we propose an attention module that learns Dual Local Attention features, called DLA in this paper. The proposed DLA module consists of two blocks, a self-attention block and an attentive pooling block, which both embed an enhanced position encoding block. The DLA module can be easily embedded into various network architectures for point cloud segmentation, naturally resulting in a new 3D semantic segmentation network with an encoder-decoder architecture; we called this network the DLA-Net. Extensive experimental results on our constructed building facade dataset demonstrate that the proposed DLA-Net achieves better performance than the state-of-the-art methods for semantic segmentation.","Semantic segmentation, Building facade, Self-attention, Attentive pooling, DLA-Net",Yanfei Su and Weiquan Liu and Zhimin Yuan and Ming Cheng and Zhihong Zhang and Xuelun Shen and Cheng Wang,https://www.sciencedirect.com/science/article/pii/S0031320321005525,https://doi.org/10.1016/j.patcog.2021.108372,0031-3203,2022,108372,123,Pattern Recognition,DLA-Net: Learning dual local attention features for semantic segmentation of large-scale building facade point clouds,article,SU2022108372,
"Craniofacial reconstruction is applied to identify human remains in the absence of determination data (e.g., fingerprinting, dental records, radiological materials, or DNA), by predicting the likeness of the unidentified remains based on the internal relationship between the skull and face. Conventional 3D methods are usually based on statistical models with poor capacity, which limit the description of such complex relationship. Moreover, the required high-quality data are difficult to collect. In this study, we present a novel craniofacial reconstruction paradigm that synthesize craniofacial images from 2D computed tomography scan of skull data. The key idea is to recast craniofacial reconstruction as an image translation task, with the goal of generating corresponding craniofacial images from 2D skull images. To this end, we design an automatic skull-to-face transformation system based on deep generative adversarial nets. The system was trained on 4551 paired skull-face images obtained from 1780 CT head scans of the Han Chinese population. To the best of our knowledge, this is the only database of this magnitude in the literature. Finally, to accurately evaluate the performance of the model, a face recognition task employing five existing deep learning algorithms, âFaceNet, âSphereFace, âCosFace, âArcFace, and âMagFace, was tested on 102 reconstruction cases in a face pool composed of 1744 CT-scan face images. The experimental results demonstrate that the proposed method can be used as an effective forensic tool.","Craniofacial reconstruction, CT scans, Deep learning",Yuan Li and Jian Wang and Weibo Liang and Hui Xue and Zhenan He and Jiancheng Lv and Lin Zhang,https://www.sciencedirect.com/science/article/pii/S0031320321005768,https://doi.org/10.1016/j.patcog.2021.108400,0031-3203,2022,108400,124,Pattern Recognition,CR-GAN: Automatic craniofacial reconstruction for personal identification,article,LI2022108400,
"The reflection effect is unavoidable when taking photos through glasses or other transparent materials, which introduces undesired information into pictures. Hence, removing the influence of reflection becomes a key problem in computer vision. One of the main obstacles of recent learning based approaches is the lacking of realistic training data. To address this issue, we introduce a new dataset synthesis method as well as a novel neural network architecture for single image reflection removal. First, we make use of the polarization characteristics of light into the synthesis of datasets, so as to obtain more realistic and diversified training dataset POL. Then, we design a novel Progressive Polarization based Reflection Removal Network (P2R2Net), which preliminary estimates the coarse background layer to guide the final reflection removal. We demonstrate that our method performs better than the state-of-the-art single image reflection removal methods through quantitative and qualitative experimental comparisons. Specifically, the average PSNR of our restored images selected from three representative benchmark datesets: âReal20â, âSIR2â and âNatureâ is improved at least 0.49 compared with existing methods and reaches to 24.52.","Deep learning, Reflection removal, Polarization, Progressive network, Convolutional neural networks",Youxin Pang and Mengke Yuan and Qiang Fu and Peiran Ren and Dong-Ming Yan,https://www.sciencedirect.com/science/article/pii/S0031320321006737,https://doi.org/10.1016/j.patcog.2021.108497,0031-3203,2022,108497,124,Pattern Recognition,Progressive polarization based reflection removal via realistic training data generation,article,PANG2022108497,
"Clustering ensemble, or consensus clustering, has emerged as a powerful tool for improving both the robustness and the stability of results from individual clustering methods. Weighted clustering ensemble arises naturally from clustering ensemble. One of the arguments for weighted clustering ensemble is that elements (clusterings or clusters) in a clustering ensemble are of different quality, or that objects or features are of varying significance. However, it is not possible to directly apply the weighting mechanisms from classification (supervised) domain to clustering (unsupervised) domain, also because clustering is inherently an ill-posed problem. This paper provides an overview of weighted clustering ensemble by discussing different types of weights, major approaches to determining weight values, and applications of weighted clustering ensemble to complex data. The unifying framework presented in this paper will help clustering practitioners select the most appropriate weighting mechanisms for their own problems.","Ensemble selection, Fuzzy clustering, Labeling correspondence, Multi-view data, Temporal data",Mimi Zhang,https://www.sciencedirect.com/science/article/pii/S003132032100604X,https://doi.org/10.1016/j.patcog.2021.108428,0031-3203,2022,108428,124,Pattern Recognition,Weighted clustering ensemble: A review,article,ZHANG2022108428,
"Recent progresses in domain adaptive semantic segmentation demonstrate the effectiveness of adversarial learning (AL) in unsupervised domain adaptation. However, most adversarial learning based methods align source and target distributions at a global image level but neglect the inconsistency around local image regions. This paper presents a novel multi-level adversarial network (MLAN) that aims to address inter-domain inconsistency at both global image level and local region level optimally. MLAN has two novel designs, namely, region-level adversarial learning (RL-AL) and co-regularized adversarial learning (CR-AL). Specifically, RL-AL models prototypical regional context-relations explicitly in the feature space of a labelled source domain and transfers them to an unlabelled target domain via adversarial learning. CR-AL fuses region-level AL and image-level AL optimally via mutual regularization. In addition, we design a multi-level consistency map that can guide domain adaptation in both input space (i.e., image-to-image translation) and output space (i.e., self-training) effectively. Extensive experiments show that MLAN outperforms the state-of-the-art with a large margin consistently across multiple datasets.","Unsupervised domain adaptation, Semantic segmentation, Adversarial learning, Self training",Jiaxing Huang and Dayan Guan and Aoran Xiao and Shijian Lu,https://www.sciencedirect.com/science/article/pii/S0031320321005641,https://doi.org/10.1016/j.patcog.2021.108384,0031-3203,2022,108384,123,Pattern Recognition,Multi-level adversarial network for domain adaptive semantic segmentation,article,HUANG2022108384,
"Multiview clustering has become an important research topic during the past decade. However, partial views of many data instances are missing in some realistic multiview learning scenarios. To handle this problem, we develop an effective incomplete multiview nonnegative representation learning (IMNRL) framework, which is suitable for incomplete multiview clustering in various situations. The IMNRL framework performs matrix factorization on multiple incomplete graphs and decomposes these incomplete graphs into a consensus nonnegative representation and view-specific spectral representations, which integrates the advantages of multiview nonnegative representation learning and graph learning. The proposed framework has the following merits: (1) it learns a consensus nonnegative embedding and view-specific embeddings simultaneously; (2) the nonnegative embedding satisfies the neighbor constraint on each incomplete view, which directly reveals the multiview clustering results. Experimental results show that the proposed framework outperforms other state-of-the-art incomplete multiview clustering algorithms.","Multiview clustering, Graph learning, Incomplete multiview clustering, Nonnegative matrix factorization",Nan Zhang and Shiliang Sun,https://www.sciencedirect.com/science/article/pii/S0031320321005884,https://doi.org/10.1016/j.patcog.2021.108412,0031-3203,2022,108412,123,Pattern Recognition,Incomplete multiview nonnegative representation learning with multiple graphs,article,ZHANG2022108412,
"Given the descriptions of classes, Zero-Shot Learning (ZSL) aims to recognize unseen samples by learning a projection between the visual features of samples and the semantic descriptions (prototypes) of classes from seen data. However, due to the inherent distribution gap between seen and unseen domains, the learned projection is generally biased to seen classes and may produce misleading relationships between unseen samples and prototypes (sample-prototype relationship). To tackle this problem, we propose a Cluster-Prototype Matching (CPM) framework which exploits the distribution information of samples to explore the cluster structure of samples and then use the robust cluster-prototype relationship to correct the biased sample-prototype relationship. Specifically, we first use an iterative cluster generation module to identify the underlying cluster structure of samples based on their embedding features, which are acquired via a basic ZSL model. Then each identified cluster will be matched with a specific class prototype through the Kuhn-Munkres algorithm, based on which we can export a sharp cluster-prototype similarity. Finally, the cluster-prototype similarity is combined with the sample-prototype similarity to determine the class labels of test samples. We apply CPM to five well-established ZSL methods and the experimental results show that CPM can significantly improve the performance of basic models and enable them achieve or beyond the state-of-the-art.","Zero-shot learning, Image classification, Cluster-prototype matching, Domain shift",Jing Zhang and Qingyong Li and YangLi-ao Geng and Wen Wang and Wenju Sun and Chuan Shi and Zhengming Ding,https://www.sciencedirect.com/science/article/pii/S0031320321006452,https://doi.org/10.1016/j.patcog.2021.108469,0031-3203,2022,108469,124,Pattern Recognition,A zero-shot learning framework via cluster-prototype matching,article,ZHANG2022108469,
"Classifier ensemble pruning is a strategy through which a subensemble can be identified via optimizing a predefined performance criterion. Choosing the optimum or suboptimum subensemble decreases the initial ensemble size and increases its predictive performance. In this article, a set of heuristic metrics will be analyzed to guide the pruning process. The analyzed metrics are based on modifying the order of the classifiers in the bagging algorithm, with selecting the first set in the queue. Some of these criteria include general accuracy, the complementarity of decisions, ensemble diversity, the margin of samples, minimum redundancy, discriminant classifiers, and margin hybrid diversity. The efficacy of those metrics is affected by the original ensemble size, the required subensemble size, the kind of individual classifiers, and the number of classes. While the efficiency is measured in terms of the computational cost and the memory space requirements. The performance of those metrics is assessed over fifteen binary and fifteen multiclass benchmark classification tasks, respectively. In addition, the behavior of those metrics against randomness is measured in terms of the distribution of their accuracy around the median. Results show that ordered aggregation is an efficient strategy to generate subensembles that improve both predictive performance as well as computational and memory complexities of the whole bagging ensemble.","Heuristic optimization, Ensemble selection, Ensemble pruning, Classifier ensemble, Machine learning, Difficult samples, Ordering-based pruning, Classifier complementariness",Amgad M. Mohammed and Enrique Onieva and MichaÅ WoÅºniak and Gonzalo MartÃ­nez-MuÃ±oz,https://www.sciencedirect.com/science/article/pii/S0031320321006695,https://doi.org/10.1016/j.patcog.2021.108493,0031-3203,2022,108493,124,Pattern Recognition,An analysis of heuristic metrics for classifier ensemble pruning based on ordered aggregation,article,MOHAMMED2022108493,
"The recent advances in Big Data have opened up the opportunity to develop competitive Global Forecasting Models (GFM) that simultaneously learn from many time series. Although, the concept of series relatedness has been heavily exploited with GFMs to explain their superiority over local statistical benchmarks, this concept remains largely under-investigated in an empirical setting. Hence, this study attempts to explore the factors that affect GFM performance, by simulating a number of datasets having controllable characteristics. The factors being controlled are along the homogeneity/heterogeneity of series, the complexity of patterns in the series, the complexity of forecasting models, and the lengths/number of series. We simulate time series from simple Data Generating Processes (DGP), such as Auto Regressive (AR), Seasonal AR and Fourier Terms to complex DGPs, such as Chaotic Logistic Map, Self-Exciting Threshold Auto-Regressive and Mackey-Glass Equations. We perform experiments on these datasets using Recurrent Neural Networks (RNN), Feed-Forward Neural Networks, Pooled Regression models and Light Gradient Boosting Models (LGBM) built as GFMs, and compare their performance against standard statistical forecasting techniques. Our experiments demonstrate that with respect to GFM performance, relatedness is closely associated with other factors such as the availability of data, complexity of data and the complexity of the forecasting technique used. Also, techniques such as RNNs and LGBMs having complex non-linear modelling capabilities, when built as GFMs are competitive methods under challenging forecasting scenarios such as short series, heterogeneous series and having minimal prior knowledge of the data patterns.","Time series forecasting, Global forecasting models, Time series simulation, Data generating processes",Hansika Hewamalage and Christoph Bergmeir and Kasun Bandara,https://www.sciencedirect.com/science/article/pii/S0031320321006178,https://doi.org/10.1016/j.patcog.2021.108441,0031-3203,2022,108441,124,Pattern Recognition,Global models for time series forecasting: A Simulation study,article,HEWAMALAGE2022108441,
"Multi-view clustering attracts more and more attention due to the fact that it can utilize the complementary and compatible information from multi-view data sets. In many graph-based multi-view clustering approaches, the graph quality is important since it influences the following clustering performance. Therefore, learning a high quality similarity graph is desired. In this paper, we propose a novel clustering method which is named as Self-weighting Multi-view Spectral Clustering based on Nuclear NormÂ (SMSC_NN). Specifically, to fully utilize the multiple view features, the common consensus representation is learned. Moreover, to capture the principal components from various view features, the nuclear norm is introduced which can make the view-specific information be well explored. Further, due to the fact that each view feature denotes a sort of specific property, the adaptive weights are assigned instead of equal view weights. In order to verify the effectiveness of the proposed method, four multi-view data sets are used to conduct the clustering experiments. Extensive experimental results demonstrate the superiority of the proposed method comparing with state-of-the-art multi-view clustering approaches. In addition, the proposed approach is experimented on the Cal101-20 data set with âsalt and pepperâ noises, and experimental results verify that the proposed SMSC_NN method can remain robust to noises.","Unsupervised learning, Multi-view clustering, Nuclear norm, Self-weighting",Shaojun Shi and Feiping Nie and Rong Wang and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320321006051,https://doi.org/10.1016/j.patcog.2021.108429,0031-3203,2022,108429,124,Pattern Recognition,Self-weighting multi-view spectral clustering based on nuclear norm,article,SHI2022108429,
"Improving the performance of gait recognition under multiple camera views (i.e., cross-view gait recognition) and various conditions is urgent. From observation, we find that adjacent body parts are inter-related while walking, and each frame in a gait sequence possesses different degrees of semantic information. In this paper, we propose a novel model, GaitSlice, to analyze the human gait based on spatio-temporal slice features. Spatially, we design Slice Extraction Device (SED) to form top-down inter-related slice features. Temporally, we introduce Residual Frame Attention Mechanism (RFAM) to acquire and highlight the key frames. To better simulate reality, GaitSlice combines parallel RFAMs with inter-related slice features to focus on the featuresâ spatio-temporal information. We evaluate our model on CASIA-B and OU-MVLP gait datasets and compare it with six typical gait recognition models by using rank-1 accuracy. The results show that GaitSlice achieves high accuracy in gait recognition under cross-view and various walking conditions.","Gait recognition, Key frame, Cross-view, Attention mechanism, Slice feature, GaitSlice",Huakang Li and Yidan Qiu and Huimin Zhao and Jin Zhan and Rongjun Chen and Tuanjie Wei and Zhihui Huang,https://www.sciencedirect.com/science/article/pii/S0031320321006294,https://doi.org/10.1016/j.patcog.2021.108453,0031-3203,2022,108453,124,Pattern Recognition,GaitSlice: A gait recognition model based on spatio-temporal slice features,article,LI2022108453,
"This paper aims to provide a compact but accessible introduction to Conformal Predictors (CP), a Machine Learning method with the distinguishing property of producing predictions that exhibit a chosen error rate. This property, referred to as validity, is backed by not only asymptotic, but also finite-sample probabilistic guarantees. CPs differ from the conventional approach to prediction in that they introduce hedging in the form of set-valued predictions. The CP validity guarantees do not require assumptions such as priors, but are of broad applicability as they rely solely on exchangeability. The CP framework is universal in the sense that it operates on top of virtually any Machine Learning method. In addition to the formal definition, this introduction discusses CP variants that can be computed efficiently (Inductive or âsplitâ CP) or that are suitable for imbalanced data sets (class-conditional CP). Finally, a short survey of the field provides references for relevant research and highlights the variety of domains in which CPs have found valuable application.","Conformal prediction, Nonparametric methods, Confidence",Paolo Toccaceli,https://www.sciencedirect.com/science/article/pii/S003132032100683X,https://doi.org/10.1016/j.patcog.2021.108507,0031-3203,2022,108507,124,Pattern Recognition,Introduction to conformal predictors,article,TOCCACELI2022108507,
"Domain knowledge of face shapes and structures plays an important role in face inpainting. However, general inpainting methods focus mainly on the resolution of generated images without considering the particular structure of human faces and generally produce inharmonious facial parts. Existing face-inpainting methods incorporate only one type of facial feature for face completion, and their results are still undesirable. To improve face inpainting quality, we propose a Domain Embedded Generative Adversarial Network (DE-GAN) for face inpainting. DE-GAN embeds three types of face domain knowledge (i.e., face mask, face part, and landmark image) via a hierarchical variational auto-encoder (HVAE) into a latent variable space to guide face completion. Two adversarial discriminators, a global discriminator and a patch discriminator, are used to judge whether the generated distribution is close to the real distribution or not. Experiments on two public face datasets demonstrate that our proposed method generates higher quality inpainting results with consistent and harmonious facial structures and appearance than existing methods and achieves the state-of-the-art performance, esp. for inpainting under-pose variations.","Face Inpainting, Domain Embedding, Adversarial Generative Model",Xian Zhang and Xin Wang and Canghong Shi and Zhe Yan and Xiaojie Li and Bin Kong and Siwei Lyu and Bin Zhu and Jiancheng Lv and Youbing Yin and Qi Song and Xi Wu and Imran Mumtaz,https://www.sciencedirect.com/science/article/pii/S0031320321005914,https://doi.org/10.1016/j.patcog.2021.108415,0031-3203,2022,108415,124,Pattern Recognition,DE-GAN: Domain Embedded GAN for High Quality Face Image Inpainting,article,ZHANG2022108415,
"Crowd counting is a computer vision task on which considerable progress has recently been made thanks to convolutional neural networks. However, it remains a challenging task even in scene-specific settings, in real-world application scenarios where no representative images of the target scene are available, not even unlabelled, for training or fine-tuning a crowd counting model. Inspired by previous work in other computer vision tasks, we propose a simple but effective solution for the above application scenario, which consists of automatically building a scene-specific training set of synthetic images. Our solution does not require from end-users any manual annotation effort nor the collection of representative images of the target scene. Extensive experiments on several benchmark data sets show that the proposed solution can improve the effectiveness of existing crowd counting methods.","Crowd counting, Scene-specific settings, Synthetic training images",Rita Delussu and Lorenzo Putzu and Giorgio Fumera,https://www.sciencedirect.com/science/article/pii/S0031320321006609,https://doi.org/10.1016/j.patcog.2021.108484,0031-3203,2022,108484,124,Pattern Recognition,Scene-specific crowd counting using synthetic training images,article,DELUSSU2022108484,
"We had previously proposed a supervised Laplacian eigenmap for visualization (SLE-ML) that can handle multi-label data. In addition, SLE-ML can control the trade-off between the class separability and local structure by a single trade-off parameter. However, SLE-ML cannot transform new data, that is, it has the âout-of-sampleâ problem. In this paper, we show that this problem is solvable, that is, it is possible to simulate the same transformation perfectly using a set of linear sums of reproducing kernels (KSLE-ML) with a nonsingular Gram matrix. We experimentally showed that the difference between training and testing is not large; thus, a high separability of classes in a low-dimensional space is realizable with KSLE-ML by assigning an appropriate value to the trade-off parameter. This offers the possibility of separability-guided feature extraction for classification. In addition, to optimize the performance of KSLE-ML, we conducted both kernel selection and parameter selection. As a result, it is shown that parameter selection is more important than kernel selection. We experimentally demonstrated the advantage of using KSLE-ML for visualization and for feature extraction compared with a few typical algorithms.","Supervised Laplacian eigenmaps, Out-of-sample problem, Multi-label problems, Kernel trick, Separability-guided feature extraction",Mariko Tai and Mineichi Kudo and Akira Tanaka and Hideyuki Imai and Keigo Kimura,https://www.sciencedirect.com/science/article/pii/S0031320321005756,https://doi.org/10.1016/j.patcog.2021.108399,0031-3203,2022,108399,123,Pattern Recognition,Kernelized Supervised Laplacian Eigenmap for Visualization and Classification of Multi-Label Data,article,TAI2022108399,
"This paper presents a new approach to a robust Gaussian process regression, creating a non-parametric Bayesian regression estimate robust to outliers. Most existing approaches replace an outlier-prone Gaussian likelihood with a non-Gaussian likelihood induced from a heavy tail distribution, such as the Laplace distribution and Student-t distribution. However, the use of a non-Gaussian likelihood would incur the need for a computationally expensive Bayesian approximate computation in the posterior inferences. The proposed approach models an outlier as a noisy and biased observation of an unknown regression function, and accordingly, the likelihood contains bias terms to explain the degree of deviations from the regression function. We introduce two bias models that handle the bias terms differently, treating a bias as an unknown and fixed quantity or treating a bias as a random quantity. We entail how the biases can be estimated accurately with other hyperparameters by a regularized maximum likelihood estimation. Conditioned on the bias estimates, the robust GP regression can be reduced to a standard GP regression problem with analytical forms of the predictive mean and variance estimates. Therefore, the proposed approach is simple and very computationally attractive. It also gives a very robust and accurate GP estimate for many tested scenarios. For the numerical evaluation, we perform a comprehensive simulation study to evaluate the proposed approach with the comparison to the existing robust GP approaches under various simulated scenarios of different outlier proportions and different noise levels. The approach is applied to data from two measurement systems, where the predictors are based on robust environmental parameter measurements and the response variables utilize more complex chemical sensing methods that contain a certain percentage of outliers. The utility of the measurement systems and value of the environmental data are improved through the computationally efficient GP regression and bias model.","Robust regression, Gaussian process, Random bias estimation, Regularized likelihood maximization, Sensor data",Chiwoo Park and David J. Borth and Nicholas S. Wilson and Chad N. Hunter and Fritz J. Friedersdorf,https://www.sciencedirect.com/science/article/pii/S0031320321006208,https://doi.org/10.1016/j.patcog.2021.108444,0031-3203,2022,108444,124,Pattern Recognition,Robust Gaussian process regression with a bias model,article,PARK2022108444,
"Most existing methods for time series clustering rely on distances calculated from the entire raw data using the Euclidean distance or Dynamic Time Warping distance. In this work, we propose to embed the time series onto higher-dimensional spaces to obtain geometric representations of the time series themselves. Particularly, the embedding on RnÃp, on the Stiefel manifold and on the unit Sphere are analyzed for their performances with respect to several yet well-known clustering algorithms. The gain brought by the geometrical representation for the time series clustering is illustrated through a large benchmark of databases. We particularly exhibit that, firstly, the embedding of the time series on higher dimensional spaces gives better results than classical approaches and, secondly, that the embedding on the Stiefel manifold - in conjunction with UMAP and HDBSCAN clustering algorithms - is the recommended framework for time series clustering.","Clustering, Time series, Delayed coordinate embedding, Embedding, Stiefel manifold, UMAP, HDBSCAN",ClÃ©ment PÃ©alat and Guillaume Bouleux and Vincent Cheutet,https://www.sciencedirect.com/science/article/pii/S0031320321005999,https://doi.org/10.1016/j.patcog.2021.108423,0031-3203,2022,108423,124,Pattern Recognition,Improved time series clustering based on new geometric frameworks,article,PEALAT2022108423,
"The so-called âattention mechanismsâ in Deep Neural Networks (DNNs) denote an automatic adaptation of DNNs to capture representative features given a specific classification task and related data. Such attention mechanisms perform both globally by reinforcing feature channels and locally by stressing features in each feature map. Channel and feature importance are learnt in the global end-to-end DNNs training process. In this paper, we present a study and propose a method with a different approach, adding supplementary visual data next to training images. We use human visual attention maps obtained independently with psycho-visual experiments, both in task-driven or in free viewing conditions, or powerful models for prediction of visual attention maps. We add visual attention maps as new data alongside images, thus introducing human visual attention into the DNNs training and compare it with both global and local automatic attention mechanisms. Experimental results show that known attention mechanisms in DNNs work pretty much as human visual attention, but still the proposed approach allows a faster convergence and better performance in image classification tasks.","Deep learning, Image classification, Object detection, Visual attention, Saliency maps",Abraham Montoya Obeso and Jenny Benois-Pineau and Mireya SaraÃ­ {GarcÃ­a VÃ¡zquez} and Alejandro Ãlvaro RamÃ­rez Acosta,https://www.sciencedirect.com/science/article/pii/S0031320321005872,https://doi.org/10.1016/j.patcog.2021.108411,0031-3203,2022,108411,123,Pattern Recognition,Visual vs internal attention mechanisms in deep neural networks for image classification and object detection,article,OBESO2022108411,
"Aggregating infinite-dimensional features has demonstrated superiority compared with their finite-dimensional counterparts. However, most existing methods approximate infinite-dimensional features with finite-dimensional representations, which inevitably results in approximation error and inferior performance. In this paper, we propose a non-approximate aggregation method that directly aggregates infinite-dimensional features rather than relying on approximation strategies. Specifically, since infinite-dimensional features are infeasible to store, represent and compute explicitly, we introduce a factorized bilinear model to capture pairwise second-order statistics of infinite-dimensional features as a global descriptor. It enables the resulting aggregation formulation to only involve the inner product in an infinite-dimensional space. The factorized bilinear model is calculated by a Sigmoid kernel to generate informative features containing infinite order statistics. Experiments on four visual tasks including the fine-grained, indoor scene, texture, and material classification, demonstrate that our method consistently achieves the state-of-the-art performance.","Feature aggregation, Infinite-dimensional features, Non-approximate method, Second-order statistics",Jindou Dai and Yuwei Wu and Zhi Gao and Yunde Jia,https://www.sciencedirect.com/science/article/pii/S0031320321005598,https://doi.org/10.1016/j.patcog.2021.108397,0031-3203,2022,108397,124,Pattern Recognition,Infinite-dimensional feature aggregation via a factorized bilinear model,article,DAI2022108397,
"Incorporating the depth (D) information to RGB images has proven the effectiveness and robustness in semantic segmentation. However, the fusion between them is not trivial due to their inherent physical meaning discrepancy, in which RGB represents RGB information but D depth information. In this paper, we propose a co-attention network (CANet) to build sound interaction between RGB and depth features. The key part in the CANet is the co-attention fusion part. It includes three modules. Specifically, the position and channel co-attention fusion modules adaptively fuse RGB and depth features in spatial and channel dimensions. An additional fusion co-attention module further integrates the outputs of the position and channel co-attention fusion modules to obtain a more representative feature which is used for the final semantic segmentation. Extensive experiments witness the effectiveness of the CANet in fusing RGB and depth features, achieving state-of-the-art performance on two challenging RGB-D semantic segmentation datasets, i.e., NYUDv2 and SUN-RGBD.","RGB-D, Multi-modal fusion, Co-attention, Semantic segmentation",Hao Zhou and Lu Qi and Hai Huang and Xu Yang and Zhaoliang Wan and Xianglong Wen,https://www.sciencedirect.com/science/article/pii/S0031320321006440,https://doi.org/10.1016/j.patcog.2021.108468,0031-3203,2022,108468,124,Pattern Recognition,CANet: Co-attention network for RGB-D semantic segmentation,article,ZHOU2022108468,
"Inspections on current graph neural networks suggest us to reconsider the computational aspect of the final aggregation. We consider that such aggregations perform a prediction smoothing and impute their potential drawbacks to be the inter-class interference implied by the underlying graphs. We aim at weakening the inter-class connections so that aggregations focus more on intra-class relations and producing smooth predictions according to weakening results. We apply a metric learning module to learn new edge weights and combine entropy losses to ensure the correspondence between the predictions and the learnt distances so that the weights of inter-class edges are reduced and predictions are smoothed according to the modified graph. Experiments on four citation networks and a Wiki network show that in comparison with other state-of-the-art graph neural networks, the proposed algorithm can improve the classification accuracy.","Adaptive graph smoothing networks, Graph convolutional networks, Semi-supervised learning, Graph node classification",Ruigang Zheng and Weifu Chen and Guocan Feng,https://www.sciencedirect.com/science/article/pii/S0031320321006683,https://doi.org/10.1016/j.patcog.2021.108492,0031-3203,2022,108492,124,Pattern Recognition,Semi-supervised node classification via adaptive graph smoothing networks,article,ZHENG2022108492,
"The paper proposes a semantic clustering based deduction learning by mimicking the learning and thinking process of human brains. Human beings can make judgments based on experience and cognition, and as a result, no one would recognize an unknown animal as a car. Inspired by this observation, we propose to train deep learning models using the clustering prior that can guide the models to learn with the ability of semantic deducing and summarizing from classification attributes, such as a cat belonging to animals while a car pertaining to vehicles. The proposed approach realizes the high-level clustering in the semantic space, enabling the model to deduce the relations among various classes during the learning process. In addition, the paper introduces a semantic prior based random search for the opposite labels to ensure the smooth distribution of the clustering and the robustness of the classifiers. The proposed approach is supported theoretically and empirically through extensive experiments. We compare the performance across state-of-the-art classifiers on popular benchmarks, and the generalization ability is verified by adding noisy labeling to the datasets. Experimental results demonstrate the superiority of the proposed approach.","Deduction learning, Clustering prior, Semantic space, Smooth semantic clustering",Wenchi Ma and Xuemin Tu and Bo Luo and Guanghui Wang,https://www.sciencedirect.com/science/article/pii/S0031320321006166,https://doi.org/10.1016/j.patcog.2021.108440,0031-3203,2022,108440,124,Pattern Recognition,Semantic clustering based deduction learning for image recognition and classification,article,MA2022108440,
"Due to the irregular shapes,various sizes and indistinguishable boundaries between the normal and infected tissues, it is still a challenging task to accurately segment the infected lesions of COVID-19 on CT images. In this paper, a novel segmentation scheme is proposed for the infections of COVID-19 by enhancing supervised information and fusing multi-scale feature maps of different levels based on the encoder-decoder architecture. To this end, a deep collaborative supervision (Co-supervision) scheme is proposed to guide the network learning the features of edges and semantics. More specifically, an Edge Supervised Module (ESM) is firstly designed to highlight low-level boundary features by incorporating the edge supervised information into the initial stage of down-sampling. Meanwhile, an Auxiliary Semantic Supervised Module (ASSM) is proposed to strengthen high-level semantic information by integrating mask supervised information into the later stage. Then an Attention Fusion Module (AFM) is developed to fuse multiple scale feature maps of different levels by using an attention mechanism to reduce the semantic gaps between high-level and low-level feature maps. Finally, the effectiveness of the proposed scheme is demonstrated on four various COVID-19 CT datasets. The results show that the proposed three modules are all promising. Based on the baseline (ResUnet), using ESM, ASSM, or AFM alone can respectively increase Dice metric by 1.12%, 1.95%,1.63% in our dataset, while the integration by incorporating three models together can rise 3.97%. Compared with the existing approaches in various datasets, the proposed method can obtain better segmentation performance in some main metrics, and can achieve the best generalization and comprehensive performance.","Semantic segmentation, Multi-scale features, Attention mechanism, Feature fusion, COVID-19",Haigen Hu and Leizhao Shen and Qiu Guan and Xiaoxin Li and Qianwei Zhou and Su Ruan,https://www.sciencedirect.com/science/article/pii/S0031320321006282,https://doi.org/10.1016/j.patcog.2021.108452,0031-3203,2022,108452,124,Pattern Recognition,Deep co-supervision and attention fusion strategy for automatic COVID-19 lung infection segmentation on CT images,article,HU2022108452,
"Hashing is one of the most promising techniques in approximate nearest neighbor search due to its time efficiency and low cost in memory. Recently, with the help of deep learning, deep supervised hashing can perform representation learning and compact hash code learning jointly in an end-to-end style, and obtains better retrieval accuracy compared to non-deep methods. However, most deep hashing methods are trained with a pair-wise loss or triplet loss in a mini-batch style, which makes them inefficient at data sampling and cannot preserve the global similarity information. Besides that, many existing methods generate hash codes with redundant or even harmful bits, which is a waste of space and may lower the retrieval accuracy. In this paper, we propose a novel deep reinforcement hashing model with redundancy elimination called Deep Reinforcement De-Redundancy Hashing (DRDH), which can fully exploit large-scale similarity information and eliminate redundant hash bits with deep reinforcement learning. DRDH conducts hash code inference in a block-wise style, and uses Deep Q Network (DQN) to eliminate redundant bits. Very promising results have been achieved on four public datasets, i.e., CIFAR-10, NUS-WIDE, MS-COCO, and Open-Images-V4, which demonstrate that our method can generate highly compact hash codes and yield better retrieval performance than those of state-of-the-art methods.","Deep reinforcement hashing, Redundancy elimination, Image retrieval, Block-wise hash code inference, Hash code mapping, Hash code de-redundancy",Juexu Yang and Yuejie Zhang and Rui Feng and Tao Zhang and Weiguo Fan,https://www.sciencedirect.com/science/article/pii/S0031320319304170,https://doi.org/10.1016/j.patcog.2019.107116,0031-3203,2020,107116,100,Pattern Recognition,Deep reinforcement hashing with redundancy elimination for effective image retrieval,article,YANG2020107116,
"Existing approaches for person re-identification have concentrated on either designing the best feature representation or learning optimal matching metrics in a static setting where the number of cameras are fixed in a network. Most approaches have neglected the dynamic and open world nature of the re-identification problem, where one or multiple new cameras may be temporarily on-boarded into an existing system to get additional information or added to expand an existing network. To address such a very practical problem, we propose a novel approach for adapting existing multi-camera re-identification frameworks with limited supervision. First, we formulate a domain perceptive re-identification method based on geodesic flow kernel that can effectively find the best source camera (already installed) to adapt with newly introduced target camera(s), without requiring a very expensive training phase. Second, we introduce a transitive inference algorithm for re-identification that can exploit the information from best source camera to improve the accuracy across other camera pairs in a network of multiple cameras. Third, we develop a target-aware sparse prototype selection strategy for finding an informative subset of source camera data for data-efficient learning in resource constrained environments. Our approach can greatly increase the flexibility and reduce the deployment cost of new cameras in many real-world dynamic camera networks. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art unsupervised alternatives whilst being extremely efficient to compute.","Person re-identification, Camera network, Model adaptation, Limited supervision, Camera on-boarding,",Rameswar Panda and Amran Bhuiyan and Vittorio Murino and Amit K. Roy-Chowdhury,https://www.sciencedirect.com/science/article/pii/S0031320319302948,https://doi.org/10.1016/j.patcog.2019.106991,0031-3203,2019,106991,96,Pattern Recognition,Adaptation of person re-identification models for on-boarding new camera(s),article,PANDA2019106991,
"In this paper, we propose a cluster-wise feature aggregation network that exploits multi-level contextual association for multi-person pose estimation. The recent popular approach for pose estimation is extracting the local maximum response from each detection heatmap that trained for a specific keypoint type. To exploit more contextual information, our network simultaneously learns complementary semantic information to encourage the detected keypoints subject to a certain contextual constraint. Specifically, our network uses dense and sparse branches to generate paired multi-peak detection heatmaps for clusters of keypoints. To enhance the feature passing through the network, we aggregate information from different branches. The in-branch aggregation enriches the detection features in each branch by absorbing the holistic human region attention. The cross-branch aggregation further strengthens the detection features by fusing global and local context information between dense and sparse branches. We demonstrate competitive performance of our network on the benchmark dataset for multi-person pose estimation.","Pose estimation, Keypoint detection, Deep learning",Ying Zhao and Zhiwei Luo and Changqin Quan and Dianchao Liu and Gang Wang,https://www.sciencedirect.com/science/article/pii/S0031320319303759,https://doi.org/10.1016/j.patcog.2019.107074,0031-3203,2020,107074,98,Pattern Recognition,Cluster-wise learning network for multi-person pose estimation,article,ZHAO2020107074,
"In underwater scenes, wavelength-dependent light absorption and scattering degrade the visibility of images and videos. The degraded underwater images and videos affect the accuracy of pattern recognition, visual understanding, and key feature extraction in underwater scenes. In this paper, we propose an underwater image enhancement convolutional neural network (CNN) model based on underwater scene prior, called UWCNN. Instead of estimating the parameters of underwater imaging model, the proposed UWCNN model directly reconstructs the clear latent underwater image, which benefits from the underwater scene prior which can be used to synthesize underwater image training data. Besides, based on the light-weight network structure and effective training data, our UWCNN model can be easily extended to underwater videos for frame-by-frame enhancement. Specifically, combining an underwater imaging physical model with optical properties of underwater scenes, we first synthesize underwater image degradation datasets which cover a diverse set of water types and degradation levels. Then, a light-weight CNN model is designed for enhancing each underwater scene type, which is trained by the corresponding training data. At last, this UWCNN model is directly extended to underwater video enhancement. Experiments on real-world and synthetic underwater images and videos demonstrate that our method generalizes well to different underwater scenes.","Underwater image and video enhancement and restoration, Underwater image synthesis, Pattern recognition, Deep learning",Chongyi Li and Saeed Anwar and Fatih Porikli,https://www.sciencedirect.com/science/article/pii/S0031320319303401,https://doi.org/10.1016/j.patcog.2019.107038,0031-3203,2020,107038,98,Pattern Recognition,Underwater scene prior inspired deep underwater image and video enhancement,article,LI2020107038,
"Cross-modal hashing methods have drawn considerable attention due to the rapid growth of multi-modal data. To obtain efficient binary codes in a low-dimensional Hamming space, most existing approaches relaxed the discrete constraint, which could cause quantization loss and even result in performance degradation. In order to avoid this bottleneck, some scholars employed iterative discrete cyclic coordinate descent (DCC) to learn hash codes bit by bit, but this was very time-consuming. To counter this problem, a simple yet novel supervised discrete cross-modal hashing framework is represented to directly learn the unified discrete binary codes with a close-form, rather than bit by bit. Furthermore, to preserve label separability, the kernel discriminant analysis is fused into the proposed framework to enrich the discrimination ability of the learned binary codes. The goal of the proposed method is to obtain the common discrete binary codes of different modalities in a shared latent Hamming space so that the different modalities of a sample can be effectively connected. Experimental study shows the encouraging results of the proposed algorithm in comparisons to the state-of-the-art baseline approaches on four real-world datasets. Especially on the LabelMe dataset, the superiority of the proposed method is obvious, with an average improvement of 9% over the best available results.","Cross-modal hashing, Discrete, Kernel discriminant analysis",Yixian Fang and Yuwei Ren,https://www.sciencedirect.com/science/article/pii/S0031320319303644,https://doi.org/10.1016/j.patcog.2019.107062,0031-3203,2020,107062,98,Pattern Recognition,Supervised discrete cross-modal hashing based on kernel discriminant analysis,article,FANG2020107062,
"Aiming at the poor performance of individual classifier in the field of fault recognition, in this paper, a new ensemble classifier is constructed to improve the classification accuracy by combining multiple classifiers based on DempsterâShafer Theory (DST). However, in some specific cases, especially when dealing with the combination of conflicting evidences, the DST may produce counter-intuitive results and loss its advantages in combining classifiers. To solve this problem, a new improved combination method is proposed to alleviate the conflicts between evidences and a new ensemble technique is developed for the combination of individual classifiers, which can be well used in the design of accurate classifier ensembles. The main advantage of the proposed combination method is that of making the combination process more efficient and accurate by defining the objective weights and subjective weights of member classifiersâ outputs. To verify the effectiveness of the proposed combination method, four individual classifiers are selected for constructing ensemble classifier and tested on Tennessee-Eastman Process (TEP) datasets and UCI machine learning datasets. The experimental results show that the ensemble classifier can significantly improve the classification accuracy and outperforms all the selected individual classifiers. By comparison with other combination methods based on DST and some state-of-the-art ensemble methods, the proposed combination method shows better abilities in dealing with the combination of individual classifiers and outperforms the others in multiple performance measurements. Finally, the proposed ensemble classifier is applied to the fault recognition in real chemical plant.","Fault recognition, Ensemble classifier, DempsterâShafer Theory, Correlation entropy, Evidence weight",Zhen Wang and Rongxi Wang and Jianmin Gao and Zhiyong Gao and Yanjie Liang,https://www.sciencedirect.com/science/article/pii/S0031320319303802,https://doi.org/10.1016/j.patcog.2019.107079,0031-3203,2020,107079,99,Pattern Recognition,Fault recognition using an ensemble classifier based on DempsterâShafer Theory,article,WANG2020107079,
"Ensemble selection is one of the most studied topics in ensemble learning because a selected subset of base classifiers may perform better than the whole ensemble system. In recent years, a great many ensemble selection methods have been introduced. However, many of these lack flexibility: either a fixed subset of classifiers is pre-selected for all test samples (static approach), or the selection of classifiers depends upon the performance of techniques that define the region of competence (dynamic approach). In this paper, we propose an ensemble selection method that takes into account each base classifier's confidence during classification and the overall credibility of the base classifier in the ensemble. In other words, a base classifier is selected to predict for a test sample if the confidence in its prediction is higher than its credibility threshold. The credibility thresholds of the base classifiers are found by minimizing the empirical 0â1 loss on the entire training observations. In this way, our approach integrates both the static and dynamic aspects of ensemble selection. Experiments on 62 datasets demonstrate that the proposed method achieves much better performance in comparison to some ensemble methods.","Ensemble method, Multiple classifier system, Ensemble selection, Classifier selection, Artificial bee colony",Tien Thanh Nguyen and Anh Vu Luong and Manh Truong Dang and Alan Wee-Chung Liew and John McCall,https://www.sciencedirect.com/science/article/pii/S0031320319304054,https://doi.org/10.1016/j.patcog.2019.107104,0031-3203,2020,107104,100,Pattern Recognition,Ensemble Selection based on Classifier Prediction Confidence,article,NGUYEN2020107104,
"Image captioning which automatically generates natural language descriptions for images has attracted lots of research attentions and there have been substantial progresses with attention based captioning methods. However, most attention-based image captioning methods focus on extracting visual information in regions of interest for sentence generation and usually ignore the relational reasoning among those regions of interest in an image. Moreover, these methods do not take into account previously attended regions which can be used to guide the subsequent attention selection. In this paper, we propose a novel method to implicitly model the relationship among regions of interest in an image with a graph neural network, as well as a novel context-aware attention mechanism to guide attention selection by fully memorizing previously attended visual content. Compared with the existing attention-based image captioning methods, ours can not only learn relation-aware visual representations for image captioning, but also consider historical context information on previous attention. We perform extensive experiments on two public benchmark datasets: MS COCO and Flickr30K, and the experimental results indicate that our proposed method is able to outperform various state-of-the-art methods in terms of the widely used evaluation metrics.","Image captioning, Relational reasoning, Context-aware attention",Junbo Wang and Wei Wang and Liang Wang and Zhiyong Wang and David Dagan Feng and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S0031320319303760,https://doi.org/10.1016/j.patcog.2019.107075,0031-3203,2020,107075,98,Pattern Recognition,Learning visual relationship and context-aware attention for image captioning,article,WANG2020107075,
"Slice sampling provides an automatical adjustment to match the characteristics of the distribution. Although this method has made great success in many situations, it becomes limited when the distribution is complex. Inspired by HigdonÂ [1], in this paper, we present a decomposed sampling framework based on slice sampling called decomposed slice sampling (DSS). We suppose that the target distribution can be divided into two multipliers so that information in each term can be used, respectively. The first multiplier is used in the first step of DSS to obtain horizontal slices and the last term is used in the second step. Simulations on four simple distributions indicate the effectiveness of our method. Compared with slice sampling and Hamiltonian Monte Carlo on Gaussian distributions in different dimensions and ten real-world datasets, the proposed method achieves better performance.","Slice sampling, Markov chain Monte Carlo, Decomposed slice sampling, Hamiltonian Monte Carlo",Jiachun Wang and Shiliang Sun,https://www.sciencedirect.com/science/article/pii/S0031320319303243,https://doi.org/10.1016/j.patcog.2019.107021,0031-3203,2020,107021,97,Pattern Recognition,Decomposed slice sampling for factorized distributions,article,WANG2020107021,
"Person attributes are often exploited as mid-level human semantic information to help promote the performance of person re-identification task. In this paper, unlike most existing methods simply taking the attribute learning as a classification problem, we perform it in a different way with the motivation that attributes are related to specific local regions, which refers to the perceptual ability of attributes. We utilize the process of attribute detection to generate corresponding attribute-part detectors, whose invariance to many influences like poses and camera views can be guaranteed. With detected local part regions, our model extracts local part features to handle the body part misalignment problem, which is another major challenge for person re-identification. The local descriptors are further refined by fused attribute information to eliminate interferences caused by detection deviation. Finally, the refined local feature works together with a holistic-level feature to constitute our final feature representation. Extensive experiments on two popular benchmarks with attribute annotations demonstrate the effectiveness of our model and competitive performance compared with state-of-the-art algorithms.","Person re-identification, Attribute detection, Part detection, Deep neural networks",Shuzhao Li and Huimin Yu and Roland Hu,https://www.sciencedirect.com/science/article/pii/S003132031930319X,https://doi.org/10.1016/j.patcog.2019.107016,0031-3203,2020,107016,97,Pattern Recognition,Attributes-aided part detection and refinement for person re-identification,article,LI2020107016,
"Segmentation of brain MR volumes into different meaningful tissue classes is an essential prerequisite for many clinical analyses. However, intensity inhomogeneity or bias field, present in MR volumes, considerably degrades the quality of segmentation. In this regard, the paper presents a new segmentation algorithm, termed as CoLoRS (Coherent Local Intensity Rough Segmentation), for brain MR volumes corrupted with bias field artifact. It judiciously integrates the merits of coherent local intensity clustering and the theory of rough sets for simultaneous segmentation and bias field correction of brain MR volumes. The proposed algorithm partitions the entire image space into a number of small overlapping neighborhood regions. The bias, in each neighborhood region, is assumed to be constant. For each individual region, an objective function is defined for coherent local intensity rough segmentation. The voxels near the center point have similar influences on local objective function. In addition, the smaller distance between center and neighboring voxels yields more contribution on the voxel of interest. The proposed algorithm uses the dual-region concept to represent the neighborhood structure more efficiently. It makes possible of separate modeling of the voxels within neighborhood, according to their locations. Each region is considered to have several tissue classes, where each tissue class consists of a core region and an overlapping region. The segmentation in fuzzy approximation spaces provides an effective mean for brain MR volume analysis, as it handles overlapping partitions and addresses vagueness in tissue class definition. The effectiveness of the proposed algorithm, along with a comparison with existing approaches, is demonstrated on several publicly available brain MR data.","Magnetic resonance image, Segmentation, Intensity inhomogeneity, Rough sets, Coherent local intensity clustering",Shaswati Roy and Pradipta Maji,https://www.sciencedirect.com/science/article/pii/S0031320319303000,https://doi.org/10.1016/j.patcog.2019.106997,0031-3203,2020,106997,97,Pattern Recognition,Rough segmentation of coherent local intensity for bias induced 3-D MR brain images,article,ROY2020106997,
"Feature fusion is an important skill to improve the performance in computer vision, the difficult problem of feature fusion is how to learn the complementary properties of different features. We recognize that feature fusion can benefit from kernel metric learning. Thus, a metric learning-based kernel transformer method for feature fusion is proposed in this paper. First, we propose a kernel transformer to convert data from data space to kernel space, which makes feature fusion and metric learning can be performed in the transformed kernel space. Second, in order to realize supervised learning, both triplets and label constraints are embedded into our model. Third, in order to solve the unknown kernel matrices, LogDet divergence is also introduced into our model. Finally, a complete optimization objective function is formed. Based on an alternating direction method of multipliers (ADMM) solver and the Karush-Kuhn-Tucker (KKT) theorem, the proposed optimization problem is solved with the rigorous theoretical analysis. Experimental results on image retrieval demonstrate the effectiveness of the proposed methods.","Feature fusion, Kernel transformer, Kernel metric learning, LogDet divergence",Shichao Kan and Linna Zhang and Zhihai He and Yigang Cen and Shiming Chen and Jikun Zhou,https://www.sciencedirect.com/science/article/pii/S0031320319303875,https://doi.org/10.1016/j.patcog.2019.107086,0031-3203,2020,107086,99,Pattern Recognition,Metric learning-based kernel transformer with triplets and label constraints for feature fusion,article,KAN2020107086,
"Despite the impressive progress in face recognition, current systems are vulnerable to presentation attacks, which subvert the face recognition systems by presenting a face artifact. Several techniques have been developed to automatically detect different presentation attacks, mostly for 2D photo print and video replay attacks. However, with the development of 3D modeling and printing technologies, 3D mask has become a more effective way to attack the face recognition systems. Over the last decade, various detection methods for 3D mask attacks have been proposed, but there is no survey yet to summarize the advances. We present a comprehensive overview of the state-of-the-art approaches in 3D mask spoofing and anti-spoofing, including existing databases and countermeasures. In addition, we quantitatively compare the performance of different mask spoofing detection methods on a common ground (i.e., using the same database and evaluation metric). The effectiveness of several 2D presentation attack detection methods is also evaluated on two 3D mask spoofing databases to show whether they are applicable or not for 3D mask attacks. Finally, we present some insights and summarize open issues to address in the future.","Face presentation attack, 3D Mask spoofing, Biometrics",Shan Jia and Guodong Guo and Zhengquan Xu,https://www.sciencedirect.com/science/article/pii/S0031320319303358,https://doi.org/10.1016/j.patcog.2019.107032,0031-3203,2020,107032,98,Pattern Recognition,A survey on 3D mask presentation attack detection and countermeasures,article,JIA2020107032,
"Plane detection is a key component for many applications, such as industrial reverse engineering and self-driving cars. However, existing plane-detection techniques are sensitive to noise and to user-defined parameters. We introduce a fast deterministic technique for plane detection in unorganized point clouds that is robust to noise and virtually independent of parameter tuning. It is based on a novel planarity test drawn from robust statistics and on a split and merge strategy. Its parameter values are automatically adjusted to fit the local distribution of samples in the input dataset, thus leading to good reconstruction of even small planar regions. We demonstrate the effectiveness of our solution on several real datasets, comparing its performance to state-of-art plane detection techniques, and showing that it achieves better accuracy, while still being one of the fastest.","Plane detection, Region growing, Robust statistics, Unorganized point clouds,",Abner M.â¯C. AraÃºjo and Manuel M. Oliveira,https://www.sciencedirect.com/science/article/pii/S0031320319304169,https://doi.org/10.1016/j.patcog.2019.107115,0031-3203,2020,107115,100,Pattern Recognition,A robust statistics approach for plane detection in unorganized point clouds,article,ARAUJO2020107115,
"One of the challenges in cross-age face recognition and verification is to effectively model the facial aging process. Despite the rapid advances in face-related areas, it is still very difficult for existing methods to simultaneously achieve accurate facial feature preservation and reliable aging during aging modeling. Aiming to address this issue, we introduce a Disentangled Representation learning and Residual Generative Adversarial Network (DR-RGAN) that represents the facial features without age interference, which is achieved by explicitly disentangling the facial features and age variation. An encoder-decoder structured generator produces aging images from unstructured facial representations by using the age characteristics provided separately. It can thus take the disentangled representation to preserve personal identity for face verification. Considering that pixel-based errors may cause a loss of detail, a VGG based content loss is further equipped to preferably preserve facial features. The discriminator is trained to distinguish the real from generated faces, carry out identification prediction, and leverage an age estimator to boost the aging accuracy. It is beneficial for obtaining more photorealistic and desirable aging effects, as well as more consistent face verification results. Extensive experiments on the CACD and LFW datasets demonstrate that our DR-RGAN generates pleasing aging imageries and achieves a high accuracy of face verification.","Face recognition, Age-invariant face verification, Representation learning, Generative adversarial network",Shuyang Zhao and Jianwu Li and Jiaxing Wang,https://www.sciencedirect.com/science/article/pii/S003132031930398X,https://doi.org/10.1016/j.patcog.2019.107097,0031-3203,2020,107097,100,Pattern Recognition,Disentangled representation learning and residual GAN for age-invariant face verification,article,ZHAO2020107097,
"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch. When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task. However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task. In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model. We show the benefit of having an explicit inductive bias towards the initial model. We eventually recommend that the baseline protocol for transfer learning should rely on a simple L2 penalty using the pre-trained model as a reference.","Transfer learning, Regularization, Convolutional networks",Xuhong Li and Yves Grandvalet and Franck Davoine,https://www.sciencedirect.com/science/article/pii/S0031320319303516,https://doi.org/10.1016/j.patcog.2019.107049,0031-3203,2020,107049,98,Pattern Recognition,A baseline regularization scheme for transfer learning with convolutional neural networks,article,LI2020107049,
"Many fields of science and engineering rely on running simulations with complex and computationally expensive models to understand the involved processes in the system of interest. Nevertheless, the high cost involved hamper reliable and exhaustive simulations. Very often such codes incorporate heuristics that ironically make them less tractable and transparent. This paper introduces an active learning methodology for adaptively constructing surrogate models, i.e. emulators, of such costly computer codes in a multi-output setting. The proposed technique is sequential and adaptive, and is based on the optimization of a suitable acquisition function. It aims to achieve accurate approximations, model tractability, as well as compact and expressive simulated datasets. In order to achieve this, the proposed Active Multi-Output Gaussian Process Emulator (AMOGAPE) combines the predictive capacity of Gaussian Processes (GPs) with the design of an acquisition function that favors sampling in low density and fluctuating regions of the approximation functions. Comparing different acquisition functions, we illustrate the promising performance of the method for the construction of emulators with toy examples, as well as for a widely used remote sensing transfer code.","Active learning, Gaussian process, Emulation, Design of experiments, Computer code, Remote sensing, Radiative transfer model",Daniel Heestermans Svendsen and Luca Martino and Gustau Camps-Valls,https://www.sciencedirect.com/science/article/pii/S0031320319304042,https://doi.org/10.1016/j.patcog.2019.107103,0031-3203,2020,107103,100,Pattern Recognition,Active emulation of computer codes with Gaussian processes â Application to remote sensing,article,SVENDSEN2020107103,
"Micro three-dimensional (3D) textured surfaces are being designed for a lot of electronic products to improve appearance and user experience. Defects are, however, inevitably caused during industrial manufacture. They are difficult to be detected due to low contrast and unclear boundary between defect and irregular textured defect-free region. To achieve robust defect detection on micro 3D textured surfaces of industrial products, this paper proposes a probabilistic saliency framework with a novel feature enhancement mechanism. Two saliency features, absolute intensity deviation and local intensity aggregation, are designed to represent the pixel-level initial saliency. Based on these two features, an iterative framework, named accumulated and aggregated shifting of intensity (AASI), is proposed to shift the intensity of each pixel according to its saliency. Finally, all the pixels are classified as defective or defect-free by fitting the AASI iteration results to two statistical models, an exponential model and a linear model. Importantly, AASI procedure is unsupervised and training-free, so it does not rely on huge training data with time-consuming manual labels. Experimental results on a large-scale image dataset taken from real-world industrial product surfaces demonstrate that the proposed approach achieves state-of-the-art accuracy in industrial applications.","Defect detection, Accumulated and aggregated shifting of intensity (AASI) procedure, Saliency description, Illumination invariance",Yaping Yan and Shunâichi Kaneko and Hirokazu Asano,https://www.sciencedirect.com/science/article/pii/S0031320319303590,https://doi.org/10.1016/j.patcog.2019.107057,0031-3203,2020,107057,98,Pattern Recognition,Accumulated and aggregated shifting of intensity for defect detection on micro 3D textured surfaces,article,YAN2020107057,
"Traditional anomaly detection procedures assume that normal observations are obtained from a single distribution. However, due to the complexities of modern industrial processes, the observations may belong to multiple operating modes with different distributions. In such cases, traditional anomaly detection procedures may trigger false alarms while the process is indeed in another normally operating mode. We propose a generalized support vector-based anomaly detection procedure called generalized support vector data description which can be used to determine the anomalies in multimodal processes. The proposed procedure constructs hyperspheres for each class in order to include as many observations as possible and keep other class observations as far apart as possible. In addition, we introduce a generalized Bayesian framework which does not only consider the prior information from each mode, but also highlights the relationships among the modes. The effectiveness of the proposed procedure is demonstrated through various simulation studies and real-life applications.","Anomaly detection, Bayesian statistics, Multimode process, Support vector data description",Mehmet Turkoz and Sangahn Kim and Youngdoo Son and Myong K. Jeong and Elsayed A. Elsayed,https://www.sciencedirect.com/science/article/pii/S0031320319304200,https://doi.org/10.1016/j.patcog.2019.107119,0031-3203,2020,107119,100,Pattern Recognition,Generalized support vector data description for anomaly detection,article,TURKOZ2020107119,
"Due to the development of deep learning networks and big data dimensionality, research on ensemble deep learning is receiving an increasing amount of attention. This paper takes the object detection task as the research domain and proposes an object detection framework based on ensemble deep learning. To guarantee the accuracy as well as real-time detection, the detector uses a Single Shot MultiBox Detector (SSD) as the backbone and combines ensemble learning with context modeling and multi-scale feature representation. Two modes were designed in order to achieve ensemble learning: NMS Ensembling and Feature Ensembling. In addition, to obtain contextual information, we used dilated convolution to expand the receptive field of the network. Compared with state-of-the-art detectors, our detector achieves superior performance on the PASCAL VOC set and the MS COCO set.","Ensemble learning, Object detection, Dilated convolution, Feature fusion",Jie Xu and Wei Wang and Hanyuan Wang and Jinhong Guo,https://www.sciencedirect.com/science/article/pii/S0031320319303991,https://doi.org/10.1016/j.patcog.2019.107098,0031-3203,2020,107098,99,Pattern Recognition,Multi-model ensemble with rich spatial information for object detection,article,XU2020107098,
"The descriptiveness of visual models is crucial for many image processing applications, however, it is still challenging to adaptively formulate such models. This paper systematically advocates a generic and adaptive appearance modeling method. For object-specific instances in images, it can adaptively generate a descriptive codebook by exploring the maximum discriminability of multi-type features. The key idea is to define feature-independent information entropy as a unified criterion to measure different features in a common entropy space. Towards this goal, a hierarchical maximum entropy (HME) model is proposed to conduct multi-feature selection based on the random forest. Specifically, the improved random forest replaces space-specific expression âdistance similarityâ with the statistical concept âentropyâ. Thus, the random forest could integrate the subspace clustering results from different feature spaces. Such integration can not only afford adaptive feature selection and cross-feature error control but also be robust to possible weak/inconsistent feature expressions. To effectively construct a class-specific appearance model, a sparse codebook model, consisting of a series of weak learners, is proposed to further explore the maximum discriminative subspaces of each object class. Finally, a maximum entropy model is proposed to formulate appearance model by optimizing the probabilistic distributions of all the codebook wordsâ response parameters. To verify the efficacy and effectiveness of the proposed model, it is applied to multi-class image classification. We conduct extensive experiments and make comprehensive evaluations w.r.t several state-of-the-art methods over PASCAL VOC 2007, VOC 2012, Caltech 101 and Caltech 256 datasets. All the results demonstrate the advantages of the our method in terms of precision, robustness, flexibility, and versatility.","Description model, Adaptive feature selection, Random forest, Hierarchical maximum entropy model, Image classification",Jizhou Ma and Shuai Li and Hong Qin and Aimin Hao,https://www.sciencedirect.com/science/article/pii/S0031320319303619,https://doi.org/10.1016/j.patcog.2019.107059,0031-3203,2020,107059,98,Pattern Recognition,Adaptive appearance modeling via hierarchical entropy analysis over multi-type features,article,MA2020107059,
"Linear discriminant analysis (LDA) is an important conventional model for data classification. Classical theory shows that LDA is Bayes consistent for a fixed data dimensionality p and a large training sample size n. However, in high-dimensional settings when pâ¯â«â¯n, LDA is difficult due to the inconsistent estimation of the covariance matrix and the mean vectors of populations. Recently, a linear programming discriminant (LPD) rule was proposed for high-dimensional linear discriminant analysis, based on the sparsity assumption over the discriminant function. It is shown that the LPD rule is Bayes consistent in high-dimensional settings. In this paper, we further show that the LPD rule is sign consistent under the sparsity assumption. Such sign consistency ensures the LPD rule to select the optimal discriminative features for high-dimensional data classification problems. Evaluations on both synthetic and real data validate our result on the sign consistency of the LPD rule.","High-dimensional linear discriminant analysis, Sign consistency, Irrepresentability condition, Linear programming",Zhen Zhang and Shengzheng Wang and Wei Bian,https://www.sciencedirect.com/science/article/pii/S003132031930384X,https://doi.org/10.1016/j.patcog.2019.107083,0031-3203,2020,107083,100,Pattern Recognition,Sign consistency for the linear programming discriminant rule,article,ZHANG2020107083,
"In this paper we raise two important question, â1. Is temporal information beneficial in recognizing actions from still images? 2. Do we know how to take the maximum advantage from them?â. To answer these question we propose a novel transfer learning problem, Temporal To Still Image Learning (i.e., T2SIL) where we learn to derive temporal information from still images. Thereafter, we use a two-stream model where still image action predictions are fused with derived temporal predictions. In T2SIL, the knowledge transferring occurs from temporal representations of videos (e.g., Optical-flow, Dynamic Image representations) to still action images. Along with the T2SIL we propose a new action still image action dataset and a video dataset sharing the same set of classes. We explore three well established transfer learning frameworks (i.e., GANs, Embedding learning and Teacher Student Networks (TSNs)) in place of the temporal knowledge transfer method. The use of derived temporal information from our TSN and Embedding learning improves still image action recognition.","Still image action recognition, Two-stream, Optical-flow, Dynamic-images",Samitha Herath and Basura Fernando and Mehrtash Harandi,https://www.sciencedirect.com/science/article/pii/S0031320319302924,https://doi.org/10.1016/j.patcog.2019.106989,0031-3203,2019,106989,96,Pattern Recognition,Using temporal information for recognizing actions from still images,article,HERATH2019106989,
"Cross-modal image generation is an important aspect of the multi-modal learning. Existing methods usually use the semantic feature to reduce the modality gap. Although these methods have achieved notable progress, there are still some limitations: (1) they usually use single modality information to learn the semantic feature; (2) they require the training data to be paired. To overcome these problems, we propose a novel semi-supervised cross-modal image generation method, which consists of two semantic networks and one image generation network. Specifically, in the semantic networks, we use image modality to assist non-image modality for semantic feature learning by using a deep mutual learning strategy. In the image generation network, we introduce an additional discriminator to reduce the image reconstruction loss. By leveraging large amounts of unpaired data, our method can be trained in a semi-supervised manner. Extensive experiments demonstrate the effectiveness of the proposed method.","Multi-modality, Semi-supervised learning, Semantic networks, Generative adversarial networks, Multi-label learning",Dan Li and Changde Du and Huiguang He,https://www.sciencedirect.com/science/article/pii/S0031320319303863,https://doi.org/10.1016/j.patcog.2019.107085,0031-3203,2020,107085,100,Pattern Recognition,Semi-supervised cross-modal image generation with generative adversarial networks,article,LI2020107085,
"The contaminated Gaussian distribution represents a simple heavy-tailed elliptical generalization of the Gaussian distribution; unlike the often-considered t-distribution, it also allows for automatic detection of mild outlying or âbadâ points in the same way that observations are typically assigned to the groups in the finite mixture model context. Starting from this distribution, we propose the contaminated factor analysis model as a method for dimensionality reduction and detection of bad points in higher dimensions. A mixture of contaminated Gaussian factor analyzers (MCGFA) model follows therefrom, and extends the recently proposed mixture of contaminated Gaussian distributions to high-dimensional data. We introduce a family of 32 parsimonious models formed by introducing constraints on the covariance and contamination structures of the general MCGFA model. We outline a variant of the expectation-maximization algorithm for parameter estimation. Various implementation issues are discussed, and the novel family of models is compared to well-established approaches on both simulated and real data.","EM algorithm, Factor analysis, Mixture models, Model-based clustering, Heavy-tailed distributions",Antonio Punzo and Martin Blostein and Paul D. McNicholas,https://www.sciencedirect.com/science/article/pii/S0031320319303346,https://doi.org/10.1016/j.patcog.2019.107031,0031-3203,2020,107031,98,Pattern Recognition,High-dimensional unsupervised classification via parsimonious contaminated mixtures,article,PUNZO2020107031,
"In this paper, we propose a novel effective non-rigid object tracking framework based on the spatial-temporal consistent saliency detection. In contrast to most existing trackers that utilize a bounding box to specify the tracked target, the proposed framework can extract accurate regions of the target as tracking outputs. It achieves a better description of the non-rigid objects and reduces the background pollution for the tracking model. Furthermore, our model has several unique characteristics. First, a tailored fully convolutional neural network (TFCN) is developed to model the local saliency prior for a given image region, which not only provides the pixel-wise outputs but also integrates the semantic information. Second, a novel multi-scale multi-region mechanism is proposed to generate local saliency maps that effectively consider visual perceptions with different spatial layouts and scale variations. Subsequently, the local saliency maps are fused via a weighted entropy method, resulting in a discriminative saliency map. Finally, we present a non-rigid object tracking algorithm based on the predicted saliency maps. By utilizing a spatial-temporal consistent saliency map (STCSM), we conduct the target-background classification and use an online fine-tuning scheme for model updating. Extensive experiments demonstrate that the proposed algorithm achieves competitive performance in both saliency detection and visual tracking, especially outperforming other related trackers on the non-rigid object tracking datasets. Source codes and compared results are released at https://github.com/Pchank/TFCNTracker.","Deep neural network, Non-rigid object tracking, Salient object detection, Spatial-temporal consistency",Pingping Zhang and Wei Liu and Dong Wang and Yinjie Lei and Hongyu Wang and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320319304315,https://doi.org/10.1016/j.patcog.2019.107130,0031-3203,2020,107130,100,Pattern Recognition,Non-rigid object tracking via deep multi-scale spatial-temporal discriminative saliency maps,article,ZHANG2020107130,
"We propose a method based on recursive binary Voronoi trees to learn a nonparametric model of the distribution underlying a given dataset. The obtained model can be used as a general tool both to extract good samples from the original dataset (e.g., for batch selection, bagging, or sample size reduction) or to generate new synthetic ones, also in a conditional fashion (e.g., to deal with imbalanced sets or to reconstruct corrupted points). In order to ensure that the distribution of the new sets, either sampled or generated, follows closely that of the original dataset, we design all the procedures according to a specific measure of distance between distributions. The use of binary recursive Voronoi structures enables the proposed algorithms to be simple, efficient and able to adapt to the shape of the original dataset. Simulation tests showcase the good performance and flexibility of the approach in various learning contexts.","Voronoi tree models, Sampling, Generative models, Density estimation, Noparametric models",Cristiano Cervellera and Danilo MacciÃ²,https://www.sciencedirect.com/science/article/pii/S003132031930305X,https://doi.org/10.1016/j.patcog.2019.107002,0031-3203,2020,107002,97,Pattern Recognition,Voronoi tree models for distribution-preserving sampling and generation,article,CERVELLERA2020107002,
"Although Faster R-CNN based text detection approaches have achieved promising results, their localization accuracy is not satisfactory in certain cases due to their sub-optimal bounding box regression based localization modules. In this paper, we address this problem and propose replacing the bounding box regression module with a novel LocNet based localization module to improve the localization accuracy of a Faster R-CNN based text detector. Given a proposal generated by a region proposal network (RPN), instead of directly predicting the bounding box coordinates of the concerned text instance, the proposal is enlarged to create a search region so that an âIn-Outâ conditional probability to each row and column of this search region is assigned, which can then be used to accurately infer the concerned bounding box. Furthermore, we present a simple yet effective two-stage approach to convert the difficult multi-oriented text detection problem to a relatively easier horizontal text detection problem, which makes our approach able to robustly detect multi-oriented text instances with accurate bounding box localization. Experiments demonstrate that the proposed approach boosts the localization accuracy of Faster R-CNN based text detectors significantly. Consequently, our new text detector has achieved superior performance on both horizontal (ICDAR-2011, ICDAR-2013 and MULTILIGUL) and multi-oriented (MSRA-TD500, ICDAR-2015) text detection benchmark tasks.","Text detection, Text localization accuracy, Faster R-CNN, LocNet, Natural scene images",Zhuoyao Zhong and Lei Sun and Qiang Huo,https://www.sciencedirect.com/science/article/pii/S0031320319302894,https://doi.org/10.1016/j.patcog.2019.106986,0031-3203,2019,106986,96,Pattern Recognition,Improved localization accuracy by LocNet for Faster R-CNN based text detection in natural scene images,article,ZHONG2019106986,
"Face completion aims to generate semantically new pixels for missing facial components. It is a challenging generative task due to large variations of face appearance. This paper studies generative face completion under structured occlusions. We treat the face completion and corruption as disentangling and fusing processes of clean faces and occlusions, and propose a jointly disentangling and fusing Generative Adversarial Network (DF-GAN). First, three domains are constructed, corresponding to the distributions of occluded faces, clean faces and structured occlusions. The disentangling and fusing processes are formulated as the transformations between the three domains. Then the disentangling and fusing networks are built to learn the transformations from unpaired data, where the encoder-decoder structure is adopted and allows DF-GAN to simulate structure occlusions by modifying the latent representations. Finally, the disentangling and fusing processes are unified into a dual learning framework along with an adversarial strategy. The proposed method is evaluated on Meshface verification problem. Experimental results on four Meshface databases demonstrate the effectiveness of our proposed method for the face completion under structured occlusions.",,Zhihang Li and Yibo Hu and Ran He and Zhenan Sun,https://www.sciencedirect.com/science/article/pii/S0031320319303747,https://doi.org/10.1016/j.patcog.2019.107073,0031-3203,2020,107073,99,Pattern Recognition,Learning disentangling and fusing networks for face completion under structured occlusions,article,LI2020107073,
"Existing logo detection methods usually consider a small number of logo classes, limited images per class and assume fine-gained object bounding box annotations. This limits their scalability to real-world dynamic applications. In this work, we tackle these challenges by exploring a web data learning principle without the need for exhaustive manual labelling. Specifically, we propose a novel incremental learning approach, called Scalable Logo Self-co-Learning (SL2), capable of automatically self-discovering informative training images from noisy web data for progressively improving model capability in a cross-model co-learning manner. Moreover, we introduce a very large (2,190,757 images of 194 logo classes) logo dataset âWebLogo-2Mâ by designing an automatic data collection and processing method. Extensive comparative evaluations demonstrate the superiority of SL2 over the state-of-the-art strongly and weakly supervised detection models and contemporary web data learning approaches.","Object detection, Logo recognition, Logo dataset, Web data mining, Self-Learning, Co-Learning",Hang Su and Shaogang Gong and Xiatian Zhu,https://www.sciencedirect.com/science/article/pii/S0031320319303061,https://doi.org/10.1016/j.patcog.2019.107003,0031-3203,2020,107003,97,Pattern Recognition,Scalable logo detection by self co-learning,article,SU2020107003,
"Recently, the hybrid convolutional neural network hidden Markov model (CNN-HMM) has been introduced for offline handwritten Chinese text recognition (HCTR) and has achieved state-of-the-art performance. However, modeling each of the large vocabulary of Chinese characters with a uniform and fixed number of hidden states requires high memory and computational costs and makes the tens of thousands of HMM state classes confusing. Another key issue of CNN-HMM for HCTR is the diversified writing style, which leads to model strain and a significant performance decline for specific writers. To address these issues, we propose a writer-aware CNN based on parsimonious HMM (WCNN-PHMM). First, PHMM is designed using a data-driven state-tying algorithm to greatly reduce the total number of HMM states, which not only yields a compact CNN by state sharing of the same or similar radicals among different Chinese characters but also improves the recognition accuracy due to the more accurate modeling of tied states and the lower confusion among them. Second, WCNN integrates each convolutional layer with one adaptive layer fed by a writer-dependent vector, namely, the writer code, to extract the irrelevant variability in writer information to improve recognition performance. The parameters of writer-adaptive layers are jointly optimized with other network parameters in the training stage, while a multiple-pass decoding strategy is adopted to learn the writer code and generate recognition results. Validated on the ICDAR 2013 competition of CASIA-HWDB database, the more compact WCNN-PHMM of a 7360-class vocabulary can achieve a relative character error rate (CER) reduction of 16.6% over the conventional CNN-HMM without considering language modeling. By adopting a powerful hybrid language model (N-gram language model and recurrent neural network language model), the CER of WCNN-PHMM is reduced to 3.17%. Moreover, the state-tying results of PHMM explicitly show the information sharing among similar characters and the confusion reduction of tied state classes. Finally, we visualize the learned writer codes and demonstrate the strong relationship with the writing styles of different writers. To the best of our knowledge, WCNN-PHMM yields the best results on the ICDAR 2013 competition set, demonstrating its power when enlarging the size of the character vocabulary.","Offline handwritten Chinese text recognition, Writer-aware CNN, Parsimonious HMM, State tying, Adaptation, Hybrid language model",Zi-Rui Wang and Jun Du and Jia-Ming Wang,https://www.sciencedirect.com/science/article/pii/S0031320319304030,https://doi.org/10.1016/j.patcog.2019.107102,0031-3203,2020,107102,100,Pattern Recognition,Writer-aware CNN for parsimonious HMM-based offline handwritten Chinese text recognition,article,WANG2020107102,
"Real data are often collected from multiple channels or comprised of different representations (i.e., views). Multi-view learning provides an elegant way to analyze the multi-view data for low-dimensional representation. In recent years, several multi-view learning methods have been designed and successfully applied in various tasks. However, existing multi-view learning methods usually work in a single layer formulation. Since the mapping between the obtained representation and the original data contains rather complex hierarchical information with implicit lower-level hidden attributes, it is desirable to fully explore the hidden structures hierarchically. In this paper, a novel deep multi-view clustering model is proposed by uncovering the hierarchical semantics of the input data in a layer-wise way. By utilizing a novel collaborative deep matrix decomposition framework, the hidden representations are learned with respect to different attributes. The proposed model is able to collaboratively learn the hierarchical semantics obtained by each layer. The instances from the same class are forced to be closer layer by layer in the low-dimensional space, which is beneficial for the subsequent clustering task. Furthermore, an idea weight is automatically assigned to each view without introducing extra hyperparameter as previous methods do. To solve the optimization problem of our model, an efficient iterative updating algorithm is proposed and its convergence is also guaranteed theoretically. Our empirical study on multi-view clustering task shows encouraging results of our model in comparison to the state-of-the-art algorithms.","Multi-view learning, Deep matrix decomposition, Clustering, Optimization algorithm",Shudong Huang and Zhao Kang and Zenglin Xu,https://www.sciencedirect.com/science/article/pii/S0031320319303188,https://doi.org/10.1016/j.patcog.2019.107015,0031-3203,2020,107015,97,Pattern Recognition,Auto-weighted multi-view clustering via deep matrix decomposition,article,HUANG2020107015,
"Facial landmark detection is a crucial prerequisite for many face analysis applications. Deep learning-based methods currently dominate the approach of addressing the facial landmark detection. However, such works generally introduce a large number of parameters, resulting in high memory cost. In this paper, we aim for a lightweight as well as effective solution to facial landmark detection. To this end, we propose an effective lightweight model, namely Mobile Face Alignment Network (MobileFAN), using a simple backbone MobileNetV2 as the encoder and three deconvolutional layers as the decoder. The proposed MobileFAN, with only 8% of the model size and lower computational cost, achieves superior or equivalent performance compared with state-of-the-art models. Moreover, by transferring the geometric structural information of a face graph from a large complex model to our proposed MobileFAN through feature-aligned distillation and feature-similarity distillation, the performance of MobileFAN is further improved in effectiveness and efficiency for face alignment. Extensive experiment results on three challenging facial landmark estimation benchmarks including COFW, 300W and WFLW show the superiority of our proposed MobileFAN against state-of-the-art methods.","Face alignment, Knowledge distillation, Lightweight model",Yang Zhao and Yifan Liu and Chunhua Shen and Yongsheng Gao and Shengwu Xiong,https://www.sciencedirect.com/science/article/pii/S0031320319304157,https://doi.org/10.1016/j.patcog.2019.107114,0031-3203,2020,107114,100,Pattern Recognition,MobileFAN: Transferring deep hidden representation for face alignment,article,ZHAO2020107114,
"Calibration and localisation of a camera sensor network is an essential requirement for higher-level computer vision tasks, such as mapping or tracking. Additionally, distributed algorithms are being increasingly used to create scalable networks robust to node failure. We propose a distributed calibration and localisation algorithm based on multi-view one-dimensional calibration, alternating direction method of multipliers, and Gaussian belief propagation. Our algorithm builds upon an existing calibration algorithm by improving the numerical conditioning and non-linear refinement. We adapt this to a distributed network, bringing local estimates at each camera node to global consensus. Simulation and experimental results show that our algorithm performs with high accuracy compared to other calibration techniques, in centralised and distributed networks, and is well suited for practical applications.","Multi-view calibration, Localisation, Distributed algorithms, Gaussian belief propagation, ADMM",Brendan Halloran and Prashan Premaratne and Peter James Vial,https://www.sciencedirect.com/science/article/pii/S0031320319303607,https://doi.org/10.1016/j.patcog.2019.107058,0031-3203,2020,107058,98,Pattern Recognition,Robust one-dimensional calibration and localisation of a distributed camera sensor network,article,HALLORAN2020107058,
"Gait recognition is one of the most important techniques for human identification at a distance. Most current gait recognition frameworks consist of several separate steps: silhouette segmentation, feature extraction, feature learning, and similarity measurement. These modules are mutually independent with each part fixed, resulting in a suboptimal performance in challenging conditions. In this paper, we integrate those steps into one framework, i.e., an end-to-end network for gait recognition, named GaitNet. It is composed of two convolutional neural networks: one corresponds to gait segmentation, and the other corresponds to classification. The two networks are modeled in one joint learning procedure which can be trained jointly. This strategy greatly simplifies the traditional step-by-step manner and is thus much more efficient for practical applications. Moreover, joint learning can automatically adjust each part to fit the global optimal objective, leading to obvious performance improvement over separate learning. We evaluate our method on three large scale gait datasets, including CASIA-B, SZU RGB-D Gait and a newly built database with complex dynamic outdoor backgrounds. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results. The code and data will be released upon request.","Gait recognition, Video-based human identification, End-to-end CNN, Joint learning",Chunfeng Song and Yongzhen Huang and Yan Huang and Ning Jia and Liang Wang,https://www.sciencedirect.com/science/article/pii/S0031320319302912,https://doi.org/10.1016/j.patcog.2019.106988,0031-3203,2019,106988,96,Pattern Recognition,GaitNet: An end-to-end network for gait based human identification,article,SONG2019106988,
"To exploit the complementary information of multi-view data, many weighted multi-view clustering methods have been proposed and have demonstrated impressive performance. However, most of these methods learn the view weights by introducing additional parameters, which can not be easily obtained in practice. Moreover, they all simply apply the learned weights on the original feature representation of each view, which may deteriorate the clustering performance in the case of high-dimensional data with redundancy and noise. In this paper, we extend information bottleneck co-clustering into a multi-view framework and propose a novel dynamic auto-weighted multi-view co-clustering algorithm to learn a group of weights for views with no need for extra weight parameters. By defining the new concept of the discrimination-compression rate, we quantify the importance of each view by evaluating the discriminativeness of the compact features (i.e., feature-wise clusters) of the views. Unlike existing weighted methods that impose weights on the original feature representations of multiple views, we apply the learned weights on the discriminative ones, which can reduce the negative impact of noisy features in high-dimensional data. To solve the optimization problem, a new two-step sequential method is designed. Experimental results on several datasets show the advantages of the proposed algorithm. To our knowledge, this is the first work incorporating weighting scheme into multi-view co-clustering framework.","Multi-view co-clustering, Information bottleneck, Weighting",Shizhe Hu and Xiaoqiang Yan and Yangdong Ye,https://www.sciencedirect.com/science/article/pii/S0031320319304029,https://doi.org/10.1016/j.patcog.2019.107101,0031-3203,2020,107101,99,Pattern Recognition,Dynamic auto-weighted multi-view co-clustering,article,HU2020107101,
"Support Vector Machines (SVMs) are vulnerable to integrity attacks, where malicious attackers distort the training data in order to compromise the decision boundary of the learned model. With increasing real-world applications of SVMs, malicious data that is classified as innocuous may have harmful consequences. This paper presents a novel framework that utilizes adversarial learning, nonlinear data projections, and game theory to improve the resilience of SVMs against such training-data-integrity attacks. The proposed approach introduces a layer of uncertainty through the use of random projections on top of the learners, making it challenging for the adversary to guess the specific configurations of the learners. To find appropriate projection directions, we introduce novel indices that ensure the contraction of the data and maximize the detection accuracy. Experiments with benchmark data sets show increases in detection rates up to 13.5% for OCSVMs and up to 14.1% for binary SVMs under different attack algorithms when compared with the respective base algorithms.","Support Vector Machines, Integrity attack",Sandamal Weerasinghe and Sarah M. Erfani and Tansu Alpcan and Christopher Leckie,https://www.sciencedirect.com/science/article/pii/S0031320319302882,https://doi.org/10.1016/j.patcog.2019.106985,0031-3203,2019,106985,96,Pattern Recognition,Support vector machines resilient against training data integrity attacks,article,WEERASINGHE2019106985,
"Recently, graph neural networks (GNNs) have shown to be effective in learning representative graph features. However, current pooling-based strategies for graph classification lack efficient utilization of graph representation information in which each node and layer have the same contribution to the output of graph-level representation. In this paper, we develop a novel architecture for extracting an effective graph representation by introducing structured multi-head self-attention in which the attention mechanism consists of three different forms, i.e., node-focused, layer-focused and graph-focused. In order to make full use of the information of graphs, the node-focused self-attention firstly aggregates neighbor node features with a scaled dot-product manner, and then the layer-focused and graph-focused self-attention serve as readout module to measure the importance of different nodes and layers to the modelâs output. Moreover, it is able to improve the performance on graph classification tasks by combining these two self-attention mechanisms with base node-level GNNs. The proposed Structured Self-attention Architecture is evaluated on two kinds of graph benchmarks: bioinformatics datasets and social network datasets. Extensive experiments have demonstrated superior performance improvement to existing methods on predictive accuracy.","Neural self-attention mechanism, Graph neural networks, Graph classification",Xiaolong Fan and Maoguo Gong and Yu Xie and Fenlong Jiang and Hao Li,https://www.sciencedirect.com/science/article/pii/S0031320319303851,https://doi.org/10.1016/j.patcog.2019.107084,0031-3203,2020,107084,100,Pattern Recognition,Structured self-attention architecture for graph-level representation learning,article,FAN2020107084,
"In this paper, we propose a novel linear binary classifier, denoted by population-guided large margin classifier (PGLMC), applicable to any sorts of data, including high-dimensional low-sample-size (HDLSS). PGLMC is conceived with a projecting direction w given by the comprehensive consideration of local structural information of the hyperplane and the statistics of the training samples. Our proposed model has several advantages compared to those widely used approaches. First, it isn't sensitive to the intercept term b. Second, it operates well with imbalanced data. Third, it is relatively simple to be implemented based on Quadratic Programming. Fourth, it is robust to the model specification for various real applications. The theoretical properties of PGLMC are proven. We conduct a series of evaluations on the simulated and five realworld benchmark data sets, including DNA classification, medical image analysis and face recognition. PGLMC outperforms the state-of-theart classification methods in most cases, or obtains comparable results.","Binary linear classifier, Data piling, High-dimension lowsample-size, Hyperplane, Large margin classification, Local structure information",Qingbo Yin and Ehsan Adeli and Liran Shen and Dinggang Shen,https://www.sciencedirect.com/science/article/pii/S0031320319303334,https://doi.org/10.1016/j.patcog.2019.107030,0031-3203,2020,107030,97,Pattern Recognition,Population-guided large margin classifier for high-dimension low-sample-size problems,article,YIN2020107030,
"Unsupervised domain adaptation aims to improve the performance of an unknown target domain by utilizing the knowledge learned from a related source domain. Given that the target label information is unavailable in the unsupervised situation, it is challenging to match the domain distributions and to transfer the source model to target applications. In this paper, a Deep Conditional Adaptation Networks (DCAN) is proposed to address the unsupervised domain adaptation problem. DCAN is implemented based on a deep neural network and attempts to learn domain invariant features based on the Wasserstein distance. A conditional adaptation strategy is presented to reduce the domain distribution discrepancy and to address category mismatch and class prior bias, which are usually ignored in marginal adaptation approaches. Furthermore, we propose a label correlation transfer algorithm to address the unsupervised issues, by generating more effective pseudo target labels based on the underlying cross-domain relationship. A set of comparative experiments were performed on standard domain adaptation benchmarks and the results demonstrate that the proposed DCAN outperforms previous adaptation methods.","Conditional domain adaptation, Deep learning, Unsupervised learning, Label transfer",Yu Chen and Chunling Yang and Yan Zhang and Yuze Li,https://www.sciencedirect.com/science/article/pii/S0031320319303735,https://doi.org/10.1016/j.patcog.2019.107072,0031-3203,2020,107072,98,Pattern Recognition,Deep conditional adaptation networks and label correlation transfer for unsupervised domain adaptation,article,CHEN2020107072,
"Multi-scale approaches have been widely used for achieving high accuracy for scene text detection, but they usually slow down the speed of the whole system. In this paper, we propose a two-stage framework for realtime multi-scale scene text detection. The first stage employs a novel Scale-based Region Proposal Network (SRPN) which can localize text of wide scale range and estimate text scale efficiently. Based on SRPN, non-text regions are filtered out, and text region proposals are generated. Moreover, based on text scale estimation by SRPN, small or big texts in region proposals are resized into a unified normal scale range. The second stage then adopts a Fully Convolutional Network based scene text detector to localize text words from proposals of the first stage. Text detector in the second stage detects texts of narrow scale range but accurately. Since most non-text regions are eliminated through SRPN efficiently, and texts in proposals are properly scaled to avoid multi-scale pyramid processing, the whole system is quite fast. We evaluate both performance and speed of the proposed method on datasets ICDAR2015, ICDAR2013, and MSRA-TD500. On ICDAR2015, our system can reach the state-of-the-art F-measure score of 85.40% at 16.5Â fps (frame per second), and competitive performance of 79.66% at 35.1Â fps, either of which is more than 5 times faster than previous best methods. On ICDAR2013 and MSRA-TD500, we also achieve remarkable speedup by keeping competitive performance. Ablation experiments are also provided to demonstrate the reasonableness of our method.","Scene text detection, Multi-scale, Speedup, Scale-based region proposal network",Wenhao He and Xu-Yao Zhang and Fei Yin and Zhenbo Luo and Jean-Marc Ogier and Cheng-Lin Liu,https://www.sciencedirect.com/science/article/pii/S0031320319303292,https://doi.org/10.1016/j.patcog.2019.107026,0031-3203,2020,107026,98,Pattern Recognition,Realtime multi-scale scene text detection with scale-based region proposal network,article,HE2020107026,
"Human action recognition is a hot research topic in the field of computer vision. The availability of low cost depth sensors in the market made the extraction of reliable skeleton maps of human objects easier. This paper proposes three subnets, referred to as SNet, TNet, and BodyNet to capture diverse spatio-temporal dynamics for action recognition task. Specifically, SNet is used to capture pose dynamics from the distance maps in the spatial domain. The second subnet (TNet) captures the temporal dynamics along the sequence. The third net (BodyNet) extracts distinct features from the fine-grained body parts in the temporal domain. With the motivation of ensemble learning, a hybrid network, referred to as HNet, is modeled using two subnets (TNet and BodyNet) to capture robust temporal dynamics. Finally, SNet and HNet are fused as one ensemble network for action classification task. Our method achieves competitive results on three widely used datasets: UTD MHAD, UT Kinect and NTU RGB+D.","Human action recognition, Distance maps, Part features, Convolutional neural networks, Long short term memory",Naveenkumar M. and Domnic S.,https://www.sciencedirect.com/science/article/pii/S0031320319304261,https://doi.org/10.1016/j.patcog.2019.107125,0031-3203,2020,107125,100,Pattern Recognition,Deep ensemble network using distance maps and body part features for skeleton based action recognition,article,M2020107125,
"Besides the binary segmentation, many retinal image segmentation methods also produce a score map, where a nonnegative score is assigned for each pixel to indicate the likelihood of being a vessel. This observation inspires us to propose a new approach as a post-processing step to improve existing methods by formulating segmentation as a matting problem. A trimap is obtained via a bi-level thresholding of the score map from existing methods, which is instrumental in focusing the attention to pixels of these unknown areas. A dedicated end-to-end matting algorithm is further developed to retrieve those vessel pixels in the unknown areas, and to produce the final vessel segmentation by minimizing global pixel loss and local matting loss. Our approach is shown to be particularly effective in rescuing thin and tiny vessels that may lead to disconnections of vessel fragments. Moreover, it is observed that our approach is capable of improving the overall segmentation performance across a broad range of existing methods.","Vessel segmentation, Retinal images, Deep learning, Local matting loss",He Zhao and Huiqi Li and Li Cheng,https://www.sciencedirect.com/science/article/pii/S0031320319303693,https://doi.org/10.1016/j.patcog.2019.107068,0031-3203,2020,107068,98,Pattern Recognition,Improving retinal vessel segmentation with joint local loss by matting,article,ZHAO2020107068,
"The inconsistency of data distributions among multiple views is one of the most crucial issues which hinder the accuracy of person re-identification. To solve the problem, this paper presents a novel similarity learning model by combining the optimization of feature representation via multi-view visual words reconstruction and the optimization of metric learning via joint discriminative transfer learning. The starting point of the proposed model is to capture multiple groups of multi-view visual words (MvVW) through an unsupervised clustering method (i.e. K-means) from human parts (e.g. head, torso, legs). Then, we construct a joint feature matrix by combining multi-group feature matrices with different body parts. To solve the inconsistent distributions under different views, we propose a method of joint transfer constraint to learn the similarity function by combining multiple common subspaces, each in charge of a sub-region. In the common subspaces, the original samples can be reconstructed based on MvVW under low-rank and sparse representation constraints, which can enhance the structure robustness and noise resistance. During the process of objective function optimization, based on confinement fusion of multi-view and multiple sub-regions, a solution strategy is proposed to solve the objective function using joint matrix transform. Taking all of these into account, the issue of person re-identification under inconsistent data distributions can be transformed into a consistent iterative convex optimization problem, and solved via the inexact augmented Lagrange multiplier (IALM) algorithm. Extensive experiments are conducted on three challenging person re-identification datasets (i.e., VIPeR, CUHK01 and PRID450S), which shows that our model outperforms several state-of-the-art methods.","Person re-identification, Feature extraction, Similarity learning",Cairong Zhao and Xuekuan Wang and Wangmeng Zuo and Fumin Shen and Ling Shao and Duoqian Miao,https://www.sciencedirect.com/science/article/pii/S0031320319303176,https://doi.org/10.1016/j.patcog.2019.107014,0031-3203,2020,107014,97,Pattern Recognition,Similarity learning with joint transfer constraints for person re-identification,article,ZHAO2020107014,
"As a new human-computer interaction way, in-air handwriting allows users to perform gesture-based writing in the midair. However, most existing in-air handwriting systems mainly focus on recognizing either isolated characters/words or only a small number of texts, making those systems far from practical applications. Instead, here we present a 3D in-air handwritten Chinese text recognitionÂ (IAHCTR) system for the first time, and construct the first public large-scale IAHCT dataset. Moreover, a novel architecture, named the temporal convolutional recurrent network (TCRN), is proposed for online HCTR. Specifically, the TCRN first applies the 1-dimensional convolution to extract local contextual features from low-level trajectories, and then it utilizes the recurrent network to capture long-term dependencies of high-level outputs. Compared with the state-of-the-art architecture, the TCRN not only avoids the domain-specific knowledge for feature image extraction, but also attains higher training efficiency with a more compact model. Empirically, this TCRN also outperforms the single recurrent network with faster prediction and higher accuracy. Experiments on CASIA-OLHWDB2 & ICDAR-2013 demonstrate that the TCRN yields the best result in comparison to the state-of-the-art methods for online HCTR.","In-air handwriting, Handwritten chinese text recognition, Temporal convolutional recurrent networks",Ji Gan and Weiqiang Wang and Ke Lu,https://www.sciencedirect.com/science/article/pii/S0031320319303280,https://doi.org/10.1016/j.patcog.2019.107025,0031-3203,2020,107025,97,Pattern Recognition,In-air handwritten Chinese text recognition with temporal convolutional recurrent network,article,GAN2020107025,
"The three-dimensional representation of the human face has emerged as a viable and effective way to characterize the facial surface for expression classification purposes. The rapid progress in the area continually demands its up-to-date characterization to guide and support research decisions, specially for newcomer researchers. This systematic literature review focus on investigating three major aspects of 3D facial expression recognition methods: face representation, preprocessing and classification experiments. The investigation of 49 specialized studies revealed the preferential types of data and regions of interest for face representation in recent years, as well as a trend towards keypoint-independent methods. In addition, it brings to light current weaknesses regarding the report of preprocessing techniques and identifies challenges concerning the current possibility of fair comparison among multiple methods. The presented findings outline essential research decisions whose the regardful report is of great value to this research community.","3D facial expression recognition, Systematic literature review, Preprocessing techniques, Face representation, Deep learning, Classification setup",Gilderlane Ribeiro Alexandre and JosÃ© Marques Soares and George AndrÃ© {Pereira ThÃ©},https://www.sciencedirect.com/science/article/pii/S0031320319304091,https://doi.org/10.1016/j.patcog.2019.107108,0031-3203,2020,107108,100,Pattern Recognition,Systematic review of 3D facial expression recognition methods,article,ALEXANDRE2020107108,
"Fast approximate nearest neighbor search has been well studied for real-valued vectors, however, the methods for binary descriptors are less developed. The paper addresses this problem by resorting to the well established techniques in Euclidean space. To this end, the binary descriptors are firstly mapped into low dimensional float vectors under the condition that the neighborhood information in the original Hamming space could be preserved in the mapped Euclidean space as much as possible. Then, KD-Tree is used to partitioning the mapped Euclidean space in order to quickly find approximate nearest neighbors for a given query point. This is identical to filter out a subset of nearest neighbor candidates in the original Hamming space due to the property of neighborhood preserving. Finally, Hamming ranking is applied to the small number of candidates to find out the approximate nearest neighbor in the original Hamming space, with only a fraction of running time compared to the bruteforce linear scan. Our experiments demonstrate that the proposed method significantly outperforms the state of the arts, obtaining improved search accuracy at various speed up factors, e.g., at least 16% improvement of search accuracy over previous methodsÂ (from 67.7% to 83.7%) when the search speed is 200 times faster than the linear scan for a one million database.","Binary feature, Feature matching, Approximate nearest neighbor search, Scalable image matching",Bin Fan and Qingqun Kong and Baoqian Zhang and Hongmin Liu and Chunhong Pan and Jiwen Lu,https://www.sciencedirect.com/science/article/pii/S0031320319303838,https://doi.org/10.1016/j.patcog.2019.107082,0031-3203,2020,107082,99,Pattern Recognition,Efficient nearest neighbor search in high dimensional hamming space,article,FAN2020107082,
"We propose in this paper a novel model-based gait recognition method, PoseGait. Gait recognition is a challenging and attractive task in biometrics. Early approaches to gait recognition were mainly appearance-based. The appearance-based features are usually extracted from human body silhouettes, which are easy to compute and have shown to be efficient for recognition tasks. Nevertheless silhouettes shape is not invariant to changes in clothing, and can be subject to drastic variations, due to illumination changes or other external factors. An alternative to silhouette-based features are model-based features. However, they are very challenging to acquire especially for low image resolution. In contrast to previous approaches, our model PoseGait exploits human 3D pose estimated from images by Convolutional Neural Network as the input feature for gait recognition. The 3D pose, defined by the 3D coordinates of joints of the human body, is invariant to view changes and other external factors of variation. We design spatio-temporal features from the 3D pose to improve the recognition rate. Our method is evaluated on two large datasets, CASIAÂ B and CASIAÂ E. The experimental results show that the proposed method can achieve state-of-the-art performance and is robust to view and clothing variations.","Gait recognition, Human body pose, Spatio-temporal feature,",Rijun Liao and Shiqi Yu and Weizhi An and Yongzhen Huang,https://www.sciencedirect.com/science/article/pii/S003132031930370X,https://doi.org/10.1016/j.patcog.2019.107069,0031-3203,2020,107069,98,Pattern Recognition,A model-based gait recognition method with body pose and human prior knowledge,article,LIAO2020107069,
"In this paper, we investigate the large-scale multi-label image classification problem when images with unknown novel classes come in stream during the training stage. It coincides with the practical requirement that usually novel classes are detected and used to update an existing image recognition system. Most existing multi-label image classification methods cannot be directly applied in this scenario, where the training and testing stages must have the same label set. In this paper, we proposed to learn a multi-label classifier and a novel-class detector alternately to solve this problem. The multi-label classifier is learned using a convolutional neural network (CNN) from the images in the known classes. We proposed a recurrent novel-class detector which is learned in the supervised manner to detect the novel class by encoding image features with the multi-label information. In the experiment, our method is evaluated on several large-scale multi-label benchmarks including MS COCO. The results show the proposed method is comparable to most existing multi-label image classification methods, which validate its efficacy when encountering streaming images with unknown classes.","Multi-label image classification, Recurrent novel-class detector, Streaming images",Yu Zhang and Yin Wang and Xu-Ying Liu and Siya Mi and Min-Ling Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319304017,https://doi.org/10.1016/j.patcog.2019.107100,0031-3203,2020,107100,99,Pattern Recognition,Large-scale multi-label classification using unknown streaming images,article,ZHANG2020107100,
"In the paper, a novel fast and robust template matching method named A-MNS based on Majority Neighbour Similarity (MNS) and the annulus projection transformation (APT) is proposed. Its essence is the MNS, a useful, rotation-invariant, low-computational-cost and robust similarity measurement. The proposed method is theoretically demonstrated and experimentally evaluated as being able to estimate the rotation angle of the target object, overcome challenges such as background clutter, occlusion, arbitrary rotation transformation, and non-rigid deformation, while performing fast matching. Empirical results evaluated on the up-to-date benchmark show that A-MNS is 4.419 times faster than DDIS (the state-of-the-art) and is also competitive in terms of its matching accuracy.","Template matching, MNS, APT",Jinxiang Lai and Liang Lei and Kaiyuan Deng and Runming Yan and Yang Ruan and Zhou Jinyun,https://www.sciencedirect.com/science/article/pii/S0031320319303322,https://doi.org/10.1016/j.patcog.2019.107029,0031-3203,2020,107029,98,Pattern Recognition,Fast and robust template matching with majority neighbour similarity and annulus projection transformation,article,LAI2020107029,
"When an individualâs fingerprint is scanned, although the global fingerprint pattern is unchanged, at the local level, between different scans the minutiae pattern may vary. Minutiae translation and rotation are caused by changing finger orientation and position shift during fingerprint acquisition. Minutiae patterns may also suffer non-linear distortion due to finger skin elasticity. Despite a variety of approaches to detecting deformations in fingerprint images, there has been no method available for capturing minutiae variations between two impressions of the same finger in a unified model. In this paper we address this issue by proposing a unified model to represent minutiae variations between fingerprint scans and formulate the changes to minutiae feature patterns. We identify the MÃ¶bius transformation as a good candidate for modelling minutiae translation, rotation and non-linear distortion, that is, different types of minutiae variations are described in a single model. Not only do we mathematically prove that the MÃ¶bius transformation based model is a unified model for capturing minutiae variations, but we also experimentally verify the effectiveness of this model using a public database.","Minutiae variations, Fingerprint pattern, MÃ¶bius transformation, Minutiae translation, Minutiae rotation, Non-linear distortion",James Moorfield and Song Wang and Wencheng Yang and Aseel Bedari and Peter Van Der Kamp,https://www.sciencedirect.com/science/article/pii/S0031320319303565,https://doi.org/10.1016/j.patcog.2019.107054,0031-3203,2020,107054,98,Pattern Recognition,A MÃ¶bius transformation based model for fingerprint minutiae variations,article,MOORFIELD2020107054,
"Graph-based clustering is an efficient method for identifying clusters in local and nonlinear data patterns. Among the existing methods, spectral clustering is one of the most prominent algorithms. However, this method is vulnerable to noise and outliers. This study proposes a robust graph-based clustering method that removes the data nodes of relatively low density. The proposed method calculates the pseudo-density from a similarity matrix, and reconstructs it using a sparse regularization model. In this process, noise and the outer points are determined and removed. Unlike previous edge cutting-based methods, the proposed method is robust to noise while detecting clusters because it cuts out irrelevant nodes. We use a simulation and real-world data to demonstrate the usefulness of the proposed method by comparing it to existing methods in terms of clustering accuracy and robustness to noisy data. The comparison results confirm that the proposed method outperforms the alternatives.","Graph-based clustering, Unsupervised learning, Spectral clustering, Pseudo-density reconstruction, Node cutting",Younghoon Kim and Hyungrok Do and Seoung Bum Kim,https://www.sciencedirect.com/science/article/pii/S0031320319303048,https://doi.org/10.1016/j.patcog.2019.107001,0031-3203,2020,107001,97,Pattern Recognition,Outer-Points shaver: Robust graph-based clustering via node cutting,article,KIM2020107001,
"Multifocus image fusion techniques primarily emphasize human vision and machine perception to evaluate an image, which often ignore depth information contained in the focus regions. In this paper, a novel 3D shape reconstruction algorithm based on nonsubsampled shearlet transform (NSST) microscopic multifocus image fusion method is proposed to mine 3D depth information from the fusion process. The shift-invariant property of NSST guarantees the spatial corresponding relationship between the image sequence and its high-frequency subbands. Since the high-frequency components of an image represent the focus level of the image, a new multidirectional modified Laplacian (MDML) as the focus measure maps the high-frequency subbands to images of various levels of depth. Next, the initial 3D reconstruction result is obtained by using an optimal level selection strategy based on the summation of the multiscale Laplace responses to exploit these depth maps. Finally, an iterative edge repair method is implemented to refine the reconstruction result. The experimental results show that the proposed method has better performance, especially when the source images have low-contrast regions.","3D shape reconstruction, Image fusion, Shape-from-focus, Microscopic imaging, Nonsubsampled shearlet transform",Tao Yan and Zhiguo Hu and Yuhua Qian and Zhiwei Qiao and Linyuan Zhang,https://www.sciencedirect.com/science/article/pii/S003132031930367X,https://doi.org/10.1016/j.patcog.2019.107065,0031-3203,2020,107065,98,Pattern Recognition,3D shape reconstruction from multifocus image fusion using a multidirectional modified Laplacian operator,article,YAN2020107065,
"State-of-the-art palmprint recognition methods have achieved significant performances. However, most of the existing methods are focused on particular scenarios such as a specific illumination or being captured using a contact-based or contactless device. Therefore, these algorithms cannot meet the ever-changing complex application requirements. To resolve this issue, this paper proposes a generic framework to represent high-level discriminative features for multiple scenarios in palmprint recognition with learned discriminative deep convolutional networks named deep discriminative representation (DDR). We propose to learn discriminative deep convolutional networks with limited palmprint training data, which is utilized to extract deep discriminative features. Then, the collaborative representation based classifier is implemented for palmprint recognition, which is flexible and practical in numerous scenarios. The experimental results demonstrate that DDR produces the best recognition performance in generic palmprint recognition compared to other state-of-the-art methods. For contact-based palmprint recognition under different lighting sources, DDR achieved the best performance on the PolyU Multi-spectral database with M_R, M_B, M_G and M_NIR, respectively. As for contactless palmprint recognition, DDR obtained the highest results on the IITD and CASIA databases.","Generic palmprint recognition, Deep discriminative networks, Deep discriminative feature, DDF-CRC",Shuping Zhao and Bob Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319303723,https://doi.org/10.1016/j.patcog.2019.107071,0031-3203,2020,107071,98,Pattern Recognition,Deep discriminative representation for generic palmprint recognition,article,ZHAO2020107071,
"Wearable/portable brain-computer interfaces (BCIs) for the long-term end use are a focus of recent BCI research. A challenge is how to update the BCI to meet changes in electroencephalography (EEG) signals, since the resource are so limited that retraining of traditional well-performed models, such as a support vector machine, is nearly impossible. To cope with this challenge, less-demanding adaptive online learning can be considered. We investigated an adaptive projected sub-gradient method (APSM) that is originated from the set theoretic estimation formulation and the projections onto convex sets theory. APSM provides a unifying framework for both adaptive classification and regression tasks. Coefficients of APSM are adjusted online as data arrive sequentially, with a regularization constraint made by projections onto a fixed closed ball. We extended the general APSM to a shrinkage form, where shrinkage closed balls were used instead of the original fixed one, expecting a more controllable fading effect and better adaptability. The convergence of shrinkage APSM was proved. It was also demonstrated that as shrinkage factor approached to 1, the limit point of shrinkage APSM would approach to the optimal solution with the least norm, which could be especially beneficial for generalization of the classifier. The performance of the proposed method was evaluated, and compared with those of the general APSM, the incremental support vector machine, and the passive aggressive algorithm, through an event-related potential-based BCI experiment. Results showed the advantage of the proposed method over the others on both the online classification performance and the easiness of tuning. Our study revealed the effectiveness of the proposed method for adaptive EEG classification, making it a promising tool for on-device training and updating of wearable/portable BCIs, as well as for application in other related fields, such as EEG-based biometrics.","Online learning, Projections, Wearable/portable brain computer interface, Electroencephalography, Event-related potential, Biometrics",Zheng Ma and Jun Cheng and Dapeng Tao,https://www.sciencedirect.com/science/article/pii/S0031320319303206,https://doi.org/10.1016/j.patcog.2019.107017,0031-3203,2020,107017,97,Pattern Recognition,Online learning using projections onto shrinkage closed balls for adaptive brain-computer interface,article,MA2020107017,
"Due to the unavailability of labeled target data, most existing unsupervised domain adaptation (UDA) methods alternately classify the unlabeled target samples and discover a low-dimensional subspace by mitigating the cross-domain distribution discrepancy. During the pseudo-label guided subspace discovery step, however, the posterior probabilities (uncertainties) from the previous target label estimation step are totally ignored, which may promote the error accumulation and degrade the adaptation performance. To address this issue, we propose to progressively increase the number of target training samples and incorporate the uncertainties to accurately characterize both cross-domain distribution discrepancy and other intra-domain relations. Specifically, we exploit maximum mean discrepancy (MMD) and within-class variance minimization for these relations, yet, these terms merely focus on the global class structure while ignoring the local structure. Then, a triplet-wise instance-to-center margin is further maximized to push apart target instances and source class centers of different classes and bring closer them of the same class. Generally, an EM-style algorithm is developed by alternating between inferring uncertainties, progressively selecting certain training target samples, and seeking the optimal feature transformation to bridge two domains. Extensive experiments on three popular visual domain adaptation datasets demonstrate that our method significantly outperforms recent state-of-the-art approaches.","Unsupervised domain adaptation, Pseudo labeling, Feature transformation, Progressive learning, Transfer learning",Jian Liang and Ran He and Zhenan Sun and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S0031320319302997,https://doi.org/10.1016/j.patcog.2019.106996,0031-3203,2019,106996,96,Pattern Recognition,Exploring uncertainty in pseudo-label guided unsupervised domain adaptation,article,LIANG2019106996,
"In recent years, many approaches that use keystroke dynamics in free text authentication have been proposed. The major drawback of the proposed approaches is that training generally requires several months, thereby resulting in low practicality. In this study, a method to detect U.S. English fraudulent messages by analyzing keyboard users' keystroke dynamics is proposed. To the best of our knowledge, this is the first study to apply keystroke dynamics to detect fraudulent instant messages. In the proposed system, each user requires only approximately 20â¯min of training in U.S. English keystroke dynamics. Furthermore, a voting-based statistical classifier is presented to improve the recognition accuracy of instant messages and prevent phishing messages. Experimental results indicate that the proposed approach outperforms other relevant published methods in terms of shorter training time, fewer false alarms, and comparable recognition accuracy.","Biometrics, Internet fraud, Instant messaging, Keystroke dynamics, Free text",Cheng-Jung Tsai and Po-Hao Huang,https://www.sciencedirect.com/science/article/pii/S0031320319303681,https://doi.org/10.1016/j.patcog.2019.107067,0031-3203,2020,107067,98,Pattern Recognition,Keyword-based approach for recognizing fraudulent messages by keystroke dynamics,article,TSAI2020107067,
"Centroid-based methods including k-means and fuzzy c-means are known as effective and easy-to-implement approaches to clustering purposes in many applications. However, these algorithms cannot be directly applied to supervised tasks. This paper thus presents a generative model extending the centroid-based clustering approach to be applicable to classification and regression tasks. Given an arbitrary loss function, the proposed approach, termed Supervised Fuzzy Partitioning (SFP), incorporates labels information into its objective function through a surrogate term penalizing the empirical risk. Entropy-based regularization is also employed to fuzzify the partition and to weight features, enabling the method to capture more complex patterns, identify significant features, and yield better performance facing high-dimensional data. An iterative algorithm based on block coordinate descent scheme is formulated to efficiently find a local optimum. Extensive classification experiments on synthetic, real-world, and high-dimensional datasets demonstrate that the predictive performance of SFP is competitive with state-of-the-art algorithms such as SVM and random forest. SFP has a major advantage over such methods, in that it not only leads to a flexible, nonlinear model but also can exploit any convex loss function in the training phase without compromising computational efficiency.","Supervised k-means, Centroid-based clustering, Entropy-based regularization, Feature weighting, Mixtures of experts",Pooya Ashtari and Fateme {Nateghi Haredasht} and Hamid Beigy,https://www.sciencedirect.com/science/article/pii/S0031320319303164,https://doi.org/10.1016/j.patcog.2019.107013,0031-3203,2020,107013,97,Pattern Recognition,Supervised fuzzy partitioning,article,ASHTARI2020107013,
"Traditional Multi View Stereo (MVS) algorithms are often difficult to deal with large-scale indoor scene reconstruction, due to the photo-consistency measurement errors in weak textured regions, which are commonly exist in indoor scenes. To solve this limitation, in this paper we proposed a point cloud completion strategy that combines learning-based depth-map completion and geometry-based consistency filtering to fill large-area missing in depth-maps. The proposed method takes nonuniform and noisy MVS depth-map as input, and completes each depth-map individually. In the completion process, we first complete depth-maps using learning based method, and then filter each depth-map using depth consistency validation with its neighboring depth-maps. This depth-map completion and geometric filtering steps are performed iteratively until the number of depth points is converged. Experiments on large-scale indoor scenes and benchmark MVS datasets demonstrate the effectiveness of the proposed methods.","Depth completion, MVS, 3D Reconstruction, Point cloud,",Hongmin Liu and Xincheng Tang and Shuhan Shen,https://www.sciencedirect.com/science/article/pii/S0031320319304133,https://doi.org/10.1016/j.patcog.2019.107112,0031-3203,2020,107112,99,Pattern Recognition,Depth-map completion for large indoor scene reconstruction,article,LIU2020107112,
"Segmentation, a useful/powerful technique in pattern recognition, is the process of identifying object outlines within images. There are a number of efficient algorithms for segmentation in Euclidean space that depend on the variational approach and partial differential equation modelling. Wavelets have been used successfully in various problems in image processing, including segmentation, inpainting, noise removal, super-resolution image restoration, and many others. Wavelets on the sphere have been developed to solve such problems for data defined on the sphere, which arise in numerous fields such as cosmology and geophysics. In this work, we propose a wavelet-based method to segment images on the sphere, accounting for the underlying geometry of spherical data. Our method is a direct extension of the tight-frame based segmentation method used to automatically identify tube-like structures such as blood vessels in medical imaging. It is compatible with any arbitrary type of wavelet frame defined on the sphere, such as axisymmetric wavelets, directional wavelets, curvelets, and hybrid wavelet constructions. Such an approach allows the desirable properties of wavelets to be naturally inherited in the segmentation process. In particular, directional wavelets and curvelets, which were designed to efficiently capture directional signal content, provide additional advantages in segmenting images containing prominent directional and curvilinear features. We present several numerical experiments, applying our wavelet-based segmentation method, as well as the common K-means method, on real-world spherical images, including an Earth topographic map, a light probe image, solar data-sets, and spherical retina images. These experiments demonstrate the superiority of our method and show that it is capable of segmenting different kinds of spherical images, including those with prominent directional features. Moreover, our algorithm is efficient with convergence usually within a few iterations.","Image segmentation, Wavelets, Curvelets, Tight frame, Sphere",Xiaohao Cai and Christopher G.R. Wallis and Jennifer Y.H. Chan and Jason D. McEwen,https://www.sciencedirect.com/science/article/pii/S0031320319303826,https://doi.org/10.1016/j.patcog.2019.107081,0031-3203,2020,107081,100,Pattern Recognition,Wavelet-based segmentation on the sphere,article,CAI2020107081,
"Recent studies have shown that deep neural networks have pushed visual tracking accuracy to new heights, but finding more robust long-term tracking is still challenging because of the dynamic foreground and background changes. This phenomenon affects the overall performance via online training sample generation. The dense sampling strategy has been widely used for its convenience, the appearance variation is severely limited by its highly spatial overlapping mechanism. The sample candidate evaluation with a classification score metric is not always reliable throughout the entire process, therefore, tracking failure is inevitable. As an effective solution, this paper proposes a novel sample-level generative adversarial network (GAN) to enrich the training data by generating massive amounts of sample-level GAN samples. These samples are not only similar to the real-life scenarios, but also could carry more diversity of deformation and motion blur to a certain degree. For occlusion invariance, a feature-level GAN is incorporated to generate more challenging feature-level GAN data by creating random occlusion masks in deep feature space. To facilitate the online learning process, a label smoothing loss regularization is introduced to achieve model regularization and over-fitting reduction by integrating the unlabeled GAN-generated training data with the realistically labeled ones. In addition, a re-detection correlation filter conservatively trained with reliable training data is employed to integrate a classification score metric to perform reliable model updates and avoid heavy degradation. Furthermore, we also carry out the re-detection correlation filter on the candidate region proposals to handle the tracking failures. The proposed tracker has shown superior performance in comparison to the other state-of-the-art tracking approaches on the OTB-2013, OTB-100, UAV123, UAV20L, and VOT2016 benchmark datasets.","Visual tracking, Sample-level generative adversarial network, Feature-level generative adversarial network, Label smoothing loss regularization, Re-detection correlation filter",Yamin Han and Peng Zhang and Wei Huang and Yufei Zha and Garth Douglas Cooper and Yanning Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319303309,https://doi.org/10.1016/j.patcog.2019.107027,0031-3203,2020,107027,97,Pattern Recognition,Robust Visual Tracking based on Adversarial Unlabeled Instance Generation with Label Smoothing Loss Regularization,article,HAN2020107027,
"Image set classification using manifolds is becoming increasingly more attractive since it considers non-Euclidean geometry. However, with the success of dictionary learning for image set classification using manifolds, how to learn an over-complete dictionary is still challenging. This paper proposes a novel prototype subspace learning method, in which a set of images is represented by a linear subspace and then mapped onto a Grassmann manifold. With this subspace representation, class prototypes and intra-class differences can be represented as principal components and variation subspaces, respectively. Isometric mapping further maps the manifolds into the symmetrical space via collaborative representation, which permits a closed-term solution. The proposed method is evaluated for face recognition, object recognition and action recognition. Extensive experimental results on the Honda, Extended YaleB, ETH-80 and Cambridge-Gesture datasets verify the superiority of the proposed method over the state-of-the-art methods.","Image set classification, Collaborative representation, Prototype learning, Grassmann manifolds",Dong Wei and Xiaobo Shen and Quansen Sun and Xizhan Gao and Wenzhu Yan,https://www.sciencedirect.com/science/article/pii/S0031320319304248,https://doi.org/10.1016/j.patcog.2019.107123,0031-3203,2020,107123,100,Pattern Recognition,Prototype learning and collaborative representation using Grassmann manifolds for image set classification,article,WEI2020107123,
"Online activity recognition which aims to detect and recognize activity instantly from a continuous video stream is a key technology in human-robot interaction. However, the partial activity observation problem, mainly due to the incomplete sequence acquisition, makes it greatly challenging. This paper proposes a novel approach, named Multi-stage Adaptive Regression (MAR), for online activity recognition with the main focus on addressing the partial observation problem. Specifically, the MAR framework delicately assembles overlapped activity observations to improve its robustness against arbitrary activity segments. Then multiple score functions corresponding to each specific performance stage are collaboratively learned via a adaptive label strategy to enhance its power of discriminating similar partial activities. Moreover, the Online Human Interaction (OHI) database is constructed to evaluate the online activity recognition in human interaction scenarios. Extensive experimental evaluations on the Multi-Modal Action Detection (MAD) database and the OHI database show that the MAR method achieves an outstanding performance over the state-of-the-art approaches.","Online activity recognition, Interaction recognition, Partial observation, Adaptive regression",Bangli Liu and Haibin Cai and Zhaojie Ju and Honghai Liu,https://www.sciencedirect.com/science/article/pii/S0031320319303553,https://doi.org/10.1016/j.patcog.2019.107053,0031-3203,2020,107053,98,Pattern Recognition,Multi-stage adaptive regression for online activity recognition,article,LIU2020107053,
"In many different classification tasks it is required to manage structured data, which are usually modeled as graphs. Moreover, these graphs can be dynamic, meaning that the vertices/edges of each graph may change over time. The goal is to exploit existing neural network architectures to model datasets that are best represented with graph structures that change over time. To the best of the authorsâ knowledge, this task has not been addressed using these kinds of architectures. Two novel approaches are proposed, which combine Long Short-Term Memory networks and Graph Convolutional Networks to learn long short-term dependencies together with graph structure. The advantage provided by the proposed methods is confirmed by the results achieved on four real world datasets: an increase of up to 12 percentage points in Accuracy and F1 scores for vertex-based semi-supervised classification and up to 2 percentage points in Accuracy and F1 scores for graph-based supervised classification.",,Franco Manessi and Alessandro Rozza and Mario Manzo,https://www.sciencedirect.com/science/article/pii/S0031320319303036,https://doi.org/10.1016/j.patcog.2019.107000,0031-3203,2020,107000,97,Pattern Recognition,Dynamic graph convolutional networks,article,MANESSI2020107000,
"The rise in the availability of video content for access via the Internet and the medium of television has resulted in the development of automatic search procedures to retrieve the desired video. Searches can be simplified and hastened by employing automatic classification of videos. This paper proposes a descriptor called the Spatio-Temporal Histogram of Radon Projections (STHRP) for representing the temporal pattern of the contents of a video and demonstrates its application to video classification and retrieval. The first step in STHRP pattern computation is to represent any video as Three Orthogonal Planes (TOPs), i.e., XY, XT and YT, signifying the spatial and temporal contents. Frames corresponding to each plane are partitioned into overlapping blocks. Radon projections are obtained over these blocks at different orientations, resulting in weighted transform coefficients that are normalized and grouped into bins. Linear Discriminant Analysis (LDA) is performed over these coefficients of the TOPs to arrive at a compact description of STHRP pattern. Compared to existing classification and retrieval approaches, the proposed descriptor is highly robust to translation, rotation and illumination variations in videos. To evaluate the capabilities of the invariant STHRP pattern, we analyse the performance by conducting experiments on the UCF-101, HMDB51, 10contexts and TRECVID data sets for classification and retrieval using a bagged tree model. Experimental evaluation of video classification reveals that STHRP pattern can achieve classification rates of 96.15%, 71.7%, 93.24% and 97.3% for the UCF-101, HMDB51,10contexts and TRECVID 2005 data sets respectively. We conducted retrieval experiments on the TRECVID 2005, JHMDB and 10contexts data sets and the results revealed that STHRP pattern is able to provide the videos relevant to the user's query in minimal time (0.05s) with a good precision rate.","Bagged trees classification model, Linear discriminant analysis, Radon Projections, Spatio temporal feature, Video classification and retrieval",A. Sasithradevi and S. Mohamed Mansoor Roomi,https://www.sciencedirect.com/science/article/pii/S0031320319304005,https://doi.org/10.1016/j.patcog.2019.107099,0031-3203,2020,107099,99,Pattern Recognition,Video classification and retrieval through spatio-temporal Radon features,article,SASITHRADEVI2020107099,
"Decision trees in random forests use a single feature in non-leaf nodes to split the data. Such splitting results in axis-parallel decision boundaries which may fail to exploit the geometric structure in the data. In oblique decision trees, an oblique hyperplane is employed instead of an axis-parallel hyperplane. Trees with such hyperplanes can better exploit the geometric structure to increase the accuracy of the trees and reduce the depth. The present realizations of oblique decision trees do not evaluate many promising oblique splits to select the best. In this paper, we propose a random forest of heterogeneous oblique decision trees that employ several linear classifiers at each non-leaf node on some top ranked partitions which are obtained via one-vs-all and two-hyperclasses based approaches and ranked based on ideal Gini scores and cluster separability. The oblique hyperplane that optimizes the impurity criterion is then selected as the splitting hyperplane for that node. We benchmark 190 classifiers on 121 UCI datasets. The results show that the oblique random forests proposed in this paper are the top 3 ranked classifiers with the heterogeneous oblique random forest being statistically better than all 189 classifiers in the literature.","Benchmarking, Classifiers, Oblique random forest, Heterogeneous, One-vs-all, Ensemble learning",Rakesh Katuwal and P.N. Suganthan and Le Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319303796,https://doi.org/10.1016/j.patcog.2019.107078,0031-3203,2020,107078,99,Pattern Recognition,Heterogeneous oblique random forest,article,KATUWAL2020107078,
"We propose the use of a novel feature, called local distance features, for time series classification. The local distance features are extracted using Dynamic Time Warping (DTW) and classified using Convolutional Neural Networks (CNN). DTW is classically as a robust distance measure for distance-based time series recognition methods. However, by using DTW strictly as a global distance measure, information about the matching is discarded. We show that this information can further be used as supplementary input information in temporal CNNs. This is done by using both the raw data and the features extracted from DTW in multi-modal fusion CNNs. Furthermore, we explore the effects of different prototype selection methods, prototype numbers, and data fusion schemes induce on the accuracy. We perform experiments on a wide range of time series datasets including three Unipen handwriting datasets, four UCI Machine Learning Repository datasets, and 85 UCR Time Series Classification Archive datasets.","Convolutional Neural Network, Time series classification, Dynamic time warping, Distance features",Brian {Kenji Iwana} and Seiichi Uchida,https://www.sciencedirect.com/science/article/pii/S0031320319303279,https://doi.org/10.1016/j.patcog.2019.107024,0031-3203,2020,107024,97,Pattern Recognition,Time series classification using local distance-based features in multi-modal fusion networks,article,KENJIIWANA2020107024,
"Image saliency detection has been widely explored in recent decades, but computational modeling of visual attention for video sequences is limited due to complicated temporal saliency extraction and fusion of spatial and temporal saliency. Inspired by Gestalt theory, we introduce a novel spatiotemporal saliency detection model in this study. First, we compute spatial and temporal saliency maps by low-level visual features. And then we merge these two saliency maps for spatiotemporal saliency prediction of video sequences. The spatial saliency map is calculated by extracting three kinds of features including color, luminance, and texture, while the temporal saliency map is computed by extracting motion features estimated from video sequences. A novel adaptive entropy-based uncertainty weighting method is designed to fuse spatial and temporal saliency maps to predict the final spatiotemporal saliency map by Gestalt theory. The Gestalt principle of similarity is used to estimate spatial uncertainty from spatial saliency, while temporal uncertainty is computed from temporal saliency by the Gestalt principle of common fate. Experimental results on three large-scale databases show that our method can predict visual saliency more accurately than the state-of-art spatiotemporal saliency detection algorithms.","Visual attention, Video saliency detection, Gestalt theory, Uncertainty weighting, Spatiotemporal saliency",Yuming Fang and Xiaoqiang Zhang and Feiniu Yuan and Nevrez Imamoglu and Haiwen Liu,https://www.sciencedirect.com/science/article/pii/S0031320319302900,https://doi.org/10.1016/j.patcog.2019.106987,0031-3203,2019,106987,96,Pattern Recognition,Video saliency detection by gestalt theory,article,FANG2019106987,
"How can we separate structural information from noise in large graphs? To address this fundamental question, we propose a graph summarization approach based on SzemerÃ©diâs Regularity Lemma, a well-known result in graph theory, which roughly states that every graph can be approximated by the union of a small number of random-like bipartite graphs called âregular pairsâ. Hence, the Regularity Lemma provides us with a principled way to describe the essential structure of large graphs using a small amount of data. Our paper has several contributions: (i) We present our summarization algorithm which is able to reveal the main structural patterns in large graphs. (ii) We discuss how to use our summarization framework to efficiently retrieve from a database the top-k graphs that are most similar to a query graph. (iii) Finally, we evaluate the noise robustness of our approach in terms of the reconstruction error and the usefulness of the summaries in addressing the graph search task.","Regularity lemma, Graph summarization, Structural patterns, Noise, Randomness, Graph similarity search",Marco Fiorucci and Francesco Pelosin and Marcello Pelillo,https://www.sciencedirect.com/science/article/pii/S0031320319303711,https://doi.org/10.1016/j.patcog.2019.107070,0031-3203,2020,107070,98,Pattern Recognition,Separating Structure from Noise in Large Graphs Using the Regularity Lemma,article,FIORUCCI2020107070,
"Person re-identification (re-ID) is to match different images of the same pedestrian. It has attracted increasing research interest in pattern recognition and machine learning. Traditionally, person re-ID is formulated as a metric learning problem with binary classification output. However, higher order relationship, such as triplet closeness among the instances, is ignored by such pair-wise based metric learning methods. Thus, the discriminative information hidden in these data is insufficiently explored. This paper proposes a new structured loss function to push the frontier of the person re-ID performance in realistic scenarios. The new loss function introduces two margin parameters. They operate as bounds to remove positive pairs of very small distances and negative pairs of large distances. A trade-off coefficient is assigned to the loss term of negative pairs to alleviate class-imbalance problem. By using a linear function with the margin-based objectives, the gradients w.r.t. weight matrices are no longer dependent on the iterative loss values in a multiplicative manner. This makes the weights update process robust to large iterative loss values. The new loss function is compatible with many deep learning architectures, thus, it induces new deep network with pair-pruning regularization for metric learning. To evaluate the performance of the proposed model, extensive experiments are conducted on benchmark datasets. The results indicate that the new loss together with the ResNet-50 backbone has excellent feature representation ability for person re-ID.","Metric learning, Feature extraction, Deep neural networks, Imbalance regularization, Person re-identification",Chuan-Xian Ren and Xiao-Lin Xu and Zhen Lei,https://www.sciencedirect.com/science/article/pii/S0031320319302985,https://doi.org/10.1016/j.patcog.2019.106995,0031-3203,2019,106995,96,Pattern Recognition,A Deep and Structured Metric Learning Method for Robust Person Re-Identification,article,REN2019106995,
"This paper suggests an efficient dual ergodicity limits-based bag-of-words (DEL-BoW) modeling technique. The suggested DEL-BoW technique estimates two limits of ergodicity of a discrete random variable (drv) that is formed from the BoW classification performance of multiple runs. The first limit of ergodicity is estimated with a relatively larger ball of convergence to keep the drv shorter. Hence both robustness against random initialization and estimation of the optimal model-order are realized with a reduced number of iterations. Once the optimal model-order is estimated, the radius of ball of convergence is reduced and a second limit of ergodicity is estimated. Reducing the ball of convergence enlarges the size of the considered performance drv that enhances the classification performance. Experiments conducted on Caltech-101, Caltech-256, 15-Scenes, and Flower-102 datasets resulted in classification accuracy of 86.91%, 72.57%, 90.57%, and 90.86%, respectively. Comparison with state-of-the-art techniques shows the excellent performance of the DEL-BoW modeling process.","Bag-of-words, Ergodicity, Statistical modeling, Stochastic process",Ibrahim F. Jasim Ghalyan,https://www.sciencedirect.com/science/article/pii/S0031320319303954,https://doi.org/10.1016/j.patcog.2019.107094,0031-3203,2020,107094,99,Pattern Recognition,Estimation of ergodicity limits of bag-of-words modeling for guaranteed stochastic convergence,article,GHALYAN2020107094,
"Face recognition has achieved great success owing to the fast development of deep neural networks in the past few years. Different loss functions can be used in a deep neural network resulting in different performance. Most recently some loss functions have been proposed, which have advanced the state of the art. However, they cannot solve the problem of margin bias which is present in class imbalanced datasets, having the so-called long-tailed distributions. In this paper, we propose to solve the margin bias problem by setting a minimum margin for all pairs of classes. We present a new loss function, Minimum Margin Loss (MML), which is aimed at enlarging the margin of those overclose class centre pairs so as to enhance the discriminative ability of the deep features. MML, together with Softmax Loss and Centre Loss, supervises the training process to balance the margins of all classes irrespective of their class distributions. We implemented MML in Inception-ResNet-v1 and conducted extensive experiments on seven face recognition benchmark datasets, MegaFace, FaceScrub, LFW, SLLFW, YTF, IJB-B and IJB-C. Experimental results show that the proposed MML loss function has led to new state of the art in face recognition, reducing the negative effect of margin bias.","Deep learning, Convolutional neural networks, Face recognition, Minimum margin loss",Xin Wei and Hui Wang and Bryan Scotney and Huan Wan,https://www.sciencedirect.com/science/article/pii/S0031320319303152,https://doi.org/10.1016/j.patcog.2019.107012,0031-3203,2020,107012,97,Pattern Recognition,Minimum margin loss for deep face recognition,article,WEI2020107012,
"This paper proposes a framework for classifying motion sequences, by extending the framework of Grassmann discriminant analysis (GDA). A problem of GDA is that its discriminant space is not necessarily optimal. This limitation becomes even more prominent when utilizing the subspace representation of randomized time warping (RTW). RTW is a sequence representation that can effectively model a motionâs temporal information by a low-dimensional subspace, simplifying the problem of comparing two sequences to that of comparing two subspaces. The key idea of the proposed enhanced GDA is projecting class subspaces onto a generalized difference subspace before mapping them on a Grassmann manifold. The GDS projection can remove overlapping components of the subspaces in the vector space, nearly orthogonalizing them. Consequently, a dictionary of orthogonalized class subspaces produces a set of more discriminant data points in the Grassmann manifold, in comparison with the original set. This set of data points can further enhance the discriminant ability of GDA. We demonstrate the validity of the proposed framework, RTW+eGDA, through experiments on motion recognition using the publicly available Cambridge gesture, KTH action, and UCF sports datasets.","Enhanced GDA, Randomized time warping, Motion recognition",Lincon S. Souza and Bernardo B. Gatto and Jing-Hao Xue and Kazuhiro Fukui,https://www.sciencedirect.com/science/article/pii/S0031320319303310,https://doi.org/10.1016/j.patcog.2019.107028,0031-3203,2020,107028,97,Pattern Recognition,Enhanced Grassmann discriminant analysis with randomized time warping for motion recognition,article,SOUZA2020107028,
"We propose a deep neural network based image-to-image translation for domain adaptation, which aims at finding translations between image domains. Despite recent GAN based methods showing promising results in image-to-image translation, they are prone to fail at preserving semantic information and maintaining image details during translation, which reduces their practicality on tasks such as facial expression synthesis. In this paper, we learn a framework with two training objectives: first, we propose a multi-domain image synthesis model, yielding a better recognition performance compared to other GAN based methods, with a focus on the data augmentation process; second, we explore the use of domain adaptation to transform the visual appearance of the images from different domains, with the detail of face characteristics (e.g., identity) well preserved. Doing so, the expression recognition model learned from the source domain can be generalized to the translated images from target domain, without the need for re-training a model for new target domain. Extensive experiments demonstrate that ExprADA shows significant improvements in facial expression recognition accuracy compared to state-of-the-art domain adaptation methods.","Visual domain adaptation, Facial expression recognition, Adversarial learning",Behzad Bozorgtabar and Dwarikanath Mahapatra and Jean-Philippe Thiran,https://www.sciencedirect.com/science/article/pii/S0031320319304121,https://doi.org/10.1016/j.patcog.2019.107111,0031-3203,2020,107111,100,Pattern Recognition,ExprADA: Adversarial domain adaptation for facial expression analysis,article,BOZORGTABAR2020107111,
"Person re-identification (Re-ID) requires discriminative features focusing on the full person to cope with inaccurate person bounding box detection, background clutter, and occlusion. Many recent person Re-ID methods attempt to learn such features describing full person details via part-based feature representation. However, the spatial context between these parts is ignored for the independent extractor on each separate part. In this paper, we propose to apply Long Short-Term Memory (LSTM) in an end-to-end way to model the pedestrian, seen as a sequence of body parts from head to foot. Integrating the contextual information strengthens the discriminative ability of local feature aligning better to full person. We also leverage the complementary information between local and global feature. Furthermore, we integrate both identification task and ranking task in one network, where a discriminative embedding and a similarity measurement are learned concurrently. This results in a novel three-branch framework named Deep-Person, which learns highly discriminative features for person Re-ID. Experimental results demonstrate that Deep-Person outperforms the state-of-the-art methods by a large margin on three challenging datasets including Market-1501, CUHK03, and DukeMTMC-reID.","Person Re-ID, LSTM, Triplet loss, End-to-end",Xiang Bai and Mingkun Yang and Tengteng Huang and Zhiyong Dou and Rui Yu and Yongchao Xu,https://www.sciencedirect.com/science/article/pii/S0031320319303395,https://doi.org/10.1016/j.patcog.2019.107036,0031-3203,2020,107036,98,Pattern Recognition,Deep-Person: Learning discriminative deep features for person Re-Identification,article,BAI2020107036,
"High-dimensional data usually exhibit intrinsic low-rank structures. With tremendous amount of streaming data generated by ubiquitous sensors in the world of Internet-of-Things, fast detection of such low-rank pattern is of utmost importance to a wide range of applications. In this work, we present an L1-subspace tracking method to capture the low-rank structure of streaming data. The method is based on the L1-norm principal-component analysis (L1-PCA) theory that offers outlier resistance in subspace calculation. The proposed method updates the L1-subspace as new data are acquired by sensors. In each time slot, the conformity of each datum is measured by the L1-subspace calculated in the previous time slot and used to weigh the datum. Iterative weighted L1-PCA is then executed through a refining function. The superiority of the proposed L1-subspace tracking method compared to existing approaches is demonstrated through experimental studies in various application fields.","Dimensionality reduction, Eigenvector decomposition, Internet-of-Things, -norm, Outliers, Principal-component analysis, Subspace learning",Ying Liu and Konstantinos Tountas and Dimitris A. Pados and Stella N. Batalama and Michael J. Medley,https://www.sciencedirect.com/science/article/pii/S003132031930295X,https://doi.org/10.1016/j.patcog.2019.106992,0031-3203,2020,106992,97,Pattern Recognition,L1-Subspace Tracking for Streaming Data,article,LIU2020106992,
"Measuring distance among data point pairs is a necessary step among numerous counts of algorithms in machine learning, pattern recognition and data mining. In the local perspective, the emphasis of all existing supervised metric learning algorithms is to shrink similar data points and to separate dissimilar ones in the local neighborhoods. This provides learning more appropriate distance metric in dealing with the within-class multi modal data. In this article, a new supervised local metric learning method named Self-Adaptive Local Metric Learning Method (SA-LM2) has been proposed. The contribution of this method is in five aspects. First, in this method, learning an appropriate metric and defining the radius of local neighborhood are integrated in a joint formulation. Second, unlike the traditional approaches, SA-LM2 learns the parameter of local neighborhood automatically thorough its formulation. As a result, it is a parameter free method, where it does not require any parameters that would need to be tuned. Third, SA-LM2 is formulated as a SemiDefinite Program (SDP) with a global convergence guarantee. Fourth, this method does not need the similar set S, the focus here is on local areasâ data points and their separation from dissimilar ones. Finally, results of SA-LM2 are less influenced by noisy input data points than the other compared global and local algorithms. Results obtained from different experiments indicate the outperformance of this algorithm over its counterparts.","Supervised metric learning, Linear metric learning, Local algorithm, Neighborhood learning, Multimodal data",Mahsa Taheri and Zahra Moslehi and Abdolreza Mirzaei and Mehran Safayani,https://www.sciencedirect.com/science/article/pii/S0031320319302973,https://doi.org/10.1016/j.patcog.2019.106994,0031-3203,2019,106994,96,Pattern Recognition,A self-adaptive local metric learning method for classification,article,TAHERI2019106994,
"There is nowadays an increasing interest in discovering relationships among input variables (also called features) from data to provide better interpretability, which yield more confidence in the solution and provide novel insights about the nature of the problem at hand. We propose a novel feature selection method, called Informative Variable Identifier (IVI), capable of identifying the informative variables and their relationships. It transforms the input-variable space distribution into a coefficient-feature space using existing linear classifiers or a more efficient weight generator that we also propose, Covariance Multiplication Estimator (CME). Informative features and their relationships are determined analyzing the joint distribution of these coefficients with resampling techniques. IVI and CME select the informative variables and then pass them on to any linear or nonlinear classifier. Experiments show that the proposed approach can outperform state-of-art algorithms in terms of feature identification capabilities, and even in classification performance when subsequent classifiers are used.","Feature selection, Interpretability, Explainable machine learning, Resampling, Classification",Sergio MuÃ±oz-Romero and Arantza Gorostiaga and Cristina Soguero-Ruiz and Inmaculada Mora-JimÃ©nez and JosÃ© Luis Rojo-Ãlvarez,https://www.sciencedirect.com/science/article/pii/S0031320319303784,https://doi.org/10.1016/j.patcog.2019.107077,0031-3203,2020,107077,98,Pattern Recognition,Informative variable identifier: Expanding interpretability in feature selection,article,MUNOZROMERO2020107077,
"In current graph embedding methods, low dimensional projections are obtained by preserving either global geometrical structure of data or local geometrical structure of data. In this paper, the PCA (Principal Component Analysis) idea of minimizing least-squares reconstruction errors is regularized with graph embedding, to unify various local manifold embedding methods within a generalized framework to keep global and local low dimensional subspace. Different from the well-known PCA method, our proposed generalized least-squares approach considers data distributions together with an instance penalty in each data point. In this way, PCA is viewed as a special instance of our proposed generalized least squares framework for preserving global projections. Applying a regulation of graph embedding, we can obtain projection that preserves both intrinsic geometrical structure and global structure of data. From the experimental results on a variety of face and handwritten digit recognition, our proposed method has advantage of superior performances in keeping lower dimensional subspaces and higher classification results than state-of-the-art graph embedding methods.","Dimensionality reduction, Graph embedding, Subspace learning, Least-squares",Xiang-Jun Shen and Si-Xing Liu and Bing-Kun Bao and Chun-Hong Pan and Zheng-Jun Zha and Jianping Fan,https://www.sciencedirect.com/science/article/pii/S0031320319303267,https://doi.org/10.1016/j.patcog.2019.107023,0031-3203,2020,107023,98,Pattern Recognition,A generalized least-squares approach regularized with graph embedding for dimensionality reduction,article,SHEN2020107023,
"In several real-world classification problems it can be impractical to collect samples from classes other than the one of interest, hence the need for classifiers trained on a single class. There is a rich literature concerning binary and multi-class time series classification but less concerning one-class learning. In this study, we investigate the little-explored one-class time series classification problem. We represent time series as vectors of dissimilarities from a set of time series referred to as prototypes. Based on this approach, we evaluate a Cartesian product of 12 dissimilarity measures, and 8 prototype methods (strategies to select prototypes). Finally, a one-class nearest neighbor classifier is used on the dissimilarity-based representations (DBR). Experimental results show that DBR are competitive overall when compared with a strong baseline on the data-sets of the UCR/UEA archive. Additionally, DBR enable dimensionality reduction, and visual exploration of data-sets.","Dissimilarity-based representations, One-class classification, Time series",Stefano Mauceri and James Sweeney and James McDermott,https://www.sciencedirect.com/science/article/pii/S0031320319304236,https://doi.org/10.1016/j.patcog.2019.107122,0031-3203,2020,107122,100,Pattern Recognition,Dissimilarity-based representations for one-class classification on time series,article,MAUCERI2020107122,
"Compressed sensing (CS) is a new technology to reconstruct image from randomized measurements, but the reconstruction procedure involves a time-consuming iterative optimization. In addition, the reconstruction quality becomes poor in low sampling rate. In order to alleviate these issues of the conventional CS image reconstruction, we propose a novel sub-pixel convolutional generative adversarial network (GAN) to learn compressed sensing reconstruction of images. The generator constructs the sub-pixel convolutional network to learn the explicit mapping from the low-dimensional measurement vector to the high-dimensional reconstruction, in which a compound loss, including reconstruction loss, measurement loss and adversarial loss, is designed to guide the network learning. By means of the adversarial training with discriminator, the generator can learn the inherent image distribution and improve the reconstruction quality. Moreover, the test image can be fast reconstructed by simply passing the low-dimensional measurement vector through the generator network. The proposed algorithm is tested on MNIST, F-MNIST and CelebA datasets, and the experimental results show that it is superior to some state-of-the-art deep learning based and iterative optimization based algorithms, in terms of both time complexity and reconstruction quality.","Compressed sensing, Sub-pixel convolutional GAN, Compound loss",Yubao Sun and Jiwei Chen and Qingshan Liu and Guangcan Liu,https://www.sciencedirect.com/science/article/pii/S003132031930353X,https://doi.org/10.1016/j.patcog.2019.107051,0031-3203,2020,107051,98,Pattern Recognition,Learning image compressed sensing with sub-pixel convolutional generative adversarial network,article,SUN2020107051,
"Writing is an important basic skill for humans. To acquire such a skill, pupils often have to practice writing for several hours each day. However, different pupils usually possess distinct writing postures. Bad postures not only affect the speed and quality of writing, but also severely harm the healthy development of pupilsâ spine and eyesight. Therefore, it is of key importance to identify or predict pupilsâ writing postures and accordingly correct bad ones. In this paper, we formulate the problem of handwriting posture prediction for the first time. Further, we propose a neural network constructed with small convolution kernels to extract features from handwriting, and incorporate unsupervised learning and handwriting data analysis to predict writing postures. Extensive experiments reveal that our approach achieves an accuracy rate of 93.3%, which is significantly higher than the 76.67% accuracy of human experts.","Pupils, Writing posture prediction, Features extracting, Neural network model, Unsupervised learning, Data analysis",Bailin Yang and Yulong Zhang and Zhenguang Liu and Xiaoheng Jiang and Mingliang Xu,https://www.sciencedirect.com/science/article/pii/S0031320319303942,https://doi.org/10.1016/j.patcog.2019.107093,0031-3203,2020,107093,100,Pattern Recognition,Handwriting posture prediction based on unsupervised model,article,YANG2020107093,
"Subspace is widely used to represent objects under different viewpoints, illuminations, identities, and more. Due to the growing amount and dimensionality of visual contents, fast search in a large-scale database with high-dimensional subspaces is an important task in many applications, such as image retrieval, clustering, video retrieval, and visual recognition. This can be facilitated by approximate nearest subspace (ANS) search which requires effective subspace representation. All existing methods for this problem represent a subspace by a point in the Euclidean or the Grassmannian space before applying the approximate nearest neighbor (ANN) search. However, the efficiency of these methods is not guaranteed because the subspace representation step can be very time consuming when coping with high-dimensional data. Moreover, the subspace to point transforming process may cause subspace structural information loss which influences the search accuracy. In this paper, we present a new approach for hashing-based ANS search which can directly binarize a subspace without transforming it into a vector. The proposed method learns the binary codes for subspaces following a similarity preserving criterion, and simultaneously leverages the learned binary codes to train matrix classifiers as hash functions. Experiments on face and action recognition and video retrieval applications show that our method outperforms several state-of-the-art methods in both efficiency and accuracy. Moreover, we also compare our method with vector-based hashing methods. Results also show the superiority of our subspace matrix based search scheme.","Nearest subspace search, Learning binary code, Hashing, Matrix classifier",Lei Zhou and Xiao Bai and Xianglong Liu and Jun Zhou and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320319303425,https://doi.org/10.1016/j.patcog.2019.107040,0031-3203,2020,107040,98,Pattern Recognition,Learning binary code for fast nearest subspace search,article,ZHOU2020107040,
"Shape analysis is important in anthropology, bioarchaeology and forensic science for interpreting useful information from human remains. In particular, teeth are morphologically stable and hence well-suited for shape analysis. In this work, we propose a framework for tooth morphometry using quasi-conformal theory. Landmark-matching TeichmÃ¼ller maps are used for establishing a 1-1 correspondence between tooth surfaces with prescribed anatomical landmarks. Then, a quasi-conformal statistical shape analysis model based on the TeichmÃ¼ller mapping results is proposed for building a tooth classification scheme. We deploy our framework on a dataset of human premolars to analyze the tooth shape variation among genders and ancestries. Experimental results show that our method achieves much higher classification accuracy with respect to both gender and ancestry when compared to the existing methods. Furthermore, our model reveals the underlying tooth shape difference between different genders and ancestries in terms of the local geometric distortion and curvatures. In particular, our experiment suggests that the shape difference between genders is mostly captured by the conformal distortion but not the curvatures, while that between ancestries is captured by both of them.","Tooth morphometry, Quasi-conformal theory, Shape analysis, TeichmÃ¼ller map, Ancestry, Sexual dimorphism, Classification",Gary P.T. Choi and Hei Long Chan and Robin Yong and Sarbin Ranjitkar and Alan Brook and Grant Townsend and Ke Chen and Lok Ming Lui,https://www.sciencedirect.com/science/article/pii/S0031320319303668,https://doi.org/10.1016/j.patcog.2019.107064,0031-3203,2020,107064,99,Pattern Recognition,Tooth morphometry using quasi-conformal theory,article,CHOI2020107064,
"The irregularity of human actions poses great challenges in video action recognition. Recently, 3D ConvNet methods have shown promising performance at modelling the motion and appearance information. However, the fixed geometric structure of 3D convolution filters largely limits the learning capacity for video action recognition. To address this problem, this paper proposes a spatio-temporal deformable ConvNet module with an attention mechanism, which takes into consideration the mutual correlations in both temporal and spatial domains, to effectively capture the long-range and long-distance dependencies in the video actions. Our attention based deformable module, as a generic module for 3D ConvNets, can adaptively learn more accurate spatio-temporal offsets to model the action irregularity. The experiments on two popular datasets (UCF-101 and HMDB-51) demonstrate that our module significantly outperforms the state-of-the-art methods.","Action recognition, Spatio-temporal deformable, Attention mechanism, 3D ConvNets",Jun Li and Xianglong Liu and Mingyuan Zhang and Deqing Wang,https://www.sciencedirect.com/science/article/pii/S0031320319303383,https://doi.org/10.1016/j.patcog.2019.107037,0031-3203,2020,107037,98,Pattern Recognition,Spatio-temporal deformable 3D ConvNets with attention for action recognition,article,LI2020107037,
"Strict â0-1â block-diagonal structure has been widely used for learning structured representation in face recognition problems. However, it is questionable and unreasonable to assume the within-class representations are the same. To circumvent this problem, in this paper, we propose a slack block-diagonal (SBD) structure for representation where the target structure matrix is dynamically updated, yet its blockdiagonal nature is preserved. Furthermore, in order to depict the noise in face images more precisely, we propose a robust dictionary learning algorithm based on mixed-noise model by utilizing the above SBD structure (SBD2L). SBD2L considers that there exists two forms of noise in data which are drawn from Laplacian and Gaussion distribution, respectively. Moreover, SBD2L introduces a low-rank constraint on the representation matrix to enhance the dictionaryâs robustness to noise. Extensive experiments on four benchmark databases show that the proposed SBD2L can achieve better classification results than several state-of-the-art dictionary learning methods.","Face recognition, Low-rank representation, Noise-robust dictionary learning, Slack block-diagonal structure",Zhe Chen and Xiao-Jun Wu and He-Feng Yin and Josef Kittler,https://www.sciencedirect.com/science/article/pii/S0031320319304194,https://doi.org/10.1016/j.patcog.2019.107118,0031-3203,2020,107118,100,Pattern Recognition,Noise-robust dictionary learning with slack block-Diagonal structure for face recognition,article,CHEN2020107118,
"In the following study, an innovative supervised dimensionality reduction technique is proposed. dCor-based Dimensionality Reduction or dDR technique is based on distance correlation; a powerful correlation measure which is applicable to arbitrary-dimensional random variables. By projecting the samples to a lower dimensional space, dDR maximizes the correlation between explanatory and response variables. The proposed dDR algorithm can be easily implemented and it is computationally efficient. Moreover, it has a closed-form and a simple solution which makes it significantly effective in many different applications. In order to apply the proposed technique on non-linear problems, the kernel version of the dDR is also derived. Extensive analyses and empirical experiments across various visualization, classification, and regression tasks indicate that our algorithm is the method of choice; as it offers statistically superior results in comparison with other state-of-the-art approaches in the literature.","Dimensionality reduction, Distance correlation (dCor), Kernel methods, Classification, Regression",Lida Abdi and Ali Ghodsi,https://www.sciencedirect.com/science/article/pii/S0031320319303541,https://doi.org/10.1016/j.patcog.2019.107052,0031-3203,2020,107052,98,Pattern Recognition,Discriminant component analysis via distance correlation maximization,article,ABDI2020107052,
"Attribute reduction is a key problem in many areas such as data mining, pattern recognition, machine learning. The problems of finding all reducts as well as finding a minimal reduct in a given data table have been proved to be NP-hard. Therefore, to overcome this difficulty, many heuristic attribute reduction methods have been developed in recent years. In the process of heuristic attribute reduction, accelerating calculation of attribute significance is very important, especially for big data cases. In this paper, we firstly propose attribute significance measures based on stripped quotient sets. Then, by using these measures, we design efficient algorithms for calculating core and reduct, in which the time complexity will be considered in detail. Additionally, we will also give properties directly related to efficiently computing the attribute significance and significantly reducing the data size in the process of calculation. By theoretical and experimental views, we will show that our method can perform efficiently for large-scale data sets.","Rough set, Attribute reduction, Core attributes, Stripped quotient set, Attribute significance measure",Nguyen Ngoc Thuy and Sartra Wongthanavasu,https://www.sciencedirect.com/science/article/pii/S0031320319303024,https://doi.org/10.1016/j.patcog.2019.106999,0031-3203,2020,106999,97,Pattern Recognition,A new approach for reduction of attributes based on stripped quotient sets,article,THUY2020106999,
"This paper develops a novel framework for a family of correlation classifiers that are reconstructed from uncertain convex programs under data perturbation. Under this framework, correlation classifiers are exploited from the pessimistic viewpoint under possible perturbation of data, and the max-min optimization problem is formulated by simplifying the original model in terms of adaptive uncertainty regions. The proposed model can be formulated as a minimization problem under proper conditions. The proximal majorization-minimization optimization (PMMO) based on Bregman divergences is devised to solve the proposed model that may be nonconvex or nonsmooth. It is found that using PMMO to solve the proposed model can exploit the convergence rate of the solution sequence in the nonconvex case. In the case of specific functions we can use the accelerated versions of first-order methods to solve the proposed model with convexity in order to make them have fast convergence rates in terms of the objective function. Extensive experiments on some data sets are conducted to demonstrate the feasibility and validity of the proposed model.","Correlation classifiers, data perturbation,  divergence, PMMO, Data classification",Zhizheng Liang and Xuewen Chen and Lei Zhang and Jin Liu and Yong Zhou,https://www.sciencedirect.com/science/article/pii/S0031320319304078,https://doi.org/10.1016/j.patcog.2019.107106,0031-3203,2020,107106,100,Pattern Recognition,Correlation classifiers based on data perturbation: New formulations and algorithms,article,LIANG2020107106,
"Clustering is among the tools for exploring, analyzing, and deriving information from data. In the case of large data sets, the real burden to the application of clustering algorithms can be their complexity and demand of control parameters. We present a new fast nonparametric clustering algorithm, UNIC, to address these challenges. To identify clusters, the algorithm evaluates the distances between selected points and other points in the set. While assessing these distances, it employs methods of robust statistics to identify the cluster borders. The performance of the proposed algorithm is assessed in an experimental study and compared with several existing clustering methods over a variety of benchmark data sets.","Cluster analysis, Hard (conventional, crisp) clustering, Nonparametric algorithms, Data mining, Big data",Nadiia Leopold and Oliver Rose,https://www.sciencedirect.com/science/article/pii/S0031320319304182,https://doi.org/10.1016/j.patcog.2019.107117,0031-3203,2020,107117,100,Pattern Recognition,UNIC: A fast nonparametric clustering,article,LEOPOLD2020107117,
"The multi-class classification is the problem of classifying the sample into one of three or more classes. In this paper, we propose an algorithm named collaborative and geometric multi-kernel learning (CGMKL) to classify multi-class data into corresponding class directly. The CGMKL uses the Multiple Empirical Kernel Learning (MEKL) to map the sample into multiple kernel spaces, and then trains the softmax function in each kernel space. To realize the collaborative learning, one regularization term, which controls the consistent outputs of samples in different kernel spaces, provides the complementary information. Moreover, another regularization term exhibits the classification result with a geometric feature by reducing the within-class distance of the outputs of samples. Extensive Experiments on the multi-class data sets validate the effectiveness of the CGMKL.","Multi-class classification, Empirical kernel mapping, Multiple empirical kernel learning, Regularized learning",Zhe Wang and Zonghai Zhu and Dongdong Li,https://www.sciencedirect.com/science/article/pii/S0031320319303528,https://doi.org/10.1016/j.patcog.2019.107050,0031-3203,2020,107050,99,Pattern Recognition,Collaborative and geometric multi-kernel learning for multi-class classification,article,WANG2020107050,
"Machine learning-based intelligent fault diagnosis methods have gained extensive popularity and been widely investigated. However, in previous works, a major assumption accepted by default is that the training and testing datasets share the same distribution. Unfortunately, this assumption is mostly invalid in real-world applications for working condition variation of rotating machine can cause the distribution discrepancy between datasets easily, which results in performance degeneration of traditional diagnosis methods. Aiming at it, although some deep learning and transfer learning-based methods are proposed and validated effective recently, the dataset distribution alignments of them mainly focus on marginal distribution alignments, which are not powerful enough in some scenarios. Hence, a novel distribution discrepancy evaluating method called auto-balanced high-order KullbackâLeibler (AHKL) divergence is proposed, which can evaluate both the first and higher-order moment discrepancies and adapt the weights between them dimensionally and automatically. Meanwhile, smooth conditional distribution alignment (SCDA) is also developed, which performs excellently in aligning the conditional distributions through introducing soft labels instead of adopting widely-used pseudo labels. Furthermore, based on AHKL divergence and SCDA, weighted joint distribution alignment (WJDA) is developed for comprehensive joint distribution alignments. Finally, built on WJDA, we construct a novel deep transfer network (DTN) for rotating machine fault diagnosis with working condition variation. Extensive experimental evaluations through 18 transfer learning cases demonstrate its validity, and further comparisons with the state of the arts also validate its superiority.","Intelligent fault diagnosis, Rotating machine, Deep transfer network, Auto-balanced high-order KL divergence, Smooth conditional distribution alignment, Weighted joint domain adaptation",Weiwei Qian and Shunming Li and Xingxing Jiang,https://www.sciencedirect.com/science/article/pii/S0031320319302961,https://doi.org/10.1016/j.patcog.2019.106993,0031-3203,2019,106993,96,Pattern Recognition,Deep transfer network for rotating machine fault analysis,article,QIAN2019106993,
"In this paper we address the problem of continuous fine-grained action segmentation, in which multiple actions are present in an unsegmented video stream. The challenge for this task lies in the need to represent the hierarchical nature of the actions and to detect the transitions between actions, allowing us to localise the actions within the video effectively. We propose a novel recurrent semi-supervised Generative Adversarial Network (GAN) model for continuous fine-grained human action segmentation. Temporal context information is captured via a novel Gated Context Extractor (GCE) module, composed of gated attention units, that directs the queued context information through the generator model, for enhanced action segmentation. The GAN is made to learn features in a semi-supervised manner, enabling the model to perform action classification jointly with the standard, unsupervised, GAN learning procedure. We perform extensive evaluations on different architectural variants to demonstrate the importance of the proposed network architecture, and show that it is capable of outperforming current state-of-the-art on three challenging datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric Activities dataset.","Human action segmentation, Generative adversarial networks, Context modelling",Harshala Gammulle and Simon Denman and Sridha Sridharan and Clinton Fookes,https://www.sciencedirect.com/science/article/pii/S0031320319303413,https://doi.org/10.1016/j.patcog.2019.107039,0031-3203,2020,107039,98,Pattern Recognition,Fine-grained action segmentation using the semi-supervised action GAN,article,GAMMULLE2020107039,
"The determination of clustering centers generally depends on the observation scale that we use to analyze the data to be clustered. An inappropriate scale usually leads to unreasonable clustering centers and thus unreasonable results. In this study, we first consider the similarity of elements in the data as the connectivity of vertices in an undirected graph, then present the concept of connection center and regard it as the clustering center of the data. Based on this definition, the determination of clustering centers and the assignment of class become very simple, natural and effective. One more crucial finding is that the clustering centers of different scales can be obtained easily by different powers of a similarity matrix, and the change of power from small to large leads to the dynamic evolution of clustering centers from local (microscopic) to global (macroscopic). Further, in this process of evolution, the number of clusters changes discontinuously, which means that the presented method can automatically skip the unreasonable number of clusters, suggest appropriate observation scales and provide corresponding clustering results.","Clustering center, Clustering, Connected graph, Connectivity",Xiurui Geng and Hairong Tang,https://www.sciencedirect.com/science/article/pii/S0031320319303656,https://doi.org/10.1016/j.patcog.2019.107063,0031-3203,2020,107063,98,Pattern Recognition,Clustering by connection center evolution,article,GENG2020107063,
"Medical image registration is an important task in automated analysis of multimodal images and temporal data involving multiple patient visits. Conventional approaches, although useful for different image types, are time consuming. Of late, deep learning (DL) based image registration methods have been proposed that outperform traditional methods in terms of accuracy and time. However, DL based methods are heavily dependent on training data and do not generalize well when presented with images of different scanners or anatomies. We present a DL based approach that can perform medical image registration of one image type despite being trained with images of a different type. This is achieved by unsupervised domain adaptation in the registration process and allows for easier application to different datasets without extensive retraining. To achieve our objective we train a network that transforms the given input image pair to a latent feature space vector using autoencoders. The resultant encoded feature space is used to generate the registered images with the help of generative adversarial networks (GANs). This feature transformation ensures greater invariance to the input image type. Experiments on chest X-ray, retinal and brain MR images show that our method, trained on one dataset gives better registration performance for other datasets, outperforming conventional methods that do not incorporate domain adaptation.","Registration, Domain adaptation, GANs, X-ray, MRI",Dwarikanath Mahapatra and Zongyuan Ge,https://www.sciencedirect.com/science/article/pii/S0031320319304108,https://doi.org/10.1016/j.patcog.2019.107109,0031-3203,2020,107109,100,Pattern Recognition,Training data independent image registration using generative adversarial networks and domain adaptation,article,MAHAPATRA2020107109,
"In this paper, we propose a generic method for the detection of line segments in SAR images. The approach relies on an a contrario framework and is inspired by the state-of-the art LSD detector. As with all a contrarioapproaches, false detections are controlled through the use of a background model, whose development is especially challenging in the framework of SAR images. Indeed, statistical characteristics of SAR images strongly differ from those of optical images, making the use of existing background models intrinsically inadequate. In order to circumvent this problem, we proceed in two steps. First, the building blocks of the detector, namely the local orientations, are computed carefully to avoid any spatial bias. Second, we propose a new background model, in which the spatial dependency between local orientations are modeled with a Markov chain. This is in strong contrast with most existing a contrario methods who heavily rely on independence assumptions. We provide a complete and detailed algorithm for our line segment detector, and perform experiments on synthetic and real images demonstrating its efficiency.","Line segments, SAR images, A contrario models, Markov chain, Local orientations",Chenguang Liu and RÃ©my Abergel and Yann Gousseau and Florence Tupin,https://www.sciencedirect.com/science/article/pii/S0031320319303371,https://doi.org/10.1016/j.patcog.2019.107034,0031-3203,2020,107034,98,Pattern Recognition,"LSDSAR, a Markovian a contrario framework for line segment detection in SAR images",article,LIU2020107034,
"In this paper, we present a novel method for salient object detection in videos. Salient object detection methods based on background prior may miss salient region when the salient object touches the frame borders. To solve this problem, we propose to detect the whole salient object via the adjunction of virtual borders. A guided filter is then applied on the temporal output to integrate the spatial edge information for a better detection of the salient object edges. At last, a global spatio-temporal saliency map is obtained by combining the spatial saliency map and the temporal saliency map together according to the entropy. The proposed method is assessed on three popular datasets (Fukuchi, FBMS and VOS) and compared to several state-of-the-art methods. The experimental results show that the proposed approach outperforms the tested methods.","Video salient object detection, Distance transform, Guided filter, Global motion",Qiong Wang and Lu Zhang and Wenbin Zou and Kidiyo Kpalma,https://www.sciencedirect.com/science/article/pii/S0031320319303012,https://doi.org/10.1016/j.patcog.2019.106998,0031-3203,2020,106998,97,Pattern Recognition,Salient video object detection using a virtual border and guided filter,article,WANG2020106998,
"Extensive eye researches provide good results when images are captured under constrained environment. However, the accuracy of eye landmarks detection depends on explicit bounding-box of eye regions and drops severely in non-ideal conditions. This paper has proposed a novel weakly supervised eye landmarks detection algorithm with object detection and recurrent learning modules. The former is combined with faster R-CNN and is competent to detect bounding-box of facial components and initial positions of the eye. The recurrent module is employed for eye landmarks refinement using the initial eye shape. The proposed algorithm can augment training data effectively and our specific format data consist of supervised and weakly supervised samples. Supervised samples have ground truth of bounding-boxes, corresponding classification labels and eye landmarks coordinates while weakly supervised data does not have eye landmarks information. Despite trained on facial images, the proposed method can detect eyes in severely occluded or local view of facial images without prerequisites of face alignment. Further experiments are performed on our supervised testing set and some public datasets. Their results demonstrate the robustness and effectiveness of the proposed method.","Eye landmarks detection, Special format data, Weakly supervised learning, Object detection, Recurrent learning module",Bin Huang and Renwen Chen and Qinbang Zhou and Wang Xu,https://www.sciencedirect.com/science/article/pii/S0031320319303772,https://doi.org/10.1016/j.patcog.2019.107076,0031-3203,2020,107076,98,Pattern Recognition,Eye landmarks detection via weakly supervised learning,article,HUANG2020107076,
"In the paper, we present a circle detector that achieves the state-of-art performance in almost every type of image. The detector represents each circle instance by a set of equally distributed arcs and searches for the same number of edge points to cover these arcs. The new formulation leads to the voting in minimizing/maximizing way which is different from the typical accumulative way adopted by Hough transform. From the formulation, circle detection is then decomposed into radius-dependent and -independent part. The calculation of independent part is computationally expensive but shared by different radii. This decomposition gets rid of the redundant computation in handling multiple radii and therefore speeds up the detection process. Originated from the sparse nature of independent part, we design a sparse structure for its batch computation which is fulfilled in just one sweep of the edge points. Circle detector based on this sparse structure is then proposed which achieves the comparable time complexity as the algorithm based on Hough transform using 2D accumulator array. For testing, we created an information-rich dataset with images coming from multiple sources. It contains five categories and covers a wide spectrum of images, ranging from true color images to the binary ones. The experimental results demonstrate that the proposed approach outperforms the solutions based on accumulative voting.","Circle detection, Hough transform, Voting, Sparse structure, Oriented chamfer distance",Yuanqi Su and Xiaoning Zhang and Bonan Cuan and Yuehu Liu and Zehao Wang,https://www.sciencedirect.com/science/article/pii/S0031320319303255,https://doi.org/10.1016/j.patcog.2019.107022,0031-3203,2020,107022,97,Pattern Recognition,A sparse structure for fast circle detection,article,SU2020107022,
"Steganalysis is a technique that detects the presence of secret information in multimedia data. Many steganalysis algorithms have been proposed with high detection accuracy; however, the difference in statistical distribution between training and testing sets can cause mismatch problems, which will degrade the performance of traditional steganalysis algorithms. To solve this problem, we propose a transferable heterogeneous feature subspace learning (THFSL) algorithm for JPEG mismatched steganalysis. Our approach considers the feature space in each domain as a combination of the domain-independent features and the domain-related features. We use the transformation matrix to transfer both the domain-independent and domain-related features from the source and target domains to a common feature subspace, where each target sample can be better represented by a combination of source samples. By imposing low-rank constraints on the domain-independent features, the structures of data can be preserved, which can capture the intrinsic structures for discriminating cover and stego images. Our method can avoid a potentially negative transfer by using a sparse matrix to model the domain-related features and, thus, is more robust to different domain changes in mismatched steganalysis. Extensive experiments on various mismatched steganalysis tasks show the superiority of the proposed method over the state-of-the art methods.","Mismatched steganalysis, Heterogeneous subspace, Domain-independent features, Domain-related features, Transfer learning",Ju Jia and Liming Zhai and Weixiang Ren and Lina Wang and Yanzhen Ren and Lefei Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319304066,https://doi.org/10.1016/j.patcog.2019.107105,0031-3203,2020,107105,100,Pattern Recognition,Transferable heterogeneous feature subspace learning for JPEG mismatched steganalysis,article,JIA2020107105,
"One of the main task for deep sea submersible is for human-machine collaborative scientific exploration, e.g., human ourselves drive the submersible and monitor cameras around the submersible to observe new species fish or strange topography in a tedious way. In this paper, by defining novel marine animals or any extreme events as novel events, we design a new deep sea novel visual event analysis framework to improve the efficiency of human-machine collaboration and improve the accuracy simultaneously. Specifically, our visual framework concerns diverse functions than most state-of-the-arts, including novel event detection, tracking and summarization. Due to the power and computation resource limitation of the submersible, we design an efficient deep learning based visual saliency method for novel event detection and propose an online object tracking strategy as well. All the experiments are depending on Chinese Jiaolong, the manned deep sea submersible, which mounts several PanCtiltCzoom (PTZ) camera and static cameras. We build a new novel deep sea event dataset and the results justify that our human-machine collaborative visual observation framework can automatically detect, track and summarize the novel deep sea event.","Underwater, Underwater robot, Visual summarization, Visual saliency, Visual tracking, Robot vision, Video analysis, Novel event, Deep sea",Yang Cong and Baojie Fan and Dongdong Hou and Huijie Fan and Kaizhou Liu and Jiebo Luo,https://www.sciencedirect.com/science/article/pii/S0031320319302705,https://doi.org/10.1016/j.patcog.2019.106967,0031-3203,2019,106967,96,Pattern Recognition,Novel event analysis for human-machine collaborative underwater exploration,article,CONG2019106967,
"Person re-identification is a very challenging task in computer vision due to severe appearance changes of a person across non-overlapping camera views. It is thus inadequate in most realistic re-identification scenarios to assess similarity using a single metric in a single feature space. Ensemble methods have been proven effective on improving the recognition rate. However, robust combination remains challenging due to the incompatibilities between different distance metrics. In this paper, we propose a novel framework for person re-identification called Boosting Ranking Ensemble (BRE), which adaptively assembles features and metrics from a ranking perspective. To further tackle the overfitting problem, we explore an effective rectifier loss in the proposed BRE framework to alleviate the influence of the misclassified samples. Extensive experimental analyses and evaluations on three commonly adopted benchmarks demonstrate the effectiveness of the proposed method, with superior performance over many state-of-the-art methods.","Person re-identification, Boosting, Ensemble",Zhaoju Li and Zhenjun Han and Junliang Xing and Qixiang Ye and Xuehui Yu and Jianbin Jiao,https://www.sciencedirect.com/science/article/pii/S0031320319301979,https://doi.org/10.1016/j.patcog.2019.05.022,0031-3203,2019,187--195,94,Pattern Recognition,High performance person re-identification via a boosting ranking ensemble,article,LI2019187,
"Graph convolutional neural networks have aroused more and more attentions on account of the ability to handle the graph-structured data defined on irregular or non-Euclidean domains. Different from the data defined on regular grids, each node in the graph-structured data has different number of neighbors, and the interactions and correlations between nodes vary at different locations, resulting in complex graph structure. However, the existing graph convolutional neural networks generally pay little attention to exploiting the graph structure information. Moreover, most existing graph convolutional neural networks employ the weight sharing strategy which lies on the statistical assumption of stationarity. This assumption is not always verified on the graph-structured data. To address these issues, we propose a method that learns Graph Structure via graph Convolutional Networks (GSCN), which introduces the graph structure parameters measuring the correlation degrees of adjacent nodes. The graph structure parameters are constantly modified the graph structure during the training phase and will help the filters of the proposed method to focus on the relevant nodes in each neighborhood. Meanwhile by combining the graph structure parameters and kernel weights, our method, which relaxes the restriction of weight sharing, is better to handle the graph-structured data of non-stationarity. In addition, the non-linear activation function ReLU and the sparse constraint are employed on the graph structure parameters to promote GSCN to focus on the important links and filter out the insignificant links in each neighborhood. Experiments on various tasks, including text categorization, molecular activity detection, traffic forecasting and skeleton-based action recognition, illustrate the validity of our method.","Deep learning, Graph convolutional neural networks, Graph structure learning, Changeable kernel sizes",Qi Zhang and Jianlong Chang and Gaofeng Meng and Shibiao Xu and Shiming Xiang and Chunhong Pan,https://www.sciencedirect.com/science/article/pii/S0031320319302432,https://doi.org/10.1016/j.patcog.2019.06.012,0031-3203,2019,308--318,95,Pattern Recognition,Learning graph structure via graph convolutional networks,article,ZHANG2019308,
"Face detection techniques have been developed for decades, and one of the remaining open challenges is detecting small faces in unconstrained conditions. The reason is that tiny faces are often lacking detailed information and blurry. In this paper, we proposed an algorithm to directly generate a clear high-resolution face from a small blurry one by adopting a generative adversarial network (GAN). Toward this end, the basic GAN formulation achieves it by super-resolving and refining sequentially (e.g.Â SR-GAN and Cycle-GAN). However, we design a novel network to address the problem of super-resolving and refining jointly. Moreover, we also introduce new training losses (i.e.Â classification loss and regression loss) to promote the generator network to recover fine details of the small faces and to guide the discriminator network to distinguish face vs.Â non-face and to refine location simultaneously. Additionally, considering the importance of contextual information when detecting tiny faces in crowded cases, the context around face regions is combined to train the proposed GAN-based network for mining those very small faces from unconstrained scenarios. Extensive experiments on the challenging datasets WIDER FACE and FDDB demonstrate the effectiveness of the proposed method in restoring a clear high-resolution face from a small blurry one, and show that the achieved performance outperforms previous state-of-the-art methods by a large margin.","Face detection, Tiny faces, Super-resolution, Generative adversarial network, Contextual information",Yongqiang Zhang and Mingli Ding and Yancheng Bai and Bernard Ghanem,https://www.sciencedirect.com/science/article/pii/S0031320319301980,https://doi.org/10.1016/j.patcog.2019.05.023,0031-3203,2019,74--86,94,Pattern Recognition,Detecting small faces in the wild based on generative adversarial network and contextual information,article,ZHANG201974,
"We introduce a method for selecting a small subset of informative, non-redundant predictors from a set of input variables, given an output variable. The core of this method is a novel measure of variable importance, which is an enhancement of the so-called âconditional permutation importanceâ (CPI). In CPI, the importance of an input variable is measured by the expected increase of a random forest (RF)âs prediction error when such variable is randomly permuted within certain groups of observations. While CPI obtains these groups from the stochastic recursive partitions that the RF carries out on the input space, our measure relies on a new approach that groups observations by means of a special form of clustering, which optimally leverages the structure of dependences existing between input variables. We show that our measure can be effectively used to recursively eliminate both unimportant and redundant input variables. Extensive experimental results illustrate the effectiveness of our method in comparison with many RF-based methods for variable selection.","Variable selection, Random forest, Permutation importance, Regression, Classification, Clustering",Gianluca Gazzola and Myong Kee Jeong,https://www.sciencedirect.com/science/article/pii/S0031320319302833,https://doi.org/10.1016/j.patcog.2019.106980,0031-3203,2019,106980,96,Pattern Recognition,Dependence-biased clustering for variable selection with random forests,article,GAZZOLA2019106980,
"Oversegmentation is a preprocessing step for many computer vision and remote sensing tasks, such as object recognition and image understanding. In this paper, a method called agglomerative oversegmentation (AOS) is proposed. AOS first designs a dual similarity of the connected pixels by considering both the pixel and region levels. The entropy rate of random walks is then employed on the image plane, and the gain of each pair of connected pixels is estimated. Finally, a fast agglomerative algorithm is developed to oversegment the image using an entropy rate gain function. Two challenging datasets are utilized in the experiments which show the promising performance of AOS. In contrast to the state-of-the-art algorithms, the oversegmentation of AOS is more consistent with the geometric structures of the target objects, especially for the objects with complex texture on remote sensing images.","Oversegmentation, Agglomerative algorithm, Entropy rate, Remote sensing",Huan Ni and Xiaonan Niu,https://www.sciencedirect.com/science/article/pii/S0031320319301864,https://doi.org/10.1016/j.patcog.2019.05.010,0031-3203,2019,324--336,93,Pattern Recognition,Agglomerative oversegmentation using dual similarity and entropy rate,article,NI2019324,
"To hallucinate super-resolution (super-res) face from a real low-quality face, a super-resolution technique based on definition-scalable inference (SRDSI) is proposed in this paper. In the proposed strategy, all high-res labeled faces are first decomposed into basic faces and enhanced faces to train a basic face and an enhanced face inferring model, and then two inferring models are used to hallucinate super-res basic face with low-definition and enhanced faces with high-frequency information from a single low-res face. Finally, the basic face is merged with its enhanced face into a super-res face with high-definition. In addition, this paper employs SIFT key-points to evaluate the similarity between the super-res face and its high-res labeled face. Experimental results show that SRDSI can effectively recover more structural information as well as SIFT key-points from real low-res faces and achieves better performance than state-of-the-art super-resolution techniques in terms of both visual and objective quality.","SIFT, PCA, Sparse representation, Deep learning, Generative adversarial networks",Xiao Hu and Peirong Ma and Zhuohao Mai and Shaohu Peng and Zhao Yang and Li Wang,https://www.sciencedirect.com/science/article/pii/S003132031930202X,https://doi.org/10.1016/j.patcog.2019.05.027,0031-3203,2019,110--121,94,Pattern Recognition,Face hallucination from low quality images using definition-scalable inference,article,HU2019110,
"We propose a robust method for the identification of coronary arteries in computed tomography angiography (CTA) images. Utilizing geometric relations among the target and reference objects, which are assumed to follow a Gaussian distribution, an anatomic and geometric model is designed by Bayesian inference, which provides robust geometric priors for the target object localization. As a prerequisite process for the identification of coronary arteries, partially broken coronary artery segments found in CTA images are grouped and reconnected by geometric analysis of higher order curves connecting the broken segments. The geometric properties such as curvature and torsion represent naturalness and consistency between the vessel segments. As a problem to identify coronary arteries from CTA images, we demonstrate the robustness and accuracy of the proposed method in comparison with existing methods including commercial workstations on a variety of CTA cases.","Computed tomography angiography, Bayesian, Localization, Coronary artery, Multiple target, Curve analysis, Curvature and torsion",Byunghwan Jeon and Yeonggul Jang and Hackjoon Shim and Hyuk-Jae Chang,https://www.sciencedirect.com/science/article/pii/S0031320319302559,https://doi.org/10.1016/j.patcog.2019.07.003,0031-3203,2019,106958,96,Pattern Recognition,Identification of coronary arteries in CT images by Bayesian analysis of geometric relations among anatomical landmarks,article,JEON2019106958,
"Salient object detection is not only important but also challenging tasks in the study of computer vision. In this paper, different from existing approaches, we propose a novel regularization model for the salient object detection, which integrates a weighted group sparsity with the convex Schatten-1 or the non-convex Schatten-2/3 and Schatten-1/2 norm, respectively. A weighted group sparsity induced norm developed in this paper is shown to be able to make the foreground being consistent within the same image patches by effectively absorbing the image geometrical structure as well as the feature similarity. The Schatten quasi-norm is successfully used to capture the lower rank of background via factorization technique, and an alternative non-convex formulation for nuclear norm is studied. The corresponding alternative direction method of multiplier (ADMM) with derived solutions are discussed in detail, and the convergence of algorithm is validated. Extensive experiments on the six widely used datasets show that the proposed approach has capacity in outperforming most of state-of-the-art models in current literature.","Low rank approximation, Schatten-p norm, Matrix decomposition, ADMM, Salient object detection",Min Li and Yao Zhang and Mingqing Xiao and Chen Xu and Weiqiang Zhang,https://www.sciencedirect.com/science/article/pii/S003132031930278X,https://doi.org/10.1016/j.patcog.2019.106975,0031-3203,2019,106975,96,Pattern Recognition,On Schatten-q quasi-norm induced matrix decomposition model for salient object detection,article,LI2019106975,
"In this paper, we show that graph matching methods based on relaxation labeling, spectral graph theory and tensor theory have the same mathematical form by employing power iteration technique. Besides, the differences among these methods are also fully discussed and can be proven that distinctions have little impact on the final matching result. Moreover, we propose a fast compatibility building procedure to accelerate the preprocessing speed which is considered to be the main time consuming part of graph matching. Finally, several experiments are conducted to verify our findings.","Graph matching, Relaxation labeling, Spectral graph theory, Tensor theory",Yuan Zhu and Jiufeng Zhou and Hong Yan,https://www.sciencedirect.com/science/article/pii/S0031320319302390,https://doi.org/10.1016/j.patcog.2019.06.008,0031-3203,2019,223--234,95,Pattern Recognition,A unified formulation of a class of graph matching techniques,article,ZHU2019223,
"Person re-identification (ReID) is a challenging problem, where global features of person images are not enough to solve unaligned image pairs. Many previous works used human pose information to acquire aligned local features to boost the performance. However, those methods need extra labeled data to train an available human pose estimation model. In this paper, we propose a novel method named Dynamically Matching Local Information (DMLI) that could dynamically align local information without requiring extra supervision. DMLI could achieve better performance, especially when encountering the human pose misalignment caused by inaccurate person detection boxes. Then, we propose a deep model name AlignedReID++ which is jointly learned with global features and local feature based on DMLI. AlignedReID++ improves the performance of global features, and could use DMLI to further increase accuracy in the inference phase. Experiments show effectiveness of our proposed method in comparison with several state-of-the-art person ReID approaches. Additionally, it achieves rank-1 accuracy of 92.8% on Market1501 and 86.2% on DukeMTMCReID with ResNet50. The code and models have been released22https://github.com/michuanhaohao/AlignedReID.","Person re-identification, CNNs, Dynamically alignment",Hao Luo and Wei Jiang and Xuan Zhang and Xing Fan and Jingjing Qian and Chi Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319302031,https://doi.org/10.1016/j.patcog.2019.05.028,0031-3203,2019,53--61,94,Pattern Recognition,AlignedReID++: Dynamically matching local information for person re-identification,article,LUO201953,
"Proper combination of multiple classifiers can achieve better performance than any individual one. As one of the most popular combination methods, weighted combination assigns the same or different but invariant weights to base classifiers without consideration of the specificity of different instances. However, each classifier usually has different predictive ability on different instances. To deal with this problem, we propose a combination method named attentional ensemble model (AEM), which learns personalized classifier weights w.r.t different instances via an attentional mechanism. Specifically, we first embed classifiers and instances into a latent space. Then the latent representations are used to profile the predictive ability of classifiers w.r.t different instances. In addition, we design an explicit diversity measure in the latent space, based on which we can enhance the diversity among base classifiers to further improve the performance of combination. Extensive experiments on 20 public data sets show the effectiveness of the proposed method compared with state-of-the-art methods.","Multiple classifier system, Ensemble learning, Attentional mechanism, Diversity-based learning",Hongzhi Liu and Yingpeng Du and Zhonghai Wu,https://www.sciencedirect.com/science/article/pii/S0031320319302791,https://doi.org/10.1016/j.patcog.2019.106976,0031-3203,2019,106976,96,Pattern Recognition,AEM: Attentional Ensemble Model for personalized classifier weight learning,article,LIU2019106976,
"Kernel entropy component analysis (KECA) is a recently proposed dimensionality reduction approach, which has showed superiority in many pattern analysis algorithms previously based on principal component analysis (PCA). The optimized KECA (OKECA) is a state-of-the-art extension of KECA and can return projections retaining more expressive power than KECA. However, OKECA is not robust to outliers and has high computational complexities attributed to its inherent properties of L2-norm. To tackle these two problems, we propose a new variant of KECA, namely L1-norm-based KECA (L1-KECA) for data transformation and feature extraction. L1-KECA attempts to find a new kernel decomposition matrix such that the extracted features store the maximum information potential, which is measured by L1-norm. Accordingly, we present a greedy iterative algorithm which has much faster convergence than OKECA's. Additionally, L1-KECA retains OKECA's capability to obtain accurate density estimation with very few features (just one or two). Moreover, a new semi-supervised L1-KECA classifier is developed and employed into the data classification. Extensive experiments on different real-world datasets validate that our model is superior to most existing KECA-based and PCA-based approaches. Code has been also made publicly available.","Kernel entropy component analysis, Density estimation, Dimensionality reduction, Feature extraction, L1-norm",Chengzu Bai and Ren Zhang and Zeshui Xu and Rui Cheng and Baogang Jin and Jian Chen,https://www.sciencedirect.com/science/article/pii/S0031320319302936,https://doi.org/10.1016/j.patcog.2019.106990,0031-3203,2019,106990,96,Pattern Recognition,L1-norm-based kernel entropy components,article,BAI2019106990,
"Fingerprints are unique and invariant, so they are widely used for biometric recognition. However, due to the problem of deformation in the actual sampling process, it may cause a great change in the fingerprint features. As the accuracy of fingerprint recognition depends on the quality of fingerprints, non-rigid registration algorithms are particularly important. Most existing non-rigid registration algorithms estimate the distortion only by minutiae and gratitude of fingerprints, while direction of the ridges is neglected or used indirectly. In this paper, we proposed a novel model based algorithm for non-rigid fingerprint registration using image fields. As direction information is very important for spatial transformation in registration and the image fields contain the direction of fingerprint ridges, by combing image fields with the traditional model based algorithm, we directly introduce orientation of the ridges to the model for estimating the distortion, and thus make a better use of the direction information of fingerprints and simplify the deformation model. Considering that delta/cores and minutia are sometimes hard to extract accurately and reliably, we used the whole image for matching directly. Experiments have been carried out on four representative databases, namely FVC2004 DB1, Tsinghua Distorted Fingerprint database, NIST SD27 database and NIST SD30 database. We also compared our algorithm with other state-of-the-art algorithms, the experimental results show the effectiveness of the proposed algorithm.","Fingerprint, Non-rigid, Registration, Image field",Sheng Lan and Zhenhua Guo and Jane You,https://www.sciencedirect.com/science/article/pii/S0031320319301967,https://doi.org/10.1016/j.patcog.2019.05.021,0031-3203,2019,48--57,95,Pattern Recognition,A non-rigid registration method with application to distorted fingerprint matching,article,LAN201948,
"3D Hand pose estimation has received an increasing amount of attention, especially since consumer depth cameras came onto the market in 2010. Although substantial progress has occurred recently, no overview has kept up with the latest developments. To bridge the gap, we provide a comprehensive survey, including depth cameras, hand pose estimation methods, and public benchmark datasets. First, a markerless approach is proposed to evaluate the tracking accuracy of depth cameras with the aid of a numerical control linear motion guide. Traditional approaches focus only on static characteristics. The evaluation of dynamic tracking capability has been long neglected. Second, we summarize the state-of-the-art methods and analyze the lines of research. Third, existing benchmark datasets and evaluation criteria are identified to provide further insight into the field of hand pose estimation. In addition, realistic challenges, recent trends, dataset creation and annotation, and open problems for future research directions are also discussed.","Hand pose estimation, Hand tracking, Depth camera, Human-computer interaction",Rui Li and Zhenyu Liu and Jianrong Tan,https://www.sciencedirect.com/science/article/pii/S0031320319301724,https://doi.org/10.1016/j.patcog.2019.04.026,0031-3203,2019,251--272,93,Pattern Recognition,"A survey on 3D hand pose estimation: Cameras, methods, and datasets",article,LI2019251,
"This paper shows that pairwise PageRank orders emerge from two-hop walks. The main tool used here refers to a specially designed sign-mirror function and a parameter curve, whose low-order derivative information implies pairwise PageRank orders with high probability. We study the pairwise correct rate by placing the Google matrix G in a probabilistic framework, where G may be equipped with different random ensembles for model-generated or real-world networks with sparse, small-world, scale-free features, the proof of which is mixed by mathematical and numerical evidence. We believe that the underlying spectral distribution of aforementioned networks is responsible for the high pairwise correct rate. Moreover, the perspective of this paper naturally leads to an O(1) algorithm for any single pairwise PageRank comparison if assuming both A=GâIn, where In denotes the identity matrix of order n, and A2 are ready on hand (e.g., constructed offline in an incremental manner), based on which it is easy to extract the top k list in O(kn), thus making it possible for PageRank algorithm to deal with super large-scale datasets in real time.","Spectral ranking, PageRank, Two-Hop",Ying Tang,https://www.sciencedirect.com/science/article/pii/S0031320319302419,https://doi.org/10.1016/j.patcog.2019.06.010,0031-3203,2019,201--210,95,Pattern Recognition,Two-hop walks indicate PageRank order,article,TANG2019201,
"Deep learning techniques have been successfully used in learning a common representation for multi-view data, wherein different modalities are projected onto a common subspace. In a broader perspective, the techniques used to investigate common representation learning falls under the categories of âcanonical correlation-basedâ approaches and âautoencoder-basedâ approaches. In this paper, we investigate the performance of deep autoencoder-based methods on multi-view data. We propose a novel step-based correlation multi-modal deep convolution neural network (CorrMCNN) which reconstructs one view of the data given the other while increasing the interaction between the representations at each hidden layer or every intermediate step. The idea of step reconstruction reduces the constraint of reconstruction of original data, instead, the objective function is optimized for reconstruction of representative features. This helps the proposed model to generalize for representation and transfer learning tasks efficiently for high dimensional data. Finally, we evaluate the performance of the proposed model on three multi-view and cross-modal problems viz., audio articulation, cross-modal image retrieval and multilingual (cross-language) document classification. Through extensive experiments, we find that the proposed model performs much better than the current state-of-the-art deep learning techniques on all three multi-view and cross-modal tasks.","Representation learning, Transfer learning, Convolution autoencoders, Multilingual document classification",Gaurav Bhatt and Piyush Jha and Balasubramanian Raman,https://www.sciencedirect.com/science/article/pii/S0031320319302146,https://doi.org/10.1016/j.patcog.2019.05.032,0031-3203,2019,12--23,95,Pattern Recognition,Representation learning using step-based deep multi-modal autoencoders,article,BHATT201912,
"Image denoising is a fundamental task in computer vision and image processing domain. In recent years, the task has been tackled with deep neural networks by learning the patterns of noises and image patches. However, because of the high diversity of natural image patches and noise distributions, a huge network with a large amount of training data is necessary to obtain a state-of-the-art performance. In this paper, we propose a novel ensemble strategy of exploiting multiple deep neural networks for efficient deep learning of image denoising. We divide the task of image denoising into several local subtasks according to the complexity of clean image patches and conquer each subtask using a network trained on its local space. Then, we combine the local subtasks at test time by applying the set of networks to each noisy patch as a weighted mixture, where the mixture weights are determined by the likelihood of each network for each noisy patch. Our methodology of using locally-learned networks based on patch complexity effectively decreases the diversity of image patches at each single network, and their adaptively-weighted mixture to the input combines the local subtasks efficiently. Extensive experimental results on Berkeley segmentation dataset and standard test images demonstrate that our strategy significantly boosts denoising performance in comparison to using a single network of the same total capacity. Furthermore, our method outperforms previous methods with much smaller training samples and trainable parameters, and so with much reduced time complexity both in training and running.","Image denoising, Deep neural networks, Stacked denoising autoencoders, Divide and conquer, Local experts, Image patch complexity, Ensemble selection, Image patch classification",Inpyo Hong and Youngbae Hwang and Daeyoung Kim,https://www.sciencedirect.com/science/article/pii/S0031320319302420,https://doi.org/10.1016/j.patcog.2019.06.011,0031-3203,2019,106945,96,Pattern Recognition,Efficient deep learning of image denoising using patch complexity local divide and deep conquer,article,HONG2019106945,
"This paper gives a comprehensive study on gait biometrics via a joint CNN-based method. Gait is a kind of behavioral biometric feature with unique advantages, e.g., long-distance, cross-view and non-cooperative perception and analysis. In this paper, the definition of gait analysis includes gait recognition and gait-based soft biometrics such as gender and age prediction. We propose to investigate these two problems in a joint CNN-based framework which has been seldom reported in the recent literature. The proposed method is efficient in terms of training time, testing time and storage. We achieve the state-of-the-art performance on several gait recognition and soft biometrics benchmarks. Also, we discuss which part of the human body is important and informative for a specific task by network visualization.","Gait recognition, Soft biometrics, Joint learning, Network visualization",Yuqi Zhang and Yongzhen Huang and Liang Wang and Shiqi Yu,https://www.sciencedirect.com/science/article/pii/S0031320319301694,https://doi.org/10.1016/j.patcog.2019.04.023,0031-3203,2019,228--236,93,Pattern Recognition,A comprehensive study on gait biometrics using a joint CNN-based method,article,ZHANG2019228,
"Distance learning is an effective technique for person re-identification. In practice, the hard negative samples usually contain more discriminative information than the easy negative samples. Therefore, itâs necessary to investigate how to make full use of the discriminative information conveyed by different types of negative samples in the distance learning process. In this paper, we propose a Hard and Easy Negative samples mining based Distance learning (HEND) approach for person re-identification, which learns the distance metric by designing different objective functions for hard and easy negative samples, such that the discriminative information contained in negative samples can be exploited more effectively. Moreover, considering that there usually exist large differences between the images captured by different cameras, we further propose a projection-based HEND approach to reduce the influence of between-camera differences to the re-identification. Experimental results on seven pedestrian image datasets demonstrate the effectiveness of the proposed approaches.","Distance learning, Symmetric triplet constraint, Negative samples division, Projection matrix, Person re-identification",Xiaoke Zhu and Xiao-Yuan Jing and Fan Zhang and Xinyu Zhang and Xinge You and Xiang Cui,https://www.sciencedirect.com/science/article/pii/S0031320319302389,https://doi.org/10.1016/j.patcog.2019.06.007,0031-3203,2019,211--222,95,Pattern Recognition,Distance learning by mining hard and easy negative samples for person re-identification,article,ZHU2019211,
"The purpose of network representation is to learn a set of latent features by obtaining community information from network structures to provide knowledge for machine learning tasks. Recent research has driven significant progress in network representation by employing random walks as the network sampling strategy. Nevertheless, existing approaches rely on domain-specifically rich community structures and fail in the network that lack topological information in its own domain. In this paper, we propose a novel algorithm for cross-domain network representation, named as CDNR. By generating the random walks from a structural rich domain and transferring the knowledge on the random walks across domains, it enables a network representation for the structural scarce domain as well. To be specific, CDNR is realized by a cross-domain two-layer node-scale balance algorithm and a cross-domain two-layer knowledge transfer algorithm in the framework of cross-domain two-layer random walk learning. Experiments on various real-world datasets demonstrate the effectiveness of CDNR for universal networks in an unsupervised way.","Network representation, Transfer learning, Random walk, Information network, Unsupervised learning, Feature learning",Shan Xue and Jie Lu and Guangquan Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319301852,https://doi.org/10.1016/j.patcog.2019.05.009,0031-3203,2019,135--148,94,Pattern Recognition,Cross-domain network representations,article,XUE2019135,
"Probabilistic Graphical Models (PGMs) are important and active research areas in machine learning and artificial intelligence. The well-known representatives of PGMs include Restricted Boltzmann Machines (RBMs), Deep Belief Networks (DBNs), Deep Boltzmann Machines (DBMs), and their variants. These PGMs open a new dimension of machine learning and directly lead to the revival of deep learning since 2006. Compared with deep deterministic models, such as deep convolutional neural networks (CNNs), PGMs are more flexible, robust and powerful for both unsupervised and supervised learning problems. As the generative models on the one hand, PGMs have the good capability of generating samples after extracting informative features of input data. On the other hand, they also can provide a good initialization of hierarchical structures for supervised learning. The PGMs with deep structures are called deep Probabilistic Graphical Networks (PGNs) due to their hierarchical structures and close relationships with deep neural networks. There are a large number of CNN-based deep deterministic networks proposed for computer vision tasks. However, these deep deterministic models are shown to possess extremely large number of model parameters, and have serious redundancy problems. Therefore, deep compression of these deterministic models have been proposed in the last few years to reduce the number of connections and nodes, while preserving the classification accuracy. This paper is the first attempt to combine deep PGNs and deep compression techniques together to derive sparse versions of the deep probabilistic models. We firstly examine that whether deep PGNs have serious problem of parameter redundancy, and compress them if necessary. From the experimental results on MNIST, Fashion-MNIST and CIFAR-10 dataset, deep PGNs can be compressed by at least 50% of their parameters through a deep pruning and retraining approach, while keeping their capabilities in both generative and discriminative tasks.","Deep compression, Probabilistic graphical models, Probabilistic graphical networks, Deep learning",Chun-Yang Zhang and Qi Zhao and C.L. Philip Chen and Wenxi Liu,https://www.sciencedirect.com/science/article/pii/S0031320319302821,https://doi.org/10.1016/j.patcog.2019.106979,0031-3203,2019,106979,96,Pattern Recognition,Deep compression of probabilistic graphical networks,article,ZHANG2019106979,
"Integrated convolutional neural network (CNN) and deep bidirectional long short-term memory (DBLSTM) based character models have achieved excellent recognition accuracies on optical character recognition (OCR) tasks, along with large amount of model parameters and massive computation cost. To deploy CNN-DBLSTM model in products with CPU server, there is an urgent need to compress and accelerate it as much as possible, especially the CNN part, which dominates both parameters and computation. In this paper, we study teacher-student learning and Tucker decomposition methods to reduce model size and runtime latency for CNN-DBLSTM based character model for OCR. We use teacher-student learning to transfer the knowledge of a large-size teacher model to a small-size compact student model, followed by Tucker decomposition to further compress the student model. For teacher-student learning, we design a novel learning criterion to bring in the guidance of succeeding LSTM layer when matching the CNN-extracted feature sequences of the large teacher and small student models. Experimental results on large scale handwritten and printed OCR tasks show that, using teacher-student learning alone achieves 9.90â¯Ãâ¯ footprint reduction and 15.23â¯Ãâ¯ inference speedup yet without degrading recognition accuracy. Combined with Tucker decomposition method, we can compress and accelerate the model further. The decomposed model achieves 11.89â¯Ãâ¯ footprint reduction and 22.16â¯Ãâ¯ inference speedup while suffering no or only a small recognition accuracy degradation against the large-size baseline model.","Optical character recognition, CNN-DBLSTM Character model, Model compression, Teacher-student learning, Tucker decomposition",Haisong Ding and Kai Chen and Qiang Huo,https://www.sciencedirect.com/science/article/pii/S0031320319302547,https://doi.org/10.1016/j.patcog.2019.07.002,0031-3203,2019,106957,96,Pattern Recognition,Compressing CNN-DBLSTM models for OCR with teacher-student learning and Tucker decomposition,article,DING2019106957,
"A set of data can be obtained from different hierarchical levels in diverse domains, such as multi-levels of genome data in omics, domestic/global indicators in finance, ancestors/descendants in phylogenetics, genealogy, and sociology. Such layered structures are often represented as a hierarchical network. If a set of different data is arranged in such a way, then one can naturally devise a network-based learning algorithm so that information in one layer can be propagated to other layers through interlayer connections. Incorporating individual networks in layers can be considered as an integration in a serial/vertical manner in contrast with parallel integration for multiple independent networks. The hierarchical integration induces several problems on computational complexity, sparseness, and scalability because of a huge-sized matrix. In this paper, we propose two versions of an algorithm, based on semi-supervised learning, for a hierarchically structured network. The naÃ¯ve version utilizes existing method for matrix sparseness to solve label propagation problems. In its approximate version, the loss in accuracy versus the gain in complexity is exploited by providing analyses on error bounds and complexity. The experimental results show that the proposed algorithms perform well with hierarchically structured data, and, outperform an ordinary semi-supervised learning algorithm.","Hierarchical graph integration, Hierarchical networks, Hierarchically structured networks, Semi-supervised learning",Myungjun Kim and Dong-gi Lee and Hyunjung Shin,https://www.sciencedirect.com/science/article/pii/S0031320319302407,https://doi.org/10.1016/j.patcog.2019.06.009,0031-3203,2019,191--200,95,Pattern Recognition,Semi-supervised learning for hierarchically structured networks,article,KIM2019191,
"Human activity recognition has been actively studied in the last three decades. Compared to human action performed by a single person, human interaction is more complex due to the involvement of more subjects and the interdependence between them. Recently, motivated by the remarkable success of deep learning techniques, many learning-based feature representations have been developed for activity recognition. This paper provides a comprehensive review of human action and interaction recognition methods, covering both hand-crafted features and learning-based features, with a special focus on data captured by RGB-D sensors. Furthermore, this review reveals practical challenges in human activity analysis along with their promising solutions and potential future directions.","Survey, RGB-D sensing, Action recognition, Interaction recognition, Deep learning",Bangli Liu and Haibin Cai and Zhaojie Ju and Honghai Liu,https://www.sciencedirect.com/science/article/pii/S0031320319301955,https://doi.org/10.1016/j.patcog.2019.05.020,0031-3203,2019,1--12,94,Pattern Recognition,RGB-D sensing based human action and interaction analysis: A survey,article,LIU20191,
"RGB-Thermal (RGB-T) object tracking receives more and more attention due to the strongly complementary benefits of thermal information to visible data. However, RGB-T research is limited by lacking a comprehensive evaluation platform. In this paper, we propose a large-scale video benchmark dataset for RGB-T tracking. It has three major advantages over existing ones: 1) Its size is sufficiently large for large-scale performance evaluation (total number of frames: 234K, maximum number of frames per sequence: 8K). 2) The alignment between RGB-T sequence pairs is highly accurate, which does not need pre- or post-processing. 3) The occlusion levels are annotated for occlusion-sensitive performance analysis of different tracking algorithms. Moreover, we propose a novel graph-based approach to learn a robust object representation for RGB-T tracking. In particular, the tracked object is represented with a graph with image patches as nodes. Given initial weights of nodes, this graph including graph structure, node weights and edge weights is dynamically learned in a unified optimization framework. Extensive experiments on the large-scale dataset are executed to demonstrate the effectiveness of the proposed tracker against other state-of-the-art tracking methods. We also provide new insights and potential research directions to the field of RGB-T object tracking.","Visual tracking, Benchmark dataset, Sparse learning, Graph representation, Information fusion",Chenglong Li and Xinyan Liang and Yijuan Lu and Nan Zhao and Jin Tang,https://www.sciencedirect.com/science/article/pii/S0031320319302808,https://doi.org/10.1016/j.patcog.2019.106977,0031-3203,2019,106977,96,Pattern Recognition,RGB-T object tracking: Benchmark and baseline,article,LI2019106977,
"Deep embedding learning becomes more attractive for discriminative feature learning, but many methods still require hard-class mining, which is computationally complex and performance-sensitive. To this end, Adaptive Large Margin N-Pair loss (ALMN) is proposed to address the aforementioned issues. First, the class center is adopted as the anchor point to avoid the difficulty on anchor selection. Then instead of exploring hard example-mining strategy, we introduce the adaptive large margin constraint, where a novel geometrical Virtual Point Generating (VPG) method is proposed to convert a fixed margin into a local-adaptive angular margin, by automatically generating a boundary training sample in feature space. The effectiveness of our method is demonstrated on fine-grained image retrieval and clustering tasks using six popular databases, including CUB, CARS, Flowers, Aircraft, Stanford Online Products and In-Shop Clothes. The results show that the proposed method achieves better performance than other state-of-the-art methods, such as N-Pair loss, Lifted loss and Triplet loss.","Embedding learning, Adaptive margin, Virtual point generating, Discriminative feature",Binghui Chen and Weihong Deng,https://www.sciencedirect.com/science/article/pii/S0031320319301839,https://doi.org/10.1016/j.patcog.2019.05.011,0031-3203,2019,353--364,93,Pattern Recognition,Deep embedding learning with adaptive large margin N-pair loss for image retrieval and clustering,article,CHEN2019353,
"Person re-identification (re-ID) and attribute recognition share a common target at learning pedestrian descriptions. Their difference consists in the granularity. Most existing re-ID methods only take identity labels of pedestrians into consideration. However, we find the attributes, containing detailed local descriptions, are beneficial in allowing the re-ID model to learn more discriminative feature representations. In this paper, based on the complementarity of attribute labels and ID labels, we propose an attribute-person recognition (APR) network, a multi-task network which learns a re-ID embedding and at the same time predicts pedestrian attributes. We manually annotate attribute labels for two large-scale re-ID datasets, and systematically investigate how person re-ID and attribute recognition benefit from each other. In addition, we re-weight the attribute predictions considering the dependencies and correlations among the attributes. The experimental results on two large-scale re-ID benchmarks demonstrate that by learning a more discriminative representation, APR achieves competitive re-ID performance compared with the state-of-the-art methods. We use APR to speed up the retrieval process by ten times with a minor accuracy drop of 2.92% on Market-1501. Besides, we also apply APR on the attribute recognition task and demonstrate improvement over the baselines.","Person re-identification, Attribute recognition",Yutian Lin and Liang Zheng and Zhedong Zheng and Yu Wu and Zhilan Hu and Chenggang Yan and Yi Yang,https://www.sciencedirect.com/science/article/pii/S0031320319302377,https://doi.org/10.1016/j.patcog.2019.06.006,0031-3203,2019,151--161,95,Pattern Recognition,Improving person re-identification by attribute and identity learning,article,LIN2019151,
"Nowadays, there are a lot of page images available and the scanning process is quite well resolved and can be done industrially. On the other hand, HTR systems can only deal with single text line images. Segmenting pages into single text line images is a very expensive process which has traditionally been done manually. This is a bottleneck which is holding back any massive industrial document processing. A baseline detection method will be presented here1. The initial problem is reformulated as a clustering problem over a set of interest points. Its design aim is to be fast and to resist the noise artifacts that usually appear in historical manuscripts: variable interline spacing, the overlapping and touching of words in adjacent lines, humidity spots, etc. Results show that this system can be used to massively detect where the text lines are in pages. Highlight: This system reached second place in the Icdar 2017 Competition on Baseline Detection (see TableÂ 1).",,MoisÃ©s Pastor,https://www.sciencedirect.com/science/article/pii/S0031320319302134,https://doi.org/10.1016/j.patcog.2019.05.031,0031-3203,2019,149--161,94,Pattern Recognition,"Text baseline detection, a single page trained system",article,PASTOR2019149,
"In numerous multimedia and multi-modal tasks from image and video retrieval to zero-shot recognition to multimedia question and answering, bridging image and text representations plays an important and in some cases an indispensable role. To narrow the modality gap between vision and language, prior approaches attempt to discover their correlated semantics in a common feature space. However, these approaches omit the intra-modal semantic consistency when learning the inter-modal correlations. To address this problem, we propose cycle-consistent embeddings in a deep neural network for matching visual and textual representations. Our approach named as CycleMatch can maintain both inter-modal correlations and intra-modal consistency by cascading dual mappings and reconstructed mappings in a cyclic fashion. Moreover, in order to achieve a robust inference, we propose to employ two late-fusion approaches: average fusion and adaptive fusion. Both of them can effectively integrate the matching scores of different embedding features, without increasing the network complexity and training time. In the experiments on cross-modal retrieval, we demonstrate comprehensive results to verify the effectiveness of the proposed approach. Our approach achieves state-of-the-art performance on two well-known multi-modal datasets, Flickr30K and MSCOCO.","Image-text matching, Embedding, Deep neural networks, Late-fusion inference",Yu Liu and Yanming Guo and Li Liu and Erwin M. Bakker and Michael S. Lew,https://www.sciencedirect.com/science/article/pii/S0031320319301840,https://doi.org/10.1016/j.patcog.2019.05.008,0031-3203,2019,365--379,93,Pattern Recognition,CycleMatch: A cycle-consistent embedding network for image-text matching,article,LIU2019365,
"In this research, we propose a novel registration method for three-dimensional (3D) reconstruction from serial section images. 3D reconstructed data from serial section images provides structural information with high resolution. However, there are three problems in 3D reconstruction: non-rigid deformation, tissue discontinuity, and accumulation of scale change. To solve the non-rigid deformation, we propose a novel non-rigid registration method using blending rigid transforms. To avoid the tissue discontinuity, we propose a target image selection method using the criterion based on the blending of transforms. To solve the scale change of tissue, we propose a scale adjustment method using the tissue area before and after registration. The experimental results demonstrate that our method can represent non-rigid deformation with a small number of control points, and is robust to a variation in staining. The results also demonstrate that our target selection method avoids tissue discontinuity and our scale adjustment reduces scale change.","Image registration, Non-rigid deformation, Transformation blending",Takehiro Kajihara and Takuya Funatomi and Haruyuki Makishima and Takahito Aoto and Hiroyuki Kubo and Shigehito Yamada and Yasuhiro Mukaigawa,https://www.sciencedirect.com/science/article/pii/S0031320319302535,https://doi.org/10.1016/j.patcog.2019.07.001,0031-3203,2019,106956,96,Pattern Recognition,Non-rigid registration of serial section images by blending transforms for 3D reconstruction,article,KAJIHARA2019106956,
"Various applications of human-computer interaction are based on the estimation of head pose, which is challenging due to different facial appearance, inhomogeneous illumination, partial occlusion, etc. In this paper, we propose a deep neural network following the Coarse-to-Fine strategy to estimate head poses. The scheme includes two branches: Coarse classification phase classifying the input image into four categories, and Fine Regression phase estimating the accurate pose parameters. The two sub-networks are trained jointly. To tackle the problem of insufficient annotated data in training process, we design a rendering pipeline to synthesize realistic head images and generate an annotated dataset with a collection of 310k head poses. The results on benchmark datasets and synthetic dataset validate the effectiveness of our approach, as well as the results on images with diverse illumination, occlusion, and motion blur. Moreover, our method can be easily extended to estimate head poses on depth images.","Head pose estimation, Coarse-to-Fine, Joint learning",Yujia Wang and Wei Liang and Jianbing Shen and Yunde Jia and Lap-Fai Yu,https://www.sciencedirect.com/science/article/pii/S0031320319302018,https://doi.org/10.1016/j.patcog.2019.05.026,0031-3203,2019,196--206,94,Pattern Recognition,A deep Coarse-to-Fine network for head pose estimation from synthetic data,article,WANG2019196,
"Time series classification has been considered as one of the most challenging problems in data mining and widely used in a broad range of fields, such as climate, finance, medicine and computer science. The main challenges of time series classification are to select the appropriate representation (feature extraction) of time series and choose the similarity metric between time series. Compared with the traditional feature extraction method, in this paper, we focus on the fusion of global features, local features and the interaction between them, while preserving the temporal information of the local features. Based on this strategy, a highly comparative approach to univariate time series classification is introduced that uses covariance matrices as interpretable features. From the perspective of probability theory, each covariance matrix can be seen as a zero-mean Gaussian distribution. Our idea is to incorporate covariance matrix into the framework of information geometry, which is to study the geometric structures on the manifolds of the probability distributions. The space of covariance matrices is a statistical (Riemannian) manifold and the geodesic distance is introduced to measure the similarity between them. Our method is to project each distribution (covariance matrix) to a vector on the tangent space of the statistical manifold. Finally, the classification is carried out in the tangent space which is a Euclidean space. Concepts of a structural and functional network are also presented which provide us an understanding of the properties of the data set and guide further interpretation to the classifier. Experimental evaluation shows that the performance of the proposed approach exceeded some competitive methods on benchmark datasets from the UCR time series repository.","Time series, Classification, Information geometry, Riemannian manifold",Jiancheng Sun and Yong Yang and Yanqing Liu and Chunlin Chen and Wenyuan Rao and Yaohui Bai,https://www.sciencedirect.com/science/article/pii/S0031320319302250,https://doi.org/10.1016/j.patcog.2019.05.040,0031-3203,2019,24--35,95,Pattern Recognition,Univariate time series classification using information geometry,article,SUN201924,
"This work focuses on clustering large sets of utterances collected from an unknown number of speakers. Since the number of speakers is unknown, we focus on exact hierarchical agglomerative clustering, followed by automatic selection of the number of clusters. Exact hierarchical clustering of a large number of vectors, however, is a challenging task due to memory constraints, which make it ineffective or unfeasible for large datasets. We propose an exact memoryâconstrained and parallel implementation of average linkage clustering for large scale datasets, showing that its computational complexity is approximately O(N2), but is much faster (up to 40 times in our experiments), than the Reciprocal Nearest Neighbor chain algorithm, which has O(N2) complexity. We also propose a very fast silhouette computation procedure that, in linear time, determines the set of clusters. The computational efficiency of our approach is demonstrated on datasets including up to 4 million speaker vectors.","Clustering, UPGMA, Similarity measures, Reciprocal Nearest Neighbor, PLDA, PSVM, Silhouette, Cluster quality measures",Sandro Cumani and Pietro Laface,https://www.sciencedirect.com/science/article/pii/S0031320319302493,https://doi.org/10.1016/j.patcog.2019.06.018,0031-3203,2019,235--246,95,Pattern Recognition,Exact memoryâconstrained UPGMA for large scale speaker clustering,article,CUMANI2019235,
"Unsupervised feature selection plays an important role in machine learning and data mining, which is very challenging because of unavailable class labels. We propose an unsupervised feature selection framework by combining the discriminative information of class labels with the subspace learning in this paper. In the proposed framework, the nonnegative Laplacian embedding is first utilized to produce pseudo labels, so as to improve the classification accuracy. Then, an optimal feature subset is selected by the subspace learning guiding by the discriminative information of class labels, on the premise of maintaining the local structure of data. We develop an iterative strategy for updating similarity matrix and pseudo labels, which can bring about more accurate pseudo labels, and then we provide the convergence of the proposed strategy. Finally, experimental results on six real-world datasets prove the superiority of the proposed approach over seven state-of-the-art ones.","Unsupervised feature selection, Nonnegative Laplacian embedding, Subspace learning, Class labels",Yong Zhang and Qing Wang and Dun-wei Gong and Xian-fang Song,https://www.sciencedirect.com/science/article/pii/S0031320319301669,https://doi.org/10.1016/j.patcog.2019.04.020,0031-3203,2019,337--352,93,Pattern Recognition,Nonnegative Laplacian embedding guided subspace learning for unsupervised feature selection,article,ZHANG2019337,
"The paper formulates the subspace clustering as a problem of structured representation learning. It is proved that the sparsity of the data representation is significantly promoted by propagating a low-rank structure, leading to a more robust description of the clustering structure. Based on a theoretical proof to support this observation, a novel subspace clustering algorithm is proposed with the structured representation. Two cascade self-expressions are leveraged to implement the propagation. One leads to a low-rank representation of the data samples by exploiting the global structure; whereas the other generates a sparse representation of the former low-rank representation to capture the neighborhood structure. The proposed representation strategy is further investigated from both a geometric and a physical perspective. Extensive evaluations on both synthetic and real datasets demonstrate that the proposed approach outperforms most state-of-the-art methods.","Clustering, Subspace segmentation, Sparse coding, Low-rank representation, Self-expression.",Yao Sui and Guanghui Wang and Li Zhang,https://www.sciencedirect.com/science/article/pii/S003132031930250X,https://doi.org/10.1016/j.patcog.2019.06.019,0031-3203,2019,261--271,95,Pattern Recognition,Sparse subspace clustering via Low-Rank structure propagation,article,SUI2019261,
"Recent research has introduced a novel method of directly monitoring the effects of potential therapies for Cystic Fibrosis (CF) airway disease by quantifying mucociliary transit (MCT). In this method, micron-sized spherical particles are deposited into rodent airways, and synchrotron X-ray images are obtained to quantify the motion of the particles. However, the accurate tracking of these particles is challenging due to low contrast, image noise, and the presence of overlapping particles. Therefore, this paper proposes a novel method for detecting and tracking circular particles and measuring their dynamics. Accurate particle detection is achieved by applying a convolutional neural network (CNN). For robust multi-object tracking, this paper proposes a confidence model utilizing appearance and neighbouring topology learned by linear discriminant analysis. We also propose a detection recovery method using multi-frame association to restore the missed particles due to overlapping. The proposed method is tested with several different datasets and shows high levels of detection and tracking accuracy. Finally, by offering visual tracking analyses that display merging and splitting events, the proposed method can provide a better understanding of airway MCT behaviour.","Convolutional neural network (CNN), LDA, Neighbuoring topology, Multi-frame association, Particle tracking",Hye-Won Jung and Sang-Heon Lee and Martin Donnelley and David Parsons and Victor Stamatescu and Ivan Lee,https://www.sciencedirect.com/science/article/pii/S0031320319301827,https://doi.org/10.1016/j.patcog.2019.05.007,0031-3203,2019,485--497,93,Pattern Recognition,Multiple particle tracking in time-lapse synchrotron X-ray images using discriminative appearance and neighbouring topology learning,article,JUNG2019485,
"Hashing methods have been intensively studied and widely used in image retrieval. Hashing methods aim to learn a group of hash functions to map original data into compact binary codes and simultaneously preserve some notion of similarity in the Hamming space. The generated binary codes are effective for image retrieval and highly efficient for large-scale data storage. The decision tree is a fast and interpretable model, but the current decision tree based hashing methods have insufficient learning ability due to the use of shallow decision trees. Most current deep hashing methods are based on deep neural networks. However, considering the deficiencies of deep neural network-based hashing, such as the presence of too many hyperparameters, poor interpretability, and requirement for expensive and powerful computational facilities during the training process, a non-deep neural network-based hashing model need to be designed to achieve efficient image retrieval with few hyperparameters, easy theoretical analysis and an efficient training process. The multi-grained cascade forest (gcForest) is a novel deep model that generates a deep forest ensemble classifier to process data layer-by-layer with multi-grained scanning and a cascade forest. To date, gcForest has not been used to generate compact binary codes; therefore, we propose a deep forest-based method for hashing learning that aims to learn shorter binary codes to achieve effective and efficient image retrieval. The experimental results show that the proposed method has better performance with shorter binary codes than other corresponding hashing methods.","Hashing learning, Image retrieval, Deep forest hashing, Shorter binary codes",Meng Zhou and Xianhua Zeng and Aozhu Chen,https://www.sciencedirect.com/science/article/pii/S0031320319302365,https://doi.org/10.1016/j.patcog.2019.06.005,0031-3203,2019,114--127,95,Pattern Recognition,Deep forest hashing for image retrieval,article,ZHOU2019114,
"Generally speaking, a spiral is a 2D curve which winds about a fixed point. Now, we present a new, alternative, and easy way to describe and generate spirals by means of the use of the Slope Chain Code (SCC) [E. Bribiesca, A measure of tortuosity based on chain coding, Pattern Recognition 46 (2013) 716â724]. Thus, each spiral is represented by only one chain. The chain elements produce a finite alphabet which allows us to use grammatical techniques for spiral classification. Spirals are composed of constant straight-line segments and their chain elements are obtained by calculating the slope changes between contiguous straight-line segments (angle of contingence) scaled to a continuous range from â1 (â180â) to 1 (180â). The SCC notation is invariant under translation, rotation, optionally under scaling, and it does not use a grid. Other interesting properties can be derived from this notation, such as: the mirror symmetry and inverse spirals, the accumulated slope, the slope change mean, and tortuosity for spirals. We introduce new concepts of projective polygonal paths and osculating polygons. We present a new spiral called the SCC polygonal spiral and its chain which is described by the numerical sequence 2n for nâ¯â¥â¯3, to the best of our knowledge this is the first time that this spiral and its chain are presented. The importance of this spiral and its chain is that this chain is covering all the slope changes of all the regular polygons composed of n edges (n-gons). Also, we describe the chain which generates the spiral of Archimedes. Finally, we present some results of different kind of spirals from the real world, including spiral patterns in shells.","Slope chain code, Polygonal curves, Polygonal paths, Osculating polygons, Broken lines, Chain coding, The SCC polygonal spiral",Ernesto Bribiesca,https://www.sciencedirect.com/science/article/pii/S003132031930247X,https://doi.org/10.1016/j.patcog.2019.06.016,0031-3203,2019,247--260,95,Pattern Recognition,The spirals of the Slope Chain Code,article,BRIBIESCA2019247,
"Common spatial pattern (CSP) analysis and its extensions have been widely used as feature extraction approaches in the brain-computer interfaces (BCIs). However, most of the CSP-based approaches do not use any prior knowledge that might be available about the two conditions (classes) to be classified. Therefore, their applications are limited to datasets that contain enough variance information about the two conditions. For example, in some event-related potential (ERP) detection applications, such as P300 speller, the information is in the time domain but not in the variance of spatial components. To address this problem, first, we present a novel feature extraction method termed extended common spatial pattern (ECSP) analysis, which uses prior knowledge available from data to produce a broader range of features than that of conventional CSP analysis. Then, similarly, we introduce the extended common temporal pattern (ECTP) analysis. Finally, to exploit both spatial and temporal information, we propose extended common spatial and temporal pattern (ECSTP) analysis. We have used BCI competition III, dataset II as our main dataset to evaluate our proposed methods. In addition, we used two other datasets, namely BCI competition II, dataset IIb and BCI competition IV, dataset IIb, to further evaluate the performance of the proposed methods. In All the datasets, the proposed methods significantly outperform the conventional CSP, CTP, and CSTP methods. More specifically, ECSTP has the best performance among the proposed methods. Moreover, classification results show that the proposed methods are competitive with other state of the art methods applied to these datasets.","Feature extraction, Event-Related potential, P300 Speller, Common spatial pattern (CSP)",Mohammad {Jalilpour Monesi} and Sepideh {Hajipour Sardouie},https://www.sciencedirect.com/science/article/pii/S0031320319302249,https://doi.org/10.1016/j.patcog.2019.05.039,0031-3203,2019,128--135,95,Pattern Recognition,Extended common spatial and temporal pattern (ECSTP): A semi-blind approach to extract features in ERP detection,article,JALILPOURMONESI2019128,
"Natural Language Processing has achieved remarkable performance in multitudinous computer tasks, but the potential capability of textual information has not been completely explored in visual saliency detection. In this paper, we learn to detect salient object from natural language by addressing the two essential issues: finding a semantic content matching the corresponding linguistic concept and recovering fine details without any pixel-level annotations. We first propose the Feature Matching Network (FMN) to explore the internal relation between the linguistic concept and visual image in the semantic space. The FMN simultaneously establishes the textual-visual pairwise affinities and generates a language-aware coarse saliency map. to refine the coarse map, the Recurrent Fine-tune Network (RFN) is proposed to enhance its predicted performance progressively by self-supervision. Our approach only leverages the caption to provide important cues of salient object, but generates a fine-detailed foreground map at a detecting speed of 72 FPS without any post-processing. Extensive experiments demonstrate that our method takes full advantage of textual information of natural language in saliency detection, and performs favorably against state-of-the-art approaches on the most existing datasets.","Saliency detection, Natural language, Textual-visual pairwise, Self-supervision",Mingyang Qian and Jinqing Qi and Lihe Zhang and Mengyang Feng and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320319302523,https://doi.org/10.1016/j.patcog.2019.06.021,0031-3203,2019,106955,96,Pattern Recognition,Language-aware weak supervision for salient object detection,article,QIAN2019106955,
"Handwritten Text Recognition is a important requirement in order to make visible the contents of the myriads of historical documents residing in public and private archives and libraries world wide. Automatic Handwritten Text Recognition (HTR) is a challenging problem that requires a careful combination of several advanced Pattern Recognition techniques, including but not limited to Image Processing, Document Image Analysis, Feature Extraction, Neural Network approaches and Language Modeling. The progress of this kind of systems is strongly bound by the availability of adequate benchmarking datasets, software tools and reproducible results achieved using the corresponding tools and datasets. Based on English and German historical documents proposed in recent open competitions at ICDAR and ICFHR conferences between 2014 and 2017, this paper introduces four HTR benchmarks in order of increasing complexity from several points of view. For each benchmark, a specific system is proposed which overcomes results published so far under comparable conditions. Therefore, this paper establishes new state of the art baseline systems and results which aim at becoming new challenges that would hopefully drive further improvement of HTR technologies. Both the datasets and the software tools used to implement the baseline systems are made freely accessible for research purposes.","Historical handwritten text recognition, Hidden Markov models, Convolutional neural networks, Recurrent neural networks, Language modeling",Joan Andreu SÃ¡nchez and VerÃ³nica Romero and Alejandro H. Toselli and Mauricio Villegas and Enrique Vidal,https://www.sciencedirect.com/science/article/pii/S0031320319302006,https://doi.org/10.1016/j.patcog.2019.05.025,0031-3203,2019,122--134,94,Pattern Recognition,A set of benchmarks for Handwritten Text Recognition on historical documents,article,SANCHEZ2019122,
"In this paper, we propose a structured general and specific multi-view subspace clustering method for image clustering. Unlike most existing multi-view subspace clustering methods which harness the shared cluster structure to preserve the consistence between different views or utilize the diversity regularization to exploit the complementary information from different views, our method learns the structured general and specific representation matrices to obtain the common and specific characteristics of different views with structure consistence and diversity regularization. The general representation matrix guarantees the consistence between different views and the specific representation matrices indicate the diversity among different views. Hence, our method can well exploit the common structure and diversity information of multi-view data. Specifically, the proposed framework can be applied into many existing multi-view subspace clustering methods. Moreover, we develop an efficient and effective optimization approach to solve the objective function of which the time and convergence analyses are also provided. Experimental results on four benchmark datasets are presented to show the effectiveness of proposed method.","Subspace clustering, Multi-view learning, Structure consistence, Diversity",Wencheng Zhu and Jiwen Lu and Jie Zhou,https://www.sciencedirect.com/science/article/pii/S0031320319301797,https://doi.org/10.1016/j.patcog.2019.05.005,0031-3203,2019,392--403,93,Pattern Recognition,Structured general and specific multi-view subspace clustering,article,ZHU2019392,
"Object tracking in crowded spaces is a challenging but very important task in computer vision applications. However, due to interactions among large-scale pedestrians and common social rules, predicting the complex human mobility in a crowded scene becomes difficult. This paper proposes a novel human trajectory prediction model in a crowded scene called the social-affinity LSTM model. Our model can learn general human mobility patterns and predict individualâ s trajectories based on their past positions, in particular, with the influence of their neighbors in the Social Affinity Map (SAM). The SAM clusters the relative positions of surrounding individuals, and represents the distribution of the relative positions by different bins with semantic descriptions. We formulate the problem of trajectory prediction together with interactions among people as a sequence generation task with social affinity. The proposed model utilizes the LSTM to learn general human moving patterns as well as the Social Affinity Map to connect neighbors with a weight matrix corresponding to SAM bins for learning the social dependencies between correlated pedestrians. By capturing the objectâ s past positions and connecting the hidden states of itâ s neighbors in different SAM bins with different elements of the weight matrix, the social-affinity LSTM is able to predict the trajectory of each pedestrian with its own features and neighborsâÂ influence. We compare the performance of our method with the Social LSTM model on several public datasets. Our model outperforms state-of-the-art methods on these datasets with the best results, especially the datasets with more social affinity phenomena.","Trajectory prediction, SAM pooling, Social-affinity LSTM",Zhao Pei and Xiaoning Qi and Yanning Zhang and Miao Ma and Yee-Hong Yang,https://www.sciencedirect.com/science/article/pii/S0031320319301712,https://doi.org/10.1016/j.patcog.2019.04.025,0031-3203,2019,273--282,93,Pattern Recognition,Human trajectory prediction in crowded scene using social-affinity Long Short-Term Memory,article,PEI2019273,
"Learning compressed representations of multivariate time series (MTS) facilitates data analysis in the presence of noise and redundant information, and for a large number of variates and time steps. However, classical dimensionality reduction approaches are designed for vectorial data and cannot deal explicitly with missing values. In this work, we propose a novel autoencoder architecture based on recurrent neural networks to generate compressed representations of MTS. The proposed model can process inputs characterized by variable lengths and it is specifically designed to handle missing data. Our autoencoder learns fixed-length vectorial representations, whose pairwise similarities are aligned to a kernel function that operates in input space and that handles missing values. This allows to learn good representations, even in the presence of a significant amount of missing data. To show the effectiveness of the proposed approach, we evaluate the quality of the learned representations in several classification tasks, including those involving medical data, and we compare to other methods for dimensionality reduction. Successively, we design two frameworks based on the proposed architecture: one for imputing missing data and another for one-class classification. Finally, we analyze under what circumstances an autoencoder with recurrent layers can learn better compressed representations of MTS than feed-forward architectures.","Representation learning, Multivariate time series, Autoencoders, Recurrent neural networks, Kernel methods",Filippo Maria Bianchi and Lorenzo Livi and Karl Ãyvind Mikalsen and Michael Kampffmeyer and Robert Jenssen,https://www.sciencedirect.com/science/article/pii/S0031320319302766,https://doi.org/10.1016/j.patcog.2019.106973,0031-3203,2019,106973,96,Pattern Recognition,Learning representations of multivariate time series with missing data,article,BIANCHI2019106973,
"Although it is of great importance to recognize weather conditions automatically, this task has not been explored thoroughly in practice. Generally, most approaches in the literature simply treat it as a common image classification task, i.e., assigning a certain weather label to each image. However, there are significant differences between weather recognition and common image classification, since several weather conditions tend to occur simultaneously, like foggy and cloudy. Obviously, a single weather label is insufficient to provide a comprehensive description of the weather conditions. In this case, we propose to utilize auxiliary weather-cues, e.g., black clouds and blue sky, for comprehensive weather description. Specifically, a multi-task framework is designed to jointly deal with the weather-cue segmentation task and weather classification task. Benefit from the intrinsic relationships lying in the two tasks, exploring the information of weather-cues can not only provide a comprehensive description of weather conditions, but also help the weather classification task to learn more effective features, and further improve the performance. Besides, we construct two large-scale weather recognition datasets equipped with both weather labels and segmentation masks of weather-cues. Experiment results demonstrate the excellent performance of our approach. The constructed two datasets will be available at https://github.com/wzgwzg/Multitask_Weather.","Weather recognition, Weather-cue map, Multi-task framework",Bin Zhao and Lulu Hua and Xuelong Li and Xiaoqiang Lu and Zhigang Wang,https://www.sciencedirect.com/science/article/pii/S0031320319302481,https://doi.org/10.1016/j.patcog.2019.06.017,0031-3203,2019,272--284,95,Pattern Recognition,Weather recognition via classification labels and weather-cue maps,article,ZHAO2019272,
"Cross-domain collaborative filtering, which transfers rating knowledge across multiple domains, has become a new way to effectively alleviate the sparsity problem in recommender systems. Different auxiliary domains are generally different in the importance to the target domain, which is hard to evaluate using previous approaches. Besides, most recommender systems only take advantage of information from user- or item-side auxiliary domains. To overcome these drawbacks, we propose a cross-domain collaborative filtering algorithm with expanding user and item features via the latent factor space of auxiliary domains in this paper. In the proposed algorithm, the recommendation problem is first formulated as a classification problem in the target domain, which takes user and item location as the feature vector, their rating as the label. Then, Funk-SVD decomposition is employed to extract extra user and item features from user- and item-side auxiliary domains, respectively, with the purpose of expanding the two-dimensional location feature vector. Finally, a classifier is trained using the C4.5 decision tree algorithm for predicting missing ratings. The proposed algorithm can make full use of user- and item-side information. We conduct extensive experiments and compare the proposed algorithm with various state-of-the-art single- and cross-domain collaborative filtering algorithms. The experimental results show that the proposed algorithm has advantages in terms of four different evaluation metrics.","Cross-domain collaborative filtering, Feature expansion, Funk-SVD decomposition, Classification, Latent factor space",Xu Yu and Feng Jiang and Junwei Du and Dunwei Gong,https://www.sciencedirect.com/science/article/pii/S0031320319302122,https://doi.org/10.1016/j.patcog.2019.05.030,0031-3203,2019,96--109,94,Pattern Recognition,A cross-domain collaborative filtering algorithm with expanding user and item features via the latent factor space of auxiliary domains,article,YU201996,
"In this paper, we consider the problem of distributed unsupervised clustering, where training data is partitioned over a set of agents, whose interaction happens over a sparse, but connected, communication network. To solve this problem, we recast the well known Expectation Maximization method in a distributed setting, exploiting a recently proposed algorithmic framework for in-network non-convex optimization. The resulting algorithm, termed as Expectation Maximization Consensus, exploits successive local convexifications to split the computation among agents, while hinging on dynamic consensus to diffuse information over the network in real-time. Convergence to local solutions of the distributed clustering problem is then established. Experimental results on well-known datasets illustrate that the proposed method performs better than other distributed Expectation-Maximization clustering approaches, while the method is faster than a centralized Expectation-Maximization procedure and achieves a comparable performance in terms of cluster validity indexes. The latter ones achieve good values in absolute range scales and prove the quality of the obtained clustering results, which compare favorably with other methods in the literature.","Distributed learning, Clustering, Gaussian mixtures, Expectation maximization, Non-Convex optimization",Rosa Altilio and Paolo {Di Lorenzo} and Massimo Panella,https://www.sciencedirect.com/science/article/pii/S0031320319301670,https://doi.org/10.1016/j.patcog.2019.04.021,0031-3203,2019,603--620,93,Pattern Recognition,Distributed data clustering over networks,article,ALTILIO2019603,
"Logo detection in real-world scene images is an important problem with applications in advertisement and marketing. Existing general-purpose object detection methods require large training data with annotations for every logo class. These methods do not satisfy the incremental demand of logo classes necessary for practical deployment since it is practically impossible to have such annotated data for new unseen logo. In this work, we develop an easy-to-implement query-based logo detection and localization system by employing a one-shot learning technique using off the shelf neural network components. Given an image of a query logo, our model searches for logo within a given target image and predicts the possible location of the logo by estimating a binary segmentation mask. The proposed model consists of a conditional branch and a segmentation branch. The former gives a conditional latent representation of the given query logo which is combined with feature maps of the segmentation branch at multiple scales in order to obtain the matching location of the query logo in a target image. Feature matching between the latent query representation and multi-scale feature maps of segmentation branch using simple concatenation operation followed by 1â¯Ãâ¯1 convolution layer makes our model scale-invariant. Despite its simplicity, our query-based logo retrieval framework achieved superior performance in FlickrLogos-32 and TopLogos-10 dataset over different existing baseline methods.","Logo retrieval, One-shot learning, Multi-scale conditioning, Similarity matching, Query retrieval",Ayan Kumar Bhunia and Ankan Kumar Bhunia and Shuvozit Ghose and Abhirup Das and Partha Pratim Roy and Umapada Pal,https://www.sciencedirect.com/science/article/pii/S0031320319302626,https://doi.org/10.1016/j.patcog.2019.106965,0031-3203,2019,106965,96,Pattern Recognition,A deep one-shot network for query-based logo retrieval,article,BHUNIA2019106965,
"Data representation is an important factor in deciding the performance of machine learning algorithms including classification. Feature construction (FC) can combine original features to form high-level ones that can help classification algorithms achieve better performance. Genetic programming (GP) has shown promise in FC due to its flexible representation. Most GP methods construct a single feature, which may not scale well to high-dimensional data. This paper aims at investigating different approaches to constructing multiple features and analysing their effectiveness, efficiency, and underlying behaviours to reveal the insight of multiple-feature construction using GP on high-dimensional data. The results show that multiple-feature construction achieves significantly better performance than single-feature construction. In multiple-feature construction, using multi-tree GP representation is shown to be more effective than using the single-tree GP thanks to the ability to consider the interaction of the newly constructed features during the construction process. Class-dependent constructed features achieve better performance than the class-independent ones. A visualisation of the constructed features also demonstrates the interpretability of the GP-based FC approach, which is important to many real-world applications.","Feature construction, Genetic programming, Classification, Class dependence, High-dimensional data",Binh Tran and Bing Xue and Mengjie Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319301815,https://doi.org/10.1016/j.patcog.2019.05.006,0031-3203,2019,404--417,93,Pattern Recognition,Genetic programming for multiple-feature construction on high-dimensional classification,article,TRAN2019404,
"With the wide availability of the Internet and the proliferation of pornographic images online, adult image detection and filtering has become very important to prevent young people from reaching these harmful contents. However, due to the large diversity in adult images, automatic adult image detection is a difficult task. In this paper, a new deep convolutional neural network (DCNN) based approach is proposed to classify images into three classes, i.e. porn, sexy, and benign. Our approach takes both the entire picture (global context) and the meaningful region (local context) information into consideration. The proposed network is composed of three parts, i.e. the image characteristics subnet to extract discriminative low-level image features, the sensitive body part detection subnet to detect adult-image related regions, and the feature extraction and fusion subnet to generate high-level features for image classification. A multi-task learning scheme is designed to optimize the network with both the global and local information. Experiments are carried out on two datasets with over 160,000 images. From the experiment results, it was observed that the proposed network achieved high classification accuracies (96.6% in the AIC dataset and 92.7% in the NPDI dataset) and outperformed the other approaches investigated.","Adult image recognition, Deep convolutional network, Global context, Local context, Multi-tasks learning",Feng Cheng and Shi-Lin Wang and Xi-Zi Wang and Alan Wee-Chung Liew and Gong-Shen Liu,https://www.sciencedirect.com/science/article/pii/S0031320319302869,https://doi.org/10.1016/j.patcog.2019.106983,0031-3203,2019,106983,96,Pattern Recognition,A global and local context integration DCNN for adult image classification,article,CHENG2019106983,
"Image-to-image translation, which translates input images to a different domain with a learned one-to-one mapping, has achieved impressive success in recent years. The success of translation mainly relies on the network architecture to reserve the structural information while modify the appearance slightly at the pixel level through adversarial training. Although these networks are able to learn the mapping, the translated images are predictable without exclusion. It is more desirable to diversify them using image-to-image translation by introducing uncertainties, i.e., the generated images hold potential for variations in colors and textures in addition to the general similarity to the input images, and this happens in both the target and source domains. To this end, we propose a novel generative adversarial network (GAN) based model, InjectionGAN, to learn a many-to-many mapping. In this model, the input image is combined with latent variables, which comprise of domain-specific attribute and unspecific random variations. The domain-specific attribute indicates the target domain of the translation, while the unspecific random variations introduce uncertainty into the model. A unified framework is proposed to regroup these two parts and obtain diverse generations in each domain. Extensive experiments demonstrate that the diverse generations have high quality for the challenging image-to-image translation tasks where no pairing information of the training dataset exits. Both quantitative and qualitative results prove the superior performance of InjectionGAN over the state-of-the-art approaches.","Generative adversarial network, Variational autoencoder, Image to image translation",Wenju Xu and Keshmiri Shawn and Guanghui Wang,https://www.sciencedirect.com/science/article/pii/S003132031930192X,https://doi.org/10.1016/j.patcog.2019.05.017,0031-3203,2019,570--580,93,Pattern Recognition,Toward learning a unified many-to-many mapping for diverse image translation,article,XU2019570,
"Considering human can learn new object successfully from just one sample, one-shot learning, where each visual class just has one labeled sample for training, has attracted more and more attention. In the past years, most researchers achieve one-shot learning by training a matching network to map a small labeled support set and an unlabeled image to its label. The support set is combined by one image with the same label as unlabeled image and few images with other labels generated by random sampling. This random sampling strategy easily generates massive over-easy support sets in which most labels are less relevant to the label of unlabeled image. It leads to the limitation of matching network for one-shot prediction over indistinguishable label sets. For this issue, we propose a novel metric to evaluate the learning difficulty of support set, where this metric jointly considers the semantic diversity and similarity of visual labels. Based on the metric, we introduce a scheduled sampling strategy to train the matching network from easy to difficult. Extensive experimental results on three datasets, including mini-Imagenet, Birds and Flowers, indicate that our method could achieve significant improvements over other previous methods.","Scheduled sampling, Matching network, From easy to difficult, One-shot learning, Difficulty metric",Lingling Zhang and Jun Liu and Minnan Luo and Xiaojun Chang and Qinghua Zheng and Alexander G. Hauptmann,https://www.sciencedirect.com/science/article/pii/S0031320319302596,https://doi.org/10.1016/j.patcog.2019.07.007,0031-3203,2019,106962,96,Pattern Recognition,Scheduled sampling for one-shot learning via matching network,article,ZHANG2019106962,
"State-of-the-art methods have achieved impressive performances on multi-oriented text detection. Yet, they usually have difficulty in handling curved and dense texts, which are common in commodity images. In this paper, we propose a network for detecting dense and arbitrary-shaped scene text by instance-aware component grouping (ICG), which is a flexible bottom-up method. To address the difficulty in separating dense text instances faced by most bottom-up methods, we propose attractive and repulsive link between text components which forces the network learning to focus more on close text instances, and instance-aware loss that fully exploits context to supervise the network. The final text detection is achieved by a modified minimum spanning tree (MST) algorithm based on the learned attractive and repulsive links. To demonstrate the effectiveness of the proposed method, we introduce a dense and arbitrary-shaped scene text dataset composed of commodity images (DAST1500). Experimental results show that the proposed ICG significantly outperforms state-of-the-art methods on DAST1500 and two curved text datasets: Total-Text and CTW1500, and also achieves very competitive performance on two multi-oriented datasets: ICDAR15 (at 7.1FPS for 1280â¯Ãâ¯768 image) and MTWI.","Scene text detection, Multi-oriented text, Curve text, Dense text",Jun Tang and Zhibo Yang and Yongpan Wang and Qi Zheng and Yongchao Xu and Xiang Bai,https://www.sciencedirect.com/science/article/pii/S0031320319302511,https://doi.org/10.1016/j.patcog.2019.06.020,0031-3203,2019,106954,96,Pattern Recognition,SegLink++: Detecting Dense and Arbitrary-shaped Scene Text by Instance-aware Component Grouping,article,TANG2019106954,
"Today's Arabic Handwriting recognition systems are able to recognize arbitrary words over a large but finite vocabulary. Systems operating with a fixed vocabulary are bound to encounter so-called out-of-vocabulary (OOV) words. The aim of this research is to propose a two-step approach that tackles the problem of OOV words in Arabic handwriting. In the first step, we exploit different types of sub-word units to detect the potential OOVs. In the recovery stage, a dynamic dictionary is built to extend the initial static word lexicon in order to cope with the detected OOVs. The recovery includes a selection step in which the best word candidates extracted from the external resource are kept. Experiments were conducted on the public benchmarking KHATT and AHTID/MW databases. The obtained results revealed that sub-word modeling could give cues for improving the detection and that the use of a dynamic dictionary significantly improves the recognition performance compared to one-step approaches that are based on a large static dictionary or the combination of different sub-word units. We achieve the state of the art results on the KHATT dataset.","Arabic Handwriting recognition, Out of vocabulary detection and recovery, Static lexicon, Dynamic lexicon, Statistical language model, Deep learning, Multi-dimensional long short term memory network",Sana Khamekhem Jemni and Yousri Kessentini and Slim Kanoun,https://www.sciencedirect.com/science/article/pii/S0031320319301785,https://doi.org/10.1016/j.patcog.2019.05.003,0031-3203,2019,507--520,93,Pattern Recognition,Out of vocabulary word detection and recovery in Arabic handwritten text recognition,article,JEMNI2019507,
"Classical sparse modeling requires accurate alignment between the query and the training data. This precondition is disadvantageous for target recognition tasks, where, although the target is present in the images, it is infeasible to perfectly register it during training. In addition, the classical approach is less powerful under unconstrained operating conditions. To solve these problems, this paper presents a new sparse signal modeling strategy in the frequency domain. Because signal energy is mainly concentrated on a small portion of low-frequency components, this set of spectrum carries vital information that can be used to discriminates the class of a target. We generated representations by aggregating low-frequency components. They were then used to build sparse signal models. More specifically, the spectral representation of training data were concatenated to form an over-complete dictionary to encode the counterpart of the query as a linear combination of themselves. Sparsity was harnessed to generate an optimal solution, from which an inference can be made. Multiple comparative analyses were made to demonstrate the advantages of the proposed strategy, especially in unconstrained environments.","Sparse representation, Transformed domain, Target recognition",Ganggang Dong and Hongwei Liu and Gangyao Kuang and Jocelyn Chanussot,https://www.sciencedirect.com/science/article/pii/S0031320319302754,https://doi.org/10.1016/j.patcog.2019.106972,0031-3203,2019,106972,96,Pattern Recognition,Target recognition in SAR images via sparse representation in the frequency domain,article,DONG2019106972,
"The multinomial distribution and the Dirichlet Compound Multinomial (DCM) are widely accepted to model count data. However, recent research showed that the Dirichlet is not the best choice as a prior to multinomial. We propose a novel model called the Multinomial Scaled Dirichlet (MSD) distribution that is the composition of the scaled Dirichlet distribution and the multinomial. Moreover, to improve the computation efficiency in high-dimensional spaces, we propose to approximate the MSD as a member of the exponential family. The performance evaluation of the proposed models is conducted through a set of extensive empirical experiments on challenging applications, namely, text classification, facial expression recognition, and texture images clustering. The results show that the proposed model, and its approximation, strive to achieve higher accuracy compared to the state-of-the-art generative models for count data clustering, while the approximation EMSD is many times faster than the corresponding MSD.","Count data, Burstiness, DAEM, Multinomial, Scaled dirichlet, Finite mixture models, Exponential family approximation, Model selection, Text collection, Image databases",Nuha Zamzami and Nizar Bouguila,https://www.sciencedirect.com/science/article/pii/S0031320319302237,https://doi.org/10.1016/j.patcog.2019.05.038,0031-3203,2019,36--47,95,Pattern Recognition,A Novel Scaled dirichlet-based statistical framework for count data modeling: Unsupervised learning and exponential approximation,article,ZAMZAMI201936,
"Direct comparison of three-dimensional (3D) objects is computationally expensive due to the need for translation, rotation, and scaling of the objects to evaluate their similarity. In applications of 3D object comparison, often identifying specific local regions of objects is of particular interest. We have recently developed a set of 2D moment invariants based on discrete orthogonal Krawtchouk polynomials for comparison of local image patches. In this work, we extend them to 3D and construct 3D Krawtchouk descriptors (3DKDs) that are invariant under translation, rotation, and scaling. The new descriptors have the ability to extract local features of a 3D surface from any region-of-interest. This property enables comparison of two arbitrary local surface regions from different 3D objects. We present the new formulation of 3DKDs and apply it to the local shape comparison of protein surfaces in order to predict ligand molecules that bind to query proteins.","3D image retrieval, Local image comparison, Region of interest, Discrete orthogonal functions, Krawtchouk polynomials, Weighted Krawtchouk polynomials, 3D Krawtchouk moments, Protein surface, Ligand binding site, Pocket comparison, Structure-based function prediction",Atilla Sit and Woong-Hee Shin and Daisuke Kihara,https://www.sciencedirect.com/science/article/pii/S0031320319301943,https://doi.org/10.1016/j.patcog.2019.05.019,0031-3203,2019,534--545,93,Pattern Recognition,Three-dimensional Krawtchouk descriptors for protein local surface shape comparison,article,SIT2019534,
"The max-pooling operation in convolutional neural networks (CNNs) downsamples the feature maps of convolutional layers. However, in doing so, it loses some spatial information. In this paper, we extract a novel feature from pooling layers, called displacement features, and combine them with the features resulting from max-pooling to capture the structural deformations for text recognition tasks. The displacement features record the location of the maximal value in a max-pooling operation. Furthermore, we analyze and mine the class-wise trends of the displacement features. The extensive experimental results and discussions demonstrate that the proposed displacement features can improve the performance of the CNN based architectures and tackle the issues with the structural deformations of max-pooling in the text recognition tasks.","Convolutional neural networks, Max-pooling, Displacement feature, Text recognition",Yuchen Zheng and Brian Kenji Iwana and Seiichi Uchida,https://www.sciencedirect.com/science/article/pii/S003132031930189X,https://doi.org/10.1016/j.patcog.2019.05.014,0031-3203,2019,558--569,93,Pattern Recognition,Mining the displacement of max-pooling for text recognition,article,ZHENG2019558,
"The classification with reject option consists to train a classifier that rejects the examples when the confidence in its prediction is low. The objective is to improve the accuracy of the non-rejected examples and the reliability of the prediction. The performances of the reject classifiers depend on both the error rate and rejection rate. Since these two values are in opposition, we have to make a trade-off between them. This paper is focused on the visualization spaces the performances of the classifiers with rejection option. We analyze two common spaces, the ROC space and the error-rejection (ER) space, then we propose a new space: the cost-reject (CR) space. We show that the ROC space is the less convenient space to represent the performances of the reject classifier. However, it can be recommended for classification problems where the importance of the two classes is different. For the ER space, we point out that the linear interpolation that is commonly used to draw the error-reject curve is not correct and leads to an overestimation of the classifier performances. From the definition of the condition error and rejection rate, we propose a new interpolation of the error-rejection curve that is unbiased. We introduce a new visualization space called the cost reject space. The CR space plots the normalized classification cost in function on the normalized rejection cost. The performance of a classifier is represented in this space by a line. The three visualization spaces are compared on problems of classification algorithms comparison. The advantages and drawbacks of each spaces are discussed and some recommendations are provided in the conclusion.","Classification with reject option, Classifier performances",Blaise Hanczar,https://www.sciencedirect.com/science/article/pii/S0031320319302870,https://doi.org/10.1016/j.patcog.2019.106984,0031-3203,2019,106984,96,Pattern Recognition,Performance visualization spaces for classification with rejection option,article,HANCZAR2019106984,
"Multi-label feature selection plays an important role in pattern recognition, which can improve multi-label classification performance. In traditional multi-label feature selection methods based on information theory, feature relevance is evaluated by the accumulated mutual information between a candidate feature and each label. However, to the best of our knowledge, traditional methods ignore the effect of label redundancy on the evaluation of feature relevance. To address this issue, we propose a new multi-label feature selection method named multi-label Feature Selection based on Label Redundancy (LRFS). First, we categorize labels into two groups: independent labels and dependent labels. Second, by analyzing the differences between independent labels and dependent labels, we propose a new feature relevance term, that is, the conditional mutual information between candidate features and each label given other labels. Finally, we combine the new feature relevance term with the feature redundancy term to design our feature selection method. To evaluate the classification performance of our method, LRFS is compared to three information-theoretical-based multi-label feature selection methods on an artificial data set. Furthermore, LRFS is compared to five algorithm adaption feature selection methods and two problem transformation feature selection methods on 12 real-world multi-label data sets. The experimental results demonstrate that LRFS outperforms the other compared methods in terms of four evaluation metrics.","Pattern recognition, Multi-label classification, Multi-label feature selection, Information theory, Label redundancy",Ping Zhang and Guixia Liu and Wanfu Gao,https://www.sciencedirect.com/science/article/pii/S0031320319302353,https://doi.org/10.1016/j.patcog.2019.06.004,0031-3203,2019,72--82,95,Pattern Recognition,Distinguishing two types of labels for multi-label feature selection,article,ZHANG201972,
"Hierarchical data representations are powerful tools to analyze images and have found numerous applications in image processing. When it comes to multimodal images however, the fusion of multiple hierarchies remains an open question. Recently, the concept of braids of partitions has been proposed as a theoretical tool and possible solution to this issue. In this paper, we demonstrate the relevance of the braid structure for the hierarchical representation of multimodal images. We first propose a fully operable procedure to build a braid of partitions from two hierarchical representations. We then derive a framework for multimodal image segmentation, relying on an energetic minimization scheme conducted on the braid structure. The proposed approach is investigated on different multimodal images scenarios, and the obtained results confirm its ability to efficiently handle the multimodal information to produce more accurate segmentation outputs.","Hierarchical representation, Multimodal image, Braid of partitions, Energy minimization, Image segmentation",Guillaume Tochon and Mauro {Dalla Mura} and Miguel Angel Veganzones and Thierry GÃ©raud and Jocelyn Chanussot,https://www.sciencedirect.com/science/article/pii/S0031320319302110,https://doi.org/10.1016/j.patcog.2019.05.029,0031-3203,2019,162--172,95,Pattern Recognition,Braids of partitions for the hierarchical representation and segmentation of multimodal images,article,TOCHON2019162,
"For multi-manifold clustering, it is still a challenging problem on how to learn the cluster number automatically from data. This paper presents a novel nonparametric Bayesian model to cluster the multi-manifold data and estimate the number of submanifolds simultaneously. Our model firstly assumes that every submanifold is a probability distribution defined in the manifold space. Then, we approximate the manifold distribution with a deep neural network. To maintain the data similarity among data, we regularize the data generation process with a modified k-nearest neighbor graph. Though the posterior inference is hard, our model leads to a very efficient deterministic optimization algorithm, which incorporates the mean field variational inference with the Graph regularized Variational Auto-Encoder (Graph-VAE). By applying the Graph-VAE, our model exhibits another advantage of realistic image generation which overcomes the conventional clustering methods. Furthermore, we expand our proposed manifold algorithm with the Dirichlet Process Mixture (DPM) to model the real datasets, in which the manifold data and non-manifold data are coexisting. Experiments on synthetic data verify our theoretical analysis. Clustering results on motion segmentation, coil20 and 3D pedestrian show that our approach can significantly improve the clustering accuracy. The handwritten database experiment demonstrates the image generation capability.","Multi-manifold clustering, Image generation, Dirichlet process mixture model, Variational inference, Graph, Deep neural network",Xulun Ye and Jieyu Zhao,https://www.sciencedirect.com/science/article/pii/S003132031930175X,https://doi.org/10.1016/j.patcog.2019.04.029,0031-3203,2019,215--227,93,Pattern Recognition,Multi-manifold clustering: A graph-constrained deep nonparametric method,article,YE2019215,
"Clustering is among the most important unsupervised learning tasks with several applications in a wide range of domains. Discriminative Clustering (DC) techniques combine the unsupervised nature of clustering with the high discriminative ability of supervised subspace learning methods by simultaneously performing clustering and learning a representation that encourages the separability of the clusters. However, in contrast with classical supervised learning tasks, where the labels are usually correct, cluster assignments are inherently noisy leading to suboptimal results when used as ground truth information with highly discriminative methods. To this end, a novel similarity-based subspace learning method, that allows for learning regularized clustering-oriented representations, avoiding the pitfalls of highly discriminative methods, such as Linear Discriminant Analysis (LDA), is proposed in this paper. The ability of the proposed method to improve the quality of the obtained clustering solutions is demonstrated using extensive experiments on four datasets.","Discriminative clustering, Subspace learning, Unsupervised learning",Nikolaos Passalis and Anastasios Tefas,https://www.sciencedirect.com/science/article/pii/S0031320319302857,https://doi.org/10.1016/j.patcog.2019.106982,0031-3203,2019,106982,96,Pattern Recognition,Discriminative clustering using regularized subspace learning,article,PASSALIS2019106982,
"Recently, several inductive and flexible nonlinear data projection methods for graph-based semi-supervised learning were proposed. These state-of-the art techniques have a good performance. However, they have not taken into account the relevance of the original features in their model estimation. In this paper, we propose a joint graph-based embedding and feature weighting for getting a flexible and inductive nonlinear data representation on manifolds. The proposed criterion explicitly estimates the feature weights together with the projected data and the linear transformation such that data smoothness and large margins are achieved in the projection space. Moreover, the paper introduces a kernel variant of the model in order to get an inductive nonlinear embedding that is close to a real nonlinear subspace for a good approximation of the embedded data. The proposed frameworks can be seamlessly used in semi-supervised and supervised settings. The resulting optimization problems can be solved efficiently. The proposed embedding methods are evaluated on six public scene and face datasets. Experiments on image classification, in a semi-supervised setting, show that our proposed methods can have a performance that is better than that of many state-of-the-art methods including linear and nonlinear methods.","Graph-based embedding, Discriminative embedding, Feature weighting, Supervised learning, Semi-supervised learning, Pattern recognition",Ruifeng Zhu and Fadi Dornaika and Yassine Ruichek,https://www.sciencedirect.com/science/article/pii/S0031320319301803,https://doi.org/10.1016/j.patcog.2019.05.004,0031-3203,2019,458--469,93,Pattern Recognition,Joint graph based embedding and feature weighting for image classification,article,ZHU2019458,
"Stacked autoencoder is effective in image denoising and classification when it is used for synthetic aperture radar (SAR) change detection. However, the resulting features may not be discriminative enough for in some sense. To alleviate this problem, in this paper we propose a stacked Fisher autoencoder (SFAE) for SAR change detection. Specifically, in the framework of SFAE, unsupervised layer-wise feature learning and supervised fine-tuning are jointly performed when training the network. The trained network can be used to detect the changes in both of the single and multi-polarization SAR datasets in real-time. The proposed SFAE has two advantages. The first one is to expand the stacked autoencoder to suit the environment with the multiplicative noise in SAR change detection. The second is that the features extracted by SFAE are more discriminative than the original stacked autoencoder due to that Fisher discriminant criterion is incorporated into SFAE. The results on the simulated and real SAR datasets indicate that the proposed SFAE algorithm has a significant advantage on multitemporal single/multi-polarization SAR (SAR/PolSAR) change detection.","Stacked fisher autoencoder (SFAE), Synthetic aperture radar (SAR), Change detection, Stacked autoencoder (SAE), Fisher criterion",Ganchao Liu and Lingling Li and Licheng Jiao and Yongsheng Dong and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320319302742,https://doi.org/10.1016/j.patcog.2019.106971,0031-3203,2019,106971,96,Pattern Recognition,Stacked Fisher autoencoder for SAR change detection,article,LIU2019106971,
"K-SVCR and Twin-KSVC are two novel algorithms to deal with multi-class problems. They have achieved good performance since they evaluate all training samples into a â1-versus-1-versus-restâ structure. But they are extremely time consuming, so it remains challenging to apply them into large-scale problems directly. Inspired by the sparse solution of SVMs, in this paper, we propose a safe sample elimination rule (SSE) for multi-class classifiers K-SVCR and Twin-KSVC, termed as SSE-K-SVCR and SSE-T-KSVC, to reduce computation time. With our rule, many redundant samples of all classes can be identified and deleted before actually solving the problem, so the scale of dual problems can be reduced a lot. And our methods are safe, i.e., they can derive identical optimal solutions as K-SVCR and Twin-KSVC, respectively. So the testing accuracy keeps unchanged. Besides, the methods can be embedded into grid search method to accelerate the whole training process, and they are effective both for penalty parameter and kernel parameter. Finally, a fast algorithm clipDCD is introduced to reduce the computation time for larger datatset. Experimental results on one artificial dataset and seventeen benchmark datasets demonstrate the effectiveness and safety of our proposed methods.","K-SVCR, Twin-KSVC, Safe elimination rule, Multi-class classification, Multi-parameter",Xinying Pang and Xianli Pan and Yitian Xu,https://www.sciencedirect.com/science/article/pii/S0031320319302225,https://doi.org/10.1016/j.patcog.2019.05.037,0031-3203,2019,1--11,95,Pattern Recognition,Multi-parameter safe sample elimination rule for accelerating nonlinear multi-class support vector machines,article,PANG20191,
"This paper proposes a deep learning based unified and generalizable framework for accurate iris detection, segmentation and recognition. The proposed framework firstly exploits state-of-the-art and iris-specific Mask R-CNN, which performs highly reliable iris detection and primary segmentation i.e., identifying iris/non-iris pixels, followed by adopting an optimized fully convolutional network (FCN), which generates spatially corresponding iris feature descriptors. A specially designed Extended Triplet Loss (ETL) function is presented to incorporate the bit-shifting and non-iris masking, which are found necessary for learning meaningful and discriminative spatial iris features. Thorough experiments on four publicly available databases suggest that the proposed framework consistently outperforms several classic and state-of-the-art iris recognition approaches. More importantly, our model exhibits superior generalization capability as, unlike popular methods in the literature, it does not essentially require database-specific parameter tuning, which is another key advantage.","Iris recognition, Deep learning, Spatially corresponding features",Zijing Zhao and Ajay Kumar,https://www.sciencedirect.com/science/article/pii/S0031320319301499,https://doi.org/10.1016/j.patcog.2019.04.010,0031-3203,2019,546--557,93,Pattern Recognition,"A deep learning based unified framework to detect, segment and recognize irises using spatially corresponding features",article,ZHAO2019546,
"This paper investigates the role of saliency to improve the classification accuracy of a Convolutional Neural Network (CNN) for the case when scarce training data is available. Our approach consists in adding a saliency branch to an existing CNN architecture which is used to modulate the standard bottom-up visual features from the original image input, acting as an attentional mechanism that guides the feature extraction process. The main aim of the proposed approach is to enable the effective training of a fine-grained recognition model with limited training samples and to improve the performance on the task, thereby alleviating the need to annotate a large dataset. The vast majority of saliency methods are evaluated on their ability to generate saliency maps, and not on their functionality in a complete vision pipeline. Our proposed pipeline allows to evaluate saliency methods for the high-level task of object recognition. We perform extensive experiments on various fine-grained datasets (Flowers, Birds, Cars, and Dogs) under different conditions and show that saliency can considerably improve the networkâs performance, especially for the case of scarce training data. Furthermore, our experiments show that saliency methods that obtain improved saliency maps (as measured by traditional saliency benchmarks) also translate to saliency methods that yield improved performance gains when applied in an object recognition pipeline.","Object recognition, Fine-grained classification, Saliency detection, Scarce training data",Carola Figueroa Flores and Abel Gonzalez-Garcia and Joost {van de Weijer} and Bogdan Raducanu,https://www.sciencedirect.com/science/article/pii/S0031320319301773,https://doi.org/10.1016/j.patcog.2019.05.002,0031-3203,2019,62--73,94,Pattern Recognition,Saliency for fine-grained object recognition in domains with scarce training data,article,FLORES201962,
"In this paper, we propose a cut-edge algorithm for spatial clustering (CutESC) based on proximity graphs. The CutESC algorithm removes edges when a cut-edge value for the edgeâs endpoints is below a threshold. The cut-edge value is calculated by using statistical features and spatial distribution of data based on its neighborhood. Also, the algorithm works without any prior information and preliminary parameter settings while automatically discovering clusters with non-uniform densities, arbitrary shapes, and outliers. However, there is an option which allows users to set two parameters to better adapt clustering solutions for particular problems. To assess advantages of CutESC algorithm, experiments have been conducted using various two-dimensional synthetic, high-dimensional real-world, and image segmentation datasets.","Spatial data mining, Clustering, Proximity graphs, Graph theory",Alper Aksac and Tansel Ãzyer and Reda Alhajj,https://www.sciencedirect.com/science/article/pii/S0031320319302468,https://doi.org/10.1016/j.patcog.2019.06.014,0031-3203,2019,106948,96,Pattern Recognition,CutESC: Cutting edge spatial clustering technique based on proximity graphs,article,AKSAC2019106948,
"Diffusion magnetic resonance imaging, a non-invasive tool to infer white matter fiber connections, produces a large number of streamlines containing a wealth of information on structural connectivity. The size of these tractography outputs makes further analyses complex, creating a need for methods to group streamlines into meaningful bundles. In this work, we address this problem by proposing a set of flexible and efficient streamline clustering approaches based on kernel dictionary learning and sparsity priors. Proposed approaches, which include L0 norm, group sparsity, and manifold regularization prior, allow streamlines to be assigned to more than one bundle, making the clustering more robust to overlapping bundles and inter-subject variations. We evaluate the performance of our method on an expert labeled dataset as well as data from the Human Connectome Project. Results highlight the ability of our method to group streamlines into plausible bundles and illustrate the impact of sparsity priors on the performance of the proposed methods. Methods presented in this work are relevant for the neuroscience studies on diffusion tractography analysis, as well as pattern recognition applications requiring the unsupervised clustering of 3D curves.","Diffusion MRI, White matter fibers, Clustering, Sparsity priors, Kernel dictionary learning, Human Connectome project",Kuldeep Kumar and Kaleem Siddiqi and Christian Desrosiers,https://www.sciencedirect.com/science/article/pii/S003132031930233X,https://doi.org/10.1016/j.patcog.2019.06.002,0031-3203,2019,83--95,95,Pattern Recognition,White matter fiber analysis using kernel dictionary learning and sparsity priors,article,KUMAR201983,
"Automatic image annotation aims at assigning a set of textual labels to an image that describes its semantics. In real-world datasets with large vocabularies, each image is usually annotated with only a subset of all possible relevant labels. This leads to the problem of learning with missing labels. Moreover, several existing approaches for image annotation aim at producing a set which contains as many relevant labels as possible for a given image. However, such a set is usually unnecessary and contains redundant labels. This leads to the task of diverse image annotation, which aims at predicting labels that are collectively representative as well as diverse. In this paper, we study a new task called diverse image annotation with missing labels (DIAML), which is a fusion of these two practical aspects of the conventional image annotation task; i.e., diverse image annotation, and image annotation with missing labels. For this, we also propose a new k-nearest neighbours (kNN) based algorithm, called per-label k-nearest neighbours (or PL-kNN), that addresses both these challenges simultaneously. For a given image, rather than identifying its neighbours in the feature-space as done in the conventional kNN algorithm, PL-kNN first creates diverse partitions of the (training) samples based on label information, and then predicts the confidence for each label using a fixed number of nearest neighbours from the corresponding partition. The label-specific partitioning and neighbourhood selection steps inherently address the issue of missing labels as well. Extensive experiments on benchmark datasets show that though conceptually simple, the proposed method consistently outperforms state-of-the-art methods that address either of these two (sub-)tasks, thus establishing a strong baseline for the new DIAML task.","Image annotation, Diverse labels, Missing labels, Nearest neighbour",Yashaswi Verma,https://www.sciencedirect.com/science/article/pii/S0031320319301931,https://doi.org/10.1016/j.patcog.2019.05.018,0031-3203,2019,470--484,93,Pattern Recognition,Diverse image annotation with missing labels,article,VERMA2019470,
"Subspace clustering techniques become paramount in pattern recognition for detecting local variations from high dimensional data. Several techniques exist in the recent literature for subspace clustering, majority of which optimize implicitly or explicitly a single cluster quality measure. Inspired by the success of multi-objective optimization in solving clustering problem, we developed a multi-objective based subspace clustering technique in this paper. The proposed technique simultaneously optimizes two subspace cluster quality measures, capable of capturing different cluster shapes/properties. Two existing cluster quality measures, XB-index and PBM-index, are modified to develop subspace cluster validity indices, and then those are used as optimization criteria. These cluster validity indices measure the appropriateness of generated subspace clusters in terms of intra-subspace cluster similarity and separation between subspace clusters. The proposed approach utilizes a new evolvable genome structure which stores the information about subspaces in its phenotype and genotype and evolves this genome structure with the help of different genetic operators. The developed algorithm is applied on ten standard real-life data sets and sixteen synthetic datasets for identifying different subspace clusters. The results obtained by this algorithm are compared against some state-of-the-art techniques with respect to different performance metrics. Experimentation reveals that the proposed algorithm is able to take advantages of its evolvable genomic structure and multi-objective based framework and it can be applied to any data set. In a part of the paper, the efficacy of the proposed technique is also shown for bi-clustering of gene-expression data sets.","Multi-objective optimization, Subspace clustering, Evolvable genome structure, Cluster validity indices, Biclustering",Dipanjyoti Paul and Sriparna Saha and Jimson Mathew,https://www.sciencedirect.com/science/article/pii/S0031320319302183,https://doi.org/10.1016/j.patcog.2019.05.033,0031-3203,2019,58--71,95,Pattern Recognition,Fusion of evolvable genome structure and multi-objective optimization for subspace clustering,article,PAUL201958,
"Multiple query criteria active learning methods have a higher potential performance than conventional active learning methods in which only one criterion is deployed for sample selection. A central issue related to multiple query criteria active learning methods concerns the development of an integration criteria strategy that makes full use of all criteria. The conventional integration criteria strategies adopted in relevant research facilitate the desired effects, but several limitations still must be addressed. For instance, some of the strategies are not sufficiently scalable during the design process, and the number and type of criteria involved are dictated. Thus, it is challenging for the user to integrate other criteria into the original process unless modifications are made to the algorithm. Other strategies are too dependent on empirical parameters, which can be acquired only by experience or cross-validation and thus lack generality; additionally, these strategies are counter to the intention of active learning, as samples need to be labeled in the validation set before the active learning process can begin. To address these limitations, we propose a novel multiple query criteria active learning method for classification tasks that employs a third strategy via weighted rank aggregation. The proposed method serves as a heuristic means to select high-value samples of high scalability and generality and is implemented through a three-step process: (1) the transformation of the sample selection to sample ranking and scoring, (2) the computation of the self-adaptive weights of each criterion, and (3) the weighted aggregation of each sample rank list. Ultimately, the sample at the top of the aggregated ranking list is the most comprehensively valuable and must be labeled. Several experiments generating 419 wins, 226 ties and 55 losses against other state-of-the-art multiple query criteria-based methods are conducted to verify that the proposed method can achieve superior results.","Multiple query criteria active learning, Integration criteria strategy, Sample query criterion, Weighted rank aggregation",Yu Zhao and Zhenhui Shi and Jingyang Zhang and Dong Chen and Lixu Gu,https://www.sciencedirect.com/science/article/pii/S0031320319301372,https://doi.org/10.1016/j.patcog.2019.03.029,0031-3203,2019,581--602,93,Pattern Recognition,A novel active learning framework for classification: Using weighted rank aggregation to achieve multiple query criteria,article,ZHAO2019581,
"Recent Progress has shown that exploitation of hidden layer neurons in convolutional neural networks (CNN) incorporating with a carefully designed activation function can yield better classification results in the field of computer vision. The paper firstly introduces a novel deep learning (DL) architecture aiming to mitigate the gradient-vanishing problem, in which the earlier hidden layer neurons could be directly connected with the last hidden layer and fed into the softmax layer for classification. We then design a generalized linear rectifier function as the activation function that can approximate arbitrary complex functions via training of the parameters. We will show that our design can achieve similar performance in a number of object recognition and video action benchmark tasks, such as MNIST, CIFAR-10/100, SVHN, Fashion-MNIST, STL-10, and UCF YoutTube Action Video datasets, under significantly less number of parameters and shallower network infrastructure, which is not only promising in training in terms of computation burden and memory usage, but is also applicable to low-computation, low-memory mobile scenarios for inference.","CNN, Computer vision, Deep learning, Activation",Zhi Chen and Pin-Han Ho,https://www.sciencedirect.com/science/article/pii/S0031320319302584,https://doi.org/10.1016/j.patcog.2019.07.006,0031-3203,2019,106961,96,Pattern Recognition,Global-connected network with generalized ReLU activation,article,CHEN2019106961,
"In multi-label learning, objects are essentially related to multiple semantic meanings, and the type of data is confronted with the impact of high feature dimensionality simultaneously, such as the bioinformatics and text mining applications. To tackle the learning problem, the key technology, i.e., feature selection, is developed to reduce dimensionality, whereas most of the previous methods for multi-label feature selection are either directly transformed from traditional single-label feature selection methods or half-baked in the label information exploitation, and thus causing the redundant or irrelevant features involved in the selected feature subset. Aimed to seek discriminative features across multiple class labels, we propose an embedded multi-label feature selection method with manifold regularization. To be specific, a low-dimensional embedding is constructed based on the original feature space to fit the label distribution for capturing the label correlations locally, which is also constrained using the label information in consideration of the co-occurrence relationships of label pairs. Following this principle, we design an optimization objective function involving l2,1-norm regularization to achieve multi-label feature selection, and the convergence is guaranteed. Empirical studies on various multi-label data sets reveal that the proposed method can obtain highly competitive performance against some state-of-the-art multi-label feature selection methods.","Multi-label learning, Feature selection, Label correlations, Manifold regularization, Optimization objective",Jia Zhang and Zhiming Luo and Candong Li and Changen Zhou and Shaozi Li,https://www.sciencedirect.com/science/article/pii/S0031320319302341,https://doi.org/10.1016/j.patcog.2019.06.003,0031-3203,2019,136--150,95,Pattern Recognition,Manifold regularized discriminative feature selection for multi-label learning,article,ZHANG2019136,
"The paper proposes a novel approach for learning kernel Support Vector Machines (SVM) from large scale data with reduced computation time. The proposed approach, termed as Subclass Reduced Set SVM (SRS-SVM), utilizes the subclass structure of data to effectively estimate the candidate support vector set. Since the candidate support vector set cardinality is only a fraction of the training set cardinality, learning SVM from the former requires less time without significantly changing the decision boundary. SRS-SVM depends on a domain knowledge related input parameter, i.e., number of subclasses. To reduce the domain knowledge dependency and to make the approach less sensitive to the subclass parameter, we extend the proposed SRS-SVM to create a robust and improved hierarchical model termed as the Hierarchical Subclass Reduced Set SVM (HSRS-SVM). Since SRS-SVM and HSRS-SVM splits non-linear optimization problem into multiple (smaller) linear optimization problems, both of them are amenable to parallelization. The effectiveness of the proposed approaches is evaluated on four synthetic and six real-world datasets. The performance is also compared with traditional solver (LibSVM) and state-of-the-art approaches such as divide-and-conquer SVM, FastFood, and LLSVM. The experimental results demonstrate that the proposed approach achieves similar classification accuracies while requiring fewer folds of reduced computation time as compared to existing solvers. We further demonstrate the suitability and improved performance of the proposed HSRS-SVM with deep learning features for face recognition using Labeled Faces in the Wild (LFW) dataset.","Support vector machines, Subclass, Subcluster, Piece-wise linear solutions, Large scale learning",Tejas Indulal Dhamecha and Afzel Noore and Richa Singh and Mayank Vatsa,https://www.sciencedirect.com/science/article/pii/S0031320319301517,https://doi.org/10.1016/j.patcog.2019.04.012,0031-3203,2019,173--190,95,Pattern Recognition,Between-subclass piece-wise linear solutions in large scale kernel SVM learning,article,DHAMECHA2019173,
"Clustering methods are becoming key as analysts try to understand what knowledge is buried inside contemporary large data sets. This article analyzes the impact of six different Hausdorff distances on sets of multivariate interval data (where, for each dimension, an interval is defined as an observation [a, b] with aâ¯â¤â¯b and with a and b taking values on the real line R1), used as the basis for Chaventâs [15, 16] divisive clustering algorithm. Advantages and disadvantages are summarized for each distance. Comparisons with two other distances for interval data, the GowdaâDiday and IchinoâYaguchi measures are included. All have specific strengths depending on the type of data present. Global normalization of a distance is not recommended; and care needs to be made when using local normalizations to ensure the features of the underlying data sets are revealed. The study is based on sets of simulated data, and on a real data set.","Interval data, Divisive clustering, Hausdorff distances, GowdaâDiday distances, IchinoâYaguchi distances, Span normalization, Euclidean normalization, Local and global normalizations",Yi Chen and L. Billard,https://www.sciencedirect.com/science/article/pii/S0031320319302729,https://doi.org/10.1016/j.patcog.2019.106969,0031-3203,2019,106969,96,Pattern Recognition,A study of divisive clustering with Hausdorff distances for interval data,article,CHEN2019106969,
"The literature postulates that the dynamic time warping (dtw) distance can cope with temporal variations but stores and processes time series in a form as if the dtw-distance cannot cope with such variations. To address this inconsistency, we first show that the dtw-distance is not warping-invariantâdespite its name and contrary to its characterization in some publications. The lack of warping-invariance contributes to the inconsistency mentioned above and to a strange behavior. To eliminate these peculiarities, we convert the dtw-distance to a warping-invariant semi-metric, called time-warp-invariant (twi) distance. Empirical results suggest that the error rates of the twi and dtw nearest-neighbor classifier are practically equivalent in a Bayesian sense. However, the twi-distance requires less storage and computation time than the dtw-distance for a broad range of problems. These results challenge the current practice of applying the dtw-distance in nearest-neighbor classification and suggest the proposed twi-distance as a more efficient and consistent option.","Time series, Dynamic time warping, Semi-metric, Nearest-neighbor rule",Brijnesh J. Jain,https://www.sciencedirect.com/science/article/pii/S0031320319301918,https://doi.org/10.1016/j.patcog.2019.05.016,0031-3203,2019,35--52,94,Pattern Recognition,Making the dynamic time warping distance warping-invariant,article,JAIN201935,
"We propose a novel method to group individuals in daily life images based on their interactions. It is extremely useful to automatically understand the mutual relation between individuals appearing in images and to group them based on their interaction. However, this is an extremely challenging task. In this paper, we determine the interacting groups more precisely by considering not only the spatial information of individuals in an image but also the emotional correlation between individuals. This is in contrast to the existing methods of determining groups based on the distribution of individuals. In order to evaluate the performance of the proposed method, we created a new dataset with the images acquired in everyday life to address the issue of detecting interacting groups. The results of intensive comparison experiments with the existing grouping methods reveal that the proposed method exhibits superior performance in determining the interacting group.","Computer vision, Detection, Combinatorial optimization",Haanju Yoo and Taekyu Eom and Jeongmin Seo and Sang-Il Choi,https://www.sciencedirect.com/science/article/pii/S0031320319301761,https://doi.org/10.1016/j.patcog.2019.05.001,0031-3203,2019,498--506,93,Pattern Recognition,Detection of interacting groups based on geometric and social relations between individuals in an image,article,YOO2019498,
"Sparse representation is a useful tool in machine learning and pattern recognition area. Sparse graphs (graphs constructed using sparse representation of data) proved to be very informative graphs for many learning tasks such as label propagation, embedding, and clustering. It has been shown that constructing an informative graph is one of the most important steps since it significantly affects the final performance of the post graph-based learning algorithm. In this paper, we introduce a new sparse graph construction method that integrates manifold constraints on the unknown sparse codes as a graph regularizer. These constraints seem to be a natural regularizer that was discarded in existing state-of-the art graph construction methods. This regularizer imposes constraints on the graph coefficients in the same way a locality preserving constraint imposes on data projection in non-linear manifold learning. The proposed method is termed Sparse Graph with Laplacian Smoothness (SGLS). We also propose a kernelized version of the SGLS method. A series of experimental results on several public image datasets show that the proposed methods can out-perform many state-of-the-art methods for the tasks of label propagation, nonlinear and linear embedding.","Graph construction, Sparse representation, Manifold constraints, Laplacian smoothness, Graph-based semi-supervised learning, Label propagation, Graph-based embedding, Classification,",F. Dornaika and L. Weng,https://www.sciencedirect.com/science/article/pii/S0031320319302456,https://doi.org/10.1016/j.patcog.2019.06.015,0031-3203,2019,285--295,95,Pattern Recognition,Sparse graphs with smoothness constraints: Application to dimensionality reduction and semi-supervised classification,article,DORNAIKA2019285,
"This paper presents a novel approach to perform fast approximate nearest neighbors search in high dimensional data, using a nearest neighbor graph created over large collections. This graph is created based on the fusion of multiple hierarchical clustering results, where a minimum-spanning-tree structure is used to connect all elements in a cluster. We propose a novel search technique to guide the navigation on the graph without computing exhaustively the distances to all neighbors in each step of the search, just to those in the direction of the query. The objective is to determine the nearest point to the query with a few number of distance calculations. We experimented in three datasets of 1 million SIFT, GIST, and GloVe features. Results show better speedups than another graph-based technique, and competitive speedups at high recall values when compared to classic and recent state-of-the-art techniques.","Approximate nearest neighbors search, Graph-based search, Hierarchical clustering, Guided search",Javier {Vargas MuÃ±oz} and Marcos A. GonÃ§alves and Zanoni Dias and Ricardo {da S. Torres},https://www.sciencedirect.com/science/article/pii/S0031320319302730,https://doi.org/10.1016/j.patcog.2019.106970,0031-3203,2019,106970,96,Pattern Recognition,Hierarchical Clustering-Based Graphs for Large Scale Approximate Nearest Neighbor Search,article,VARGASMUNOZ2019106970,
"Numerous methods have been proposed for person re-identificationÂ (Re-ID) with promising performances. While most of them neglect the matching efficiency which is crucial in real-world applications. Recently, several hashing based approaches have been developed, which consider the importance of matching speed in large-scale datasets. Despite the considerable efficiency of these traditional and deep learning based hashing methods, the concomitant matching accuracy reduction is unacceptable in practical application. Towards this end, we propose a novel deep hashing framework, namely Consistency-Preserving Deep Hashing (CPDH), aiming to bridge the gap between the effective high-dimensional feature and low-dimensional binary vector by focusing on the consistency preservation of hash code. First, CPDH designs a new hash structure to extract the hash code. Next, a noise consistency cost is proposed to improve robustness of both hash code and high-dimensional feature. Finally, a topology consistency cost is provided to maintain the ordinal relation between the high-dimensional feature space and Hamming space. Comprehensive experimental results on three widely-used benchmark datasets demonstrate the superior performance of proposed method as compared with existing state-of-the-art approaches.","Convolutional neural network, Fast person re-identification, Deep hashing, Consistency preservation",Diangang Li and Yihong Gong and De Cheng and Weiwei Shi and Xiaoyu Tao and Xinyuan Chang,https://www.sciencedirect.com/science/article/pii/S0031320319302213,https://doi.org/10.1016/j.patcog.2019.05.036,0031-3203,2019,207--217,94,Pattern Recognition,Consistency-Preserving deep hashing for fast person re-identification,article,LI2019207,
"In this paper, we investigate the problem of video-based person re-identification (re-id) which matches peopleâs video clips across non-overlapping camera views at different time. A key challenge of video-based person re-id is a personâs appearance and motion would always display differently and take effects unequally at disjoint camera views due to the change of lighting, viewpoint, background and etc., which we call the âview-biasâ problem. However, many previous video-based person re-id approaches have not quantified the importance of different types of features at different camera views, so that the two types of important features (i.e. appearance and motion features) do not collaborate effectively and thus the âview-biasâ problem remains unsolved. To address this problem, we propose a Deep Asymmetric Metric learning (DAM) method that embeds a proposed asymmetric distance metric learning loss into a two-stream deep neural network for jointly learning view-specific and feature-specific transformations to overcome the âview-biasâ problem in video-based person re-id. As learning these view-specific transformations become expensive when there are large amount of camera views, a clustering-based DAM method is developed to make our DAM scalable. Extensive evaluations have been carried out on three public datasets: PRID2011, iLIDS-VID and MARS. Our results verify that learning view-specific and feature-specific transformations are beneficial, and the presented DAM has empirically performed more effectively overall for video-based person re-id on challenging benchmarks.","Person re-identification, Visual surveillance",Jingke Meng and Ancong Wu and Wei-Shi Zheng,https://www.sciencedirect.com/science/article/pii/S0031320319301487,https://doi.org/10.1016/j.patcog.2019.04.008,0031-3203,2019,430--441,93,Pattern Recognition,Deep asymmetric video-based person re-identification,article,MENG2019430,
"While weight sparseness-based regularization has been used to learn better deep features for image recognition problems, it introduced a large number of variables for optimization and can easily converge to a local optimum. The L2-norm regularization proposed for face recognition reduces the impact of the noisy information, while expression information is also suppressed during the regularization. A feature sparseness-based regularization that learns deep features with better generalization capability is proposed in this paper. The regularization is integrated into the loss function and optimized with a deep metric learning framework. Through a toy example, it is showed that a simple network with the proposed sparseness outperforms the one with the L2-norm regularization. Furthermore, the proposed approach achieved competitive performances on four publicly available datasets, i.e., FER2013, CK+, Oulu-CASIA and MMI. The state-of-the-art cross-database performances also justify the generalization capability of the proposed approach.","Expression recognition, Feature sparseness, Deep metric learning, Fine tuning, Generalization capability",Weicheng Xie and Xi Jia and Linlin Shen and Meng Yang,https://www.sciencedirect.com/science/article/pii/S0031320319302699,https://doi.org/10.1016/j.patcog.2019.106966,0031-3203,2019,106966,96,Pattern Recognition,Sparse deep feature learning for facial expression recognition,article,XIE2019106966,
"General object detection task mainly takes axis-aligned bounding-boxes as the detection outputs. To address more challenging scenarios, such as curved text detection and multi-oriented object detection in aerial images, we propose a novel two-stage approach for shape robust object detection. In the first stage, a locally sliding line-based point regression (LocSLPR) approach is presented to estimate the outline of the object, which is denoted as the intersections of the sliding lines and the bounding-box of the object. To make full use of information, we only regress partial coordinates and calculate the remaining coordinates by the sliding rule. We find that regression can achieve higher precision with fewer parameters than the segmentation method. In the second stage, a rotated cascade region-based convolutional neural network (RCR-CNN) is used to gradually regress the target object, which can further improve the performance of our system. Experiments demonstrate that our method achieves state-of-the-art performance in several quadrangular object detection tasks. For example, our method yielded a score of 0.796 in the ICPR 2018 Contest on Robust Reading for Multi-Type Web Images, where we won first place for text detection tasks. The method also achieved 69.2% mAP on Task 1 of the ICPR 2018 Contest on Object Detection in Aerial Images, which was our best single model, where we also won first place. In addition, the method outperforms the previously published best record on the curved text dataset (CTW1500).","Object detection, Text detection, Aerial images, Curved text, Rotated cascade R-CNN",Yixing Zhu and Chixiang Ma and Jun Du,https://www.sciencedirect.com/science/article/pii/S0031320319302614,https://doi.org/10.1016/j.patcog.2019.106964,0031-3203,2019,106964,96,Pattern Recognition,Rotated cascade R-CNN: A shape robust detector with coordinate regression,article,ZHU2019106964,
We propose a reinforced quasi-random forest for classification task. Reinforcement is performed iteratively by adding new trees to the forest. Our method assigns an importance to each of the attributes and identifies the attributes that causes the mis-classification of data points during training. The new trees are constructed using the mis-classified data points with reduced set of attributes. The attributes for splitting the nodes of the reinforced trees are found in a deterministic manner. Hence the new trees are quasi-random in nature. The best out of all the new trees are found using a novel electrostatic model. These trees are termed as reinforced trees. Additions of reinforced trees to the existing forest ensure maximum reduction in classification error. The efficacy of the proposed method is established through experiments on breast cancer datasets for detecting mitotic nuclei. Results of our method show significant improvement compared to other state-of-the-art approaches. Results on benchmark datasets show as much as 14% reduction in classification error.,"Random forest, Reinforcement learning, Orthogonal decision trees, Importance of attributes",Angshuman Paul and Dipti Prasad Mukherjee,https://www.sciencedirect.com/science/article/pii/S0031320319301888,https://doi.org/10.1016/j.patcog.2019.05.013,0031-3203,2019,13--24,94,Pattern Recognition,Reinforced quasi-random forest,article,PAUL201913,
"Multi-Person Tracking (MPT) is often addressed within the detection-to-association paradigm. In such approaches, human detections are first extracted in every frame and person trajectories are then recovered by a procedure of data association (usually offline). However, their performances usually degenerate in presence of detection errors, mutual interactions and occlusions. In this paper, we present a deep learning based MPT approach that learns instance-aware representations of tracked persons and robustly online infers states of the tracked persons. Specifically, we design a multi-branch neural network (MBN), which predicts the classification confidences and locations of all targets by taking a batch of candidate regions as input. In our MBN architecture, each branch (instance-subnet) corresponds to an individual to be tracked and new branches can be dynamically created for handling newly appearing persons. Then based on the output of MBN, we construct a joint association matrix that represents meaningful states of tracked persons (e.g., being tracked or disappearing from the scene) and solve it by using the efficient Hungarian algorithm. Moreover, we allow the instance-subnets to be updated during tracking by online mining hard examples, accounting to person appearance variations over time. We comprehensively evaluate our framework on a popular MPT benchmark, demonstrating its excellent performance in comparison with recent online MPT methods.","Representation learning, Online tracking, Multi-person tracking, Data association",Hefeng Wu and Yafei Hu and Keze Wang and Hanhui Li and Lin Nie and Hui Cheng,https://www.sciencedirect.com/science/article/pii/S0031320319301645,https://doi.org/10.1016/j.patcog.2019.04.018,0031-3203,2019,25--34,94,Pattern Recognition,Instance-aware representation learning and association for online multi-person tracking,article,WU201925,
"Extreme learning machines (ELMs), especially kernel ELMs (KELMs), have achieved great success in providing efficient and effective solutions to classification applications. This paper proposes a simple but effective expectation kernel ELM (EKELM) to improve the classification abilities of ELMs. EKELM is based on a new family of positive semidefinite (PSD) kernel functions, namely expectation kernels (EKs), to learn similarities between data samples by combining the advances of random feature mapping and conventional kernel functions. EK provides a new insight to model ELMs from the perspective of kernel approximation using random sampling techniques. Particularly, we show that the distribution of random sampling weights, i.e. the input weight of ELM, has deep influences on the classification performance, and we use Gaussian distributions to generate random weights in EKELM. Besides, the number of sampling samples, i.e. the number of hidden neurons in ELM, can be reduced by using a proper nonlinear kernel function. We test the proposed EKELM on 20 benchmark classification datasets, the results prove that using a Gaussian distribution to generate random sampling weights has better performance than uniform distribution. Also, the results show that nonlinear RBF EKELM achieves comparable classification performance compared with the conventional RBF kernel, and requires much less random samples compared with linear EKELM.","Bounded random feature mapping, Expectation kernel, Extreme learning machine, Kernel learning, Random sampling",Wenyu Zhang and Zhenjiang Zhang and Lifu Wang and Han-Chieh Chao and Zhangbing Zhou,https://www.sciencedirect.com/science/article/pii/S0031320319302572,https://doi.org/10.1016/j.patcog.2019.07.005,0031-3203,2019,106960,96,Pattern Recognition,Extreme learning machines with expectation kernels,article,ZHANG2019106960,
"Binarization is often the first step in many document analysis tasks and plays a key role in the subsequent steps. In this paper, we formulate binarization as an image-to-image generation task and introduce the conditional generative adversarial networks (cGANs) to solve the core problem of multi-scale information combination in binarization task. Our generator consists of two stages: In the first stage, sub-generator G1 learns to extract text pixels from an input image. Different scales of the input image are processed by G1 and corresponding binary images are generated. In the second stage, our sub-generator G2 learns a combination of results at different scales from the first stage and produces the final binary result. We conduct comprehensive experiments of the proposed method on nine public document image binarization datasets. Experimental results show that compared with many classical and state-of-the-art approaches, our method gains promising performance in the accuracy and robustness of binarization.","Cascaded generator, Conditional generative adversarial networks, Document image binarization, Image generation, Historical document analysis",Jinyuan Zhao and Cunzhao Shi and Fuxi Jia and Yanna Wang and Baihua Xiao,https://www.sciencedirect.com/science/article/pii/S0031320319302717,https://doi.org/10.1016/j.patcog.2019.106968,0031-3203,2019,106968,96,Pattern Recognition,Document image binarization with cascaded generators of conditional generative adversarial networks,article,ZHAO2019106968,
"Facial aging variation is a major problem for face recognition systems due to large intra-personal variations caused by age progression. A major challenge is to develop an efficient, discriminative feature representation and matching framework, which is robust to facial aging variations. In this paper, we propose a robust deep-feature encoding-based discriminative model for age-invariant face recognition. Our method learns high-level deep features using a pre-trained deep-CNN model. These features are then encoded by learning a codebook, which converts each of the features into a discriminant S-dimensional codeword for image representation. By incorporating the locality information in the whole learning process, a closed-form solution is obtained for both the codebook-updating and encoding stages. As the features of the same person at different ages should have certain correlations, canonical correlation analysis is utilized to fuse the pair of training features, for two different ages, to make the codebook discriminative in terms of age progression. In the testing stage, the gallery and query image's features are encoded using the learned codebook. Then, linear mapping based on linear regression is employed for face matching. We evaluate our method on three publicly available challenging facial aging datasets, FGNET, MORPH Album 2, and Large Age-Gap (LAG). Experimental results show that our proposed method outperforms various state-of-the-art age-invariant face recognition methods, in terms of the rank-1 recognition accuracy.","Age-invariant face recognition, Canonical correlation analysis, Deep learning, Discriminative model, Feature encoding, Linear regression",M. Saad Shakeel and Kin-Man Lam,https://www.sciencedirect.com/science/article/pii/S0031320319301748,https://doi.org/10.1016/j.patcog.2019.04.028,0031-3203,2019,442--457,93,Pattern Recognition,Deep-feature encoding-based discriminative model for age-invariant face recognition,article,SHAKEEL2019442,
"The primate visual system has inspired the development of deep artificial neural networks, which have revolutionized the computer vision domain. Yet these networks are much less energy-efficient than their biological counterparts, and they are typically trained with backpropagation, which is extremely data-hungry. To address these limitations, we used a deep convolutional spiking neural network (DCSNN) and a latency-coding scheme. We trained it using a combination of spike-timing-dependent plasticity (STDP) for the lower layers and reward-modulated STDP (R-STDP) for the higher ones. In short, with R-STDP a correct (resp. incorrect) decision leads to STDP (resp. anti-STDP). This approach led to an accuracy of 97.2% on MNIST, without requiring an external classifier. In addition, we demonstrated that R-STDP extracts features that are diagnostic for the task at hand, and discards the other ones, whereas STDP extracts any feature that repeats. Finally, our approach is biologically plausible, hardware friendly, and energy-efficient.","Spiking neural networks, Deep architecture, Digit recognition, STDP, Reward-modulated STDP, Latency coding",Milad Mozafari and Mohammad Ganjtabesh and Abbas Nowzari-Dalini and Simon J. Thorpe and TimothÃ©e Masquelier,https://www.sciencedirect.com/science/article/pii/S0031320319301906,https://doi.org/10.1016/j.patcog.2019.05.015,0031-3203,2019,87--95,94,Pattern Recognition,Bio-inspired digit recognition using reward-modulated spike-timing-dependent plasticity in deep convolutional networks,article,MOZAFARI201987,
"Recognizing a person across different non-overlapping camera views, is the task of person re-identification. For achieving the task, an effective way is to learn a discriminative metric by minimizing the within-class variance and maximizing the between-class variance simultaneously. However, the dimension of sample feature vector is usually greater than the number of training samples, as a result, the within-class scatter matrix is singular and the metric cannot be learned. In this paper, we propose to solve the singularity problem by employing the pseudo-inverse of the within-class scatter matrix and learning an orthogonal transformation for the metric. The proposed method can be effectively solved with a closed-form solution and no parameters required to tune. In addition, we develop a kernel version against non-linearity in person re-identification, and a fast version for more efficient solution. In experiments, we prove the validity and advantage of the proposed method for solving the singularity problem in person re-identification, and analyze the effectiveness of both kernel version and fast version. Extensively comparative experiments on VIPeR, PRID2011, CUHK01 and CUHK03 person re-identification benchmark datasets, show the state-of-the-art results of the proposed method.","Person re-identification, Metric learning, Singularity problem, Orthogonal discriminant analysis",Min Cao and Chen Chen and Xiyuan Hu and Silong Peng,https://www.sciencedirect.com/science/article/pii/S0031320319302201,https://doi.org/10.1016/j.patcog.2019.05.035,0031-3203,2019,218--229,94,Pattern Recognition,Towards fast and kernelized orthogonal discriminant analysis on person re-identification,article,CAO2019218,
"Evolving environments challenge researchers with non stationary data flows where the concepts â or states â being tracked can change over time. This requires tracking algorithms suited to represent concept evolution and in some cases, e.g. real industrial environments, also suited to represent time dependent features. This paper proposes a unified approach to track evolving environments that uses a two-stages distance-based and density-based clustering algorithm. In this approach data samples are fed as input to the distance based clustering stage in an incremental, online fashion, and they are then clustered to form Î¼-clusters. The density-based algorithm analyses the micro-clusters to provide the final clusters: thanks to a forgetting process, clusters may emerge, drift, merge, split or disappear, hence following the evolution of the environment. This algorithm has proved to be able to detect high overlapping clusters even in multi-density distributions, making no assumption about cluster convexity. It shows fast response to data streams and good outlier rejection properties.","Dynamic clustering, Data mining, On-line learning, Time-series, Data streams, Multi-density clustering",Nathalie {Barbosa Roa} and Louise TravÃ©-MassuyÃ¨s and Victor H. Grisales-Palacio,https://www.sciencedirect.com/science/article/pii/S0031320319301992,https://doi.org/10.1016/j.patcog.2019.05.024,0031-3203,2019,162--186,94,Pattern Recognition,DyClee: Dynamic clustering for tracking evolving environments,article,BARBOSAROA2019162,
"In this paper, a graph-based, supervised classification method for multimodal data is introduced. It can be applied on data of any type consisting of any number of modalities and can also be used for the classification of datasets with missing modalities. The proposed method maps the features extracted from every modality to a space where the intrinsic structure of the multimodal data is kept. In order to map the extracted features of the different modalities into the same space and, at the same time, maintain the feature distances between similar and dissimilar modality data instances, a metric learning method is used. The proposed method has been evaluated on NUS-Wide, NTU-RGBD and AV-Letters multimodal datasets and has shown competitive results with the state-of-the-art methods in the field, while is able to cope with datasets with missing modalities.","Multimodal fusion, Multimodal metric learning, Multimodal classification, Distance graphs",Michalis Angelou and Vassilis Solachidis and Nicholas Vretos and Petros Daras,https://www.sciencedirect.com/science/article/pii/S0031320319302444,https://doi.org/10.1016/j.patcog.2019.06.013,0031-3203,2019,296--307,95,Pattern Recognition,Graph-based multimodal fusion with metric learning for multimodal classification,article,ANGELOU2019296,
"One key issue in managing a large scale 3D shape dataset is to identify an effective way to retrieve a shape-of-interest. The sketch-based query, which enjoys the flexibility in representing the userâs intention, has received growing interests in recent years due to the popularization of the touchscreen technology. Essentially, the sketch depicts an abstraction of a shape in a certain view while the shape contains the full 3D information. Matching between them is a cross-modality retrieval problem, and the state-of-the-art solution is to project the sketch and the 3D shape into a common space with which the cross-modality similarity can be calculated by the feature similarity/distance within. However, for a given query, only part of the viewpoints of the 3D shape is representative. Thus, blindly projecting a 3D shape into a feature vector without considering what is the query will inevitably bring query-unrepresentative information. To handle this issue, in this work we propose a Deep Point-to-Subspace Metric Learning (DPSML) framework to project a sketch into a feature vector and a 3D shape into a subspace spanned by a few selected basis feature vectors. The similarity between them is defined as the distance between the query feature vector and its closest point in the subspace by solving an optimization problem on the fly. Note that, the closest point is query-adaptive and can reflect the viewpoint information that is representative to the given query. To efficiently learn such a deep model, we formulate it as a classification problem with a special classifier design. To reduce the redundancy of 3D shapes, we also introduce a Representative-View Selection (RVS) module to select the most representative views of a 3D shape. By conducting extensive experiments on various datasets, we show that the proposed method can achieve superior performance over its competitive baseline methods and attain the state-of-the-art performance.","Sketch-based 3D shape retrieval, Cross-modality discrepancy, Representative-view selection, Point-to-subspace distance",Yinjie Lei and Ziqin Zhou and Pingping Zhang and Yulan Guo and Zijun Ma and Lingqiao Liu,https://www.sciencedirect.com/science/article/pii/S0031320319302845,https://doi.org/10.1016/j.patcog.2019.106981,0031-3203,2019,106981,96,Pattern Recognition,Deep point-to-subspace metric learning for sketch-based 3D shape retrieval,article,LEI2019106981,
"With the advancement of storage and processing technology, an enormous amount of data is collected on a daily basis in many applications. Nowadays, advanced data analytics have been used to mine the collected data for useful information and make predictions, contributing to the competitive advantages of companies. The increasing data volume, however, has posed many problems to classical batch learning systems, such as the need to retrain the model completely with the newly arrived samples or the impracticality of storing and accessing a large volume of data. This has prompted interest on incremental learning that operates on data streams. In this study, we develop an incremental online multi-label classification (OMLC) method based on a weighted clustering model. The model is made to adapt to the change of data via the decay mechanism in which each sample's weight dwindles away over time. The clustering model therefore always focuses more on newly arrived samples. In the classification process, only clusters whose weights are greater than a threshold (called mature clusters) are employed to assign labels for the samples. In our method, not only is the clustering model incrementally maintained with the revealed ground truth labels of the arrived samples, the number of predicted labels in a sample are also adjusted based on the Hoeffding inequality and the label cardinality. The experimental results show that our method is competitive compared to several well-known benchmark algorithms on six performance measures in both the stationary and the concept drift settings.","Multi-label classification, Incremental learning, Online learning, Clustering, Data stream, Concept drift",Tien Thanh Nguyen and Manh Truong Dang and Anh Vu Luong and Alan Wee-Chung Liew and Tiancai Liang and John McCall,https://www.sciencedirect.com/science/article/pii/S0031320319302328,https://doi.org/10.1016/j.patcog.2019.06.001,0031-3203,2019,96--113,95,Pattern Recognition,Multi-label classification via incremental clustering on an evolving data stream,article,NGUYEN201996,
"Subspace learning for dimensionality reduction is an important topic in pattern analysis and machine learning, and it has extensive applications in feature representation and image classification. Linear discriminant analysis (LDA) is a well-known subspace learning approach for supervised dimensionality reduction due to its effectiveness and efficacy in discriminant analysis. However, LDA is not stable and suffers from the singularity problem when addressing small sample size and high-dimensional data. In this paper, we develop a novel subspace learning model, named sparse approximation to discriminant projection learning (SADPL), to learn the sparse projection matrix. Different from the traditional LDA-based methods, we learn the projection matrix based on a new objective function rather than the Fisher criterion, which avoids the matrix singularity problem. In order to distinguish which features play an important role in discriminant analysis, we embed a feature selection framework to the subspace learning model to select the informative features. Finally, we can attain a convex objective function which can be solved by an effective optimization algorithm, and theoretically prove the convergence of the proposed optimization algorithm. Extensive experiments on all sorts of image classification tasks, such as face recognition, palmprint recognition, object categorization and texture classification show that our SADPL achieves competitive performance compared to the state-of-the-art methods.","Image classification, Feature selection, Subspace learning, Discriminant analysis, Dimensionality reduction",Yu-Feng Yu and Chuan-Xian Ren and Min Jiang and Man-Yu Sun and Dao-Qing Dai and Guodong Guo,https://www.sciencedirect.com/science/article/pii/S0031320319302602,https://doi.org/10.1016/j.patcog.2019.106963,0031-3203,2019,106963,96,Pattern Recognition,Sparse approximation to discriminant projection learning and application to image classification,article,YU2019106963,
"Salient Object Detection (SOD), which aims to find the most important region of interest and segment the relevant objects/items in that region, is an important yet challenging task in computer vision and image processing. This vision problem is inspired by the fact that human perceives the main scene elements with high priorities. Thus, accurate detection of salient objects in complex scenes is critical for human-computer interaction. In this paper, we present a novel reflective feature learning framework, which results in high detection accuracy while maintaining a compact model design. The proposed framework utilizes a hyper-densely reflective feature fusion network (named HyperFusion-Net) to automatically predict the most important area and segment the associated objects in an end-to-end manner. Specifically, inspired by the human perception system and image reflection separation, we first decompose the input images into reflective image pairs by content-preserving transforms. Then, the complementary information of reflective image pairs is jointly extracted by an Interweaved Convolutional Neural Network (ICNN) and hierarchically combined with a hyper-dense fusion mechanism. Based on the fused multi-scale features, our method finally achieves a promising way of predicting salient objects, in which we cast the SOD as a pixel-wise classification problem. Extensive experiments on seven public datasets demonstrate that the proposed method consistently outperforms other state-of-the-art methods with a large margin.","Salient object detection, Image reflection separation, Multiple feature fusion, Convolutional Neural Network",Pingping Zhang and Wei Liu and Yinjie Lei and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320319301876,https://doi.org/10.1016/j.patcog.2019.05.012,0031-3203,2019,521--533,93,Pattern Recognition,Hyperfusion-Net: Hyper-densely reflective feature fusion for salient object detection,article,ZHANG2019521,
"Instance selection is one of the most important preprocessing steps in many machine learning tasks. Due to huge data size that is common in many current problems, removing redundant, useless, erroneous or noisy instances is a frequent initial step that is performed before other data mining algorithms are applied. Instance selection as part of this data reduction task is a relevant problem in current data mining research. Many instance selection methods hypothesize a certain way of characterizing the most important instances and then implement an algorithm to keep those important instances. The problem with these methods is that their success depends on the data fulfilling the underlying hypothesis. Other methods just add or delete instances considering only their effect on the accuracy of the nearest neighbor rule. In this paper, we present a new method of this second kind that uses boosting to obtain a subset of instances that is able to improve the classification accuracy of the whole dataset with a significant reduction. The instances are incrementally added by selecting those that maximize the accuracy of the subset using the weighting of instances from the construction of ensembles of classifiers and the step-wise addition of new instances. The method is compared using a large set of 205 different datasets with standard methods and shows the best overall performance.","Instance selection, Boosting, Instance-based learning",Aida {de Haro-GarcÃ­a} and Gonzalo Cerruela-GarcÃ­a and NicolÃ¡s GarcÃ­a-Pedrajas,https://www.sciencedirect.com/science/article/pii/S0031320319302560,https://doi.org/10.1016/j.patcog.2019.07.004,0031-3203,2019,106959,96,Pattern Recognition,Instance selection based on boosting for instance-based learners,article,DEHAROGARCIA2019106959,
"Recently mutual information based feature selection criteria have gained popularity for their superior performances in different applications of pattern recognition and machine learning areas. However, these methods do not consider the correction while computing mutual information for finite samples. Again, finding appropriate discretization of features is often a necessary step prior to feature selection. However, existing researches rarely discuss both discretization and feature selection simultaneously. To solve these issues, Joint Bias corrected Mutual Information (JBMI) is firstly proposed in this paper for feature selection. Secondly, a framework namely modified discretization and feature selection based on mutual information is proposed that incorporates JBMI based feature selection and dynamic discretization, both of which use a Ï2 based searching method. Experimental results on thirty benchmark datasets show that in most of the cases, the proposed methods outperform the state-of-the-art methods.","Feature selection, Mutual information, Bias, Dynamic discretization",Sadia Sharmin and Mohammad Shoyaib and Amin Ahsan Ali and Muhammad Asif Hossain Khan and Oksam Chae,https://www.sciencedirect.com/science/article/pii/S0031320319300809,https://doi.org/10.1016/j.patcog.2019.02.016,0031-3203,2019,162--174,91,Pattern Recognition,Simultaneous feature selection and discretization based on mutual information,article,SHARMIN2019162,
"In recent years, there has been a growing interest in the study of dictionary learning for face recognition. Most of the conventional dictionary learning methods focus only on a single resolution, which ignores the variability of resolutions of real-world face images. In order to address the above issue, this paper proposes a novel multi-resolution dictionary learning method that provides multiple dictionaries each being associated with a resolution. Especially, to enhance the robustness of the model, our method adds a relatively strong constraint to keep the similarity of representations obtained using different dictionaries in the training phase. We compare the proposed method to several state-of-the-art dictionary learning methods by applying this method to multi-resolution face recognition. The experimental results demonstrate that our method outperforms many recently proposed dictionary learning methods. The MATLAB codes of the proposed method will be available at http://www.yongxu.org/lunwen.html.","Dictionary learning, Multi-resolution, Face recognition",Xiaoling Luo and Yong Xu and Jian Yang,https://www.sciencedirect.com/science/article/pii/S0031320319301736,https://doi.org/10.1016/j.patcog.2019.04.027,0031-3203,2019,283--292,93,Pattern Recognition,Multi-resolution dictionary learning for face recognition,article,LUO2019283,
"In this paper, we develop a novel Dirichlet densifier that can be used to increase the edge density in undirected graphs. Dirichlet densifiers are implicit minimizers of the spectral gap for the Laplacian spectrum of a graph. One consequence of this property is that they can be used improve the estimation of meaningful commute distances for mid-size graphs by means of topological modifications of the original graphs. This results in a better performance in clustering and ranking. To do this, we identify the strongest edges and from them construct the so called line graph, where the nodes are the potential qâstep reachable edges in the original graph. These strongest edges are assumed to be stable. By simulating random walks on the line graph, we identify potential new edges in the original graph. This approach is fully unsupervised and it is both more scalable and robust than recent explicit spectral methods, such as the Semi-Definite Programming (SDP) densifier and the sufficient condition for decreasing the spectral gap. Experiments show that our method is only outperformed by some choices of the parameters of a related method, the anchor graph, which relies on pre-computing clusters representatives, and that the proposed method is effective on a variety of real-world datasets.","Graph densification, Dirichlet problems, Random walkers, Commute times",Manuel Curado and Francisco Escolano and Miguel A. Lozano and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320319300767,https://doi.org/10.1016/j.patcog.2019.02.012,0031-3203,2019,56--68,91,Pattern Recognition,Dirichlet densifiers for improved commute times estimation,article,CURADO201956,
"Many recent algorithms based on convolutional neural network (CNN) for blind image quality assessment (BIQA) share a common two-stage structure, i.e., local quality measurement followed by global pooling. In this paper, we mainly focus on the pooling stage and propose an attention-based pooling network (APNet) for BIQA. The core idea is to introduce a learnable pooling that can model human visual attention in a data-driven manner. Specifically, the APNet is built by incorporating an attention module and allows for a joint learning of local quality and local weights. It can automatically learn to assign visual weights while generating quality estimations. Moreover, we further introduce a correlation constraint between the estimated local quality and attention weight in the network to regulate the training. The constraint penalizes the case in which the local quality estimation on a region attracting more attention differs a lot from the overall quality score. Experimental results on benchmark databases demonstrate that our APNet achieves state-of-the-art prediction accuracy. By yielding an attention weight map as by-product, our model gives a better interpretability on the learned pooling.","Image quality assessment, Perceptual image quality, Visual attention, Convolutional neural network, Learnable pooling",Jie Gu and Gaofeng Meng and Shiming Xiang and Chunhong Pan,https://www.sciencedirect.com/science/article/pii/S0031320319300925,https://doi.org/10.1016/j.patcog.2019.02.021,0031-3203,2019,332--344,91,Pattern Recognition,Blind image quality assessment via learnable attention-based pooling,article,GU2019332,
"We propose an efficient and robust algorithm to reconstruct the volumes of multi-labeled objects from sets of cross sections without overlapping regions, artificial gaps, or mismatched interfaces. The algorithm can handle cross sections wherein different regions have different labels. The present study represents a multicomponent extension of our previous work (Li etÂ al. (2015), [1]), wherein we modified the original CahnâHilliard (CH) equation by adding a fidelity term to keep the solution close to the single-labeled slice data. The classical CH equation possesses desirable properties, such as smoothing and conservation. The key idea of the present work is to employ a multicomponent CH system to reconstruct multicomponent volumes without self-intersections. We utilize the linearly stabilized convex splitting scheme introduced by Eyre with the Fourier-spectral method so that we can use a large time step and solve the discrete equation quickly. The proposed algorithm is simple and produces smooth volumes that closely preserve the original volume data and do not self-intersect. Numerical results demonstrate the effectiveness and robustness of the proposed method.","Multicomponent, Volume reconstruction, CahnâHilliard equation, Cross section interpolating",Yibao Li and Jing Wang and Bingheng Lu and Darae Jeong and Junseok Kim,https://www.sciencedirect.com/science/article/pii/S0031320319301463,https://doi.org/10.1016/j.patcog.2019.04.006,0031-3203,2019,124--133,93,Pattern Recognition,Multicomponent volume reconstruction from slice data using a modified multicomponent CahnâHilliard system,article,LI2019124,
"Network traffic classification is an essential component for service differentiation, network design and management and security systems. The limitations of traditional port-based and payload methods have been addressed by recent promising studies which rely on the analysis of the statistics of traffic flows and the use of machine learning techniques. However, due to the high cost of manual labeling, it is hard to obtain sufficient, reliable and up-to-date labeled data for effective IP traffic classification. This paper proposes a novel semi-supervised approach, called SemTra, which automatically alleviates the shortage of labeled flows for machine learning by exploiting the advantages of both supervised and unsupervised models. In particular, SemTra involves the following: (i) generating multi-view representations of the original data based on dimensionality reduction methods to have strong discrimination ability, (ii) incorporating the generated representations into the ensemble clustering model to provide a combined clustering output with better quality and stability, (iii) adapting the concept of self-training to iteratively utilize the few labeled data along with unlabeled within local and global viewpoints; and (iiii) obtaining the final class decision by combining the decisions of mapping strategy of clusters, the local self-training and global self-training approaches. Extensive experiments were carried out to compare the effectiveness of SemTra over representative semi-supervised methods using sixteen network traffic datasets.","Internet traffic classification, Semi-supervised learning, Multiview",Adil Fahad and Abdulmohsen Almalawi and Zahir Tari and Kurayman Alharthi and Fawaz S. {Al Qahtani} and Mohamed Cheriet,https://www.sciencedirect.com/science/article/pii/S0031320319300652,https://doi.org/10.1016/j.patcog.2019.02.001,0031-3203,2019,1--12,91,Pattern Recognition,SemTra: A semi-supervised approach to traffic flow labeling with minimal human effort,article,FAHAD20191,
"Subspace learning is a matrix decomposition method. Some algorithms apply subspace learning to feature selection, but they ignore the local discriminative information contained in data. In this paper, we propose a new unsupervised feature selection algorithm to address this issue, which is called local discriminative based sparse subspace learning for feature selection (LDSSL). We first introduce a local discriminant model in our feature selection framework of subspace learning. This model preserves both the local discriminant structure and local geometric structure of the data, simultaneously. It can not only improve the discriminate ability of the algorithm, but also utilize the local geometric structure information contained in data. Local discriminant model is a linear model, which cannot deal with nonlinear data effectively. Therefore, we need to kernelize the local discriminant model to get a nonlinear version. We next introduce the L1-norm to constrain the feature selection matrix, and this can ensure the sparsity of the feature selection matrix and improve the algorithm's discriminate ability. Then we give the objective function, convergence proof and iterative update rules of the algorithm. We compare LDSSL with eight state-of-the-art algorithms on six datasets. The experimental results show that LDSSL is more effective than eight other feature selection algorithms.","Local discriminant model, Subspace learning, Sparse constraint, Feature selection",Ronghua Shang and Yang Meng and Wenbing Wang and Fanhua Shang and Licheng Jiao,https://www.sciencedirect.com/science/article/pii/S0031320319301347,https://doi.org/10.1016/j.patcog.2019.03.026,0031-3203,2019,219--230,92,Pattern Recognition,Local discriminative based sparse subspace learning for feature selection,article,SHANG2019219,
"Spiking neural networks (SNNs) equipped with latency coding and spike-timing dependent plasticity rules offer an alternative to solve the data and energy bottlenecks of standard computer vision approaches: they can learn visual features without supervision and can be implemented by ultra-low power hardware architectures. However, their performance in image classification has never been evaluated on recent image datasets. In this paper, we compare SNNs to auto-encoders on three visual recognition datasets, and extend the use of SNNs to color images. The analysis of the results helps us identify some bottlenecks of SNNs: the limits of on-center/off-center coding, especially for color images, and the ineffectiveness of current inhibition mechanisms. These issues should be addressed to build effective SNNs for image recognition.","Feature learning, Unsupervised learning, Spiking neural networks, Spike-timing dependent plasticity, Auto-encoders, Image recognition",Pierre Falez and Pierre Tirilly and Ioan Marius Bilasco and Philippe Devienne and Pierre Boulet,https://www.sciencedirect.com/science/article/pii/S0031320319301621,https://doi.org/10.1016/j.patcog.2019.04.016,0031-3203,2019,418--429,93,Pattern Recognition,Unsupervised visual feature learning with spike-timing-dependent plasticity: How far are we from traditional feature learning approaches?,article,FALEZ2019418,
"In this paper, we propose a multi-level thresholding model based on gray-level & local-average histogram (GLLA) and TsallisâHavrdaâCharvÃ¡t entropy for RGB color image. We validate the multi-level thresholding formulation by using the mathematical induction method. We apply particle swarm optimization (PSO) algorithm to obtain the optimal threshold values for each component of a RGB image. By assigning the mean values from each thresholded class, we obtain three segmented component images independently. We conduct the experiments extensively on The Berkeley Segmentation Dataset and Benchmark (BSDS300) and calculate the average four performance indices (BDE, PRI, GCE and VOI) to show the effectiveness and reasonability of the proposed method.","Color image segmentation, Multi-level thresholding, Two-dimensional histogram, TsallisâHavrdaâCharvÃ¡t entropy, PSO",Surina Borjigin and Prasanna K. Sahoo,https://www.sciencedirect.com/science/article/pii/S0031320319301104,https://doi.org/10.1016/j.patcog.2019.03.011,0031-3203,2019,107--118,92,Pattern Recognition,Color image segmentation based on multi-level TsallisâHavrdaâCharvÃ¡t entropy and 2D histogram using PSO algorithms,article,BORJIGIN2019107,
"Nowadays, a lot of new classification and clustering techniques have been proposed for microarray data analysis. However, the multiclass microarray data classification is still regarded as a tough task because of the small sample size problem and the class imbalance problem. In this paper, we propose a novel error correcting output code (ECOC) algorithm for the classification of multiclass microarray data based on the data complexity (DC) theory. In this algorithm, an ECOC coding matrix is generated based on a hierarchical partition of the class space with the aim of Minimizing Data Complexity (named as ECOC-MDC). As the partition process can be mapped as a binary tree, a compact ensemble with high discrimination power is produced. The performance of ECOC-MDC is compared with some state-of-art ECOC algorithms on six multiclass microarray data sets, and it is found that the proposed algorithm can obtain better results in most cases. The correlation between DC measures and the dichotomizersâ performances is checked, and the observations confirm that high complexity in data usually leads to high error rates of the connected dichotomizers. But the error correcting mechanism in the ECOC framework can effectively improve our algorithm's generalization ability. In short, ECOC-MDC can produce a compact ensemble system with high error correction capability through the application of diverse DC measures. Our Matlab code is available at: github.com/MLDMXM2017/ECOC-MDC.","Error correcting output codes (ECOC), Data complexity, Microarray data, Multiclass",MengXin Sun and KunHong Liu and QingQiang Wu and QingQi Hong and BeiZhan Wang and Haiying Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319300378,https://doi.org/10.1016/j.patcog.2019.01.047,0031-3203,2019,346--362,90,Pattern Recognition,A novel ECOC algorithm for multiclass microarray data classification based on data complexity analysis,article,SUN2019346,
"Early action detection aims to detect the action as soon as possible, and has a variety of applications. In the field of computer vision, the first learning formulation on early event detection, the max-margin early event detector(MMED), is investigated by Hoai and Torre [1, 2]. In this study, the purpose of the proposed approach is to reduce the large number of constraints generated in the MMED method. The basic idea is to remove the redundancy among every three consecutive constraints in the MMED method. The number of constraints is reduced from O(IL3) to O(IL2) where I is the number of training examples and L is the length of the example time series. Proof and the experimental results are provided to show the correctness and feasibility of the proposed approach.","Early detection, Max-margin, MMED",Zhi-Fang Yang and Yi-Cyuan Lin,https://www.sciencedirect.com/science/article/pii/S0031320319300810,https://doi.org/10.1016/j.patcog.2019.02.017,0031-3203,2019,111--122,91,Pattern Recognition,Reduction in number of constraints in max-margin early event detectors,article,YANG2019111,
"Makeup is widely used to improve facial attractiveness and is well accepted by the public. However, different makeup styles will result in significant facial appearance changes. It remains a challenging problem to match makeup and non-makeup face images. This paper proposes a learning from generation approach for makeup-invariant face verification by introducing a bi-level adversarial network (BLAN). To alleviate the negative effects from makeup, we first generate non-makeup images from makeup ones, and then use the synthesized non-makeup images for further verification. Specifically, there are two adversarial sub-networks on different levels in BLAN, with the one on pixel level for reconstructing appealing facial images and the other on feature level for preserving identity information. For the non-makeup image generation module, a two-path network that involves both global and local structures is applied to improve the synthesis quality. Moreover, we make the generator well constrained by incorporating multiple perceptual losses. All the modules are embedded in an end-to-end network and jointly reduce the sensing gap between makeup and non-makeup images. Experimental results on three benchmark makeup face datasets demonstrate that our method achieves state-of-the-art verification accuracy across makeup status and can produce photo-realistic non-makeup face images.","Face verification, Makeup-invariant, Generative adversarial network",Yi Li and Lingxiao Song and Xiang Wu and Ran He and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S003132031930024X,https://doi.org/10.1016/j.patcog.2019.01.013,0031-3203,2019,99--108,90,Pattern Recognition,Learning a bi-level adversarial network with global and local perception for makeup-invariant face verification,article,LI201999,
"Recent studies have shown that aggregating convolutional features of a Convolutional Neural Network (CNN) can obtain impressive performance for a variety of computer vision tasks. The Symmetric Positive Definite (SPD) matrix becomes a powerful tool due to its remarkable ability to learn an appropriate statistic representation to characterize the underlying structure of visual features. In this paper, we propose a method of aggregating deep convolutional features into a robust representation through the SPD generation and the SPD transformation under an end-to-end deep network. To this end, several new layers are introduced in our method, including a nonlinear kernel generation layer, a matrix transformation layer, and a vector transformation layer. The nonlinear kernel generation layer is employed to aggregate convolutional features into a kernel matrix which is guaranteed to be an SPD matrix. The matrix transformation layer is designed to project the original SPD representation to a more compact and discriminative SPD manifold. The vectorization and normalization operations are performed in the vector transformation layer to take the upper triangle elements of the SPD representation and carry out the power normalization and l2 normalization to reduce the redundancy and accelerate the convergence. The SPD matrix in our network can be considered as a mid-level representation bridging convolutional features and high-level semantic features. Results of extensive experiments show that our method notably outperforms state-of-the-art methods.","Feature aggregation, SPD Matrix, Riemannian manifold, Deep convolutional network",Zhi Gao and Yuwei Wu and Xingyuan Bu and Tan Yu and Junsong Yuan and Yunde Jia,https://www.sciencedirect.com/science/article/pii/S0031320319301062,https://doi.org/10.1016/j.patcog.2019.03.007,0031-3203,2019,1--12,92,Pattern Recognition,Learning a robust representation via a deep network on symmetric positive definite manifolds,article,GAO20191,
"Person re-identification (re-id) is to match people across disjoint camera views in a multi-camera system, and re-id has been an important technology applied in smart city in recent years. However, the majority of existing person re-id methods assumes all data samples are available in advance for training. However, in a real-world scenario person images detected from multi-camera system are coming sequentially, and thus these methods are not designed for processing sequential data in an online way. While there is a few work on discussing online re-id, most of them require considerable storage of all passed labelled data samples that have been ever observed. In this work, we present an one-pass person re-id model that adapts the re-id model based on each newly observed data and no passed data are required for each update. More specifically, we develop a Sketch online Discriminant Analysis (SoDA) by embedding sketch processing into Fisher discriminant analysis (FDA). SoDA can efficiently keep the main data variations of all passed samples in a low rank matrix when processing sequential data samples, and estimate the approximate within-class variance (i.e. within-class covariance matrix) from the sketch data information. We provide theoretical analysis on the effect of the estimated approximate within-class covariance matrix. In particular, we derive upper and lower bounds on the Fisher discriminant score (i.e. the quotient between between-class variation and within-class variation after feature transformation) in order to investigate how the optimal feature transformation learned by SoDA sequentially approximates the offline FDA that is learned on all observed data. Extensive experimental results have shown the effectiveness of our SoDA and empirically support our theoretical analysis.","Online learning, Person re-identification, Discriminant feature extraction",Wei-Hong Li and Zhuowei Zhong and Wei-Shi Zheng,https://www.sciencedirect.com/science/article/pii/S0031320319301220,https://doi.org/10.1016/j.patcog.2019.03.015,0031-3203,2019,237--250,93,Pattern Recognition,One-pass person re-identification by sketch online discriminant analysis,article,LI2019237,
"Recently, attributes that contain high-level semantic information of image are always used as a complementary knowledge to improve image captioning performance. However, the use of attributes in prior works cannot excavate the latent visual concepts effectively. At each time step, the semantic information which is sensitive to the predicted word could be different. In this paper, we propose a Dense Semantic Embedding Network (DSEN) for this task. The distinct operation of this network is to densely embed the attributes with the multi-modal of image and text at each step of word generation. The discriminative semantic information hidden in these attributes is formatted in form of global likelihood probabilities. As a result, this dense embedding can modulate the feature distributions of the image, text modals and the hidden states to explicit semantic representation. Furthermore, to improve the discrimination of attributes, a Threshold ReLU (TReLU) is proposed. In addition, a bidirectional LSTM structure is incorporated into the DSEN to capture both the previous and future contexts. Extensive experiments on the COCO and Flickr30K datasets achieve superior results when compared with the state-of-the-art models for the tasks of both image captioning and image-text cross modal retrieval. Most remarkably, our method obtains outstanding performance on the retrieval task, compared with the state-of-the-art models.","Image captioning, Retrieval, High-level semantic information, Visual concept, Densely embedding, Long short-term memory",Xinyu Xiao and Lingfeng Wang and Kun Ding and Shiming Xiang and Chunhong Pan,https://www.sciencedirect.com/science/article/pii/S0031320319300494,https://doi.org/10.1016/j.patcog.2019.01.028,0031-3203,2019,285--296,90,Pattern Recognition,Dense semantic embedding network for image captioning,article,XIAO2019285,
"In recent years, collaborative representation-based classification (CRC) methods have shown impressive performance in many recognition tasks. However, when the training data have different distributions with the testing data, the performance of CRC will be degraded significantly. On the other hand, concatenating training data from different sources as a single data set will affect the performance of CRC, as the shift exists between the different source domains. To address these problems, in this paper, we propose a Jointly Discriminative projection and Dictionary learning for domain adaptive Collaborative Representation-based Classification method (JD2-CRC). As the distributions of different source domains may be dissimilar, the data from all domains are projected into a common feature subspace where the latent shared structures can be found. Then a compact dictionary is learned to represent the projected data well. To find the most suitable projection matrices and dictionary for CRC, we design the objective function of JD2-CRC,according to the classification rule of CRC in feature subspace, which minimizes the ratio of within-class reconstruction errors over between-class reconstruction errors. Different to traditional optimization methods, an effective optimization procedure is presented based on gradient descent. Thus, the obtained collaborative representations have a better discriminability and suit the classification rule of CRC well. The experimental results demonstrate that the proposed method can achieve superior performance against other state-of-the-art methods.","Collaborative representation, Dimensionality reduction, Dictionary learning, Domain adptation",Zhichao Zheng and Huaijiang Sun,https://www.sciencedirect.com/science/article/pii/S003132031930010X,https://doi.org/10.1016/j.patcog.2019.01.004,0031-3203,2019,325--336,90,Pattern Recognition,Jointly discriminative projection and dictionary learning for domain adaptive collaborative representation-based classification,article,ZHENG2019325,
"Low variance direction of the training dataset can carry crucial information when building a performant one-class classifier. Covariance-guided One-Class Support Vector Machine (COSVM) emphasizes the low variance direction of the training dataset which results in higher accuracy. However, in the case of large scale datasets, or sequentially obtained data, it shows a serious performance degradation and requires a large memory and an important training time. Thus, in this paper, we investigate the effectiveness of using the low variance directions in an incremental approach. In fact, incremental learning is more effective when dealing with dynamic or important amount of data. More precisely, we control the possible changes of support vectors after the addition of new data points, while emphasizing the low variance directions of the training data, in order to improve classification performance. An extensive comparison of the incremental COSVM to contemporary batch and incremental one-class classifiers on artificial and real-world datasets demonstrates the advantage and the superiority of our proposed model.","One-Class classification, Incremental learning, Support vector machine, Low variance directions",Takoua Kefi-Fatteh and Riadh Ksantini and Mohamed-BÃ©cha KaÃ¢niche and Adel Bouhoula,https://www.sciencedirect.com/science/article/pii/S0031320319300998,https://doi.org/10.1016/j.patcog.2019.02.027,0031-3203,2019,308--321,91,Pattern Recognition,A novel incremental one-class support vector machine based on low variance direction,article,KEFIFATTEH2019308,
"In this paper, we study what are the most important factors that deteriorate the performance of the k-means algorithm, and how much this deterioration can be overcome either by using a better initialization technique, or by repeating (restarting) the algorithm. Our main finding is that when the clusters overlap, k-means can be significantly improved using these two tricks. Simple furthest point heuristic (Maxmin) reduces the number of erroneous clusters from 15% to 6%, on average, with our clustering benchmark. Repeating the algorithm 100 times reduces it further down to 1%. This accuracy is more than enough for most pattern recognition applications. However, when the data has well separated clusters, the performance of k-means depends completely on the goodness of the initialization. Therefore, if high clustering accuracy is needed, a better algorithm should be used instead.","Clustering algorithms, K-means, Initialization, Clustering accuracy, Prototype selection",Pasi FrÃ¤nti and Sami Sieranoja,https://www.sciencedirect.com/science/article/pii/S0031320319301608,https://doi.org/10.1016/j.patcog.2019.04.014,0031-3203,2019,95--112,93,Pattern Recognition,How much can k-means be improved by using better initialization and repeats?,article,FRANTI201995,
"In the presentation of the graph edit distance in 1983 and other newer bibliography, authors state that it is necessary to apply the distance restrictions (non-negativity, identity of indiscernible elements, symmetry and triangle inequality) to each of the edit functions (insertion, deletion and substitution of nodes and edges) involved in the process of computing the graph edit distance to make the graph edit distance a true distance. Moreover, graph edit distance algorithms presented in the last three decades have been based on mapping the edit path that transforms a graph into the other one into a bijection of the graphs in which some null nodes have been added. In this paper, we show that the triangle inequality does not need to be imposed in each edit function if the graph edit distance is defined through an edit path; however, it is necessary if it is defined as a graph bijection. This is an important finding since the triangle inequality is the only restriction that relates different edit functions and concerns the process of tuning the edit functions given a specific application. Hence, on one hand, it would encourage research to define new algorithms based on the edit path instead of the graph bijection and, on the other hand, use edit functions without the restriction, for instance, that the sum of the costs of insertion and deletion of a pair of nodes has to be larger or equal than the cost of substituting them, which could increase the recognition ratio of a concrete application.","Graph edit distance, Distance definition, Sub-optimality",Francesc Serratosa,https://www.sciencedirect.com/science/article/pii/S0031320319300639,https://doi.org/10.1016/j.patcog.2019.01.043,0031-3203,2019,250--256,90,Pattern Recognition,Graph edit distance: Restrictions to be a metric,article,SERRATOSA2019250,
"Increasing the number of parameters seems to have improved convolutional neural networks, e.g. increasing the depth or width of the networks. In this paper, we propose a scheme to improve CNNs by deriving the six sub-filters from a filter, which share parameters among them and enhance the expressibility of the filter. We first defined the sub-filters of a filter, and by visualizing a well-trained CNN, we verified that these sub-filters could recognize multiple meaningful patterns with different visual characteristics, even when the filter containing them was not activated. These findings revealed that the filter has the potential to recognize multiple patterns. Inspired by these findings, we proposed the filter-in-filter (FIF) scheme to enhance the expressibility of a filter, by making full use of its sub-filters to recognize multiple meaningful sub-patterns. We verified the effectiveness of FIF on three image classification benchmark datasets, namely Tiny ImageNet, CIFAR-100 and ImageNet. Our experimental results showed that our models achieved consistent improvement over the base CNNs on the benchmark datasets, e.g. AlexNet and VGG16 using FIF achieved approximately 1% improvement on ImageNet. The sub-filters share the parameters and most of the computational cost with the filter containing them; therefore, FIF does not increase the number of parameters and increases the computational cost only slightly.","Sub-pattern, Sub-filter, Expressibility of filter, Visualization, Filter-in-filter",Guotian Xie and Kuiyuan Yang and Jianhuang Lai,https://www.sciencedirect.com/science/article/pii/S0031320319300640,https://doi.org/10.1016/j.patcog.2019.01.044,0031-3203,2019,391--403,91,Pattern Recognition,Filter-in-Filter: Low Cost CNN Improvement by Sub-filter Parameter Sharing,article,XIE2019391,
"Accurate 3D object recognition and 6-DOF pose estimation have been pervasively applied to a variety of applications, such as unmanned warehouse, cooperative robots, and manufacturing industry. How to extract a robust and representative feature from the point clouds is an inevitable and important issue. In this paper, an unsupervised feature learning network is introduced to extract 3D keypoint features from point clouds directly, rather than transforming point clouds to voxel grids or projected RGB images, which saves computational time while preserving the object geometric information as well. Specifically, the proposed network features in a stacked point feature encoder, which can stack the local discriminative features within its neighborhoods to the original point-wise feature counterparts. The main framework consists of both offline training phase and online testing phase. In the offline training phase, the stacked point feature encoder is trained first and then generate feature database of all keypoints, which are sampled from synthetic point clouds of multiple model views. In the online testing phase, each feature extracted from the unknown testing scene is matched among the database by using the K-D tree voting strategy. Afterwards, the matching results are achieved by using the hypothesis & verification strategy. The proposed method is extensively evaluated on four public datasets and the results show that ours deliver comparable or even superior performances than the state-of-the-arts in terms of F1-score, Average of the 3D distance (ADD) and Recognition rate.","Stacked 3D feature encoder, 3D object recognition, 6-DOF pose estimation, Geometric information preservation",Hongsen Liu and Yang Cong and Chenguang Yang and Yandong Tang,https://www.sciencedirect.com/science/article/pii/S0031320319301335,https://doi.org/10.1016/j.patcog.2019.03.025,0031-3203,2019,135--145,92,Pattern Recognition,Efficient 3D object recognition via geometric information preservation,article,LIU2019135,
"Laplacian spectral kernels and distances (e.g., biharmonic, heat diffusion, wave kernel distances) are easily defined through a filtering of the Laplacian eigenpairs. They play a central role in several applications, such as dimensionality reduction with spectral embeddings, diffusion geometry, image smoothing, geometric characterisations and embeddings of graphs. Extending the results recently derived in the discrete setting [38,39] to the continuous case, we propose a novel definition of the Laplacian spectral kernels and distances, whose approximation requires the solution of a set of inhomogeneous Laplace equations. Their discrete counterparts are equivalent to a set of sparse, symmetric, and well-conditioned linear systems, which are efficiently solved with iterative methods. Finally, we discuss the optimality of the Laplacian spectrum for the approximation of the spectral kernels, the relation between the spectral and Green kernels, and the stability of the spectral distances with respect to the evaluation of the Laplacian spectrum and to multiple Laplacian eigenvalues.","Laplacian spectrum, Spectral distances, Spectral kernels, Heat kernel, Diffusion distances and geometry, Shape and graph analysis",Giuseppe PatanÃ©,https://www.sciencedirect.com/science/article/pii/S003132031930144X,https://doi.org/10.1016/j.patcog.2019.04.004,0031-3203,2019,68--78,93,Pattern Recognition,A unified definition and computation of Laplacian spectral distances,article,PATANE201968,
"Multiple instance regression (MIR) operates on a collection of bags, where each bag contains many instances sharing the same real-valued label. Only few instances, called primary instances, contribute to the bag labels. The remaining ones are noisy observations. The goal in MIR is to identify the primary instances within each bag and learn a regression model that can predict the label of a previously unseen bag. In this paper, we show that regression models can be identified as clusters when appropriate features and distances are used. We introduce an algorithm, called Robust Fuzzy Clustering for Multiple Instance Regression (RFC-MIR), that can learn multiple linear models simultaneously. First, RFC-MIR uses constrained fuzzy memberships to obtain an initial partition where instances can belong to multiple models with various degrees. Then, it uses unconstrained possibilistic memberships to allow the initial local models to expand and converge to the global model. These memberships are also used to identify the primary instances within each bag. After clustering, the possibilistic memberships are used to identify the optimal number of regression models. We evaluate our approach on synthetic data sets generated by varying the dimensionality of the feature space, the number of instances per bag, and the noise level. We also validate the RFC-MIR using two real applications: prediction of the yearly average yield of a crop using remote sensing data; and drug activity prediction. These applications have been used consistently to validate existing MIR algorithms. We show that our approach achieves higher accuracy than existing methods.","Multiple instance regression, Fuzzy clustering, Possibilistic clustering, Multiple model regression",Mohamed Trabelsi and Hichem Frigui,https://www.sciencedirect.com/science/article/pii/S0031320319300524,https://doi.org/10.1016/j.patcog.2019.01.030,0031-3203,2019,424--435,90,Pattern Recognition,Robust fuzzy clustering for multiple instance regression,article,TRABELSI2019424,
"In this work we study permutation synchronisation for the challenging case of partial permutations, which plays an important role for the problem of matching multiple objects (e.g.Â images or shapes). The term synchronisation refers to the property that the set of pairwise matchings is cycle-consistent, i.e.Â in the full matching case all compositions of pairwise matchings over cycles must be equal to the identity. Motivated by clustering and matrix factorisation perspectives of cycle-consistency, we derive an algorithm to tackle the permutation synchronisation problem based on non-negative factorisations. In order to deal with the inherent non-convexity of the permutation synchronisation problem, we use an initialisation procedure based on a novel rotation scheme applied to the solution of the spectral relaxation. Moreover, this rotation scheme facilitates a convenient Euclidean projection to obtain a binary solution after solving our relaxed problem. In contrast to state-of-the-art methods, our approach is guaranteed to produce cycle-consistent results. We experimentally demonstrate the efficacy of our method and show that it achieves better results compared to existing methods.","Partial permutation synchronisation, Multi-matching, Spectral decomposition, Non-negative matrix factorisation",Florian Bernard and Johan Thunberg and Jorge Goncalves and Christian Theobalt,https://www.sciencedirect.com/science/article/pii/S0031320319301293,https://doi.org/10.1016/j.patcog.2019.03.021,0031-3203,2019,146--155,92,Pattern Recognition,Synchronisation of partial multi-matchings via non-negative factorisations,article,BERNARD2019146,
"Multi-label image classification problem is one of the most important and fundamental problems in computer vision. In an image with multiple labels, the objects usually locate at various positions with different scales and poses. Moreover, some labels are associated with the entire image instead of a small region. Therefore, both the global and local information are important for classification. To effectively extract and make full use of these information, in this paper, we present a novel deep Dual-stream nEtwork for the muLTi-lAbel image classification task, DELTA for short. As its name indicates, it is composed of two streams, i.e., the Multi-Instance network and the Global Priors network. The former is used to extract the multi-scale class-related local instances features by modeling the classification problem in a multi-instance learning framework. The latter is devised to capture the global priors from the input image as the global information. These two streams are fused by the final fusion layer. In this way, DELTA can extract and make full use of both the global and local information for classification. Extensive experiments on three benchmark datasets, i.e., PASCAL VOC 2007, PASCAL VOC 2012 and Microsoft COCO, demonstrate that DELTA significantly outperforms several state-of-the-art methods. Moreover, DELTA can automatically locate the key image patterns that trigger the labels.","Deep neural network, Dual-stream network, Multi-label image classification, Multi-instance learning",Wan-Jin Yu and Zhen-Duo Chen and Xin Luo and Wu Liu and Xin-Shun Xu,https://www.sciencedirect.com/science/article/pii/S0031320319301050,https://doi.org/10.1016/j.patcog.2019.03.006,0031-3203,2019,322--331,91,Pattern Recognition,DELTA: A deep dual-stream network for multi-label image classification,article,YU2019322,
"L2-normalization is an effective method to enhance the discriminant power of deep representation learning. However, without exploiting the geometric properties of the feature space, the generally used gradient based optimization methods are failed to track the global information during training. In this paper, we propose a novel deep metric learning model based on the directional distribution. By defining the loss function based on the von MisesâFisher distribution, we propose an effective alternative learning algorithm by periodically updating the class centers. The proposed metric learning not only captures the global information about the embedding space but also yields an approximate representation of the class distribution during training. Considering classification and retrieval tasks, our experiments on benchmark datasets demonstrate an improvement from the proposed algorithm. Particularly, with a small number of convolutional layers, a significant accuracy upsurge can be observed compared to widely used gradient-based methods.","Deep distance metric learning, Directional statistics, Image retrieval, Image similarity learning",Xuefei Zhe and Shifeng Chen and Hong Yan,https://www.sciencedirect.com/science/article/pii/S0031320319301451,https://doi.org/10.1016/j.patcog.2019.04.005,0031-3203,2019,113--123,93,Pattern Recognition,Directional statistics-based deep metric learning for image classification and retrieval,article,ZHE2019113,
"The conventional k-modes algorithm and its variants have been extensively used for categorical data clustering. However, these algorithms have some drawbacks, e.g., they can be trapped into local optima and sensitive to initial clusters/modes. Our numerical experiments even showed that the k-modes algorithm could not identify the optimal clustering results for some special datasets regardless the selection of the initial centers. In this paper, we developed an integer linear programming (ILP) approach for the k-modes clustering, which is independent to the initial solution and can obtain directly the optimal results for small-sized datasets. We also developed a heuristic algorithm that implements iterative partial optimization in the ILP approach based on a framework of variable neighborhood search, known as IPO-ILP-VNS, to search for near-optimal results of medium and large sized datasets with controlled computing time. Experiments on 38 datasets, including 27 synthesized small datasets and 11 known benchmark datasets from the UCI site were carried out to test the proposed ILP approach and the IPO-ILP-VNS algorithm. The experimental results outperformed the conventional and other existing enhanced k-modes algorithms in literature, updated 9 of the UCI benchmark datasets with new and improved results.","Categorical clustering, Variable neighborhood search, Data mining, Integer linear programming",Yiyong Xiao and Changhao Huang and Jiaoying Huang and Ikou Kaku and Yuchun Xu,https://www.sciencedirect.com/science/article/pii/S0031320319300482,https://doi.org/10.1016/j.patcog.2019.01.042,0031-3203,2019,183--195,90,Pattern Recognition,Optimal mathematical programming and variable neighborhood search for k-modes categorical data clustering,article,XIAO2019183,
"Filters are the commonly used techniques for texture feature extraction. In this paper, we present a texture feature extraction method based on multiple wavelet filters. For modeling the multiple wavelet coefficients, we develop Marginal Distribution Covariance Model (MDCM) in which the data points are projected into Cumulative Distribution Function (CDF) space and then the covariance model is constructed in the CDF space. MDCM can capture the dependence of variables, so it can be applied to model the texture features, in which the dependence exists, such as image intensities, color features and wavelet filter features. According to the characteristics of different wavelet filter features, we construct the different MDCMs in the three wavelet transform domains: Orthogonal Wavelet Transform (OWT), Dual Tree Complex Wavelet Transform (DTCWT) and Gabor Wavelet Transform (GWT). Experiments show the proposed method which uses multiple wavelet features can provide a promising performance compared with the state-of-the-art methods.","Texture representation, Covariance matrix, Gabor wavelet, Dual tree complex wavelet, Orthogonal wavelet, Convolutional neural network",Chaorong Li and Yuanyuan Huang and Xingchun Yang and Huafu Chen,https://www.sciencedirect.com/science/article/pii/S0031320319301438,https://doi.org/10.1016/j.patcog.2019.04.003,0031-3203,2019,246--257,92,Pattern Recognition,Marginal distribution covariance model in the multiple wavelet domain for texture representation,article,LI2019246,
"We address the problem of comparing deformable 3D objects represented by graphs such as triangular tessellations. We propose a new graph matching technique to measure the distance between these graphs. The proposed approach is based on a new decomposition of triangular tessellations into triangle-stars. The algorithm ensures a minimum number of disjoint triangle-stars, provides improved dissimilarity by covering larger neighbors and allows the creation of descriptors that are invariant or at least oblivious under the most common deformations. The present approach is based on an approximation of the Graph Edit Distance, which is fault-tolerant to noise and distortion, thus making our technique particularly suitable for the comparison of deformable objects. Classification is performed with supervised machine learning techniques. Our approach defines a metric space using graph embedding and graph kernel techniques. It is proved that the proposed distance is a pseudo-metric. Its time complexity is determined and the method is evaluated against benchmark databases. Our experimental results confirm the performances and the accuracy of our system.","Graph matching, Graph edit distance, Graph decomposition, Graph embedding, Graph metric, Graph classification, Pattern recognition, 3D object recognition, Deformable object recognition, Metric learning",Kamel Madi and Eric Paquet and Hamamache Kheddouci,https://www.sciencedirect.com/science/article/pii/S0031320319300627,https://doi.org/10.1016/j.patcog.2019.01.040,0031-3203,2019,297--307,90,Pattern Recognition,New graph distance for deformable 3D objects recognition based on triangle-stars decomposition,article,MADI2019297,
"Learning based hashing technologies have been widely adopted in multimedia retrieval as they could afford efficient storage and extract semantic information for high-dimensional data. However, the major difficulty of learning based hashing is the discrete constraint imposed on the required hashing codes, which makes the optimization generally to be NP-hard. In this paper, a novel supervised learning based discrete hashing (SLDH) approach is proposed to learn compact binary codes under the deep learning framework for image retrieval. We adopt multilayer network to convert the original features into binary codes, while it should exploit the semantic relevance of manual labels and keep the semantic similarity. For this purpose, we propose the objective function to obtain the binary codes including: 1) making full use of manual labels to get implicit semantic information; 2) using the weighted similarity matrix to keep the semantic similarity; 3) relaxing the discrete constraint to a normalized optimization problem; 4) adding the orthogonality constraint on binary codes to reduce the information redundancy. The objective function is optimized with the alternating direction method and modified alternating direction of multipliers(ADMM) algorithm with efficient iteration. Experiments are conducted on three databases and the results demonstrate the superiority to several state-of-the-art hashing based image retrieval methods.","Hashing, Supervised learning, Neural network, Optimization",Qing Ma and Cong Bai and Jinglin Zhang and Zhi Liu and Shengyong Chen,https://www.sciencedirect.com/science/article/pii/S003132031930130X,https://doi.org/10.1016/j.patcog.2019.03.022,0031-3203,2019,156--164,92,Pattern Recognition,Supervised learning based discrete hashing for image retrieval,article,MA2019156,
"Nowadays, graph-based dimensionality reduction approaches have become more and more popular due to their successful utilization for classification and clustering tasks. In these approaches, how to establish an appropriate graph is critical. To address this issue, a novel graph-based dimensionality reduction framework termed joint graph optimization and projection learning (JGOPL) is proposed in this paper. Compared with existing dimensionality reduction approaches, there are three main advantages of JGOPL. First, through performing the graph optimization and low-dimensional feature learning simultaneously, our proposed approach can accomplish the tasks of graph construction and dimensionality reduction jointly. Second, the l21-norm based distance measurement is adopted in the loss function of our JGOPL so that its robustness to the negative influence caused by the outliers or variations of data can be improved. Third, in order to well exploit and preserve the local structure information of high-dimensional data, a locality constraint is introduced into the proposed JGOPL to discourage a sample from connecting with the distant samples during graph optimization. Extensive classification and clustering experiments are carried out on seven publicly available databases to demonstrate the effectiveness of our approach. At last, the locality constraint and graph optimization strategy proposed in this paper is not only limited to dimensionality reduction, but also can be incorporated into other relevant graph-based tasks (such as spectral clustering).","Graph optimization, Projection learning, Dimensionality reduction, Robustness",Yugen Yi and Jianzhong Wang and Wei Zhou and Yuming Fang and Jun Kong and Yinghua Lu,https://www.sciencedirect.com/science/article/pii/S0031320319301323,https://doi.org/10.1016/j.patcog.2019.03.024,0031-3203,2019,258--273,92,Pattern Recognition,Joint graph optimization and projection learning for dimensionality reduction,article,YI2019258,
"Automatic face recognition in the wild still suffers from low-quality, low resolution, noisy, and occluded input images that can severely impact identification accuracy. In this paper, we present a novel technique to enhance the quality of such extreme low-resolution face images beyond the current state of the art. We model the correlation between high and low resolution faces in a multi-resolution pyramid and show that we can recover the original structure of an un-seen extreme low-resolution face image. By exploiting domain knowledge of the structure of the input signal and using sparse recovery optimization algorithms, we can recover a consistent sparse representation of the extreme low-resolution signal. The proposed super-resolution method is robust to noise and face alignment, and can handle extreme low-resolution faces up to 16x magnification factor with just 7Â pixels between the eyes. Moreover, the formulation of the proposed algorithm allows for simultaneous occlusion removal capability, a desirable property that other super-resolution algorithms do not possess, to the best of our knowledge. Most importantly, we show that our method generalizes on real-world low-quality surveillance images, showing the potentially big impact this can have in a real-world scenario.","Sparse signal recovery (SSR), Single-image super-resolution (SSR), Extreme low resolution",Ramzi Abiantun and Felix Juefei-Xu and Utsav Prabhu and Marios Savvides,https://www.sciencedirect.com/science/article/pii/S0031320319300597,https://doi.org/10.1016/j.patcog.2019.01.032,0031-3203,2019,308--324,90,Pattern Recognition,SSR2: Sparse signal recovery for single-image super-resolution on faces with extreme low resolutions,article,ABIANTUN2019308,
"Network representation learning (NRL) aims to map vertices of a network into a low-dimensional space which preserves the network structure and its inherent properties. Most existing methods for network representation adopt shallow models which have relatively limited capacity to capture highly non-linear network structures, resulting in sub-optimal network representations. Therefore, it is nontrivial to explore how to effectively capture highly non-linear network structure and preserve the global and local structure in NRL. To solve this problem, in this paper we propose a new graph convolutional autoencoder architecture based on a depth-based representation of graph structure, referred to as the depth-based subgraph convolutional autoencoder (DS-CAE), which integrates both the global topological and local connectivity structures within a graph. Our idea is to first decompose a graph into a family of K-layer expansion subgraphs rooted at each vertex aimed at better capturing long-range vertex inter-dependencies. Then a set of convolution filters slide over the entire sets of subgraphs of a vertex to extract the local structural connectivity information. This is analogous to the standard convolution operation on grid data. In contrast to most existing models for unsupervised learning on graph-structured data, our model can capture highly non-linear structure by simultaneously integrating node features and network structure into network representation learning. This significantly improves the predictive performance on a number of benchmark datasets.","Network representation learning, Graph convolutional neural network, Node classification",Zhihong Zhang and Dongdong Chen and Zeli Wang and Heng Li and Lu Bai and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320319300354,https://doi.org/10.1016/j.patcog.2019.01.045,0031-3203,2019,363--376,90,Pattern Recognition,Depth-based subgraph convolutional auto-encoder for network representation learning,article,ZHANG2019363,
"Segmentation of the brain into gray matter, white matter, and cerebrospinal fluid (CSF) using magnetic resonance (MR) imaging plays a fundamental role in neuroimaging research and clinical settings. Due to the complexity of brain anatomy, low image quality, and insufficient training data, both traditional and deep learning segmentation methods have a limited performance. In this paper, we propose a multi-model, multi-size and multi-view deep neural network (M3Net) for brain MR image segmentation, which uses three identical modules to segment transaxial, coronal, and sagittal MR slices, respectively. Each module consists of multi-size U-Nets and multi-size back propagation neural networks. It also uses a probabilistic atlas to explore brain anatomy and a convolutional auto-encoder (CAE) to restore MR images. The proposed M3Net model was evaluated against widely used segmentation methods on both synthetic and clinical studies. Our results suggest that the proposed model is able to segment Brain MR Images more accurately.","Brain image segmentation, Deep learning, U-Net, Convolutional auto-encoder, Back propagation neural network, Magnetic resonance imaging image",Jie Wei and Yong Xia and Yanning Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319301049,https://doi.org/10.1016/j.patcog.2019.03.004,0031-3203,2019,366--378,91,Pattern Recognition,"M3Net: A multi-model, multi-size, and multi-view deep neural network for brain magnetic resonance image segmentation",article,WEI2019366,
"Integrating the structure prior in modeling has achieved considerable attention in pattern recognition and computer vision. Most current state-of-the-art methods (such as low rank representation and structured sparsity) search for a structured metric to fit the structure of the estimated variate, which either bear high time complexity (e.g., compute singular value decomposition for large-scale matrices), or cannot effectively exploit structure information of a matrix variate. In this work, we introduce a nesting-structured nuclear norm to characterize the matrix variate with structure prior and provide a unified framework for solving nesting-structured nuclear norm minimization (NSNM) problem by resorting to an improved sub-gradient method. This not only takes local and global structures of the matrix variate into joint consideration, but also enjoys the lower time complexity than traditional nuclear norm minimization. The revealed statistical meaning explains the rationality of the proposed method. Moreover, we apply NSNM to matrix regression and completion problems, respectively. The extensive experiments for face recognition and large-scale matrix completion clearly demonstrate the superiority of NSNM over some existing methods.","Nesting-structured nuclear norm, Low rank, Structured sparsity, Matrix regression, Matrix completion, Sub-gradient method",Lei Luo and Jian Yang and Yigong Zhang and Yong Xu and Heng Huang,https://www.sciencedirect.com/science/article/pii/S0031320319300755,https://doi.org/10.1016/j.patcog.2019.02.011,0031-3203,2019,147--161,91,Pattern Recognition,Nesting-structured nuclear norm minimization for spatially correlated matrix variate,article,LUO2019147,
"Binary observation has been widely reported in the literature to localize or track moving objects due to its simple realization and good performance in improving energy efficiency. However, with the implementation of logic operators, the new observation models are out of the range of standard compressive sensing context, and thus lack of effective recovery algorithm. The purpose of this paper is to develop effective recovery algorithms and analyze their performance. Two kinds of recovery algorithms are developed and they are inspired from the matching pursuit method and Bayesian method, respectively. Theoretical conditions are also formulated to guarantee the successful recovery and the proposed algorithms are verified by a series of numerical experiments. Moreover, a construction method for the measurement matrix is also proposed, which is essential for model design. It is hoped that the proposed theories and algorithms can make contribution to the related applications of pattern recognition.","Binary sparse signal recovery, Logic observation, Matching pursuit method, Bayesian method",Xiao-Li Hu and Jiajun Wen and Zhihui Lai and Wai Keung Wong and Linlin Shen,https://www.sciencedirect.com/science/article/pii/S0031320319300238,https://doi.org/10.1016/j.patcog.2019.01.018,0031-3203,2019,147--160,90,Pattern Recognition,Binary sparse signal recovery algorithms based on logic observation,article,HU2019147,
"Image retrieval algorithms based on the whole image exhibit high complexity due to background interference, low-level description abilities and large storage requirements, while image retrieval algorithms based on the saliency detection have been found to have low accuracy owing to the lack of important information in extracted salient regions caused by the uncertainty of the salient regions of the image. In this paper, we propose a shadowed-set-based image retrieval algorithm, and develop techniques of an automatic selection of two threshold parameters by combining saliency detection and edge detection, which automatically determine shadowed regions. The developed algorithm uses shadowed set theory to divide the image into salient regions, non-salient regions and shadowed regions, in order to extract the useful information of the image and ignore irrelevant one. As a consequence, this leads to the salient regions and the shadowed regions to be jointly involved in the retrieval process. The experimental results reported for several datasets show that the proposed algorithm can effectively improve the retrieval accuracy compared with the existing state-of-the-art algorithms.","Shadowed sets, Saliency detection, Image retrieval, Image segmentation, Edge detection",Hongyun Zhang and Ting Zhang and Witold Pedrycz and Cairong Zhao and Duoqian Miao,https://www.sciencedirect.com/science/article/pii/S0031320319300512,https://doi.org/10.1016/j.patcog.2019.01.029,0031-3203,2019,390--403,90,Pattern Recognition,Improved adaptive image retrieval with the use of shadowed sets,article,ZHANG2019390,
"The evidence accumulation model is an approach for collecting the information of base partitions in a clustering ensemble method, and can be viewed as a kernel transformation from the original data space to a co-association matrix. However, cluster structure information may be partially lost in this transformation; hence, some methods proposed in the literature try to find the lost information and return it to the ensemble process. In this paper, an interesting phenomenon is introduced: remove some evidences from the co-association matrix, which can result in more accurate clustering results. The intuitive explanation for this is that some evidences in the original co-association matrix could be noise, with negative effects on the final clustering. However, it is difficult to detect those evidences practically, let alone remove them from the matrix. To remedy this problem, we remove multiple level evidences having low occurrence frequencies, because negative evidences do not normally occur regularly in the base partitions. Subsequently, we use normalized cut to achieve multiple clustering results. To discriminate the optimal ensemble result, an internal validity index, which uses only the co-association matrix, is specially designed for the clustering ensemble. The experimental results on 16 datasets demonstrate that the proposed scheme outperforms some state-of-the-art clustering ensemble approaches.","Clustering ensemble, Co-association matrix, Path-based distance",Caiming Zhong and Lianyu Hu and Xiaodong Yue and Ting Luo and Qiang Fu and Haiyong Xu,https://www.sciencedirect.com/science/article/pii/S0031320319301281,https://doi.org/10.1016/j.patcog.2019.03.020,0031-3203,2019,93--106,92,Pattern Recognition,Ensemble clustering based on evidence extracted from the co-association matrix,article,ZHONG201993,
"Action recognition is a challenging task in the field of computer vision. The deficiency in training samples is a bottleneck problem in the current action recognition research. With the explosive growth of Internet data, some researchers try to use prior knowledge learned from various video sources to assist in recognizing the action video of the target domain, which is called knowledge adaptation. Based on this idea, we propose a novel framework for action recognition, called Semantic Adaptation based on the Vector of Locally Max Pooled deep learned Features (SA-VLMPF). The proposed framework consists of three parts: Two-Stream Fusion Network (TSFN), Vector of Locally Max-Pooled deep learned Features (VLMPF) and Semantic Adaptation Model (SAM). TSFN adopts a cascaded convolution fusion strategy to combine the convolutional features extracted from two-stream network. VLMPF retains the long-term information in videos and removes the irrelevant information by capturing multiple local features and extracting the features with the highest response to action category. SAM first maps the data of the auxiliary domain and the target domain into the high-level semantic representation through the deep network. Then the obtained high-level semantic representations from auxiliary domain are adapted into target domain in order to optimize the target classifier. Compared with the existing methods, the proposed methods can utilize the advantages of deep learning methods in obtaining the high-level semantic information to improve the performance of knowledge adaptation. At the same time, SA-VLMPF can make full use of the auxiliary data to make up for the insufficiency of training samples. Multiple experiments are conducted on several couples of datasets to validate the effectiveness of the proposed framework. The results show that the proposed SA-VLMPF outperforms the state-of-the-art knowledge adaptation methods.","Knowledge adaptation, Two-stream network, Video representation, Action recognition, Cascaded convolution fusion strategy",Junxuan Zhang and Haifeng Hu,https://www.sciencedirect.com/science/article/pii/S0031320319300470,https://doi.org/10.1016/j.patcog.2019.01.027,0031-3203,2019,196--209,90,Pattern Recognition,Domain learning joint with semantic adaptation for human action recognition,article,ZHANG2019196,
"This paper presents a novel method that combines coupled hidden Markov models (HMM) and non-Gaussian mixture models based on independent component analyzer mixture models (ICAMM). The proposed method models the joint behavior of a number of synchronized sequential independent component analyzer mixture models (SICAMM), thus we have named it generalized SICAMM (G-SICAMM). The generalization allows for flexible estimation of complex data densities, subspace classification, blind source separation, and accurate modeling of both local and global dynamic interactions. In this work, the structured result obtained by G-SICAMM was used in two ways: classification and interpretation. Classification performance was tested on an extensive number of simulations and a set of real electroencephalograms (EEG) from epileptic patients performing neuropsychological tests. G-SICAMM outperformed the following competitive methods: Gaussian mixture models, HMM, Coupled HMM, ICAMM, SICAMM, and a long short-term memory (LSTM) recurrent neural network. As for interpretation, the structured result returned by G-SICAMM on EEGs was mapped back onto the scalp, providing a set of brain activations. These activations were consistent with the physiological areas activated during the tests, thus proving the ability of the method to deal with different kind of data densities and changing non-stationary and non-linear brain dynamics.","Dynamic modeling, Non-Gaussian mixtures, ICA, HMM, EEG",Gonzalo Safont and Addisson Salazar and Luis Vergara and Enriqueta GÃ³mez and Vicente Villanueva,https://www.sciencedirect.com/science/article/pii/S0031320319301682,https://doi.org/10.1016/j.patcog.2019.04.022,0031-3203,2019,312--323,93,Pattern Recognition,Multichannel dynamic modeling of non-Gaussian mixtures,article,SAFONT2019312,
"Noisy labeled data represent a rich source of information that often are easily accessible and cheap to obtain, but label noise might also have many negative consequences if not accounted for. How to fully utilize noisy labels has been studied extensively within the framework of standard supervised machine learning over a period of several decades. However, very little research has been conducted on solving the challenge posed by noisy labels in non-standard settings. This includes situations where only a fraction of the samples are labeled (semi-supervised) and each high-dimensional sample is associated with multiple labels. In this work, we present a novel semi-supervised and multi-label dimensionality reduction method that effectively utilizes information from both noisy multi-labels and unlabeled data. With the proposed Noisy multi-label semi-supervised dimensionality reduction (NMLSDR) method, the noisy multi-labels are denoised and unlabeled data are labeled simultaneously via a specially designed label propagation algorithm. NMLSDR then learns a projection matrix for reducing the dimensionality by maximizing the dependence between the enlarged and denoised multi-label space and the features in the projected space. Extensive experiments on synthetic data, benchmark datasets, as well as a real-world case study, demonstrate the effectiveness of the proposed algorithm and show that it outperforms state-of-the-art multi-label feature extraction algorithms.","Label noise, Multi-label learning, Semi-supervised learning, Dimensionality reduction",Karl Ãyvind Mikalsen and Cristina Soguero-Ruiz and Filippo Maria Bianchi and Robert Jenssen,https://www.sciencedirect.com/science/article/pii/S0031320319300615,https://doi.org/10.1016/j.patcog.2019.01.033,0031-3203,2019,257--270,90,Pattern Recognition,Noisy multi-label semi-supervised dimensionality reduction,article,MIKALSEN2019257,
"Real-time object tracking plays an important role in many computer vision systems, yet in complex scenarios, it is still a very challenging problem. In this paper, we propose a new visual tracking algorithm via a manifold regularized discriminative dual dictionary (MD3) model. First, a dual dictionary is introduced to avoid the calculation of representation coefficient in distance function construction. Second, the local background templates are utilized to keep the learned dictionaries discriminative. Third, the manifold regularization on representation coefficient is proposed to ensure that MD3 model has a bit error tolerance on the object update. We formulate object tracking in a particle filter framework, in which the observation model is calculated as the reconstruction error between learned dictionaries and the candidate template. Extensive experiments in various tracking scenarios are performed to evaluate the proposed method, and the results interpret that the tracking accuracy as well as the computational cost can be improved as compared with the state-of-the-art approaches.","Visual tracking, Dictionary learning, Dual dictionary, Manifold regularization",Lingfeng Wang and Chunhong Pan,https://www.sciencedirect.com/science/article/pii/S003132031930072X,https://doi.org/10.1016/j.patcog.2019.02.008,0031-3203,2019,272--280,91,Pattern Recognition,Visual object tracking via a manifold regularized discriminative dual dictionary model,article,WANG2019272,
"Two-stream convolutional neural networks show great promise for action recognition tasks. However, most two-stream based approaches train the appearance and motion subnetworks independently, which may lead to the decline in performance due to the lack of interactions among two streams. To overcome this limitation, we propose a Spatiotemporal Distilled Dense-Connectivity Network (STDDCN) for video action recognition. This network implements both knowledge distillation and dense-connectivity (adapted from DenseNet). Using this STDDCN architecture, we aim to explore interaction strategies between appearance and motion streams along different hierarchies. Specifically, block-level dense connections between appearance and motion pathways enable spatiotemporal interaction at the feature representation layers. Moreover, knowledge distillation among two streams (each treated as a student) and their last fusion (treated as teacher) allows both streams to interact at the high level layers. The special architecture of STDDCN allows it to gradually obtain effective hierarchical spatiotemporal features. Moreover, it can be trained end-to-end. Finally, numerous ablation studies validate the effectiveness and generalization of our model on two benchmark datasets, including UCF101 and HMDB51. Simultaneously, our model achieves promising performances.","Two-stream, Action recognition, Dense-connectivity, Knowledge distillation",Wangli Hao and Zhaoxiang Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319301037,https://doi.org/10.1016/j.patcog.2019.03.005,0031-3203,2019,13--24,92,Pattern Recognition,Spatiotemporal distilled dense-connectivity network for video action recognition,article,HAO201913,
"We propose a local tree-structured low-rank representation (TS-LRR) model to detect salient objects under the complicated background with diverse local regions, which is problematic for most low-rank matrix recovery (LRMR) based salient object detection methods. We first impose a local tree-structured low-rank constraint on the representation coefficients matrix to capture the complicated background. Specifically, a primitive background dictionary is constructed for TS-LRR to promote its background representation ability, and thus enlarge the gap between the salient objects and the background. We then impose a group-sparsity constraint on the sparse error matrix with the intention to ensure the saliency consistency among patches with similar features. At last, a foreground consistency is introduced to identically highlight the distinctive regions within the salient object. Experimental results on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model over the state-of-the-art methods.","Salient object detection, Structured low-rank representation, Background dictionary, Foreground consistency",Qiang Zhang and Zhen Huo and Yi Liu and Yunhui Pan and Caifeng Shan and Jungong Han,https://www.sciencedirect.com/science/article/pii/S0031320319301311,https://doi.org/10.1016/j.patcog.2019.03.023,0031-3203,2019,119--134,92,Pattern Recognition,Salient object detection employing a local tree-structured low-rank representation and foreground consistency,article,ZHANG2019119,
"Although biometrics is considered more competent than password-based or token-based approach in identity management, biometric templates are vulnerable to adversary attacks that may lead to irreversible identity loss. One of the promising remedies for biometric template protection is cancelable biometrics. In this paper, a novel binary cancelable fingerprint template design based on Partial Local Structure (PLS) descriptor and Permutated Randomized Non-Negative Least Square (PR-NNLS) is proposed. The PLS descriptor is an alignment-free minutia descriptor, which is conceived to be coupled with the PR-NNLS to derive a binary protected fingerprint template that satisfies non-invertibility, unlinkability, cancelability and performance criteria. The PR-NNLS formulation is unique in such a way that the noninvertible transformation is applied to the PLS descriptor dictionary instead of applying it to the minutiae descriptor, which often invites performance deterioration. The evaluations have been carried out with five subsets from FVC 2002 and 2004 databases where the proposed method is attested to fulfill the aforementioned four template protection criteria. We also analyze four privacy and security attacks targeted to cancelable biometrics.","Cancelable biometrics, Partial local structure descriptor, Randomized non-negative least square, Binary representation, Fingerprint template protection",Jun Beom Kho and Jaihie Kim and Ig-Jae Kim and Andrew B.J. Teoh,https://www.sciencedirect.com/science/article/pii/S0031320319300585,https://doi.org/10.1016/j.patcog.2019.01.039,0031-3203,2019,245--260,91,Pattern Recognition,Cancelable fingerprint template design with randomized non-negative least squares,article,KHO2019245,
"RGB-D indoor scene classification is an essential and challenging task. Although convolutional neural network (CNN) achieves excellent results on RGB-D object recognition, it has several limitations when extended towards RGB-D indoor scene classification. 1) The semantic cues such as objects of the indoor scene have high spatial variabilities. The spatially rigid global representation from CNN is suboptimal. 2) The cluttered indoor scene has lots of redundant and noisy semantic cues; thus discerning discriminative information among them should not be ignored. 3) Directly concatenating or summing global RGB and Depth information as presented in popular methods cannot fully exploit the complementarity between two modalities for complicated indoor scenarios. To address the above problems, we propose a novel unified framework named Multi-modal Attentive Pooling Network (MAPNet) in this paper. Two orderless attentive pooling blocks are constructed in MAPNet to aggregate semantic cues within and between modalities meanwhile maintain the spatial invariance. The Intra-modality Attentive Pooling (IAP) block aims to mine and pool discriminative semantic cues in each modality. The Cross-modality Attentive Pooling (CAP) block is extended to learn different contributions across two modalities, which further guides the pooling of the selected discriminative semantic cues of each modality. We further show that the proposed model is interpretable, which helps to understand mechanisms of both scene classification and multi-modal fusion in MAPNet. Extensive experiments and analysis on SUN RGB-D Dataset and NYU Depth Dataset V2 show the superiority of MAPNet over current state-of-the-art methods.","Indoor scene classification, Multi-modal fusion, RGB-D, Attentive pooling",Yabei Li and Zhang Zhang and Yanhua Cheng and Liang Wang and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S003132031930069X,https://doi.org/10.1016/j.patcog.2019.02.005,0031-3203,2019,436--449,90,Pattern Recognition,MAPNet: Multi-modal attentive pooling network for RGB-D indoor scene classification,article,LI2019436,
"DBSCAN is one of clustering algorithms which can report arbitrarily-shaped clusters and noises without requiring the number of clusters as a parameter (unlike the other clustering algorithms, k-means, for example). Because the running time of DBSCAN has quadratic order of growth, i.e. O(n2), research studies on improving its performance have been received a considerable amount of attention for decades. Grid-based DBSCAN is a well-developed algorithm whose complexity is improved to O(nlogân) in 2D space, while requiring Î©(n4/3) to solve when dimension â¯â¥â¯3. However, we find that Grid-based DBSCAN suffers from two problems: neighbour explosion and redundancies in merging, which make the algorithms infeasible in high dimensional space. In this paper we first propose a novel algorithm called GDCF which utilizes bitmap indexing to support efficient neighbour grid queries. Second, based on the concept of union-find algorithm we devise a forest-like structure, called cluster forest, to alleviate the redundancies in the merging. Moreover, we find that running the cluster forest in different orders can lead to a different number of merging operations needed to perform in the merging step. We propose to perform the merging step in a uniform random order to optimize the number of merging operations. However, for high-density database, a bottleneck could be occurred, we further propose a low-density-first order to alleviate this bottleneck. The experiments resulted on both real-world and synthetic datasets demonstrate that the proposed algorithm outperforms the state-of-the-art exact/approximate DBSCAN and suggests a good scalability.","Density-based clustering, Grid-based DBSCAN, Union-find algorithm",Thapana Boonchoo and Xiang Ao and Yang Liu and Weizhong Zhao and Fuzhen Zhuang and Qing He,https://www.sciencedirect.com/science/article/pii/S0031320319300500,https://doi.org/10.1016/j.patcog.2019.01.034,0031-3203,2019,271--284,90,Pattern Recognition,Grid-based DBSCAN: Indexing and inference,article,BOONCHOO2019271,
"The presence of limited spatio-temporal resolution in dynamic scenes renders segmentation of foreground objects problematic, as it brings negative effects on candidate object missing or motion boundary overfilling caused by large displacements of corresponding points in consecutive frames. To alleviate these problems, our general framework introduces a novel agnostic attribute video object segmentation method that is suitable for segmenting foreground objects in dynamic scenes at low spatio-temporal resolution. We employ a fully connected network (FCN) to facilitate estimation of class-agnostic object proposals based on the semantic classification attributes. Instead of directly deriving a hard classification into objects, we propose a scheme by fusing different top ranked soft scores in the semantic space that allows the model to directly estimate probabilistic foreground hypotheses. A unified conditional random field model is proposed to incorporate the proposal information derived from the soft prediction scores and consequently build up an unary energy functional with additional location and appearance potentials. The pairwise energy functional imposes both spatial and temporal consistency constraints simultaneously on appearance, location and unary potentials. Our experiments on spatio-temporal subsampled video segmentation benchmarks demonstrate the effectiveness of the proposed method for robust segmentation of class-agnostic objects in dynamic scenes despite of abrupt motion and large displacements caused by limited spatio-temporal resolution.","Video object segmentation, Conditional random field, Class-agnostic, Semantic space, Spatio-temporal resolution",Yinhui Zhang and Zifen He,https://www.sciencedirect.com/science/article/pii/S0031320319300986,https://doi.org/10.1016/j.patcog.2019.02.026,0031-3203,2019,261--271,91,Pattern Recognition,Agnostic attribute segmentation of dynamic scenes with limited spatio-temporal resolution,article,ZHANG2019261,
"In this work, the issue of depth filling is addressed using a self-supervised feature learning model that predicts missing depth pixel values based on the context and structure of the scene. A fully-convolutional generative model is conditioned on the available depth information and full RGB colour information from the scene and trained in an adversarial fashion to complete scene depth. Since ground truth depth is not readily available, synthetic data is instead used with a separate model developed to predict where holes would appear in a sensed (non-synthetic) depth image based on the contents of the RGB image. The resulting synthetic data with realistic holes is utilized in training the depth filling model which makes joint use of a reconstruction loss which employs the Discrete Cosine Transform for more realistic outputs, an adversarial loss which measures the distribution distances via the Wasserstein metric and a bottleneck feature loss that aids in better contextual feature execration. Additionally, the model is adversarially adapted to perform well on naturally-obtained data with no available ground truth. Qualitative and quantitative evaluations demonstrate the efficacy of the approach compared to contemporary depth filling techniques. The strength of the feature learning capabilities of the resulting deep network model is also demonstrated by performing the task of monocular depth estimation using our pre-trained depth hole filling model as the initialization for subsequent transfer learning.","Depth image, Hole filling, Self-supervised learning, Generative model, Adversarial training, Feature distance, Domain adaptation",Amir Atapour-Abarghouei and Samet Akcay and GrÃ©goire {Payen de La Garanderie} and Toby P. Breckon,https://www.sciencedirect.com/science/article/pii/S0031320319300743,https://doi.org/10.1016/j.patcog.2019.02.010,0031-3203,2019,232--244,91,Pattern Recognition,"Generative adversarial framework for depth filling via Wasserstein metric, cosine transform and domain transfer",article,ATAPOURABARGHOUEI2019232,
"Despite the recent deep learning (DL) revolution, kernel machines still remain powerful methods for action recognition. DL has brought the use of large datasets and this is typically a problem for kernel approaches, which are not scaling up efficiently due to kernel Gram matrices. Nevertheless, kernel methods are still attractive and more generally applicable since they can equally manage different sizes of the datasets, also in cases where DL techniques show some limitations. This work investigates these issues by proposing an explicit approximated representation that, together with a linear model, is an equivalent, yet scalable, implementation of a kernel machine. Our approximation is directly inspired by the exact feature map that is induced by an RBF Gaussian kernel but, unlike the latter, it is finite dimensional and very compact. We justify the soundness of our idea with a theoretical analysis which proves the unbiasedness of the approximation, and provides a vanishing bound for its variance, which is shown to decrease much rapidly than in alternative methods in the literature. In a broad experimental validation, we assess the superiority of our approximation in terms of (1) ease and speed of training, (2) compactness of the model, and (3) improvements with respect to the state-of-the-art performance.","Kernel machines, Kernel approximation, Action recognition, Skeletal joints, Covariance representation",Jacopo Cavazza and Pietro Morerio and Vittorio Murino,https://www.sciencedirect.com/science/article/pii/S0031320319301396,https://doi.org/10.1016/j.patcog.2019.03.031,0031-3203,2019,25--35,93,Pattern Recognition,Scalable and compact 3D action recognition with approximated RBF kernel machines,article,CAVAZZA201925,
"Cross-modality face recognition aims to identify faces across different modalities, such as matching sketches with photos, low resolution face images with high resolution images, and near infrared images with visual lighting images, which is challenging because of the modality gap caused by texture, resolution, and illumination variations. Existing approaches either utilized hand-crafted approaches which ignore inherent data distribution characteristic, or applied deep learning-based algorithms on holistic face images with facial local information ignored. In this paper, we propose a deep local descriptor learning framework for cross-modality face recognition, which aims to learn discriminant and compact local information directly from raw facial patches. A novel cross-modality enumeration loss is proposed to eliminate the modality gap on local patch level, which is then integrated into a convolutional neural networks for deep local descriptor extraction. The proposed deep local descriptor can be easily applied to any traditional face recognition systems, and we use Fisherface as an example in the paper. Extensive experiments on six widely used cross-modality face recognition datasets demonstrate the superiority of proposed method over state-of-the-art methods.","Cross modality, Local descriptor, Composite sketch, ID photo, NIR-VIS matching",Chunlei Peng and Nannan Wang and Jie Li and Xinbo Gao,https://www.sciencedirect.com/science/article/pii/S0031320319300603,https://doi.org/10.1016/j.patcog.2019.01.041,0031-3203,2019,161--171,90,Pattern Recognition,DLFace: Deep local descriptor for cross-modality face recognition,article,PENG2019161,
"Conformal prediction uses the degree of strangeness (nonconformity) of data instances to determine the confidence values of new predictions. We propose an inductive conformal predictor for convolutional neural networks (CNNs), referring to it as ICP-CNN, which uses a novel nonconformity measure that produces reliable confidence values. Furthermore, ICP-CNN is used to improve classification performance through active learning, selecting instances from an unlabeled pool based on the evaluation of three criteria: informativeness, diversity, and information density. Distance metric learning is employed to measure diversity, using a similarity measure that adapts to the database being used. Moreover, information density is considered to filter outliers. Experiments conducted on face and object recognition databases demonstrate that ICP-CNN improves the classification performance of CNNs, outperforming previously proposed active learning techniques, while producing reliable confidence values.","Conformal prediction, Convolutional neural networks, Active learning, Distance metric learning, Image classification",Sergio Matiz and Kenneth E. Barner,https://www.sciencedirect.com/science/article/pii/S003132031930055X,https://doi.org/10.1016/j.patcog.2019.01.035,0031-3203,2019,172--182,90,Pattern Recognition,Inductive conformal predictor for convolutional neural networks: Applications to active learning for image classification,article,MATIZ2019172,
"Fitting conics from images is a preliminary step for its plentiful applications. It is a common sense that geometric distance based fitting methods are better than algebraic distance based ones. However, for a long time, there has not been a geometric distance between a point and a general conic that allows easy computation and achieves high accuracy simultaneously. Though Sampson distance is widely accepted, it is only a first-order approximation. For other geometric distances, the computations are too complex to be popular in practice. In this paper, we derive a new geometric distance between a point and a general conic, called Polar-N-Direction distance. The distance can be adapted to a projective transformation because it is computed along the normal direction of the polar line of the point, making conic fitting more robust. Moreover, Polar-N-Direction distance is accurate and simultaneously still analytical in an explicit representation, which is quite easy to be implemented. Then, based on the distance, a new cost function is constructed. The conic fitting optimization by minimizing this cost function has all the merits of the geometric distance based methods and simultaneously avoids their limitations. Experiments show that the conic fitting method is greatly efficient.","Conic fitting, Geometric distance, Sampson distance",Yihong Wu and Haoren Wang and Fulin Tang and Zhiheng Wang,https://www.sciencedirect.com/science/article/pii/S0031320319300329,https://doi.org/10.1016/j.patcog.2019.01.023,0031-3203,2019,415--423,90,Pattern Recognition,Efficient conic fitting with an analytical Polar-N-Direction geometric distance,article,WU2019415,
"Using dropout in Visual Question Answering (VQA) is a common practice to prevent overfitting. However, the current way to use dropout in multi-path networks may cause two problems: the co-adaptations of neurons and the explosion of output variance. In this paper, we propose coherent dropout and siamese dropout mechanism to solve the two problems, respectively. Specifically, in coherent dropout, the relevant dropout layers in multiple paths are forced to work coherently to maximize the ability of preventing neuron co-adaptations. We show that the coherent dropout is simple in implementation but very effective to overcome overfitting. As for the explosion of output variance, we develop a siamese dropout mechanism to explicitly minimize the difference between the two output vectors produced from the same input data during training phase. Such mechanism can reduce the gap between training and inference phases and make the VQA model more robust. With the help of the two techniques, we further design an enhanced question encoder called Multi-path Stacked Residual RNNs which is deeper and wider and more powerful than current shallow question encoder. Extensive experiments are conducted to verify the effectiveness of coherent dropout, siamese dropout and the enhanced question encoder. And the results show that our methods can bring clear improvements to the state-of-the-art VQA models on VQA-v1 and VQA-v2 datasets.","Visual question answering, Coherent dropout, Siamese dropout, Enhanced question encoder",Zhiwei Fang and Jing Liu and Yong Li and Yanyuan Qiao and Hanqing Lu,https://www.sciencedirect.com/science/article/pii/S0031320319300573,https://doi.org/10.1016/j.patcog.2019.01.038,0031-3203,2019,404--414,90,Pattern Recognition,Improving visual question answering using dropout and enhanced question encoder,article,FANG2019404,
"This paper presents a novel iterative deep learning framework and applies it to document enhancement and binarization. Unlike the traditional methods that predict the binary label of each pixel on the input image, we train the neural network to learn the degradations in document images and produce uniform images of the degraded input images, which in turn allows the network to refine the output iteratively. Two different iterative methods have been studied in this paper: recurrent refinement (RR) that uses the same trained neural network in each iteration for document enhancement and stacked refinement (SR) that uses a stack of different neural networks for iterative output refinement. Given the learned nature of the uniform and enhanced image, the binarization map can be easily obtained through use of a global or local threshold. The experimental results on several public benchmark data sets show that our proposed method provides a new, clean version of the degraded image, one that is suitable for visualization and which shows promising results for binarization using Otsuâs global threshold, based on enhanced images learned iteratively by the neural network.","Document enhancement and binarization, Convolutional neural networks, Iterative deep learning, Recurrent refinement",Sheng He and Lambert Schomaker,https://www.sciencedirect.com/science/article/pii/S0031320319300330,https://doi.org/10.1016/j.patcog.2019.01.025,0031-3203,2019,379--390,91,Pattern Recognition,DeepOtsu: Document enhancement and binarization using iterative deep learning,article,HE2019379,
"Facial Expression Recognition (FER) has long been a challenging task in the field of computer vision. In this paper, we present a novel model, named Deep Attentive Multi-path Convolutional Neural Network (DAM-CNN), for FER. Different from most existing models, DAM-CNN can automatically locate expression-related regions in an expressional image and yield a robust image representation for FER. The proposed model contains two novel modules: an attention-based Salient Expressional Region Descriptor (SERD) and the Multi-Path Variation-Suppressing Network (MPVS-Net). SERD can adaptively estimate the importance of different image regions for FER task, while MPVS-Net disentangles expressional information from irrelevant variations. By jointly combining SERD and MPVS-Net, DAM-CNN is able to highlight expression-relevant features and generate a variation-robust representation for expression classification. Extensive experimental results on both constrained datasets (CK+, JAFFE, TFEID) and unconstrained datasets (SFEW, FER2013, BAUM-2i) demonstrate the effectiveness of our DAM-CNN model.","Attention, Convolutional neural network, Facial expression recognition, Multi-Path variation-suppressing network, Salient expressional region descriptor",Siyue Xie and Haifeng Hu and Yongbo Wu,https://www.sciencedirect.com/science/article/pii/S0031320319301268,https://doi.org/10.1016/j.patcog.2019.03.019,0031-3203,2019,177--191,92,Pattern Recognition,Deep multi-path convolutional neural network joint with salient region attention for facial expression recognition,article,XIE2019177,
"Although template matching has been widely studied in the fields of image processing and computer vision, current template matching methods still cannot address large-scale changes and rotation changes simultaneously. In this study, we propose a novel adaptive radial ring code histograms (ARRCH) image descriptor for large-scale and rotation-invariant template matching. The image descriptor is constructed by (1) identifying, inside the template, a set of concentric ring regions around a reference point, (2) detecting âstableâ pixels based on the ASGO, which is tolerant with respect to large scale change, (3) extracting a rotation-invariant feature for each âstableâ pixel, and (4) discretizing the features in a separate histogram for each concentric ring region in the scale space. Finally, an ARRCH image descriptor is obtained by chaining the histograms of all concentric ring regions for each scale. In matching mode, a sliding window approach is used to extract descriptors, which are compared with the template one, and a coarse-to-fine search strategy is employed to detect the scale of the target image. To demonstrate the performance of the ARRCH, several experiments are carried out, including a parameter experiment and a large-scale and rotation change matching experiment, and some applications are presented. The experimental results demonstrate that the proposed method is more resistant to large-scale and rotation differences than previous state-of-the-art matching methods.","Template matching, Adaptive radial ring code histograms, Large-scale and rotation-invariant features",Hua Yang and Chenghui Huang and Feiyue Wang and Kaiyou Song and Shijiao Zheng and Zhouping Yin,https://www.sciencedirect.com/science/article/pii/S0031320319301025,https://doi.org/10.1016/j.patcog.2019.03.003,0031-3203,2019,345--356,91,Pattern Recognition,Large-scale and rotation-invariant template matching using adaptive radial ring code histograms,article,YANG2019345,
"Low-rank or sparse tensor recovery finds many applications in computer vision and machine learning. The recently proposed regularized multilinear regression and selection (Remurs) model assumes the true tensor to be simultaneously low-Tucker-rank and sparse, and has been successfully applied in fMRI analysis. However, the statistical performance of Remurs-like models is still lacking. To address this problem, a minimization problem based on a newly defined tensor nuclear-l1-norm is proposed, to recover a simultaneously low-Tucker-rank and sparse tensor from its degraded observations. Then, an M-ADMM-based algorithm is developed to efficiently solve the problem. Further, the statistical performance is analyzed by establishing a deterministic upper bound on the estimation error for general noise. Also, under Gaussian noise, non-asymptotic upper bounds for two specific settings, i.e., noisy tensor decomposition and random Gaussian design, are given. Experiments on synthetic datasets demonstrate that the proposed theorems can precisely predict the scaling behavior of the estimation error.","Tensor recovery, Statistical performance, Tucker rank, Tensor de-noising, Tensor compressive sensing",Xiangrui Li and Andong Wang and Jianfeng Lu and Zhenmin Tang,https://www.sciencedirect.com/science/article/pii/S003132031930113X,https://doi.org/10.1016/j.patcog.2019.03.014,0031-3203,2019,193--203,93,Pattern Recognition,Statistical performance of convex low-rank and sparse tensor recovery,article,LI2019193,
"Images with intensity inhomogeneity pose significant challenges in image segmentation. Local region-based level set models have recently been recognized as promising methods to segment such images. In these models, local intensity information in a neighborhood of predetermined size is extracted and then embedded into the energy functional, guiding the evolution of deformable contour toward desired boundaries. The local neighborhood intensities are assumed to be rather constant; therefore, the selection of neighborhood size greatly influences effectiveness and robustness. Complex image characteristics, such as variation in degree of intensity inhomogeneity and noise levels among regions, can lead to severe challenges for accurate image segmentation when using only a fixed scale parameter for local regions. We propose a new multi-scale local feature-based level set method for image segmentation with an improved strategy based on previous studies of multi-scale image filtering methods, which allow for automatic selection of filtering scale parameters. Our novel method can adaptively determine the optimal scale parameter for each pixel during contour evolution, alleviating the challenges caused by severe intensity inhomogeneity. First, we define a Local Maximum Description Difference feature (LMDD), based on multi-scale local region descriptors. We incorporate the LMDD, associated with the maximum response of multi-scale high-pass filters for each pixel, into three local region based level set models with Chan-Vese (CV)-like structure to construct the energy functional. Finally, we complete the segmentation through minimization of this energy. Our experimental results illustrate the good performance of the proposed level set method for segmenting images with severe intensity inhomogeneity.","Intensity inhomogeneity, Level set, Local maximum description difference, Local region descriptor, Multi-scale",Hai Min and Li Xia and Junwei Han and Xiaofeng Wang and Qianqian Pan and Hao Fu and Hongzhi Wang and Stephen T.C. Wong and Hai Li,https://www.sciencedirect.com/science/article/pii/S0031320319300731,https://doi.org/10.1016/j.patcog.2019.02.009,0031-3203,2019,69--85,91,Pattern Recognition,A multi-scale level set method based on local features for segmentation of images with intensity inhomogeneity,article,MIN201969,
"We present a family of methods for 2Dâ3D registration spanning both deterministic and non-deterministic branch-and-bound approaches. Critically, the methods exhibit invariance to the underlying scene primitives, enabling e.g. points and lines to be treated on an equivalent basis, potentially enabling a broader range of problems to be tackled while maximising available scene information, all scene primitives being simultaneously considered. Being a branch-and-bound based approach, the method furthermore enjoys intrinsic guarantees of global optimality; while branch-and-bound approaches have been employed in a number of computer vision contexts, the proposed method represents the first time that this strategy has been applied to the 2Dâ3D correspondence-free registration problem from points and lines. Within the proposed procedure, deterministic and probabilistic procedures serve to speed up the nested branch-and-bound search while maintaining optimality. Experimental evaluation with synthetic and real data indicates that the proposed approach significantly increases both accuracy and robustness compared to the state of the art.","2Dâ3D registration, Multi-modal registration, Branch-and-bound, Global optimisation",Mark Brown and David Windridge and Jean-Yves Guillemaut,https://www.sciencedirect.com/science/article/pii/S0031320319301426,https://doi.org/10.1016/j.patcog.2019.04.002,0031-3203,2019,36--54,93,Pattern Recognition,A family of globally optimal branch-and-bound algorithms for 2Dâ3D correspondence-free registration,article,BROWN201936,
"Real world data are often represented by multiple distinct feature sets, and some prior knowledge is provided, such as labels of some examples or pairwise constraints between several sample pairs. Accordingly, task of multi-view clustering arises from a complex information aggregation of multiple sources of feature sets and knowledge prior. In this paper, we propose to optimize the cluster indicator, which representing the class labels is an intuitive reflection of the clustering structure. Besides, the prior indicating the same level of semantics can be directly utilized guiding the learned clustering structure. Furthermore, feature selection is embedded into the above process to select views and features in each view, which leads to the most discriminative views and features chosen for every single cluster. To these ends, an objective is accordingly proposed with an efficient optimization strategy and convergence analysis. Extensive experiments demonstrate that our model performs better than the state-of-the-art methods.","Multi-view clustering, Feature selection, Prior information, Cluster indicator",Qiyue Yin and Junge Zhang and Shu Wu and Hexi Li,https://www.sciencedirect.com/science/article/pii/S0031320319301700,https://doi.org/10.1016/j.patcog.2019.04.024,0031-3203,2019,380--391,93,Pattern Recognition,Multi-view clustering via joint feature selection and partially constrained cluster label learning,article,YIN2019380,
"The majority of real-world problems require addressing incomplete data. The use of the structural expectation-maximization algorithm is the most common approach toward learning Bayesian networks from incomplete datasets. However, its main limitation is its demanding computational cost, caused mainly by the need to make an inference at each iteration of the algorithm. In this paper, we propose a new method with the purpose of guaranteeing the efficiency of the learning process while improving the performance of the structural expectation-maximization algorithm. We address the first objective by applying an upper bound to the treewidth of the models to limit the complexity of the inference. To achieve this, we use an efficient heuristic to search the space of the elimination orders. For the second objective, we study the advantages of directly computing the score with respect to the observed data rather than an expectation of the score, and provide a strategy to efficiently perform these computations in the proposed method. We perform exhaustive experiments on synthetic and real-world datasets of varied dimensionalities, including datasets with thousands of variables and hundreds of thousands of instances. The experimental results support our claims empirically.","Structural expectation-maximization, Bayesian network, Incomplete data, Inference complexity, Structure learning",Marco Benjumeda and Sergio Luengo-Sanchez and Pedro LarraÃ±aga and Concha Bielza,https://www.sciencedirect.com/science/article/pii/S0031320319300974,https://doi.org/10.1016/j.patcog.2019.02.025,0031-3203,2019,190--199,91,Pattern Recognition,Tractable learning of Bayesian networks from partially observed data,article,BENJUMEDA2019190,
"Devanagari and Bengali scripts are two of the most popular scripts in India. Most of the existing word recognition studies in these two scripts have relied upon the widely used Hidden Markov Model (HMM), in spite of its familiar shortcomings. The existing works were evaluated against and performed well in their chosen metrics. But, the existing word recognition systems in these two scripts could not achieve more than 90% recognition accuracy. This article proposes a novel approach for online handwritten cursive and non-cursive word recognition in Devanagari and Bengali scripts based on two recently developed models of Recurrent Neural Network (RNN)âLongâShort Term Memory (LSTM) and Bidirectional LongâShort Term Memory (BLSTM). The proposed approach divides each word horizontally into three zonesâupper, middle, and lower, to reduce the variations in basic stroke order within a word. Next, the word portions from middle zone are re-segmented into its basic strokes. Various structural and directional features are then extracted from each basic stroke of the word separately for each zone. These zone wise basic stroke features are then studied using both LSTM and BLSTM versions of RNN. Most of the existing word recognition systems in these two scripts have followed word based class labelling approach, whereas proposed system has followed the basic stroke based class labelling approach. An exhaustive experiment on large datasets has been performed to evaluate the performance of the proposed approach using both RNN and HMM to make a comparative performance analysis. Experimental results show that the proposed RNN based system is superior over HMM achieving 99.50% and 95.24% accuracies in Devanagari and Bengali scripts respectively and outperforms existing HMM based systems in the literature as well.","Online handwriting, Word recognition, Indian scripts, Horizontal zone division, RNN, LSTM, BLSTM",Rajib Ghosh and Chirumavila Vamshi and Prabhat Kumar,https://www.sciencedirect.com/science/article/pii/S0031320319301384,https://doi.org/10.1016/j.patcog.2019.03.030,0031-3203,2019,203--218,92,Pattern Recognition,RNN based online handwritten word recognition in Devanagari and Bengali scripts using horizontal zoning,article,GHOSH2019203,
"Ridge regression (RR) and its variants are fundamental methods for multivariable data analysis, which have been widely used to deal with different problems in pattern recognition or classification. However, these methods have their common drawback. That is, the number of the learned projections is limited by the number of class. Moreover, most of these methods do not consider the local structure of the data, which makes them less competitive in the case when data are lying on a lower dimensional manifold. Therefore, in this paper, we propose a robust jointly sparse regression method to integrate the locality geometric structure with generalized orthogonality constraint and joint sparsity into a regression modal to address these problems. The optimization model can be solved by an alternatively iterative algorithm using orthogonal matching pursuit (OMP) and singular value decomposition. Experimental results on face and non-face image database demonstrate the superiority of the proposed method. The matlab code can be found at http://www.scholat.com/laizhihui.","Dimensionality reduction, Local structure, Joint sparsity, Orthogonality, Orthogonal matching pursuit",Dongmei Mo and Zhihui Lai,https://www.sciencedirect.com/science/article/pii/S0031320319301529,https://doi.org/10.1016/j.patcog.2019.04.011,0031-3203,2019,164--178,93,Pattern Recognition,Robust Jointly Sparse Regression with Generalized Orthogonal Learning for Image Feature Selection,article,MO2019164,
"Fine-Grained Image Classification (FGIC) aims to distinguish the images within a subordinate category. Recently, many FGIC methods have been proposed and huge progress has been made in the aspects of part detection and feature learning for FGIC. However, FGIC still remains a challenging task due to the large intra-class variance and small inter-class variance. To classify fine-grained images accurately, this paper proposes to exploit spatial relation to capture more discriminative details for FGIC. The proposed method contains two core modules: part selection module and representation module. The part selection module utilizes intrinsic spatial relation between object parts to select object part pairs with high discrimination power. The representation module exploits the interaction between object parts to describe the selected part pairs and construct a semantic image representation for FGIC. The proposed method is evaluated on CUB-200-2011 and FGVC-Aircraft datasets. Experimental results show that the classification accuracy of the proposed method can reach 85.5% on CUB-200-2011 and 86.9% on FGVC-Aircraft respectively, which exceed comparison methods obviously.","Fine-grained image classification, Spatial relation, Convolutional neural network",Lei Qi and Xiaoqiang Lu and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S0031320319300718,https://doi.org/10.1016/j.patcog.2019.02.007,0031-3203,2019,47--55,91,Pattern Recognition,Exploiting spatial relation for fine-grained image classification,article,QI201947,
"Most video based action recognition approaches create the video-level representation by temporally pooling the features extracted at every frame. The pooling methods they adopt, however, usually completely or partially ignore the dynamic information contained in the temporal domain, which may undermine the discriminative power of the resulting video representation since the video sequence order could unveil the evolution of a specific event or action. To overcome this drawback and explore the importance of incorporating the temporal order information, in this paper we propose a novel temporal pooling approach to aggregate the frame-level features. Inspired by the capacity of Convolutional Neural Networks (CNN) in making use of the internal structure of images for information abstraction, we propose to apply the temporal convolution operation to the frame-level representations to extract the dynamic information. However, directly implementing this idea on the original high-dimensional feature will result in parameter explosion. To handle this issue, we propose to treat the temporal evolution of the feature value at each feature dimension as a 1D signal and learn a unique convolutional filter bank for each 1D signal. By conducting experiments on three challenging video-based action recognition datasets, HMDB51, UCF101, and Hollywood2, we demonstrate that the proposed method is superior to the conventional pooling methods.","Action recognition, Convolutional neural network, Temporal pooling",Peng Wang and Lingqiao Liu and Chunhua Shen and Heng Tao Shen,https://www.sciencedirect.com/science/article/pii/S0031320319301013,https://doi.org/10.1016/j.patcog.2019.03.002,0031-3203,2019,357--365,91,Pattern Recognition,Order-aware convolutional pooling for video based action recognition,article,WANG2019357,
"We present an efficient multi-atlas segmentation and correction model with level set formulation for 3D brain MR images in this paper. We define a new energy functional by combining a weighted label fusion term, a bias field based image information fitting term and a regularization term together. More image information is taken into consideration in the new image data term to substantially improve the segmentation accuracy, especially when serious inhomogeneity and bias field exist in regions of interest in MR images. We introduce a spatially weight function and incorporate it into the label fusion term to increase the robustness of our segmentation algorithm to atlases with different registration accuracy. The new energy functional is in the form of L1 regularization problems, and we minimize it with the split Bregman method to ensure the segmentation efficiency. We apply the proposed model to segment six tissues in 3D brain MR images, including the amygdala, caudate, hippocampus, pallidum, putamen and thalamus. Experimental results have shown that our model can segment regions of interest accurately and eliminate bias field simultaneously. Quantitative comparisons with related methods have demonstrated the superiority of our model in terms of accuracy, efficiency and robustness.","Level set formulation, Multi-atlas label fusion, Tissue segmentation, Bias correction, MR images",Yunyun Yang and Wenjing Jia and Yunna Yang,https://www.sciencedirect.com/science/article/pii/S0031320319300561,https://doi.org/10.1016/j.patcog.2019.01.031,0031-3203,2019,450--463,90,Pattern Recognition,Multi-atlas segmentation and correction model with level set formulation for 3D brain MR images,article,YANG2019450,
"Recent studies on large-scale image classification mainly focus on categorizing images into 1000 object classes, and all these 1000 object classes are atomic and mutually exclusive in the semantic space. However, for a much larger set of image categories (such as the ImageNet 10k dataset), some of them may come from the high-level (non-leaf) nodes of the concept ontology and could contain some other lower-level categories semantically. The research that classifies images into large numbers of image categories with such inter-category subsumption correlations has received rare attention. In this paper, a Visual-Semantic Tree is learned to organize 10k image categories hierarchically in a coarse-to-fine fashion, where both the inter-category visual similarities and inter-category semantic correlations are seamlessly integrated for tree construction. Additionally, a deep learning method is developed by integrating the Visual-Semantic Tree with deep CNNs to learn more discriminative tree classifiers for large-scale image classification. Our experimental results have demonstrated that the proposed Visual-Semantic Tree can effectively organize large-scale structural image categories and significantly boost the classification accuracy rates for both atomic image categories and high-level image categories.","Visual-semantic tree, Inter-category correlation, Multi-task learning, Deep convolutional neural network, Large-scale image classification",Ji Zhang and Kuizhi Mei and Yu Zheng and Jianping Fan,https://www.sciencedirect.com/science/article/pii/S0031320319300962,https://doi.org/10.1016/j.patcog.2019.02.024,0031-3203,2019,175--189,91,Pattern Recognition,Learning multi-layer coarse-to-fine representations for large-scale image classification,article,ZHANG2019175,
"The significant properties of the maximum likelihood (ML) estimate are consistency, normality, and efficiency. While it has been proven that these properties are valid when the sample size approaches infinity, the behavior of an ML estimator when working with small sample sizes is largely unknown. However, in real tasks, we usually do not have sufficient data to completely fulfill the conditions of an optimal ML estimate. The question arises as to what amount of data is required to be able to estimate a Gaussian model that provides sufficiently accurate likelihood estimates. This issue is addressed with respect to the number of dimensions of the pattern space.","Maximum-likelihood estimate, Likelihood function, Gaussian model, Gaussian mixture model, Sample size, Dimensionality, Pattern space, Heteroscedastic data.",Josef V. Psutka and Josef Psutka,https://www.sciencedirect.com/science/article/pii/S0031320319300445,https://doi.org/10.1016/j.patcog.2019.01.046,0031-3203,2019,25--33,91,Pattern Recognition,Sample size for maximum-likelihood estimates of Gaussian model depending on dimensionality of pattern space,article,PSUTKA201925,
"The level set model is a popular method for object segmentation. However, most existing level set models perform poorly in color images since they only use grayscale intensity information to defined their energy functions. To address this shortcoming, in this paper, we propose a new saliency-guided level set model (SLSM), which can automatically segment objects in color images guided by visual saliency. Specifically, we first define a global saliency-guided energy term to extract the color objects approximately. Then, by integrating information from different color channels, we define a novel local multichannel based energy term to extract the color objects in detail. In addition, unlike using a length regularization term in the conventional level set models, we achieve segmentation smoothness by incorporating our SLSM into a graph cuts formulation. More importantly, the proposed SLSM is automatically initialized by saliency detection. Finally, the evaluation on public benchmark databases and our collected database demonstrates that the new SLSM consistently outperforms many state-of-the-art level set models and saliency detecting methods in accuracy and robustness.","Level set model, Object segmentation, Visual saliency, Graph cuts, Automatic initialization",Qing Cai and Huiying Liu and Yiming Qian and Sanping Zhou and Xiaojun Duan and Yee-Hong Yang,https://www.sciencedirect.com/science/article/pii/S0031320319301657,https://doi.org/10.1016/j.patcog.2019.04.019,0031-3203,2019,147--163,93,Pattern Recognition,Saliency-guided level set model for automatic object segmentation,article,CAI2019147,
"This study proposes a novel feature-guided Gaussian mixture model (FG-GMM) for image matching, which generally requires matching two sets of feature points extracted from the provided images. The problem is formulated as the estimation of a feature-guided mixture of densities: a GMM is fitted to one point set, in which both the centers and local features of the Gaussian densities are constrained to coincide with another point set. The said problem is solved under a unified maximum-likelihood framework, in which an iterative semi-supervised expectation-maximization algorithm initialized by the confident feature correspondence is also implemented. This algorithm is flexible and has a general scope, which can handle both rigid and non-rigid image transformations. The transformation in the non-rigid case is specified in a reproducing kernel Hilbert space, and a sparse approximation is adopted to accomplish rapid implementation. Extensive experiments on different real images show that the proposed approach consistently outperforms other state-of-the-art methods, which validates its robustness.","Image matching, Feature-guided, Gaussian mixture model, Local geometric constraint, Semi-supervised EM",Jiayi Ma and Xingyu Jiang and Junjun Jiang and Yuan Gao,https://www.sciencedirect.com/science/article/pii/S0031320319301414,https://doi.org/10.1016/j.patcog.2019.04.001,0031-3203,2019,231--245,92,Pattern Recognition,Feature-guided Gaussian mixture model for image matching,article,MA2019231,
"This paper proposes an online multi-object tracking algorithm for image observations using a top-down Bayesian formulation that seamlessly integrates state estimation, track management, handling of false positives, false negatives and occlusion into a single recursion. This is achieved by modeling the multi-object state as labeled random finite set and using the Bayes recursion to propagate the multi-object filtering density forward in time. The proposed filter updates tracks with detections but switches to image data when detection loss occurs, thereby exploiting the efficiency of detection data and the accuracy of image data. Furthermore the labeled random finite set framework enables the incorporation of prior knowledge that detection loss in the middle of the scene are likely to be due to occlusions. Such prior knowledge can be exploited to improve occlusion handling, especially long occlusions that can lead to premature track termination in on-line multi-object tracking. Tracking performance is compared to state-of-the-art algorithms on synthetic data and well-known benchmark video datasets.","Online multi-object tracking, Track-before-detect, Random finite set",Du Yong Kim and Ba-Ngu Vo and Ba-Tuong Vo and Moongu Jeon,https://www.sciencedirect.com/science/article/pii/S0031320319300688,https://doi.org/10.1016/j.patcog.2019.02.004,0031-3203,2019,377--389,90,Pattern Recognition,A labeled random finite set online multi-object tracker for video data,article,KIM2019377,
"Action recognition using pose information has drawn much attention recently. However, most previous approaches treat human pose as a whole or just use pose to extract robust features. Actually, human body parts play an important role in action, and so modeling spatio-temporal information of body parts can effectively assist in classifying actions. In this paper, we propose a Part-aligned Pose-guided Recurrent Network (P2RN) for action recognition. The model mainly consists of two modules, i.e., part alignment module and part pooling module, which are used for part representation learning and part-related feature fusion, respectively. The part-alignment module incorporates an auto-transformer attention, aiming to capture spatial configuration of body parts and predict pose attention maps. While the part pooling module exploits both symmetry and complementarity of body parts to produce fused body representation. The whole network is a recurrent network which can exploit the body representation and simultaneously model spatio-temporal evolutions of human body parts. Experiments on two publicly available benchmark datasets show the state-of-the-art performance and demonstrate the power of the two proposed modules.","Action recognition, Part alignment, Auto-transformer attention",Linjiang Huang and Yan Huang and Wanli Ouyang and Liang Wang,https://www.sciencedirect.com/science/article/pii/S0031320319301098,https://doi.org/10.1016/j.patcog.2019.03.010,0031-3203,2019,165--176,92,Pattern Recognition,Part-aligned pose-guided recurrent network for action recognition,article,HUANG2019165,
"Deep metric learning methods aim to transform data features from original scattered space to a discriminative subspace in an end-to-end way, and they have shown promising results on wide applications. Triplet loss functions are the most popular models to tackle deep metric learning problem as they simultaneously enhance separability between different classes and compactness of each class in the embedded subspace. Therefore, effective triplets selection is crucial to the classification performance. However, most of these methods only focus on mining hard negative pairs, which refer to the nearest sample pairs of different classes, while fail to take subtle cluster structure of each class into consideration. To take such information into the metric learning model, a novel scheme based on subtype fuzzy clustering is proposed in this paper. By defining a new clustering degree, we conduct fuzzy clustering for each class to mine classification-oriented subtype structure. The new clustering degree is inversely proportional to the pairwise distance, thus, we can choose the positive pairs of the highest clustering degrees directly based on the farthest distances within each class. This new sampling approach avoids off-line clustering step, for which the network weights update procedure has to be temporarily paused. In other words, our method builds positive pairs without explicit clustering degree computation or off-line clustering. Our method inputs the selected positive pairs and negative pairs into the standard triplet loss to achieve network feature learning. Experimental results show competitive metric learning performance on three benchmark datasets.","Metric learning, Deep networks, Triplet loss, Fuzzy clustering, Online sampling",Chuan-Xian Ren and Ju-Zheng Li and Pengfei Ge and Xiao-Lin Xu,https://www.sciencedirect.com/science/article/pii/S0031320319300548,https://doi.org/10.1016/j.patcog.2019.01.037,0031-3203,2019,210--219,90,Pattern Recognition,Deep metric learning via subtype fuzzy clustering,article,REN2019210,
"Discriminative dictionary learning (DDL) has demonstrated significantly improved performance for image classification. However, most of the existing DDL methods just adopt the single-layer dictionary learning architecture, which narrows the discriminative ability of the coding vectors. Another limitation of these methods is that the atoms of the learned dictionary are easily affected by the noise in the original data. To this end, a powerful architecture, called the multi-layer discriminative dictionary learning (MDDL) with locality constraint, is proposed for image classification. Through the multi-layer dictionary learning, the robust dictionary is obtained in the final layer, where the separability of coding vectors from different classes is also increased. Meanwhile, benefiting from joint classifier training and multi-layer dictionary learning, the discriminability of the learned coding vectors is further enhanced. Besides, by utilizing the graph Laplacian matrices based on the learned dictionaries, not only the locality information of the original data is preserved, but also it can avoid very large values in the coding vectors to reduce the test error caused by overfitting. In addition, an iterative algorithm is devised to efficiently solve the proposed MDDL. The experimental results demonstrate that our methods can achieve promising classification results on well-known benchmark image datasets.","Multi-layer discriminative dictionary learning, Locality constraint, Classifier training, Image classification",Jianqiang Song and Xuemei Xie and Guangming Shi and Weisheng Dong,https://www.sciencedirect.com/science/article/pii/S0031320319300822,https://doi.org/10.1016/j.patcog.2019.02.018,0031-3203,2019,135--146,91,Pattern Recognition,Multi-layer discriminative dictionary learning with locality constraint for image classification,article,SONG2019135,
"Purely data-driven methods often fail to learn accurate conditional probability table (CPT) parameters of discrete Bayesian networks (BNs) when training data are scarce or incomplete. A practical and efficient means of overcoming this problem is to introduce qualitative parameter constraints derived from expert judgments. To exploit such knowledge, in this paper, we provide a constrained maximum a posteriori (CMAP) method to learn CPT parameters by incorporating convex constraints. To further improve the CMAP method, we present a type of constrained Bayesian Dirichlet priors that is compatible with the given constraints. Combined with the CMAP method, we propose an improved expectation maximum algorithm to process incomplete data. Experiments are conducted on learning standard BNs from complete and incomplete data. The results show that the proposed method outperforms existing methods, especially when data are extremely limited or incomplete. This finding suggests the potential effective application of CMAP to real-world problems. Moreover, a real facial action unit (AU) recognition case with incomplete data is conducted by applying different parameter learning methods. The results show that the recognition accuracy of respective recognition methods can be improved by the AU BN, which is trained by the proposed method.","Bayesian network, Parameter learning, Expert judgment, Facial action unit",Yu Yang and Xiaoguang Gao and Zhigao Guo and Daqing Chen,https://www.sciencedirect.com/science/article/pii/S0031320319300706,https://doi.org/10.1016/j.patcog.2019.02.006,0031-3203,2019,123--134,91,Pattern Recognition,Learning Bayesian networks using the constrained maximum a posteriori probability method,article,YANG2019123,
"This paper proposes a novel connected components labeling (CCL) approach that introduces a gamma signal to record certain mask pixelsâ values to eliminate duplicated pixel checking and regulate the labeling process for higher efficiency. A new block-based two-scan CCL algorithm, Eight-Connected Gamma-Signal-regulated (ECGS) algorithm, is designed and developed by applying this approach to evaluate a block of 2â¯Ãâ¯2Â pixels (with just 6 mask pixels) in each iteration such that the total number of operations is considerably reduced and the labeling efficiency is significantly improved. The experiments conducted on a public benchmark, YACCLAB (Yet Another Connected Components Labeling Benchmark), have demonstrated that the proposed ECGS algorithm can outperform current state-of-the-art CCL algorithms for a number of digital images.","Connected components labeling, Object detection, Object recognition, Pattern recognition, Image analysis",Danyang Zhang and Huadong Ma and Linqiang Pan,https://www.sciencedirect.com/science/article/pii/S0031320319300949,https://doi.org/10.1016/j.patcog.2019.02.022,0031-3203,2019,281--290,91,Pattern Recognition,A gamma-signal-regulated connected components labeling algorithm,article,ZHANG2019281,
"This work addresses the recurring challenge of real-time monophonic and polyphonic audio source classification. The whole normalized power spectrum (NPS) is directly involved in the proposed process, avoiding complex and hazardous traditional feature extraction. It is also a natural candidate for polyphonic events thanks to its additive property in such cases. The classification task is performed through a nonparametric kernel-based generative modeling of the power spectrum. Advantage of this model is twofold: it is almost hypothesis free and it allows to straightforwardly obtain the maximum a posteriori classification rule of online signals. Moreover it makes use of the monophonic dataset to build the polyphonic one. Then, to reach the real-time target, the complexity of the method can be tuned by using a standard hierarchical clustering preprocessing of the prototypes, revealing a particularly efficient computation time and classification accuracy trade-off. The proposed method, called RARE (for Real-time Audio Recognition Engine) reveals encouraging results both in monophonic and polyphonic classification tasks on benchmark and owned datasets, including also the targeted real-time situation. In particular, this method benefits from several advantages compared to the state-of-the-art methods including a reduced training time, no feature extraction, the ability to control the computation - accuracy trade-off and no training on already mixed sounds for polyphonic classification.","Real-time, Audio classification, Machine learning, Monophonic, Polyphonic, Generative model, Nonparametric estimation",Maxime Baelde and Christophe Biernacki and RaphaÃ«l Greff,https://www.sciencedirect.com/science/article/pii/S0031320319301244,https://doi.org/10.1016/j.patcog.2019.03.017,0031-3203,2019,82--92,92,Pattern Recognition,Real-Time monophonic and polyphonic audio classification from power spectra,article,BAELDE201982,
"Each area fingerprint scanner (sensor) has a unique scanner pattern that can be used to distinguish one scanner from another. The pattern results from imperfections in the conversion of the signal acquired from the object, applied to the scanner, into a digital image. The scanner pattern is a sufficiently unique, persistent, and unalterable intrinsic characteristic of the fingerprint scanners even to those of the same technology, manufacturer, and model. We propose a model of the image acquisition in widely-used capacitive area fingerprint scanners and a method to extract the pattern from a single image and compare it with a similarly extracted pattern from another image. The method is very simple and computationally efficient yet extremely accurate, achieving an equal error rate in the order of 1e-7 for 22 scanners of exactly the same model, and is also robust in a wide range of conditions. The scanner pattern can be used to enhance the security of a biometric system by detecting an attack in which an image containing the fingerprint pattern of the legitimate user and acquired by the authentic fingerprint scanner has been replaced by another image that may still contain the fingerprint pattern of the legitimate user but has been acquired by another, unauthentic fingerprint scanner, i.e., for scanner authentication.","Authentication, Biometric, Fingerprint, Scanner, Sensor, Pattern, Noise.",Vladimir I. Ivanov and John S. Baras,https://www.sciencedirect.com/science/article/pii/S0031320319301001,https://doi.org/10.1016/j.patcog.2019.03.001,0031-3203,2019,230--249,94,Pattern Recognition,Authentication of area fingerprint scanners,article,IVANOV2019230,
"High-dimensional data requires scalable algorithms. We propose and analyze four scalable and related algorithms for semi-supervised discriminant analysis (SDA). These methods are based on Krylov subspace methods and therefore exploit data sparsity and the shift-invariance of Krylov subspaces. In addition, centralization was derived for the semi-supervised setting. The proposed methods are evaluated on an industry-scale data set from a pharmaceutical company to predict compound activity on target proteins. The results show that our methods only require a few seconds, significantly improving computation time on the state of the art.","Semi-supervised learning, Semi-supervised discriminant analysis, Large-scale",Joris Tavernier and Jaak Simm and Karl Meerbergen and Joerg Kurt Wegner and Hugo Ceulemans and Yves Moreau,https://www.sciencedirect.com/science/article/pii/S0031320319300792,https://doi.org/10.1016/j.patcog.2019.02.015,0031-3203,2019,86--99,91,Pattern Recognition,Fast semi-supervised discriminant analysis for binary classification of large data sets,article,TAVERNIER201986,
"In this paper we derive practical and novel upper bounds for the resubstitution error estimate by assessing the number of linear decision functions within the problem of pattern recognition in neuroimaging. Linear classifiers and regressors have been considered in many fields, where the number of predictors far exceeds the number of training samples available, to overcome the limitations of high complexity models in terms of computation, interpretability and overfitting. Typically in neuroimaging this is the rule rather than the exception, since the dimensionality of each observation (millions of voxels) in relation to the number of available samples (hundred of scans) implies a high risk of overfitting. Based on classical combinatorial geometry, we estimate the number of hyperplanes or linear decision rules and the corresponding distribution-independent performance bounds, comparing it to those obtained by the use of the VC-dimension concept. Experiments on synthetic and neuroimaging data demonstrate the performance of resubstitution error estimators, which are often overlooked in heterogeneous scenarios where their performance is similar to that obtained by cross-validation methods.","Resubsitution error estimate, Lineal classifiers, Upper bounds, Neuroimaging, VC dimension",Juan M. GÃ³rriz and Javier Ramirez and John Suckling,https://www.sciencedirect.com/science/article/pii/S0031320319301402,https://doi.org/10.1016/j.patcog.2019.03.032,0031-3203,2019,1--13,93,Pattern Recognition,On the computation of distribution-free performance bounds: Application to small sample sizes in neuroimaging,article,GORRIZ20191,
"Recent studies have demonstrated advantages of the representations learned by Convolutional Neural Networks (CNNs) in providing an appealing paradigm for visual classification tasks. Most existing methods adopt activations from the last fully connected layer as the image representation. This paper advocates exploiting appropriately convolutional layer activations to constitute a powerful descriptor for texture classification under an end-to-end learning framework. The main component of our method is a new locality-aware coding layer conducted with the locality constraint, where the dictionary and the encoding representation are learned simultaneously. The layer is readily amenable to training via the backpropagation as the locality-aware coding process has an analytical solution. It is capable of capturing class-specific information which makes the learned convolutional features more robust. The resulting representation is particularly useful for texture classification. Comprehensive experiments on the DTD, FMD and KTH-T2b datasets show that our approach notably outperforms the state-of-the-art methods.","Deep convolutional feature, Sparse coding, Locality-aware, Texture classification",Xingyuan Bu and Yuwei Wu and Zhi Gao and Yunde Jia,https://www.sciencedirect.com/science/article/pii/S0031320319300676,https://doi.org/10.1016/j.patcog.2019.02.003,0031-3203,2019,34--46,91,Pattern Recognition,Deep convolutional network with locality and sparsity constraints for texture classification,article,BU201934,
"A major issue in the classification of class imbalanced datasets involves the determination of the most suitable performance metrics to be used. In previous work using several examples, it has been shown that imbalance can exert a major impact on the value and meaning of accuracy and on certain other well-known performance metrics. In this paper, our approach goes beyond simply studying case studies and develops a systematic analysis of this impact by simulating the results obtained using binary classifiers. A set of functions and numerical indicators are attained which enables the comparison of the behaviour of several performance metrics based on the binary confusion matrix when they are faced with imbalanced datasets. Throughout the paper, a new way to measure the imbalance is defined which surpasses the Imbalance Ratio used in previous studies. From the simulation results, several clusters of performance metrics have been identified that involve the use of Geometric Mean or Bookmaker Informedness as the best null-biased metrics if their focus on classification successes (dismissing the errors) presents no limitation for the specific application where they are used. However, if classification errors must also be considered, then the Matthews Correlation Coefficient arises as the best choice. Finally, a set of null-biased multi-perspective Class Balance Metrics is proposed which extends the concept of Class Balance Accuracy to other performance metrics.","Classification, Performance measures, Imbalanced datasets, Class Balance Metrics",Amalia Luque and Alejandro Carrasco and Alejandro MartÃ­n and Ana {de las Heras},https://www.sciencedirect.com/science/article/pii/S0031320319300950,https://doi.org/10.1016/j.patcog.2019.02.023,0031-3203,2019,216--231,91,Pattern Recognition,The impact of class imbalance in classification performance metrics based on the binary confusion matrix,article,LUQUE2019216,
"This paper formulates superpixel segmentation as a pixel labeling problem and proposes a quaternary labeling algorithm to generate superpixel lattice. It is achieved by seaming overlapped patches regularly placed on the image plane. Patch seaming is formulated as a pixel labeling problem, where each label indexes one patch. Once the optimal seaming is completed, all pixels covered by one retained patch constitute one superpixel. Further, four kinds of patches are distinguished and assembled into four layers correspondingly, and the patch indexes are mapped to the quaternary layer indexes. It significantly reduces the number of labels and greatly improves labelling efficiency. Furthermore, an objective function is developed to achieve optimal segmentation. Lattice structure is guaranteed by fixing patch centers to be superpixel centers, compact superpixels are assured by horizontal and vertical constraints enforced on the smooth terms, and coherent superpixels are achieved by iteratively refining the data terms. Extensive experiments on BSDS data set demonstrate that SQL algorithm significantly improves labeling efficiency, outperforms the other superpixel lattice methods, and is competitive with state-of-the-art methods without lattice guarantee. Superpixel lattice allows contextual relationships among superpixels to be easily modeled by either MRFs or CNN.","Superpixels, Segmentation, Seaming, Pixel labeling, Graph cuts",Dengfeng Chai,https://www.sciencedirect.com/science/article/pii/S0031320319301128,https://doi.org/10.1016/j.patcog.2019.03.012,0031-3203,2019,52--63,92,Pattern Recognition,SQL: Superpixels via quaternary labeling,article,CHAI201952,
"Improving CT images by increasing the number of scans, hence increasing the ionizing radiation dose, can increase the probability of inducing cancer in the patient. Using fewer images but improving them by accurate reconstruction is better solution. In this paper, an adaptive variational Partial Differential Equation (PDE) model is proposed for image reconstruction. L2 energy of the image gradient and the Total Variation (TV) are combined to form a new functional, which is introduced to an optimization problem. The dynamic behaviors of the model are formed by a threshold function, and then the L2 term is applied in the lower-density region to increase reconstruction speed, and the TV term is applied in the higher-density region to preserve the most important image features. The threshold function is asymptotically controlled by an evolutionary PDE and is more suitable for complex images. The efficiency and accuracy of the proposed model are demonstrated in numerical experiments.","Image reconstruction, Combined functional, Partial differential equation, Regional analysis, Variational analysis",Wei Wei and Bin Zhou and Dawid PoÅap and Marcin WoÅºniak,https://www.sciencedirect.com/science/article/pii/S0031320319301086,https://doi.org/10.1016/j.patcog.2019.03.009,0031-3203,2019,64--81,92,Pattern Recognition,A regional adaptive variational PDE model for computed tomography image reconstruction,article,WEI201964,
"Subspace segmentation or clustering remains a challenge of interest in computer vision when handling complex noise existing in high-dimensional data. Most of the current sparse representation or minimum-rank based techniques are constructed on â1-norm or â2-norm losses, which is sensitive to outliers. Finite mixture model, as a class of powerful and flexible tools for modeling complex noise, becomes a must. Among all the choices, exponential family mixture is extremely useful in practice due to its universal approximation ability for any continuous distribution and hence covers a broader scope of characteristics of noise distribution. Equipped with such a modeling idea, this paper focuses on the complex noise contaminated subspace clustering problem by using finite mixture of exponential power (MoEP) distributions. We then harness a penalized likelihood function to perform automatic model selection and hence avoid over-fitting. Moreover, we introduce a novel prior on the singular values of representation matrix, which leads to a novel penalty in our nonconvex and nonsmooth optimization. The parameters of the MoEP model can be estimated with a Maximum A Posteriori (MAP) method. Meanwhile, the subspace is computed with joint weighted âp-norm and Schatten-q quasi-norm minimization. Both theoretical and experimental results show the effectiveness of our method.","Subspace clustering, Noises modelling, Finite mixture model, Nonconvex and nonsmooth optimization",Xianglin Guo and Xingyu Xie and Guangcan Liu and Mingqiang Wei and Jun Wang,https://www.sciencedirect.com/science/article/pii/S0031320319301360,https://doi.org/10.1016/j.patcog.2019.03.028,0031-3203,2019,55--67,93,Pattern Recognition,Robust Low-rank subspace segmentation with finite mixture noise,article,GUO201955,
"Convolutional Neural Network has become very common in the field of computer vision in recent years. But it comes with a severe restriction regarding the size of the input image. Most convolutional neural networks are designed in a way so that they can only accept images of a fixed size. This creates several challenges during data acquisition and model deployment. The common practice to overcome this limitation is to reshape the input images so that they can be fed into the networks. Many standard pre-trained networks and datasets come with a provision of working with square images. In this work we analyze 25 different reshaping methods across 6 datasets corresponding to different domains trained on three famous architectures namely Inception-V3, which is an extension of GoogLeNet, the Residual Networks (Resent-18) and the 121-Layer deep DenseNet. While some of the reshaping methods like âinterpolationâ and âcroppingâ have been commonly used with convolutional neural networks, some uncommon techniques like âcontainingâ, âtilingâ and âmirroringâ have also been demonstrated. In total, 450 neural networks were trained from scratch to provide various analyses regarding the convergence of the validation loss and the accuracy obtained on the test data. Statistical measures have been provided to demonstrate the dependence between parameter choices and datasets. Several key observations were noted such as the benefits of using randomized processes, poor performance of the commonly used âcroppingâ techniques and so on. The paper intends to provide empirical evidence to guide the reader to choose a proper technique of reshaping inputs for their convolutional neural networks. The official code is available in https://github.com/DVLP-CMATERJU/Reshaping-Inputs-for-CNN.","Deep learning, Convolutional neural network, Reshaping, Resizing, Input size",Swarnendu Ghosh and Nibaran Das and Mita Nasipuri,https://www.sciencedirect.com/science/article/pii/S0031320319301505,https://doi.org/10.1016/j.patcog.2019.04.009,0031-3203,2019,79--94,93,Pattern Recognition,Reshaping inputs for convolutional neural network: Some common and uncommon methods,article,GHOSH201979,
"The representation of online handwriting is an important aspect of handwriting applications, which involves the extraction of various spatial and temporal attributes for analysis and individualization of handwritten patterns. In this work, a model based representation is proposed for online handwriting using a multi-component sinusoidal model. The method extracts sinusoidal parameters from handwriting by modeling its horizontal and vertical velocities between each successive pair of zero crossing points with a half period of the sine function. Thus, each velocity profile is represented by the sinusoidal oscillations whose parameters are modulated at the zero-crossing points. The use of multiple oscillations to model the velocities results in a better representation of the complex trajectories. The parameters of the proposed model are computed iteratively from its residual signals. We hypothesize that the analysis of the sinusoidal components and its parameters may provide added dynamic information about the handwriting. The efficacy of the proposal is demonstrated for online signature representation and synthetic variability generation by modifying the extracted parameters. Further, the proposed feature set is also employed for online handwriting recognition task. It is observed that the multi-component sinusoidal representation combined with existing point-based features provide an improvement in the recognition performance.","Sinusoidal/oscillatory model, Handwriting synthesis, Sinusoidal features, Handwriting recognition",Himakshi Choudhury and S.R. Mahadeva Prasanna,https://www.sciencedirect.com/science/article/pii/S0031320319300779,https://doi.org/10.1016/j.patcog.2019.02.013,0031-3203,2019,200--215,91,Pattern Recognition,Representation of online handwriting using multi-component sinusoidal model,article,CHOUDHURY2019200,
"Extending previous work on quantile classifiers (q-classifiers) we propose the q*-classifier for the class imbalance problem. The classifier assigns a sample to the minority class if the minority class conditional probability exceeds 0â¯<â¯q*â¯<â¯1, where q* equals the unconditional probability of observing a minority class sample. The motivation for q*-classification stems from a density-based approach and leads to the useful property that the q*-classifier maximizes the sum of the true positive and true negative rates. Moreover, because the procedure can be equivalently expressed as a cost-weighted Bayes classifier, it also minimizes weighted risk. Because of this dual optimization, the q*-classifier can achieve near zero risk in imbalance problems, while simultaneously optimizing true positive and true negative rates. We use random forests to apply q*-classification. This new method which we call RFQ is shown to outperform or is competitive with existing techniques with respect to G-mean performance and variable selection. Extensions to the multiclass imbalanced setting are also considered.","Weighted Bayes classifier, Response-based sampling, Class imbalance, Minority class, Random forests",Robert OâBrien and Hemant Ishwaran,https://www.sciencedirect.com/science/article/pii/S0031320319300536,https://doi.org/10.1016/j.patcog.2019.01.036,0031-3203,2019,232--249,90,Pattern Recognition,A random forests quantile classifier for class imbalanced data,article,OBRIEN2019232,
"Among the variety of algorithms that have been developed for clustering, prototype-based approaches are very popular due to their low computational complexity, allowing real-life applications. In such algorithms, the data set is summarized by a small set of prototypes. Each prototype usually represents a cluster of objects. However, the definition of prototypes for complex objects defined by their relations (relational data) is not an easy task. Few works have been done yet in relational prototype-based clustering. Because relational data are described by a full matrix of dissimilarities, the most important challenge is the computation and memory costs, especially when the number of objects to analyze is very large and for the analysis of data streams (data sets with a dynamic structure varying over time). The combination of these three characteristics (size, complexity and evolution) presents a major challenge and few satisfactory solutions exist at the moment, despite increasingly evident needs. This paper focus on the development of new clustering approaches adapted to big and dynamic relational data. The main idea is to use a set of fixed support points chosen among the objects of the data set, independently from the clusters, and use these support points as a basis for the definition of a representation space, using the Barycentric Coordinates formalism. We demonstrate the qualities of the proposed approaches theoretically and experimentally on a set of artificial and real relational data. We also propose an extension adapted to relational data stream analysis, allowing a dynamic creation and suppression of prototypes to follow the dynamic of the data structure. This dynamic approach is applied on a real data set to detect and follow the dynamic of areas of interest over time in userâs web navigation. We tested different measures of similarity between URLs and different methods of automatic labeling to characterize the clusters. The results are convincing and encouraging, the clusters are homogeneous with clear associated topics. The dynamics of userâs interest can be recorded and visualized for each cluster. Remarkable patterns can be associated to precise events or usual timing and cycles in userâs interest.","Clustering, Relational data, Barycentric coordinates, Data stream",Parisa Rastin and GuÃ©naÃ«l Cabanes and Basarab Matei and YounÃ¨s Bennani and Jean-Marc Marty,https://www.sciencedirect.com/science/article/pii/S0031320319300937,https://doi.org/10.1016/j.patcog.2019.02.020,0031-3203,2019,291--307,91,Pattern Recognition,A new sparse representation learning of complex data: Application to dynamic clustering of web navigation,article,RASTIN2019291,
"Although online handwritten Chinese characters recognition has been explored for decades, it is still a challenging task to recognize handwritten Chinese characters accurately. In this paper, we propose an end-to-end recognizer for online handwritten Chinese characters recognition based on a new recurrent neural network (RNN). In the system, two new computing architectures are proposed based on traditional RNN system. One is the variance constraint, and the other is attention weight vector. The variance constraint is used to increase the representation ability of RNN, and the attention weight vector is used to describe the importance of hidden layer states at different time steps. Benefited from the two innovations, the recognition system obtains higher recognition accuracy with fewer parameters. Experiments are carried out on two handwritten Chinese character datasets, IAHCC-UCAS2016 dataset and ICDAR-2013 competition database. The experimental results show that the two innovations are effective, and the proposed end-to-end recognizer obtains better performance than the state-of-the-art methods.","Online handwritten Chinese character recognition, Recurrent neural network, New computing architectures",Haiqing Ren and Weiqiang Wang and Chenglin Liu,https://www.sciencedirect.com/science/article/pii/S003132031930161X,https://doi.org/10.1016/j.patcog.2019.04.015,0031-3203,2019,179--192,93,Pattern Recognition,Recognizing online handwritten Chinese characters using RNNs with new computing architectures,article,REN2019179,
"In this work, we describe a License Plate Recognition (LPR) system designed around convolutional neural networks (CNNs) trained on synthetic images to avoid collecting and annotating the thousands of images required to train a CNN. First, we propose a framework for generating synthetic license plate images, accounting for the key variables required to model the wide range of conditions affecting the aspect of real plates. Then, we describe a modular LPR system designed around two CNNs for plate and character detection enjoying common training procedures and train the CNNs and experiment on three different datasets of real plate images collected from different countries. Our synthetically trained system outperforms multiple competing systems trained on real images, showing that synthetic images are effective at training a CNNs for LPR if the training images have sufficient variance of the key variables controlling the plate aspect.","License plate recognition (LPR), Convolutional neural network (CNN), Synthetic training",Tomas BjÃ¶rklund and Attilio Fiandrotti and Mauro Annarumma and Gianluca Francini and Enrico Magli,https://www.sciencedirect.com/science/article/pii/S0031320319301475,https://doi.org/10.1016/j.patcog.2019.04.007,0031-3203,2019,134--146,93,Pattern Recognition,Robust license plate recognition using neural networks trained on synthetic images,article,BJORKLUND2019134,
"Sound analysis research has mainly been focused on speech and music processing. The deployed methodologies are not suitable for analysis of sounds with varying background noise, in many cases with very low signal-to-noise ratio (SNR). In this paper, we present a method for the detection of patterns of interest in audio signals. We propose novel trainable feature extractors, which we call COPE (Combination of Peaks of Energy). The structure of a COPE feature extractor is determined using a single prototype sound pattern in an automatic configuration process, which is a type of representation learning. We construct a set of COPE feature extractors, configured on a number of training patterns. Then we take their responses to build feature vectors that we use in combination with a classifier to detect and classify patterns of interest in audio signals. We carried out experiments on four public data sets: MIVIA audio events, MIVIA road events, ESC-10 and TU Dortmund data sets. The results that we achieved (recognition rate equal to 91.71% on the MIVIA audio events, 94% on the MIVIA road events, 81.25% on the ESC-10 and 94.27% on the TU Dortmund) demonstrate the effectiveness of the proposed method and are higher than the ones obtained by other existing approaches. The COPE feature extractors have high robustness to variations of SNR. Real-time performance is achieved even when the value of a large number of features is computed.","Audio analysis, Event detection, Peaks of energy, Representation learning, Trainable feature extractors",Nicola Strisciuglio and Mario Vento and Nicolai Petkov,https://www.sciencedirect.com/science/article/pii/S0031320319301232,https://doi.org/10.1016/j.patcog.2019.03.016,0031-3203,2019,25--36,92,Pattern Recognition,Learning representations of sound using trainable COPE feature extractors,article,STRISCIUGLIO201925,
"The automatic image annotation can provide semantic illustrations to understand image contents, and builds a foundation to develop algorithms that can search images within a large database. However, most current methods focus on solving the annotation problem by modeling the image visual content and tag semantic information, which overlooks the additional information, such as scene descriptions and locations. Moreover, the majority of current annotation datasets are visually consistent and only annotated by common visual objects and attributes, which makes the classic methods vulnerable to handle the more diverse image annotation. To address above issues, we propose to annotate images via collective knowledge, that is, we uncover relationships between the image and its neighbors by measuring similarities among metadata and conduct the metric learning to obtain the representations of image contents, we also generate semantic representations for images given collective semantic information from their neighbors. Two representations from different paradigms are embedded together to train an annotation model. We ground our model on the heritage image collection we collected from the library online open data. Annotations on the heritage image collection are not limited to common visual objects, and are highly relevant to historical events, and the diversity of the heritage image content is much larger than the current datasets, which makes it more suitable for this task. Comprehensive experimental results on the benchmark dataset indicate that the proposed model achieves the best performance compared to baselines and state-of-the-art methods.","Annotation diversity, Image annotation, Representation learning, Collective knowledge, Heritage image collection",Junjie Zhang and Qi Wu and Jian Zhang and Chunhua Shen and Jianfeng Lu and Qiang Wu,https://www.sciencedirect.com/science/article/pii/S0031320319301633,https://doi.org/10.1016/j.patcog.2019.04.017,0031-3203,2019,204--214,93,Pattern Recognition,Heritage image annotation via collective knowledge,article,ZHANG2019204,
"The iterative closest point (ICP) algorithm has the advantage of high accuracy and fast speed for point set registration, but it performs poorly when the point sets have a large number of outliers and noises. To solve this problem, in this paper, a novel robust scale ICP algorithm is proposed by introducing maximum correntropy criterion (MCC) as the similarity measure. As the correntropy has the property of eliminating the interference of outliers and noises compared to the commonly used Euclidean distance, we use it to build a new model for scale registration problem and propose the robust scale ICP algorithm. Similar to the traditional ICP algorithm, this algorithm computes the index mapping of the correspondence and a transformation matrix alternatively, but we restrict the transformation matrix to include only rotation, translation and a scale factor. We show that our algorithm converges monotonously to a local maximum for any given initial parameters. Experiments on synthetic and real datasets demonstrate that the proposed algorithm greatly outperforms state-of-the-art methods in terms of matching accuracy and run-time, especially when the data contain severe outliers.","Iterative closest point, Correntropy, Scale transformation, Point set registration, Outliers",Zongze Wu and Hongchen Chen and Shaoyi Du and Minyue Fu and Nan Zhou and Nanning Zheng,https://www.sciencedirect.com/science/article/pii/S0031320319301116,https://doi.org/10.1016/j.patcog.2019.03.013,0031-3203,2019,14--24,93,Pattern Recognition,Correntropy based scale ICP algorithm for robust point set registration,article,WU201914,
"Curved text detection is a difficult problem that has not been addressed sufficiently. To highlight the difficulties in reading curved text in a real environment, we constructed a curved text dataset called CTW1500, which includes over 10,000 text annotations in 1500 images, and used it to formulate a polygon-based curved text detector that can detect curved text without using an empirical combination. With the seamless integration of recurrent transverse and longitudinal offset connection, our method explores context information instead of predicting points independently, resulting in smoother and more accurate detection. Our approach is designed as a universal method, meaning it can be trained using rectangular or quadrilateral bounding boxes, requiring no extra effort. Experimental results on the CTW1500 dataset and Total-text demonstrated that our method with only a light backbone can outperform state-of-the-art methods by a large margin. Our method also achieved state-of-the-art performance on the MSRA-TD500 dataset, demonstrating its promising generalization ability. Code, datasets, and label-tool are available at https://github.com/Yuliang-Liu/Curve-Text-Detector.","Scene text, Curved dataset, LSTM, CNN, Deep learning",Yuliang Liu and Lianwen Jin and Shuaitao Zhang and Canjie Luo and Sheng Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319300664,https://doi.org/10.1016/j.patcog.2019.02.002,0031-3203,2019,337--345,90,Pattern Recognition,Curved scene text detection via transverse and longitudinal sequence connection,article,LIU2019337,
"Traditional machine learning is generally committed to obtaining classifiers which are well-performed over unlabeled test data. This usually relies on two critical assumptions: firstly, sufficient labeled training data are available; secondly, training and testing data are drawn from the same distribution and the same feature space. Unfortunately, in most cases, the actual situation is difficult to meet the above conditions. Transfer learning scheme is naturally proposed to alleviate this problem. In order to get robust classifiers with relatively lower computational costs, we incorporate the rationale of Support Vector Machine (SVM) into transfer learning scheme and propose a novel SVM-based transfer learning model, abbreviated as TrSVM. In this method, support vector sets are extracted to represent the source domain. New training datasets are respectively constructed by combining each support vector set and target labeled dataset. On the basis of these training datasets, a number of new base classifiers can be acquired. Since performance of a classifiers ensemble is generally superior to that of individual classifiers, ensemble selection is utilized in our work. A hybrid transfer learning algorithm, integrating the Genetic Algorithm based Selective Ensemble (GASEN) with TrSVM, is proposed, and abbreviated as TrGASVM, naturally. GASEN is a genetic algorithm-based heuristic algorithm for solving combinatorial optimization problems. It can not only enhance the generalization ability of an ensemble, but also alleviate the local minimum problem of greedy ensemble pruning methods. Since TrGASVM is under frame of TrSVM and GASEN, it inevitably inherits the advantages of both algorithms. The reasonable incorporation of TrSVM with GASEN endows TrGASVM with favorable transfer learning capability, with its effectiveness being demonstrated by the experimental results on three real-world text classification datasets.","Transfer learning, Ensemble selection, Genetic Algorithm based Selective Ensemble algorithm (GASEN), Support Vector Machine based Transfer Learning algorithm (TrSVM), Transfer Learning algorithm incorporating TrSVM with GASEN (TrGASVM)",Rui Ye and Qun Dai and MeiLing Li,https://www.sciencedirect.com/science/article/pii/S0031320319301359,https://doi.org/10.1016/j.patcog.2019.03.027,0031-3203,2019,192--202,92,Pattern Recognition,A hybrid transfer learning algorithm incorporating TrSVM with GASEN,article,YE2019192,
"Microarray gene expression data-based tumor classification is an active and challenging issue. In this paper, an integrated tumor classification framework is presented, which aims to exploit information in existing available samples, and focuses on the small sample problem and unbalanced classification problem. Firstly, an inverse space sparse representation based classification (ISSRC) model is proposed by considering the characteristics of gene-based tumor data, such as sparsity and a small number of training samples. A decision information factors (DIF)-based gene selection method is constructed to enhance the representation ability of the ISSRC. It is worth noting that the DIF is established from reducing clinical misdiagnosis rate and dimension of small sample data. For further improving the representation ability and classification stability of the ISSRC, feature learning is conducted on the selected gene subset. The feature learning method is constructed by complementing the advantages of non-negative matrix factorization (NMF) and deep learning. Without confusion, the ISSRC combined with gene selection and feature learning is called the integrated ISSRC, whose stability, optimization and the corresponding convergence are analyzed. Extensive experiments on six public microarray gene expression datasets show the integrated ISSRC-based tumor classification framework is superior to classical and state-of-the-art methods. There are significant improvements in classification accuracy, specificity and sensitivity, whether there is a tumor in the early diagnosis, what kind of tumor, or whether metastasis occurs after tumor surgery.","Tumor classification, Microarray gene expression data, Decision information genes, Layer-wise pre-training sparse NMF, Inverse space sparse representation",Xiaohui Yang and Wenming Wu and Yunmei Chen and Xianqi Li and Juan Zhang and Dan Long and Lijun Yang,https://www.sciencedirect.com/science/article/pii/S0031320319301591,https://doi.org/10.1016/j.patcog.2019.04.013,0031-3203,2019,293--311,93,Pattern Recognition,An integrated inverse space sparse representation framework for tumor classification,article,YANG2019293,
"Cross-view classification that means to classify samples from heterogeneous views is a significant yet challenging problem in computer vision. An effective solution to this problem is the multi-view subspace learningÂ (MvSL), which intends to find a common subspace for multi-view data. Although great progress has been made, existing methods usually fail to find a suitable subspace when multi-view data lies on nonlinear manifolds, thus leading to performance deterioration. To circumvent this drawback, we propose Multi-view Common Component Discriminant AnalysisÂ (MvCCDA) to handle view discrepancy, discriminability and nonlinearity in a joint manner. Specifically, our MvCCDA incorporates supervised information and local geometric information into the common component extraction process to learn a discriminant common subspace and to discover the nonlinear structure embedded in multi-view data. Optimization and complexity analysis of MvCCDA are also presented for completeness. Our MvCCDA is competitive with the state-of-the-art MvSL based methods on four benchmark datasets, demonstrating its superiority.","Cross-view classification, Local geometry preservation, Multi-view learning, Subspace learning",Xinge You and Jiamiao Xu and Wei Yuan and Xiao-Yuan Jing and Dacheng Tao and Taiping Zhang,https://www.sciencedirect.com/science/article/pii/S0031320319301074,https://doi.org/10.1016/j.patcog.2019.03.008,0031-3203,2019,37--51,92,Pattern Recognition,Multi-view common component discriminant analysis for cross-view classification,article,YOU201937,
"In this paper, a new set of moment invariants, named Racah Moment Invariants (RMI), is introduced in the field of image analysis. This new set can be used to describe pattern feature independently of Rotation, Scaling and Translation transforms. Moreover, new fast and accurate algorithm, using recursive method, is developed for accelerating the computation time of the newly proposed invariants, as well as, for enhancing their numerical stability. Subsequently, several experiments have been performed. Initially, the numerical stability and computational cost are depicted. Secondly, the global and local features extraction are clearly illustrated. Then, invariability property and noise robustness are investigated. Finally, the discrimination power and the classification accuracy of the proposed invariants are extensively tested on several publicly available databases. The presented theoretical and experimental results, clearly show that the proposed method can be extremely useful in the fields of image classification.","Racah moment invariants, Racah polynomials, Fast algorithm, Accurate computation, Direct method, Recursive method, Image classification, Pattern recognition",Rachid Benouini and Imad Batioua and Khalid Zenkouar and Azeddine Zahi and Hakim El Fadili and Hassan Qjidaa,https://www.sciencedirect.com/science/article/pii/S0031320319300780,https://doi.org/10.1016/j.patcog.2019.02.014,0031-3203,2019,100--110,91,Pattern Recognition,Fast and accurate computation of Racah moment invariants for image classification,article,BENOUINI2019100,
"Brain tumor segmentation from Magnetic Resonance Imaging scans is vital for both the diagnosis and treatment of brain cancers. It is widely accepted that accurate segmentation depends on multi-level information. However, exiting deep architectures for brain tumor segmentation fail to explicitly encourage the models to learn high-quality hierarchical features. In this paper, we propose a series of approaches to enhance the quality of the learnt hierarchical features. Our contributions incorporate four aspects. First, we extend the popular DeepMedic model to Multi-Level DeepMedic to make use of multi-level information for more accurate segmentation. Second, we propose a novel dual-force training scheme to promote the quality of multi-level features learnt from deep models. It is a general training scheme and can be applied to many exiting architectures, e.g., DeepMedic and U-Net. Third, we design a label distribution-based loss function as an auxiliary classifier to encourage the high-level layers of deep models to learn more abstract information. Finally, we propose a novel Multi-Layer Perceptron-based post-processing approach to refine the prediction results of deep models. Extensive experiments are conducted on two most recent brain tumor segmentation datasets, i.e., BRATS 2017 and BRATS 2015 datasets. Results on the two databases indicate that the proposed approaches consistently promote the segmentation performance of the two popular deep models.","Brain tumor segmentation, Dual-force network, Convolutional neural network, Label distribution, Post-processing",Shengcong Chen and Changxing Ding and Minfeng Liu,https://www.sciencedirect.com/science/article/pii/S0031320318303947,https://doi.org/10.1016/j.patcog.2018.11.009,0031-3203,2019,90--100,88,Pattern Recognition,Dual-force convolutional neural networks for accurate brain tumor segmentation,article,CHEN201990,
"In the present era of âBig Dataâ, data collection involving massive amount of features with a mix of variable types is commonplace. Mixture model-based techniques for statistical cluster analysis of mixed numerical and categorical feature data have their limitations, due to the difficulty in specifying appropriate component-densities when common multivariate distributions become invalid. This problem is particularly apparent in applications where the outcome feature variables are in a categorical form. An example of such an application is the analysis of binary morbidity data in national health survey, where the aims are to quantify heterogeneous comorbidity patterns of health conditions and identify (risk)-features of individuals that explain the heterogeneity. In this paper, we propose an unsupervised mixture regression model of multivariate generalised Bernoulli distributions for cluster analysis on the basis of categorical outcome features and mixed risk features. The proposed method is illustrated using simulated data and two real data sets concerning comorbidity patterns among 20,788 Australians who participated in the 2007â2008 National Health Survey (NHS) and among 470 patients who were recruited in a randomised controlled trial of a health intervention about in-patient detoxification from alcohol, heroin or cocaine in Boston. The method is also readily applicable to cluster more general mixed-feature data via the framework of consensus clustering.","Mixture model, Mixed feature, Cluster analysis, Comorbidity, Generalised Bernoulli distribution",Shu-Kay Ng and Richard Tawiah and Geoffrey J. McLachlan,https://www.sciencedirect.com/science/article/pii/S0031320318304084,https://doi.org/10.1016/j.patcog.2018.11.022,0031-3203,2019,261--271,88,Pattern Recognition,Unsupervised pattern recognition of mixed data structures with numerical and categorical features using a mixture regression modelling framework,article,NG2019261,
"Standard active learning assumes that human annotations are always obtainable whenever new samples are selected. This, however, is unrealistic in many real-world applications where human experts are not readily available at all times. In this paper, we consider the single shot setting: all the required samples should be chosen in a single shot and no human annotation can be exploited during the selection process. We propose a new method, Active Learning through Random Labeling (ALRL), which substitutes single human annotator for multiple, what we will refer to as, pseudo annotators. These pseudo annotators always provide uniform and random labels whenever new unlabeled samples are queried. This random labeling enables standard active learning algorithms to also exhibit the exploratory behavior needed for single shot active learning. The exploratory behavior is further enhanced by selecting the most representative sample via minimizing nearest neighbor distance between unlabeled samples and queried samples. Experiments on real-world datasets demonstrate that the proposed method outperforms several state-of-the-art approaches.","Active learning, Pseudo annotators, Random labeling, Single shot, Exploration and exploitation, Minimizing nearest neighbor distance",Yazhou Yang and Marco Loog,https://www.sciencedirect.com/science/article/pii/S0031320318304485,https://doi.org/10.1016/j.patcog.2018.12.027,0031-3203,2019,22--31,89,Pattern Recognition,Single shot active learning using pseudo annotators,article,YANG201922,
"The community has been going deeper and deeper in designing one cutting edge network after another, yet some works are there suggesting that we may have gone too far in this dimension. Some researchers unravelled a residual network into an exponentially wider one, and assorted the success of residual networks to fusing a large amount of relatively shallow models. Since some of their early claims are still not settled, we in this paper dig more on this topic, i.e., the unravelled view of residual networks. Based on that, we try to find a good compromise between the depth and width. Afterwards, we walk through a typical pipeline of developing a deep-learning-based algorithm. We start from a group of relatively shallow networks, which perform as well or even better than the current (much deeper) state-of-the-art models on the ImageNet classification dataset. Then, we initialize fully convolutional networks (FCNs) using our pre-trained models, and tune them for semantic image segmentation. Results show that the proposed networks, as pre-trained features, can boost existing methods a lot. Even without exhausting the sophistical techniques to improve the classic FCN model, we achieve comparable results with the best performers on four widely-used datasets, i.e., Cityscapes, PASCAL VOC, ADE20k and PASCAL-Context. The code and pre-trained models are released for public access11https://github.com/itijyou/ademxapp.","Image classification, Semantic segmentation, Residual network",Zifeng Wu and Chunhua Shen and Anton {van den Hengel},https://www.sciencedirect.com/science/article/pii/S0031320319300135,https://doi.org/10.1016/j.patcog.2019.01.006,0031-3203,2019,119--133,90,Pattern Recognition,Wider or Deeper: Revisiting the ResNet Model for Visual Recognition,article,WU2019119,
"Land-use classification in very high spatial resolution images is critical in the remote sensing field. Consequently, remarkable efforts have been conducted towards developing increasingly accurate approaches for this task. In recent years, deep learning has emerged as a dominant paradigm for machine learning, and methodologies based on deep convolutional neural networks have received particular attention from the remote sensing community. These methods typically utilize transfer learning and/or data augmentation to accommodate a small number of labeled images in the publicly available datasets in this field. However, they typically require powerful computers and/or a long time for training. In this work, we propose a simple and novel method for land-use classification in very high spatial resolution images, which efficiently combines transfer learning with a sparse representation. Specifically, the proposed method performs the classification of land-use scenes using a modified version of the well-known sparse representation-based classification method. While this method directly uses the training images to form dictionaries, which are employed to classify test images, our method utilizes a pre-trained deep convolutional neural network and the Gaussian mixture model to generate more robust and compact âdictionaries of deep features.â The effectiveness of the proposed method was evaluated on two publicly available datasets: UC Merced and Brazilian CerradoâSavana. The experimental results suggest that our method can potentially outperform state-of-the-art techniques for land-use classification in very high spatial resolution images.","Deep learning, Dictionary learning, Feature learning, Land-use classification, Sparse representation",Eliezer Flores and Maciel Zortea and Jacob Scharcanski,https://www.sciencedirect.com/science/article/pii/S0031320318304400,https://doi.org/10.1016/j.patcog.2018.12.019,0031-3203,2019,32--44,89,Pattern Recognition,Dictionaries of deep features for land-use scene classification of very high spatial resolution images,article,FLORES201932,
"Person search in real-world scenarios is a new challenging computer version task with many meaningful applications. The challenge of this task mainly comes from: (1) unavailable bounding boxes for pedestrians and the model needs to search for the person over the whole gallery images; (2) huge variance of visual appearance of a particular person owing to varying poses, lighting conditions, and occlusions. To address these two critical issues in modern person search applications, we propose a novel Individual Aggregation Network (IAN) that can accurately localize persons by learning to minimize intra-person feature variations. IAN is built upon the state-of-the-art object detection framework, i.e., faster R-CNN, so that high-quality region proposals for pedestrians can be produced in an online manner. In addition, to relieve the negative effect caused by varying visual appearances of the same individual, IAN introduces a novel center loss that can increase the intra-class compactness of feature representations. The engaged center loss encourages persons with the same identity to have similar feature characteristics. Extensive experimental results on two benchmarks, i.e., CUHK-SYSU and PRW, well demonstrate the superiority of the proposed model. In particular, IAN achieves 77.23% mAP and 80.45% top-1 accuracy on CUHK-SYSU, which outperform the state-of-the-art by 1.7% and 1.85%, respectively.","person search, re-identification, pedestrian detection, softmax loss, center loss, dropout",Jimin Xiao and Yanchun Xie and Tammam Tillo and Kaizhu Huang and Yunchao Wei and Jiashi Feng,https://www.sciencedirect.com/science/article/pii/S0031320318303790,https://doi.org/10.1016/j.patcog.2018.10.028,0031-3203,2019,332--340,87,Pattern Recognition,IAN: The Individual Aggregation Network for Person Search,article,XIAO2019332,
"In recommender systems, the classical matrix factorization model for collaborative filtering only considers joint interactions between users and items. In contrast, context-aware recommender systems (CARS) use contexts to improve recommendation performance. Some early CARS models treat user, item and context equally, unable to capture contextual impact accurately. More recent models perform context operations on users and items separately, leading to âdouble-countingâ of contextual information. This paper proposes a new model, Joint Interaction with Context Operation (JICO), to integrate the joint interaction model with the context operation model, via two layers. The joint interaction layer models interactions between users and items via an interaction tensor. The context operation layer captures contextual information via a contextual operating tensor. We evaluate JICO on four datasets and conduct novel studies, including varying contextual influence and time split recommendation. JICO consistently outperforms competing methods, while providing many useful insights to assist further analysis.","Recommender system, Collaborative filtering, Matrix factorization, Context aware, Joint interaction, Tensor",Peizhen Bai and Yan Ge and Fangling Liu and Haiping Lu,https://www.sciencedirect.com/science/article/pii/S0031320318304242,https://doi.org/10.1016/j.patcog.2018.12.003,0031-3203,2019,729--738,88,Pattern Recognition,Joint interaction with context operation for collaborative filtering,article,BAI2019729,
"This paper tackles the problem of people re-identification by using soft biometrics features. The method works on RGB-D data (color point clouds) to determine the best matching among a database of possible users. For each subject under testing, skeletal information in three-dimensions is used to regularize the pose and to create a skeleton standard posture (SSP). A partition grid, whose sizes depend on the SSP, groups the samples of the point cloud accordingly to their position. Every group is then studied to build the person signature. The same grid is then used for the other subjects of the database to preserve information about possible shape differences among users. The effectiveness of this novel method has been tested on three public datasets. Numerical experiments demonstrate an improvement of results with reference to the current state-of-the-art, with recognition rates of 97.84% (on a partition of BIWI RGBD-ID), 61.97% (KinectREID) and 89.71% (RGBD-ID), respectively.","People re-identification, Color-based descriptor, Skeleton standard posture, Partition grid, RGB-D sensor, Color point cloud",Cosimo Patruno and Roberto Marani and Grazia Cicirelli and Ettore Stella and Tiziana D'Orazio,https://www.sciencedirect.com/science/article/pii/S0031320319300093,https://doi.org/10.1016/j.patcog.2019.01.003,0031-3203,2019,77--90,89,Pattern Recognition,People re-identification using skeleton standard posture and color descriptors from RGB-D data,article,PATRUNO201977,
"Person Re-Identification (Re-Id) is a challenging task focusing on identifying the same person among disjoint camera views. A number of deep learning algorithms have been reported for this task in fully-supervised fashion which requires a large amount of labeled training data, while obtaining high quality labels for Re-Id is extremely time consuming. To address this problem, we propose a semi-supervised Re-Id framework by using only a small portion of labeled data and some additional unlabeled samples. This paper approaches the problem by constructing a set of heterogeneous Convolutional Neural Networks (CNNs) fine-tuned using the labeled portion, and then propagating the labels to the unlabeled portion for further fine-tuning the overall system. In this work, label estimation is a key component during the propagation process. We propose a novel multi-view clustering method, which integrates features of multiple heterogeneous CNNs to cluster and generate pseudo labels for unlabeled samples. Then we fine-tune each of the multiple heterogeneous CNNs by minimizing an identification loss and a verification loss simultaneously, using training data with both true labels and pseudo labels. The procedure is iterated until the estimation of pseudo labels no longer changes. Extensive experiments on three large-scale person Re-Id datasets demonstrate the effectiveness of the proposed method.","Person re-identification, Semi-supervised learning, Convolutional neural network, Multi-view clustering",Xiaomeng Xin and Jinjun Wang and Ruji Xie and Sanping Zhou and Wenli Huang and Nanning Zheng,https://www.sciencedirect.com/science/article/pii/S0031320318304126,https://doi.org/10.1016/j.patcog.2018.11.025,0031-3203,2019,285--297,88,Pattern Recognition,Semi-supervised person re-identification using multi-view clustering,article,XIN2019285,
"The autoencoder is an artificial neural network that performs nonlinear dimension reduction and learns hidden representations of unlabeled data. With a linear transfer function it is similar to the principal component analysis (PCA). While both methods use weight vectors for linear transformations, the autoencoder does not come with any indication similar to the eigenvalues in PCA that are paired with eigenvectors. We propose a novel autoencoder node saliency method that examines whether the features constructed by autoencoders exhibit properties related to known class labels. The supervised node saliency ranks the nodes based on their capability of performing a learning task. It is coupled with the normalized entropy difference (NED). We establish a property for NED values to verify classifying behaviors among the top ranked nodes. By applying our methods to real datasets, we demonstrate their ability to provide indications on the performing nodes and explain the learned tasks in autoencoders.","Autoencoder, Latent representations, Unsupervised learning, Neural networks, Node selection, Model interpretation",Ya Ju Fan,https://www.sciencedirect.com/science/article/pii/S0031320318304369,https://doi.org/10.1016/j.patcog.2018.12.015,0031-3203,2019,643--653,88,Pattern Recognition,Autoencoder node saliency: Selecting relevant latent representations,article,FAN2019643,
"Tensor analysis methods have played an important role in identifying human gaits using high dimensional data. However, when view angles change, it becomes more and more difficult to recognize cross-view gait by learning only a set of multi-linear projection matrices. To address this problem, a general tensor representation framework for cross-view gait recognition is proposed in this paper. There are three criteria of tensorial coupled mappings in the proposed framework. (1) Coupled multi-linear locality-preserved criterion (CMLP) aims to detect the essential tensorial manifold structure via preserving local information. (2) Coupled multi-linear marginal fisher criterion (CMMF) aims to encode the intra-class compactness and inter-class separability with local relationships. (3) Coupled multi-linear discriminant analysis criterion (CMDA) aims to minimize the intra-class scatter and maximize the inter-class scatter. For the three tensor algorithms for cross-view gaits, two sets of multi-linear projection matrices are iteratively learned using alternating projection optimization procedures. The proposed methods are compared with the recently published cross-view gait recognition approaches on CASIA(B) and OU-ISIR gait database. The results demonstrate that the performances of the proposed methods are superior to existing state-of-the-art cross-view gait recognition approaches.","Gait recognition, Cross-view gait, Tensor representation, Framework",Xianye Ben and Peng Zhang and Zhihui Lai and Rui Yan and Xinliang Zhai and Weixiao Meng,https://www.sciencedirect.com/science/article/pii/S0031320319300251,https://doi.org/10.1016/j.patcog.2019.01.017,0031-3203,2019,87--98,90,Pattern Recognition,A general tensor representation framework for cross-view gait recognition,article,BEN201987,
"There are two types of information in each handwritten word image: explicit information which can be easily read or derived directly, such as lexical content or word length, and implicit attributes such as the authorâs identity. Whether features learned by a neural network for one task can be used for another task remains an open question. In this paper, we present a deep adaptive learning method for writer identification based on single-word images using multi-task learning. An auxiliary task is added to the training process to enforce the emergence of reusable features. Our proposed method transfers the benefits of the learned features of a convolutional neural network from an auxiliary task such as explicit content recognition to the main task of writer identification in a single procedure. Specifically, we propose a new adaptive convolutional layer to exploit the learned deep features. A multi-task neural network with one or several adaptive convolutional layers is trained end-to-end, to exploit robust generic features for a specific main task, i.e., writer identification. Three auxiliary tasks, corresponding to three explicit attributes of handwritten word images (lexical content, word length and character attributes), are evaluated. Experimental results on two benchmark datasets show that the proposed deep adaptive learning method can improve the performance of writer identification based on single-word images, compared to non-adaptive and simple linear-adaptive approaches.","Writer identification, Deep adaptive learning, Handwritten word attributes, Multi-task learning",Sheng He and Lambert Schomaker,https://www.sciencedirect.com/science/article/pii/S0031320318303832,https://doi.org/10.1016/j.patcog.2018.11.003,0031-3203,2019,64--74,88,Pattern Recognition,Deep adaptive learning for writer identification based on single handwritten word images,article,HE201964,
"Datasets are often collected from different resources or comprised of multiple representations (i.e., views). Multi-view clustering aims to analyze the multi-view data in an unsupervised way. Owing to the efficiency of uncovering the hidden structures of data, graph-based approaches have been investigated widely for various multi-view learning tasks. However, similarity measurement in these methods is challenging since the construction of similarity graph is impacted by several factors such as the scale of data, neighborhood size, choice of similarity metric, noise and outliers. Moreover, nonlinear relationships usually exist in real-world datasets, which have not been considered by most existing methods. In order to address these challenges, a novel model which simultaneously performs multi-view clustering task and learns similarity relationships in kernel spaces is proposed in this paper. The target optimal graph can be directly partitioned into exact c connected components if there are c clusters. Furthermore, our model can assign ideal weight for each view automatically without additional parameters as previous methods do. Since the performance is often sensitive to the input kernel matrix, the proposed model is further extended with multiple kernel learning ability. With the proposed joint model, three subtasks including construct the most accurate similarity graph, automatically allocate optimal weight for each view and find the cluster indicator matrix can be simultaneously accomplished. By this joint learning, each subtask can be mutually enhanced. Experimental results on benchmark datasets demonstrate that our model outperforms other state-of-the-art multi-view clustering algorithms.","Graph learning, Multi-view clustering, Multiple kernel learning, Auto-weighted strategy",Shudong Huang and Zhao Kang and Ivor W. Tsang and Zenglin Xu,https://www.sciencedirect.com/science/article/pii/S0031320318303959,https://doi.org/10.1016/j.patcog.2018.11.007,0031-3203,2019,174--184,88,Pattern Recognition,Auto-weighted multi-view clustering via kernelized graph learning,article,HUANG2019174,
"The recognition of handwritten text is challenging as there are virtually infinite ways a human can write the same message. Deep learning approaches for handwriting analysis have recently demonstrated breakthrough performance using both lexicon-based architectures and recurrent neural networks. This paper presents a fully convolutional network architecture which outputs arbitrary length symbol streams from handwritten text. A preprocessing step normalizes input blocks to a canonical representation which negates the need for costly recurrent symbol alignment correction. When a lexicon is known, we further introduce a probabilistic character error rate to correct errant word blocks. Our multi-state convolutional method is the first to demonstrate state-of-the-art results on both lexicon-based and arbitrary symbol based handwriting recognition benchmarks.","Handwriting recognition, Fully convolutional neural networks, Deep learning",Raymond Ptucha and Felipe {Petroski Such} and Suhas Pillai and Frank Brockler and Vatsala Singh and Paul Hutkowski,https://www.sciencedirect.com/science/article/pii/S0031320318304370,https://doi.org/10.1016/j.patcog.2018.12.017,0031-3203,2019,604--613,88,Pattern Recognition,Intelligent character recognition using fully convolutional neural networks,article,PTUCHA2019604,
"In network embedding, random walks play a fundamental role in preserving network structures. However, random walk methods have two limitations. First, they are unstable when either the sampling frequency or the number of node sequences changes. Second, in highly biased networks, random walks are likely to bias to high-degree nodes and neglect the global structure information. To solve the limitations, we present in this paper a network diffusion embedding method. To solve the first limitation, our method uses a diffusion driven process to capture both depth and breadth information in networks. Temporal information is also included into node sequences to strengthen information preserving. To solve the second limitation, our method uses the network inference method based on information diffusion cascades to capture the global network information. Experiments show that the new proposed method is more robust to highly unbalanced networks and well performed when sampling under each node is rare.","Network embedding, Cascades, Diffusion process, Network inference, Dimension reduction",Yong Shi and Minglong Lei and Hong Yang and Lingfeng Niu,https://www.sciencedirect.com/science/article/pii/S0031320318304254,https://doi.org/10.1016/j.patcog.2018.12.004,0031-3203,2019,518--531,88,Pattern Recognition,Diffusion network embedding,article,SHI2019518,
"This paper proposes a new global and local weighted signed pressure force (SPF) based active contour model (ACM) to segment various types of images. First, by introducing the normalized global minimum absolute differences as the coefficients of global inner and outer region fitting centers, a new global weighted SPF (GWSPF) is defined, which makes the best of the difference information of inner and outer regions and improves segmentation performance. Second, by introducing the normalized local minimum absolute differences as the coefficients of local inner and outer region fitting centers similarly, a new local weighted SPF (LWSPF) is defined and added to the above global weighted SPF. Third, the global and local within-class variances of the image are used to weight the GWSPF and the LWSPF, which can automatically adjust the effect degrees of the GWSPF and the LWSPF. Experiments on many kinds of real-world images have validated that the proposed model is superior to popular ACMs in segmentation accuracy, in addition, it is robust to the initial curve.","Active contour, GWSPF, LWSPF, Global and local within-class variances",Bin Han and Yiquan Wu,https://www.sciencedirect.com/science/article/pii/S0031320318304497,https://doi.org/10.1016/j.patcog.2018.12.028,0031-3203,2019,715--728,88,Pattern Recognition,Active contours driven by global and local weighted signed pressure force for image segmentation,article,HAN2019715,
"Automatic image analysis is a crucial component of many intelligent systems designed for high-level understanding of documents. Most document image understanding systems are usually based on applying pattern recognition techniques to conventional three channel RGB images. Airborne and satellite based macro scale Hyperspectral Imaging (HSI) systems are well established for geosciences. Recently, owing to advancements in imaging speed and reduced camera costs, micro scale HSI systems are also gaining importance in ground based applications such as hyperspectral document image analysis. HSI is non destructive and offers new opportunities via measuring richer information along spectral dimension by imaging the document in contiguous bands across the electromagnetic spectrum. Hyperspectral document imaging has shown potential for solving many challenging problems of document image analysis including signature extraction, ink or document aging, information retrieval from historical document images, paintings and forensic analysis of documents. In this paper, we explore the potential of HSI for document image analysis and present a comprehensive review of the literature and future prospects. We highlight and discuss the challenges involved in the acquisition and processing of hyperspectral document images. A review of commercial HSI systems for document image analysis is also presented.","Hyperspectral document imaging, Signature extraction, Historical document image analysis, Cultural heritage, Ink mismatch detection",Rizwan Qureshi and Muhammad Uzair and Khurram Khurshid and Hong Yan,https://www.sciencedirect.com/science/article/pii/S0031320319300366,https://doi.org/10.1016/j.patcog.2019.01.026,0031-3203,2019,12--22,90,Pattern Recognition,"Hyperspectral document image processing: Applications, challenges and future prospects",article,QURESHI201912,
"Voice analysis provides a non-invasive way for disease detection, in which most methods only consider a single audio, although different audios contain complementary information and a fusion of them is beneficial. In this paper, a novel model JOLL4R (JOint Learning based on Label Relaxed low-Rank Ridge Regression) is proposed to fuse audios for voice based disease detection. First, the model couples the regression losses from two audios together to jointly learn a transformation matrix for each audio. Secondly, the conventional zero-one regression targets are relaxed by the Ïµ-dragging technique so that the margins between different classes are enlarged. Third, low-rank constraint is imposed to exploit the correlation structure among different classes. The proposed algorithm not only enables to consider multiple audios, but also adjusts the weight of each audio adaptively. Due to the design of losses coupling, Ïµ-dragging technique, and low rank constraint, high performance is achieved. Experiments conducted on two disease detection tasks, each with six types of fusion, show that our fusion approach outperforms the case of using a single audio and another two fusion methods. Finally, key factors in JOLL4R are analyzed.","Joint learning, Ridge regression, Low-rank regression, Ïµ-dragging technique, Voice based pathology detection",Kebin Wu and David Zhang and Guangming Lu and Zhenhua Guo,https://www.sciencedirect.com/science/article/pii/S003132031830339X,https://doi.org/10.1016/j.patcog.2018.09.013,0031-3203,2019,130--139,87,Pattern Recognition,Joint learning for voice based disease detection,article,WU2019130,
"Many batch learning algorithms have been introduced for offline multi-label classification (MLC) over the years. However, the increasing data volume in many applications such as social networks, sensor networks, and traffic monitoring has posed many challenges to batch MLC learning. For example, it is often expensive to re-train the model with the newly arrived samples, or it is impractical to learn on the large volume of data at once. The research on incremental learning is therefore applicable to a large volume of data and especially for data stream. In this study, we develop a Bayesian-based method for learning from multi-label data streams by taking into consideration the correlation between pairs of labels and the relationship between label and feature. In our model, not only the label correlation is learned with each arrived sample with ground truth labels but also the number of predicted labels are adjusted based on Hoeffding inequality and the label cardinality. We also extend the model to handle missing values, a problem common in many real-world data. To handle concept drift, we propose a decay mechanism focusing on the age of the arrived samples to incrementally adapt to the change of data. The experimental results show that our method is highly competitive compared to several well-known benchmark algorithms under both the stationary and concept drift settings.","Multi-label classification, Multi-label learning, Online learning, Data stream, Concept drift, Label correlation, Feature dependence",Tien Thanh Nguyen and Thi Thu Thuy Nguyen and Anh Vu Luong and Quoc Viet Hung Nguyen and Alan Wee-Chung Liew and Bela Stantic,https://www.sciencedirect.com/science/article/pii/S0031320319300123,https://doi.org/10.1016/j.patcog.2019.01.007,0031-3203,2019,35--51,90,Pattern Recognition,Multi-label classification via label correlation and first order feature dependance in a data stream,article,NGUYEN201935,
"Recently, generative adversarial networks (GANs) have demonstrated high-quality reconstruction in face completion. There is still much room for improvement over the conventional GAN models that do not explicitly address the texture details problem. In this paper, we propose a Laplacian-pyramid-based generative framework for face completion. This framework can produce more realistic results (1) by deriving precise content information of missing face regions in a coarse-to-fine fashion and (2) by propagating the high-frequency details from the surrounding area via a modified residual learning model. Specifically, for the missing regions, we design a Laplacian-pyramid-based convolutional network framework that can predict missing regions under different resolutions; this framework takes advantage of multiscale features shared from low levels and extracted from middle layers for the next finer level. For high-frequency details, we construct a new residual learning network to eliminate color discrepancies between the missing and surrounding regions progressively. Furthermore, a multiloss function is proposed to supervise the generative process. To optimize the model, we train the entire generative model with deep supervision using a joint reconstruction loss, which ensures that the generated image is as realistic as the original. Extensive experiments on benchmark datasets show that the proposed framework exhibits superior performance over state-of-the-art methods in terms of predictive accuracy, both quantitatively and qualitatively.","Face completion, Generative adversarial network, Laplacian pyramid",Qiang Wang and Huijie Fan and Gan Sun and Yang Cong and Yandong Tang,https://www.sciencedirect.com/science/article/pii/S0031320318304096,https://doi.org/10.1016/j.patcog.2018.11.020,0031-3203,2019,493--505,88,Pattern Recognition,Laplacian pyramid adversarial network for face completion,article,WANG2019493,
"We tackle the problem of predicting a grasping action in ego-centric video for the assistance to upper-limb amputees. Our work is based on paradigms of neuroscience that state that human gaze expresses intention and anticipates actions. In our scenario, human gaze fixations are recorded by a glass-worn eye-tracker and then used to predict the grasping actions. We have studied two aspects of the problem: which object from a given taxonomy will be grasped, and when is the moment to trigger the grasping action. To recognize objects, we using gaze to guide Convolutional Neural Networks (CNN) to focus on an object-to-grasp area. However, the acquired sequence of fixations is noisy due to saccades toward distractors and visual fatigue, and gaze is not always reliably directed toward the object-of-interest. To deal with this challenge, we use video-level annotations indicating the object to be grasped and a weak loss in Deep CNNs. To detect a moment when a person will take an object we take advantage of the predictive power of Long-Short Term Memory networks to analyze gaze and visual dynamics. Results show that our method achieves better performance than other approaches on a real-life dataset.","Human perception, Grasping action prediction, Weakly supervised active object detection",IvÃ¡n GonzÃ¡lez-DÃ­az and Jenny Benois-Pineau and Jean-Philippe Domenger and Daniel Cattaert and Aymar {de Rugy},https://www.sciencedirect.com/science/article/pii/S0031320318304011,https://doi.org/10.1016/j.patcog.2018.11.013,0031-3203,2019,223--235,88,Pattern Recognition,Perceptually-guided deep neural networks for ego-action prediction: Object grasping,article,GONZALEZDIAZ2019223,
"In multi-label data, each instance is associated with a set of labels, instead of one label. Similar to single-label data, feature selection plays an important role in improving classification performance. In multi-label classification, each class label might be specified by some particular characteristics of its own which are called label-specific features. In this paper, a fast accurate filter-based feature selection method is exclusively designed for multi-label datasets to find label-specific features. It maps the features to a multi-dimensional space based on a filter method, and selects the most salient features with the help of Pareto-dominance concepts from multi-objective optimization domain. Our proposed method can be used as online feature selection that deals with problems in which features arrive sequentially while the number of data samples is fixed. In this method, the number of features to be selected is specified during the process of feature selection. However, sometimes it is desired to predefine the number of features. For this reason, an extension of the proposed method is presented to solve this problem. To prove the performance of the proposed methods, several experiments are conducted on some multi-label datasets and the results are compared to five well-established multi-label feature selection methods. The results show the superiority of the proposed methods in terms of different multi-label classification criteria and execution time.","Multi-label dataset, Feature selection, Label-specific features, Pareto dominance, Online feature selection",Shima Kashef and Hossein Nezamabadi-pour,https://www.sciencedirect.com/science/article/pii/S0031320318304412,https://doi.org/10.1016/j.patcog.2018.12.020,0031-3203,2019,654--667,88,Pattern Recognition,A label-specific multi-label feature selection algorithm based on the Pareto dominance concept,article,KASHEF2019654,
"By exploiting the low-dimensional structure of high-dimensional data, sparse representation based classifiers (SRC) has recently attracted massive attention in pattern recognition. In this paper, we study a natural generalization of SRC, i.e., block sparse representation based classifiers (BSRC), which takes into account the block structure of the dictionary. Our contributions are two-fold: (1) we provide theoretical guarantees for BSRC and theoretically show that BSRC performs perfect classification for any test sample under both cases of independent subspaces and arbitrary subspaces settings; (2) we extend BSRC and propose three robust BSRC methods based on M-estimators originating in robust statistics. This is motivated by the observation that many previous representation based classifiers utilize the mean square error (MSE) criterion as the loss function, which is sensitive to outliers and complicated noises in reality. In contrast, M-estimators has shown much stronger robustness than MSE against gross corruptions. We demonstrate the efficacy of the proposed methods through experiments on both synthetic and real-world databases for block sparse recovery, handwritten digit recognition and robust face recognition.","Representation based classifier, Block sparsity, Subspace, M-estimator",Yulong Wang and Yuan Yan Tang and Luoqing Li and Xianwei Zheng,https://www.sciencedirect.com/science/article/pii/S0031320318304138,https://doi.org/10.1016/j.patcog.2018.11.026,0031-3203,2019,198--209,88,Pattern Recognition,"Block sparse representation for pattern classification: Theory, extensions and applications",article,WANG2019198,
"Radiomics is a medical imaging technique that aims at extracting a large amount of features from one or several modalities of medical images, in order to help diagnose and treat diseases like cancers. Many recent studies have shown that Radiomics features can offer a lot of useful information that physicians cannot extract from these images, and can be efficiently associated with other information like gene or protein data. However, most of the classification studies in Radiomics report the use of feature selection methods without identifying the underlying machine learning challenges. In this paper, we first show that the Radiomics classification problem should be viewed as a high dimensional, low sample size, multi-view learning problem. Then, we propose a dissimilarity-based method for merging the information from the different views, based on Random Forest classifiers. The proposed approach is compared to different state-of-the-art Radiomics and multi-view solutions, on different public multi-view datasets as well as on Radiomics datasets. In particular, our experiments show that the proposed approach works better than the state-of-the-art methods from the Radiomics, as well as from the multi-view learning literature.","Radiomics, Dissimilarity space, Random forest, Machine learning, Feature selection, Multi-view learning, High dimension, Low sample size",Hongliu Cao and Simon Bernard and Robert Sabourin and Laurent Heutte,https://www.sciencedirect.com/science/article/pii/S003132031830400X,https://doi.org/10.1016/j.patcog.2018.11.011,0031-3203,2019,185--197,88,Pattern Recognition,Random forest dissimilarity based multi-view learning for Radiomics application,article,CAO2019185,
"Single sample per person face recognition is one of the most challenging problems in face recognition (FR), where only single sample per person (SSPP) is enrolled in the gallery set for training. Although the existing patch-based methods have achieved great success in FR with SSPP, they still have limitations in feature extraction and identification stages when handling complex facial variations. In this work, we propose a new patch-based method called Robust Heterogeneous Discriminative Analysis (RHDA), for FR with SSPP. To enhance the robustness against complex facial variations, we first present a new graph-based Fisher-like criterion, which incorporates two manifold embeddings, to learn heterogeneous discriminative representations of image patches. Specifically, for each patch, the Fisher-like criterion is able to preserve the reconstruction relationship of neighboring patches from the same person, while suppressing the similarities between neighboring patches from the different persons. Then, we introduce two distance metrics, i.e., patch-to-patch distance and patch-to-manifold distance, and develop a fusion strategy to combine the recognition outputs of above two distance metrics via a joint majority voting for identification. Experimental results on various benchmark datasets demonstrate the effectiveness of the proposed method.","Face recognition, Single sample per person, Heterogeneous representation, Fisher-like criterion, Joint majority voting",Meng Pang and Yiu-ming Cheung and Binghui Wang and Risheng Liu,https://www.sciencedirect.com/science/article/pii/S0031320319300111,https://doi.org/10.1016/j.patcog.2019.01.005,0031-3203,2019,91--107,89,Pattern Recognition,Robust heterogeneous discriminative analysis for face recognition with single sample per person,article,PANG201991,
"Orthogonal moments and their invariants to similarity transformations for monochrome and gray-scale images are widely used in many pattern recognition and image processing applications. Quaternion orthogonal moments are used with color images. Recently, the multi-channel framework is proposed as a successful alternative of the quaternion orthogonal moments in representation and recognition of the color images. In this paper, a new set of multi-channel orthogonal moments and their invariants to rotation, scaling and translation (RST) is proposed for color image representation and recognition. The proposed multi-channel moments are based on the orthogonal radial substituted Chebyshev functions. The multi-channel orthogonal radial substituted Chebyshev moments (MORSCMs) are defined in polar coordinates over a unit circle. An accurate kernel-based method is utilized for accurate computation of the MORSCMs. A series of experiments is performed to validate this new set of multi-channel moments and compare its performance with the existing quaternion and multi-channel orthogonal moments. The obtained results ensure the superiority of the proposed MORSCMs over all existing moments in representation and recognition of the color images.","Multi-channel orthogonal moments, Quaternion orthogonal moments, Chebyshev rational moments, RST, Color image reconstruction, Recognition rates",Khalid M. Hosny and Mohamed M. Darwish,https://www.sciencedirect.com/science/article/pii/S0031320318304023,https://doi.org/10.1016/j.patcog.2018.11.014,0031-3203,2019,153--173,88,Pattern Recognition,New set of multi-channel orthogonal moments for color image representation and recognition,article,HOSNY2019153,
"Deep kernel learning aims at designing nonlinear combinations of multiple standard elementary kernels by training deep networks. This scheme has proven to be effective, but intractable when handling large-scale datasets especially when the depth of the trained networks increases; indeed, the complexity of evaluating these networks scales quadratically w.r.t. the size of training data and linearly w.r.t. the depth of the trained networks. In this paper, we address the issue of efficient computation in Deep Kernel Networks (DKNs) by designing effective maps in the underlying Reproducing Kernel Hilbert Spaces (RKHS). Given a pretrained DKN, our method builds its associated Deep Map Network (DMN) whose inner product approximates the original network while being far more efficient. The design principle of our method is greedy and achieved layer-wise, by finding maps that approximate DKNs at different (input, intermediate and output) layers. This design also considers an extra fine-tuning step based on unsupervised learning, that further enhances the generalization ability of the trained DMNs. When plugged into SVMs, these DMNs turn out to be as accurate as the underlying DKNs while being at least an order of magnitude faster on large-scale datasets, as shown through extensive experiments on the challenging ImageCLEF, COREL5k benchmarks and the Banana dataset.","Multiple kernel learning, Kernel design, Deep networks, Efficient computation, Image annotation",Mingyuan Jiu and Hichem Sahbi,https://www.sciencedirect.com/science/article/pii/S0031320318304266,https://doi.org/10.1016/j.patcog.2018.12.005,0031-3203,2019,447--457,88,Pattern Recognition,Deep representation design from deep kernel networks,article,JIU2019447,
"Mood disorders, including unipolar depression (UD) and bipolar disorder (BD), have become some of the commonest mental health disorders. The absence of diagnostic markers of BD can cause misdiagnosis of the disorder as UD on initial presentation. Short-term detection, which could be used in early detection and intervention, is desirable. This study proposed an approach for short-term detection of mood disorders based on elicited speech responses. Speech responses of participants were obtained through interviews by a clinician after participants viewed six emotion-eliciting videos. A domain adaptation method based on a hierarchical spectral clustering algorithm was proposed to adapt a labeled emotion database into a collected unlabeled mood database for alleviating the data bias problem in an emotion space. For modeling the local variation of emotions in each response, a convolutional neural network (CNN) with an attention mechanism was used to generate an emotion profile (EP) of each elicited speech response. Finally, long short-term memory (LSTM) was employed to characterize the temporal evolution of EPs of all six speech responses. Moreover, an attention model was applied to the LSTM network for highlighting pertinent speech responses to improve detection performance instead of treating all responses equally. For evaluation, this study elicited emotional speech data from 15 people with BD, 15 people with UD, and 15 healthy controls. Leave-one-group-out cross-validation was employed for the compiled database and proposed method. CNN- and LSTM-based attention models improved the mood disorder detection accuracy of the proposed method by approximately 11%. Furthermore, the proposed method achieved an overall detection accuracy of 75.56%, outperforming support-vector-machine- (62.22%) and CNN-based (66.67%) methods.","Mood disorder detection, Convolutional neural network, Long short-term memory, Attention model",Kun-Yi Huang and Chung-Hsien Wu and Ming-Hsiang Su,https://www.sciencedirect.com/science/article/pii/S0031320318304382,https://doi.org/10.1016/j.patcog.2018.12.016,0031-3203,2019,668--678,88,Pattern Recognition,Attention-based convolutional neural network and long short-term memory for short-term detection of mood disorders based on elicited speech responses,article,HUANG2019668,
"In this paper, we introduce a new blind source separation (BSS) method for temporal correlated noncircular sources that uses widely linear filter (WLF) model to efficiently describe the temporal structure. The algorithm consists of a WLF coefficients estimator followed by complex matrix joint diagonalization. In the derivation of the new BSS algorithm, an emerging matrix joint diagonalization issue needs to be addressed. Subsequently, a complex matrix joint diagonalization algorithm based on successive Shear and Givens rotations (SGR) is proposed to deal with the new joint diagonalization problem. Simulations of SGR show that it converges fast and converges to excellent value, and it shows a satisfactory anti-perturbation performance of matrix for the new BSS algorithm. We compare the performance of the new BSS algorithm with several competing algorithms, and the simulation results demonstrate the superior performance of the proposed BSS algorithm for both noncircular complex Gaussian and non-Gaussian sources.","Blind source separation, Widely linear filter, Complex matrix joint diagonalization, Shear and Givens rotations",Jiong Li and Hang Zhang and Pengfei Wang,https://www.sciencedirect.com/science/article/pii/S0031320318303613,https://doi.org/10.1016/j.patcog.2018.10.016,0031-3203,2019,285--295,87,Pattern Recognition,Blind separation of temporally correlated noncircular sources using complex matrix joint diagonalization,article,LI2019285,
"Detecting carried objects is one of the requirements for developing systems to reason about activities involving people and objects. We present an approach to detect carried objects from a single video frame with a novel method that incorporates features from multiple scales. Initially, a foreground mask in a video frame is segmented into multi-scale superpixels. Then the human-like regions in the segmented area are identified by matching a set of extracted features from superpixels against learned features in a codebook. A carried object probability map is generated using the complement of the matching probabilities of superpixels to human-like regions and background information. A group of superpixels with a high carried object probability and a strong edge support is then merged to obtain the shape of the carried object. We applied our method to two challenging datasets, and results show that our method is competitive with or better than the state-of-the-art.","Superpixel, Shape context, Codebook, Carried object",Farnoosh Ghadiri and Robert Bergevin and Guillaume-Alexandre Bilodeau,https://www.sciencedirect.com/science/article/pii/S0031320318304308,https://doi.org/10.1016/j.patcog.2018.12.009,0031-3203,2019,134--150,89,Pattern Recognition,From superpixel to human shape modelling for carried object detection,article,GHADIRI2019134,
"Street-level scene segmentation aims to label each pixel of street-view images into specific semantic categories. It has been attracting growing interest due to various real-world applications, especially in the area of autonomous driving. However, this pixel-wise labeling task is very challenging under the complex street-level scenes and large-scale object categories. Motivated by the scene layout of street-view images, in this work we propose a novel Spatial Gated Attention (SGA) module, which automatically highlights the attentive regions for pixel-wise labeling, resulting in effective street-level scene segmentation. The proposed module takes as input the multi-scale feature maps based on a Fully Convolutional Network (FCN) backbone, and produces the corresponding attention mask for each feature map. The learned attention masks can neatly highlight the regions of interest while suppress background clutter. Furthermore, we propose an efficient multi-scale feature interaction mechanism which is able to adaptively aggregate the hierarchical features. Based on the proposed mechanism, the features of different levels are adaptively re-weighted according to the local spatial structure and the surrounding contextual information. Consequently, the proposed modules are able to boost standard FCN architectures and result in an enhanced pixel-wise segmentation for street-level scene images. Extensive experiments on three public available street-level benchmarks demonstrate that the proposed Gated Attention Network (GANet) approach achieves consistently superior performance and outperforms the very recent state-of-the-art methods.","Scene segmentation, Fully convolutional network, Spatial gated attention, Street-level image understanding",Pingping Zhang and Wei Liu and Hongyu Wang and Yinjie Lei and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320318304424,https://doi.org/10.1016/j.patcog.2018.12.021,0031-3203,2019,702--714,88,Pattern Recognition,Deep gated attention networks for large-scale street-level scene segmentation,article,ZHANG2019702,
"In many applications, image deblurring is a pre-requisite to improve the sharpness of an image before it can be further processed. Iterative methods are widely used for deblurring images but care must be taken to ensure that the iterative process is robust, meaning that the process does not diverge and reaches the solution reasonably fast, two goals that sometimes compete against each other. In practice, it remains challenging to choose parameters for the iterative process to be robust. We propose a new approach consisting of relaxed initialization and pixel-wise updates of the step size for iterative methods to achieve robustness. The first novel design of the approach is to modify the initialization of existing iterative methods to stop a noise term from being propagated throughout the iterative process. The second novel design is the introduction of a vectorized step size that is adaptively determined through the iteration to achieve higher stability and accuracy in the whole iterative process. The vectorized step size aims to update each pixel of an image individually, instead of updating all the pixels by the same factor. In this work, we implemented the above designs based on the Landweber method to test and demonstrate the new approach. Test results showed that the new approach can deblur images from noisy observations and achieve a low mean squared error with a more robust performance.","Image deblurring, Image restoration, Iterative algorithms, Relaxed initialization, Noise removal, Landweber method",Taihao Li and Huai Chen and Min Zhang and Shupeng Liu and Shunren Xia and Xinhua Cao and Geoffrey S. Young and Xiaoyin Xu,https://www.sciencedirect.com/science/article/pii/S0031320319300196,https://doi.org/10.1016/j.patcog.2019.01.019,0031-3203,2019,134--146,90,Pattern Recognition,A new design in iterative image deblurring for improved robustness and performance,article,LI2019134,
"Convolutional networks, such as convolutional neural networks, are extensively applied in pattern recognition systems, where the convolution kernel plays an important role in performance improvement. Inspired by this, we integrate the convolution operation into statistical modeling in this paper and develop a novel probabilistic generative model called convolutional factor analysis (CFA), which is more applicable to statistical recognition with limited training data. And then, the CFA model is applied to radar automatic target recognition based on high-resolution range profile (HRRP), where sufficient training data are frequently unavailable due to the sampling rate limitation of real radar systems. As a dictionary learning method, the dimension of each dictionary atom in our CFA model is much lower than that in the traditional factor analysis (FA) model. Meanwhile, the model also makes it possible to capture the basic structures of observed data, thus requiring a fewer dictionary atoms to describe observations. Due to these two properties, the dictionary size of the CFA model is much smaller. Therefore, compared to the traditional FA model, the CFA model has the lower order of model complexity and can be learned better under a small amount of training data. In addition, owing to the conjugate property, the model parameters can be inferred via variational Bayesian (VB) algorithm, and the commutative law of convolution operation is also exploited to simplify the derivations of the posteriors. Experiments on synthetic and measured HRRP data show that basic structures of data can be represented by the dictionaries learned via our CFA model, and the better recognition performance can also be achieved by our method with small training data size.","Convolutional factor analysis (CFA), Dictionary learning, Radar automatic target recognition (RATR), High-resolution range profile (HRRP), Variational Bayesian (VB)",Jian Chen and Lan Du and Hua He and Yuchen Guo,https://www.sciencedirect.com/science/article/pii/S003132031830356X,https://doi.org/10.1016/j.patcog.2018.10.014,0031-3203,2019,140--156,87,Pattern Recognition,Convolutional factor analysis model with application to radar automatic target recognition,article,CHEN2019140,
"In this paper, we address the registration of historical WWII images to present-day ortho-photo maps for the purpose of geolocalization. Due to the challenging nature of this problem, we propose to register the images jointly as a group rather than in a step-by-step manner. To this end, we exploit Hough voting spaces as pairwise registration estimators and show how they can be integrated into a probabilistic groupwise registration framework that can be efficiently optimized. The feature-based nature of our registration framework allows to register images with a-priori unknown translational and rotational relations, and is also able to handle scale changes of up to 30% in our test data due to a final geometrically guided matching step. The superiority of the proposed method over existing pairwise and groupwise registration methods is demonstrated on eight highly challenging sets of historical images with corresponding ortho-photo maps.","Image registration, Geolocalization, Remote sensing, Optimization, Hough voting",Sebastian Zambanini,https://www.sciencedirect.com/science/article/pii/S0031320319300342,https://doi.org/10.1016/j.patcog.2019.01.024,0031-3203,2019,66--77,90,Pattern Recognition,Feature-based groupwise registration of historical aerial images to present-day ortho-photo maps,article,ZAMBANINI201966,
"Multi-view data represented by different features have been involved in many machine learning applications. Efficiently exploiting and preserving the correlative yet complementary information in multiple views remains challenging in multi-view learning. Comparing with existing methods that separately cope with each view, we propose a supervised multi-view feature learning framework to handle diverse views with a unified perception. Specifically, we fuse the multi-view data by mapping the concatenation of original features to a discriminative low-dimensional subspace, where the features from different views are adaptively assigned with the learned optimal weights. This strategy can simultaneously preserve the correlative and the complementary information, which is further enhanced to be more discriminative for subsequent classification. An efficient iterative algorithm is devised to optimize the formulated framework with closed-form solutions. Comprehensive evaluations with several state-of-the-art competitors demonstrate the efficiency and the superiority of the proposed method.","Multi-view learning, Supervised learning, Classification",Muli Yang and Cheng Deng and Feiping Nie,https://www.sciencedirect.com/science/article/pii/S0031320318304035,https://doi.org/10.1016/j.patcog.2018.11.015,0031-3203,2019,236--245,88,Pattern Recognition,Adaptive-weighting discriminative regression for multi-view classification,article,YANG2019236,
"We propose a novel method for measuring the nasal similarity among 3D faces. Firstly, we construct a representation for the nose shape, which is composed of a set of geodesic curves, each crosses the bridge of the nose. Next, using these geodesic curves, we formulate a similarity measure to compare among noses in the curve shape space. Under the Riemannian framework, the shape space is a quotient space for which the scaling, translation and rotation are removed. Since the nose similarity measure is based on the shape comparison, the proposed method has the following advantages: (1) the similarity measure is robust to facial expressions since the nose is not affected by facial expressions; (2) the geometric features of the nose shape match well with the human perception; (3) the similarity measure is independent of the mesh grid because the chosen nose curves are not sensitive to the triangular mesh model. We construct a nasal hierarchical structure for noses organization which is based on nose similarity measure results. In our experiments, we evaluate the performance of the proposed method and compare it with competing methods on three public face databases namely, FRGC2.0, Texas3D and BosphorusDB. The results show superiority of the proposed method in terms of both the speed and the accuracy when the nasal measurements are processed in the nasal hierarchical structure and the nasal samples with low sampling rate (5%-25% of original point cloud).","Shape space, Nose similarity measure, Riemannian manifold",Chenlei Lv and Zhongke Wu and Xingce Wang and Mingquan Zhou and Kar-Ann Toh,https://www.sciencedirect.com/science/article/pii/S0031320318304278,https://doi.org/10.1016/j.patcog.2018.12.006,0031-3203,2019,458--469,88,Pattern Recognition,Nasal similarity measure of 3D faces based on curve shape space,article,LV2019458,
"Multimodal data fusion has shown great advantages in uncovering information that could be overlooked by using single modality. In this paper, we consider the integration of high-dimensional multi-modality imaging and genetic data for Alzheimerâs disease (AD) diagnosis. With a focus on taking advantage of both phenotype and genotype information, a novel structured sparsity, defined by â1, p-norm (pâ¯>â¯1), regularized multiple kernel learning method is designed. Specifically, to facilitate structured feature selection and fusion from heterogeneous modalities and also capture feature-wise importance, we represent each feature with a distinct kernel as a basis, followed by grouping the kernels according to modalities. Then, an optimally combined kernel presentation of multimodal features is learned in a data-driven approach. Contrary to the Group Lasso (i.e., â2, 1-norm penalty) which performs sparse group selection, the proposed regularizer enforced on kernel weights is to sparselyselect concise feature set within each homogenous group and fuse the heterogeneous feature groups by taking advantage of dense norms. We have evaluated our method using data of subjects from Alzheimerâs Disease Neuroimaging Initiative (ADNI) database. The effectiveness of the method is demonstrated by the clearly improved prediction diagnosis and also the discovered brain regions and SNPs relevant to AD.","Structured sparsity, Multimodal features, Multiple kernel learning, Feature selection, Alzheimerâs disease diagnosis",Jialin Peng and Xiaofeng Zhu and Ye Wang and Le An and Dinggang Shen,https://www.sciencedirect.com/science/article/pii/S0031320318304151,https://doi.org/10.1016/j.patcog.2018.11.027,0031-3203,2019,370--382,88,Pattern Recognition,Structured sparsity regularized multiple kernel learning for Alzheimerâs disease diagnosis,article,PENG2019370,
"Multi-task learning (MTL) aims to enhance generalization performance by exploring the inherent structures across tasks. Most existing MTL methods are based on the assumption that the tasks are positively correlated, and utilize the shared structures among tasks to improve learning performance. By contrast, there also exist competitive structure (negative relationships) among tasks in some real-world applications, and conventional MTL methods which explore shared structures across tasks may lead to unsatisfactory performance in this setting. Another challenge, especially in a high dimensional setting, is to exclude irrelevant features (sparse structure) from the final model. For this purpose, this work propose a new method, which is referred to as Sparse Exclusive Lasso (SpEL) for multi-task learning. The proposed SpEL is able to capture the competitive relationship among tasks (competitive structure), while remove unimportant features which are common across the tasks from the final model (sparse structure). Experimental studies on synthetic and real data indicate that the proposed method can significantly improve learning performance by identifying sparse and task-competitive structures simultaneously.","Multi-task learning, Sparse exclusive lasso, Task-competitive",Cheng Liu and Chu-Tao Zheng and Sheng Qian and Si Wu and Hau-San Wong,https://www.sciencedirect.com/science/article/pii/S0031320318304394,https://doi.org/10.1016/j.patcog.2018.12.018,0031-3203,2019,689--701,88,Pattern Recognition,Encoding sparse and competitive structures among tasks in multi-task learning,article,LIU2019689,
"Dimensionality reduction in high dimensional multi-view datasets is an important research topic. It can keep essential features to improve performance in subsequent tasks such as classification and clustering. This paper proposes a generalized framework, which extends the PCA idea of minimizing least squares reconstruction errors, to include data distribution and multiple dictionaries for preserving outliers-free global structures in multi-view datasets. To also preserve local manifold structures, multiple local graphs are incorporated. Finally two models, in Multi-dictionary Least Squares Framework regularized with Multi-graph Embeddings (MD-MGE), are proposed for preserving both global and local structures. Extensive experimental results on four multi-view datasets prove both methods outperform the existing comparative methods. Also, their accuracy rates improvements are statistically significant on all cases below the significance level of 0.05.","Multi-view dimension reduction, Least squares, Multiple graphs, Feature extraction, Classification",Timothy {Apasiba Abeo} and Xiang-Jun Shen and Bing-Kun Bao and Zheng-Jun Zha and Jianping Fan,https://www.sciencedirect.com/science/article/pii/S0031320319300226,https://doi.org/10.1016/j.patcog.2019.01.012,0031-3203,2019,1--11,90,Pattern Recognition,A generalized multi-dictionary least squares framework regularized with multi-graph embeddings,article,APASIBAABEO20191,
"Deep networks have recently seen significant application to the analysis of medical image data, particularly for segmentation and disease classification. However, there are many situations in which the purpose of analysing a medical image is to perform parameter estimation, assess connectivity or determine geometric relationships. Some of these tasks are well served by probabilistic trackers, including Kalman and particle filters. In this work, we explore how the probabilistic outputs of a single-architecture deep network may be coupled to a probabilistic tracker, taking the form of a particle filter. The tracker provides information not easily available with current deep networks, such as a unique ordering of points along vessel centrelines and edges, whilst the construction of observation models for the tracker is simplified by the use of a deep network. We use the analysis of retinal images in several datasets as the problem domain, and compare estimates of vessel width in a standard dataset (REVIEW) with manually determined measurements.","Particle filtering, Deep neural network, Deep Belief Net, Fundus image, Width estimation, Tracking",FatmatÃ¼lzehra Uslu and Anil Anthony Bharath,https://www.sciencedirect.com/science/article/pii/S0031320318303625,https://doi.org/10.1016/j.patcog.2018.10.017,0031-3203,2019,157--169,87,Pattern Recognition,A recursive Bayesian approach to describe retinal vasculature geometry,article,USLU2019157,
"Multi-label learning is concerned with the classification of data with multiple class labels. This is in contrast to the traditional classification problem where every data instance has a single label. Due to the exponential size of output space, exploiting intrinsic information in feature and label spaces has been the major thrust of research in recent years and use of parametrization and embedding have been the prime focus. Researchers have studied several aspects of embedding which include label embedding, input embedding, dimensionality reduction and feature selection. These approaches differ from one another in their capability to capture other intrinsic properties such as label correlation, local invariance etc. We assume here that the input data form groups and as a result, the label matrix exhibits a sparsity pattern and hence the labels corresponding to objects in the same group have similar sparsity. In this paper, we study the embedding of labels together with the group information with an objective to build an efficient multi-label classifier. We assume the existence of a low-dimensional space onto which the feature vectors and label vectors can be embedded. In order to achieve this, we address three sub-problems namely; (1) Identification of groups of labels; (2) Embedding of label vectors to a low rank-space so that the sparsity characteristic of individual groups remains invariant; and (3) Determining a linear mapping that embeds the feature vectors onto the same set of points, as in stage 2, in the low-dimensional space. We compare our method with seven well-known algorithms on twelve benchmark data sets. Our experimental analysis manifests the superiority of our proposed method over state-of-art algorithms for multi-label learning.","Multi-label classification, Label embedding, Matrix factorization",Vikas Kumar and Arun K Pujari and Vineet Padmanabhan and Venkateswara Rao Kagita,https://www.sciencedirect.com/science/article/pii/S0031320319300184,https://doi.org/10.1016/j.patcog.2019.01.009,0031-3203,2019,23--34,90,Pattern Recognition,Group preserving label embedding for multi-label classification,article,KUMAR201923,
"This study proposes a divergence-curl-driven framework for the perception of crowd motion states. In this framework, the characteristics of a flow field, divergence and curl, are used to analyze crowd states. As a collective motion, the movement of a pedestrian crowd shows coherent structural properties. By using the methods of fluid mechanics and the feature visualization of flow fields, a physical characteristic descriptor of crowd motion is established that can model the motion state in a crowd flow field. Given the significance of the temporal comparison of motion states for detecting changes in crowds, a method based on the temporal context of motion is presented to measure changes in the distribution of the physical characteristic descriptors of crowd motion. This method can be used to calculate differences in the distribution of the flow fieldâs physical characteristics between each state and measure these subtle continuous changes on the sample points, thereby obtaining a quantified metric of changes in a crowdâs motion state. Experiments on crowd event datasets demonstrate the effectiveness of our proposed framework for detecting crowd state changes and abnormal activity.","Crowd state analysis, Physical characteristics, Temporal context of motion",Xiao-Han Chen and Jian-Huang Lai,https://www.sciencedirect.com/science/article/pii/S003132031830414X,https://doi.org/10.1016/j.patcog.2018.11.023,0031-3203,2019,342--355,88,Pattern Recognition,Detecting abnormal crowd behaviors based on the div-curl characteristics of flow fields,article,CHEN2019342,
"Learning a visual category with few labeled samples is a challenging problem in machine learning, which has motivated the multi-source adaptation learning technique, which exploits to transfer multiple prior discriminative models to target domain. Under this paradigm, however, different visual features at hand cannot be effectively exploited to represent a target object with versatility for boosting the adaptation performance. Besides, existing multi-source adaptation schemes mostly focus on either visual understanding or feature learning, independently. This may lead to the so-called semantic gap between the low-level features and the high-level semantics. Last but not the least, how to discriminatively select the prior models is yet another unresolved issue. To address these issues, we propose a novel co-regression framework with Multi-Source adaptation Multi-Feature Representation (MSMFR) for visual recognition, which jointly explores robust multi-feature co-regression, latent space learning, and representative sources selection, by integrating them into a unified framework for joint visual understanding and feature learning. Specifically, MSMFR conducts the multi-feature co-regression by simultaneously uncovering multiple latent spaces and minimizing the co-regression residual by taking correlations among multiple feature representations into account. Furthermore, MSMFR also automatically selects the representative (or discriminative) source models for each target feature representation via formulating a row-sparsity pursuit problem. The validity of our method is examined by three challenging visual domain adaptation tasks on several benchmark datasets, which demonstrate the superiority of our method in comparison with several state-of-the-arts.","Multi-source adaptation, Multi-feature representation, Latent space, Group sparsity",Jianwen Tao and Di Zhou and Fangyu Liu and Bin Zhu,https://www.sciencedirect.com/science/article/pii/S0031320318303741,https://doi.org/10.1016/j.patcog.2018.10.023,0031-3203,2019,296--316,87,Pattern Recognition,Latent multi-feature co-regression for visual recognition by discriminatively leveraging multi-source models,article,TAO2019296,
"Importance of clustering is signified by its applications in many scientific fields and the uninterrupted presence of related algorithmic methods in the literature for more than half a century. The majority of these methods have complex operation and most of the times aim on a specific application field. Here we present K-Networks (K-Nets), a deterministic algorithm based on a network structure whose nodes represent data points and can intrinsically work within a single or multi-layer architecture. The number of interconnections of each node with the rest of the network indicates the clusteringresolution degree and can be the same or different in every layer. K-Nets automatically detects symmetric structures in data that can be utilized to recover clusters of different size, shape, or partitions composed of a specific number of clusters. We provide evidence that K-Nets performance compares favorably with other well established clustering algorithms in a wide range of fields, while it can be employed in cases when data dimensionality prohibits their application.",,Ioannis A. Maraziotis and Stavros Perantonis and Andrei Dragomir and Dimitris Thanos,https://www.sciencedirect.com/science/article/pii/S0031320318303984,https://doi.org/10.1016/j.patcog.2018.11.010,0031-3203,2019,470--481,88,Pattern Recognition,K-Nets: Clustering through nearest neighbors networks,article,MARAZIOTIS2019470,
"Minimum sum-of-squares clustering (MSSC) is a widely used clustering model, of which the popular K-means algorithm constitutes a local minimizer. It is well known that the solutions of K-means can be arbitrarily distant from the true MSSC global optimum, and dozens of alternative heuristics have been proposed for this problem. However, no other algorithm has been predominantly adopted in the literature. This may be related to differences of computational effort, or to the assumption that a near-optimal solution of the MSSC has only a marginal impact on clustering validity. In this article, we dispute this belief. We introduce an efficient population-based metaheuristic that uses K-means as a local search in combination with problem-tailored crossover, mutation, and diversification operators. This algorithm can be interpreted as a multi-start K-means, in which the initial center positions are carefully sampled based on the search history. The approach is scalable and accurate, outperforming all recent state-of-the-art algorithms for MSSC in terms of solution quality, measured by the depth of local minima. This enhanced accuracy leads to clusters which are significantly closer to the ground truth than those of other algorithms, for overlapping Gaussian-mixture datasets with a large number of features. Therefore, improved global optimization methods appear to be essential to better exploit the MSSC model in high dimension.","Clustering, Minimum sum-of-squares, Global optimization, Hybrid genetic algorithm, K-means, Unsupervised learning",Daniel Gribel and Thibaut Vidal,https://www.sciencedirect.com/science/article/pii/S0031320318304436,https://doi.org/10.1016/j.patcog.2018.12.022,0031-3203,2019,569--583,88,Pattern Recognition,HG-means: A scalable hybrid genetic algorithm for minimum sum-of-squares clustering,article,GRIBEL2019569,
"Reusable model design becomes desirable with the rapid expansion of computer vision and pattern recognition applications. In this paper, we focus on the reusability of pre-trained deep convolutional models. Specifically, different from treating pre-trained models as feature extractors, we reveal more treasures beneath convolutional layers, i.e., the convolutional activations could act as a detector for the common object in the object co-localization problem. We propose a simple yet effective method, termed Deep Descriptor Transformation (DDT), for evaluating the correlations of descriptors and then obtaining the category-consistent regions, which can accurately locate the common object in a set of unlabeled images, i.e., object co-localization. Empirical studies validate the effectiveness of the proposed DDT method. On benchmark object co-localization datasets, DDT consistently outperforms existing state-of-the-art methods by a large margin. Moreover, DDT also demonstrates good generalization ability for unseen categories and robustness for dealing with noisy data. Beyond those, DDT can be also employed for harvesting web images into valid external data sources for improving performance of both image recognition and object detection.","Unsupervised object discovery, Object co-localization, Deep descriptor transformation, Pre-trained CNN models",Xiu-Shen Wei and Chen-Lin Zhang and Jianxin Wu and Chunhua Shen and Zhi-Hua Zhou,https://www.sciencedirect.com/science/article/pii/S003132031830373X,https://doi.org/10.1016/j.patcog.2018.10.022,0031-3203,2019,113--126,88,Pattern Recognition,Unsupervised object discovery and co-localization by deep descriptor transformation,article,WEI2019113,
"We propose a new visual tracking algorithm leveraging multi-level visual attention to take full use of the information during tracking. Visual attention has been widely applied in many visual tasks, such as image captioning and question answering. However, most existing attention models only focus on one or two aspects, ignoring the other useful information in visual tracking. Here, we think there are four main attentional aspects in the tracking task and propose a unified network to leverage multi-level visual attention, which includes layer-wise attention, temporal attention, spatial attention and channel-wise attention. Considering that deep features of different levels may be suitable for different scenarios, we propose to train an attention network in the off-line stage to facilitate feature selection in online tracking. To better exploit the temporal consistency assumption of visual tracking, we implement the attention network with long short term memory (LSTM) units, which are capable of capturing the historical context information to perform more reliable inference at the current time step. Different from the image classification task, background clutter is more complicated in the tracking task. Thus, we purify the features by spatial attention and channel-wise attention to effectively suppress the background noise and highlight the target region. In addition, we also enforce deep feature sharing across target candidates using Region of Interest pooling, allowing the features of all candidates to be extracted in only one forward pass of the DNN. To further improve tracking accuracy, a promoting strategy for trackers with detection results of a generic object detector is proposed, reducing the risk of tracking drifts. The proposed tracking algorithm compares favorably against state-of-the-art methods on three popular benchmark datasets. Extensive experimental evaluations demonstrate the effectiveness of the proposed techniques.","Visual tracking, Deep neural network, Attention model, Long short term memory",Boyu Chen and Peixia Li and Chong Sun and Dong Wang and Gang Yang and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320318303509,https://doi.org/10.1016/j.patcog.2018.10.005,0031-3203,2019,80--93,87,Pattern Recognition,Multi attention module for visual tracking,article,CHEN201980,
"Speaker naming has recently received considerable attention in identifying the active speaking character in a movie video, and face cue alone is generally insufficient to achieve reliable performance due to its significant appearance variations. In this paper, we treat the speaker naming task as a group of matched audio-face pair finding problems, and present an efficient attention guided deep audio-face fusion approach to detect the active speakers. First, we start with VGG-encoding of face images and extract the Mel-Frequency Cepstrum Coefficients from audio signals. Then, two efficient audio encoding modules, namely two-layer Long Short-Term Memory encoding and two-dimensional convolution encoding, are addressed to discriminate the high-level audio features. Meanwhile, we train an end-to-end audio-face common attention model to discriminate the face attention vector, featuring adaptively to accommodate various face variations. Further, an efficient factorized bilinear model is presented to deeply fuse the paired audio-face features, whereby the joint audio-face representation can be reliably obtained for speaker naming. Extensive experiments highlight the superiority of the proposed approach and show its very competitive performance with the state-of-the-arts.","Speaker naming, Deep audio-face fusion, Common attention model, Factorized bilinear model",Xin Liu and Jiajia Geng and Haibin Ling and Yiu-ming Cheung,https://www.sciencedirect.com/science/article/pii/S0031320318304321,https://doi.org/10.1016/j.patcog.2018.12.011,0031-3203,2019,557--568,88,Pattern Recognition,Attention guided deep audio-face fusion for efficient speaker naming,article,LIU2019557,
"The discriminative correlation filter (DCF) has shown impressive performance in visual tracking. Context has two functions in DCF: addressing the disturbance in target locating, and supplying cues for locating the target within the context. To improve the context utilization, we introduce a multi-level context-adaptive tracking (MCAT) approach for DCF tracking. Firstly, a multi-level context representationâcalled a context pyramidâis proposed to exploit the relationship between the target and its context for better visual tracking. Secondly, for each level of the context pyramid, we control the effect of context in DCF learning and tracking using context-adaptive spatial windows. An accurate target model can thereby be learned, even when the background clutter is severe. Moreover, the target can be more easily tracked when the background is weakened by the spatial window. Thirdly, a robust prediction of the target position is obtained with the multi-level structure of the context pyramid. Experimental results showed that, with conventional hand-crafted features, our tracker provided state-of-the-art performance on OTB100 comparable to those of deep-learning-based trackers.","Visual tracking, Correlation filter, Context-adaptive tracker, Context pyramid",Peng Liu and Chang Liu and Wei Zhao and Xianglong Tang,https://www.sciencedirect.com/science/article/pii/S0031320318303595,https://doi.org/10.1016/j.patcog.2018.10.013,0031-3203,2019,216--225,87,Pattern Recognition,Multi-level context-adaptive correlation tracking,article,LIU2019216,
"Monitoring the properties of single sample robust analyses of multivariate data as a function of breakdown point or efficiency leads to the adaptive choice of the best values of these parameters, eliminating arbitrary decisions about their values and so increasing the quality of estimators. Monitoring the trimming proportion in robust cluster analysis likewise leads to improved estimators. We illustrate these procedures on a sample of 424 cows with bovine phlegmon. For clustering we use a method which includes constraints on the eigenvalues of the dispersion matrices, so avoiding thread shaped clusters. The âcar-bikeâ plot reveals the stability of clustering as the trimming level changes. The pattern of clusters and outliers alters appreciably for low levels of trimming.","Bovine phlegmon, âCar-bikeâ plot, Clustering, Eigenvalue constraint, Forward search, MCD, MM-Estimation, Modified BIC, Outliers",Marco Riani and Anthony C. Atkinson and Andrea Cerioli and Aldo Corbellini,https://www.sciencedirect.com/science/article/pii/S0031320318304047,https://doi.org/10.1016/j.patcog.2018.11.016,0031-3203,2019,246--260,88,Pattern Recognition,Efficient robust methods via monitoring for clustering and multivariate data analysis,article,RIANI2019246,
"Learning with label proportions (LLP) is a weakly supervised learning problem that is conceivable in many real-world applications, where the training data is given in bags of instances, and only knowing the proportions of data points belonging to a particular category for each bag. However, how to effectively address the LLP problem with high dimensional data and some labeled samples is still a challenging problem. In this paper, we firstly propose a novel learning problem called semi-weakly learning with label proportions (SLLP), which has more extensive application scenarios. Then, we contribute a novel method based on non-negative matrix factorization, called Proportion Constrained Matrix Factorization (PCMF). It can not only effectively incorporate the label and proportion information, but also explore the local manifold structure information of training data. Moreover, the proposed method can make the data points from the same class be more likely merged together in the latent representation space, which leads to the more discriminating power. Sufficient experimental results on the benchmark datasets demonstrates its superiority over the state-of-the-art methods for LLP problem and efficiency on solving the SLLP problem.","Matrix factorization, Semi-weakly learning with label proportions, Weakly supervised, Dimension reduction, Local geometric consistency",Zhensong Chen and Yong Shi and Zhiquan Qi,https://www.sciencedirect.com/science/article/pii/S0031320319300299,https://doi.org/10.1016/j.patcog.2019.01.016,0031-3203,2019,13--24,91,Pattern Recognition,Constrained matrix factorization for semi-weakly learning with label proportions,article,CHEN201913,
"Recent deep models advance the task of semantic visual parsing by increasing the depth of networks and the resolution (size) of the predicted labelmaps. However, the contextual information within each layer and between layers is not fully explored. Long Short Term Memory Networks(LSTM) that learn to propagate information is well-suited to model pixels dependencies with respect to spacial locations within layers and depths across layers. Unlike previous LSTM-based methods that tend to enhance representation of each pixel only by involving the information from adjacent area. This work proposes Progressively Diffused Networks (PDNs) to deal with complex semantic parsing tasks. It can explore spatial dependencies in a larger field that represents the rich contextual information among pixels. The proposed model has three appealing properties. First, it enables information to be progressively broadcast across feature maps by stacking multiple diffusion layers. Second, in each layer, multiple convolutional LSTMs are adopted to generate a series of feature maps with different ranges of contexts. Third, in each LSTM unit, a special type of atrous filters are designed to capture the short range and long range dependencies from various neighbors. Extensive experiments demonstrate the effectiveness of PDNs to substantially improve the performances of existing LSTM-based models.","Visual understanding, Image segmentation, Recurrent neural networks, Representation learning",Ruimao Zhang and Wei Yang and Zhanglin Peng and Pengxu Wei and Xiaogang Wang and Liang Lin,https://www.sciencedirect.com/science/article/pii/S0031320319300214,https://doi.org/10.1016/j.patcog.2019.01.011,0031-3203,2019,78--86,90,Pattern Recognition,Progressively diffused networks for semantic visual parsing,article,ZHANG201978,
"Optimization is an important issue in machine learning because many machine learning models are reformulated as optimization problems. Different kinds of machine learning algorithms mainly focus on minimizing their empirical loss like deep learning, logistic regression, and support vector machine. Because data is explosively growing, it is challenging to deal with a large-scale optimization problem. Recently, stochastic second-order methods have emerged to attract much attention due to their efficiency in each iteration. These methods show good performance on training machine learning algorithms like logistic regression and support vector machine. However, the computational complexity of existing stochastic second-order methods heavily depends on the condition number of the Hessian. In this paper, we propose a new Newton-like method called Preconditioned Newton Conjugate Gradient with Sketched Hessian (PNCG). The runtime complexity of PNCG is at most logarithmic in the condition number of the Hessian. PNCG exhibits advantages over existing subsampled Newton methods especially when the Hessian matrix in question is ill-conditioned. We also show that our method has good performance on training machine learning algorithm empirically. The results show consistent improvements in computational efficiency.",,Haishan Ye and Guangzeng Xie and Luo Luo and Zhihua Zhang,https://www.sciencedirect.com/science/article/pii/S0031320318304163,https://doi.org/10.1016/j.patcog.2018.11.031,0031-3203,2019,629--642,88,Pattern Recognition,Fast stochastic second-order method logarithmic in condition number,article,YE2019629,
"Zero-shot complex event detection has been an emerging task in coping with the scarcity of labeled training videos in practice. Aiming to progress beyond the state-of-the-art zero-shot event detection, we propose a new zero-shot event detection approach, which exploits the semantic correlation between an event and concepts. Based on the concept detectors pre-trained from external sources, our method learns the semantic correlation from the concept vocabulary and emphasizes on the most related concepts for the zero-shot event detection. Particularly, a novel Event-Adaptive Concept Integration algorithm is introduced to estimate the effectiveness of semantically related concepts by assigning different weights to them. As opposed to assigning weights by an invariable strategy, we compute the weights of concepts using the area under score curve. The assigned weights are incorporated into the confidence score vector statistically to better characterize the event-concept correlation. Our algorithm is proved to be able to harness the related concepts discriminatively tailored for a target event. Extensive experiments are conducted on the challenging TRECVID event video datasets, which demonstrate the advantage of our approach over the state-of-the-art methods.","Zero-shot event detection, Concept relevance mining, Semantic concept",Zhihui Li and Lina Yao and Xiaojun Chang and Kun Zhan and Jiande Sun and Huaxiang Zhang,https://www.sciencedirect.com/science/article/pii/S003132031830431X,https://doi.org/10.1016/j.patcog.2018.12.010,0031-3203,2019,595--603,88,Pattern Recognition,Zero-shot event detection via event-adaptive concept relevance mining,article,LI2019595,
"The multi-view sparse representation based visual tracking has attracted increasing attention because the sparse representations of different object features can complement with each other. Since the robustness of different object features is actually not the same in challenging video sequences, it may contain unreliable features (the features with low robustness) in multi-view sparse representation. In this case, how to highlight the useful information of unreliable features for proper multi-feature fusion has become a tough work. To solve this problem, we propose a multi-view discriminant sparse representation method for robust visual tracking, in which we firstly divide the multi-view observations into different groups, and then estimate the sparse representations of multi-view group projections for calculating the observation likelihood. The advantages of the proposed sparse representation method are two-folds: 1) It can properly fuse the observation groups with reliable and unreliable features by using an online updated discriminant matrix to explore the group similarity in multi-feature space. 2) It introduces a nonlocal regularizer to enforce the spatial smoothness among the sparse representations of different group projections, which can enhance the robustness of multi-view sparse representation. Experimental results show that our method can achieve a better tracking performance than state-of-the-art tracking methods do.","Sparse representation, Visual tracking, Multi-view learning, Dual group structure",Bin Kang and Wei-Ping Zhu and Dong Liang and Mingkai Chen,https://www.sciencedirect.com/science/article/pii/S0031320318303911,https://doi.org/10.1016/j.patcog.2018.11.005,0031-3203,2019,75--89,88,Pattern Recognition,Robust visual tracking via nonlocal regularized multi-view sparse representation,article,KANG201975,
"Compared with its 2D counterpart, a 3D palmprint image contains not only the 3D structure-based but also the 2D texture-based features of the palmprint. In this paper, we propose a precision direction code and compact surface type (PDCST) method for 3D palmprint representation and identification. Specifically, we propose the precision direction code (PDC) to depict the 2D texture-based features by exploiting not only the visible but also the potential direction features of the palmprint. Moreover, we use a simple yet efficient compact surface type (CST) to represent the 3D structure-based features of the palmprint. We combine the PDC and CST forming the PDCST descriptor to represent the multiple level and multiple dimensional features of 3D palmprint images. The two-phase sparse representation scheme is used to perform PDCST-based feature identification. Extensive inter-comparative and intra-comparative experimental results on three widely used palmprint databases clearly demonstrate the effectiveness of the proposed method.","Biometrics, 3D palmprint identification, Precision direction extraction, Compact surface type",Lunke Fei and Bob Zhang and Yong Xu and Wei Jia and Jie Wen and Jigang Wu,https://www.sciencedirect.com/science/article/pii/S0031320318303637,https://doi.org/10.1016/j.patcog.2018.10.018,0031-3203,2019,237--247,87,Pattern Recognition,Precision direction and compact surface type representation for 3D palmprint identification,article,FEI2019237,
"Density and distance based clustering are two distinct approaches to the same problem. In this contribution, a novel algorithm is presented in order to exploit the benefits of both approaches. This is achieved, not by combining those approaches into a single notion, but by utilizing the advantages of each one, depending on what each step of the algorithm aims to achieve. To be precise, the Window Density Function is utilized to provide regions of high density and hence a region of clusters or a part of a cluster. Affinity Propagation is, consequently, utilized to provide a group of clusters within such a region. Finally, these regions are merged to form actual clusters. The proposed methodology is tested on a variety of synthetic and real-life datasets. The algorithm presented in this contribution outperforms other well-known algorithms, with which it is compared to, in the majority of the datasets used.","Clustering algorithms, Density based clustering, Distance based clustering, Evolutionary clustering, Window density function",Emmanouil K. Ikonomakis and George M. Spyrou and Michael N. Vrahatis,https://www.sciencedirect.com/science/article/pii/S0031320318303510,https://doi.org/10.1016/j.patcog.2018.10.007,0031-3203,2019,190--202,87,Pattern Recognition,Content driven clustering algorithm combining density and distance functions,article,IKONOMAKIS2019190,
"This paper presents a fast and accurate method for matching oblique aerial image pairs. In order to achieve accurate matching results, we must consider viewpoint differences between the input images in addition to rotation and scaling. Existing methods that match aerial image pairs with viewpoint differences undergo heavy computation and have difficulty finding correspondences. In this paper, we propose a homography matrix evaluation method based on a geometric approach to increase the accuracy of image matching results. In addition, we achieve faster matching through an iterative transform simulation that reduces computational complexity. Experimental results show that the proposed method improves aerial image matching in terms of computational efficiency while achieving successful matching results.","SIFT descriptor, Feature matching, Aerial image, Viewpoint change, Homography matrix",Woo-Hyuck Song and Hong-Gyu Jung and In-Youb Gwak and Seong-Whan Lee,https://www.sciencedirect.com/science/article/pii/S0031320318303753,https://doi.org/10.1016/j.patcog.2018.10.027,0031-3203,2019,317--331,87,Pattern Recognition,Oblique aerial image matching based on iterative simulation and homography evaluation,article,SONG2019317,
"Due to its easiness to construct and interpret, along with its good performance, naive Bayes (NB) is widely used to address classification problems in real-world applications. In order to alleviate its conditional independence assumption, a mass of attribute weighting approaches have been proposed. However, almost all these approaches assign each attribute a same (global) weight for all classes. In this paper, we call them the general attribute weighting and argue that for NB attribute weighting should be class-specific (class-dependent). Based on this premise, we propose a new paradigm for attribute weighting called the class-specific attribute weighting, which discriminatively assigns each attribute a specific weight for each class. We call the resulting model class-specific attribute weighted naive Bayes (CAWNB). CAWNB selects class-specific attribute weights to maximize the conditional log likelihood (CLL) objective function or minimize the mean squared error (MSE) objective function, and thus two different versions are created, which we denote as CAWNBCLL and CAWNBMSE, respectively. Extensive empirical studies show that CAWNBCLL and CAWNBMSE all obtain more satisfactory experimental results compared with NB and other existing state-of-the-art general attribute weighting approaches. We believe that for NB class-specific attribute weighting could be a more fine-grained attribute weighting approach than general attribute weighting.","Naive Bayes, Attribute weighting, Weight optimization",Liangxiao Jiang and Lungan Zhang and Liangjun Yu and Dianhong Wang,https://www.sciencedirect.com/science/article/pii/S0031320318304205,https://doi.org/10.1016/j.patcog.2018.11.032,0031-3203,2019,321--330,88,Pattern Recognition,Class-specific attribute weighted naive Bayes,article,JIANG2019321,
"Commute time is a robust measure on graphs based on random walks. It has been successfully applied in many application domains including personalized search, collaborative filtering and network intrusion detection. However, the computation of the commute time is expensive since it involves the eigen decomposition of the graph Laplacian matrix. There has been effort to approximate the commute time but they only work in an offline mode. In this work, an accurate and efficient approximation for computing the commute time is proposed in an incremental fashion in order to facilitate online applications. Using the incremental commutime, we design an online anomaly detection application where the commute time of each new arriving data point to any point in the current graph can be estimated in constant time. The proposed approach shows its high accuracy and efficiency in synthetic and real datasets for online applications. It takes only 8 milliseconds on average to detect anomalies online on the DBLP graph which has more than 600,000 nodes and 2 millions edges. We also discuss the use of incremental commute time for other online applications such as classification, graph ranking and clustering.","Commute time, Random walks, Online learning, Anomaly detection, Manifold learning",Nguyen Lu Dang Khoa and Yang Wang and Sanjay Chawla,https://www.sciencedirect.com/science/article/pii/S0031320318303996,https://doi.org/10.1016/j.patcog.2018.11.012,0031-3203,2019,101--112,88,Pattern Recognition,Incremental commute time and its online applications,article,KHOA2019101,
"Nowadays, the majority of fingerprint quality, matching and feature extraction algorithms are developed and trained on fingerprints of adults. Accordingly, the processing of childrenâs fingerprints presents performance issues derived for the most part from: (1) their smaller size and finer ridge structure; (2) their higher variability over time due to the displacement of minutiae induced by growth. The present article is focused on the second factor. The rapid growth of children fingerprints causes a significant displacement of the minutiae points between samples of the same finger acquired with a few years distance from each other. This displacement results in a decrease of the accuracy of fingerprint recognition systems when the reference and probe sample drift apart in time. This effect is known as biometric ageing. In the present study we propose to address this issue by developing and validating a minutiae-based growth model, derived from a database of over 60,000 childrenâs fingerprints, acquired in real operational conditions, ranging between 5 and 16 years of age, with a time difference between fingerprint pairs re-enrolments of up to 6 years. We analyze two potential application scenarios for the developed growth model. On one hand, we use the model to grow childrenâs fingerprints in order to spread out the minutiae points to attain sizes similar to those of a sample captured at a later point in time. On the other hand, we apply the model to rejuvenate fingerprints enrolled at a later stage by contracting the minutiae points so that their location is more similar to those of a sample acquired earlier. In both scenarios, the application of the growth model to produce artificially grown/rejuvenated fingerprint minutiae templates results in a significant improvement of the matching scores compared to the ones produced by original fingerprints.","Biometrics, Fingerprint recognition, Ageing, Fingerprint growth, Children, Adolescents",Rudolf Haraksim and Javier Galbally and Laurent Beslay,https://www.sciencedirect.com/science/article/pii/S0031320318304448,https://doi.org/10.1016/j.patcog.2018.12.024,0031-3203,2019,614--628,88,Pattern Recognition,Fingerprint growth model for mitigating the ageing effect on childrenâs fingerprints matching,article,HARAKSIM2019614,
"Limited by the existing imagery hardware and contaminated with noise or shading, spatial deterioration and spectral distortion exist in hyperspectral images (HSIs). Spectral-spatial quality enhancement was seldom addressed in a clear way albeit of first importance in HSI interpretation. In this paper, we present a promising quality enhancement method in a spectral-spatial combination framework for removing unwanted components and enhancing useful features. Our approach, called saliency detection and feature enhancement (SDFE), combines the theory of structure tensor with a deep convolutional neural network (CNN) to solve an HSI quality enhancement problem that has rarely been identified. Considering the different contribution rates of each band, an adaptive weighting method based on the eigenvalues of structure tensor is proposed to fuse the selected key band group. Then, a saliency detection method is presented to extract edge areas and corners. Owning to the success of CNN in visual-based issues, we utilize it to further enhance the saliency and obtain high-quality spatial information. To extract high-quality spectral features, the nonnegative matrix factorization (NMF) algorithm is used to extract spectral information from the original HSI. The experimental result enjoys a fact of identical materials with the similar signatures, which is useful for the subsequent application. Furthermore, our approach has a powerful influence on target detection.","Hyperspectral image, Quality enhancement, Structure tensor, Deep neural networks, Adaptive weighting, Nonnegative matrix factorization",Weiying Xie and Yanzi Shi and Yunsong Li and Xiuping Jia and Jie Lei,https://www.sciencedirect.com/science/article/pii/S003132031830390X,https://doi.org/10.1016/j.patcog.2018.11.004,0031-3203,2019,139--152,88,Pattern Recognition,High-quality spectral-spatial reconstruction using saliency detection and deep feature enhancement,article,XIE2019139,
"3D registration is a very active topic, spanning research areas such as computational geometry, computer graphics and pattern recognition. It aims to solve spatial transformation that aligns two point clouds. In this work we propose the use of a single direction sensor, such as an accelerometer or a magnetometer, commonly available on contemporary mobile platforms, such as tablets and smartphones. Both sensors have been heavily investigated earlier, but only for joint use with other sensors, such as gyroscopes and GPS. We show a time-efficient and accurate 3D registration method that takes advantage of only either an accelerometer or a magnetometer. We demonstrate a 3D reconstruction of individual point clouds and the proposed 3D registration method on a tablet equipped with an accelerometer or a magnetometer. However, we point out that the proposed method is not restricted to mobile platforms. Indeed, it can easily be applied in any 3D measurement system that is upgradable with some ubiquitous direction sensor, for example by adding a smartphone equipped with either an accelerometer or a magnetometer. We compare the proposed method against several state-of-the-art methods implemented in the open source Point Cloud Library (PCL). The proposed method outperforms the PCL methods tested, both in terms of processing time and accuracy.","3D rigid registration, 3D reconstruction, Smartphone, Tablet, Accelerometer, Magnetometer, Structured light pattern",Tomislav PribaniÄ and Tomislav PetkoviÄ and Matea ÄonliÄ,https://www.sciencedirect.com/science/article/pii/S0031320318304291,https://doi.org/10.1016/j.patcog.2018.12.008,0031-3203,2019,532--546,88,Pattern Recognition,3D registration based on the direction sensor measurements,article,PRIBANIC2019532,
"Brain functional networks (BFNs) constructed from resting-state functional magnetic resonance imaging (rs-fMRI) have been widely applied to the analysis and diagnosis of brain diseases, such as Alzheimer's disease and its prodrome, namely mild cognitive impairment (MCI). Constructing a meaningful brain network based on, for example, sparse representation (SR) is the most essential step prior to the subsequent analysis or disease identification. However, the independent coding process of SR fails to capture the intrinsic locality and similarity characteristics in the data. To address this problem, we propose a novel weighted graph (Laplacian) regularized SR framework, based on which BFN can be optimized by considering both intrinsic correlation similarity and local manifold structure in the data, as well as sparsity prior of the brain connectivity. Additionally, the non-convergence of the graph Laplacian in the self-representation model has been solved properly. Combined with a pipeline of sparse feature selection and classification, the effectiveness of our proposed method is demonstrated by identifying MCI based on the constructed BFNs.","Graph Laplacian regularization, Sparse representation, Brain functional network, Mild cognitive impairment (MCI)",Renping Yu and Lishan Qiao and Mingming Chen and Seong-Whan Lee and Xuan Fei and Dinggang Shen,https://www.sciencedirect.com/science/article/pii/S0031320319300287,https://doi.org/10.1016/j.patcog.2019.01.015,0031-3203,2019,220--231,90,Pattern Recognition,Weighted graph regularized sparse brain network construction for MCI identification,article,YU2019220,
"The way similarity is measured among time series is of paramount importance in many data mining and machine learning tasks. For instance, Elastic Similarity Measures are widely used to determine whether two time series are similar to each other. Indeed, in off-line time series mining, these measures have been shown to be very effective due to their ability to handle time distortions and mitigate their effect on the resulting distance. In the on-line setting, where available data increase continuously over time and not necessary in a stationary manner, stream mining approaches are required to be fast with limited memory consumption and capable of adapting to different stationary intervals. In this sense, the computational complexity of Elastic Similarity Measures and their lack of flexibility to accommodate different stationary intervals, make these similarity measures incompatible with the requirements mentioned. To overcome these issues, this paper adapts the family of Elastic Similarity Measures â which includes Dynamic Time Warping, Edit Distance, Edit Distance for Real Sequences and Edit Distance with Real Penalty â to the on-line setting. The proposed adaptation is based on two main ideas: a forgetting mechanism and the incremental computation. The former makes the similarity consistent with streaming time series characteristics by giving more importance to recent observations, whereas the latter reduces the computational complexity by avoiding unnecessary computations. In order to assess the behavior of the proposed similarity measure in on-line settings, two different experiments have been carried out. The first aims at showing the efficiency of the proposed adaptation, to do so we calculate and compare the computation time for the elastic measures and their on-line adaptation. By analyzing the results drawn from a distance-based streaming machine learning model, the second experiment intends to show the effect of the forgetting mechanism on the resulting similarity value. The experimentation shows, for the aforementioned Elastic Similarity Measures, that the proposed adaptation meets the memory, computational complexity and flexibility constraints imposed by streaming data.","Time series, Streaming data, Dynamic time warping, Elastic similarity measures",Izaskun Oregi and Aritz PÃ©rez and Javier {Del Ser} and Jose A. Lozano,https://www.sciencedirect.com/science/article/pii/S003132031830428X,https://doi.org/10.1016/j.patcog.2018.12.007,0031-3203,2019,506--517,88,Pattern Recognition,On-line Elastic Similarity Measures for time series,article,OREGI2019506,
"Hashing based cross-media method has been become an increasingly popular technique in facilitating large-scale multimedia retrieval task, owing to its effectiveness and efficiency. Most existing cross-media hashing methods learn hash functions in a batch based mode. However, in practical applications, data points often emerge in a streaming manner, which makes batch based hashing methods loss their efficiency. In this paper, we propose an Online Latent Semantic Hashing (OLSH) method to address this issue. Only newly arriving multimedia data points are utilized to retrain hash functions efficiently and meanwhile preserve the semantic correlations in old data points. Specifically, for learning discriminative hash codes, discrete labels are mapped to a continuous latent semantic space where the relative semantic distances in data points can be measured more accurately. And then, we propose an online optimization scheme towards the challenging task of learning hash functions efficiently on streaming data points, and the computational complexity and memory cost are much less than the size of training dataset at each round. Extensive experiments across many real-world datasets, e.g. Wiki, Mir-Flickr25K and NUS-WIDE, show the effectiveness and efficiency of the proposed method.","Cross-media retrieval, Online learning, Hashing, Latent semantic concept",Tao Yao and Gang Wang and Lianshan Yan and Xiangwei Kong and Qingtang Su and Caiming Zhang and Qi Tian,https://www.sciencedirect.com/science/article/pii/S0031320318304333,https://doi.org/10.1016/j.patcog.2018.12.012,0031-3203,2019,1--11,89,Pattern Recognition,Online latent semantic hashing for cross-media retrieval,article,YAO20191,
"This work addresses three important yet challenging problems of handwritten text understanding: word recognition, query-by-example (QBE) word spotting and query-by-string (QBS) word spotting. In most existing approaches, these related tasks are considered independently. We propose a single unified framework based on deep learning to solve all three tasks efficiently and simultaneously. In this framework, an end-to-end deep neural network architecture is used for the joint embedding of handwritten word texts and images. Word images are embedded via a convolution neural network (CNN), which is trained to predict a representation modeling character-level information. The output of the last convolutional layer is considered as representation in the joint embedding subspace. Likewise, a recurrent neural network (RNN) is used to map a sequence of characters to the joint subspace representation. Finally, a model based on multi-layer perceptrons is proposed to predict the matching probability between two embedding vectors. Experiments on five databases of documents written in three languages show our method to yield state-of-the-art performance for QBE and QBS word spotting. The proposed method also obtains competitive results for word recognition, when compared against approaches tailored specifically for this task.","Representation learning, Word image representation, Word text representation, Word spotting, Word recognition",Mohamed Mhiri and Christian Desrosiers and Mohamed Cheriet,https://www.sciencedirect.com/science/article/pii/S0031320318304059,https://doi.org/10.1016/j.patcog.2018.11.017,0031-3203,2019,312--320,88,Pattern Recognition,Word spotting and recognition via a joint deep embedding of image and text,article,MHIRI2019312,
"Tone-mapping operators (TMOs), which are designed to convert high dynamic range (HDR) images to standard low dynamic range (LDR) images for displaying on conventional devices, have gained extensive attention recently. The quality of tone-mapped images generated by different TMOs varies significantly, which depend upon the image contents and the parameter settings. A quality index that can accurately evaluate the performances of TMOs is thus highly needed. With this motivation, this paper presents a blind quality index based on luminance partition for tone-mapped images. It is based on the fact that the Human Visual System (HVS) has different sensitivities to image regions with different luminance levels. Specifically, two adaptive thresholds are first employed to segment an image into the dark, bright and normal areas. Then, we calculate the quality-aware features from different luminance areas: 1) local entropy feature is extracted from the dark and bright areas to measure the information loss due to the overexposure or underexposure during the tone mapping process; 2) local colorfulness feature is extracted from the normal area to evaluate the reproduction of colors. With the consideration that the perception of image quality depends on the combined effects of the salient local distortion and global quality degradation, the global contrast feature is also calculated and integrated for better evaluation performance. Moreover, to take advantage of the hierarchical characteristic of the HVS, all features are calculated under a multi-resolution framework. Eventually, the extracted features are mapped into an objective quality score based on the random forest regression. The proposed metric is shown to outperform those state-of-the-art metrics according to extensive experiments conducted on two publicly available databases.","Tone-mapping operators, Tone-mapped image, Human visual system, Luminance partition, Multi-resolution representation, Random forest regression",Pengfei Chen and Leida Li and Xinfeng Zhang and Shanshe Wang and Allen Tan,https://www.sciencedirect.com/science/article/pii/S0031320319300202,https://doi.org/10.1016/j.patcog.2019.01.010,0031-3203,2019,108--118,89,Pattern Recognition,Blind quality index for tone-mapped images based on luminance partition,article,CHEN2019108,
"In this paper, color-depth conditional generative adversarial networks (CDcGAN) are proposed to resolve the problems of simultaneous color image super-resolution and depth image super-resolution in 3D videos. Firstly, a generative network is presented to leverage the mutual information of the low-resolution color image and low-resolution depth image so that they can enhance each other considering their geometric structural similarity in the same scene. Secondly, three auxiliary losses of data loss, total variation loss, and 8-connected gradient difference loss are introduced to train this generative network to ensure that the generated images are close to the real ones in addition to the adversarial loss. Finally, we study the CDcGAN and its variants. Experimental results show that the proposed approach can produce the high-quality color image and depth image from a pair of low-quality images, and it is superior to several other leading methods. Additionally, it has also been used to resolve the problems of concurrent image smoothing and edge detection, as well as the problem of HR-color-image-guided depth super-resolution to show the effectiveness and universality of the proposed method.","Generative adversarial networks, Super-resolution, Image smoothing, Edge detection",Lijun Zhao and Huihui Bai and Jie Liang and Bing Zeng and Anhong Wang and Yao Zhao,https://www.sciencedirect.com/science/article/pii/S0031320318304175,https://doi.org/10.1016/j.patcog.2018.11.028,0031-3203,2019,356--369,88,Pattern Recognition,Simultaneous color-depth super-resolution with conditional generative adversarial networks,article,ZHAO2019356,
"In streaming data classification, most of the existing methods assume that all arrived evolving data are completely labeled. One challenge is that some applications where only small amount of labeled examples are available for training. Incremental semi-supervised learning algorithms have been proposed for regularizing neural networks by incorporating various side information, such as pairwise constraints or user-provided labels. However, it is hard to put them into practice, especially for non-stationary environments due to the effectiveness and parameter sensitivity of such algorithms. In this paper, we propose a novel incremental semi-supervised learning framework on streaming data. Each layer of model is comprised of a generative network, a discriminant structure and the bridge. The generative network uses dynamic feature learning based on autoencoders to learn generative features from streaming data which has been demonstrated its potential in learning latent feature representations. In addition, the discriminant structure regularizes the network construction via building pairwise similarity and dissimilarity constraints. It is also used for facilitating the parameter learning of the generative network. The network and structure are integrated into a joint learning framework and bridged by enforcing the correlation of their parameters, which balances the flexible incorporation of supervision information and numerical tractability for non-stationary environments as well as explores the intrinsic data structure. Moreover, an efficient algorithm is designed to solve the proposed optimization problem and we also give an ensemble method. Particularly, when multiple layers of model are stacked, the performance is significantly boosted. Finally, to validate the effectiveness of the proposed method, extensive experiments are conducted on synthetic and real-life datasets. The experimental results demonstrate that the performance of the proposed algorithms is superior to some state-of-the-art approaches.","Semi-supervised learning, Dynamic feature learning, Streaming data, Classification",Yanchao Li and Yongli Wang and Qi Liu and Cheng Bi and Xiaohui Jiang and Shurong Sun,https://www.sciencedirect.com/science/article/pii/S0031320318303923,https://doi.org/10.1016/j.patcog.2018.11.006,0031-3203,2019,383--396,88,Pattern Recognition,Incremental semi-supervised learning on streaming data,article,LI2019383,
"The performance of gait recognition can be adversely affected by many sources of variation such as view angle, clothing, presence of and type of bag, posture, and occlusion, among others. To extract invariant gait features, we proposed a method called GaitGANv2 which is based on generative adversarial networks (GAN). In the proposed method, a GAN model is taken as a regressor to generate a canonical side view of a walking gait in normal clothing without carrying any bag. A unique advantage of this approach is that, unlike other methods, GaitGANv2 does not need to determine the view angle before generating invariant gait images. Indeed, only one model is needed to account for all possible sources of variation such as with or without carrying accessories and varying degrees of view angle. The most important computational challenge, however, is to address how to retain useful identity information when generating the invariant gait images. To this end, our approach differs from the traditional GAN in that GaitGANv2 contains two discriminators instead of one. They are respectively called fake/real discriminator and identification discriminator. While the first discriminator ensures that the generated gait images are realistic, the second one maintains the human identity information. The proposed GaitGANv2 represents an improvement over GaitGANv1 in that the former adopts a multi-loss strategy to optimize the network to increase the inter-class distance and to reduce the intra-class distance, at the same time. Experimental results show that GaitGANv2 can achieve state-of-the-art performance.","Gait recognition, Generative adversarial networks, Invariant feature",Shiqi Yu and Rijun Liao and Weizhi An and Haifeng Chen and Edel B. GarcÃ­a and Yongzhen Huang and Norman Poh,https://www.sciencedirect.com/science/article/pii/S0031320318303649,https://doi.org/10.1016/j.patcog.2018.10.019,0031-3203,2019,179--189,87,Pattern Recognition,GaitGANv2: Invariant gait feature extraction using generative adversarial networks,article,YU2019179,
"Designing efficient algorithms for mining massive high-speed data streams has become one of the contemporary challenges for the machine learning community. Such models must display highest possible accuracy and ability to swiftly adapt to any kind of changes, while at the same time being characterized by low time and memory complexities. However, little attention has been paid to designing learning systems that will allow us to gain a better understanding of incoming data. There are few proposals on how to design interpretable classifiers for drifting data streams, yet most of them are characterized by a significant trade-off between accuracy and interpretability. In this paper, we show that it is possible to have all of these desirable properties in one model. We introduce ERulesD2S: evolving rule-based classifier for drifting data Streams. By using grammar-guided genetic programming, we are able to obtain accurate sets of rules per class that are able to adapt to changes in the stream without a need for an explicit drift detector. Additionally, we augment our learning model with new proposals for rule propagation and data stream sampling, in order to maintain a balance between learning and forgetting of concepts. To improve efficiency of mining massive and non-stationary data, we implement ERulesD2S parallelized on GPUs. A thorough experimental study on 30 datasets proves that ERulesD2S is able to efficiently adapt to any type of concept drift and outperform state-of-the-art rule-based classifiers, while using small number of rules. At the same time ERulesD2S is highly competitive to other single and ensemble learners in terms of accuracy and computational complexity, while offering fully interpretable classification rules. Additionally, we show that ERulesD2S can scale-up efficiently to high-dimensional data streams, while offering very fast update and classification times. Finally, we present the learning capabilities of ERulesD2S for sparsely labeled data streams.","Machine learning, Data streams, Concept drift, Genetic programming, Rule-based classification, GPU, High-performance data mining",Alberto Cano and Bartosz Krawczyk,https://www.sciencedirect.com/science/article/pii/S0031320318303765,https://doi.org/10.1016/j.patcog.2018.10.024,0031-3203,2019,248--268,87,Pattern Recognition,Evolving rule-based classifiers with genetic programming on GPUs for drifting data streams,article,CANO2019248,
"Spatial resolution enhancement is a pre-requisite for integrating unmanned aerial vehicle (UAV) datasets with the data from other sources. However, the mobility of UAV platforms, along with radiometric and atmospheric distortions, makes the task difficult. In this paper, various convolutional neural network (CNN) architectures are explored for resolving the issues related to sub-pixel classification and super-resolution of drone-derived datasets. The main contributions of this work are: 1) network-inversion based architectures for super-resolution and sub-pixel mapping of drone-derived images taking into account their spectral-spatial characteristics and the distortions prevalent in them 2) a feature-guided transformation for regularizing the inversion problem 3) loss functions for improving the spectral fidelity and inter-label compatibility of coarser to finer-scale mapping 4) use of multi-size kernel units for avoiding over-fitting. The proposed approach is the first of its kind in using neural network inversion for super-resolution and sub-pixel mapping. Experiments indicate that the proposed super-resolution approach gives better results in comparison with the sparse-code based approaches which generally result in corrupted dictionaries and sparse codes for multispectral aerial images. Also, the proposed use of neural network inversion, for projecting spatial affinities to sub-pixel maps, facilitates the consideration of coarser-scale texture and color information in modeling the finer-scale spatial-correlation. The simultaneous consideration of spectral bands, as proposed in this study, gives better super-resolution results when compared to the individual band enhancements. The proposed use of different data-augmentation strategies, for emulating the distortions, improves the generalization capability of the framework. Sensitivity of the proposed super-resolution and sub-pixel mapping frameworks with regard to the network parameters is thoroughly analyzed. The experiments over various standard datasets as well as those collected from known locations indicate that the proposed frameworks perform better when compared to the prominent published approaches.","Sub-pixel mapping, Super-resolution, Convolutional neural network, Class distribution, Drone, UAV",Pattathal V. Arun and Ittai Herrmann and Krishna M. Budhiraju and Arnon Karnieli,https://www.sciencedirect.com/science/article/pii/S0031320318304217,https://doi.org/10.1016/j.patcog.2018.11.033,0031-3203,2019,431--446,88,Pattern Recognition,Convolutional network architectures for super-resolution/sub-pixel mapping of drone-derived images,article,ARUN2019431,
"In recent years, deep neural networks have achieved remarkable successes in many pattern recognition tasks. However, the high computational cost and large memory overhead hinder them from applications on resource-limited devices. To address this problem, many deep network acceleration and compression methods have been proposed. One group of methods adopt decomposition and pruning techniques to accelerate and compress a pre-trained model. Another group designs single compact unit to stack their own networks. These methods are subject to complicated training processes, or lack of generality and extensibility. In this paper, we propose a general framework of architecture distillation, namely LightweightNet, to accelerate and compress convolutional neural networks. Rather than compressing a pre-trained model, we directly construct the lightweight network based on a baseline network architecture. The LightweightNet, designed based on a comprehensive analysis of the network architecture, consists of network parameter compression, network structure acceleration, and non-tensor layer improvement. Specifically, we propose the strategy of low-dimensional features of fully-connected layers for substantial memory saving, and design multiple efficient compact blocks to distill convolutional layers of baseline network with accuracy-sensitive distillation rule for notable time saving. Finally, it can effectively reduce the computational cost and the model size by >4Ã with negligible accuracy loss. Benchmarks on MNIST, CIFAR-10, ImageNet and HCCR (handwritten Chinese character recognition) datasets demonstrate the advantages of the proposed framework in terms of speed, performance, storage and training process. In HCCR, our method even outperforms traditional handcrafted features-based classifiers in terms of speed and storage while maintaining state-of-the-art recognition performance.","Deep network acceleration and compression, Architecture distillation, Lightweight network",Ting-Bing Xu and Peipei Yang and Xu-Yao Zhang and Cheng-Lin Liu,https://www.sciencedirect.com/science/article/pii/S0031320318303807,https://doi.org/10.1016/j.patcog.2018.10.029,0031-3203,2019,272--284,88,Pattern Recognition,LightweightNet: Toward fast and lightweight convolutional neural networks via architecture distillation,article,XU2019272,
"Solving a supervised learning problem requires to label a training set. This task is traditionally performed by an expert, who provides a label for each sample. The proliferation of social web services (e.g., Amazon Mechanical Turk) has introduced an alternative crowdsourcing approach. Anybody with a computer can register in one of these services and label, either partially or completely, a dataset. The effort of labeling is then shared between a great number of annotators. However, this approach introduces scientifically challenging problems such as combining the unknown expertise of the annotators, handling disagreements on the annotated samples, or detecting the existence of spammer and adversarial annotators. All these problems require probabilistic sound solutions which go beyond the naive use of majority voting plus classical classification methods. In this work we introduce a new crowdsourcing model and inference procedure which trains a Gaussian Process classifier using the noisy labels provided by the annotators. Variational Bayes inference is used to estimate all unknowns. The proposed model can predict the class of new samples and assess the expertise of the involved annotators. Moreover, the Bayesian treatment allows for a solid uncertainty quantification. Since when predicting the class of a new sample we might have access to some annotations for it, we also show how our method can naturally incorporate this additional information. A comprehensive experimental section evaluates the proposed method with synthetic and real experiments, showing that it consistently outperforms other state-of-the-art crowdsourcing approaches.","Crowdsourcing, Classification, Gaussian processes, Bayesian modeling, Variational inference",Pablo Ruiz and Pablo Morales-Ãlvarez and Rafael Molina and Aggelos K. Katsaggelos,https://www.sciencedirect.com/science/article/pii/S0031320318304060,https://doi.org/10.1016/j.patcog.2018.11.021,0031-3203,2019,298--311,88,Pattern Recognition,Learning from crowds with variational Gaussian processes,article,RUIZ2019298,
"Unsupervised feature selection is a challenging task to gain relevant features for improving learning performance due to lack of the label information. Traditional unsupervised feature selection methods are often vector-based, which may ignore the location information of original matrix element. In this paper, we propose a joint sparse matrix regression and nonnegative spectral analysis model for two-dimensional unsupervised feature selection. To obtain proper label information under unsupervised condition, we adopt a nonnegative spectral clustering technique to yield the clustering labels as the pseudo class labels. To directly select the relevant feature on matrix data, we construct a regression relationship between matrix data and the pseudo class labels by deploying left and right regression matrices. Our proposed method can integrate the merits of both sparse matrix regression and nonnegative spectral clustering for feature selection. An efficient optimization algorithm is designed to solve our proposed optimization problem. Extensive experimental results on clustering and classification demonstrate the effectiveness of our proposed method.","Unsupervised learning, Two-dimensional feature selection, Sparse matrix regression, Nonnegative spectral analysis",Haoliang Yuan and Junyu Li and Loi Lei Lai and Yuan Yan Tang,https://www.sciencedirect.com/science/article/pii/S0031320319300275,https://doi.org/10.1016/j.patcog.2019.01.014,0031-3203,2019,119--133,89,Pattern Recognition,Joint sparse matrix regression and nonnegative spectral analysis for two-dimensional unsupervised feature selection,article,YUAN2019119,
"Existing dimensionality reduction methods are adept at revealing hidden underlying manifolds arising from high-dimensional data and thereby producing a low-dimensional representation. However, the smoothness of the manifolds produced by classic techniques over sparse and noisy data is not guaranteed. In fact, the embedding generated using such data may distort the geometry of the manifold and thereby produce an unfaithful embedding. Herein, we propose a framework for nonlinear dimensionality reduction that generates a manifold in terms of smooth geodesics that is designed to treat problems in which manifold measurements are either sparse or corrupted by noise. Our method generates a network structure for given high-dimensional data using a nearest neighbors search and then produces piecewise linear shortest paths that are defined as geodesics. Then, we fit points in each geodesic by a smoothing spline to emphasize the smoothness. The robustness of this approach for sparse and noisy datasets is demonstrated by the implementation of the method on synthetic and real-world datasets.","Manifold, Nonlinear dimensionality reduction, Smoothing spline, Geodesics, Noisy measurements",Kelum Gajamannage and Randy Paffenroth and Erik M. Bollt,https://www.sciencedirect.com/science/article/pii/S0031320318303650,https://doi.org/10.1016/j.patcog.2018.10.020,0031-3203,2019,226--236,87,Pattern Recognition,A nonlinear dimensionality reduction framework using smooth geodesics,article,GAJAMANNAGE2019226,
"Many successful algorithms for analyzing ECG signals leverage data-driven models that are learned for each specific user. Unfortunately, a few algorithmic challenges are still to be addressed before employing these models in wearable devices, thus enabling online and long-term monitoring. In particular, since the heartbeats morphology changes with the heart rate, models learned in resting conditions need to be adapted to analyze ECG signals recorded during everyday activities. We propose an online ECG monitoring solution where normal heartbeats of each specific user are modeled by dictionaries yielding sparse representations, and heartbeats that do not conform to this model are detected as anomalous. We track heart rate variations by adapting the user-specific dictionary with a set of user-independent, linear, transformations. Our experiments demonstrate that these transformations can be successfully learned from a public dataset of ECG signals and that, thanks to an optimized anomaly-detection algorithm, our solution enables online and long-term ECG monitoring.","Online and long-term ECG monitoring, Anomaly detection, Domain adaptation, Wearable devices, Sparse representations",Diego Carrera and Beatrice Rossi and Pasqualina Fragneto and Giacomo Boracchi,https://www.sciencedirect.com/science/article/pii/S0031320318304102,https://doi.org/10.1016/j.patcog.2018.11.019,0031-3203,2019,482--492,88,Pattern Recognition,Online anomaly detection for long-term ECG monitoring using wearable devices,article,CARRERA2019482,
"Histopathological image analysis works as âgold standardâ for cancer diagnosis. Its computer-aided approach has attracted considerable attention in the field of digital pathology, which highly depends on the feature representation for histopathological images. The principal component analysis network (PCANet) is a novel unsupervised deep learning framework that has shown its effectiveness for feature representation learning. However, PCA is susceptible to noise and outliers to affect the performance of PCANet. The Grassmann average (GA) is superior to PCA on robustness. In this work, a GA network (GANet) algorithm is proposed by embedding GA algorithm into the PCANet framework. Moreover, since quaternion algebra is an excellent tool to represent color images, a quaternion-based GANet (QGANet) algorithm is further developed to learn effective feature representations containing color information for histopathological images. The experimental results based on three histopathological image datasets indicate that the proposed QGANet achieves the best performance on the classification of color histopathological images among all the compared algorithms.","Principal component analysis network (PCANet), Quaternion algebra, Quaternion Grassmann averages network (QGANet), Color histopathological image",Jun Shi and Xiao Zheng and Jinjie Wu and Bangming Gong and Qi Zhang and Shihui Ying,https://www.sciencedirect.com/science/article/pii/S0031320318304345,https://doi.org/10.1016/j.patcog.2018.12.013,0031-3203,2019,67--76,89,Pattern Recognition,Quaternion Grassmann average network for learning representation of histopathological image,article,SHI201967,
"Spectral-clustering based methods have recently attracted considerable attention in the field of subspace segmentation. The approximately block-diagonal graphs achieved by this kind of methods usually contain some noise, i.e., nonzero elements in the off-diagonal region, due to outlier contamination or complex intrinsic structure of the dataset. In the experiment of most previous work, the number of the subspaces is often no more than 10. In this situation, this kind of noise almost has no influence on the segmentation results. However, the segmentation performance could be negatively affected by the noise when the number of subspaces is large, which is quite common in the real-world applications. In this paper, we address the problem of LSN subspace segmentation, i.e., large subspace number subspace segmentation. We first show that the approximately block-diagonal graph with the smaller difference in its diagonal blocks will be more robust to the off-diagonal noise mentioned above. Then, by using the infinity norm to control the bound of the difference in the diagonal blocks, we propose infinity norm minimization for LSN subspace segmentation. Experimental results demonstrate the effectiveness of our method.","Subspace segmentation, Large subspace number, Infinity norm, Spectral-clustering based methods",Kewei Tang and Zhixun Su and Yang Liu and Wei Jiang and Jie Zhang and Xiyan Sun,https://www.sciencedirect.com/science/article/pii/S0031320318304461,https://doi.org/10.1016/j.patcog.2018.12.025,0031-3203,2019,45--54,89,Pattern Recognition,Subspace segmentation with a large number of subspaces using infinity norm minimization,article,TANG201945,
"Fingerprint minutiae information is an unordered and variable-sized collection of minutiae locations and orientations. Advanced template protection algorithms which require a fixed-length binary template cannot operate on minutiae points. In this paper, we propose a novel framework that provides practical solutions that can be used in developing secure fingerprint verification systems. The framework, by using a GMM-SVM fingerprint representation scheme, first generates fixed-length feature vectors from minutiae point sets. The fixed-length representation enables the application of modern cryptographic alternatives based on homomorphic encryption to minutiae template protection. Our framework then utilizes an asymmetric locality sensitive hashing (ALSH) in order to convert the generated fixed-length but real valued GMM-SVM feature vector to a binary bit string. This binarization step transforms the matching process to calculating Hamming distance between binary vectors and expedites fingerprint matching. The verification performance of the framework is evaluated on FVC2002DB1A and DB2A databases.","Fingerprint verification, Biometric template protection, Minutiae protection, Fixed-length minutiae representation",Berkay Topcu and Hakan Erdogan,https://www.sciencedirect.com/science/article/pii/S0031320318304187,https://doi.org/10.1016/j.patcog.2018.11.029,0031-3203,2019,409--420,88,Pattern Recognition,Fixed-length asymmetric binary hashing for fingerprint verification through GMM-SVM based representations,article,TOPCU2019409,
"Sparse representation-based brain functional network modeling often results in large inter-subject variability in the network structure. This could reduce the statistical power in group comparison, or even deteriorate the generalization capability of the individualized diagnosis of brain diseases. Although group sparse representation (GSR) can alleviate such a limitation by increasing network similarity across subjects, it could, in turn, fail in providing satisfactory separability between the subjects from different groups (e.g., patients vs. controls). In this study, we propose to integrate individual functional connectivity (FC) information into the GSR-based network construction framework to achieve higher between-group separability while maintaining the merit of within-group consistency. Our method was based on an observation that the subjects from the same group have generally more similar FC patterns than those from different groups. To this end, we propose our new method, namely âstrength and similarity guided GSR (SSGSR)â, which exploits both BOLD signal temporal correlation-based âlow-orderâ FC (LOFC) and inter-subject LOFC-profile similarity-based âhigh-orderâ FC (HOFC) as two priors to jointly guide the GSR-based network modeling. Extensive experimental comparisons are carried out, with the rs-fMRI data from mild cognitive impairment (MCI) subjects and healthy controls, between the proposed algorithm and other state-of-the-art brain network modeling approaches. Individualized MCI identification results show that our method could achieve a balance between the individually consistent brain functional network construction and the adequately maintained inter-group brain functional network distinctions, thus leading to a more accurate classification result. Our method also provides a promising and generalized solution for the future connectome-based individualized diagnosis of brain disease.","Alzheimers disease, Mild cognitive impairment, Resting-state functional magnetic resonance imaging (rs-fMRI), Functional connectivity, Brain functional network, Group sparse representation, Diagnosis",Yu Zhang and Han Zhang and Xiaobo Chen and Mingxia Liu and Xiaofeng Zhu and Seong-Whan Lee and Dinggang Shen,https://www.sciencedirect.com/science/article/pii/S0031320318304229,https://doi.org/10.1016/j.patcog.2018.12.001,0031-3203,2019,421--430,88,Pattern Recognition,Strength and similarity guided group-level brain functional network construction for MCI diagnosis,article,ZHANG2019421,
"Traditional collaborative representation based classification (CRC) method usually faces the challenge of data uncertainty hence results in poor performance, especially in the presence of appearance variations in pose, expression and illumination. To overcome this issue, this paper presents a CRC-based face classification method by jointly using block weighted LBP and analysis dictionary learning. To this end, we first design a block weighted LBP histogram algorithm to form a set of local histogram-based feature vectors instead of using raw images. By this means we are able to effectively decrease data redundancy and uncertainty derived from image noises and appearance variations. Second, we adopt an analysis dictionary learning model as the projection transform to construct an analysis subspace, in which a new sample is characterized with the improved sparsity of its reconstruction coefficient vector. The crucial role of the analysis dictionary learning method in CRC is revealed by its capacity of the collaborative representation in an analytic coefficient space. Extensive experimental results conducted on a set of well-known face databases demonstrate the merits of the proposed method.","Analysis dictionary learning, Block weighted LBP, Collaborative representation based classification, Face classification",Xiaoning Song and Youming Chen and Zhen-Hua Feng and Guosheng Hu and Tao Zhang and Xiao-Jun Wu,https://www.sciencedirect.com/science/article/pii/S0031320318303935,https://doi.org/10.1016/j.patcog.2018.11.008,0031-3203,2019,127--138,88,Pattern Recognition,Collaborative representation based face classification exploiting block weighted LBP and analysis dictionary learning,article,SONG2019127,
"The use of sparse representation (SR) and collaborative representation (CR) for pattern classification has been widely studied in tasks such as face recognition and object categorization. Despite the success of SR/CR based classifiers, it is still arguable whether it is the â1-norm sparsity or the â2-norm collaborative property that brings the success of SR/CR based classification. In this paper, we investigate the use of nonnegative representation (NR) for pattern classification, which is largely ignored by previous work. Our analyses reveal that NR can boost the representation power of homogeneous samples while limiting the representation power of heterogeneous samples, making the representation sparse and discriminative simultaneously and thus providing a more effective solution to representation based classification than SR/CR. Our experiments demonstrate that the proposed NR based classifier (NRC) outperforms previous representation based classifiers. With deep features as inputs, it also achieves state-of-the-art performance on various visual classification tasks.","Pattern classification, Nonnegative representation, Collaborative representation, Sparse representation",Jun Xu and Wangpeng An and Lei Zhang and David Zhang,https://www.sciencedirect.com/science/article/pii/S003132031830445X,https://doi.org/10.1016/j.patcog.2018.12.023,0031-3203,2019,679--688,88,Pattern Recognition,"Sparse, collaborative, or nonnegative representation: Which helps pattern classification?",article,XU2019679,
"Nowadays, Convolutional Neural NetworkÂ (CNN) has achieved great success in various computer vision tasks. However, in classic CNN models, convolution and fully connected (FC) layers just perform linear transformations to their inputs. Non-linearity is often added by activation and pooling layers. It is natural to explore and extend convolution and FC layers non-linearly with affordable costs. In this paper, we first investigate the power mean function, which is proved effective and efficient in SVM kernel learning. Then, we investigate the power mean kernel, which is a non-linear kernel having linear computational complexity with the asymmetric kernel approximation function. Motivated by this scalable kernel, we propose Power Mean Transformation, which nonlinearizes both convolution and FC layers. It only needs a small modification on current CNNs, and improves the performance with a negligible increase of model size and running time. Experiments on various tasks show that Power Mean Transformation can improve classification accuracy, bring generalization ability and add different non-linearity to CNN models. Large performance gain on tiny models shows that Power Mean Transformation is especially effective in resource restricted deep learning scenarios like mobile applications. Finally, we add visualization experiments to illustrate why Power Mean Transformation works.","Non-linearity in deep learning, Pre-trained CNN models, Object recognition, Transfer learning",Chen-Lin Zhang and Jianxin Wu,https://www.sciencedirect.com/science/article/pii/S0031320318304503,https://doi.org/10.1016/j.patcog.2018.12.029,0031-3203,2019,12--21,89,Pattern Recognition,Improving CNN linear layers with power mean non-linearity,article,ZHANG201912,
"Clustering is an essential data mining tool that aims to discover inherent cluster structure in data. For most applications, applying clustering is only appropriate when cluster structure is present. As such, the study of clusterability, which evaluates whether data possesses such structure, is an integral part of cluster analysis. However, methods for evaluating clusterability vary radically, making it challenging to select a suitable measure. In this paper, we perform an extensive comparison of measures of clusterability and provide guidelines that clustering users can reference to select suitable measures for their applications.","Clusterability, Cluster structure, Cluster tendency, Dimension reduction, Multimodality tests",Andreas Adolfsson and Margareta Ackerman and Naomi C. Brownstein,https://www.sciencedirect.com/science/article/pii/S0031320318303777,https://doi.org/10.1016/j.patcog.2018.10.026,0031-3203,2019,13--26,88,Pattern Recognition,"To cluster, or not to cluster: An analysis of clusterability methods",article,ADOLFSSON201913,
"Various factors such as identity-specific attributes, pose, illumination and expression affect the appearance of face images. Disentangling the identity-specific factors is potentially beneficial for facial expression recognition (FER). Existing image-based FER systems either use hand-crafted or learned features to represent a single face image. In this paper, we propose a novel FER framework, named identity-disentangled facial expression recognition machine (IDFERM), in which we untangle the identity from a query sample by exploiting its difference from its references (e.g., its mined or generated frontal and neutral normalized faces). We demonstrate a possible ârecognition via generationâ scheme which consists of a novel hard negative generation (HNG) network and a generalized radial metric learning (RML) network. For FER, generated normalized faces are used as hard negative samples for metric learning. The difficulty of threshold validation and anchor selection are alleviated in RML and its distance comparisons are fewer than those of traditional deep metric learning methods. The expression representations of RML achieve superior performance on the CKâ¯+â¯, MMI and Oulu-CASIA datasets, given a single query image for testing.","Hard negative generation, Adaptive metric learning, Face normalization, Facial expression recognition",Xiaofeng Liu and B.V.K. {Vijaya Kumar} and Ping Jia and Jane You,https://www.sciencedirect.com/science/article/pii/S0031320318303819,https://doi.org/10.1016/j.patcog.2018.11.001,0031-3203,2019,1--12,88,Pattern Recognition,Hard negative generation for identity-disentangled facial expression recognition,article,LIU20191,
"In this research, we have developed a new algorithm to compute the moments defined in a rectangular region. By applying the recurrent formulas, symmetry properties, and particularly the parallelized matrix operations, our proposed computational method can improve the efficiency of computing Legendre, Gegenbauer, and Jacobi moments extensively with highly satisfied accuracy. To verify this new computational algorithm, the image reconstructions from the higher orders of Legendre, Gegenbauer, and Jacobi moments are performed on a testing image sized at 1024â¯Ãâ¯1024 with very encouraging results. It took only a few seconds to compute moments and conduct the image reconstructions from the 1000-th order of the Legendre, Gegenbauer, and Jacobi moments with the PSNR values up to 45. By utilizing our new algorithm, image analysis and recognition applications using the higher orders of moments defined in a rectangular region in the range of milliseconds will be possible.","Real-time moment computing, Legendre moments, Gegenbauer moments, Jacobi moments",Marcel Nwali and Simon Liao,https://www.sciencedirect.com/science/article/pii/S0031320319300019,https://doi.org/10.1016/j.patcog.2019.01.001,0031-3203,2019,151--160,89,Pattern Recognition,A new fast algorithm to compute continuous moments defined in a rectangular region,article,NWALI2019151,
"Most current fingerprint indexing systems are based on minutiae-only local structures and index local features directly. For minutiae local structure, missing and spurious neighboring minutiae significantly degrade the retrieval accuracy. To overcome this issue, we employs deep convolutional neural network to learn a minutia descriptor representing the local ridge structures. Instead of indexing local features, we aggregate various number of learned Minutia-centred Deep Convolutional (MDC) features of one fingerprint into a fixed-length feature vector to improve retrieval efficiency. In this paper, a novel aggregating method is proposed, which employs 1-D convolutional neural network to learn a discriminative and compact representation of fingerprint. In order to understand the MDC feature, a steerable fingerprint generation method is proposed to verify that it describes the attributes of minutiae and ridges. Comprehensive experimental results on five benchmark databases show that the proposed method achieves better performance on accuracy and efficiency than other prominent approaches.","Fingerprint indexing, Deep convolutional neural network, Aggregating local features, Representation learning, Minutia descriptor",Dehua Song and Yao Tang and Jufu Feng,https://www.sciencedirect.com/science/article/pii/S0031320318304072,https://doi.org/10.1016/j.patcog.2018.11.018,0031-3203,2019,397--408,88,Pattern Recognition,Aggregating minutia-centred deep convolutional features for fingerprint indexing,article,SONG2019397,
"Accurate segmentation of retinal vessel from fundus image is a prerequisite for the computer-aided diagnosis of ophthalmology diseases. In this paper, we propose a novel and robust cascade classification framework for retinal vessel segmentation. Our classification model envelops a set of computationally efficient Mahalanobis distance classifiers to form a highly nonlinear decision. Different from other nonlinear classifiers that need a predefined nonlinear kernel or need iterative training, the proposed cascade classification framework is trained by a one-pass feed forward process. Thus, the degree of nonlinearity of the proposed classifier is not predefined, but determined by the complexity of the data structure. Experimental evaluations on three diverse publicly available databases show that the proposed cascade classification framework achieves 95.41â96.40% vessel segmentation accuracy, and outperforms the state-of-the-art methods in terms of F1-score and Matthew correlation coefficient consistently on all three diverse databases. A qualitative comparison between different segmentation approaches demonstrates the superiority of the proposed method in dealing with typically challenging retinal structures. The proposed cascade classification framework consistently yields a high performance for retinal vessel segmentation, and delineates a more complete and accurate vessel tree. As an adaptive and effective solution to the difficult classification problems, the proposed technique can be flexibly extended to other image recognition tasks.","Fundus image, Retinal vessel segmentation, Cascade classification, Dimensionality reduction",Xiaohong Wang and Xudong Jiang and Jianfeng Ren,https://www.sciencedirect.com/science/article/pii/S0031320318304199,https://doi.org/10.1016/j.patcog.2018.11.030,0031-3203,2019,331--341,88,Pattern Recognition,Blood vessel segmentation from fundus image by a cascade classification framework,article,WANG2019331,
"Geosensor data forecasting has high practical value in government affairs such as prompt response and decision making. However, the spatial correlation across distinct sites and the temporal correlation within each site pose challenges to accurate forecasting. In this paper, a geosensor data forecasting tensor framework for significant societal events is proposed. Specifically, a tensor pattern is used to model the geosensor data, based on which a tensor decomposition algorithm is then developed to estimate future values of geosensor data. The proposed approach not only combines and utilizes the multi-mode correlations, but also well extracts the underlying factors in each mode of tensor and mines the multi-dimensional structures of geosensor data. In addition, a rank increasing strategy is used to determine tensor rank automatically, and a sliding window strategy is used to improve the prediction accuracy. Extensive experimental evaluations illustrate the superiority of our approach compared with the state-of-the-arts.","Internet of things (IoTs), Significant societal events, Geosensor data, Forecasting, Tensor decomposition",Lihua Zhou and Guowang Du and Ruxin Wang and Dapeng Tao and Lizhen Wang and Jun Cheng and Jing Wang,https://www.sciencedirect.com/science/article/pii/S0031320318303662,https://doi.org/10.1016/j.patcog.2018.10.021,0031-3203,2019,27--37,88,Pattern Recognition,A tensor framework for geosensor data forecasting of significant societal events,article,ZHOU201927,
"In this paper, we propose a linear classifier design method in the weight space. A linear classifier is completely determined by a weight vector. To design a linear classifier is equivalent to finding a weight vector. When there are a number of training samples, each training sample represents a plane in the weight space. On one side of the plane, the training sample is correctly classified while it is incorrectly classified on the other side. Thus, finding the optimal linear classifier can be formulated as finding a subspace that provides the best classification accuracy. In this paper, we have developed an algorithm to find such optimal subspaces in the weight space. Although we could not find the optimal linear classifier due to prohibitive computational complexity, the proposed design method produced noticeable improvements in some cases. Experimental results show that the proposed linear classifier performed better than or equivalent to existing linear classifiers.","Interior point, Linear classifier, Optimal linear classifier, Pyramid subspace, Weight space",Chulhee Lee and Seongyoun Woo,https://www.sciencedirect.com/science/article/pii/S0031320318304114,https://doi.org/10.1016/j.patcog.2018.11.024,0031-3203,2019,210--222,88,Pattern Recognition,Linear classifier design in the weight space,article,LEE2019210,
"Vehicle re-identification (Re-ID) is one of the primary components of an automated visual surveillance system. It aims to automatically identify/search vehicles in a multi-camera network usually having non-overlapping field-of-views. Majority of the approaches dealing with the re-ID problem tackle it in a supervised manner which have certain limitations that pose challenges of generalization e.g., large amount of annotated data is required for training and is often limited to the dynamic growth of the data. Unsupervised learning techniques can potentially cope with such issues by drawing inference directly from the unlabeled input data and have been effectively employed in the context of person re-ID. To this end, this paper presents an approach that essentially formulates the whole vehicle re-ID problem into an unsupervised learning paradigm using a progressive two step cascaded framework. It combines a CNN architecture for feature extraction and an unsupervised technique to enable self-paced progressive learning. It also incorporates the contextual information into the proposed progressive framework that significantly improves the convergence of the learned algorithm. Moreover, the approach is generic and has been the first attempt to tackle the vehicle re-ID problem in an unsupervised manner. The performance of the proposed algorithm has been thoroughly analyzed over two large publically available benchmark datasets VeRi and VehicleID for vehicle re-ID using image-to-image and cross-camera search strategies and achieved better performance in comparison to current state-of-the-art approaches using standard evaluation metrics.","Vehicle re-id, Deep learning, Unsupervised, Clustering, Visual surveillance, Progressive learning, Self pace",R.M.S. Bashir and M. Shahzad and M.M. Fraz,https://www.sciencedirect.com/science/article/pii/S0031320319300147,https://doi.org/10.1016/j.patcog.2019.01.008,0031-3203,2019,52--65,90,Pattern Recognition,VR-PROUD: Vehicle Re-identification using PROgressive Unsupervised Deep architecture,article,BASHIR201952,
"In this paper, we propose a novel and efficient multi-stage approach, which combines both semi-supervised learning and fine-grained learning to improve the performance of classification model learned only from a few samples. The fine-grained category recognition process utilized in our method is dubbed as MSR. In this process, we cut images into multi-scaled parts to feed into the network to learn more fine-grained features. By assigning these image cuts with dynamic weights, we can reduce the negative impact of background information and thus achieve a more accurate prediction. Furthermore, we present the voted pseudo label (VPL) which is an efficient method of semi-supervised learning. In this approach, for unlabeled data, VPL picks up the classes with non-confused labels verified by the consensus prediction of different classification models. These two methods can be applied to most neural network models and training methods. Inspired from classifier-based adaptation, we also propose a mix deep CNN architecture (MixDCNN). Both the VPL and MSR are integrated with the MixDCNN. Comprehensive experiments demonstrate the effectiveness of VPL and MSR. Without bottles and jars, we achieve the state-of-the-art or even better performance in two fine-grained recognition tasks on the datasets of Stanford Dogs and CUB Birds, with the accuracy of 95.6% and 85.2%, respectively.","Semi-supervised learning, Fine-grained feature learning, Mixture of DCNNs, Image classification",Danyu Lai and Wei Tian and Long Chen,https://www.sciencedirect.com/science/article/pii/S0031320318304230,https://doi.org/10.1016/j.patcog.2018.12.002,0031-3203,2019,547--556,88,Pattern Recognition,Improving classification with semi-supervised and fine-grained learning,article,LAI2019547,
"Time series classification has attracted much attention in the last two decades. However, in many real-world applications, the acquisition of sufficient amounts of labeled training data is costly, while unlabeled data is usually easily to be obtained. In this paper, we study the problem of learning discriminative features (segments) from both labeled and unlabeled time series data. The discriminative segments are often referred to as shapelets. We present a new Semi-Supervised Shapelets Learning (SSSL for short) model to efficiently learn shapelets by using both labeled and unlabeled time series data. Briefly, SSSL engages both labeled and unlabeled time series data in an integrated model that considers the least squares regression, the power of the pseudo-labels, shapelets regularization, and spectral analysis. The experimental results on real-world data demonstrate the superiority of our approach over existing methods.","Time series, Feature selection, Semi-supervised learning, Classification",Haishuai Wang and Qin Zhang and Jia Wu and Shirui Pan and Yixin Chen,https://www.sciencedirect.com/science/article/pii/S0031320318304473,https://doi.org/10.1016/j.patcog.2018.12.026,0031-3203,2019,55--66,89,Pattern Recognition,Time series feature learning with labeled and unlabeled data,article,WANG201955,
"In this paper, we propose a Bayesian inference method for the generalized Gamma mixture model (GÎMM) based on variational expectation-maximization algorithm. Specifically, the shape parameters, the inverse scale parameters, and the mixing coefficients in the GÎMM are treated as random variables, while the power parameters are left as parameters without assigning prior distributions. The help function is designed to approximate the lower bound of the variational objective function, which facilitates the assignment of the conjugate prior distributions and leads to the closed-form update equations. On this basis, the variational E-step and the variational M-step are alternatively implemented to infer the posteriors of the variables and estimate the parameters. The computational demand is reduced by the proposed method. More importantly, the effective number of components of the GÎMM can be determined automatically. The experimental results demonstrate the effectiveness of the proposed method especially in modeling the asymmetric and heavy-tailed data.","Finite mixture models, Generalized Gamma distribution, Variational expectation-maximization (VEM), Maximum likelihood estimation, Extended factorized approximation",Chi Liu and Heng-Chao Li and Kun Fu and Fan Zhang and Mihai Datcu and William J. Emery,https://www.sciencedirect.com/science/article/pii/S0031320318303789,https://doi.org/10.1016/j.patcog.2018.10.025,0031-3203,2019,269--284,87,Pattern Recognition,Bayesian estimation of generalized Gamma mixture model based on variational EM algorithm,article,LIU2019269,
"Image-based pupil detection, which aims to find the pupil location in an image, has been an active research topic in computer vision community. Learning-based approaches can achieve preferable results given large amounts of training data with eye center annotations. However, there are limited publicly available datasets with accurate eye center annotations and it is unreliable and time-consuming for manually labeling large amounts of training data. In this paper, inspired by learning from synthetic data in Parallel Vision framework, we introduce a step of parallel imaging built upon Generative Adversarial Networks (GANs) to generate adversarial synthetic images. In particular, we refine the synthetic eye images by the improved SimGAN using adversarial training scheme. For the computational experiments, we further propose a coarse-to-fine pupil detection framework based on shape augmented cascade regression models learning from the adversarial synthetic images. Experiments on benchmark databases of BioID, GI4E, and LFW show that the proposed work performs significantly better over other state-of-the-art methods by leveraging the power of cascade regression and adversarial image synthesis.","Cascade regression, GANs, Pupil detection",Chao Gou and Hui Zhang and Kunfeng Wang and Fei-Yue Wang and Qiang Ji,https://www.sciencedirect.com/science/article/pii/S0031320318304357,https://doi.org/10.1016/j.patcog.2018.12.014,0031-3203,2019,584--594,88,Pattern Recognition,Cascade learning from adversarial synthetic images for accurate pupil detection,article,GOU2019584,
"Irregular text is widely used. However, it is considerably difficult to recognize because of its various shapes and distorted patterns. In this paper, we thus propose a multi-object rectified attention network (MORAN) for general scene text recognition. The MORAN consists of a multi-object rectification network and an attention-based sequence recognition network. The multi-object rectification network is designed for rectifying images that contain irregular text. It decreases the difficulty of recognition and enables the attention-based sequence recognition network to more easily read irregular text. It is trained in a weak supervision way, thus requiring only images and corresponding text labels. The attention-based sequence recognition network focuses on target characters and sequentially outputs the predictions. Moreover, to improve sensitivity of the attention-based sequence recognition network, a fractional pickup method is proposed for an attention-based decoder in the training phase. With the rectification mechanism, the MORAN can read both regular and irregular scene text. Extensive experiments on various benchmarks are conducted, which show that the MORAN achieves state-of-the-art performance. The source code is available.11https://github.com/Canjie-Luo/MORAN_v2.","Scene text recognition, Optical character recognition, Deep learning",Canjie Luo and Lianwen Jin and Zenghui Sun,https://www.sciencedirect.com/science/article/pii/S0031320319300263,https://doi.org/10.1016/j.patcog.2019.01.020,0031-3203,2019,109--118,90,Pattern Recognition,MORAN: A Multi-Object Rectified Attention Network for scene text recognition,article,LUO2019109,
"It is obvious to see that most of the datasets do not have exactly equal number of samples for each class. However, there are some tasks like detection of fraudulent transactions, for which class imbalance is overwhelming and one of the classes has very low (even less than 10% of the entire data) amount of samples. These tasks often fall under outlier detection. Moreover, there are some scenarios where there may be multiple subsets of the outlier class. In such cases, it should be treated as a multiple outlier type detection scenario. In this article, we have proposed a system that can efficiently handle all the aforementioned problems. We have used stacked autoencoders to extract features and then used an ensemble of probabilistic neural networks to do a majority voting and detect the outliers. Such a system is seen to have a better and reliable performance as compared to the other outlier detection systems in most of the datasets tested upon. It is seen that use of autoencoders clearly enhanced the outlier detection performance.","Deep learning, Autoencoders, Probabilistic neural networks, Ensemble learning, Outlier detection",Debasrita Chakraborty and Vaasudev Narayanan and Ashish Ghosh,https://www.sciencedirect.com/science/article/pii/S0031320319300020,https://doi.org/10.1016/j.patcog.2019.01.002,0031-3203,2019,161--171,89,Pattern Recognition,Integration of deep feature extraction and ensemble learning for outlier detection,article,CHAKRABORTY2019161,
"This paper proposes a new graph convolutional neural network architecture based on a depth-based representation of graph structure deriving from quantum walks, which we refer to as the quantum-based subgraph convolutional neural network (QS-CNNs). This new architecture captures both the global topological structure and the local connectivity structure within a graph. Specifically, we commence by establishing a family of K-layer expansion subgraphs for each vertex of a graph by quantum walks, which captures the global topological arrangement information for substructures contained within a graph. We then design a set of fixed-size convolution filters over the subgraphs, which helps to characterise multi-scale patterns residing in the data. The idea is to apply convolution filters sliding over the entire set of subgraphs rooted at a vertex to extract the local features analogous to the standard convolution operation on grid data. Experiments on eight graph-structured datasets demonstrate that QS-CNNs architecture is capable of outperforming fourteen state-of-the-art methods for the tasks of node classification and graph classification.","Graph convolutional neural networks, Spatial construction, Quantum walks, Subgraph",Zhihong Zhang and Dongdong Chen and Jianjia Wang and Lu Bai and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320318303820,https://doi.org/10.1016/j.patcog.2018.11.002,0031-3203,2019,38--49,88,Pattern Recognition,Quantum-based subgraph convolutional neural networks,article,ZHANG201938,
"Procedural model fitting (PMF) is a generalization of classical model fitting and has numerous applications for computer vision and computer graphics. The task of PMF is to search a geometric model set for the model that is most similar to a set of data points. We propose a strict and robust similarity estimator for PMF to handle imperfect data. The proposed estimator is based on the error from model to data, while most other estimators are based on the error from data to model. We then use the proposed estimator to guide the cuckoo search algorithm to search for the most similar model. To accelerate the search process, we also propose a coarse-to-fine model dividing strategy to early reject dissimilar models. In this paper, the proposed PMF method is applied to fit building models on laser scanning data. It is also applied to fit character models on eighteen variants of imperfect MNIST data to achieve few-shot pattern recognition. In the 5-shot recognition, our method outperforms the state-of-the-art method on thirteen variants of the imperfect data. In particular, for one of the data corrupted by grid lines, our method obtains a high accuracy of 65%, whereas the state-of-the-art method only obtains an accuracy of 30%.","Complex model fitting, Imperfect point set, Inverse procedural modeling, Probabilistic program induction, Few-shot pattern recognition",Zongliang Zhang and Jonathan Li and Yulan Guo and Xin Li and Yangbin Lin and Guobao Xiao and Cheng Wang,https://www.sciencedirect.com/science/article/pii/S0031320318302619,https://doi.org/10.1016/j.patcog.2018.07.027,0031-3203,2019,120--131,85,Pattern Recognition,Robust procedural model fitting with a new geometric similarity estimator,article,ZHANG2019120,
"Given two graphs, the graph matching problem is to align the two vertex sets so as to minimize the number of adjacency disagreements between the two graphs. The seeded graph matching problem is the graph matching problem when we are first given a partial alignment that we are tasked with completing. In this article, we modify the state-of-the-art approximate graph matching algorithm âFAQâ of Vogelstein etÂ al. (2015) to make it a fast approximate seeded graph matching algorithm, adapt its applicability to include graphs with differently sized vertex sets, and extend the algorithm so as to provide, for each individual vertex, a nomination list of likely matches. We demonstrate the effectiveness of our algorithm via simulation and real data experiments; indeed, knowledge of even a few seeds can be extremely effective when our seeded graph matching algorithm is used to recover a naturally existing alignment that is only partially observed.","Hungarian algorithm, Quadratic assignment problem (QAP), Vertex alignment",Donniell E. Fishkind and Sancar Adali and Heather G. Patsolic and Lingyao Meng and Digvijay Singh and Vince Lyzinski and Carey E. Priebe,https://www.sciencedirect.com/science/article/pii/S0031320318303431,https://doi.org/10.1016/j.patcog.2018.09.014,0031-3203,2019,203--215,87,Pattern Recognition,Seeded graph matching,article,FISHKIND2019203,
"In this paper, we propose an unsupervised representation method to learn principal orientations and residual descriptor (PORD) for action recognition. Our PORD aims to learn the statistic principal orientations and to represent the local features of action videos with residual values. The existing hand-crafted feature based methods require high prior knowledge and lack of the ability to represent the distribution of features of the dataset. Most of the deep learned feature based methods are data adaptive, but they do not consider the projection orientations of features nor the loss of locally aggregated descriptors of the quantization. We propose a method of principal orientations and residual descriptor considering that the principal orientations reflect the distribution of local features in the dataset and the residual of projection contains discriminative information of local features. Moreover, we propose a multi-modality PORD method by reducing the modality gap of the RGB channels and the depth channel at the feature level to make our method applicable to RGB-D action recognition. To evaluate the performance, we conduct experiments on five challenging action datasets: Hollywood2, UCF101, HMDB51, MSRDaily, and MSR-Pair. The results show that our method is competitive with the state-of-the-art methods.","Action recognition, Unsupervised learning, Trajectories, Principal orientation, Residual value",Lei Chen and Zhanjie Song and Jiwen Lu and Jie Zhou,https://www.sciencedirect.com/science/article/pii/S0031320318303157,https://doi.org/10.1016/j.patcog.2018.08.016,0031-3203,2019,14--26,86,Pattern Recognition,Learning principal orientations and residual descriptor for action recognition,article,CHEN201914,
"Multispectral images of color-thermal pairs have shown more effective than a single color channel for pedestrian detection, especially under challenging illumination conditions. However, there is still a lack of studies on how to fuse the two modalities effectively. In this paper, we deeply compare six different convolutional network fusion architectures and analyse their adaptations, enabling a vanilla architecture to obtain detection performances comparable to the state-of-the-art results. Further, we discover that pedestrian detection confidences from color or thermal images are correlated with illumination conditions. With this in mind, we propose an Illumination-aware Faster R-CNN (IAF R-CNN). Specifically, an Illumination-aware Network is introduced to give an illumination measure of the input image. Then we adaptively merge color and thermal sub-networks via a gate function defined over the illumination value. The experimental results on KAIST Multispectral Pedestrian Benchmark validate the effectiveness of the proposed IAF R-CNN.","Multispectral pedestrian detection, Illumination-aware, Gated fusion",Chengyang Li and Dan Song and Ruofeng Tong and Min Tang,https://www.sciencedirect.com/science/article/pii/S0031320318303030,https://doi.org/10.1016/j.patcog.2018.08.005,0031-3203,2019,161--171,85,Pattern Recognition,Illumination-aware faster R-CNN for robust multispectral pedestrian detection,article,LI2019161,
"Ear characteristic is a promising biometric modality that has demonstrated good biometric performance. In this paper, we investigate a novel and challenging problem to verify a subject (or user) based on the ear characteristics after undergoing ear surgery. Ear surgery is performed to reconstruct the abnormal ear structures both locally and globally to beautify the overall appearance of the ear. Ear surgery performed for both for beautification and corrections alters the original ear characteristics to the greater extent that will challenge the comparison and subsequently verification performance of the ear recognition systems. This work presents a new database of images from 211 subjects with surgically altered ear along with corresponding pre and post-surgery samples. We then propose a novel scheme for ear verification based on the features extracted using a bank of filters learnt using Topographic Locally Competitive Algorithm (T-LCA) and comparison is carried out using Robust Probabilistic Collaborative Representation Classifier (R-ProCRC). Extensive experiments are carried out on both clean (normal) and surgically altered ear database to evaluate the performance of the proposed ear verification scheme. We also present a comprehensive performance analysis by comparing the performance of the proposed ear recognition scheme with eight different state-of-the-art ear verification system. Furthermore, we also present a new scheme to detect both deformed and surgically altered ear using one-class classification. Experimental results indicate the magnitude of problem in verifying the surgically altered ears and the signifies the need for considerable research in this direction.","Ear recognition, Surgery, Locally competitive algorithm, Collaborative representation",R. Raghavendra and Kiran B. Raja and Sushma Venkatesh and Christoph Busch,https://www.sciencedirect.com/science/article/pii/S0031320318302188,https://doi.org/10.1016/j.patcog.2018.06.008,0031-3203,2018,416--429,83,Pattern Recognition,Improved ear verification after surgery - An approach based on collaborative representation of locally competitive features,article,RAGHAVENDRA2018416,
"We formulate the task of density based clustering as energy minimization, using both binary/pairwise energy term and unary/data energy term (the latter was largely ignored in previous clustering methods). Binary energy is defined in terms of inhomogeneity in local point density. While most previous methods use binary/pairwise energy only, the unary/data energy can represent the natural tendency of a given point belonging to a given cluster, which is also crucial for the clustering. Since our energy is expressed as the sum of a unary (data) term and a binary (pairwise or smoothness) term, we can thus make a perfect analogy with the energy model used in vision and borrow everything (such as the optimization algorithms) from vision field to clustering under this mapping. This correspondence provides an entirely new view point in handling the clustering problem, and in fact many mature methods and algorithms are already provided in the vision field and can be adopted by the clustering field readily. During our energy optimization, a sequence of energy minima are found to recursively partition the points, and thus find a hierarchical embedding of clusters that are increasingly homogeneous in density. Disjoint clusters with the same density are identified separately. Our clustering method is totally unsupervised (which is superior to most existing methods, as those listed below). It does not need any user input parameters (say number of segments, any bandwidth parameter, cutoff distance/scale, etc.), except one can specify the homogeneity criterion â the degree of acceptable fluctuation in density within a cluster (which is target-related), or let it be specified automatically in a hierarchical way. We conduct experiments on both synthetic datasets and real-image tasks. Experimental results on synthetic datasets show that our method is able to handle clusters of different shapes, sizes and densities. We present the performance of our approach using the commonly used energy optimization algorithms from vision such as ICM, LBP, Graph-cut, Mean field theory algorithm, as well as the integer programming algorithm. We also contrast our performance with several other commonly used clustering algorithms, such as k-means, fussy c-means, DBSCAN, as well as a recent state-of-the-art clustering method as reported in [40]. Our experiments on real-image tasks further validate the performance of the method. In addition, we show that the family of commonly used spectral, graph clustering algorithms (such as Normalized-cut) uses only the binary energy term while ignoring the unary energy term; therefore, their energy model is incomplete compared with ours.","Unsupervised/hierarchical clustering, Energy minimization, Statistical physics, Integer programming, Unary/data energy, Ising model, Local density, Connected component analysis, Image segmentation, Normalized-cut",Huiguang Yang and Narendra Ahuja,https://www.sciencedirect.com/science/article/pii/S0031320318303273,https://doi.org/10.1016/j.patcog.2018.09.008,0031-3203,2019,265--280,86,Pattern Recognition,Clustering as physically inspired energy minimization,article,YANG2019265,
"Enhancement, followed by segmentation, quantification and modelling of blood vessels in retinal images plays an essential role in computer-aided retinopathy diagnosis. In this paper, we introduce the bowler-hat transform method a new approach based on mathematical morphology for vessel enhancement. The proposed method combines different structuring elements to detect innate features of vessel-like structures. We evaluate the proposed method qualitatively and quantitatively and compare it with the state-of-the-art methods using both synthetic and real datasets. Our results establish that the proposed method achieves high-quality vessel-like structure enhancement in both synthetic examples and clinically relevant retinal images. The bowler-hat transform is shown to be able to detect fine vessels while still remaining robust at junctions.","Image enhancement, Mathematical morphology, Bowler-hat transform, Blood vessel enhancement",ÃiÄdem Sazak and Carl J. Nelson and Boguslaw Obara,https://www.sciencedirect.com/science/article/pii/S0031320318303558,https://doi.org/10.1016/j.patcog.2018.10.011,0031-3203,2019,739--750,88,Pattern Recognition,The multiscale bowler-hat transform for blood vessel enhancement in retinal images,article,SAZAK2019739,
"This paper proposes three measures to quantify the characteristics of online signature templates in terms of distinctiveness, complexity and repeatability. A distinctiveness measure of a signature template is computed from a set of enrolled signature samples and a statistical assumption about random signatures. Secondly, a complexity measure of the template is derived from a set of enrolled signature samples. Finally, given a signature template, a measure to quantify the repeatability of the online signature is derived from a validation set of samples. These three measures can then be used as an indicator for the performance of the system in rejecting random forgery samples and skilled forgery samples and the performance of users in providing accepted genuine samples, respectively. The effectiveness of these three measures and their applications are demonstrated through experiments performed on three online signature datasets and one keystroke dynamics dataset using different verification algorithms.","Biometric, Online signature, Biometric template quality, Distinctiveness, Complexity, Repeatability",Napa Sae-Bae and Nasir Memon and Pitikhate Sooraksa,https://www.sciencedirect.com/science/article/pii/S0031320318302589,https://doi.org/10.1016/j.patcog.2018.07.024,0031-3203,2018,332--344,84,Pattern Recognition,"Distinctiveness, complexity, and repeatability of online signature templates",article,SAEBAE2018332,
"Recently, weakly-supervised object detection has attracted much attention, since it does not require expensive bounding-box annotations while training the network. Although significant progress has also been made, there is still a large gap on the performance between weakly-supervised and fully-supervised object detection. To mitigate this gap, some works try to use the pseudo ground truths generated by a weakly-supervised detector to train a supervised detector. However, such approaches incline to find the most representative parts instead of the whole body of an object, and only seek one ground truth bounding-box per class even though many same-class instances exist in an image. To address these issues, we propose a weakly-supervised to fully-supervised framework (W2F), where a weakly-supervised detector is implemented using multiple instance learning. And then, we propose a pseudo ground-truth excavation (PGE) algorithm to find the accurate pseudo ground truth bounding-box for each instance. Moreover, the pseudo ground-truth adaptation (PGA) algorithm is designed to further refine those pseudo ground truths mined by PGE algorithm. Finally, the mined pseudo ground truths are used as supervision to train a fully-supervised detector. Additionally, we also propose an iterative ground-truth learning (IGL) approach, which enhances the quality of the pseudo ground truths by using the predictions of the fully-supervised detector iteratively. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 benchmarks strongly demonstrate the effectiveness of our method. We obtain 53.1% and 49.4% mAP on VOC2007 and VOC2012 respectively, which is a significant improvement over previous state-of-the-art methods.","Weakly-supervised learning, Object detection, Pseudo ground truth, Iterative learning, Deep learning",Yongqiang Zhang and Yaicheng Bai and Mingli Ding and Yongqiang Li and Bernard Ghanem,https://www.sciencedirect.com/science/article/pii/S0031320318302346,https://doi.org/10.1016/j.patcog.2018.07.005,0031-3203,2018,68--81,84,Pattern Recognition,Weakly-supervised object detection via mining pseudo ground truth bounding-boxes,article,ZHANG201868,
"Multi-view clustering is a hot research topic due to the urgent need for analyzing a vast amount of heterogeneous data. Although many multi-view clustering methods have been developed, yet they mostly neglect the view-insufficiency issue. That is, most of the existing multi-view clustering methods assume that each individual view is sufficient for discovering the cluster structure, which is however not guaranteed in real applications. In this paper, we propose a novel multi-view clustering method termed multi-view intact space clustering (MVIC), which is able to simultaneously recover the latent intact space from multiple insufficient views and discover the cluster structure from the intact space. For each view, a view generation function is designed to map the latent intact space representation into the view representation. With the view representation given, the latent intact space can be restored by mapping back from each individual view representation. Then matrix factorization is used to obtain the cluster structure in intact space by factorizing the latent intact space representation into the cluster centroids and the cluster assignments. Therefore, the proposed model is composed of two components, namely the reconstruction error of the latent intact space and the distortion error of data clustering in intact space. An alternating iterative method is designed to solve the optimization of the model, in which the latent intact space and the cluster structure are updated in an interplay manner. Experimental results conducted on a wide-range of multi-view datasets have confirmed the superiority of our method over state-of-the-art approaches.","Multi-view clustering, Latent intact space, View-insufficiency, Representation learning",Ling Huang and Hong-Yang Chao and Chang-Dong Wang,https://www.sciencedirect.com/science/article/pii/S003132031830342X,https://doi.org/10.1016/j.patcog.2018.09.016,0031-3203,2019,344--353,86,Pattern Recognition,Multi-view intact space clustering,article,HUANG2019344,
"In this paper, we propose the use of data symmetries, in the sense of equivalences under signal transformations, as priors for learning symmetry-adapted data representations, i.e., representations that are equivariant to these transformations. We rely on a group-theoretic definition of equivariance and provide conditions for enforcing a learned representation, for example the weights in a neural network layer or the atoms in a dictionary, to have the structure of a group and specifically the group structure in the distribution of the input. By reducing the analysis of generic group symmetries to permutation symmetries, we devise a regularization scheme for representation learning algorithm, using an unlabeled training set. The proposed regularization is aimed to be a conceptual, theoretical and computational proof of concept for symmetry-adapted representation learning, where the learned data representations are equivariant or invariant to transformations, without explicit knowledge of the underlying symmetries in the data.","Representation learning, Equivariant representations, Invariant representations, Dictionary learning, Convolutional neural networks, Regularization, Data transformations",Fabio Anselmi and Georgios Evangelopoulos and Lorenzo Rosasco and Tomaso Poggio,https://www.sciencedirect.com/science/article/pii/S0031320318302620,https://doi.org/10.1016/j.patcog.2018.07.025,0031-3203,2019,201--208,86,Pattern Recognition,Symmetry-adapted representation learning,article,ANSELMI2019201,
"Along with the emergence of domain adaptation in semi-supervised setting, dealing with the noisy and complex data in classifier adaptation underscores its growing importance. We believe a large amount of unlabeled data in target domain, which are always only used in distribution alignment, are more of a great source of information for this challenge. In this paper, we propose a novel Transfer Fredholm Multiple Kernel Learning (TFMKL) framework to suppress the noise for complex data distributions. Firstly, with exploring unlabeled target data, TFMKL learns a cross-domain predictive model by developing a Fredholm integral based kernel prediction framework which is proven to be effective in noise suppression. Secondly, TFMKL explicitly extends the applied range of unlabeled target samples into adaptive classifier building and distribution alignment. Thirdly, multiple kernels are explored to induce an optimal learning space. Correspondingly, TFMKL is distinguished with allowing for noise resiliency, facilitating knowledge transfer and analyzing complex data characteristics at the same time. Furthermore, an effective optimization procedure is presented based on the reduced gradient, guaranteeing rapid convergence. We emphasize the adaptability of TFMKL to different domain adaptation tasks due to its extension to different predictive models. In particular, two models based on square loss and hinge loss respectively are proposed within the TFMKL framework. Comprehensive empirical studies on benchmark data sets verify the effectiveness and the noise resiliency of our proposed methods.","Domain adaptation, Semi-supervised learning, Multiple kernel learning, Hilbert space embedding of distributions",Wei Wang and Hao Wang and Zhaoxiang Zhang and Chen Zhang and Yang Gao,https://www.sciencedirect.com/science/article/pii/S0031320318302747,https://doi.org/10.1016/j.patcog.2018.07.035,0031-3203,2019,185--197,85,Pattern Recognition,Semi-supervised domain adaptation via Fredholm integral based kernel methods,article,WANG2019185,
"This paper presents a robust method for rotation estimation of planar point sets using the Angular Radon Spectrum (ARS). Given a Gaussian Mixture Model (GMM) representing the point distribution, the ARS is a continuous function derived from the Radon Transform of such distribution. The ARS characterizes the orientation of a point distribution by measuring its alignment w.r.t. a pencil of parallel lines. By exploting its translation and angular-shift invariance, the rotation angle between two point sets can be estimated through the correlation of the corresponding spectra. Beside its definition, the novel contributions of this paper include the efficient computation of the ARS and of the correlation function through their Fourier expansion, and a new algorithm for assessing the rotation between two point sets. Moreover, experiments with standard benchmark datasets assess the performance of the proposed algorithm and other state-of-the-art methods in presence of noisy and incomplete data.","Rotation estimation, Gaussian mixture models",Dario {Lodi Rizzini},https://www.sciencedirect.com/science/article/pii/S0031320318302504,https://doi.org/10.1016/j.patcog.2018.07.017,0031-3203,2018,182--196,84,Pattern Recognition,Angular Radon spectrum for rotation estimation,article,LODIRIZZINI2018182,
"Crowdsourced data in science might be severely error-prone due to the inexperience of annotators participating in the project. In this work, we present a procedure to detect specific structures in an image given tags provided by multiple annotators and collected through a crowdsourcing methodology. The procedure consists of two stages based on the ExpectationâMaximization (EM) algorithm, one for clustering and the other one for detection, and it gracefully combines data coming from annotators with unknown reliability in an unsupervised manner. An online implementation of the approach is also presented that is well suited to crowdsourced streaming data. Comprehensive experimental results with real data from the MalariaSpot project are also included.","Crowdsourcing, Unreliable annotators, Unsupervised method, Online EM algorithm, MalariaSpot",Alba PagÃ¨s-Zamora and Margarita Cabrera-Bean and Carles DÃ­az-Vilor,https://www.sciencedirect.com/science/article/pii/S0031320318303200,https://doi.org/10.1016/j.patcog.2018.09.001,0031-3203,2019,209--223,86,Pattern Recognition,Unsupervised online clustering and detection algorithms using crowdsourced data for malaria diagnosis,article,PAGESZAMORA2019209,
"Script identification plays a significant role in analysing documents and videos. In this paper, we focus on the problem of script identification in scene text images and video scripts. Because of low image quality, complex background and similar layout of characters shared by some scripts like Greek, Latin, etc., text recognition in those cases become challenging. In this paper, we propose a novel method that involves extraction of local and global features using CNN-LSTM framework and weighting them dynamically for script identification. First, we convert the images into patches and feed them into a CNN-LSTM framework. Attention-based patch weights are calculated applying softmax layer after LSTM. Next, we do patch-wise multiplication of these weights with corresponding CNN to yield local features. Global features are also extracted from last cell state of LSTM. We employ a fusion technique which dynamically weights the local and global features for an individual patch. Experiments have been done in four public script identification datasets: SIW-13, CVSI2015, ICDAR-17 and MLe2e. The proposed framework achieves superior results in comparison to conventional methods.","Script identification, Convolutional neural network, Long short-term memory, Local feature, Global feature, Attention network, Dynamic weighting",Ankan Kumar Bhunia and Aishik Konwer and Ayan Kumar Bhunia and Abir Bhowmick and Partha P. Roy and Umapada Pal,https://www.sciencedirect.com/science/article/pii/S0031320318302590,https://doi.org/10.1016/j.patcog.2018.07.034,0031-3203,2019,172--184,85,Pattern Recognition,Script identification in natural scene image and video frames using an attention based Convolutional-LSTM network,article,BHUNIA2019172,
"This paper introduces novel insights to improve the state-of-the-art line-based unsupervised observation and abstraction models of man-made environments. The increasing use of autonomous UAVs inside buildings and around human-made structures demands new accurate and comprehensive representation of their operation environments. Most of the 3D scene abstraction methods use invariant feature point matching, nevertheless some sparse 3D point clouds do not concisely represent the structure of the environment. The presented approach is based on observation and representation models using the straight line segments. The goal of the work is a complete method based on the matching of lines, that provides a complementary approach to state-of-the-art methods when facing 3D scene representation of poor texture environments for future autonomous UAV. Oppositely to other recently published methods obtaining 3D line abstractions, the proposed method features 3D segment abstraction in the absence of a previously generated point based reconstruction. Another advantage is the ability to group the resulting 3D lines according to different planes, for exploiting coplanar line intersections. These intersections are used like feature points in the reconstruction process. It has been proved that this method exclusively based on lines can obtain spatial information in the adverse situations when a SIFT-like SfM pipeline fails to generate a dense point cloud.","3D abstraction, Reconstruction, Line-based sketching, UAV",Roi Santos and Xose M. Pardo and Xose R. Fdez-Vidal,https://www.sciencedirect.com/science/article/pii/S0031320318303443,https://doi.org/10.1016/j.patcog.2018.09.017,0031-3203,2019,354--367,86,Pattern Recognition,Scene wireframes sketching for Unmanned Aerial Vehicles,article,SANTOS2019354,
"In this paper a new nonparametric scene parsing approach is proposed which has three steps: image retrieval, label transferring and label gathering. In our approach, to incorporate the contextual knowledge in scene parsing, we propose to integrate both parametric and nonparametric context models into a unified framework. We adopt a co-occurrence graph to be our parametric context model to learn the co-occurrence frequency of objects. To consider different preferences of the co-occurring of one object with the other objects, the concept of co-occurring priority is introduced in this paper for the first time. Next, by using the learned co-occurrence graph and the context knowledge of the set of retrieved images, we propose new ways to incorporate contextual information in all three steps of nonparametric scene parsing approach. To evaluate our proposed approach, it is applied on MSRC-21 and SiftFlow datasets. The results show that our approach outperforms its competitors.","Scene parsing, Parametric, Nonparametric, Context model, Co-occurrence graph, Unified framework",Parvaneh Aliniya and Parvin Razzaghi,https://www.sciencedirect.com/science/article/pii/S0031320318302474,https://doi.org/10.1016/j.patcog.2018.07.013,0031-3203,2018,165--181,84,Pattern Recognition,Parametric and nonparametric context models: A unified approach to scene parsing,article,ALINIYA2018165,
"Prediction of complex human activities from a partially observed video is valuable in many practical applications but is a challenging problem. When a video is partially observed, maximizing the representational power of the given video is more important than modeling the temporal dynamics of the activity. In this paper, we propose a novel human activity descriptor for prediction, which can maximize the discriminative power of a system in a compact and efficient way using pre-trained deep networks. Specifically, the proposed descriptor can capture the potentially important pairwise relationships between objects without prior knowledge or preset attributes. The relationship information is automatically reflected during the descriptor construction procedure based on objectâs participation ratios, local and global motion activations. Pre-trained Convolutional Neural Networks are utilized without additional model training procedure. From a practical point of view, the proposed method is more cost-effective when implementing a smart surveillance system. In the experiments, we evaluate the proposed methods in two cases: (1) prediction accuracy with different observation ratios, and (2) the effect of pre-trained network and layer selection. Experimental results from five public datasets verified the efficacy of the proposed method by outperforming competing methods with stable high-performance regardless of network selection.","Pre-trained CNN, Human activity prediction, Human interaction, Sub-volume co-occurrence matrix",Dong-Gyu Lee and Seong-Whan Lee,https://www.sciencedirect.com/science/article/pii/S0031320318303042,https://doi.org/10.1016/j.patcog.2018.08.006,0031-3203,2019,198--206,85,Pattern Recognition,Prediction of partially observed human activity based on pre-trained deep representation,article,LEE2019198,
"Low-textured image stitching remains a challenging problem. It is difficult to achieve good alignment of images and it is easy to break images structures are often broken due to insufficient and unreliable point correspondences. Moreover, because of the viewpoint variations between multiple images, the stitched images suffer from projective distortions. To solve these problems, this paper presents a line-guided local warping method with a global similarity constraint for image stitching. Line features which serve well for geometric descriptions and scene constraints, are employed to guide image stitching accurately. On one hand, the line features are integrated into a local warping model through a designed weight function. On the other hand, line features are adopted to impose strong geometric constraints, including line correspondence and line colinearity, to improve the stitching performance through mesh optimization. To mitigate projective distortions, we adopt a global similarity constraint, which is integrated with the projective warps via a designed weight strategy. This constraint causes the final warp to slowly change from a projective to a similarity transformation across the image. Finally, the images undergo a two-stage alignment scheme that provides accurate alignment and reduces projective distortion. We evaluate our method on a series of images and compare it with several other methods. The experimental results demonstrate that the proposed method provides a convincing stitching performance and that it outperforms other state-of-the-art methods.","Image stitching, Image alignment, Similarity constraint, Line features, Local warping",Tian-Zhu Xiang and Gui-Song Xia and Xiang Bai and Liangpei Zhang,https://www.sciencedirect.com/science/article/pii/S0031320318302231,https://doi.org/10.1016/j.patcog.2018.06.013,0031-3203,2018,481--497,83,Pattern Recognition,Image stitching by line-guided local warping with global similarity constraint,article,XIANG2018481,
"Multi-view subspace clustering, which aims to partition a set of multi-source data into their underlying groups, has recently attracted intensive attention from the communities of pattern recognition and data mining. This paper proposes a novel multi-view subspace clustering model that attempts to form an informative intactness-aware similarity based on the intact space learning technique. More specifically, we learn an intact space by integrating encoded complementary information. An informative similarity matrix is simultaneously constructed, which enforces the constructed similarity to have maximum dependence with its latent intact points by adopting the HilbertâSchmidt Independence Criterion (HSIC). A new explanation on the advantages of such intactness-aware similarity has been provided (i.e., the similarity is learned according to the local connectivity). To effectively and efficiently seek the optimal solution of the associated problem, a new ADMM based algorithm is designed. Moreover, to show the merit of the proposed joint optimization, we also conduct the clustering in two separated steps. Extensive experimental results on six benchmark datasets are provided to reveal the effectiveness of the proposed algorithm and its superior performance over other state-of-the-art alternatives.","Intact space, Intactness-aware similarity, Multi-view subspace clustering",Xiaobo Wang and Zhen Lei and Xiaojie Guo and Changqing Zhang and Hailin Shi and Stan Z. Li,https://www.sciencedirect.com/science/article/pii/S0031320318303285,https://doi.org/10.1016/j.patcog.2018.09.009,0031-3203,2019,50--63,88,Pattern Recognition,Multi-view subspace clustering with intactness-aware similarity,article,WANG201950,
"In the context of image analysis, the Binary Partition Tree (BPT) is a classical data structure for the hierarchical modelling of images at different scales. BPTs belong both to the families of graph-based models and morphological hierarchies. They constitute an efficient way to define sets of nested partitions of image support, that further provide knowledge-guided reduced research spaces for optimization-based segmentation procedures. Basically, a BPT is built in a mono-feature way, i.e. for one given image, and one given metric, by merging pairs of connected image regions that are similar in the induced feature space. Our goal is to design a new family of BPTs, dealing with the need to directly manage multiple features within its building process. Then, we propose a generalization of the BPT construction framework, allowing one to embed multiple features. The cornerstone of our approach relies on a collaborative strategy used to establish a consensus between different metrics, thus enabling to obtain a unified hierarchical segmentation space. In particular, this provides alternatives to the complex issue of metric construction from several âpossibly non-comparableâ features. To reach that goal, we first revisit the BPT construction algorithm to describe it in a graph-based formalism. Then, we present the structural and algorithmic evolutions and impacts when embedding multiple features in BPT construction. Final experiments illustrate how this multi-feature framework can be used to build BPTs from multiple metrics computed through the (potentially multiple) image content(s).","Binary Partition Tree, Morphological hierarchies, Multiple features, Graph-based image processing, Image segmentation",Jimmy Francky Randrianasoa and Camille Kurtz and Ãric Desjardin and Nicolas Passat,https://www.sciencedirect.com/science/article/pii/S0031320318302358,https://doi.org/10.1016/j.patcog.2018.07.003,0031-3203,2018,237--250,84,Pattern Recognition,Binary Partition Tree construction from multiple features for image segmentation,article,RANDRIANASOA2018237,
"Convolutional Neural Network based action recognition methods have achieved significant improvements in recent years. The 3D convolution extends the 2D convolution to the spatial-temporal domain for better analysis of human activities in videos. The 3D convolution, however, involves many more parameters than the 2D convolution. Thus, it is much more expensive on computation, costly on storage, and difficult to learn. This work proposes efficient asymmetric one-directional 3D convolutions to approximate the traditional 3D convolution. To improve the feature learning capacity of asymmetric 3D convolutions, a set of local 3D convolutional networks, called MicroNets, are proposed by incorporating multi-scale 3D convolution branches. Then, an asymmetric 3D-CNN deep model is constructed by MicroNets for the action recognition task. Moreover, to avoid training two networks on the RGB and Flow frames separately as most works do, a simple but effective multi-source enhanced input is proposed, which fuses useful information of the RGB and Flow frame at the pre-processing stage. The asymmetric 3D-CNN model is evaluated on two of the most challenging action recognition benchmarks, UCF-101 and HMDB-51. The asymmetric 3D-CNN model outperforms all the traditional 3D-CNN models in both effectiveness and efficiency, and its performance is comparable with that of recent state-of-the-art action recognition methods on both benchmarks.","Asymmetric 3D convolution, MicroNets, 3D-CNN, Action recognition",Hao Yang and Chunfeng Yuan and Bing Li and Yang Du and Junliang Xing and Weiming Hu and Stephen J. Maybank,https://www.sciencedirect.com/science/article/pii/S0031320318302632,https://doi.org/10.1016/j.patcog.2018.07.028,0031-3203,2019,1--12,85,Pattern Recognition,Asymmetric 3D Convolutional Neural Networks for action recognition,article,YANG20191,
"Due to the ability of capturing the geometry structure of data manifold, context-sensitive similarity has demonstrated impressive performances in the retrieval task. The key idea of context-sensitive similarity is that the similarity between two data points can be more reliably estimated with the local context of other points in the affinity graph. Therefore, neighborhood selection is a crucial factor for those algorithms, which affects the performance dramatically. In this paper, we propose a new algorithm called Smooth Neighborhood (SN) that mines the neighborhood structure to satisfy the manifold assumption. By doing so, nearby points on the underlying manifold are guaranteed to yield similar neighbors as much as possible. Moreover, SN is adjusted to tackle multiple affinity graphs by imposing a weight learning paradigm, and this is the primary difference compared with related works which are only applicable with one affinity graph. Finally, we integrate SN with Sparse Contextual Activation (SCA), a representative context-sensitive similarity proposed recently. Extensive experimental results and comparisons manifest that with the neighborhood structure generated by SN, the proposed framework can yield state-of-the-art performances on shape retrieval, image retrieval and 3D model retrieval.","Object retrieval, Context-sensitive similarity, 3D shape, Re-ranking, Rank aggregation",Song Bai and Shaoyan Sun and Xiang Bai and Zhaoxiang Zhang and Qi Tian,https://www.sciencedirect.com/science/article/pii/S0031320318302115,https://doi.org/10.1016/j.patcog.2018.06.001,0031-3203,2018,353--364,83,Pattern Recognition,Improving context-sensitive similarity via smooth neighborhood for object retrieval,article,BAI2018353,
"This paper proposes a novel regularizer named Structured Decorrelation Constraint, to address both the generalization and optimization of deep neural networks, including multiple-layer perceptrons and convolutional neural networks. Our proposed regularizer reduces overfitting by breaking the co-adaptions between the neurons with an explicit penalty. As a result, the network is capable of learning non-redundant representations. Meanwhile, the proposed regularizer encourages the networks to learn structured high-level features to aid the networksâ optimization during training. To this end, neurons are constrained to behave obeying a group prior. Our regularizer applies to various types of layers, including fully connected layers, convolutional layers and normalization layers. The loss of our regularizer can be directly minimized along with the networkâs classification loss by stochastic gradient descent. Experiments show that the proposed regularizer obviously relieves the overfitting problem of the existing deep networks. It yields much better performance on extensive datasets than the conventional regularizers like Dropout.","Deep networks, Overfitting, Decorrelation",Jihai Yang and Wei Xiong and Shijun Li and Chang Xu,https://www.sciencedirect.com/science/article/pii/S0031320318303169,https://doi.org/10.1016/j.patcog.2018.08.017,0031-3203,2019,224--235,86,Pattern Recognition,Learning structured and non-redundant representations with deep neural networks,article,YANG2019224,
"In real-world applications, factors such as head pose variation, occlusion, and poor image quality make facial expression recognition (FER) an open challenge. In this paper, a novel conditional convolutional neural network enhanced random forest (CoNERF) is proposed for FER in unconstrained environment. Our method extracts robust deep salient features from saliency-guided facial patches to reduce the influence from various distortion types, such as illumination, occlusion, low image resolution, etc. A conditional CoNERF is devised to enhance decision trees with the capability of representation learning from transferred convolutional neural networks and to model facial expression of different perspectives with conditional probabilistic learning. In the learning process, we introduce a neurally connected split function (NCSF) as the node splitting strategy in the CoNERF. Experiments were conducted using public CK+, JAFFE, multi-view BU-3DEF and LFW datasets. Compared to the state-of-the-art methods, the proposed method achieved much improved performance and great robustness with an average accuracy of 94.09% on the multi-view BU-3DEF dataset, 99.02% on CK+ and JAFFE frontal facial datasets, and 60.9% on LFW dataset. In addition, in contrast to deep neural networks which require large-scale training data, conditional CoNERF performs well even when there are only a small amount of training data.","Classification, Feature extraction, Facial expression recognition, Head pose alignment, Conditional CoNERF",Yuanyuan Liu and Xiaohui Yuan and Xi Gong and Zhong Xie and Fang Fang and Zhongwen Luo,https://www.sciencedirect.com/science/article/pii/S0031320318302516,https://doi.org/10.1016/j.patcog.2018.07.016,0031-3203,2018,251--261,84,Pattern Recognition,Conditional convolution neural network enhanced random forest for facial expression recognition,article,LIU2018251,
"In this work we describe a fast and stable algorithm for the computation of the orthogonal moments of an image. Indeed, orthogonal moments are characterized by a high discriminative power, but some of their possible formulations are characterized by a large computational complexity, which limits their real-time application. This paper describes in detail an approach based on recurrence relations, and proposes an optimized Matlab implementation of the corresponding computational procedure, aiming to solve the above limitations and put at the communityâs disposal an efficient and easy to use software. In our experiments we evaluate the effectiveness of the recurrence formulation, as well as its performance for the reconstruction task, in comparison to the closed form representation, often used in the literature. The results show a sensible reduction in the computational complexity, together with a greater accuracy in reconstruction. In order to assess and compare the accuracy of the computed moments in texture analysis, we perform classification experiments on six well-known databases of texture images. Again, the recurrence formulation performs better in classification than the closed form representation. More importantly, if computed from the GLCM of the image using the proposed stable procedure, the orthogonal moments outperform in some situations some of the most diffused state-of-the-art descriptors for texture classification.","Texture descriptor, Moment, Local binary pattern, Co-occurrence matrix, Classification",Cecilia {Di Ruberto} and Lorenzo Putzu and Giuseppe Rodriguez,https://www.sciencedirect.com/science/article/pii/S003132031830222X,https://doi.org/10.1016/j.patcog.2018.06.012,0031-3203,2018,498--510,83,Pattern Recognition,Fast and accurate computation of orthogonal moments for texture analysis,article,DIRUBERTO2018498,
"Two-stage Cumulative Attribute (CA) regression has been found effective in regression problems of computer vision such as facial age and crowd density estimation. The first stage regression maps input features to cumulative attributes that encode correlations between target values. The previous works have dealt with single output regression. In this work, we propose cumulative attribute spaces for 2- and 3-output (multivariate) regression. We show how the original CA space can be generalized to multiple output by the Cartesian product (CartCA). However, for target spaces with more than two outputs the CartCA becomes computationally infeasible and therefore we propose an approximate solution - multi-view CA (MvCA) - where CartCA is applied to output pairs. We experimentally verify improved performance of the CartCA and MvCA spaces in 2D and 3D face pose estimation and three-output (RGB) illuminant estimation for color constancy.","Multivariate regression, Cumulative attribute space, Head pose, Color constancy",Ke Chen and Kui Jia and Heikki Huttunen and Jiri Matas and Joni-Kristian KÃ¤mÃ¤rÃ¤inen,https://www.sciencedirect.com/science/article/pii/S0031320318303601,https://doi.org/10.1016/j.patcog.2018.10.015,0031-3203,2019,29--37,87,Pattern Recognition,Cumulative attribute space regression for head pose estimation and color constancy,article,CHEN201929,
"The moment-based method is a fundamental approach to the extraction of affine invariants. However, only integer-order traditional moments can be used to construct affine invariants. No invariants can be constructed by moments with an order lower than 2. Consequently, the obtained invariants are sensitive to noise. In this paper, the moment order is generalized from integer to non-integer. However, the moment order cannot simply be generalized from integer to non-integer to achieve affine invariance. The difficulty of this generalization lies in the fact that the angular factor owing to shearing in the affine transform can hardly be eliminated for non-integer order moments. In order to address this problem, the Mellin polar coordinate moment (MPCM) is proposed, which is directly defined by a repeated integral. The angular factor can easily be eliminated by appropriately selecting a repeated integral. A method is provided for constructing affine invariants by means of MPCMs. The traditional affine moment invariants (AMIs) can be derived in terms of the proposed MPCM. Furthermore, affine invariants constructed with real-order (lower than 2) MPCMs can be derived using the proposed method. These invariants may be more robust to noise than AMIs. Several experiments were conducted to evaluate the proposed method performance.","Mellin polar coordinate moment, Mellin transform, Repeated integral, Affine moment invariants, Affine transform",Jianwei Yang and Liang Zhang and Yuan Yan Tang,https://www.sciencedirect.com/science/article/pii/S0031320318302759,https://doi.org/10.1016/j.patcog.2018.07.036,0031-3203,2019,37--49,85,Pattern Recognition,Mellin polar coordinate moment and its affine invariance,article,YANG201937,
"Despite recent progress of pedestrian detection, it remains a challenging problem to detect pedestrians that are partially occluded due to the uncertainty and diversity of partial occlusion patterns. Following a commonly used framework of handling partial occlusions by part detection, we propose a multi-label learning approach to jointly learn part detectors to capture partial occlusion patterns. The part detectors share a set of decision trees which are learned and combined via boosting to exploit part correlations and also reduce the computational cost of applying these part detectors for pedestrian detection. The learned decision trees capture the overall distribution of all the parts. When used as a pedestrian detector individually, our part detectors learned jointly show better performance than their counterparts learned separately in different occlusion situations. For occlusion handling, several methods are explored to integrate the part detectors learned by the proposed approach. Context is also exploited to further improve the performance. The proposed approach is applied to hand-crafted channel features and features learned by a deep convolutional neural network, respectively. Experiments on the Caltech and CUHK datasets show state-of-the-art performance of our approach for detecting occluded pedestrians, especially heavily occluded ones.","Pedestrian detection, Part detectors, Multi-label learning, Occlusion handling, Detector integration, Context",Chunluan Zhou and Junsong Yuan,https://www.sciencedirect.com/science/article/pii/S0031320318303170,https://doi.org/10.1016/j.patcog.2018.08.018,0031-3203,2019,99--111,86,Pattern Recognition,Multi-label learning of part detectors for occluded pedestrian detection,article,ZHOU201999,
"As the use of deep methods become widespread in the scientific community, causing major changes in systems architecture and position in terms of knowledge acquisition, we report here our insights about how document analysis systems are built. Where does the expertise really lie? In the features, in the decision making step, in the system design, in the data illustrating the problem to be solved? The examination of the practices of researchers in this field, and their evolution, allows us to conclude that the tools that are used, and related issues, have become more and more complex over time. Nevertheless, human skill is needed to activate these tools and to imagine new ones.","Features, Machine learning, Deep methods, Handcraft approach",Nicole Vincent and Jean-Marc Ogier,https://www.sciencedirect.com/science/article/pii/S0031320318303297,https://doi.org/10.1016/j.patcog.2018.09.010,0031-3203,2019,281--289,86,Pattern Recognition,Shall deep learning be the mandatory future of document analysis problems?,article,VINCENT2019281,
"We introduce a class of mathematical algorithms with the aim of establishing a framework of finding a group average and extracting prominent features in a group of landmark represented shapes or image templates. A group average is an estimator that is said to best represent the common features of the group being studied. The proposed algorithms, as a tool of feature extraction, extract information about momentum at each landmark through the process of template matching. Once the convergence criterion is satisfied numerically, the algorithms produce a group average and a local coordinate system for each member of the observing group, in terms of the residual momentum. We present several examples to illustrate the use of the proposed algorithms for finding a group average. Using the metrics computed between the group average and each member of the group, we successfully run a cluster analysis for datasets that contain a heavy percentage of outliers. Finally, we apply the collected residual momenta computed in the proposed algorithms in some statistical methods to demonstrate a potential application of the algorithms for detecting structure abnormality.","Group average, Pattern recognition, Features extraction, Landmark, Template matching, Residual momentum, Cluster analysis, Outliers, Structure abnormality",Snehalata Huzurbazar and Dongyang Kuang and Long Lee,https://www.sciencedirect.com/science/article/pii/S0031320318303212,https://doi.org/10.1016/j.patcog.2018.09.002,0031-3203,2019,172--187,86,Pattern Recognition,Landmark-based algorithms for group average and pattern recognition,article,HUZURBAZAR2019172,
"Due to the great texture discrepancies between photos and sketches, matching face sketches against mug shot photos is a challenging yet important topic in face recognition community, with potential applications in law enforcement and security. Despite the great progress achieved in recent years, existing works mainly focus on face recognition from SINGLE sketch. However, in real-world applications it is more practical to obtain MULTIPLE stylistic sketches for recognizing the suspect, such as clue from multiple eyewitnesses, cooperation with multiple forensic artists, even drawn with different kinds of pencils. It is challenging due to the difficulty of taking multiple sketches into consideration in these scenes. To this end, we present a comprehensive investigation of face recognition from multiple stylistic sketches. Considering that there are two types of sketches available, i.e. hand-drawn sketch and composite sketch, we propose three specific scenarios: (1) recognition from multiple hand-drawn sketches, (2) recognition from both hand-drawn sketch and composite sketches, and (3) recognition from multiple composite sketches. In additional to the definitions of the proposed scenarios, we also provide evaluation protocols and benchmark performance on public available datasets. Through extensive experiments on benchmarks, plenty of challenges in proposed scenarios are finally discussed and there are many possible future directions worth further investigation. The main purpose of this paper is to open a first fundamental study on the topic of face recognition from multiple stylistic sketches. Possible high technical and superior methodologies are encouraged in the future. Related materials of this paper are publicly available at http://chunleipeng.com/FRMSketches.html.","Face sketch recognition, Viewed sketch, Composite sketch, Fusion",Chunlei Peng and Xinbo Gao and Nannan Wang and Jie Li,https://www.sciencedirect.com/science/article/pii/S0031320318302486,https://doi.org/10.1016/j.patcog.2018.07.014,0031-3203,2018,262--272,84,Pattern Recognition,"Face recognition from multiple stylistic sketches: Scenarios, datasets, and evaluation",article,PENG2018262,
"Dynamic Ensemble Selection (DES) techniques aim to select one or more competent classifiers for the classification of each new test sample. Most DES techniques estimate the competence of classifiers using a given criterion over the region of competence of the test sample, usually defined as the set of nearest neighbors of the test sample in the validation set. Despite being very effective in several classification tasks, DES techniques can select classifiers that classify all samples in the region of competence as being from the same class. The Frienemy Indecision REgion DES (FIRE-DES) tackles this problem by pre-selecting classifiers that correctly classify at least one pair of samples from different classes in the region of competence of the test sample. However, FIRE-DES applies the pre-selection for the classification of a test sample if and only if its region of competence is composed of samples from different classes (indecision region), even though this criterion is not reliable for determining if a test sample is located close to the borders of classes (true indecision region) when the region of competence is obtained using classical nearest neighbors approach. Because of that, FIRE-DES mistakes noisy regions for true indecision regions, leading to the pre-selection of incompetent classifiers, and mistakes true indecision regions for safe regions, leaving samples in such regions without any pre-selection. To tackle these issues, we propose the FIRE-DES++, an enhanced FIRE-DES that removes noise and reduces the overlap of classes in the validation set; and defines the region of competence using an equal number of samples of each class, avoiding selecting a region of competence with samples of a single class. Experiments are conducted using FIRE-DES++ with 8 different dynamic selection techniques on 64 classification datasets. Experimental results show that FIRE-DES++ increases the classification performance of all DES techniques considered in this work, outperforming FIRE-DES with 7 out of the 8 DES techniques, and outperforming state-of-the-art DES frameworks.","Ensemble of classifiers, Dynamic ensemble selection, Classifier competence, Prototype selection",Rafael M.O. Cruz and Dayvid V.R. Oliveira and George D.C. Cavalcanti and Robert Sabourin,https://www.sciencedirect.com/science/article/pii/S0031320318302760,https://doi.org/10.1016/j.patcog.2018.07.037,0031-3203,2019,149--160,85,Pattern Recognition,FIRE-DES++: Enhanced online pruning of base classifiers for dynamic ensemble selection,article,CRUZ2019149,
"In this paper, we develop a sparse method for unsupervised dimension reduction for data from an exponential-family distribution. Our idea extends previous work on Generalised Principal Component Analysis by adding L1 and SCAD penalties to introduce sparsity. We demonstrate the significance and advantages of our method with synthetic and real data examples. We focus on the application to text data which is high-dimensional and non-Gaussian by nature and discuss the potential advantages of our methodology in achieving dimension reduction.","Dimension reduction, PCA, Text mining, Exponential family",Luke Smallman and Andreas Artemiou and Jennifer Morgan,https://www.sciencedirect.com/science/article/pii/S0031320318302243,https://doi.org/10.1016/j.patcog.2018.06.014,0031-3203,2018,443--455,83,Pattern Recognition,Sparse Generalised Principal Component Analysis,article,SMALLMAN2018443,
"A method is developed to characterise highly irregular shape patterns, especially those appearing in biomedical settings. A collection of best-fitting ellipsoids is found using principal component analysis, and features are defined based on these ellipsoids in four different ways. The method is defined in a general setting, but is illustrated using two-dimensional images of dimorphic yeast exhibiting pseudohyphal growth, three-dimensional images of cancellous bone and three-dimensional images of marbling in beef. Classifiers successfully distinguish between the yeast colonies with a mean classification accuracy ofÂ 0.843 (SD=0.021), and between cancellous bone from rats in different experimental groups with a mean classification accuracy ofÂ 0.745 (SD=0.024). A strong correlation (R2=0.797) is found between marbling ratio and a shape feature. Key aspects of the method are that local shape patterns, including orientation, are learned automatically from the data, and the method applies to objects that are irregular in shape to the point where landmark points cannot be identified between samples.","Shape analysis, Dimorphic yeast, Pseudohyphal growth, Cancellous bone, Marbling in beef",Amelia Gontar and Hayden Tronnolone and Benjamin J. Binder and Murk J. Bottema,https://www.sciencedirect.com/science/article/pii/S003132031830219X,https://doi.org/10.1016/j.patcog.2018.06.009,0031-3203,2018,365--374,83,Pattern Recognition,Characterising shape patterns using features derived from best-fitting ellipsoids,article,GONTAR2018365,
"Learning appropriate distance metric from data can significantly improve the performance of machine learning tasks under investigation. In terms of the distance metric representation forms in the models, distance metric learning (DML) approaches can be generally divided into two categories: parametric and non-parametric. The first category needs to make parametric assumption on the distance metric and learns the parameters, easily leading to overfitting and limiting model flexibility. The second category abandons the above assumption and instead, directly learns a non-parametric distance metric whose complexity can be adjusted according to the number of available training data, and makes the model representation relatively flexible. In this paper we follow the idea of the latter category and develop a non-parametric DML approach. The main challenge of our work concerns the formulation and learning of non-parametric distance metric. To meet this, we use Gaussian Process (GP) to extend the bilinear similarity into a non-parametric metric (here we abuse the concept of metric) and then learn this metric for specific task. As a result, our approach learns not only nonlinear metric that inherits the flexibility of GP but also representative features for the follow-up tasks. Compared with the existing GP-based feature learning approaches, our approach can provide accurate similarity prediction in the new feature space. To the best of our knowledge, this is the first work that directly uses GP as non-parametric metric. In the experiments, we compare our approach with related GP-based feature learning approaches and DML approaches respectively. The results demonstrate the superior performance of our approach.","Metric learning, Gaussian process, Bilinear similarity, Non-parametric metric",Ping Li and Songcan Chen,https://www.sciencedirect.com/science/article/pii/S0031320318303571,https://doi.org/10.1016/j.patcog.2018.10.010,0031-3203,2019,17--28,87,Pattern Recognition,Gaussian process approach for metric learning,article,LI201917,
"Paired RGB and depth images are becoming popular multi-modal data adopted in computer vision tasks. Traditional methods based on Convolutional Neural Networks (CNNs) typically fuse RGB and depth by combining their deep representations in a late stage with only one path, which can be ambiguous and insufficient for fusing large amounts of cross-modal data. To address this issue, we propose a novel multi-scale multi-path fusion network with cross-modal interactions (MMCI), in which the traditional two-stream fusion architecture with single fusion path is advanced by diversifying the fusion path to a global reasoning one and another local capturing one and meanwhile introducing cross-modal interactions in multiple layers. Compared to traditional two-stream architectures, the MMCI net is able to supply more adaptive and flexible fusion flows, thus easing the optimization and enabling sufficient and efficient fusion. Concurrently, the MMCI net is equipped with multi-scale perception ability (i.e., simultaneously global and local contextual reasoning). We take RGB-D saliency detection as an example task. Extensive experiments on three benchmark datasets show the improvement of the proposed MMCI net over other state-of-the-art methods.","RGB-D, Convolutional neural networks, Multi-path, Saliency detection",Hao Chen and Youfu Li and Dan Su,https://www.sciencedirect.com/science/article/pii/S0031320318303054,https://doi.org/10.1016/j.patcog.2018.08.007,0031-3203,2019,376--385,86,Pattern Recognition,Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection,article,CHEN2019376,
"In extreme learning machines (ELM), the hidden node parameters are randomly generated and the output weights can be analytically computed. To overcome the bad feature extraction ability of the shallow architecture of ELM, the hierarchical ELM has been extensively studied as a deep architecture with multilayer neural network. However, the commonly used mean square error (MSE) criterion is very sensitive to outliers and impulsive noises, generally existing in real world data. In this paper, we investigate the correntropy to improve the robustness of the multilayer ELM and provide sparser representation. The correntropy, as a nonlinear measure of similarity, is robust to outliers and can approximate different norms (from â0 to â2). A new full correntropy based multilayer extreme learning machine (FC-MELM) algorithm is proposed to handle the classification of datasets which are corrupted by impulsive noises or outliers. The contributions of this paper are three-folds: (1) The MSE based reconstruction loss is replaced by the correntropy based loss function; In this way, the robustness of the ELM based multilayer algorithms is enhanced. (2) The traditional â1-based sparsity penalty term is also replaced by a correntropy-based sparsity penalty term, which can further improve the performance of the proposed algorithm with a sparser representation of the data. The combination of (1) and (2) provides the correntropy-based ELM autoencoder. (3) The FC-MELM is proposed by using the correntropy-based ELM autoencoder as a building block. It is notable that the FC-MELM is trained in a forward manner, which means fine-tuning procedure is not required. Thus, the FC-MELM has great advantage in learning efficiently when compared with traditional deep learning algorithms. The good property of the proposed algorithm is confirmed by the experiments on well-known benchmark datasets, including the MNIST datasets, the NYU Object Recognition Benchmark dataset, and the Moore network traffic dataset. Finally, the proposed FC-MELM algorithm is applied to address Computer Aided Cancer Diagnosis. Experiments conducted on the well-known Wisconsin Breast Cancer Data (Diagnostic) dataset are presented and show that the proposed FC-MELM outperforms state-of-the-art methods in solving computer aided cancer diagnosis problems.","Deep learning, Extreme learning machine, Correntropy, Unsupervised feature learning, Computer aided cancer diagnosis",Chen Liangjun and Paul Honeine and Qu Hua and Zhao Jihong and Sun Xia,https://www.sciencedirect.com/science/article/pii/S0031320318302401,https://doi.org/10.1016/j.patcog.2018.07.011,0031-3203,2018,357--370,84,Pattern Recognition,Correntropy-based robust multilayer extreme learning machines,article,LIANGJUN2018357,
"This paper proposes an effective method for accurately recovering vessel structures and intensity information from the X-ray coronary angiography (XCA) images of moving organs or tissues. Specifically, a global logarithm transformation of XCA images is implemented to fit the X-ray attenuation sum model of vessel/background layers into a low-rank, sparse decomposition model for vessel/background separation. The contrast-filled vessel structures are extracted by distinguishing the vessels from the low-rank backgrounds by using a robust principal component analysis and by constructing a vessel mask via Radon-like feature filtering plus spatially adaptive thresholding. Subsequently, the low-rankness and inter-frame spatio-temporal connectivity in the complex and noisy backgrounds are used to recover the vessel-masked background regions using tensor completion of all other background regions, while the twist tensor nuclear norm is minimized to complete the background layers. Finally, the method is able to accurately extract vesselsâ intensities from the noisy XCA data by subtracting the completed background layers from the overall XCA images. We evaluated the vessel visibility of resulting images on real X-ray angiography data and evaluated the accuracy of vessel intensity recovery on synthetic data. Experiment results show the superiority of the proposed method over the state-of-the-art methods.","X-ray coronary angiography, Tensor completion, Robust principal component analysis, Vessel segmentation, Layer separation, Vessel enhancement, Vessel recovery",Binjie Qin and Mingxin Jin and Dongdong Hao and Yisong Lv and Qiegen Liu and Yueqi Zhu and Song Ding and Jun Zhao and Baowei Fei,https://www.sciencedirect.com/science/article/pii/S0031320318303455,https://doi.org/10.1016/j.patcog.2018.09.015,0031-3203,2019,38--54,87,Pattern Recognition,Accurate vessel extraction via tensor completion of background layer in X-ray coronary angiograms,article,QIN201938,
"Dynamic Classifier Selection (DCS) techniques have difficulty in selecting the most competent classifier in a pool, even when its presence is assured. Since the DCS techniques rely only on local data to estimate a classifiers competence, the manner in which the pool is generated could affect the choice of the best classifier for a given instance. That is, the global perspective in which pools are generated may not help the DCS techniques in selecting a competent classifier for instances that are likely to be misclassified. Thus, it is proposed in this work an online pool generation method that produces a locally accurate pool for test samples in difficult regions of the feature space. The difficulty of a given area is determined by the estimated classification difficulty of the instances in it. That way, by using classifiers that were generated in a local scope, it could be easier for the DCS techniques to select the best one for those instances they would most probably misclassify. For the query samples surrounded by easy instances, a simple nearest neighbors rule is used in the proposed method. In order to identify in which cases the local pool is used in the proposed scheme, an analysis on the correlation between instance hardness and DCS techniques is performed in this work, and it is proposed the use of an instance hardness measure that conveys the degree of local class overlap near a given sample. Experimental results show that the DCS techniques were more able to select the most competent classifier for difficult instances when using the proposed local pool than when using a globally generated pool. Moreover, the proposed technique yielded significantly greater recognition rates in comparison to a Bagging-generated pool and two other global generation schemes for all DCS techniques evaluated. The performance of the proposed technique was also significantly superior to three state-of-the-art classification models and was statistically equivalent to five of them.","Multiple classifier systems, Instance hardness, Pool generation, Dynamic classifier selection",Mariana A. Souza and George D.C. Cavalcanti and Rafael M.O. Cruz and Robert Sabourin,https://www.sciencedirect.com/science/article/pii/S0031320318302802,https://doi.org/10.1016/j.patcog.2018.08.004,0031-3203,2019,132--148,85,Pattern Recognition,Online local pool generation for dynamic classifier selection,article,SOUZA2019132,
"Liver segmentation and volumetry in native MR-volume data is an important topic in epidemiological research. Manual liver segmentation is extremely time-consuming and often infeasible requiring automatized methods. Automatic liver segmentation is challenging because of the large variability in liver shape and appearance and the low contrast to neighboring organs. We present a fully automatized liver segmentation framework that uses a sequence of modules based on individualized model knowledge on liver appearance and shape. Liver probability maps are computed that incorporate organ-specific features like MR-intensity distributions, inner-organ MR-differences and liver positions. Probability map generation differentiates automatically between fatty and non-fatty livers. Moreover, we improve an existing technique for prior shape level set segmentation to delineate the liver in tissue-specific liver probability maps and to recognize cystic hepatic tissue. Dice coefficients of 0.937 and low volumetric errors on 35 test data sets confirm the robust segmentation quality of the framework.","Expectation maximization, Subject-specific shape model, 3D prior shape level set segmentation, Bayesian probability, Normalized cross correlation, Principal component analysis",Oliver Gloger and Klaus TÃ¶nnies,https://www.sciencedirect.com/science/article/pii/S0031320318302528,https://doi.org/10.1016/j.patcog.2018.07.018,0031-3203,2018,288--300,84,Pattern Recognition,Subject-Specific prior shape knowledge in feature-oriented probability maps for fully automatized liver segmentation in MR volume data,article,GLOGER2018288,
"Three discriminative representations for face presentation attack detection are introduced in this paper. Firstly we design a descriptor called spatial pyramid coding micro-texture (SPMT) feature to characterize local appearance information. Secondly we utilize the SSD, which is a deep learning framework for detection, to excavate context cues and conduct end-to-end face presentation attack detection. Finally we design a descriptor called template face matched binocular depth (TFBD) feature to characterize stereo structures of real and fake faces. For accurate presentation attack detection, we also design two kinds of representation combinations. Firstly, we propose a decision-level cascade strategy to combine SPMT with SSD. Secondly, we use a simple score fusion strategy to combine face structure cues (TFBD) with local micro-texture features (SPMT). To demonstrate the effectiveness of our design, we evaluate the representation combination of SPMT and SSD on three public datasets, which outperforms all other state-of-the-art methods. In addition, we evaluate the representation combination of SPMT and TFBD on our dataset and excellent performance is also achieved.","Face presentation attack detection, Template face registration, Binocular depth, Spatial pyramid coding, Micro-texture, SSD, Decision-level cascade strategy",Xiao Song and Xu Zhao and Liangji Fang and Tianwei Lin,https://www.sciencedirect.com/science/article/pii/S0031320318303182,https://doi.org/10.1016/j.patcog.2018.08.019,0031-3203,2019,220--231,85,Pattern Recognition,Discriminative representation combinations for accurate face spoofing detection,article,SONG2019220,
"Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.","Scene text proposal, Pooling based grouping, Scene text detection, Scene text reading, Scene text spotting",Dinh NguyenVan and Shijian Lu and Shangxuan Tian and Nizar Ouarti and Mounir Mokhtari,https://www.sciencedirect.com/science/article/pii/S0031320318303583,https://doi.org/10.1016/j.patcog.2018.10.012,0031-3203,2019,118--129,87,Pattern Recognition,A pooling based scene text proposal technique for scene text reading in the wild,article,NGUYENVAN2019118,
"As more and more digitized manga (Japanese comics) books are available, efficient and effective access to manga is urgently needed. Among various elements of manga, characterâs face plays one of the most important roles in access and retrieval. We propose a deep neural network method to do manga face detection, which is a challenging but relatively unexplored topic. Given a manga page, we first find candidate regions based on the selective search scheme. Three convolutional neural networks are then proposed to detect manga faces of various appearance. We extract information from the entire object region and several local regions, and integrate multi-scale information in an early fusion manner or a late fusion manner. The proposed methods are evaluated based on a large-scale benchmark. Convincing performance compared to the state-of-the-art face detection modules designed for human faces is demonstrated.","Manga face detection, Deep neural network, Early fusion, Late fusion",Wei-Ta Chu and Wei-Wei Li,https://www.sciencedirect.com/science/article/pii/S0031320318303066,https://doi.org/10.1016/j.patcog.2018.08.008,0031-3203,2019,62--72,86,Pattern Recognition,Manga face detection based on deep neural networks fusing global and local information,article,CHU201962,
"In this paper, we propose a novel image background subtraction framework based on KDE. Firstly a new data structure called Mino Vector (MV) is designed for each pixel; we define dynamic nature (DN) for pixels of a scene and rank them in terms of DN for getting quantized results named dynamic rank (DR). Then, the varying KDE is adopted and implemented which significantly improves the estimation accuracy. Unlike using a global threshold in literature, we adaptively set a threshold for each pixel according to its DR. Inspired by the popular computer game Tetris, we present a Tetris update scheme (TUS) to update the background model in which the bottom row will be cleared, so do noises when the update condition is met. In experiments, we evaluate our framework on a well-known video dataset, CDnet 2012. Our results indicate that our framework achieves competitive results when compared with the state-of-the-art methods.","Background modeling, Mino vector, Dynamic nature, KDE, Denoising, Tetris update scheme",Guian Zhang and Zhiyong Yuan and Qianqian Tong and Mianlun Zheng and Jianhui Zhao,https://www.sciencedirect.com/science/article/pii/S0031320318302371,https://doi.org/10.1016/j.patcog.2018.07.006,0031-3203,2018,28--38,84,Pattern Recognition,A novel framework for background subtraction and foreground detection,article,ZHANG201828,
"The segmentation of abnormal regions on dermoscopic images is an important step for automated computer aided diagnosis (CAD) of skin lesions. Recent methods based on fully convolutional networks (FCN) have been very successful for dermoscopic image segmentation. However, they tend to overfit to the visual characteristics that are present in the dominant non-melanoma studies and therefore, perform poorly on the complex visual characteristics exhibited by melanoma studies, which usually consists of fuzzy boundaries and heterogeneous textures. In this paper, we propose a new method for automated skin lesion segmentation that overcomes these limitations via a novel deep class-specific learning approach which learns the important visual characteristics of the skin lesions of each individual class (melanoma vs. non-melanoma) on an individual basis. We also introduce a new probability-based, step-wise integration to combine complementary segmentation results derived from individual class-specific learning models. We achieved an average Dice coefficient of 85.66% on the ISBI 2017 Skin Lesion Challenge (SLC), 91.77% on the ISBI 2016 SLC and 92.10% on the PH2 datasets with corresponding Jaccard indices of 77.73%, 85.92% and 85.90%, respectively, for the same datasets. Our experiments on three well-established public benchmark datasets demonstrate that our method is more effective than other state-of-the-art methods for skin lesion segmentation.","Dermoscopic, Melanoma, Segmentation, Fully convolutional networks (FCN)",Lei Bi and Jinman Kim and Euijoon Ahn and Ashnil Kumar and Dagan Feng and Michael Fulham,https://www.sciencedirect.com/science/article/pii/S0031320318302772,https://doi.org/10.1016/j.patcog.2018.08.001,0031-3203,2019,78--89,85,Pattern Recognition,Step-wise integration of deep class-specific learning for dermoscopic image segmentation,article,BI201978,
"In recent face recognition techniques utilizing the color information, researchers tried to select one conventional color space or learn a color space from the given training data to achieve better performance. RQCr, DCS and ZRG-NII color spaces have gained reputation as effective color spaces in which the face recognition performs better than in the others. However, at the moment, how to construct effective color spaces for face recognition has not been thoroughly studied. In this paper, we propose a color space LuC1C2 based on a framework of constructing effective color spaces for face recognition tasks. It is composed of one luminance component Lu and two chrominance components C1, C2. The luminance component Lu is selected from 4 different luminance candidates by comparing their R,G,B coefficients and color sensor properties. For the two chrominance components C1, C2, the directions of their transform vectors are determined by the discriminant analysis and the covariance analysis in the chrominance subspace of the RGB color space. The magnitudes of their transform vectors are determined by the discriminant values of Lu, C1, C2. Extensive experiments are conducted on 4 benchmark databases to evaluate our proposed color space LuC1C2. The experimental results obtained by using 2 different color features and 3 different dimension reduction methods show that our proposed color space LuC1C2 achieves consistently better face recognition performance than state-of-the-art color spaces on 3 databases. We also show that the proposed color space achieves higher face verification rate than the state of the arts on FRGC database. Furthermore, the face verification performance is improved significantly by combining CNN features with simple raw-pixel features from the proposed LuC1C2 color space on both LFW and FRGC databases.","Color face recognition, Color space, Color sensor analysis, Chrominance subspace, Discriminant analysis, Covariance analysis",Ze Lu and Xudong Jiang and Alex Kot,https://www.sciencedirect.com/science/article/pii/S0031320318302255,https://doi.org/10.1016/j.patcog.2018.06.015,0031-3203,2018,456--468,83,Pattern Recognition,Color space construction by optimizing luminance and chrominance components for face recognition,article,LU2018456,
"The muscular dystrophies are made up of a diverse group of rare genetic diseases characterized by progressive loss of muscle strength and muscle damage. Since there is no cure for muscular dystrophy and clinical outcome measures are limited, it is critical to assess the progression of MD objectively. Imaging muscle replacement by fibrofatty tissue has been shown to be a robust biomarker to monitor disease progression in DMD. In magnetic resonance imaging (MRI) data, specific texture patterns are found to correlate to certain MD subtypes and thus present a potential way for automatic assessment. In this paper, we first apply state-of-the-art convolutional neural networks (CNNs) to perform accurate MD image classification and then propose an effective visualization method to highlight the important image textures. With a dystrophic MRI dataset, we found that the best CNN model delivers an 91.7% classification accuracy, which significantly outperforms non-deep learning methods, e.g., â¯>â¯40% improvement has been found over the traditional mean fat fraction (MFF) criterion for DMD and CMD classification. After investigating every single neuron at the top layer of CNN model, we found the superior classification ability of CNN can be explained by its 91 and 118 neurons were performing better than the MFF criterion under the measurements of Euclidean and Chi-square distance, respectively. In order to further interpret CNNs predictions, we tested an improved class activation mapping (ICAM) method to visualize the important regions in the MRI images. With this ICAM, CNNs are able to locate the most discriminative texture patterns of DMD in soleus, lateral gastrocnemius, and medial gastrocnemius; for CMD, the critical texture patterns are highlighted in soleus, tibialis posterior, and peroneus.","Muscular dystrophy, Convolutional neural network, MRI analysis, Texture classification, Abnormality detection",Jinzheng Cai and Fuyong Xing and Abhinandan Batra and Fujun Liu and Glenn A. Walter and Krista Vandenborne and Lin Yang,https://www.sciencedirect.com/science/article/pii/S0031320318303108,https://doi.org/10.1016/j.patcog.2018.08.012,0031-3203,2019,368--375,86,Pattern Recognition,Texture analysis for muscular dystrophy classification in MRI with improved class activation mapping,article,CAI2019368,
"In many engineering and medical imaging fields, shape, and margin descriptors play an important role in challenging pattern recognition and classification problems. This paper presents a circular mesh-based shape and margin descriptor (CMSMD) for object recognition and classification. This shape descriptor is the first to have the functions of both structural and global contour-based descriptors. In the proposed descriptor, object contours are embedded in a circular mesh and labelled using circular mesh-based border labelling. New features can also be derived using the proposed descriptor. Further, an algorithm that employs the CMSMD characteristics can be utilised to identify the convexity and concavity of the embedded contours in linear complexity. The effectiveness of the proposed descriptor was demonstrated by performing lesion detection using dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) results. The obtained accuracy of 94.69% shows that the proposed descriptor is superior to the existing shape descriptors used for lesion detection in DCE-MRI.","Shape descriptor, Structural and global descriptor, Circular mesh-based shape and margin descriptor, Convex and concave regions, Lesion detection, Dynamic contrast enhanced magnetic resonance imaging",Malu G. and Sherly Elizabeth and Sumod {Mathew Koshy},https://www.sciencedirect.com/science/article/pii/S003132031830236X,https://doi.org/10.1016/j.patcog.2018.07.004,0031-3203,2018,97--111,84,Pattern Recognition,Circular mesh-based shape and margin descriptor for object detection,article,G201897,
"In this paper, a novel robust one-class support vector machine (OCSVM) based on the rescaled hinge loss function is proposed to enhance the robustness of the conventional OCSVM against outliers. The optimization problem of the proposed robust OCSVM can be iteratively solved by the half-quadratic optimization technique. Compared to OCSVM, robust OCSVM may achieve higher generalization performance from the theoretical analysis. Moreover, the robustness of robust OCSVM against outliers is explained from the weighted viewpoint. Experimental results on the synthetic and benchmark data sets demonstrate that the proposed robust OCSVM is superior to the conventional OCSVM and the other two related approaches.","One-class classification, One-class support vector machine, Hinge loss function, Half-quadratic optimization",Hong-Jie Xing and Man Ji,https://www.sciencedirect.com/science/article/pii/S0031320318302498,https://doi.org/10.1016/j.patcog.2018.07.015,0031-3203,2018,152--164,84,Pattern Recognition,Robust one-class support vector machine with rescaled hinge loss function,article,XING2018152,
"SLAM is generally addressed using natural landmarks such as keypoints or texture, but it poses some limitations, such as the need for enough textured environments and high computational demands. In some cases, it is preferable sacrificing the flexibility of such methods for an increase in speed and robustness by using artificial landmarks. The recent work [1] proposes an off-line method to obtain a map of squared planar markers in large indoor environments. By freely distributing a set of markers printed on a piece of paper, the method estimates the marker poses from a set of images, given that at least two markers are visible in each image. Afterwards, camera localization can be done, in the correct scale. However, an off-line process has several limitations. First, errors can not be detected until the whole process is finished, e.g., an insufficient number of markers in the scene or markers not properly spotted in the capture stage. Second, the method is not incremental, so, in case of requiring the expansion of the map, it is necessary to repeat the whole process from start. Finally, the method can not be employed in real-time systems with limited computational resources such as mobile robots or UAVs. To solve these limitations, this work proposes a real-time solution to the problems of simultaneously localizing the camera and building a map of planar markers. This paper contributes with a number of solutions to the problems arising when solving SLAM from squared planar markers, coining the term SPM-SLAM. The experiments carried out show that our method can be more robust, precise and fast, than visual SLAM methods based on keypoints or texture.","Fiducial markers, Marker mapping, SLAM",Rafael MuÃ±oz-Salinas and Manuel J. MarÃ­n-Jimenez and R. Medina-Carnicer,https://www.sciencedirect.com/science/article/pii/S0031320318303224,https://doi.org/10.1016/j.patcog.2018.09.003,0031-3203,2019,156--171,86,Pattern Recognition,SPM-SLAM: Simultaneous localization and mapping with squared planar markers,article,MUNOZSALINAS2019156,
"In this paper, we present a new set of fractional-order orthogonal moments, named Fractional-order Chebyshev Moments (FCM). We initially introduce the necessary relations and properties to define the FCM in the Cartesian coordinates. Then, we provide the theoretical framework to construct the Fractional-order Chebyshev Moment Invariants (FCMI), which are invariants with respect to rotation, scaling and translation transforms. In addition, we devoted a substantial attention to enhance their computational time and numerical accuracy. Consequently, the numerical experiments are carried out to demonstrate the validity of the introduced fractional-order moments and moment invariants in comparison with the classical methods, with regard to image representation capability and object recognition accuracy on several publicly available databases. The presented theoretical and experimental results demonstrate the efficiency and the superiority of the proposed method.","Fractional-order orthogonal moments, Fractional-order Chebyshev polynomials, Moment invariants, Image representation, Pattern recognition, Fast and accurate computation",Rachid Benouini and Imad Batioua and Khalid Zenkouar and Azeddine Zahi and Said Najah and Hassan Qjidaa,https://www.sciencedirect.com/science/article/pii/S0031320318303467,https://doi.org/10.1016/j.patcog.2018.10.001,0031-3203,2019,332--343,86,Pattern Recognition,Fractional-order orthogonal Chebyshev Moments and Moment Invariants for image representation and pattern recognition,article,BENOUINI2019332,
"Nonnegative matrix factorization (NMF) plays a vital role in data mining and machine learning fields. Standard NMF utilizes the Frobenius norm while robust NMF uses the robust â2,1-norm to measure the quality of factorization, given the assumption of i.i.d Gaussian noise model and i.i.d Laplacian noise model, respectively. In this paper, we propose a novel elastic loss which is intercalated and adapted between Frobenius norm and â2,1-norm. Inspired by this, we derive an elastic NMF model guided by the elastic loss with incorporating geometry manifold information while enforcing sparsity of coefficients at intra-cluster level via â1,2-norm. The new formulation is more robust to noises while preserving the stronger capability of clustering. We propose an EM-like algorithm (using an auxiliary function) to solve the resultant optimization problem, whose convergence can be rigorously proved. The extensive experiments demonstrate the effectiveness of the novel elastic NMF model on benchmarks.","NMF, Elastic, Robust, Manifold, Clustering, Exclusive LASSO",He Xiong and Deguang Kong,https://www.sciencedirect.com/science/article/pii/S0031320318302383,https://doi.org/10.1016/j.patcog.2018.07.007,0031-3203,2019,464--475,90,Pattern Recognition,Elastic nonnegative matrix factorization,article,XIONG2019464,
"Online streaming feature selection which deals with streaming features in an online manner plays a critical role in big data problems. Many approaches have been proposed to handle this problem. However, most existing methods need domain information before learning and specify some parameters in advance. In real-world applications, we cannot always require the domain information and it is a big challenge to specify uniform parameters for all different types of data sets. Motivated by this, we propose a new online streaming feature selection method based on adaptive density neighborhood relation, named OFS-Density. More specifically, with the neighborhood rough set theory, OFS-Density does not require the domain information before learning. Meanwhile, we propose a new adaptive neighborhood relation using the density information of the surrounding instances, which does not need to specify any parameters in advance. By the fuzzy equal constraint, OFS-Density can select features with a low redundancy. Finally, experimental studies on fourteen datasets show that OFS-Density is superior to traditional feature selection methods with the same numbers of features and state-of-the-art online streaming feature selection algorithms in an online manner.","Feature selection, Online feature selection, Streaming features, Neighborhood rough set",Peng Zhou and Xuegang Hu and Peipei Li and Xindong Wu,https://www.sciencedirect.com/science/article/pii/S0031320318303078,https://doi.org/10.1016/j.patcog.2018.08.009,0031-3203,2019,48--61,86,Pattern Recognition,OFS-Density: A novel online streaming feature selection method,article,ZHOU201948,
"Logistic regression is by far the most widely used classifier in real-world applications. In this paper, we benchmark the state-of-the-art active learning methods for logistic regression and discuss and illustrate their underlying characteristics. Experiments are carried out on three synthetic datasets and 44 real-world datasets, providing insight into the behaviors of these active learning methods with respect to the area of the learning curve (which plots classification accuracy as a function of the number of queried examples) and their computational costs. Surprisingly, one of the earliest and simplest suggested active learning methods, i.e., uncertainty sampling, performs exceptionally well overall. Another remarkable finding is that random sampling, which is the rudimentary baseline to improve upon, is not overwhelmed by individual active learning techniques in many cases.","Active learning, Logistic regression, Experimental design, Benchmark, Preference maps",Yazhou Yang and Marco Loog,https://www.sciencedirect.com/science/article/pii/S0031320318302140,https://doi.org/10.1016/j.patcog.2018.06.004,0031-3203,2018,401--415,83,Pattern Recognition,A benchmark and comparison of active learning for logistic regression,article,YANG2018401,
"Unsupervised multi-task learning exploits the shared knowledge to improve performances by learning related tasks simultaneously. In this paper, we propose an unsupervised multi-task learning method with hierarchical data structure. It strengthens similarities between instances in the same cluster, and increases diversities of instances by utilizing instances from related clusters. Firstly, we introduce Representative Dual Features (RepDFs) that possess representative capabilities in the feature space and the sample space for each cluster concurrently. Secondly, we explore hierarchical structural similarities between clusters in related tasks from the topological perspective: 1) feature basis matrix, which learns compact representations for features in the feature space; and 2) sample refined matrix, which preserves local structures in the sample space. Thirdly, we adopt RepDFs to measure correlations between clusters and incorporate hierarchical structural similarities to conduct knowledge transfer among tasks. Experimental results on real-world data sets demonstrate the effectiveness and superiority of the proposed method over existing multi-task clustering methods.","Multi-task learning, hierarchical structure, unsupervised learning, structural similarity,",Wenming Cao and Sheng Qian and Si Wu and Hau-San Wong,https://www.sciencedirect.com/science/article/pii/S0031320318303194,https://doi.org/10.1016/j.patcog.2018.08.021,0031-3203,2019,248--264,86,Pattern Recognition,Unsupervised Multi-task Learning with Hierarchical Data Structure,article,CAO2019248,
"Scene parsing is an indispensable component in understanding the semantics within a scene. Traditional methods rely on handcrafted local features and probabilistic graphical models to incorporate local and global cues. Recently, methods based on fully convolutional neural networks have achieved new records on scene parsing. An important strategy common to these methods is the aggregation of hierarchical features yielded by a deep convolutional neural network. However, typical algorithms usually aggregate hierarchical convolutional features via concatenation or linear combination, which cannot sufficiently exploit the diversities of contextual information in multi-scale features and the spatial inhomogeneity of a scene. In this paper, we propose a mixture-of-experts scene parsing network (MoE-SPNet) that incorporates a convolutional mixture-of-experts layer to assess the importance of features from different levels and at different spatial locations. In addition, we propose a variant of mixture-of-experts called the adaptive hierarchical feature aggregation (AHFA) mechanism which can be incorporated into existing scene parsing networks that use skip-connections to fuse features layer-wisely. In the proposed networks, different levels of features at each spatial location are adaptively re-weighted according to the local structure and surrounding contextual information before aggregation. We demonstrate the effectiveness of the proposed methods on two scene parsing datasets including PASCAL VOC 2012 and SceneParse150 based on two kinds of baseline models FCN-8s and DeepLab-ASPP.","Scene parsing, Mixture-of-experts, Attention, Convolutional neural network",Huan Fu and Mingming Gong and Chaohui Wang and Dacheng Tao,https://www.sciencedirect.com/science/article/pii/S0031320318302541,https://doi.org/10.1016/j.patcog.2018.07.020,0031-3203,2018,226--236,84,Pattern Recognition,MoE-SPNet: A mixture-of-experts scene parsing network,article,FU2018226,
"This paper presents a novel ellipse fitting method to simultaneously estimate the Euclidean pose and structure of a surface of revolution (SOR) by minimizing the geometric reprojection error of the visible cross sections in image space. This geometric error function and its Jacobian matrix are explicitly derived to enable Levenberg-Marquardt (LM) optimization. With the obtained pose and structure, the Euclidean shape of a SOR can be reconstructed by generating the ellipse tangency to the apparent contour of the SOR. Given the real size of several visible cross sections, this approach can be extended to perform a real-time 3D tracking of the SOR. Additionally, this technique can be also generalized to fitting for imaged parallel circles. Sufficient experiments validate the accuracy and the real-time performance of the proposed method.","Ellipse fitting, Pose estimation, Reconstruction, 3D tracking, Surface of revolution, Real time",Chang Liu and Weiduo Hu,https://www.sciencedirect.com/science/article/pii/S0031320318302784,https://doi.org/10.1016/j.patcog.2018.08.002,0031-3203,2019,90--108,85,Pattern Recognition,Real-time geometric fitting and pose estimation for surface of revolution,article,LIU201990,
"Some researchers have introduced the fuzzy learning into tracking and the kernelized fuzzy least squares support vector machine (FLS-SVM) has achieved great success in building the appearance model. However, the kernel used in FLS-SVM is fixed, which may potentially limit the adaptivity to different conditions. In this paper, we introduce metric learning into the FLS-SVM classifier and propose a novel tracking method based on the combination of fuzzy learning and metric learning to address the above issue. First, we propose a new fuzzy least squares support vector machine with metric learning (FLS-SVM-ML) algorithm, which embeds metric learning into the FLS-SVM method and is used to learn the kernel in FLS-SVM adaptively. Moreover, we present a two-stage iterative optimization process to solve the optimization problem. Second, we apply the proposed FLS-SVM-ML method into tracking based on the FLS-SVM tracking framework. By introducing the metric learning, the FLS-SVM-ML method can be used to improve the adaptivity of the appearance model to different video sequences and different frames in the same sequence. Experimental results demonstrate that the proposed tracking method can achieve competitive tracking results and outperform many state-of-the-art methods in the benchmark datasets.","Object tracking, Metric learning, Fuzzy least squares support vector machine with metric learning(FLS-SVM-ML)",Shunli Zhang and Wei Lu and Weiwei Xing and Li Zhang,https://www.sciencedirect.com/science/article/pii/S0031320318302267,https://doi.org/10.1016/j.patcog.2018.07.012,0031-3203,2018,112--125,84,Pattern Recognition,Using fuzzy least squares support vector machine with metric learning for object tracking,article,ZHANG2018112,
"Fitting discrete data obtained by image acquisition devices to a curve is a common task in many fields of science and engineering. In particular, the parabola is some of the most employed shape features in electrical engineering and telecommunication applications. Standard curve fitting techniques to solve this problem involve the minimization of squared errors. However, most of these procedures are sensitive to noise. Here, we propose an algorithm based on the minimization of absolute errors accompanied by a normalization of the directrix vector that leads to an improved stability of the method. This way, our proposal is substantially resilient to noisy samples in the input dataset. Experimental results demonstrate the good performance of the algorithm in terms of speed and accuracy when compared to previous approaches, both for synthetic and real data.","Parabolic fitting, Geometric curve fitting, Noise, Minimization of absolute errors, Robust estimation",Ezequiel LÃ³pez-Rubio and Karl Thurnhofer-Hemsi and Elidia Beatriz BlÃ¡zquez-Parra and Ãscar David {de CÃ³zar-MacÃ­as} and M. Carmen LadrÃ³n-de-Guevara-MuÃ±oz,https://www.sciencedirect.com/science/article/pii/S003132031830253X,https://doi.org/10.1016/j.patcog.2018.07.019,0031-3203,2018,301--316,84,Pattern Recognition,A fast robust geometric fitting method for parabolic curves,article,LOPEZRUBIO2018301,
"Probabilistic latent semantic analysis (PLSA) is a popular data analysis method with the objective to discover the underlying semantic structure of input data. In this work, we describe a method for probabilistic topic analysis in image and text based on a new representation of graph-regularized PLSA (GPLSA). In GPLSA, data entities are mapped to an undirected graph, where similarities between topic compositions on the graph are measured by the divergence between discrete probabilities. Such divergence is essentially incorporated as a graph-regularizer that augments the original PLSA algorithm. Furthermore, we extend the GPLSA algorithms to multiple data modalities based on the connections between data entities of each modality. We propose efficient multiplicative iterative algorithms for GPLSA with three popular regularizers, namely â1, â2 and symmetric KL divergences. In each case, we derive simple efficient numerical solutions that require only matrix arithmetic operations during the optimization. Experimental results demonstrate the efficacy of GPLSA over state-of-the-art methods.","Probabilistic latent semantic analysis, Graph regularization, Topic analysis, Clustering",Xin Wang and Ming-Ching Chang and Lan Wang and Siwei Lyu,https://www.sciencedirect.com/science/article/pii/S0031320318303236,https://doi.org/10.1016/j.patcog.2018.09.004,0031-3203,2019,236--247,86,Pattern Recognition,Efficient algorithms for graph regularized PLSA for probabilistic topic modeling,article,WANG2019236,
"Symmetry is an important feature in natural and man-made objects; particularly, mirror symmetry is a relevant task in fields such as computer vision and pattern recognition. In the current work, we propose a new method to characterize mirror-symmetry in open and closed curves represented by means of the Slope Chain Code. This representation is invariant under scale, rotation, and translation, highly desirable properties for object recognition applications. The proposed method detects symmetries through simple inversion, concatenation and reflection operations on the chains, thus allowing the classification of symmetrical and asymmetrical contours. It also introduces a measure to quantify the degree of symmetry in quasi-mirror-symmetrical objects. Furthermore, it allows the identification of multiple symmetry axes and their location. Results show high performances in symmetrical/asymmetrical classification (0.9 recall, 0.9 accuracy, 0.97 precision) and axesâ detection (0.8 recall, 0.84 accuracy, 0.99 precision). Compared to other methods, the proposed algorithm provides properties such as: global, local, and multiple axesâ detection, as well as the capability to classify symmetrical objects, which makes it adequate for several practical applications, like the three exemplified in the paper.","Symmetry detection, Slope Chain Code, Chain coding, 2D curves",Montserrat Alvarado-Gonzalez and Wendy Aguilar and Edgar GarduÃ±o and Carlos Velarde and Ernesto Bribiesca and VerÃ³nica Medina-BaÃ±uelos,https://www.sciencedirect.com/science/article/pii/S0031320318303479,https://doi.org/10.1016/j.patcog.2018.10.002,0031-3203,2019,67--79,87,Pattern Recognition,Mirror symmetry detection in curves represented by means of the Slope Chain Code,article,ALVARADOGONZALEZ201967,
"Laplacian support vector machine (SVM) for semi-supervised classification has attracted much attention in recent years. As an extension to improve the computational speed, Laplacian twin parametric-margin SVM (LTPSVM) has shown outstanding performance. However, it is still challenging to handle large-scale data. To address this issue, a safe sample screening rule (SSSR) for LTPSVM is proposed in this paper. It could significantly reduce the computational cost. Our proposed SSSR removes most redundant samples, and reduces the scale of optimization problems without sacrificing the optimal accuracy. The most important advantage of SSSR is the safety, i.e., the solutions are exactly the same as the original ones. Numerical experiments on both a synthetical data set and 14 real world data sets have verified the effectiveness of our proposed method.","Semi-supervised learning, Laplacian graph, Support vector machine, Safe screening",Zhiji Yang and Yitian Xu,https://www.sciencedirect.com/science/article/pii/S0031320318302309,https://doi.org/10.1016/j.patcog.2018.06.018,0031-3203,2018,1--12,84,Pattern Recognition,A safe sample screening rule for Laplacian twin parametric-margin support vector machine,article,YANG20181,
"We study sequential nonlinear regression and introduce an online algorithm that elegantly mitigates, via an adaptively incremental hierarchical structure, convergence and undertraining issues of conventional nonlinear regression methods. Particularly, we present a piecewise linear (or nonlinear) regression algorithm that partitions the regressor space and learns a linear model at each region to combine. Unlike the conventional approaches, our algorithm effectively learns the optimal regressor space partition with the desired complexity in a completely sequential and data driven manner. Our algorithm sequentially and asymptotically achieves the performance of the optimal twice differentiable regression function for any data sequence without any statistical assumptions. The introduced algorithm can be efficiently implemented with a computational complexity that is only logarithmic in the length of data. In our experiments, we demonstrate significant gains for the well-known benchmark real data sets when compared to the state-of-the-art techniques.","Online regression, Sequential learning, Nonlinear models, Incremental decision trees",N. Denizcan Vanli and Muhammed O. Sayin and Mohammadreza {Mohaghegh N.} and Huseyin Ozkan and Suleyman S. Kozat,https://www.sciencedirect.com/science/article/pii/S0031320318303121,https://doi.org/10.1016/j.patcog.2018.08.014,0031-3203,2019,1--13,86,Pattern Recognition,Nonlinear regression via incremental decision trees,article,VANLI20191,
"Good authentication performance and liveness detection are two key requirements in many authentication systems. To avoid replay attacks, a novel visual speaker authentication scheme with random prompt texts is proposed. Compared with the fixed password scenario, visual speaker authentication with random prompt texts is much more challenging because it is impossible to ask the client to pronounce every possible prompt text to be used as training samples. In order to solve this problem, a new deep convolutional neural network is proposed in this paper and it has three functional parts, namely, the lip feature network, the identity network, and the content network. In the lip feature network, a series of 3D residual units have been adopted, which can depict the static and dynamic characteristics of the lip biometrics comprehensively. By considering the distinguishing features of the identity and content authentication tasks, the identity network and the content network are designed accordingly. An end-to-end, multi-task learning scheme is proposed which can optimize the weights of all the above three networks simultaneously. Experiments have been carried out to evaluate the performance of the proposed network under both the fixed-password and the random prompt texts scenario. From the experimental results, it is shown that the proposed approach can achieve superior performance in the fixed-password scenario compared with several state-of-the-art approaches. Furthermore, it also achieves satisfactory authentication results in the random prompt texts scenario and thus it provides a reliable solution for user authentication where liveness is guaranteed.","Visual speaker authentication, Deep convolutional neural network, Multi-tasks learning, Liveness detection",Feng Cheng and Shi-Lin Wang and Alan Wee-Chung Liew,https://www.sciencedirect.com/science/article/pii/S0031320318302152,https://doi.org/10.1016/j.patcog.2018.06.005,0031-3203,2018,340--352,83,Pattern Recognition,Visual speaker authentication with random prompt texts by a dual-task CNN framework,article,CHENG2018340,
"Identifying the type of a scanned form image greatly facilitates automated processing, including field segmentation and field recognition. Contrary to most prior work, we focus on unsupervised type identification, where the possible form types for a given collection are not known apriori. Our target domain is noisy collections of form images that contain structurally similar, yet objectively different, form types, which are challenging to differentiate in an unsupervised setting. This work presents a novel algorithm: CONFIRM (Clustering Of Noisy Form Images using Robust Matching), which simultaneously discovers the set of form types in a collection and assigns a type to each form. CONFIRM matches type-set text and rule lines between forms to create collection-specific features, which we show outperform the Bag of Visual Word (BoVW) approach employed by the current state-of-the-art in form image clustering. CONFIRM scales well to large document collections with a bootstrap clustering process, in which only a small subset of the data is clustered directly, and the rest of the data is assigned to clusters in linear time. We show that CONFIRM reduces cluster impurity on average by 44% compared to the state-of-the art on 5 collections of historical forms that contain structurally similar form types.","Form processing, Document analysis, Document image clustering, Historical document processing, Clustering",Chris Tensmeyer and Tony Martinez,https://www.sciencedirect.com/science/article/pii/S0031320318303480,https://doi.org/10.1016/j.patcog.2018.10.004,0031-3203,2019,1--16,87,Pattern Recognition,CONFIRM â Clustering of noisy form images using robust matching,article,TENSMEYER20191,
"Mining negative sequential patterns (NSP) is an important tool for nonoccurring behavior analysis, and it is much more challenging than mining positive sequential patterns (PSPs) due to the high computational complexity and huge search space when obtaining the support of negative sequential candidates (NSCs). Very few NSP mining algorithms are available and most of them are very inefficient since they obtain the support of NSC by scanning the database repeatedly. Instead, the state-of-the-art NSP mining algorithm e-NSP only uses the PSPâs information stored in an array structure to âcalculateâ the support of NSC by equations, without database re-scanning. This makes e-NSP highly efficient, particularly on sparse datasets. However, when datasets become dense, the key process to obtain the support of NSC in e-NSP becomes very time-consuming and needs to be improved. In this paper, we propose a novel and efficient data structure, a bitmap, to obtain the support of NSC. We correspondingly propose a fast NSP mining algorithm, f-NSP, which uses a bitmap to store the PSPâs information and then obtain the support of NSC only by bitwise operations, which is much faster than the hash method in e-NSP. Experimental results on real-world and synthetic datasets show that f-NSP is not only tens to hundreds of times faster than e-NSP, but also saves more than ten-fold the storage spaces of e-NSP, particularly on dense datasets with a large number of elements in a sequence or a small number of itemsets. Further, we find that f-NSP consumes more storage space than e-NSP when PSPâs support is less than a support threshold sdsup, a value obtained through our theoretical analysis of storage space. Accordingly, we propose a self-adaptive storage strategy and a corresponding algorithm f-NSP+ to overcome this deficiency. f-NSP+ can automatically choose a bitmap or an array structure to store PSP information according to PSP support. Experimental results show that f-NSP+ saves more storage spaces of f-NSP, and has similar time efficiency as f-NSP.","Nonoccurring behavior analysis, Sequential patterns, Negative sequential patterns, Bitmap",Xiangjun Dong and Yongshun Gong and Longbing Cao,https://www.sciencedirect.com/science/article/pii/S0031320318302310,https://doi.org/10.1016/j.patcog.2018.06.016,0031-3203,2018,13--27,84,Pattern Recognition,F-NSP+: A fast negative sequential patterns mining method with self-adaptive data storage,article,DONG201813,
"This article proposes a framework for model-based point pattern learning using point process theory. Likelihood functions for point pattern data derived from point process theory enable principled yet conceptually transparent extensions of learning tasks, such as classification, novelty detection and clustering, to point pattern data. Furthermore, tractable point pattern models as well as solutions for learning and decision making from point pattern data are developed.","Point pattern, Point process, Random finite set, Multiple instance learning, Classification, Novelty detection, Clustering",Ba-Ngu Vo and Nhan Dam and Dinh Phung and Quang N. Tran and Ba-Tuong Vo,https://www.sciencedirect.com/science/article/pii/S0031320318302395,https://doi.org/10.1016/j.patcog.2018.07.008,0031-3203,2018,136--151,84,Pattern Recognition,Model-based learning for point pattern data,article,VO2018136,
"Approaches to design metrics between hidden Markov models (HMM) can be divided into two classes: data-based and parameter-based. The latter has the clear advantage of being deterministic and faster but only a very few similarity measures that can be applied to mixture-based HMMs have been proposed so far. Most of these metrics apply to the discrete or Gaussian HMMs and no comparative study have been led to the best of our knowledge. With the recent development of HMMs based on the Dirichlet and generalized Dirichlet distributions for proportional data modeling, we propose to design three new parametric similarity measures between these HMMs. Extensive experiments on synthetic data show the reliability of these new measures where the existing ones fail at giving expected results when some parameters vary. Illustration on real data show the clustering capability of these measures and their potential applications.","Hidden Markov models, Similarity measure, Dirichlet, Generalized Dirichlet",Elise Epaillard and Nizar Bouguila,https://www.sciencedirect.com/science/article/pii/S003132031830311X,https://doi.org/10.1016/j.patcog.2018.08.013,0031-3203,2019,207--219,85,Pattern Recognition,Data-free metrics for Dirichlet and generalized Dirichlet mixture-based HMMs â A practical study,article,EPAILLARD2019207,
"In this paper, to address the issue that ensembling k-nearest neighbor (kNN) classifiers with resampling approaches cannot generate component classifiers with a large diversity, we consider ensembling kNN through a multimodal perturbation-based method. Since kNN is sensitive to the input attributes, we propose a weighted heterogeneous distance Metric (WHDM). By using a WHDM and evidence theory, a progressive kNN classifier is developed. Based on a progressive kNN, the random subspace method, attribute reduction, and Bagging, a novel algorithm termed RRSB (reduced random subspace-based Bagging) is proposed for construct ensemble classifier, which can increase the diversity of component classifiers without damaging the accuracy of the component classifiers. In detail, RRSB adopts the perturbation on the learning parameter with a weighted heterogeneous distance metric, the perturbation on the input space with random subspace and attribute reduction, the perturbation on the training data with Bagging, and the perturbation on the output target of k neighbors with evidence theory. In the experimental stage, the value of k, the different perturbations on RRSB and the ensemble size are analyzed. In addition, RRSB is compared with other multimodal perturbation-based ensemble algorithms on multiple UCI data sets and a KDD data set. The results from the experiments demonstrate the effectiveness of RRSB for kNN ensembling.","Distance metric, -nearest neighbor, Ensemble learning, Random subspace, Evidence theory",Youqiang Zhang and Guo Cao and Bisheng Wang and Xuesong Li,https://www.sciencedirect.com/science/article/pii/S0031320318302796,https://doi.org/10.1016/j.patcog.2018.08.003,0031-3203,2019,13--25,85,Pattern Recognition,A novel ensemble method for k-nearest neighbor,article,ZHANG201913,
"Multi-label classification has been successfully applied to image annotation, information retrieval, text categorization, etc. When the number of classes increases significantly, the traditional multi-label learning models will become computationally impractical. Label space dimension reduction (LSDR) is then developed to alleviate the effect of the high dimensionality of labels. However, almost all the existing LSDR methods focus on single-view learning. In this paper, we develop a multi-view label embedding (MVLE) model by exploiting the multi-view correlations. The label space and feature space of each view are bridged by a latent space. To exploit the consensus among different views, multi-view latent spaces are correlated by HilbertâSchmidt independence criterion(HSIC). For a test sample, it is firstly embedded to the latent space of each view and then projected to the label space. The prediction is conducted by combining the multi-view outputs. Experiments on benchmark databases show that MVLE outperforms the state-of-the-art LSDR algorithms in both multi-view settings and different multi-view learning strategies.","Multi-label classification, Multi-view label embedding, Label space dimension reduction",Pengfei Zhu and Qi Hu and Qinghua Hu and Changqing Zhang and Zhizhao Feng,https://www.sciencedirect.com/science/article/pii/S0031320318302279,https://doi.org/10.1016/j.patcog.2018.07.009,0031-3203,2018,126--135,84,Pattern Recognition,Multi-view label embedding,article,ZHU2018126,
"Multi-label feature selection has grabbed intensive attention in many big data applications. However, traditional multi-label feature selection methods generally ignore a real-world scenario, i.e., the features constantly flow into the model one by one over time. To address this problem, we develop a novel online multi-label streaming feature selection method based on neighborhood rough set to select a feature subset which contains strongly relevant and non-redundant features. The main motivation is that data mining based on neighborhood rough set does not require any priori knowledge of the feature space structure. Moreover, neighborhood rough set deals with mixed data without breaking the neighborhood and order structure of data. In this paper, we first introduce the maximum-nearest-neighbor of instance to granulate all instances which can solve the problem of granularity selection in neighborhood rough set, and then generalize neighborhood rough set in single-label to fit multi-label learning. Meanwhile, an online multi-label streaming feature selection framework, which includes online importance selection and online redundancy update, is presented. Under this framework, we propose a criterion to select the important features relative to the currently selected features, and design a bound on pairwise correlations between features under label set to filter out redundant features. An empirical study using a series of benchmark datasets demonstrates that the proposed method outperforms other state-of-the-art multi-label feature selection methods.","Online feature selection, Multi-label learning, Neighborhood rough set, Granularity",Jinghua Liu and Yaojin Lin and Yuwen Li and Wei Weng and Shunxiang Wu,https://www.sciencedirect.com/science/article/pii/S0031320318302553,https://doi.org/10.1016/j.patcog.2018.07.021,0031-3203,2018,273--287,84,Pattern Recognition,Online multi-label streaming feature selection based on neighborhood rough set,article,LIU2018273,
"The recovery of the intrinsic geometric structures of data collections is an important problem in data analysis. Supervised extensions of several manifold learning approaches have been proposed in the recent years. Meanwhile, existing methods primarily focus on the embedding of the training data, and the generalization of the embedding to initially unseen test data is rather ignored. In this work, we build on recent theoretical results on the generalization performance of supervised manifold learning algorithms. Motivated by these performance bounds, we propose a supervised manifold learning method that computes a nonlinear embedding while constructing a smooth and regular interpolation function that extends the embedding to the whole data space in order to achieve satisfactory generalization. The embedding and the interpolator are jointly learnt such that the Lipschitz regularity of the interpolator is imposed while ensuring the separation between different classes. Experimental results on several image data sets show that the proposed method outperforms traditional classifiers and the supervised dimensionality reduction algorithms in comparison in terms of classification accuracy in most settings.","Manifold learning, Dimensionality reduction, Supervised learning, Out-of-sample, Nonlinear embeddings",Cem Ãrnek and Elif Vural,https://www.sciencedirect.com/science/article/pii/S0031320318303522,https://doi.org/10.1016/j.patcog.2018.10.006,0031-3203,2019,55--66,87,Pattern Recognition,Nonlinear supervised dimensionality reduction via smooth regular embeddings,article,ORNEK201955,
"Multiple ordinal output classification (MOOC) which specifically refers to learning an association between individual inputs (e.g. face images) and a set of discrete ordinal response/output variables (e.g. facial action units), is a special case of multi-output classification and also a relatively-understudied topic in machine learning. It is very challenging in how to jointly model the relationship among multiple output variables and their discrete ordinal values. In this paper, we propose an effective formulation to address the above challenging problems. Under this formulation, the objective function is convex and thus leads to a convex multiple ordinal output classification (ConMOOC). Specifically, we use a regularization formulation to model the relationships among multiple output variables and an effective threshold-based loss function to fit their ordinal values. To enhance ability of the model, we also apply the kernel trick to provide a nonlinear extension. For efficiency, we use an alternating iteration method to learn the optimal model parameters for each variable as well as the relationships between different variables. Experiments conducted on synthetic and real datasets demonstrate that ConMOOC not only achieves effective classification performance but also reveals the structures among output variables. To the best of our knowledge, MOOC as a general machine learning task is the first time to be studied.","Multiple ordinal output classification, Multiple discrete ordinal variables, Ordinal regression, Relationships, Convex function",Zhongchen Ma and Songcan Chen,https://www.sciencedirect.com/science/article/pii/S0031320318303248,https://doi.org/10.1016/j.patcog.2018.09.005,0031-3203,2019,73--84,86,Pattern Recognition,A convex formulation for multiple ordinal output classification,article,MA201973,
"Lung cancer is one of the leading causes of cancer-related death worldwide. Early diagnosis can effectively reduce the mortality, and computer-aided diagnosis (CAD) as an important way to assist doctors has developed rapidly. In particular, automated pulmonary nodule detection in computed tomography (CT) images is crucial to CAD. It is a challenging task to quickly locate the exact positions of lung nodules. In this paper, a novel automated pulmonary nodule detection framework with 2D convolutional neural network (CNN) is proposed to assist the CT reading process. Firstly, we adjust the structure of Faster R-CNN with two region proposal networks and a deconvolutional layer to detect nodule candidates, and then three models are trained for three kinds of slices for later result fusion. Secondly, a boosting architecture based on 2D CNN is designed for false positive reduction, which is a classifier to distinguish true nodules from the candidates. The misclassified samples are still kept for retraining a model which boosts the sensitivity for pulmonary nodule detection. Finally, the results of these networks are fused to vote out the final classification results. Extensive experiments are conducted on LUNA16, and the sensitivity of nodule candidate detection achieves 86.42%. For the false positive reduction, the sensitivity reaches 73.4% and 74.4% at 1/8 and 1/4 FPs/scan, respectively. It illustrates that the proposed method can obviously achieve accurate pulmonary nodule detection.","Nodule detection, Convolutional neural network, False positive reduction, Computer-aided diagnosis",Hongtao Xie and Dongbao Yang and Nannan Sun and Zhineng Chen and Yongdong Zhang,https://www.sciencedirect.com/science/article/pii/S0031320318302711,https://doi.org/10.1016/j.patcog.2018.07.031,0031-3203,2019,109--119,85,Pattern Recognition,Automated pulmonary nodule detection in CT images using deep convolutional neural networks,article,XIE2019109,
"In this work, we propose a novel deep Hierarchical Guidance and Regularization (HGR) learning framework for end-to-end monocular depth estimation, which well integrates a hierarchical depth guidance network and a hierarchical regularization learning method for fine-grained depth prediction. The two properties in our proposed HGR framework can be summarized as: (1) the hierarchical depth guidance network automatically learns hierarchical depth representations by supervision guidance and multiple side conv-operations from the basic CNN, leveraging the learned hierarchical depth representations to progressively guide the upsampling and prediction process of upper deconv-layers; (2) the hierarchical regularization learning method integrates various-level information of depth maps, optimizing the network to predict depth maps with similar structure to ground truth. Comprehensive evaluations over three public benchmark datasets (including NYU Depth V2, KITTI and Make3D datasets) well demonstrate the state-of-the-art performance of our proposed depth estimation framework.","Depth estimation, Multi-regularization, Deep neural network",Zhenyu Zhang and Chunyan Xu and Jian Yang and Ying Tai and Liang Chen,https://www.sciencedirect.com/science/article/pii/S0031320318301869,https://doi.org/10.1016/j.patcog.2018.05.016,0031-3203,2018,430--442,83,Pattern Recognition,Deep hierarchical guidance and regularization learning for end-to-end depth estimation,article,ZHANG2018430,
"Open-set activity recognition remains as a challenging problem because of complex activity diversity. In previous works, extensive efforts have been paid to construct a negative set or set an optimal threshold for the target set. In this paper, a model based on Generative Adversarial Network (GAN), called âOpenGANâ is proposed to address the open-set recognition without manual intervention during the training process. The generator produces fake target samples, which serve as an automatic negative set, and the discriminator is redesigned to output multiple categories together with an âunknownâ class. We evaluate the effectiveness of the proposed method on measured micro-Doppler radar dataset and the MOtion CAPture (MOCAP) database from Carnegie Mellon University (CMU). The comparison results with several state-of-the-art methods indicate that OpenGAN provides a promising open-set solution to human activity recognition even under the circumstance with few known classes. Ablation studies are also performed, and it is shown that the proposed architecture outperforms other variants and is robust on both datasets.","Open-set recognition, Generative adversarial network (GAN), Human activity, Micro-Doppler radar",Yang Yang and Chunping Hou and Yue Lang and Dai Guan and Danyang Huang and Jinchen Xu,https://www.sciencedirect.com/science/article/pii/S003132031830270X,https://doi.org/10.1016/j.patcog.2018.07.030,0031-3203,2019,60--69,85,Pattern Recognition,Open-set human activity recognition based on micro-Doppler signatures,article,YANG201960,
"Binarization plays a key role in the automatic information retrieval from document images. This process is usually performed in the first stages of document analysis systems, and serves as a basis for subsequent steps. Hence it has to be robust in order to allow the full analysis workflow to be successful. Several methods for document image binarization have been proposed so far, most of which are based on hand-crafted image processing strategies. Recently, Convolutional Neural Networks have shown an amazing performance in many disparate duties related to computer vision. In this paper we discuss the use of convolutional auto-encoders devoted to learning an end-to-end map from an input image to its selectional output, in which activations indicate the likelihood of pixels to be either foreground or background. Once trained, documents can therefore be binarized by parsing them through the model and applying a global threshold. This approach has proven to outperform existing binarization strategies in a number of document types.","Binarization, Document analysis, Auto-encoders, Convolutional Neural Networks",Jorge Calvo-Zaragoza and Antonio-Javier Gallego,https://www.sciencedirect.com/science/article/pii/S0031320318303091,https://doi.org/10.1016/j.patcog.2018.08.011,0031-3203,2019,37--47,86,Pattern Recognition,A selectional auto-encoder approach for document image binarization,article,CALVOZARAGOZA201937,
"Orthogonal polynomial kernels have been recently introduced to enhance support vector machine classifiers by reducing their number of support vectors. Previous works have studied these kernels as isolated cases and discussed only particular aspects. In this paper, a novel formulation of orthogonal polynomial kernels that includes and improves previous proposals (Legendre, Chebyshev and Hermite) is presented. Two undesired effects that must be avoided in order to use orthogonal polynomial kernels are identified and resolved: the Annihilation and the Explosion effects. The proposed formulation is studied by means of introducing a new family of orthogonal polynomial kernels based on Gegenbauer polynomials and comparing it against other kernels. Experimental results reveal that the Gegenbauer family competes with the RBF kernel in accuracy while requiring fewer support vectors and overcomes other classical and orthogonal kernels.","SVM classifier, Orthogonal polynomials, Gegenbauer kernel, Binary classification",Luis Carlos Padierna and MartÃ­n Carpio and Alfonso Rojas-DomÃ­nguez and HÃ©ctor Puga and HÃ©ctor Fraire,https://www.sciencedirect.com/science/article/pii/S0031320318302280,https://doi.org/10.1016/j.patcog.2018.07.010,0031-3203,2018,211--225,84,Pattern Recognition,A novel formulation of orthogonal polynomial kernel functions for SVM classifiers: The Gegenbauer family,article,PADIERNA2018211,
"Completely automated iris recognition has emerged as an integral part of e-business and e-governance infrastructure which has acquired billions of iris images under near-infrared illumination to establish the identity of individuals. A range of e-business and surveillance applications can provide iris images that are acquired under visible illumination. Therefore, development of accurate cross-spectral iris matching capabilities is highly desirable. This paper investigates cross-spectral iris recognition using a range of deep learning architectures. Our experimental results on two publicly available cross-spectral iris databases, from 209 and 120 different subjects respectively, indicate outperforming results and validate our approach for the cross-spectral iris matching. Our observations indicate that the self-learned features generated from the convolution neural networks (CNN) are generally sparse and offer great potential for template compression. Therefore, this paper also introduces the iris recognition with supervised discrete hashing that can not only achieve more accurate performance but also offer a significant reduction in the size of iris templates. Most accurate cross-spectral matching performance is achieved by incorporating supervised discrete hashing on the features learned from the trained CNN with softmax cross-entropy loss. The proposed approach not only achieves outperforming results over other considered CNN architecture but also offers significantly reduced template size as compared with the other iris recognition methods available in the literature.","Cross-spectral, Iris recognition, Deep learning, Convolutional neural network, Hashing",Kuo Wang and Ajay Kumar,https://www.sciencedirect.com/science/article/pii/S003132031830308X,https://doi.org/10.1016/j.patcog.2018.08.010,0031-3203,2019,85--98,86,Pattern Recognition,Cross-spectral iris recognition using CNN and supervised discrete hashing,article,WANG201985,
"Person re-identification (ReID) is a challenging task due to arbitrary human pose variations, background clutters, etc. It has been studied extensively in recent years, but the multifarious local and global features are still not fully exploited by either ignoring the interplay between whole-body images and body-part images or missing in-depth examination of specific body-part images. In this paper, we propose a novel attention-driven multi-branch network that learns robust and discriminative human representation from global whole-body images and local body-part images simultaneously. Within each branch, an intra-attention network is designed to search for informative and discriminative regions within the whole-body or body-part images, where attention is elegantly decomposed into spatial-wise attention and channel-wise attention for effective and efficient learning. In addition, a novel inter-attention module is designed which fuses the output of intra-attention networks adaptively for optimal person ReID. The proposed technique has been evaluated over three widely used datasets CUHK03, Market-1501 and DukeMTMC-ReID, and experiments demonstrate its superior robustness and effectiveness as compared with the state of the arts.","Person re-identification, Visual attention, Pose estimation, Deep neural networks",Fan Yang and Ke Yan and Shijian Lu and Huizhu Jia and Xiaodong Xie and Wen Gao,https://www.sciencedirect.com/science/article/pii/S0031320318303133,https://doi.org/10.1016/j.patcog.2018.08.015,0031-3203,2019,143--155,86,Pattern Recognition,Attention driven person re-identification,article,YANG2019143,
"Steganalysis research is committed to distinguishing the steganography media from the normal one correctly. However, individual differences of carriers disturb the detection inevitably and greatly. Existing methods treat all detection results with the same confidence level, or prior accuracy, which may make the prior accuracy overestimate or underestimate the real result. This paper presents a novel performance evaluation method of steganalysis based on posterior accuracy. Adaptive Convolution Feature (ACF) is calculated by the adaptive convolution, then a quantitative value S based on the ACF is modeled to posterior testify and estimate the detection confidence of the image under test. By clustering of carrier noise, we classify the images which we believe they have a similar confidence of the same cluster. The distribution of ACF from each cluster indicates that the S value works well for confidence evaluation. The experimental results show that the S value can distinguish and identify high-confidence samples from the low-confidence, which will greatly improve the real performance of steganalysis. Besides, it improves the steganalysis accuracy of the whole image set by matching S values between training and prediction samples.","Steganography, Steganalysis, Carrier security, Adaptive convolution, Posterior accuracy",Lina Wang and Yibo Xu and Liming Zhai and Yanzhen Ren and Bo Du,https://www.sciencedirect.com/science/article/pii/S0031320318303492,https://doi.org/10.1016/j.patcog.2018.10.003,0031-3203,2019,106--117,87,Pattern Recognition,A posterior evaluation algorithm of steganalysis accuracy inspired by residual co-occurrence probability,article,WANG2019106,
"High inter-personal similarity has been universally acknowledged as the principal challenge of automatic face recognition since the earliest days of research in this area. The challenge is particularly prominent when images or videos are acquired in largely unconstrained conditions âin the wildâ, and intra-personal variability due to illumination, pose, occlusions, and a variety of other confounds is extreme. Counter to the general consensus and intuition, in this paper I demonstrate that in some contexts, high inter-personal similarity can be used to advantage, i.e. it can help improve recognition performance. I start by a theoretical introduction of this key conceptual novelty which I term âquasi-transitive similarityâ, describe an approach that implements it in practice, and demonstrate its effectiveness empirically. The results on a most challenging real-world data set show impressive performance, and open avenues to future research on different technical approaches which make use of this novel idea.","Meta-algorithm, Paradigm change, Retrieval, Intra-class, Inter-class, Similarity, Dissimilarity",Ognjen ArandjeloviÄ,https://www.sciencedirect.com/science/article/pii/S0031320318302164,https://doi.org/10.1016/j.patcog.2018.06.006,0031-3203,2018,388--400,83,Pattern Recognition,Reimagining the central challenge of face recognition: Turning a problem into an advantage,article,ARANDJELOVIC2018388,
"Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms.","Adversarial machine learning, Evasion attacks, Poisoning attacks, Adversarial examples, Secure learning, Deep learning",Battista Biggio and Fabio Roli,https://www.sciencedirect.com/science/article/pii/S0031320318302565,https://doi.org/10.1016/j.patcog.2018.07.023,0031-3203,2018,317--331,84,Pattern Recognition,Wild patterns: Ten years after the rise of adversarial machine learning,article,BIGGIO2018317,
"A complex network is a condensed representation of the relational topological framework of a complex system. A main reason for the existence of such networks is the transmission of items through the entities of these complex systems. Here, we consider a communicability function that accounts for the routes through which items flow on networks. Such a function induces a natural embedding of a network in a Euclidean high-dimensional sphere. We use one of the geometric parameters of this embedding, namely the angle between the position vectors of the nodes in the hyperspheres, to extract structural information from networks. First we propose a simple method for visualizing networks by reducing the dimensionality of the communicability space to 3-dimensional spheres. Secondly, we use clustering analysis to cluster the nodes of the networks based on their similarities in terms of their capacity to successfully deliver information through the network. After testing these approaches in benchmark networks and compare them with the most used clustering methods in networks we analyze two real-world examples. In the first, consisting of a citation network, we discover citation groups that reflect the level of mathematics used in their publications. In the second, we discover groups of genes that coparticipate in human diseases, reporting a few genes that coparticipate in cancer and other diseases. Both examples emphasize the potential of the current methodology for the discovery of new patterns in relational data.","Networks, Clustering algorithms, Geometric embedding, Communicability, Matrix functions, Network communities",MarÃ­a Pereda and Ernesto Estrada,https://www.sciencedirect.com/science/article/pii/S0031320318303418,https://doi.org/10.1016/j.patcog.2018.09.018,0031-3203,2019,320--331,86,Pattern Recognition,Visualization and machine learning analysis of complex networks in hyperspherical space,article,PEREDA2019320,
"Subspace clustering refers to the problem of finding low-dimensional subspaces (clusters) for high-dimensional data. Current state-of-the-art subspace clustering methods are usually based on spectral clustering, where an affinity matrix is learned by the self-expressive model, i.e., reconstructing every data point by a linear combination of all other points while regularizing the coefficients using the â1 norm. The sparsity nature of â1 norm guarantees the subspace-preserving property (i.e., no connection between clusters) of affinity matrix under certain condition, but the connectedness property (i.e., fully connected within clusters) is less considered. In this paper, we propose a novel affinity learning method by incorporating the sparse representation and diffusion process. Instead of using sparse coefficients directly as the affinity values, we apply the â1 norm as a neighborhood selection criterion, which could capture the local manifold structure. An effective diffusion process is then deployed to spread such local information along with the global geometry of data manifold. Each pairwise affinity is augmented and re-evaluated by the context of data point pair, yielding significant enhancements of within-cluster connectivity. Extensive experiments on synthetic data and real-world data have demonstrated the effectiveness of the proposed method in comparison to other state-of-the-art methods.","Subspace clustering, Diffusion process, Affinity learning",Qilin Li and Wanquan Liu and Ling Li,https://www.sciencedirect.com/science/article/pii/S0031320318302322,https://doi.org/10.1016/j.patcog.2018.07.002,0031-3203,2018,39--50,84,Pattern Recognition,Affinity learning via a diffusion process for subspace clustering,article,LI201839,
"In this paper, we present a new method that provides a substantial speed-up of person detection while showing high classification accuracy. Our method learns a Gaussian Mixture Model of locations and scales of the persons in the scene under observation. The model is learnt in an unsupervised way from a set of detections extracted from a small number of frames, so that each component of the mixture represents the expectation of finding a target in a region of the image at a specific scale. At runtime, the windows that most likely contain a person are sampled from the components and evaluated by the classifier. Experimental results show that replacing the classic sliding window approach with our scene-dependent proposals in state of the art person detectors allows us to drastically reduce the computational complexity while granting equal or higher performance in terms of accuracy.","Person detection, Scene-dependent proposals, Gaussian mixture model, Scene modelling",Federico Bartoli and Giuseppe Lisanti and Svebor Karaman and Alberto Del Bimbo,https://www.sciencedirect.com/science/article/pii/S0031320318303534,https://doi.org/10.1016/j.patcog.2018.10.008,0031-3203,2019,170--178,87,Pattern Recognition,Scene-dependent proposals for efficient person detection,article,BARTOLI2019170,
"In this paper, a novel method for introducing multiplex data relationships to the SVM optimization process is presented. Different properties about the training data are encoded in graph structures, in the form of pairwise data relationships. Then, they are incorporated to the SVM optimization problem, as modified graph-regularized basekernels, each highlighting a different property about the training data. The contribution of each graph-regularized kernel to the SVM classification problem, is estimated automatically. Thereby, the solution of the proposed modified SVM optimization problem lies in a regularized space, where data similarity is expressed by a linear combination of multiple single-graph regularized kernels. The proposed method exploits and extends the findings of Multiple Kernel Learning and graph-based SVM method families. It is shown that the available kernel options for the former can be broadened, and the exhaustive parameter tuning for the latter can be eliminated. Moreover, both method families can be considered as special cases of the proposed formulation, hereafter. Our experimental evaluation in visual data classification problems denote the superiority of the proposed method. The obtained classification performance gains can be explained by the exploitation of multiplex data relationships, during the classifier optimization process.","Multiplex data relationships, Support Vector Machine, Graph-based regularization, Multiple Kernel Learning",Vasileios Mygdalis and Anastasios Tefas and Ioannis Pitas,https://www.sciencedirect.com/science/article/pii/S0031320318302723,https://doi.org/10.1016/j.patcog.2018.07.032,0031-3203,2019,70--77,85,Pattern Recognition,Exploiting multiplex data relationships in Support Vector Machines,article,MYGDALIS201970,
"Within a supervised classification framework, labeled data are used to learn classifier parameters. Prior to that, it is generally required to perform dimensionality reduction via feature extraction. These preprocessing steps have motivated numerous research works aiming at recovering latent variables in an unsupervised context. This paper proposes a unified framework to perform classification and low-level modeling jointly. The main objective is to use the estimated latent variables as features for classification and to incorporate simultaneously supervised information to help latent variable extraction. The proposed hierarchical Bayesian model is divided into three stages: a first low-level modeling stage to estimate latent variables, a second stage clustering these features into statistically homogeneous groups and a last classification stage exploiting the (possibly badly) labeled data. Performance of the model is assessed in the specific context of hyperspectral image interpretation, unifying two standard analysis techniques, namely unmixing and classification.","Bayesian model, Supervised learning, Image interpretation, Markov random field",Adrien Lagrange and Mathieu Fauvel and StÃ©phane May and Nicolas Dobigeon,https://www.sciencedirect.com/science/article/pii/S0031320318302607,https://doi.org/10.1016/j.patcog.2018.07.026,0031-3203,2019,26--36,85,Pattern Recognition,Hierarchical Bayesian image analysis: From low-level modeling to robust supervised learning,article,LAGRANGE201926,
"Many historical manuscripts that hold trustworthy memories of the past societies contain information organized in a structured layout (e.g. census, birth or marriage records). The precious information stored in these documents cannot be effectively used nor accessed without costly annotation efforts. The transcription driven by the semantic categories of words is crucial for the subsequent access. In this paper we describe an approach to extract information from structured historical handwritten text images and build a knowledge representation for the extraction of meaning out of historical data. The method extracts information, such as named entities, without the need of an intermediate transcription step, thanks to the incorporation of context information through language models. Our system has two variants, the first one is based on bigrams, whereas the second one is based on recurrent neural networks. Concretely, our second architecture integrates a Convolutional Neural Network to model visual information from word images together with a Bidirecitonal Long Short Term Memory network to model the relation among the words. This integrated sequential approach is able to extract more information than just the semantic category (e.g. a semantic category can be associated to a person in a record). Our system is generic, it deals with out-of-vocabulary words by design, and it can be applied to structured handwritten texts from different domains. The method has been validated with the ICDAR IEHHR competition protocol, outperforming the existing approaches.","Document image analysis, Handwritten documents, Named entity recognition, Deep neural networks",J. Ignacio Toledo and Manuel Carbonell and Alicia FornÃ©s and Josep LladÃ³s,https://www.sciencedirect.com/science/article/pii/S0031320318303145,https://doi.org/10.1016/j.patcog.2018.08.020,0031-3203,2019,27--36,86,Pattern Recognition,Information extraction from historical handwritten document images with a context-aware neural model,article,TOLEDO201927,
"Feature selection is one of the trending challenges in multi-label classification. In recent years a lot of methods have been proposed. However the existing approaches assume that all the features have the same cost. This assumption may be inappropriate when the acquisition of the feature values is costly. For example in medical diagnosis each diagnostic value extracted by a clinical test is associated with its own cost. In such cases it may be better to choose a model with an acceptable classification performance but a much lower cost. We propose a novel method which incorporates the feature cost information into the learning process. The method, named Cost-Sensitive Classifier Chains, combines classifier chains and penalized logistic regression with a modified elastic-net penalty which takes into account costs of the features. We prove the stability and provide a bound on generalization error of our algorithm. We also propose the adaptive version in which penalty factors are changing during fitting the consecutive models in the chain. The methods are applied on real datasets: MIMIC-II and Hepatitis for which the cost information is provided by experts. Moreover, we propose an experimental framework in which the features are observed with measurement errors and the costs depend on the quality of the features. The framework allows to compare the cost-sensitive methods on benchmark datasets for which the cost information is not provided. The proposed method can be recommended in a situation when one wants to balance low costs and high prediction performance.","Multi-label classification, Cost-sensitive feature selection, Classifier chains, Logistic regression, Stability, Generalization error bounds",PaweÅ Teisseyre and Damien Zufferey and Marta SÅomka,https://www.sciencedirect.com/science/article/pii/S0031320318303388,https://doi.org/10.1016/j.patcog.2018.09.012,0031-3203,2019,290--319,86,Pattern Recognition,Cost-sensitive classifier chains: Selecting low-cost features in multi-label classification,article,TEISSEYRE2019290,
"Being able to describe the content of an image, adapted to a particular application, is essential in various domains related to image analysis and pattern recognition. In this context, taking into account the spatial organization of objects is fundamental to increase both the understanding and the accuracy of the perceived similarity between images. In this article, we first present the Force Histogram Decomposition (FHD), a graph-based hierarchical descriptor that allows to characterize the spatial relations and shape information between the pairwise structural subparts of objects. Then, we propose a novel bags-of-features framework based on such descriptors, in order to produce discriminative structural features that are tailored for particular object classification tasks. An advantage of this learning procedure is its compatibility with traditional bags-of-features frameworks, allowing for hybrid representations gathering structural and local features. Experimental results obtained both on the recognition of structured objects from color images and on a parts-based scene recognition task highlight the interest of this approach.","Spatial relations, Relative position descriptors, Bags of relations, Structural object description, Hierarchical representation, Force histograms",MichaÃ«l ClÃ©ment and Camille Kurtz and Laurent Wendling,https://www.sciencedirect.com/science/article/pii/S0031320318302292,https://doi.org/10.1016/j.patcog.2018.06.017,0031-3203,2018,197--210,84,Pattern Recognition,Learning spatial relations and shapes for structural object description and scene recognition,article,CLEMENT2018197,
"We propose a sparse Convolutional Autoencoder (CAE) for simultaneous nucleus detection and feature extraction in histopathology tissue images. Our CAE detects and encodes nuclei in image patches in tissue images into sparse feature maps that encode both the location and appearance of nuclei. A primary contribution of our work is the development of an unsupervised detection network by using the characteristics of histopathology image patches. The pretrained nucleus detection and feature extraction modules in our CAE can be fine-tuned for supervised learning in an end-to-end fashion. We evaluate our method on four datasets and achieve state-of-the-art results. In addition, we are able to achieve comparable performance with only 5% of the fully-supervised annotation cost.","Pathology image analysis, Convolutional neural network, Unsupervised learning, Semi-supervised learning",Le Hou and Vu Nguyen and Ariel B. Kanevsky and Dimitris Samaras and Tahsin M. Kurc and Tianhao Zhao and Rajarsi R. Gupta and Yi Gao and Wenjin Chen and David Foran and Joel H. Saltz,https://www.sciencedirect.com/science/article/pii/S0031320318303261,https://doi.org/10.1016/j.patcog.2018.09.007,0031-3203,2019,188--200,86,Pattern Recognition,Sparse autoencoder for unsupervised nucleus detection and representation in histopathology images,article,HOU2019188,
"We present, in this paper, a novel paradigm for assessing Alzheimerâs disease and aging by analyzing impairment of handwriting (HW) on tablets, a challenging problem that is still in its infancy. The state of the art is dominated by methods that assume a unique behavioral trend for each cognitive profile or age group, and that extract global kinematic parameters, assessed by standard statistical tests or classification models, for discriminating the neuropathological disorders (Alzheimerâs (AD), Mild Cognitive Impairment (MCI)) from Healthy Controls (HC), or HC age groups from each other. Our work tackles these two major limitations as follows. First, instead of considering a unique behavioral pattern for each cognitive profile or age group, we relax this heavy constraint by allowing the emergence of multimodal behavioral patterns. We achieve this by performing semi or unsupervised learning to uncover homogeneous clusters of subjects, and then we analyze how much information these clusters carry on the cognitive profiles (or age groups). Second, instead of relying on global kinematic parameters, mostly consisting of their average, we refine the encoding either by a semi-global parameterization, or by modeling the full dynamics of each parameter, harnessing thereby the rich temporal information inherently characterizing online HW. To illustrate the power of our paradigm, we present three studies, one regarding age, and two regarding Alzheimerâs. Thanks to our modeling, we obtain new findings that are the first of their kind on this research field. On aging, unlike previous works reporting only one pattern of HW change with age, our study, based on a semiglobal parametrization scheme, uncovers three major aging HW styles, one specific to aged subjects and two shared with other age groups. On Alzheimerâs, a striking finding is revealed: two major clusters are unveiled, one dominated by HC and MCI subjects, and one by MCI and ES-AD, thus revealing that MCI patients have fine motor skills leaning towards either HCâs or ES-ADâs. Our paper introduces also a new temporal representation learning from HW trajectories that uncovers a rich set of features simultaneously like the full velocity profile, size and slant, fluidity, and shakiness, and reveals, in a naturally explainable way, how these HW features conjointly characterize, with fine and subtle details, the cognitive profiles.","Online handwriting, âs, Mild Cognitive Impairment, Aging, Unsupervised & semi-supervised learning, Temporal representation learning",MounÃ®m A. El-Yacoubi and Sonia Garcia-Salicetti and Christian Kahindo and Anne-Sophie Rigaud and Victoria Cristancho-Lacroix,https://www.sciencedirect.com/science/article/pii/S0031320318302693,https://doi.org/10.1016/j.patcog.2018.07.029,0031-3203,2019,112--133,86,Pattern Recognition,From aging to early-stage Alzheimer's: Uncovering handwriting multimodal behaviors by semi-supervised learning and sequential representation learning,article,ELYACOUBI2019112,
"Generalizability of algorithms for binary cancer vs. no cancer classification is unknown for clinically more significant multi-class scenarios where intermediate categories have different risk factors and treatment strategies. We present a system that classifies whole slide images (WSI) of breast biopsies into five diagnostic categories. First, a saliency detector that uses a pipeline of four fully convolutional networks, trained with samples from records of pathologistsâ screenings, performs multi-scale localization of diagnostically relevant regions of interest in WSI. Then, a convolutional network, trained from consensus-derived reference samples, classifies image patches as non-proliferative or proliferative changes, atypical ductal hyperplasia, ductal carcinoma in situ, and invasive carcinoma. Finally, the saliency and classification maps are fused for pixel-wise labeling and slide-level categorization. Experiments using 240 WSI showed that both saliency detector and classifier networks performed better than competing algorithms, and the five-class slide-level accuracy of 55% was not statistically different from the predictions of 45 pathologists. We also present example visualizations of the learned representations for breast cancer diagnosis.","Digital pathology, Breast histopathology, Whole slide imaging, Region of interest detection, Saliency detection, Multi-class classification, Deep learning",Baris Gecer and Selim Aksoy and Ezgi Mercan and Linda G. Shapiro and Donald L. Weaver and Joann G. Elmore,https://www.sciencedirect.com/science/article/pii/S0031320318302577,https://doi.org/10.1016/j.patcog.2018.07.022,0031-3203,2018,345--356,84,Pattern Recognition,Detection and classification of cancer in whole slide breast histopathology images using deep convolutional networks,article,GECER2018345,
"Multimodal learning has been an important and challenging problem for decades, which aims to bridge the modality gap between heterogeneous representations, such as vision and language. Unlike many current approaches which only focus on either multimodal matching or classification, we propose a unified network to jointly learn multimodal matching and classification (MMC-Net) between images and texts. The proposed MMC-Net model can seamlessly integrate the matching and classification components. It first learns visual and textual embedding features in the matching component, and then generates discriminative multimodal representations in the classification component. Combining the two components in a unified model can help in improving their performance. Moreover, we present a multi-stage training algorithm by minimizing both of the matching and classification loss functions. Experimental results on four well-known multimodal benchmarks demonstrate the effectiveness and efficiency of the proposed approach, which achieves competitive performance for multimodal matching and classification compared to state-of-the-art approaches.","Vision and language, Multimodal matching, Multimodal classification, Deep learning",Yu Liu and Li Liu and Yanming Guo and Michael S. Lew,https://www.sciencedirect.com/science/article/pii/S0031320318302334,https://doi.org/10.1016/j.patcog.2018.07.001,0031-3203,2018,51--67,84,Pattern Recognition,Learning visual and textual representations for multimodal matching and classification,article,LIU201851,
"Learning to hash is one of the most popular techniques in image retrieval, but few work investigates its robustness to noise corrupted images in which the unknown pattern of noise would heavily deteriorate the performance. To deal with this issue, we present in this paper a Bayesian denoising hashing algorithm whose output can be regarded a denoised version of the input hash code. We show that our method essentially seeks to reconstruct a new but more robust hash code by preserving the original input information while imposing extra constraints so as to correct the corrupted bits. We optimized this model in variational Bayes framework which has a closed-form update in each iteration that is more efficient than numerical optimization. Furthermore, our method can be added at the top of any original hashing layer, serving as a post-processing denoising layer with no change to previous training procedure. Experiments on three popular datasets demonstrate that the proposed method yields robust and meaningful hash code, which significantly improves the performance of state-of-the-art hash learning methods on challenging tasks such as large-scale natural image retrieval and retrieval with corrupted images.","Image retrieval, Denoising hashing, Probabilistic model, Variational Bayes",Dong Wang and Ge Song and Xiaoyang Tan,https://www.sciencedirect.com/science/article/pii/S003132031830325X,https://doi.org/10.1016/j.patcog.2018.09.006,0031-3203,2019,134--142,86,Pattern Recognition,Bayesian denoising hashing for robust image retrieval,article,WANG2019134,
"Action recognition plays a fundamental role in computer vision and has drawn growing attention recently. This paper addresses this issue conditioned on extreme Low Resolution (abbreviated as eLR). Generally, eLR video is often susceptible to noise, thus extracting a robust representation is of great challenge. Besides, due to the limitation of video resolution, eLR video cannot be cropped or resized randomly, then it is inevitably complicated to design and to train a deep network for eLR video. This paper proposes a novel network for robust video representation by employing pseudo tensor low rank regularization. A new Video Low Rank Representation model (named VLRR) is first proposed to recover the inherent robust component of a given video, and then the recovered term is introduced to a convolutional Network (denoted pLRN) as an auxiliary pseudo Low Rank guidance. Benefitting from the auxiliary guidance, pLRN can learn an approximate low rank term end-to-end. Besides, this paper presents a new initialization strategy for eLR recognition neTwork based on Tensor factorization (dubbed TenneT). TenneT is data-driven and learns the convolutional kernels totally from the video distribution while without any back-propagation. It outperforms random initialization both in speed and accuracy. Experiments on benchmark datasets demonstrate the effectiveness and superiority of the proposed method.","Pseudo low rank, Data driven, Low resolution, Action recognition",Tingzhao Yu and Lingfeng Wang and Chaoxu Guo and Huxiang Gu and Shiming Xiang and Chunhong Pan,https://www.sciencedirect.com/science/article/pii/S0031320318302735,https://doi.org/10.1016/j.patcog.2018.07.033,0031-3203,2019,50--59,85,Pattern Recognition,Pseudo low rank video representation,article,YU201950,
"Comparative radiography is a forensic identification technique traditionally involving the manual comparison of ante-mortem and post-mortem radiographs, thus being time consuming and error prone. The main objective is to propose and validate a computer-aided comparative radiography paradigm based on the 3D bone scan-2D radiograph superimposition process of any bone or cavity. The proposal follows an image registration methodology to automatically search for the ante-mortem radiograph acquisition parameters from the forensic objectâs silhouette considering occlusions. The underlying optimization problem is complex since a close initialization cannot be assumed and the image intensities are not reliable or not captured. Several experiments were performed to validate the method. First, we study its accuracy and robustness with synthetic images of clavicles, patellae and frontal sinuses. Second, we study how optimization performance and both variability and differences in the segmentation performed by human operators affect the identification using synthetic and real images of frontal sinuses.","Forensic identification, Comparative radiography, 3D-2D image registration, Computer vision, Evolutionary computation",Oscar GÃ³mez and Oscar IbÃ¡Ã±ez and Andrea Valsecchi and Oscar CordÃ³n and Tzipi Kahana,https://www.sciencedirect.com/science/article/pii/S0031320318302218,https://doi.org/10.1016/j.patcog.2018.06.011,0031-3203,2018,469--480,83,Pattern Recognition,3D-2D silhouette-based image registration for comparative radiography-based forensic identification,article,GOMEZ2018469,
"Because covariance features with the form of symmetric positive definite matrices lie on Riemannian manifold, classification on Riemannian manifold could possess high performance in many applications. Unfortunately, the applicability of classification methods developed on Riemannian manifold is limited by their huge computational complexities, particularly with the feature data on high-dimensional Riemannian manifold. To alleviate the problem of computational cost, in this paper, a simple yet efficient dimensionality reduction algorithm, bilinear isometric Riemannian embedding, is derived to construct a low-dimensional embedding from high-dimensional Riemannian manifold. To this end, we model the bilinear isometric mapping to identify a low-dimensional embedding that maximizes the preservation of Riemannian geodesic distance. A supervised classification method, embedding discriminant analysis, is then proposed based on the low-dimensional embedding. Experimental results on image and electroencephalogram reveal that the proposed algorithms can efficiently extract the distance-preserving embedding and obtain higher classification performance.","Covariance feature, Dimensionality reduction, Isometric projection, Riemannian manifold, Pattern classification",Xiaofeng Xie and Zhu Liang Yu and Zhenghui Gu and Yuanqing Li,https://www.sciencedirect.com/science/article/pii/S0031320318303546,https://doi.org/10.1016/j.patcog.2018.10.009,0031-3203,2019,94--105,87,Pattern Recognition,Classification of symmetric positive definite matrices based on bilinear isometric Riemannian embedding,article,XIE201994,
"Cross-modal hashing has drawn increasing research interests in multimedia retrieval due to the explosive growth of multimedia big data. It is such a challenging topic due to the heterogeneity gap and high storage cost. However, most of the previous methods based on conventional linear projections and relaxation scheme fail to capture the nonlinear relationship among samples and suffers from large quantization loss, which result in an unsatisfactory performance of cross-modal retrieval. To address these issues, this paper is dedicated to learning discrete nonlinear hash functions by deep learning. A novel framework of cross-modal deep neural networks is proposed to learn binary codes directly. We formulate the similarity preserving in the framework, and also bit-independent as well as binary constraints are imposed on the hash codes. Specifically, we consider intra-modality similarity preserving at each hidden layer of the networks. Inter-modality similarity preserving is formulated by the output of each individual network. By so doing, the cross correlation can be encoded into the network training (i.e. hash functions learning) by back propagation algorithm. The final objective is solved by alternative optimization in an iterative fashion. Experimental results on four datasets i.e. NUS-WIDE, MIR Flickr, Pascal VOC, and LabelMe demonstrate the effectiveness of the proposed method, which is significantly superior to state-of-the-art cross-modal hashing approaches.","Cross-modal retrieval, deep learning, discrete hashing, alternative optimization",Fangming Zhong and Zhikui Chen and Geyong Min,https://www.sciencedirect.com/science/article/pii/S0031320318301924,https://doi.org/10.1016/j.patcog.2018.05.018,0031-3203,2018,64--77,83,Pattern Recognition,Deep Discrete Cross-Modal Hashing for Cross-Media Retrieval,article,ZHONG201864,
"We present a generic unsupervised method to increase the discriminative power of image vectors obtained from a broad family of deep neural networks for object retrieval. This goal is accomplished by simultaneously selecting and weighting informative deep convolutional features using the replicator equation, commonly used to capture the essence of selection in evolutionary game theory. The proposed method includes three major steps: First, efficiently detecting features within Regions of Interest (ROIs) using a simple algorithm, as well as trivially collecting a subset of background features. Second, assigning unassigned features by optimizing a standard quadratic problem using the replicator equation. Finally, using the replicator equation again in order to partially address the issue of feature burstiness. We provide theoretical time complexity analysis to show that our method is efficient. Experimental results on several common object retrieval benchmarks using both pre-trained and fine-tuned deep networks show that our method compares favorably to the state-of-the-art. We also publish an easy-to-use Matlab implementation of the proposed method for reproducing our results.","Object retrieval, Replicator equation, Deep feature selection, Deep feature weighting",Shanmin Pang and Jihua Zhu and Jiaxing Wang and Vicente Ordonez and Jianru Xue,https://www.sciencedirect.com/science/article/pii/S0031320318301808,https://doi.org/10.1016/j.patcog.2018.05.010,0031-3203,2018,150--160,83,Pattern Recognition,Building discriminative CNN image representations for object retrieval using the replicator equation,article,PANG2018150,
"Graph similarity is an important notion with many applications. Graph edit distance is one of the most flexible graph similarity measure available. The main problem with this measure is that in practice it can only be computed for small graphs due to its exponential time complexity. The present paper is concerned with efficient solutions with high quality approximation of graph edit distance. In particular, we present a novel upper bound computation framework for graph edit distance. It is based on breadth-first hierarchical views of the graphs and a novel hierarchical traversing and matching method to build a graph mapping. The main advantage of this framework is that it combines map construction with edit counting in easy and straightforward manner. It also allows to compare the graphs from different hierarchical views to improve the bound. Furthermore, to avoid the complexity of multi-view comparisons and preserve distance accuracy, two new view-selection methods, based on the vertex and edge star structures, are introduced to scale the computations. Contrasting our approach with the state-of-the-art overestimation methods, experiments show that it delivers comparable upper bounds with over three orders of magnitude speedup on real data graphs. Experiments also show that this approach improves the classification accuracy of the KNN classifiers by over 15 percent when compared with the state-of-the-art overestimation methods.","Graph similarity, Graph edit distance, Upper bound",Karam Gouda and Mona Arafa and Toon Calders,https://www.sciencedirect.com/science/article/pii/S0031320318301092,https://doi.org/10.1016/j.patcog.2018.03.019,0031-3203,2018,210--224,80,Pattern Recognition,A novel hierarchical-based framework for upper bound computation of graph edit distance,article,GOUDA2018210,
"Convolutional neural network (CNN) has demonstrated its superior ability to achieve amazing accuracy in computer vision field. Nevertheless, for practical domain-specific image recognition tasks, it still remains difficult to obtain massive high-quality labeled datasets due to the strong requirements for extensive, tedious manual processing. Inspired by the well-known observation that human brain can accurately recognize objects without relying on massive congeneric examples, we propose a novel deep variance network (DVN) to further enhance the generalization ability of CNN in this paper, which could still produce higher recognition accuracy even with unbalanced training datasets than original CNN. The key idea of our DVN is built upon the intrinsic exploitation of inter-class homogeneity and intra-class heterogeneity. Towards such goal, we make the first attempt to incorporate a hierarchical Bayesian model into the powerful CNN framework, which can transfer the joint feature distribution from certain objectâs complete training dataset to other objectâs incomplete training dataset in an iterative way. In each training cycle, the CNN-resulted features are clustered into discrimination-related subspaces to guide the learning and adaptive adjustment of homogeneity and heterogeneity over unbalanced training datasets. In practice, we furnish several state-of-the-art deep networks with our proposed DVN, and conduct extensive experiments and comprehensive evaluations over CIFAR-10, MNIST, and SVHN benchmarks. The experiments have shown that, most of the furnished deep networks can benefit from our DVN, wherein they gain at most 6.9% accuracy improvement over CIFAR-10 benchmark, 52.83% error reduction over MNIST benchmark, and an improvement of 6.2% over SVHN datasets.","Deep variance network, Unbalanced training datasets, Convolutional neural network, Homogeneity, Heterogeneity",Shuai Li and Wenfeng Song and Hong Qin and Aimin Hao,https://www.sciencedirect.com/science/article/pii/S0031320318301250,https://doi.org/10.1016/j.patcog.2018.03.035,0031-3203,2018,294--308,81,Pattern Recognition,"Deep variance network: An iterative, improved CNN framework for unbalanced training datasets",article,LI2018294,
"In last years, most human action recognition works have used dense trajectories features, to achieve state-of-the-art results. Histograms of Oriented Gradients (HOG), Histogram of Optical Flow (HOF) and Motion Boundary Histograms (MBH) features are extracted from regions and being tracked across the frames. The goal of this paper is to improve the performance obtained by means of Improved Dense Trajectories (IDTs), adding new features based on temporal templates. We construct these templates considering a video sequence as a third-order tensor and computing three different projections. We use several functions for projecting the fibers from the video sequences, and combined them by means of sum pooling. As a first contribution of our work, we present in detail the method based on tensor projections. First, we have assessed the results obtained using only template based action recognition. Next, in order to achieve state-of-art recognition rates, we have fused our features with those of IDTs.This is the second contribution of the article. Experiments on four different public datasets have shown that this technique improves IDTs performance and that the results outperform the ones obtained by most of the state-of-the-art techniques for action recognition.","Action recognition, Subtensors, Dense trajectories, Keypoint descriptors, Temporal template",Josep Maria Carmona and Joan Climent,https://www.sciencedirect.com/science/article/pii/S0031320318301493,https://doi.org/10.1016/j.patcog.2018.04.015,0031-3203,2018,443--455,81,Pattern Recognition,Human action recognition by means of subtensor projections and dense trajectories,article,CARMONA2018443,
"Visual attributes, from simple objects (e.g., backpacks, hats) to soft-biometrics (e.g., gender, height, clothing) have proven to be a powerful representational approach for many applications such as image description and human identification. In this paper, we introduce a novel method to combine the advantages of both multi-task and curriculum learning in a visual attribute classification framework. Individual tasks are grouped after performing hierarchical clustering based on their correlation. The clusters of tasks are learned in a curriculum learning setup by transferring knowledge between clusters. The learning process within each cluster is performed in a multi-task classification setup. By leveraging the acquired knowledge, we speed-up the process and improve performance. We demonstrate the effectiveness of our method via ablation studies and a detailed analysis of the covariates, on a variety of publicly available datasets of humans standing with their full-body visible. Extensive experimentation has proven that the proposed approach boosts the performance by 4%â10%.","Curriculum learning, Multi-task classification, Visual attributes",Nikolaos Sarafianos and Theodoros Giannakopoulos and Christophoros Nikou and Ioannis A. Kakadiaris,https://www.sciencedirect.com/science/article/pii/S0031320318300840,https://doi.org/10.1016/j.patcog.2018.02.028,0031-3203,2018,94--108,80,Pattern Recognition,Curriculum learning of visual attribute clusters for multi-task classification,article,SARAFIANOS201894,
"Clustering is an important technique to deal with large scale data which are explosively created in internet. Most data are high-dimensional with a lot of noise, which brings great challenges to retrieval, classification and understanding. No current existing approach is âoptimalâ for large scale data. For example, DBSCAN requires O(n2) time, Fast-DBSCAN only works well in 2 dimensions, and Ï-Approximate DBSCAN runs in O(n) expected time which needs dimension D to be a relative small constant for the linear running time to hold. However, we prove theoretically and experimentally that Ï-Approximate DBSCAN degenerates to an O(n2) algorithm in very high dimension such that 2Dâ¯>â¯â¯>â¯n. In this paper, we propose a novel local neighborhood searching technique, and apply it to improve DBSCAN, named as NQ-DBSCAN, such that a large number of unnecessary distance computations can be effectively reduced. Theoretical analysis and experimental results show that NQ-DBSCAN averagely runs in O(n*log(n)) with the help of indexing technique, and the best case is O(n) if proper parameters are used, which makes it suitable for many realtime data.","DBSCAN, -Approximate DBSCAN, NQ-DBSCAN",Yewang Chen and Shengyu Tang and Nizar Bouguila and Cheng Wang and Jixiang Du and HaiLin Li,https://www.sciencedirect.com/science/article/pii/S0031320318302103,https://doi.org/10.1016/j.patcog.2018.05.030,0031-3203,2018,375--387,83,Pattern Recognition,A fast clustering algorithm based on pruning unnecessary distance computations in DBSCAN for high-dimensional data,article,CHEN2018375,
"Several studies were devoted to the usage of the Fuzzy C-Means (FCM) algorithm to collaborative clustering, especially in the realm of data analysis, data mining, and pattern recognition. In this study, a novel interval-valued fuzzy set-based approach to realize collaborative clustering is presented. In collaborative clustering diagram, the local clustering results acquired locally (at a specific data site) impact clustering carried out at some other data sites. Those clustering methods endowed with interval-valued fuzzy sets help cope with uncertainties present in the data and the nature of the collaborative process itself. The validity indices such as fuzzy silhouette and SSE (Sum of Squared Error) are extended to quantify results produced by collaborative fuzzy clustering. Several experimental studies are presented using which we demonstrate the advantages of the proposed algorithms.","Fuzzy C-Means (FCM), Collaborative fuzzy clustering, Interval - valued fuzzy clustering, Interval type-2 fuzzy sets, Clustering validity index",Long Thanh Ngo and Trong Hop Dang and Witold Pedrycz,https://www.sciencedirect.com/science/article/pii/S0031320318301377,https://doi.org/10.1016/j.patcog.2018.04.006,0031-3203,2018,404--416,81,Pattern Recognition,Towards interval-valued fuzzy set-based collaborative fuzzy clustering algorithms,article,NGO2018404,
"Writer identification based on handwritten fragments has been reported to give interesting performance. However, while the fragmentation process, inconsistent fragments are generated and affect badly the identification accuracy. Hence, in this paper, a clustered-based One-Class Classifier (OCC) is proposed in order to generate more robust classification model than the distance-based classifier for handwritten fragments. Besides, the problem of inconsistent fragments expands its effect to the test step. Thus, a Dynamic Fragment Weighting Combination (DFWC) rule is proposed to reduce the effect of inconsistent test fragments. Furthermore, due to the difficulty of performing a generic descriptor, three different descriptors related systems are designed and combined through an effective combination scheme based on Choquet fuzzy integral operator. Experimental results conducted on the well-known IFN/ENIT and IAM datasets show good adaptation of the OCC with DFWC. Moreover, the Choquet combination scheme offers more improvements to achieve 97.56% and 94.51% for the used datasets, respectively. The obtained results highlight the reliability of the proposed system in comparison with recent studies for writer identification issue.","Clustered one-class classifiers, Dynamic fragment weighting combination, Choquet fuzzy integral, Writer identification, Text fragments",Bilal Hadjadji and Youcef Chibani,https://www.sciencedirect.com/science/article/pii/S0031320318301651,https://doi.org/10.1016/j.patcog.2018.05.001,0031-3203,2018,147--162,82,Pattern Recognition,Two combination stages of clustered One-Class Classifiers for writer identification from text fragments,article,HADJADJI2018147,
"Accurate ellipse detection in image streams at real-time execution is an open challenge. We present a novel fast and robust ellipse detection method. The method adopts arcs selection, smart grouping, and repeated utilization of gradient information to significantly reduce the computations otherwise needed without compromising the detection effectiveness. Geometric properties calculable with few computations, such as arc smoothness, relative placement of curves, and region of confidence for ellipse centres, are utilized for this purpose. An exhaustive sensitivity analysis of the method's control parameters has been performed. It reveals range of values that support consistent performance over diverse challenging datasets with complex background, multiple differently sized ellipses, and occluded, overlapping ellipses. The method's performance is compared with six state-of-the-art detectors over four diverse datasets. Among all the tested methods, the proposed method demonstrates the best balance between detection effectiveness (the best or the second best F-measure scores) and computation time (>40â¯Hz) across all the datasets.","Ellipse detection, Geometric approach, Gradient analysis, Centre estimation, Arc classification",Huixu Dong and Dilip K. Prasad and I-Ming Chen,https://www.sciencedirect.com/science/article/pii/S0031320318301134,https://doi.org/10.1016/j.patcog.2018.03.023,0031-3203,2018,112--130,81,Pattern Recognition,Accurate detection of ellipses with false detection control at video rates using a gradient analysis,article,DONG2018112,
"The convex clustering formulation of Chi and Lange (2015) is revisited. While this formulation can be precisely and efficiently solved, it uses the standard Euclidean metric to measure the distance between the data points and their corresponding cluster centers and hence its performance deteriorates significantly in the presence of outlier features. To address this issue, this paper considers a formulation that combines convex clustering with metric learning. It is shown that: (1) for any given positive definite Mahalanobis distance metric, the problem of convex clustering can be precisely and efficiently solved using the Alternating Direction Method of Multipliers; (2) the problem of learning a positive definite Mahalanobis distance metric admits a closed-form solution; (3) an algorithm that alternates between convex clustering and metric learning can provide a significant performance boost over not only the original convex clustering formulation but also the recently proposed robust convex clustering formulation of Wang etÂ al. (2017).","Convex clustering, Alternating Direction Method of Multipliers, Metric learning, Unsupervised learning",Xiaopeng Lucia Sui and Li Xu and Xiaoning Qian and Tie Liu,https://www.sciencedirect.com/science/article/pii/S0031320318301535,https://doi.org/10.1016/j.patcog.2018.04.019,0031-3203,2018,575--584,81,Pattern Recognition,Convex clustering with metric learning,article,SUI2018575,
"Fuzzy classifiers have been studied in the area of fuzzy sets for a long time resulting in a number of architectures. In this study, we thoroughly investigate and critically assess fuzzy rule-based classifiers. A topology of the classifier is discussed along with a discussion of the role of fuzzy set technology in the construction of condition and conclusion parts of the classification rules. Some optimization mechanisms utilized in the adjustment of information granules forming the rules are presented. Performance of the fuzzy classifiers is quantified in terms of their accuracy and an area under curve (AUC) determined for the receiver operating characteristics (ROC). The performance of the classifier is evaluated vis-Ã -vis a collection of triangular norms used in the construction of the fuzzy classifiers. Experimental studies involve synthetic and publicly available data. Furthermore, comparative studies include the experiments with the commonly used non-fuzzy classifiers.","Fuzzy classifiers, Performance analysis, Logic processing, Receiver operating characteristics (), Triangular norms, Fuzzy clustering, Particle swarm optimizer (PSO)",Xingchen Hu and Witold Pedrycz and Xianmin Wang,https://www.sciencedirect.com/science/article/pii/S0031320318301018,https://doi.org/10.1016/j.patcog.2018.03.011,0031-3203,2018,156--167,80,Pattern Recognition,Fuzzy classifiers with information granules in feature space and logic-based computing,article,HU2018156,
"Traditional image clustering methods take a two-step approach, feature learning and clustering, sequentially. However, recent research results demonstrated that combining the separated phases in a unified framework and training them jointly can achieve a better performance. In this paper, we first introduce fully convolutional auto-encoders for image feature learning and then propose a unified clustering framework to learn image representations and cluster centers jointly based on a fully convolutional auto-encoder and soft k-means scores. At initial stages of the learning procedure, the representations extracted from the auto-encoder may not be very discriminative for latter clustering. We address this issue by adopting a boosted discriminative distribution, where high score assignments are highlighted and low score ones are de-emphasized. With the gradually boosted discrimination, clustering assignment scores are discriminated and cluster purities are enlarged. Experiments on several vision benchmark datasets show that our methods can achieve a state-of-the-art performance.","Image clustering, Fully convolutional auto-encoder, Representation learning, Discriminatively boosted clustering",Fengfu Li and Hong Qiao and Bo Zhang,https://www.sciencedirect.com/science/article/pii/S0031320318301936,https://doi.org/10.1016/j.patcog.2018.05.019,0031-3203,2018,161--173,83,Pattern Recognition,Discriminatively boosted image clustering with fully convolutional auto-encoders,article,LI2018161,
"We present VIGO, a novel online Bayesian classifier for both binary and multiclass problems. In our model, variational inference for multivariate distribution technique is exploited to approximate the class conditional probability density functions of data in an online manner. To handle concept drift that could arise in streaming data, we develop 2 new adaptive methods based on VIGO, which we called VIGOw and VIGOd. While VIGOw naturally adapts to any kind of changing environments, VIGOd maximises the benefit of a static environment as long as it does not detect any change. Extensive experiments on big/medium real-world/synthetic datasets demonstrate the superior performance of our algorithms over many state-of-the-art methods in the literature.","Online learning, Variational inference, Bayesian classifier, Data stream, Concept drift",Thi Thu Thuy Nguyen and Tien Thanh Nguyen and Alan Wee-Chung Liew and Shi-Lin Wang,https://www.sciencedirect.com/science/article/pii/S0031320318301419,https://doi.org/10.1016/j.patcog.2018.04.007,0031-3203,2018,280--293,81,Pattern Recognition,Variational inference based bayes online classifiers with concept drift adaptation,article,NGUYEN2018280,
"A recent theoretical analysis shows the equivalence between non-negative matrix factorization (NMF) and spectral clustering based approach to subspace clustering. As NMF and many of its variants are essentially linear, we introduce a nonlinear NMF with explicit orthogonality and derive general kernel-based orthogonal multiplicative update rules to solve the subspace clustering problem. In nonlinear orthogonal NMF framework, we propose two subspace clustering algorithms, named kernel-based non-negative subspace clustering KNSC-Ncut and KNSC-Rcut and establish their connection with spectral normalized cut and ratio cut clustering. We further extend the nonlinear orthogonal NMF framework and introduce a graph regularization to obtain a factorization that respects a local geometric structure of the data after the nonlinear mapping. The proposed NMF-based approach to subspace clustering takes into account the nonlinear nature of the manifold, as well as its intrinsic local geometry, which considerably improves the clustering performance when compared to the several recently proposed state-of-the-art methods.","Subspace clustering, Non-negative matrix factorization, Orthogonality, Kernels, Graph regularization",Dijana ToliÄ and Nino Antulov-Fantulin and Ivica Kopriva,https://www.sciencedirect.com/science/article/pii/S003132031830164X,https://doi.org/10.1016/j.patcog.2018.04.029,0031-3203,2018,40--55,82,Pattern Recognition,A nonlinear orthogonal non-negative matrix factorization approach to subspace clustering,article,TOLIC201840,
"Clusters may exist in different subspaces of a multidimensional dataset. Traditional full-space clustering algorithms have difficulty in identifying these clusters. Various subspace clustering algorithms have used different subspace search strategies. They require clustering to assess whether cluster(s) exist in a subspace. In addition, all of them perform clustering by measuring similarity between points in the given feature space. As a result, the subspace selection and clustering processes are tightly coupled. In this paper, we propose a new subspace clustering framework named CSSub (Clustering by Shared Subspaces). It enables neighbouring core points to be clustered based on the number of subspaces they share. It explicitly splits candidate subspace selection and clustering into two separate processes, enabling different types of cluster definitions to be employed easily. Through extensive experiments on synthetic and real-world datasets, we demonstrate that CSSub discovers non-redundant subspace clusters with arbitrary shapes in noisy data; and it significantly outperforms existing state-of-the-art subspace clustering algorithms.","Subspace clustering, Shared subspaces, Density-based clustering",Ye Zhu and Kai Ming Ting and Mark J. Carman,https://www.sciencedirect.com/science/article/pii/S0031320318302073,https://doi.org/10.1016/j.patcog.2018.05.027,0031-3203,2018,230--244,83,Pattern Recognition,Grouping points by shared subspaces for effective subspace clustering,article,ZHU2018230,
"Inspired by the class-selectivity of the neurons in the inferior temporal (IT) area of the human visual cortex, we propose a novel discriminative feature learning method to improve the object recognition performance of convolutional neural network (CNN) without increasing the network complexity. Specifically, we apply the proposed entropyâorthogonality loss (EOL) to the penultimate layer of the CNN models in the training phase. The EOL explicitly enables the feature vectors learned by a CNN model have the following properties: (1) each dimension of the feature vectors only responds strongly to as few classes as possible, and (2) the feature vectors from different classes are as orthogonal as possible. When combined with the softmax loss, the EOL not only can enlarge the differences in the between-class feature vectors, but also can reduce the variations in the within-class feature vectors. Therefore, the discriminative ability of the learned feature vectors is highly improved. The EOL is general and independent of the CNN structure. Comprehensive experimental comparisons with both the image classification and face verification task on several benchmark datasets demonstrate that utilizing the proposed EOL during training can remarkably improve performance of CNN models compared to the corresponding baseline models trained without utilizing the EOL.","Convolutional neural network (CNN), Discriminative feature learning, Entropy, Orthogonality, Object recognition",Weiwei Shi and Yihong Gong and De Cheng and Xiaoyu Tao and Nanning Zheng,https://www.sciencedirect.com/science/article/pii/S0031320318301262,https://doi.org/10.1016/j.patcog.2018.03.036,0031-3203,2018,71--80,81,Pattern Recognition,Entropy and orthogonality based deep discriminative feature learning for object recognition,article,SHI201871,
"This paper presents a robust regression approach for image binarization under significant background variations and observation noise. The work is motivated by the need of identifying foreground regions in noisy microscopic images or degraded document images, where significant background variations and observation noise make image binarization challenging. The proposed method first estimates the background of an input image, subtracts the estimated background from the input image, and performs a global thresholding operation to the subtracted outcome thus achieving the binary image of the foreground. A robust regression approach is proposed to estimate the background intensity surface with minimal effects of the foreground intensities and observation noise, and a global threshold selector is proposed on the basis of a model selection criterion in a sparse regression. The proposed approach is validated using 26 test images and the corresponding ground truths, and the outcomes are compared with those of nine existing image binarization methods. The approach is also combined with three morphological segmentation methods to show how the proposed approach can improve their image segmentation outcomes.","Image binarization, Background subtraction, Robust regression, Document image analysis, Microscopy image analysis",Garret D. Vo and Chiwoo Park,https://www.sciencedirect.com/science/article/pii/S0031320318301389,https://doi.org/10.1016/j.patcog.2018.04.005,0031-3203,2018,224--239,81,Pattern Recognition,Robust regression for image binarization under heavy noise and nonuniform background,article,VO2018224,
"Novelty detection scheme in bearing vibration signals of rotating system is investigated in this article. One class support vector machine (OC-SVM) is used for novelty detection. It focuses on the required preprocessing steps including denoising, feature extraction, vectorization, normalization and dimensionality reduction, and a systematic method is proposed for each of them. A new scheme is used for denoising which presents the best combination of mother wavelet and thresholding rule for each signal. The required features are extracted from time and time-frequency domains, and the best mother wavelet for extracting features from each signal, is presented by means of energy-to-Shannon entropy ratio criterion. It is shown that the load factor is the most important factor to impose nonlinearity in vibration signals of bearings compared to fault type and fault intensity factors. Besides, for the first time, this paper demonstrates that by increasing the nonlinearity in the signals, the statistical traditional or combinations of statistical traditional and nonlinear features fail to classify data completely and only the nonlinear features have this capability. The proposed systematic preprocessing improves the efficiency of OC-SVM novelty detection upto 100% in some cases, when applied to three different data sets. Also, it yields a satisfying result compared to other similar works, in the field of classification.","Novelty detection, OC-SVM, Nonlinear feature, Wavelet, Bearing vibration signal, Entropy",Mohammad Saleh Sadooghi and Siamak {Esmaeilzadeh Khadem},https://www.sciencedirect.com/science/article/pii/S0031320318301663,https://doi.org/10.1016/j.patcog.2018.05.002,0031-3203,2018,14--33,83,Pattern Recognition,Improving one class support vector machine novelty detection scheme using nonlinear features,article,SADOOGHI201814,
"This work introduces a novel visual fiducial tag appropriate for applications of automatic identification. The proposed tag is based in Order Type, a construction defined in Computational Geometry, which is invariant to 3D translation, rotation, and projective transformations. Three main contributions are presented: first we describe the design of the proposed tags, the procedures for detecting them from an image, and the algorithms for computing an identifier from them. Second, we analyze the feasibility of the proposal in three different conditions of tagâs rotation, distance to the tag, and the effect of noise in point positions for the recognition process. Third, we show the applicability of the proposed tags with simulated images. The conducted experiments indicate that the tags are very robust to the image generation process, suitable for automatic identification up to 3472 different tags, and also for the pose estimation in Computer Vision applications.","Order Type, Computational geometry, Visual fiducial tag, Point pattern matching, Automatic identification, Computer vision",Heriberto Cruz-HernÃ¡ndez and Luis Gerardo {de la Fraga},https://www.sciencedirect.com/science/article/pii/S0031320318301146,https://doi.org/10.1016/j.patcog.2018.03.024,0031-3203,2018,213--223,81,Pattern Recognition,"A fiducial tag invariant to rotation, translation, and perspective transformations",article,CRUZHERNANDEZ2018213,
"Word embedding models are able to accurately model the semantic content of words. The process of extracting a set of word embedding vectors from a text document is similar to the feature extraction step of the Bag-of-Features (BoF) model, which is usually used in computer vision tasks. This gives rise to the proposed Bag-of-Embedded Words (BoEW) model that can efficiently represent text documents overcoming the limitations of previously predominantly used techniques, such as the textual Bag-of-Words model. The proposed method extends the regular BoF model by a) incorporating a weighting mask that allows for altering the importance of each learned codeword and b) by optimizing the model end-to-end (from the word embeddings to the weighting mask). Furthermore, the BoEW model also provides a fast way to fine-tune the learned representation towards the information need of the user using relevance feedback techniques. Finally, a novel spherical entropy objective function is proposed to optimize the learned representation for retrieval using the cosine similarity metric.","Word embeddings, Bag-of-words, Bag-of-features, Dictionary learning, Relevance feedback, Information retrieval",Nikolaos Passalis and Anastasios Tefas,https://www.sciencedirect.com/science/article/pii/S0031320318301420,https://doi.org/10.1016/j.patcog.2018.04.008,0031-3203,2018,254--267,81,Pattern Recognition,Learning bag-of-embedded-words representations for textual information retrieval,article,PASSALIS2018254,
"In this paper, we present a novel strategy for evolving prototype based clusters that uses a weighting scheme to âprogressively forgetâ old samples. The rate of forgetfulness can be controlled by a single intuitive memory parameter. This weighting scheme can be used to create efficient dynamic summaries, such as mean or covariance, of data streams. Using this weighting scheme we have developed evolving versions of the K-means and Gaussian Mixture models algorithms. They can analyze the incoming data in an online manner and they are specially geared towards dealing with concept drift originated by changes in the underlying data distribution. The algorithms were validated over a simulated database where a wide variety of concept drift situations occur and over real data related to property sales, showing their capability to follow changes in data.","Evolving clustering, Data stream, Concept drift, Gaussian mixture models, K-means, Cluster evolution",David G. MÃ¡rquez and Abraham Otero and Paulo FÃ©lix and Constantino A. GarcÃ­a,https://www.sciencedirect.com/science/article/pii/S0031320318301547,https://doi.org/10.1016/j.patcog.2018.04.020,0031-3203,2018,16--30,82,Pattern Recognition,A novel and simple strategy for evolving prototype based clustering,article,MARQUEZ201816,
"Maximum relevance and minimum redundancy (mRMR) has been well recognised as one of the best feature selection methods. This paper proposes a Kernel Partial Least Square (KPLS) based mRMR method, aiming for easy computation and improving classification accuracy for high-dimensional data. Experiments with this approach have been conducted on seven real-life datasets of varied dimensionality and number of instances, with performance measured on four different classifiers: Naive Bayes, Linear Discriminant Analysis, Random Forest and Support Vector Machine. Experimental results have exhibited the advantage of the proposed method over several competing feature selection techniques.","Feature selection, Kernel partial least square, Regression coefficients, Relevance, Classification",Upasana Talukdar and Shyamanta M Hazarika and John Q. Gan,https://www.sciencedirect.com/science/article/pii/S0031320318301821,https://doi.org/10.1016/j.patcog.2018.05.012,0031-3203,2018,91--106,83,Pattern Recognition,A Kernel Partial least square based feature selection method,article,TALUKDAR201891,
"The shape features of images are essential to image recognition, comparison and retrieval since most users are more interested in recognizing or comparing images by shape than by color and texture [1]. Comparing or retrieving images by shape is still envisioned as one of the most challenging works in image comparison and retrieval because of the lack of effective and efficient representations of shape features in image comparison and retrieval. In this paper, we propose a scalable and comparable shape representation, namely âReversed Sketchâ, is proposed, on which a shape feature extraction and utilization framework is built. With this representation, we represent an image object using a polygon extracted from the contour of the image object by a force-driving sliding box algorithm. A polygon evolution algorithm is then proposed for transforming the first wiggly polygon into a more sketchy form for efficiently processing, which makes our shape representation more scalable. Also, we present a comparable metric drawn from this representation combined with the comparing algorithm, which is invariant to scaling, rotation and translation and thereby is suitable for image recognition, registration and comparison. The proposed shape representation is especially suitable for image retrieval because with it a hierarchical index which is very useful for image retrieval can be built on the image dataset. Extensive experiments are carried out and the experiment results show that with our shape representation the shape features can be quickly extracted from an image, simplified as needed, and used to efficiently comparing shapes in accord with peopleâs perception. Experiment results derived with practical datasets indicate that our framework can achieve better comparing precision and efficiency, compared with some other methods.","Shape representation, Image matching, Contour detection, Polygon evolution, Content-based image retrieval",Ming Huang and JiaJun Lin and Ning Chen and Wei An and WeiJian Zhu,https://www.sciencedirect.com/science/article/pii/S0031320318300852,https://doi.org/10.1016/j.patcog.2018.03.001,0031-3203,2018,168--182,80,Pattern Recognition,Reversed Sketch: A scalable and comparable shape representation,article,HUANG2018168,
"Many high-dimensional data in computer vision essentially lie in multiple low-dimensional subspaces. Recently developed subspace clustering methods have shown good effectiveness in recovering the underlying low-dimensional subspace structure of high-dimensional data. The state-of-the-art methods show that sparseness and grouping effect of the affinity matrix are important for subspace clustering. The Structured Sparse Subspace Clustering (SSSC) model is a unified optimization framework for learning both the self-representation of the data and their subspace segmentation. But the SSSC only considers structured sparseness property of the affinity matrix. In this work, we define a concept of grouping-effect-within-cluster (GEWC) to group data from the same subspace together. Based on GEWC, we design a new regularization term coupling the self-representation matrix and the segmentation matrix. The new regularization term interactively enforces both to have the expected properties: the segmentation matrix enforces the self-representation coefficient vectors to have large cosine similarity, or GEWC, whenever the data points are drawn from the same subspace and they have the same cluster labels. On the other hand, the self-representation matrix enforces data to have the same cluster labels whenever their self-representation coefficient vectors have large cosine similarity. Incorporating the new penalty into the SSSC model, we present a new unified minimization framework for affinity learning and subspace clustering. The new model considers not only structured sparseness but also GEWC. Experimental results on several commonly used datasets demonstrate that our method outperforms other state-of-the-art methods in revealing the subspace structure of high-dimensional data.","Subspace clustering, Grouping-effect-within-clusters, Affinity matrix learning",Huazhu Chen and Weiwei Wang and Xiangchu Feng,https://www.sciencedirect.com/science/article/pii/S0031320318301948,https://doi.org/10.1016/j.patcog.2018.05.020,0031-3203,2018,107--118,83,Pattern Recognition,Structured Sparse Subspace Clustering with Within-Cluster Grouping,article,CHEN2018107,
"Many computer-aided systems have been developed for Human epithelial type 2 (HEp-2) cell classification recently, but there is still a big performance gap between them and specialist doctors. Inspired by the recent successes of convolutional neural network, we proposed a deep cross residual network (DCRNet) for HEp-2 cell classification. A cross connection based residual block was proposed to increase the information flow among different network layers. We used two benchmark datasets to evaluate our system. The state-of-art results, i.e. the average class accuracy of 80.8% in the International Conference on Pattern Recognition (ICPR) 2012 dataset and the mean class accuracy of 85.1% in the Indirect Immunofluorescence Image (I3A) dataset, were achieved. Our result on the ICPR 2012 dataset is so far the best among all works reported in the literature. Our algorithm was winner of the most recent ICPR 2016 contest and the accuracy beat all of the top performers in the previous International Conference on Image Processing (ICIP) 2013 and the ICPR 2014 contests.","Convolutional neural network, Cross connection, Deep cross residual network, HEp-2 classification",Linlin Shen and Xi Jia and Yuexiang Li,https://www.sciencedirect.com/science/article/pii/S0031320318301705,https://doi.org/10.1016/j.patcog.2018.05.005,0031-3203,2018,68--78,82,Pattern Recognition,Deep cross residual network for HEp-2 cell staining pattern classification,article,SHEN201868,
"In this paper, we present a novel clustering scheme based on binary embeddings, which provides compact and informative binary representations of high-dimensional objects. The binary representations are obtained with a collection of one-class classifiers learned from (pseudo) randomly selected points in the dataset. To cluster the binary representations, we consider two approaches: a mixture of Bernoulli distributions and a recent biclustering approach called CRAFT. The empirical evaluation in comparison with both classic and recent clustering methods, based on 12 different datasets, provides encouraging results. The main feature of the proposed method is that it is agnostic to the shape of the clusters.","Clustering, Binary embedding, Finite mixture models, Biclustering, One-class classification",Manuele Bicego and MÃ¡rio A.T. Figueiredo,https://www.sciencedirect.com/science/article/pii/S003132031830181X,https://doi.org/10.1016/j.patcog.2018.05.011,0031-3203,2018,52--63,83,Pattern Recognition,Clustering via binary embedding,article,BICEGO201852,
"The aim of this paper is to design an algorithm based on nonsmooth optimization techniques to solve the minimum sum-of-squares clustering problems in very large data sets. First, the clustering problem is formulated as a nonsmooth optimization problem. Then the limited memory bundle method [Haarala et al., 2007] is modified and combined with an incremental approach to design a new clustering algorithm. The algorithm is evaluated using real world data sets with both the large number of attributes and the large number of data points. It is also compared with some other optimization based clustering algorithms. The numerical results demonstrate the efficiency of the proposed algorithm for clustering in very large data sets.","Cluster analysis, Nonsmooth optimization, Nonconvex optimization, Bundle methods, Limited memory methods",Napsu Karmitsa and Adil M. Bagirov and Sona Taheri,https://www.sciencedirect.com/science/article/pii/S0031320318302085,https://doi.org/10.1016/j.patcog.2018.05.028,0031-3203,2018,245--259,83,Pattern Recognition,Clustering in large data sets with the limited memory bundle method,article,KARMITSA2018245,
"Recent region proposal generation methods show a low Intersection-of-Union with the ground-truth boxes. Because they simply regress the coordinates of the bounding boxes by exploiting the single-layer output of convolutional neural networks. This paper proposes a hierarchical objectness network for region proposal generation and object detection to address the inaccurate localization problem. Instead of regressing the coordinates, we subtly localize the objects by predicting the stripe objectness, i.e., a group of probabilities reflecting the existence of the object in each location of the candidate proposal. Additionally, we construct the hierarchical features by reversely connecting multiple convolutional layers to detect objects with large-scale variations. Our experimental results demonstrate that our method performs better than the state-of-the-art region proposal generation methods in terms of recall. Moreover, by integrating with advanced object detection frameworks, our method achieves superior object detection results.","Object detection, Object localization, Region proposal generation, Convolutional neural network",Juan Wang and Xiaoming Tao and Mai Xu and Yiping Duan and Jianhua Lu,https://www.sciencedirect.com/science/article/pii/S0031320318301791,https://doi.org/10.1016/j.patcog.2018.05.009,0031-3203,2018,260--272,83,Pattern Recognition,Hierarchical objectness network for region proposal generation and object detection,article,WANG2018260,
"A computer-aided detection (CAD) tool for locating and detecting polyps can help reduce the chance of missing polyps during colonoscopy. Nevertheless, state-of-the-art algorithms were either computationally complex or suffered from low sensitivity and therefore unsuitable to be used in real clinical setting. In this paper, a novel regression-based Convolutional Neural Network (CNN) pipeline is presented for polyp detection during colonoscopy. The proposed pipeline was constructed in two parts: 1) to learn the spatial features of colorectal polyps, a fast object detection algorithm named ResYOLO was pre-trained with a large non-medical image database and further fine-tuned with colonoscopic images extracted from videos; and 2) temporal information was incorporated via a tracker named Efficient Convolution Operators (ECO) for refining the detection results given by ResYOLO. Evaluated on 17,574 frames extracted from 18 endoscopic videos of the AsuMayoDB, the proposed method was able to detect frames with polyps with a precision of 88.6%, recall of 71.6% and processing speed of 6.5 frames per second, i.e. the method can accurately locate polyps in more frames and at a faster speed compared to existing methods. In conclusion, the proposed method has great potential to be used to assist endoscopists in tracking polyps during colonoscopy.","Smart cancer screening, Therapeutic endoscopy, Endoscopic Informatics, Body Sensor Network, Deep Learning, Health Informatics",Ruikai Zhang and Yali Zheng and Carmen C.Y. Poon and Dinggang Shen and James Y.W. Lau,https://www.sciencedirect.com/science/article/pii/S0031320318302000,https://doi.org/10.1016/j.patcog.2018.05.026,0031-3203,2018,209--219,83,Pattern Recognition,Polyp detection during colonoscopy using a regression-based convolutional neural network with a tracker,article,ZHANG2018209,
"In the last decades historical handwritten documents have become increasingly available in digital form. Yet, the accessibility to these documents with respect to browsing and searching remained limited as full automatic transcription is often not possible or not sufficiently accurate. This paper proposes a novel reliable approach for template-based keyword spotting in historical handwritten documents. In particular, our framework makes use of different graph representations for segmented word images and a sophisticated matching procedure. Moreover, we extend our method to a spotting ensemble. In an exhaustive experimental evaluation on four widely used benchmark datasets we show that the proposed approach is able to keep up or even outperform several state-of-the-art methods for template- and learning-based keyword spotting.","Handwritten keyword spotting, Graph representation, Bipartite graph matching, Ensemble methods",Michael Stauffer and Andreas Fischer and Kaspar Riesen,https://www.sciencedirect.com/science/article/pii/S0031320318301274,https://doi.org/10.1016/j.patcog.2018.04.001,0031-3203,2018,240--253,81,Pattern Recognition,Keyword spotting in historical handwritten documents based on graph matching,article,STAUFFER2018240,
"Relief-like algorithms have been widely used as feature selection to reduce the dimension of high-dimensional data which involves thousands of irrelevant variables because of their low computational cost and high accuracy. Classical Relief algorithms have not exactly shown the dynamic procedure that updates weight iteratively. This paper proposes an innovative feature weight estimation method, called dynamic representation and neighbor sparse reconstruction-based Relief (DRNSR-Relief). Similar to the classical Relief algorithms, the goal of DRNSR-Relief is to maximize the expected margin in the weighted feature space. A dynamic representation framework is introduced to show the dynamic relationship between the expected margin vector and the weight vector. To achieve better neighbor reconstruction, DRNSR-Relief decomposes a nonlinear problem into a set of locally linear ones through local hyperplane with l1 regularization and then estimates feature weights in a large margin framework. With the help of gradient ascent method, we can guarantee the convergence of DRNSR-Relief. To demonstrate the validity and the effectiveness of our formulation for feature selection in supervised learning, we perform extensive experiments on synthetic and real-world datasets. Experimental results indicate that DRNSR-Relief is very promising.","Feature weighting, Feature selection, Relief, Sparse learning, Local hyperplane,  regularization, Classification",Xiaojuan Huang and Li Zhang and Bangjun Wang and Zhao Zhang and Fanzhang Li,https://www.sciencedirect.com/science/article/pii/S0031320318301031,https://doi.org/10.1016/j.patcog.2018.03.014,0031-3203,2018,388--403,81,Pattern Recognition,Feature weight estimation based on dynamic representation and neighbor sparse reconstruction,article,HUANG2018388,
"In this paper, we propose a new gait representationâgait dynamics graph (GDG) for individual identification. For each gait sequence, lower limbs joint angles are extracted as gait parameters, and gait system dynamics underlying time-varying gait parameter trajectories is captured by using deterministic learning algorithm. Gait dynamics graph (GDG) is then generated by plotting the extracted dynamics information into three-dimensional graphic. Unlike other gait representations, which are not embedded with dynamics information, GDG demonstrates nonlinear gait dynamics in a new, visually intuitive manner using three-dimensional graphic representation. Both direct matching method and nonlinear dynamics analysis method can be used for GDG recognition independently. The performance of the proposed representation is evaluated and compared with the other representations experimentally on five large benchmark gait databases. This kind of gait representation is embedded with more distinctive information and preserves temporal dynamics information of human walking, which does not rely on shape or silhouettes information. Experimental results show that the GDG representation can further improve recognition rates and avoid the great drop of recognition rate when the training and test sets are under different walking conditions.","Gait representation, Gait recognition, Gait dynamics graph, Biometrics",Muqing Deng and Cong Wang and Tongjia Zheng,https://www.sciencedirect.com/science/article/pii/S0031320318302127,https://doi.org/10.1016/j.patcog.2018.06.002,0031-3203,2018,287--298,83,Pattern Recognition,Individual identification using a gait dynamics graph,article,DENG2018287,
"Performing a Random Projection from the feature space associated to a kernel function may be important for two main reasons. (1) As a consequence of the JohnsonâLindestrauss lemma, the resulting low-dimensional representation will preserve most of the structure of data in the kernel feature space and (2) an efficient linear classifier trained on transformed data might approximate the accuracy of its nonlinear counterparts. In this paper, we present a novel method to perform Random Projections from the feature space of homogeneous polynomial kernels. As opposed to other kernelized Random Projection proposals, our method focuses on a specific kernel family to preserve some of the beneficial properties of the original Random Projection algorithm (e.g. data independence and efficiency). Our extensive experimental results evidence that the proposed method efficiently approximates a Random Projection from the kernel feature space, preserving pairwise distances and enabling a boost on linear classification accuracies.","Random Projection, Homogeneous polynomial kernel, Nonlinear dimensionality reduction",Daniel LÃ³pez-SÃ¡nchez and AngÃ©lica GonzÃ¡lez Arrieta and Juan M. Corchado,https://www.sciencedirect.com/science/article/pii/S0031320318301675,https://doi.org/10.1016/j.patcog.2018.05.003,0031-3203,2018,130--146,82,Pattern Recognition,Data-independent Random Projections from the feature-space of the homogeneous polynomial kernel,article,LOPEZSANCHEZ2018130,
"The variety of event categories and event boundary information have resulted in limited success for acoustic event detection systems. To deal with this, we propose to utilize the long contextual information, low-dimensional discriminant global bottleneck features and category-specific bottleneck features. By concatenating several adjacent frames together, the use of contextual information makes it easier to cope with acoustic signals with long duration. Global and category-specific bottleneck features can extract the prior knowledge of the event category and boundary, which is ideally matched by the task of an event detection system. Evaluations on the UPC-TALP and ITC-IRST databases of highly variable acoustic events demonstrate the effectiveness of the proposed approaches by achieving a 5.30% and 4.44% absolute error rate improvement respectively compared to the state of art technique.","Acoustic event detection, Contextual information, Global bottleneck features, Category-specific bottleneck features",Xianjun Xia and Roberto Togneri and Ferdous Sohel and David Huang,https://www.sciencedirect.com/science/article/pii/S0031320318301158,https://doi.org/10.1016/j.patcog.2018.03.025,0031-3203,2018,1--13,81,Pattern Recognition,Random forest classification based acoustic event detection utilizing contextual-information and bottleneck features,article,XIA20181,
"Feature selection is the process of selecting subset of features that still provide maximum amount of the information that otherwise is provided by the entire set of conditional attributes. Many approaches have been proposed so far in literature for this purpose. Recently the rough set based approaches have become dominant. Majority of these approaches use attribute dependency to find significance of attributes. Problem with this measure is that it uses positive region to calculate dependency which is a computationally expensive job. As a consequence, it degrades the performance of the feature selection algorithms using this measure. In this paper, we have proposed a new heuristic based dependency calculation technique by avoiding the positive region. The proposed method uses a heuristics approach by finding the consistent records regarding each decision class in the dataset. Using this method, allows us to calculate dependency by avoiding the positive region, which ultimately enhances the computational efficiency of the underlying feature selection algorithm thus enabling it to be used for dataset beyond smaller size. In order to calculate dependency by using the proposed method, we have used a two-dimensional grid as intermediate data structure. Number of feature selection algorithms were used with proposed solution on various publically available datasets to justify it. A comparison framework was used to compare the proposed solution with conventional methods. Results have justified the proposed solution both in terms of its efficiency and effectiveness.","Rough set theory, Heuristic dependency calculation, Reducts, Dependency, Positive region",Muhammad Summair Raza and Usman Qamar,https://www.sciencedirect.com/science/article/pii/S0031320318301432,https://doi.org/10.1016/j.patcog.2018.04.009,0031-3203,2018,309--325,81,Pattern Recognition,A heuristic based dependency calculation technique for rough set theory,article,RAZA2018309,
"We propose a method to select the subset of features in multiple linear regression models that considers the collinearity between features. The proposed method first detects collinear groups of features and then uses collinear groupwise feature selection constraints to estimate the coefficients of the regression model. The constraints simultaneously control the number of features selected and predefined collinear feature groups. We manage the multicollinearity in the regression model by controlling the parameters of the fusion group constraint. To address the NP-hard problem of the proposed method, we propose a modified discrete first-order algorithm. We use simulation and real-world data to demonstrate the usefulness of the proposed method by comparing it to existing regularization and discrete optimization-based methods in terms of predictive accuracy, bias, and variance. The comparison confirms that the proposed method outperforms the alternatives.","Multiple linear regression, Machine learning, Feature selection, Multicollinearity, Mixed-integer quadratic programming, Best subset selection",Younghoon Kim and Seoung Bum Kim,https://www.sciencedirect.com/science/article/pii/S0031320318301833,https://doi.org/10.1016/j.patcog.2018.05.013,0031-3203,2018,1--13,83,Pattern Recognition,Collinear groupwise feature selection via discrete fusion group regression,article,KIM20181,
"Haze removal (or dehazing) is very important for many applications in computer vision. Because depth information and atmospheric light are usually unknown in practice, haze removal is a challenging problem, especially for single image dehazing. In this paper, we propose a new variational model for removing haze from a single input image. The proposed model combines Koschmiederâs law with Retinex assumption that an image is the product of illumination and reflection. We assume that scene depth and surface radiance are spatially piecewise smooth, total variation is thus used for regularization in our model. The proposed model is defined as a constrained optimization problem, which is solved by an alternating minimization scheme and a fast gradient projection algorithm. Theoretical analyses are given for the proposed model and algorithm. Some numerical examples are presented, which have shown that our model has the best visual effect and the highest average PSNR (Peak Signal-to-Noise Ratio) compared to six relevant models in the literature.","Dehazing, Total variation, Variational method, Gradient projection algorithm",Wei Wang and Chuanjiang He and Xiang-Gen Xia,https://www.sciencedirect.com/science/article/pii/S0031320318300864,https://doi.org/10.1016/j.patcog.2018.03.009,0031-3203,2018,196--209,80,Pattern Recognition,A constrained total variation model for single image dehazing,article,WANG2018196,
"High-dimensional data with very few instances are typical in many application domains. Selecting a highly discriminative subset of the original features is often the main interest of the end user. The widely-used feature selection protocol for such type of data consists of two steps. First, features are selected from the data (possibly through cross-validation), and, second, a cross-validation protocol is applied to test a classifier using the selected features. The selected feature set and the testing accuracy are then returned to the user. For the lack of a better option, the same low-sample-size dataset is used in both steps. Questioning the validity of this protocol, we carried out an experiment using 24 high-dimensional datasets, three feature selection methods and five classifier models. We found that the accuracy returned by the above protocol is heavily biased, and therefore propose an alternative protocol which avoids the contamination by including both steps in a single cross-validation loop. Statistical tests verify that the classification accuracy returned by the proper protocol is significantly closer to the true accuracy (estimated from an independent testing set) compared to that returned by the currently favoured protocol.","Feature selection, Wide datasets, Experimental protocol, Training/testing, Cross-validation",Ludmila I. Kuncheva and Juan J. RodrÃ­guez,https://www.sciencedirect.com/science/article/pii/S003132031830102X,https://doi.org/10.1016/j.patcog.2018.03.012,0031-3203,2018,660--673,81,Pattern Recognition,On feature selection protocols for very low-sample-size data,article,KUNCHEVA2018660,
"Learning the distance metric between pairs of examples is of great importance for visual recognition, especially for person re-identification (Re-Id). Recently, the contrastive and triplet loss are proposed to enhance the discriminative power of the deeply learned features, and have achieved remarkable success. As can be seen, either the contrastive or triplet loss is just one special case of the Euclidean distance relationships among these training samples. Therefore, we propose a structured graph Laplacian embedding algorithm, which can formulate all these structured distance relationships into the graph Laplacian form. The proposed method can take full advantages of the structured distance relationships among these training samples, with the constructed complete graph. Besides, this formulation makes our method easy-to-implement and super-effective. When embedding the proposed algorithm with the softmax loss for the CNN training, our method can obtain much more robust and discriminative deep features with inter-personal dispersion and intra-personal compactness, which is essential to person Re-Id. We did experiments on top of three popular networks, namely AlexNet [1], DGDNet [2] and ResNet50 [3], on recent four widely used Re-Id benchmark datasets, and it shows that the proposed structure graph Laplacian embedding is very effective.","Person re-identification, Structured, Graph Laplacian, Deep learning",De Cheng and Yihong Gong and Xiaojun Chang and Weiwei Shi and Alexander Hauptmann and Nanning Zheng,https://www.sciencedirect.com/science/article/pii/S0031320318301717,https://doi.org/10.1016/j.patcog.2018.05.007,0031-3203,2018,94--104,82,Pattern Recognition,Deep feature learning via structured graph Laplacian embedding for person re-identification,article,CHENG201894,
"Despite significant progress in machine learning, pedestrian detection in the real-world is still regarded as one of the challenging problems, limited by occluded appearances, cluttered backgrounds, and bad visibility at night. This has caused detection approaches using multi-spectral sensors such as color and thermal which could be complementary to each other. In this paper, we propose a novel sensor fusion framework for detecting pedestrians even in challenging real-world environments. We design a convolutional neural network (CNN) architecture that consists of three-branch detection models taking different modalities as inputs. Unlike existing methods, we consider all detection probabilities from each modality in a unified CNN framework and selectively use them through a channel weighting fusion (CWF) layer to maximize the detection performance. An accumulated probability fusion (APF) layer is also introduced to combine probabilities from different modalities at the proposal-level. We formulate these sub-networks into a unified network, so that it is possible to train the whole network in an end-to-end manner. Our extensive evaluation demonstrates that the proposed method outperforms the state-of-the-art methods on the challenging KAIST, CVC-14, and DIML multi-spectral pedestrian datasets.","Multi-spectral sensor fusion, Pedestrian detection, Channel weighting fusion, Probabilistic fusion",Kihong Park and Seungryong Kim and Kwanghoon Sohn,https://www.sciencedirect.com/science/article/pii/S0031320318300906,https://doi.org/10.1016/j.patcog.2018.03.007,0031-3203,2018,143--155,80,Pattern Recognition,Unified multi-spectral pedestrian detection based on probabilistic fusion networks,article,PARK2018143,
"Monocular depth estimation is very challenging in complex compositions depicting multiple objects of diverse scales. Albeit the recent great progress thanks to the deep convolutional neural networks, the state-of-the-art monocular depth estimation methods still fall short to handle such real-world challenging scenarios. In this paper, we propose a deep end-to-end learning framework to tackle these challenges, which learns the direct mapping from a color image to the corresponding depth map. First, we represent monocular depth estimation as a multi-category dense labeling task by contrast to the regression-based formulation. In this way, we could build upon the recent progress in dense labeling such as semantic segmentation. Second, we fuse different side-outputs from our front-end dilated convolutional neural network in a hierarchical way to exploit the multi-scale depth cues for monocular depth estimation, which is critical in achieving scale-aware depth estimation. Third, we propose to utilize soft-weighted-sum inference instead of the hard-max inference, transforming the discretized depth scores to continuous depth values. Thus, we reduce the influence of quantization error and improve the robustness of our method. Extensive experiments have been conducted on the Make3D, NYU v2, and KITTI datasets and superior performance have been achieved on NYU v2 and KITTI datasets compared with current state-of-the-art methods, which shows the superiority of our method. Furthermore, experiments on the NYU v2 dataset reveal that our classification based model is able to learn the probability distribution of depth.","Monocular depth estimation, Deep convolutional neural network, Soft-weighted-sum-inference, Dilated convolution",Bo Li and Yuchao Dai and Mingyi He,https://www.sciencedirect.com/science/article/pii/S0031320318302097,https://doi.org/10.1016/j.patcog.2018.05.029,0031-3203,2018,328--339,83,Pattern Recognition,Monocular depth estimation with hierarchical fusion of dilated CNNs and soft-weighted-sum inference,article,LI2018328,
"When a comparison between time series is required, measurement functions provide meaningful scores to characterize similarity between sequences. Quite often, time series appear warped in time, i.e, although they may exhibit amplitude and shape similarity, they appear dephased in time. The most common algorithm to overcome this challenge is the Dynamic Time Warping, which aligns each sequence prior establishing distance measurements. However, Dynamic Time Warping takes only into account amplitude similarity. A distance which characterizes the degree of time warping between two sequences can deliver new insights for applications where the timing factor is essential, such well-defined movements during sports or rehabilitation exercises. We propose a novel measurement called Time Alignment Measurement, which delivers similarity information on the temporal domain. We demonstrate the potential of our approach in measuring performance of time series alignment methodologies and in the characterization of synthetic and real time series data acquired during human movement.","Time series, Time warping, Similarity, Distance, Signal alignment",Duarte Folgado and MarÃ­lia Barandas and Ricardo Matias and Rodrigo Martins and Miguel Carvalho and Hugo Gamboa,https://www.sciencedirect.com/science/article/pii/S0031320318301286,https://doi.org/10.1016/j.patcog.2018.04.003,0031-3203,2018,268--279,81,Pattern Recognition,Time Alignment Measurement for Time Series,article,FOLGADO2018268,
"Complex activity recognition is challenging due to the inherent uncertainty and diversity of performing a complex activity. Normally, each instance of a complex activity has its own configuration of atomic actions and their temporal dependencies. In our previous work, we proposed an atomic action-based Bayesian model that constructs Allenâs interval relation networks to characterize complex activities in a probabilistic generative way: By introducing latent variables from the Chinese restaurant process, our approach is able to capture all possible styles of a particular complex activity as a unique set of distributions over atomic actions and relations. However, a major limitation of our previous models is their fixed network structures, which may lead to an overtrained or undertrained model owing to unnecessary or missing links in a network. In this work, we present an improved model that network structures can be automatically learned from empirical data, allowing itself to characterize complex activities with structural varieties. In addition, a new dataset of complex hand activities has been constructed and made publicly available, which is much larger in size than any existing datasets. Empirical evaluations on benchmark datasets as well as our in-house dataset demonstrate the competitiveness of our approach.","Complex activity recognition, Structure learning, Bayesian network, Interval, Probabilistic generative model, American Sign Language dataset",Li Liu and Shu Wang and Bin Hu and Qingyu Qiong and Junhao Wen and David S. Rosenblum,https://www.sciencedirect.com/science/article/pii/S0031320318301560,https://doi.org/10.1016/j.patcog.2018.04.022,0031-3203,2018,545--561,81,Pattern Recognition,Learning structures of interval-based Bayesian networks in probabilistic generative model for human complex activity recognition,article,LIU2018545,
"Canonical correlation analysis(CCA) and its variants have been playing an important role in dimension reduction and information fusion. Most of the existing CCA-related algorithms can only handle fully-paired scenarios, where all the samples from different views require to be rigorously paired. However, majority of the samples only provide partial pairwise correspondences in most cases. In such semi-paired scenarios, traditional methods often perform poorly due to overfitting. Furthermore, most CCA-related algorithms only focus on the correlation between different views but ignore preserving the within-view similarity which is helpful to make samples in the same class mapped more compactly onto canonical correlation space. In addition, since CCA is unable to exploit discriminative information effectively, improved performance is limited in many tasks. To solve these challenging problems, we propose a novel method named joint Intermodal and Intramodal Semi-paired Canonical Correlation Analysis (I2SCCA), which properly handle the semi-paired scenarios. At the same time, the method preserves the within-view similarity information by making two neighbor samples in the same view closer in the projected space. Different from traditional similarity matrix construction, we incorporate a rank constraint of Laplacian matrix into the graph-based clustering algorithms. With fewer parameters, a graph that has exactly c (the number of clusters) connected components is learned without requiring any post graph-cut processing. Based on the obtained graph, we estimate the cross-view correlation through similarity measure of the single view. Specially, taking advantage of clustering assumption, discriminative information is derived although there is no label available. Finally, we extend I2SCCA to multi-view and non-linear cases. Extensive experiments on several datasets demonstrate the effectiveness of I2SCCA.","Semi-paired learning, Canonical correlation analysis, Clustering",Xin Guo and Song Wang and Yun Tie and Lin Qi and Ling Guan,https://www.sciencedirect.com/science/article/pii/S0031320318301043,https://doi.org/10.1016/j.patcog.2018.03.013,0031-3203,2018,36--49,81,Pattern Recognition,Joint intermodal and intramodal correlation preservation for semi-paired learning,article,GUO201836,
"Contactless 3D fingerprint identification has gained significant attentions in recent years as it can offer more hygienic, accurate and ubiquitous personal identification. Despite such advantages, contactless 3D imaging often results in partial 3D fingerprints as it requires relatively higher cooperation from users during the contactless 3D imaging. Such contactless 3D fingerprint images significantly degrade matching accuracy due to partial 3D fingerprint imaging. This paper proposes an end-to-end contactless 3D fingerprint representation learning model based on convolutional neural network (CNN). The proposed model includes one fully convolutional network for fingerprint segmentation and three Siamese networks to learn multi-view 3D fingerprint feature representation. Contactless partial 3D fingerprint identification is a more challenging problem due to its high degree of freedom during contactless 3D fingerprint acquisition and is also addressed by using proposed model. We therefore investigate multi-view 3D fingerprint recognition and partial 3D fingerprint using proposed approach. Comparative experimental results, presented in this paper using state-of-the-art 3D fingerprint recognition method, demonstrate the effectiveness of the proposed multi-view approach and illustrate a significant improvement of state-of-the-art 3D fingerprint recognition methods.","Contactless 3D fingerprint recognition, Partial 3D fingerprint identification, Multi-view CNN",Chenhao Lin and Ajay Kumar,https://www.sciencedirect.com/science/article/pii/S0031320318301687,https://doi.org/10.1016/j.patcog.2018.05.004,0031-3203,2018,314--327,83,Pattern Recognition,Contactless and partial 3D fingerprint recognition using multi-view deep representation,article,LIN2018314,
"Vote-boosting is a sequential ensemble learning method in which the individual classifiers are built on different weighted versions of the training data. To build a new classifier, the weight of each training instance is determined in terms of the degree of disagreement among the current ensemble predictions for that instance. For low class-label noise levels, especially when simple base learners are used, emphasis should be made on instances for which the disagreement rate is high. When more flexible classifiers are used and as the noise level increases, the emphasis on these uncertain instances should be reduced. In fact, at sufficiently high levels of class-label noise, the focus should be on instances on which the ensemble classifiers agree. The optimal type of emphasis can be automatically determined using cross-validation. An extensive empirical analysis using the beta distribution as emphasis function illustrates that vote-boosting is an effective method to generate ensembles that are both accurate and robust.","Ensemble learning, Boosting, Uncertainty-based emphasis, Robust classification",Maryam Sabzevari and Gonzalo MartÃ­nez-MuÃ±oz and Alberto SuÃ¡rez,https://www.sciencedirect.com/science/article/pii/S0031320318301961,https://doi.org/10.1016/j.patcog.2018.05.022,0031-3203,2018,119--133,83,Pattern Recognition,Vote-boosting ensembles,article,SABZEVARI2018119,
"This paper explores the problem of circle fitting for incomplete (partial arc) laser scanning point cloud data in the presence of outliers. In mobile laser scanning, data are commonly incomplete because of the orientation of the scanning unit to the surveying objects and the limited street-based positions. Also, multiple structures in the built environment often produce clustered outliers. To address these problems, this paper combines robust Principal Component Analysis (PCA) and robust regression with an efficient algebraic circle fitting method to develop two algorithms for circle fitting. Experimental efforts show that the proposed algorithms are statistically robust and can tolerate a high-percentage (exceeding 44%) of clustered outliers with insignificant error levels, while still achieving better shape recognition compared to existing competitive methods. For example, for a simulation of 1000 quarter circle datasets including 20% clustered outliers, RANSAC estimated the circle radius with a Mean Squared Error (MSE) of 172.10, whereas the proposed algorithms fit circles with an MSE of less than 0.42. The algorithms have potential in many areas including building information modeling, particle tracking, product quality control, arboreal assessment, and road asset monitoring.","3D modeling, Feature extraction, Object detection, Point cloud processing, Remote sensing, Robust statistics, Surface fitting",Abdul Nurunnabi and Yukio Sadahiro and Debra F. Laefer,https://www.sciencedirect.com/science/article/pii/S0031320318301444,https://doi.org/10.1016/j.patcog.2018.04.010,0031-3203,2018,417--431,81,Pattern Recognition,Robust statistical approaches for circle fitting in laser scanning three-dimensional point cloud data,article,NURUNNABI2018417,
"In this paper, we present a simple yet effective Boolean map based representation that exploits connectivity cues for visual tracking. We describe a target object with histogram of oriented gradients and raw color features, of which each one is characterized by a set of Boolean maps generated by uniformly thresholding their values. The Boolean maps effectively encode multi-scale connectivity cues of the target with different granularities. The fine-grained Boolean maps capture spatially structural details that are effective for precise target localization while the coarse-grained ones encode global shape information that are robust to large target appearance variations. Finally, all the Boolean maps form together a robust representation that can be approximated by an explicit feature map of the intersection kernel, which is fed into a logistic regression classifier with online update, and the target location is estimated within a particle filter framework. The proposed representation scheme is computationally efficient and facilitates achieving favorable performance in terms of accuracy and robustness against the state-of-the-art tracking methods on the OTB50 and VOT2016 benchmark datasets.","Visual tracking, Boolean map, Logistic regression, Intersection kernel",Kaihua Zhang and Qingshan Liu and Jian Yang and Ming-Hsuan Yang,https://www.sciencedirect.com/science/article/pii/S0031320318301201,https://doi.org/10.1016/j.patcog.2018.03.029,0031-3203,2018,147--160,81,Pattern Recognition,Visual tracking via Boolean map representations,article,ZHANG2018147,
"In this paper, we aim to provide a survey on the applications of deep learning for cancer detection and diagnosis and hope to provide an overview of the progress in this field. In the survey, we firstly provide an overview on deep learning and the popular architectures used for cancer detection and diagnosis. Especially we present four popular deep learning architectures, including convolutional neural networks, fully convolutional networks, auto-encoders, and deep belief networks in the survey. Secondly, we provide a survey on the studies exploiting deep learning for cancer detection and diagnosis. The surveys in this part are organized based on the types of cancers. Thirdly, we provide a summary and comments on the recent work on the applications of deep learning to cancer detection and diagnosis and propose some future research directions.",,Zilong Hu and Jinshan Tang and Ziming Wang and Kai Zhang and Ling Zhang and Qingling Sun,https://www.sciencedirect.com/science/article/pii/S0031320318301845,https://doi.org/10.1016/j.patcog.2018.05.014,0031-3203,2018,134--149,83,Pattern Recognition,Deep learning for image-based cancer detection and diagnosisâ¯ââ¯A survey,article,HU2018134,
"In large-scale visual recognition tasks, researchers are usually faced with some challenging problems, such as the extreme imbalance in the number of training data between classes or the lack of annotated data for some classes. In this paper, we propose a novel neural network architecture that automatically synthesizes pseudo feature representations for the classes in lack of annotated images. With the supply of semantic attributes for classes, the proposed Attribute-Based Synthetic Network (ABS-Net) can be applied to zero-shot learning (ZSL) scenario and conventional supervised learning (CSL) scenario as well. For ZSL tasks, the pseudo feature representations can be viewed as annotated feature-level instances for novel concepts, which facilitates the construction of unseen class predictor. For CSL tasks, the pseudo feature representations can be viewed as products of data augmentation on training set, which enriches the interpretation capacity of CSL systems. We demonstrate the effectiveness of the proposed ABS-Net in ZSL and CSL settings on a synthetic colored MNIST dataset (C-MNIST). For several popular ZSL benchmark datasets, our architecture also shows competitive results on zero-shot recognition task, especially leading to tremendous improvement to state-of-the-art mAP on zero-shot retrieval task.","Pseudo feature representation, Zero-shot learning, Supervised learning, Data augmentation, Attribute learning",Jiang Lu and Jin Li and Ziang Yan and Fenghua Mei and Changshui Zhang,https://www.sciencedirect.com/science/article/pii/S0031320318300876,https://doi.org/10.1016/j.patcog.2018.03.006,0031-3203,2018,129--142,80,Pattern Recognition,Attribute-Based Synthetic Network (ABS-Net): Learning more from pseudo feature representations,article,LU2018129,
"Markov networks and Bayesian networks are two popular models for classification. VapnikâChervonenkis dimension and Euclidean dimension are two measures of complexity of a class of functions, which can be used to measure classification capability of classifiers. One can use VapnikâChervonenkis dimension of the class of functions associated with a classifier to construct an estimate of its generalization error. In this paper, we study VapnikâChervonenkis dimension and Euclidean dimension of concept classes induced by discrete Markov networks and Bayesian networks. We show that these two dimensional values of the concept class induced by a discrete Markov network are identical, and the value equals dimension of the toric ideal corresponding to this Markov network as long as the toric ideal is nontrivial. Based on this result, one can compute the dimensional value in terms of a computer algebra system directly. Furthermore, for a general Bayesian network, we show that dimension of the corresponding toric ideal offers an upper bound of Euclidean dimension. In addition, we illustrate how to use VapnikâChervonenkis dimension to estimate generalization error in binary classification.","Bayesian networks, Classification, Markov networks, Toric ideal, VapnikâChervonenkis dimension",Benchong Li and Youlong Yang,https://www.sciencedirect.com/science/article/pii/S0031320318301602,https://doi.org/10.1016/j.patcog.2018.04.026,0031-3203,2018,31--37,82,Pattern Recognition,Complexity of concept classes induced by discrete Markov networks and Bayesian networks,article,LI201831,
"Recently, sparse representation-based (SR) methods have been presented for the fusion of multi-focus images. However, most of them independently consider the local information from each image patch during sparse coding and fusion, giving rise to the spatial artifacts on the fused image. In order to overcome this issue, we present a novel multi-focus image fusion method by jointly considering information from each local image patch as well as its spatial contextual information during the sparse coding and fusion in this paper. Specifically, we employ a robust sparse representation (LR_RSR, for short) model with a Laplacian regularization term on the sparse error matrix in the sparse coding phase, ensuring the local consistency among the spatially-adjacent image patches. In the subsequent fusion process, we define a focus measure to determine the focused and de-focused regions in the multi-focus images by collaboratively employing information from each local image patch as well as those from its 8-connected spatial neighbors. As a result of that, the proposed method is likely to introduce fewer spatial artifacts to the fused image. Moreover, an over-complete dictionary with small atoms that maintains good representation capability, rather than using the input data themselves, is constructed for the LR_RSR model during sparse coding. By doing that, the computational complexity of the proposed fusion method is greatly reduced, while the fusion performance is not degraded and can be even slightly improved. Experimental results demonstrate the validity of the proposed method, and more importantly, it turns out that our LR-RSR algorithm is more computationally efficient than most of the traditional SR-based fusion methods.","Multi-focus image fusion, Robust sparse representation, Dictionary construction, Spatial contextual information, Spatial consistency",Qiang Zhang and Tao Shi and Fan Wang and Rick S. Blum and Jungong Han,https://www.sciencedirect.com/science/article/pii/S0031320318302139,https://doi.org/10.1016/j.patcog.2018.06.003,0031-3203,2018,299--313,83,Pattern Recognition,Robust sparse representation based multi-focus image fusion with dictionary construction and local spatial consistency,article,ZHANG2018299,
"As a hot topic in machine learning, supervised learning is applied to both classification and recognition frequently. However, parameter-tuning in most supervised methods is a laborious work due to its complexity and unpredictability. In this paper, we propose an auto-weighted approach, termed as auto-weighted 2-dimensional maximum margin criterion, which updates the introduced weight in each iteration automatically to leverage the associated terms, so that the weight becomes insensitive to initialization. In addition, the proposed method extracts features from 2-order data directly, i.e., image data. Moreover, we have an observation that the objective value in the proposed method could directly reflect the performance in classification task under the varying dimensionality, which is much beneficial to selection of the optimal dimensionality. Extensive experiments on several datasets are conducted to validate that our method is of great superiority compared to other approaches.","Supervised learning, Auto-weighted parameter, 2-dimensional criterion, Dimensionality selection, Classification",Han Zhang and Feiping Nie and Rui Zhang and Xuelong Li,https://www.sciencedirect.com/science/article/pii/S003132031830195X,https://doi.org/10.1016/j.patcog.2018.05.021,0031-3203,2018,220--229,83,Pattern Recognition,Auto-weighted 2-dimensional maximum margin criterion,article,ZHANG2018220,
"Monocular 3D face reconstruction from a single image has been an active research topic due to its wide applications. It has been demonstrated that the 3D face can be reconstructed efficiently using a PCA-based subspace model for facial shape representation and facial landmarks for model parameter estimation. However, due to the limited expressiveness of the subspace model and the inaccuracy of landmark detection, most existing methods are not robust to pose and illumination variation. To overcome this limitation, this work proposes a coupled-dictionary model for parametric facial shape representation and a two-stage framework for 3D face reconstruction from a single 2D image by using facial landmarks. Motivated by image super-resolution, the proposed coupled-model consists of two dictionaries for the sparse and the dense 3D facial shapes, respectively. In the first stage, the sparse 3D face is estimated from facial landmarks by using partial least-squares regression. In the second stage, the dense 3D face is reconstructed by 3D super-resolution on the estimated sparse 3D face. Comprehensive experimental evaluations demonstrate that the proposed coupled-dictionary model outperforms the PCA-based subspace model in 3D face modeling accuracy and that the proposed framework achieves much lower reconstruction error on facial images with pose and illumination variations compared to state-of-the-art algorithms. Moreover, qualitative analysis demonstrates that the proposed method is generalizable to different types of data, including facial images, portraits, and facial sketches.","Dictionary learning, Sparse coding, 3D super-resolution, 3D face modeling, 3D face reconstruction",Pengfei Dou and Yuhang Wu and Shishir K. Shah and Ioannis A. Kakadiaris,https://www.sciencedirect.com/science/article/pii/S0031320318300918,https://doi.org/10.1016/j.patcog.2018.03.002,0031-3203,2018,515--527,81,Pattern Recognition,Monocular 3D facial shape reconstruction from a single 2D image with coupled-dictionary learning and sparse coding,article,DOU2018515,
"The active contour model is a widely used method for image segmentation. Most existing active contour models yield poor performance when applied to images with severe intensity inhomogeneity. To address this issue, we propose an adaptive-scale active contour model (ASACM) based on image entropy and semi-naive Bayesian classifier, which achieves simultaneous segmentation and bias field estimation for images with severe intensity inhomogeneity. Firstly, an adaptive scale operator is constructed to adaptively adjust the scale of the ASACM according to the degree of the intensity inhomogeneity. Secondly, we define an improved bias field estimation term via distributing a dependent-membership function for each pixel to estimate the bias field in severe inhomogeneous images. Thirdly, a new penalty term is proposed using piecewise polynomial, which helps to avoid time-consuming re-initialization process and instability in conventional penalty term. The experimental results demonstrate that the proposed ASACM consistently outperforms many state-of-the-art models in segmentation accuracy, segmentation efficiency and robustness w.r.t initialization and noise.","Active contour model, Image segmentation, Intensity inhomogeneous image, Adaptive scale operator, Bias field estimation",Qing Cai and Huiying Liu and Sanping Zhou and Jingfeng Sun and Jing Li,https://www.sciencedirect.com/science/article/pii/S0031320318301729,https://doi.org/10.1016/j.patcog.2018.05.008,0031-3203,2018,79--93,82,Pattern Recognition,An adaptive-scale active contour model for inhomogeneous image segmentation and bias field estimation,article,CAI201879,
"Constructing a good graph that can capture intrinsic data structures is critical for graph-based semi-supervised learning methods, which are widely applied for hyperspectral image (HSI) classification with small amount of labeled samples. Among the existing graph construction methods, sparse representation (SR)-based methods have shown impressive performance on semi-supervised HSI classification tasks. However, most SR-based algorithms fail to consider the rich spatial information of HSI, which has been shown beneficial for classification tasks. In this paper, we propose a spatial and class structure regularized sparse representation (SCSSR) graph for semi-supervised HSI classification. Specifically, spatial information has been incorporated into SR model via the graph Laplacian regularization, it assumes that the spatial neighbors should have similar representation coefficients, the obtained coefficient matrix thus can reflect the similarity between samples more accurately. Besides, we also incorporate probabilistic class structure, which implies the probabilistic relationship between each sample and each class, into SR model to further improve discriminability of graph. The experimental results on Hyperion and AVIRIS hyperspectral data show that our method outperforms state of the art methods.","Spatial regularization, Probabilistic class structure, Sparse representation (SR), Semi-supervised learning (SSL), Hyperspectral image (HSI) classification",Yuanjie Shao and Nong Sang and Changxin Gao and Li Ma,https://www.sciencedirect.com/science/article/pii/S0031320318301171,https://doi.org/10.1016/j.patcog.2018.03.027,0031-3203,2018,81--94,81,Pattern Recognition,Spatial and class structure regularized sparse representation graph for semi-supervised hyperspectral image classification,article,SHAO201881,
"Low rank subspace and multi-task learning have been introduced into object tracking to pursuit the accurate representation. However, many existing methods regularize all rank components equally, and shrink with the same threshold. In addition, these methods ignore the discriminative and structured information among tasks during the tracking. In this paper, we propose an online discriminative multi-task tracker with structured and weighted low rank regularization (ODMT-SL). Specifically, the total tracking task is accomplished by the combination of multiple subtasks, and each subtask corresponds to the trace of the image patch from the tracked object. In order to improve the flexibility of multi-task tracker, the weighted nuclear norm is introduced to adaptively assign different tracking importance on different rank components of multiple tasks. The geometric structure relationship among and inside candidates (or training samples) are mined to learn the collaborate representation, according to the discriminative subspace and optimal classifier. They are simultaneously learned and updated by minimizing the developed tracking model. The best candidate is selected by jointly evaluating the normalized metric. The proposed tracker is empirically compared with the state-of-the-art trackers on a large set of public video sequences. Both quantitative and qualitative comparisons demonstrate that the proposed algorithm performs well in terms of effectiveness, accuracy and robustness.","Robust multi-subtask learning, Structured and weighted low rank, Group-sparsity regularization, Normalized collaboration metric",Baojie Fan and Xiaomao Li and Yang Cong and Yandong Tang,https://www.sciencedirect.com/science/article/pii/S0031320318301298,https://doi.org/10.1016/j.patcog.2018.04.002,0031-3203,2018,528--544,81,Pattern Recognition,Structured and weighted multi-task low rank tracker,article,FAN2018528,
"Distance metric learning has motivated a great deal of research over the last years due to its robustness for many pattern recognition problems. In this paper, we develop a supervised distance metric learning method that aims to improve the performance of nearest-neighbor classification. Our method is inspired by the large-margin principle, resulting in an objective function based on a sum of margin violations to be minimized. Due to the use of the ramp loss function, the corresponding objective function is nonconvex, making it more challenging. To overcome this limitation, we formulate our distance metric learning problem as an instance of difference of convex functions (DC) programming. This allows us to design a more robust method than when using standard optimization techniques. The effectiveness of this method is empirically demonstrated through extensive experiments on several standard benchmark data sets.","Distance metric learning, Nearest neighbor, Linear transformation, DC programming",Bac Nguyen and Bernard {De Baets},https://www.sciencedirect.com/science/article/pii/S0031320318301572,https://doi.org/10.1016/j.patcog.2018.04.024,0031-3203,2018,562--574,81,Pattern Recognition,An approach to supervised distance metric learning based on difference of convex functions programming,article,NGUYEN2018562,
"Hashing has become a popular tool on histopathology image analysis due to the significant gain in both computation and storage. However, most of current hashing techniques learn features and binary codes individually from whole images, or emphasize the inter-class difference but neglect the relevance order within the same classes. To alleviate these issues, in this paper, we propose a novel pairwise based deep ranking hashing framework. We first define a pairwise matrix to preserve intra-class relevance and inter-class difference. Then we propose an objective function that utilizes two identical continuous matrices generated by the hyperbolic tangent (tanh) function to approximate the pairwise matrix. Finally, we incorporate the objective function into a deep learning architecture to learn features and binary codes simultaneously. The proposed framework is validated on 5356 skeletal muscle and 2176 lung cancer images with four types of diseases, and it can achieve 97.49% classification accuracy, 97.49% mean average precision (MAP) with 100 returned images, and 0.51 NDCG score with 50 retrieved neighbors on 2032 query images.","Histopathology images, Classification, Retrieval, Ranking hashing, Deep learning",Xiaoshuang Shi and Manish Sapkota and Fuyong Xing and Fujun Liu and Lei Cui and Lin Yang,https://www.sciencedirect.com/science/article/pii/S0031320318301055,https://doi.org/10.1016/j.patcog.2018.03.015,0031-3203,2018,14--22,81,Pattern Recognition,Pairwise based deep ranking hashing for histopathology image classification and retrieval,article,SHI201814,
"Cost-sensitive learning can be found in many real-world applications and represents an important learning paradigm in machine learning. The recently proposed cost-sensitive hinge loss support vector machine (CSHL-SVM) guarantees consistency with the cost-sensitive Bayes risk, and this technique provides better generalization accuracy compared to traditional cost-sensitive support vector machines. In practice, data typically appear in the form of sequential chunks, also called an on-line scenario. However, conventional batch learning algorithms waste a considerable amount of time under the on-line scenario due to re-training of a model from scratch. To make CSHL-SVM more practical for the on-line scenario, we propose a chunk incremental learning algorithm for CSHL-SVM, which can update a trained model without re-training from scratch when incorporating a chunk of new samples. Our method is efficient because it can update the trained model for not only one sample at a time but also multiple samples at a time. Our experimental results on a variety of datasets not only confirm the effectiveness of CSHL-SVM but also show that our method is more efficient than the batch algorithm of CSHL-SVM and the incremental learning method of CSHL-SVM only for a single sample.","Cost-sensitive learning, Chunk incremental learning, Hinge loss, Support vector machines",Bin Gu and Xin Quan and Yunhua Gu and Victor S. Sheng and Guansheng Zheng,https://www.sciencedirect.com/science/article/pii/S0031320318301973,https://doi.org/10.1016/j.patcog.2018.05.023,0031-3203,2018,196--208,83,Pattern Recognition,Chunk incremental learning for cost-sensitive hinge loss support vector machine,article,GU2018196,
"The widespread availability of hand-held devices like tablets, phablets and smart phones, along with their new handwriting digitizing and their increased computing powers, enable these to process the graphomotor dimension and the lognormal trends of human handwriting. By exploiting such capacity, it becomes possible to extend these mobile devices into Personal Digital Bodyguards (PDBs). PDBs will be able to supplement people's sensitive data protection with signature verification, equipment use security with writer authentication and handwritten CAPTCHAs processing (e-security), and to enhance human-machine interaction performances through words spotting and handwriting recognition (e-recognition). For young children, these tools will turn into interactive toys helping them to learn and master their fine motor control and become better writers. For advanced students they will enable sophisticated systems for (e-learning) and (e-testing). Moreover, PDBs will also be able to provide the user with fine motor control monitoring, which can detect stress, aging and health problems (e-health). This paper presents a prospective survey of various projects dealing with these five e-fields of investigation, focussing on state of the art results and providing directions in research and development, under the theoretical umbrella of the Kinematic Theory of human movements and its Lognormality Principle. From a practical point of view, the concept of lognormality provides a fundamental common thread, an integrative psychophysical standpoint to track the graphomotor problems of signature verification, writer identification, handwriting generation, recognition and learning.","Signature verification, Handwriting learning and recognition, Neuromuscular disorder, Lognormality principle, Mobile devices, E-security, E-recognition, E-interaction, E-health, E-learning, E-testing tools, Personal digital body guard",RÃ©jean Plamondon and Giuseppe Pirlo and Ãric Anquetil and CÃ©line RÃ©mi and Hans-Leo Teulings and Masaki Nakagawa,https://www.sciencedirect.com/science/article/pii/S0031320318301456,https://doi.org/10.1016/j.patcog.2018.04.012,0031-3203,2018,633--659,81,Pattern Recognition,"Personal digital bodyguards for e-security, e-learning and e-health: A prospective survey",article,PLAMONDON2018633,
"Line matching across views is a fundamental task in many applications. Existing methods are hardly applicable to across view scenarios due to the limitation of line descriptors and matching strategy. In this paper, we present a novel line-points invariant based on a new projective invariant named characteristic number. The invariant is able to reflect the intrinsic geometry of line and points, which keeps invariant across views. The construction of this invariant uses intersections of coplanar lines instead of endpoints, rendering more robust matching across views. Accurate homography recovered from the invariant allows all lines, even those without interest points around them, a chance to be matched. Extensive comparisons with the state-of-the-art validate the matching accuracy and robustness of the proposed method to projective transformations. The method also performs well for image pairs with similar textures and those of low textures.","Line matching, Characteristic number, Line-points projective invariant",Qi Jia and Xin Fan and Xinkai Gao and Meiyu Yu and Haojie Li and Zhongxuan Luo,https://www.sciencedirect.com/science/article/pii/S0031320318301213,https://doi.org/10.1016/j.patcog.2018.03.031,0031-3203,2018,471--483,81,Pattern Recognition,Line matching based on line-points invariant and local homography,article,JIA2018471,
"In the past decade, network community discovery has attracted great attention from quite a few researchers, and community structure is one of the most significant properties in complex networks. This paper presents a novel method for network community discovery based on deep sparse filtering. The features of the network are extracted by sparse filtering, an unsupervised deep learning algorithm, from an efficient representation of the network. Consequently, extracted features are employed to partition the network. Experiment results on both synthetic and real-world network datasets indicate that the proposed algorithm especially based on SârensenâDiceâs similarity matrix representation of the network is efficient and it outperforms several state-of-art algorithms in discovering community structure.","Community discovery, Sparse filtering, Deep learning, Network representation",Yu Xie and Maoguo Gong and Shanfeng Wang and Bin Yu,https://www.sciencedirect.com/science/article/pii/S003132031830116X,https://doi.org/10.1016/j.patcog.2018.03.026,0031-3203,2018,50--59,81,Pattern Recognition,Community discovery in networks with deep sparse filtering,article,XIE201850,
"In unsupervised domain adaptation, a key research problem is joint distribution alignment across the source and target domains. However, direct alignment of the source and target joint distributions is infeasible, because the target conditional distribution cannot be known without target labels. Instead of estimating target labels for target conditional distribution approximation, this paper proposes a new criterion of domain-shared group sparsity that is an equivalent condition for conditional distribution alignment. Together with marginal distribution alignment, we develop a domain-shared group-sparse dictionary learning model to learn domain-shared representations with aligned joint distributions. A cross-domain label propagation method is then proposed to train a classifier for the target domain using the domain-shared group-sparse representations and the target-specific information from the target data. The proposed method outperforms eight state-of-the-art unsupervised domain adaptation algorithms for cross-domain face recognition and cross-dataset object recognition with hand-drafted and deep features. Experimental results across multiple sub-domains show that the proposed method also performs well across datasets with large variance. Our results are quantitatively and qualitatively analyzed, and experiments of parameter sensitivity and convergence analysis are performed to show the effectiveness of the proposed method.","Domain adaptation, Dictionary learning",Baoyao Yang and Andy J. Ma and Pong C. Yuen,https://www.sciencedirect.com/science/article/pii/S0031320318301614,https://doi.org/10.1016/j.patcog.2018.04.027,0031-3203,2018,615--632,81,Pattern Recognition,Learning domain-shared group-sparse representation for unsupervised domain adaptation,article,YANG2018615,
"Long-tail distribution is widespread in many practical applications, where most categories contain only a small number of samples. As sufficient instances cannot be obtained for describing the intra-class diversity of the minority classes, the separating hyperplanes learned by traditional machine learning methods are usually heavily skewed. Resampling techniques and cost-sensitive algorithms have been introduced to enhance the statistical power of the minority classes, but they cannot infer more reliable class boundaries beyond the description of samples in the training set. To address this issue, we cluster the original categories into super-class to produce a relatively balanced distribution in the super-class space. Moreover, the knowledge shared among categories belonging to a certain super-class can facilitate the generalization of the minority classes. However, existing super-class construction methods have some inherent disadvantages. Specifically, taxonomy-based methods suffer a gap between the semantic space and the feature space, and the performance of learning-based algorithms strongly depends on the features and data distribution. In this paper, we propose a deep super-class learning (DSCL) model to tackle the problem of long-tail distributed image classification. Motivated by the observation that classes belonging to the same super-class usually have more similar evaluations on the features than those belonging to different super-classes, we design a block-structured sparse constraint and attach it on the top of a convolutional neural network. Thus, the proposed DSCL model can accomplish representation learning, classifier training, and super-class construction in a unified end-to-end learning procedure. We compared the proposed model with several super-class construction methods on two public image datasets. Experimental results show that the super-class construction strategy is effective for the long-tail distributed classification, and the DSCL model can achieve better results than the other methods.","Super-class construction, Block-structured sparsity, Deep learning, Long-tail distribution",Yucan Zhou and Qinghua Hu and Yu Wang,https://www.sciencedirect.com/science/article/pii/S0031320318300888,https://doi.org/10.1016/j.patcog.2018.03.003,0031-3203,2018,118--128,80,Pattern Recognition,Deep super-class learning for long-tail distributed image classification,article,ZHOU2018118,
"Gliomas are the most common and aggressive primary brain tumours, with a short-life expectancy in their highest grade. Magnetic Resonance Imaging is the most common imaging technique to assess brain tumours. However, performing manual segmentation is a difficult and tedious task, mainly due to the large amount of information to be analysed. Therefore, there is a need for automatic and robust segmentation methods. We propose an automatic hierarchical brain tumour segmentation pipeline using Extremely Randomized Trees with appearance- and context-based features. Some of these features are computed over non-linear transformations of the Magnetic Resonance Imaging images. Our proposal was evaluated using the publicly available 2013 Brain Tumour Segmentation Challenge database, BRATS 2013. In the Challenge dataset, the proposed approach obtained a Dice Similarity Coefficient of 0.85, 0.79, and 0.75 for the complete, core, and enhancing regions, respectively.","Brain tumour, Magnetic resonance imaging, Image segmentation, Hierarchy of classifiers, Extremely randomized trees, Machine learning",Adriano Pinto and SÃ©rgio Pereira and Deolinda Rasteiro and Carlos A. Silva,https://www.sciencedirect.com/science/article/pii/S0031320318301699,https://doi.org/10.1016/j.patcog.2018.05.006,0031-3203,2018,105--117,82,Pattern Recognition,Hierarchical brain tumour segmentation using extremely randomized trees,article,PINTO2018105,
"In this paper we deal with the problem of addressing multi-class problems with decomposition strategies. Based on the divide-and-conquer principle, a multi-class problem is divided into a number of easier to solve sub-problems. In order to do so, binary decomposition is considered to be the most popular approach. However, when using this strategy we may deal with the problem of non-competent classifiers. Otherwise, recent studies highlighted the potential usefulness of one-class classifiers for this task. Despite not using all the available knowledge, one-class classifiers have several desirable properties that may benefit the decomposition task. From this perspective, we propose a novel approach for combining one-class classifiers to solve multi class problems based on dynamic ensemble selection, which allows us to discard non-competent classifiers to improve the robustness of the combination phase. We consider the neighborhood of each instance to decide whether a classifier may be competent or not. We further augment this with a threshold option that prevents from the selection of classifiers corresponding to classes with too little examples in this neighborhood. To evaluate the usefulness of our approach an extensive experimental study is carried out, backed-up by a thorough statistical analysis. The results obtained show the high quality of our proposal and that the dynamic selection of one-class classifiers is a useful tool for decomposing multi-class problems.","Machine learning, Classifier ensemble, One-class classification, Multi-class decomposition, Dynamic classifier selection, Ensemble pruning",Bartosz Krawczyk and Mikel Galar and MichaÅ WoÅºniak and Humberto Bustince and Francisco Herrera,https://www.sciencedirect.com/science/article/pii/S0031320318301857,https://doi.org/10.1016/j.patcog.2018.05.015,0031-3203,2018,34--51,83,Pattern Recognition,Dynamic ensemble selection for multi-class classification with one-class classifiers,article,KRAWCZYK201834,
"In this paper, we present an algorithm that can accurately and robustly detect regions of copy-move forgery. We firstly adapt and enhance a coherency sensitive hashing method to establish the feature correspondences in an image. Then, a local bidirectional coherency error is proposed to refine the feature correspondences via iteration over the enhanced coherency sensitive search. When the variation in the local bidirectional coherency error of the host image is not larger than a specified threshold, the iterative process stops, indicating that the feature correspondences are stable. In the end, from the stable feature correspondences, the copy-move forgery regions are easily detected using the local bidirectional coherency error of each feature. The experimental results show the proposed detection method achieves real-time or near real-time effectiveness; at the same time, it can achieve very good detection results compared with the state-of-the-art copy-move forgery detection algorithms, even under various challenging conditions.","Copy-move forgery detection, Enhanced coherency sensitive searching, Local bidirectional coherency error",Xiuli Bi and Chi-Man Pun,https://www.sciencedirect.com/science/article/pii/S0031320318301183,https://doi.org/10.1016/j.patcog.2018.03.028,0031-3203,2018,161--175,81,Pattern Recognition,Fast copy-move forgery detection using local bidirectional coherency error refinement,article,BI2018161,
"In this paper we introduce an evidence combining inference approach based on factor graphs. The method presented here is quite general in nature and exploits the capability of factor graphs to combine results from multiple algorithms which correspond to different generative models or graphical structures. We do this by using layers across the factor graph to represent each of the algorithms under consideration. For purposes of inference, we convert each of these layers into a simplicial complex using a convex hull algorithm. This allows us to obtain a simplicial spanning tree for each of these simplicial complexes. Making use of this simplicial spanning tree, which corresponds to the reparameterisation of the junction tree of the factor graph, exact inference can be performed using the sum/max-product algorithm. Furthermore, we employ a Procrustean transformation so as to avoid degenerate cases in the inference process. We illustrate how the method can be used for evidence combining in image defogging and compare it against other alternatives elsewhere in literature.","Factor graphs, Evidence combining, Simplicial spanning tree, Procrustes transformation, Maximum a-posteriori inference, Image defogging",Lawrence Mutimbu and Antonio Robles-Kelly,https://www.sciencedirect.com/science/article/pii/S0031320318301584,https://doi.org/10.1016/j.patcog.2018.04.023,0031-3203,2018,56--67,82,Pattern Recognition,A factor graph evidence combining approach to image defogging,article,MUTIMBU201856,
"Clustering ensembles (CE) comprise a class of pattern recognition methods that take a set of data clusterings (base partitions) as input and generate a consensus, better-quality partition as output. This work tackles the CE problem from a hedonic game theoretical perspective. In the modeled cooperative game, data points are viewed as players while clusters are regarded as coalitions. Interestingly, we show that by using an evidence-accumulation based similarity measure our novel Hedonic Game based Clustering Ensemble (HGCE) algorithm always converges to a Nash stable coalition structure, that is, to a clustering solution that cannot be unilaterally improved from the standpoint of each data point. A variant of the algorithm is also introduced, which is insensitive to the way the data points are ordered in the data set. In order to assess the potentials of HGCE and contrast its performance with that exhibited by a number of CE methods, experiments have been conducted on several artificial and real-world data sets, the majority of which related to bioinformatics. Overall, the empirical results and statistical tests relative to two well-known external validity measures ratify the usefulness and competitiveness of the proposed approach, also showing that HGCE is computationally efficient and resilient to random perturbations to the set of base partitions.","Data clustering, Clustering ensemble, Hedonic game, Nash stability, Evidence accumulation",Nelson C. Sandes and AndrÃ© L.V. Coelho,https://www.sciencedirect.com/science/article/pii/S0031320318301067,https://doi.org/10.1016/j.patcog.2018.03.017,0031-3203,2018,95--111,81,Pattern Recognition,Clustering ensembles: A hedonic game theoretical approach,article,SANDES201895,
"In this paper, we propose a novel approach for learning multi-label classifiers with the help of privileged information. Specifically, we use similarity constraints to capture the relationship between available information and privileged information, and use ranking constraints to capture the dependencies among multiple labels. By integrating similarity constraints and ranking constraints into the learning process of classifiers, the privileged information and the dependencies among multiple labels are exploited to construct better classifiers during training. A maximum margin classifier is adopted, and an efficient learning algorithm of the proposed method is also developed. We evaluate the proposed method on two applications: multiple object recognition from images with the help of implicit information about object importance conveyed by the list of manually annotated image tags; and multiple facial action unit detection from low-resolution images augmented by high-resolution images. Experimental results demonstrate that the proposed method can effectively take full advantage of privileged information and dependencies among multiple labels for better object recognition and better facial action unit detection.","Privileged information, Multi-label classification, Similarity constraints",Shangfei Wang and Shiyu Chen and Tanfang Chen and Xiaoxiao Shi,https://www.sciencedirect.com/science/article/pii/S0031320318301225,https://doi.org/10.1016/j.patcog.2018.03.033,0031-3203,2018,60--70,81,Pattern Recognition,Learning with privileged information for multi-Label classification,article,WANG201860,
"Most of the traditional pattern classifiers assume their input data to be well-behaved in terms of similar underlying class distributions, balanced size of classes, the presence of a full set of observed features in all data instances, etc. Practical datasets, however, show up with various forms of irregularities that are, very often, sufficient to confuse a classifier, thus degrading its ability to learn from the data. In this article, we provide a birdâs eye view of such data irregularities, beginning with a taxonomy and characterization of various distribution-based and feature-based irregularities. Subsequently, we discuss the notable and recent approaches that have been taken to make the existing stand-alone as well as ensemble classifiers robust against such irregularities. We also discuss the interrelation and co-occurrences of the data irregularities including class imbalance, small disjuncts, class skew, missing features, and absent (non-existing or undefined) features. Finally, we uncover a number of interesting future research avenues that are equally contextual with respect to the regular as well as deep machine learning paradigms.","Data irregularities, Class imbalance, Small disjuncts, Class-distribution skew, Missing features, Absent features",Swagatam Das and Shounak Datta and Bidyut B. Chaudhuri,https://www.sciencedirect.com/science/article/pii/S0031320318300931,https://doi.org/10.1016/j.patcog.2018.03.008,0031-3203,2018,674--693,81,Pattern Recognition,"Handling data irregularities in classification: Foundations, trends, and future challenges",article,DAS2018674,
"In this paper, we investigate the non-zero representation coefficients of Locally Linear KNN (LLKNN) and propose a nonnegative extension of LLKNN (NLLKNN) model for image recognition, where representation coefficients are restricted to be nonnegative to avoid meaningless and unreasonable negative coefficients. A multiplicative iterative algorithm with proof of convergence is proposed to solve the proposed NLLKNN model. Then NLLKNN based classifier (NLLKNNC) and Nonnegative Locally Linear Nearest Mean Classifier (NLLNMC) are proposed. We also investigate the robustness of NLLKNNC and NLLNMC to noises and occlusions. The effectiveness of the proposed methods is evaluated on several image recognition tasks such as scene recognition and face recognition. Extensive experimental results show that the proposed algorithm converges very fast and the proposed methods outperform some representative image recognition methods.","Sparse minimization, Nonnegative, Locally linear KNN, Robustness, Image recognition",Si-Bao Chen and Yu-Lan Xu and Chris H.Q. Ding and Bin Luo,https://www.sciencedirect.com/science/article/pii/S0031320318301985,https://doi.org/10.1016/j.patcog.2018.05.024,0031-3203,2018,78--90,83,Pattern Recognition,A Nonnegative Locally Linear KNN model for image recognition,article,CHEN201878,
"Visual tracking is still a challenging task as the objects suffer significant appearance changes, fast motion, and serious occlusion. In this paper, we propose an occlusion-aware part-based tracker for robust visual tracking. We first present a novel occlusion-aware part-based model based on correlation filters to integrate the global model and part-based model adaptively. It can effectively employ both the global and local information to improve the robustness of the tracker. Then we propose an integral pipeline aiming to the long-term tracking under the correlation filters, which achieves the state-of-the-art performance. In this tracking pipeline, we adopt separate translation and scale estimation. For translation estimation, we exploit and jointly learn the hierarchical features of deep Convolutional Neural Networks (CNNs) to locate the target center accurately. Then we learn an independent scale correlation filter to handle the scale variation. This design realizes scale adaptation of the target preferably, and reduces computational complexity efficiently. We further ameliorate the model update method by introducing the original reliable information. It greatly alleviates the error accumulation of the incorrect information and efficiently achieves long-term tracking. Extensive experimental results on several different challenging benchmark datasets show that our proposed tracker achieves outstanding performance against the state-of-the-art methods.","Visual tracking, Correlation filters, Convolutional neural networks, Object occlusion, Online model update",Xin Wang and Zhiqiang Hou and Wangsheng Yu and Lei Pu and Zefenfen Jin and Xianxiang Qin,https://www.sciencedirect.com/science/article/pii/S0031320318301468,https://doi.org/10.1016/j.patcog.2018.04.011,0031-3203,2018,456--470,81,Pattern Recognition,Robust occlusion-aware part-based visual tracking with object scale adaptation,article,WANG2018456,
"While texture analysis is largely addressed for images, the comparison of the geometric reliefs on surfaces embedded in the 3D space is still an open challenge. Starting from the Local Binary Pattern (LBP) description originally defined for images, we introduce the edge-Local Binary Pattern (edgeLBP) as a local description able to capture the evolution of repeated, geometric patterns on surface meshes. Our extension is independent of the surface representation, indeed the edgeLBP is able to deal with surface tessellations characterized by non-uniform vertex distributions and different types of faces, such as triangles, quadrangles and, in general, convex polygons. Besides the desirable robustness properties the edgeLBP exhibits over a number of examples, we show how this description performs well for 3D pattern retrieval and compare our performances with the participants to a recent 3D pattern retrieval and classification contest [1].","LBP extension, surface tessellations, 3D pattern retrieval, 3D pattern classification",Elia {Moscoso Thompson} and Silvia Biasotti,https://www.sciencedirect.com/science/article/pii/S0031320318301626,https://doi.org/10.1016/j.patcog.2018.04.028,0031-3203,2018,1--15,82,Pattern Recognition,Description and retrieval of geometric patterns on surface meshes using an edge-based LBP approach,article,MOSCOSOTHOMPSON20181,
"For sparse representation or sparse coding based image classification, the dictionary, which is required to faithfully and robustly represent query images, plays an important role on its success. Learning dictionaries from the training data for sparse coding has shown state-of-the-art results in image classification and face recognition. However, for face recognition, conventional dictionary learning methods cannot well learn a reliable and robust dictionary due to suffering from the small-sample-size problem. The other significant issue is that current dictionary learning do not completely cover the important components of signal representation (e.g., commonality, particularity, and disturbance), which limit their performance. In order to solve the above issues, in this paper, we propose a novel robust, discriminative and comprehensive dictionary learning (RDCDL) method, in which a robust dictionary is learned from comprehensive training sample diversities generated by extracting and generating facial variations. Especially, to completely represent the commonality, particularity and disturbance, class-shared, class-specific and disturbance dictionary atoms are learned to represent the data from different classes. Discriminative regularizations on the dictionary and the representation coefficients are used to exploit discriminative information, which effectively improves the classification capability of the dictionary. The proposed RDCDL method is extensively evaluated on benchmark face image databases, and it shows superior performance to many state-of-the-art dictionary learning methods for face recognition.","Dictionary learning, Face recognition, Sparse representation",Guojun Lin and Meng Yang and Jian Yang and Linlin Shen and Weicheng Xie,https://www.sciencedirect.com/science/article/pii/S0031320318301109,https://doi.org/10.1016/j.patcog.2018.03.021,0031-3203,2018,341--356,81,Pattern Recognition,"Robust, discriminative and comprehensive dictionary learning for face recognition",article,LIN2018341,
"Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent studyÂ (Tommasi etÂ al., 2015) shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.","Domain adaptation, Batch normalization, Neural networks",Yanghao Li and Naiyan Wang and Jianping Shi and Xiaodi Hou and Jiaying Liu,https://www.sciencedirect.com/science/article/pii/S003132031830092X,https://doi.org/10.1016/j.patcog.2018.03.005,0031-3203,2018,109--117,80,Pattern Recognition,Adaptive Batch Normalization for practical domain adaptation,article,LI2018109,
"Humans are able to simultaneously identify a person and recognize his or her action based on biological motions. Previous work usually treats action recognition and person identification from motions as two separate tasks with different objectives. In this paper, we present an end-to-end framework to perform these two tasks together. Inspired by the recent success of deep recurrent neural networks (RNN) for skeleton based action recognition, we propose a new pipeline to recognize both actions and persons from skeletons extracted by RGBD sensors. The structure includes two subnets and is end-to-end trainable. The former is skeleton transformation, which accommodates viewpoint changes and noise. The latter is multi-task RNN for joint learning and various architectures are explored including a novel architecture that learns the joint probability between the two output variables. Experiments on 3D action recognition benchmark datasets demonstrate the benefits of multi-task learning and our method dramatically outperforms the existing state-of-the-art in action recognition.","Content and style, Action recognition, Person identification from motions, Skeleton transformation, Multi-task RNN",Hongsong Wang and Liang Wang,https://www.sciencedirect.com/science/article/pii/S0031320318301195,https://doi.org/10.1016/j.patcog.2018.03.030,0031-3203,2018,23--35,81,Pattern Recognition,Learning content and style: Joint action recognition and person identification from human skeletons,article,WANG201823,
"Computer vision tasks prefer the images focused at the related objects for a better performance, which requests a Auto-ReFocusing (ARF) function for using light field cameras. However, the current ARF schemes are time-consuming in practice, because they commonly need to render an image sequence for finding the optimally refocused frame. This paper presents an efficient ARF solution for light-field cameras based on modeling the refocusing point spread function (R-PSF). The R-PSF holds a simple linear relationship between refocusing depth and defocus blurriness. Such a linear relationship enables to determine the two candidates of the optimally refocused frame from only one initial refocused image. Because our method only involves three times of refocusing rendering for finding the optimally refocused frame, it is much more efficient than the current ârendering and selectionâ solutions which need to render a large number of refocused images.","Auto-refocusing, Detection-based focusing, Blurriness measure, Light-field photography",Chi Zhang and Guangqi Hou and Zhaoxiang Zhang and Zhenan Sun and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S0031320318301110,https://doi.org/10.1016/j.patcog.2018.03.020,0031-3203,2018,176--189,81,Pattern Recognition,Efficient auto-refocusing for light field camera,article,ZHANG2018176,
"Many real-world applications propose the request for sharing knowledge among different tasks or datasets. Transfer learning has been proposed to solve this kind of problems and it has been successfully applied in supervised learning and semi-supervised learning settings. However, its adoption in clustering, one of the most classical research problems in machine learning and data mining, is still scarce. Spectral clustering, as a major clustering algorithm with wide applications and better performance than k-means typically, has not been well incorporated with knowledge transfer. In this paper, we first consider the problem of learning from only one auxiliary unlabeled dataset for spectral clustering and propose a novel algorithm called transfer spectral clustering (TSC). Then, it is extended to the settings with multiple auxiliary tasks. TSC assumes the feature embeddings being shared with the auxiliary tasks and utilizes co-clustering to extract useful information from the auxiliary datasets to improve the clustering performance. TSC involves not only the data manifold information of individual task but also the feature manifold information shared between related tasks. An in-depth explanation of our algorithm together with a convergence analysis are provided. As demonstrated by the extensive experiments, TSC can effectively improve the clustering performance by using auxiliary unlabeled data when compared with other state-of-the-art clustering algorithms.","Transfer learning, Spectral clustering, Co-clustering, Multi-task learning",Wenhao Jiang and Wei Liu and Fu-lai Chung,https://www.sciencedirect.com/science/article/pii/S0031320318301511,https://doi.org/10.1016/j.patcog.2018.04.018,0031-3203,2018,484--496,81,Pattern Recognition,Knowledge transfer for spectral clustering,article,JIANG2018484,
"A scene image is typically composed of successive background contexts and objects with regular shapes. To acquire such spatial information, we propose a new type of spatial partitioning scheme and a modified pyramid matching kernel based on spatial pyramid matching (SPM). A dense histogram of oriented gradients (HOG) is used as a low-level visual descriptor. Furthermore, inspired by the expressive coding ability of autoencoders, we also propose another approach that encodes local descriptors into mid-level features using various autoencoders. The learned mid-level features are encouraged to be sparse, robust and contractive. Then, modified spatial pyramid pooling and local normalization of the mid-level features facilitate the generation of high-level image signatures for scene classification. Comprehensive experimental results on publicly available scene datasets demonstrate the effectiveness of our methods.","Spatial pyramid matching (SPM), Spatial partition, Histogram of oriented gradients (HOG), Autoencoder, Scene recognition",Lin Xie and Feifei Lee and Li Liu and Zhong Yin and Yan Yan and Weidong Wang and Junjie Zhao and Qiu Chen,https://www.sciencedirect.com/science/article/pii/S0031320318301596,https://doi.org/10.1016/j.patcog.2018.04.025,0031-3203,2018,118--129,82,Pattern Recognition,Improved spatial pyramid matching for scene recognition,article,XIE2018118,
"In this paper, we propose a the gradient direction-based hierarchical adaptive sparse and low-rank (GD-HASLR) model, to solve the real-world occluded face recognition problem. In the real-world scenario, neutral face images as training data are very few, usually a single image per subject. The proposed GD-HASLR has the ability to tackle this scenario. We first utilize the robustness of image gradient direction features with the proposed generalized image gradient direction. We then propose a novel hierarchical sparse and low-rank model, which combines sparse representation on dictionary learning and low-rank representation on the error, which are usually messy in the gradient direction domain. We call this scenario the weak low-rankness optimization. We solve this problem efficiently under the alternating direction method of multipliers framework, resulting in the optimum error term that has a similar weak low-rank structure as the reference error map. The recognition accuracy can be enhanced greatly via weak low-rankness optimization. Extensive experiments are conducted using real-world disguise/occlusion data and synthesized contiguous occlusion data. These results show that with very few neutral face images as training data, the proposed GD-HASLR model has the best performance compared to other state-of-the-art methods, including popular convoluntional neural nework-based methods.","Occluded face recognition, Robust sparse representation, Low-rank regression model",Cho Ying Wu and Jian Jiun Ding,https://www.sciencedirect.com/science/article/pii/S0031320318301079,https://doi.org/10.1016/j.patcog.2018.03.016,0031-3203,2018,256--268,80,Pattern Recognition,Occluded face recognition using low-rank regression with generalized gradient direction,article,WU2018256,
"We present a dual-feature based point set registration method with global-local structural preservation. A finite mixture model which is able to deal with two features is first constructed. The mixture structure descriptor (MSD) is then obtained by smoothly combining the local structure descriptor (LSD) with the original coordinates through an annealing scheme. Substituting the MSD into the constructed model a fuzzy corresponding matrix is acquired. Next, the energy function which contains three main terms is formulated in the reproducing kernel Hilbert space. The first term is based on the L2 estimation (L2E) criterion, and the other two terms play a complementary role to improve the robustness and accuracy for transformation estimation at both global and local scales. The performances of our method in synthetic data, sequence data and real data against nine state-of-the-art methods are tested where our method shows favorable performance when compared with other methods.","Registration, Dual-feature, Finite mixture model, Regularization, Structural preservation",Su Zhang and Kun Yang and Yang Yang and Yi Luo and Ziquan Wei,https://www.sciencedirect.com/science/article/pii/S003132031830089X,https://doi.org/10.1016/j.patcog.2018.03.004,0031-3203,2018,183--195,80,Pattern Recognition,Non-rigid point set registration using dual-feature finite mixture model and global-local structural preservation,article,ZHANG2018183,
"This work proposes a new type of classifier called Morphological Classifier (MC). MCs aggregate concepts from mathematical morphology and supervised learning. The outcomes of this aggregation are classifiers that may preserve shape characteristics of classes, subject to the choice of a stopping criterion and structuring element. MCs are fundamentally based on set theory, and their classification model can be a mathematical set itself. Two types of morphological classifiers are proposed in the current work, namely, Morphological k-NN (MkNN) and Morphological Dilation Classifier (MDC), which demonstrate the feasibility of the approach. This work provides evidence regarding the advantages of MCs, e.g., very fast classification times as well as competitive accuracy rates. The performance of MkNN and MDC was tested using p-dimensional datasets. MCs tied or outperformed 14 well established classifiers in 5 out of 8 datasets. In all occasions, the obtained accuracies were higher than the average accuracy obtained with all classifiers. Moreover, the proposed implementations utilize the power of the Graphics Processing Units (GPUs) to speed up processing.","Morphological classifier, Supervised learning, Mathematical morphology, Set theory",Ã.O. Rodrigues and A. Conci and P. Liatsis,https://www.sciencedirect.com/science/article/pii/S0031320318302206,https://doi.org/10.1016/j.patcog.2018.06.010,0031-3203,2018,82--96,84,Pattern Recognition,Morphological classifiers,article,RODRIGUES201882,
"Image inpainting is a process of reconstructing missing regions, or removing unwanted objects automatically by propagating intensity and texture information from surrounding parts of the image in a visually plausible manner. We propose a new exemplar-based image inpainting algorithm, which uses the recently developed metric called the perceptual-fidelity aware mean squared error (PAMSE). The PAMSE is a Gaussian-smoothed mean squared error (MSE) and approximates a weighted sum of the gradient of MSE, the Laplacian of MSE, and MSE itself. We show that, compared to MSE, PAMSE is a promising perceptual fidelity metric for application to image inpainting and leads to better performance in propagating texture and geometric structure simultaneously.","Exemplar-based image inpainting, Object removal, PAMSE, Texture synthesis, Geometric structure propagation",Ding Ding and Sundaresh Ram and Jeffrey J. Rodriguez,https://www.sciencedirect.com/science/article/pii/S0031320318301997,https://doi.org/10.1016/j.patcog.2018.05.025,0031-3203,2018,174--184,83,Pattern Recognition,Perceptually aware image inpainting,article,DING2018174,
"Lung cancer has been recognized as the primary global cause of death among cancer patients. This work is intended to develop a methodology for diagnosis of lung nodules using images from the Lung Image Database Consortium and Image Database Resource Initiative. The proposed method uses image processing and pattern recognition techniques. To differentiate the patterns of malignant and benign forms, we used index basic taxic weights and standardized taxic weights. Finally, we applied a convolutional neural network for classification. In the test stage, we applied the proposed methodology to 50,580 (14,184 malignant and 36,396 benign) nodules from the image database. The proposed method presents promising results for the diagnosis of malignancy and benignity, achieving an accuracy of 92.63%, sensitivity of 90.7%, specificity of 93.47%, and receiver operating characteristic curve of 0.934. These results are promising and demonstrate a real rate of correct detections using the texture features. Because precocious detection allows faster therapeutic intervention, and thus a more favorable prognosis for the patient, we propose herein a methodology that contributes to the field in this aspect.","Lung cancer, Phylogenetic diversity index, Convolutional neural network",Antonio Oseas {de Carvalho Filho} and Aristofanes CorrÃªa Silva and Anselmo Cardoso {de Paiva} and Rodolfo AcatauassÃº Nunes and Marcelo Gattass,https://www.sciencedirect.com/science/article/pii/S0031320318301237,https://doi.org/10.1016/j.patcog.2018.03.032,0031-3203,2018,200--212,81,Pattern Recognition,Classification of patterns of benignity and malignancy based on CT using topology-based phylogenetic diversity index and convolutional neural network,article,DECARVALHOFILHO2018200,
"Blind image quality assessment (BIQA) aims at precisely estimating human perceived image quality with no access to a reference. Recently, several attempts have been made to develop BIQA methods based on deep neural networks (DNNs). Although these methods obtained promising performance, they have some limitations: (1) their DNN models are actually âshallowâ in term of depth; and (2) these methods typically use the output of the last layer in the DNN model as the feature representation for quality prediction. Since the representation depth has been demonstrated beneficial for various vision tasks, it is significant to explore very deep networks for learning BIQA models. Besides, the information in the last layer may unduly generalize over local artifacts which are highly related to quality degradation. On the contrary, intermediate layers may be sensitive to local degradations but will not capture high-level semantics. Thus, reasoning at multiple levels of representation is necessary in the IQA task. In this paper, we propose to extract multi-level representations from a very deep DNN model for learning an effective BIQA model, and consequently present a simple but extraordinarily effective BIQA framework, codenamed BLINDER (BLind Image quality predictioN via multi-level DEep Representations). Thorough experiments have been conducted on five standard databases, which show that a significant improvement can be achieved by adopting multi-level deep representations. Besides, BLINDER considerably outperforms previous state-of-the-art BIQA methods for authentically distorted images.","Image quality assessment, Deep learning, Convolutional Neural Networks (CNN), Multi-level deep representation, Support vector regression",Fei Gao and Jun Yu and Suguo Zhu and Qingming Huang and Qi Tian,https://www.sciencedirect.com/science/article/pii/S003132031830150X,https://doi.org/10.1016/j.patcog.2018.04.016,0031-3203,2018,432--442,81,Pattern Recognition,Blind image quality prediction by exploiting multi-level deep representations,article,GAO2018432,
"This paper proposes a new method of personal identification that introduces the analysis of lip prints. In spite of its important role in forensic and biometric applications, the results of previous investigations into lip prints are scanty. This is mainly due to the difficulties that accompany any analysis of the lips: lips are very flexible and pliable, and successive lip print impressions â even those obtained from the same person â may significantly differ from one other. Our articleâs principal contribution is a proposal for a new personal identification methodology and application that uses, for the first time, a strategy for biometric and forensic analysis of lip print structure. As a result of our analysis we propose a lip print pattern for each individual. This pattern contains only such furrows that occur on the greatest number of lip prints obtained from the same person, where these furrowsâ locations and inclinations remain similar across the lip prints obtained. It should be noted that in our approach, instead of lip photos we employ lip prints, such as can be obtained at a crime scene. It is worth noticing also that we propose a new method of personal identification where, instead of popular machine learning methods,the furrow-analysis of lip prints is introduced. Thus, no classifier learning phase is required. According to the authorsâ convictions, based on reports in the literature, the proposed approach describes for the first time a strategy as to how lip print structures could be analyzed in biometric applications. The main novelty of this paper is its use of multiple lip print furrows from the same person to determine the most common lip furrow distribution in four different directions. On the basis of the pattern recognition of the lip furrows, the identity of the person will be determined. The proposed methodâs effectiveness is 92.73%, determined using a database containing 350 lip prints.","Person identification, Lip print pattern, Forensics, Biometrics",Krzysztof Wrobel and Rafal Doroz and Piotr Porwik and Marcin Bernas,https://www.sciencedirect.com/science/article/pii/S0031320318301638,https://doi.org/10.1016/j.patcog.2018.04.030,0031-3203,2018,585--600,81,Pattern Recognition,Personal identification utilizing lip print furrow based patterns. A new approach,article,WROBEL2018585,
"Due to the factors like rapidly fast motion, cluttered backgrounds, arbitrary object appearance variation and shape deformation, an effective target representation plays a key role in robust visual tracking. Existing methods often employ bounding boxes for target representations, which are easily polluted by noisy clutter backgrounds that may cause drifting problem when the target undergoes large-scale non-rigid or articulated motions. To address this issue, in this paper, motivated by the spatio-temporal nonlocality of target appearance reoccurrence in a video, we explore the nonlocal information to accurately represent and segment the target, yielding an object likelihood map to regularize a correlation filter (CF) for visual tracking. Specifically, given a set of tracked target bounding boxes, we first generate a set of superpixels to represent the foreground and background, and then update the appearance of each superpixel with its long-term spatio-temporally nonlocal counterparts. Then, with the updated appearances, we formulate a spatio-temporally graphical model comprised of the superpixel label consistency potentials. Afterwards, we generate segmentation by optimizing the graphical model via iteratively updating the appearance model and estimating the labels. Finally, with the segmentation mask, we obtain an object likelihood map that is employed to adaptively regularize the CF learning by suppressing the clutter background noises while making full use of the long-term stable target appearance information. Extensive evaluations on the OTB50, SegTrack, Youtube-Objects datasets demonstrate the effectiveness of the proposed method, which performs favorably against some state-of-art methods.","Visual tracking, Video segmentation, Nonlocal appearance learning, Graphical model, Optical flow",Kaihua Zhang and Xuejun Li and Huihui Song and Qingshan Liu and Wei Lian,https://www.sciencedirect.com/science/article/pii/S0031320318301912,https://doi.org/10.1016/j.patcog.2018.05.017,0031-3203,2018,185--195,83,Pattern Recognition,Visual tracking using spatio-temporally nonlocally regularized correlation filter,article,ZHANG2018185,
"Vehicle verification in different scenes is a nontrivial problem that cannot be solved by simple correspondence matching. In the paper, the verification problem is treated as a binary classification problem. If the two vehicles in two views are the same, they are a positive pair; otherwise, a negative pair. Here, we propose an effective sparse representation (SR) method called Boost K-SVD to generate the feature vectors for vehicle representation. In Boost K-SVD, the particle filtering is first applied for the initial atom selection. Then, it finds the atoms satisfying the restricted isometry property (RIP). Finally, we propose a discrimination criterion to determine the optimal dictionary size. Instead of initial random atom selection, Boost K-SVD generates the atoms incrementally to create a more compact dictionary. Furthermore, the dictionary with RIP can produce sparser representation vectors with higher verification accuracy. Experimental results show that our method has better performance compared with the other methods.","Boost K-SVD, K-SVD, Vehicle verification, Atom initializing, Restricted isometry property (RIP)",Shih-Chung Hsu and I-Cheng Chang and Chung-Lin Huang,https://www.sciencedirect.com/science/article/pii/S0031320318300827,https://doi.org/10.1016/j.patcog.2018.02.031,0031-3203,2018,131--146,81,Pattern Recognition,Vehicle verification between two nonoverlapped views using sparse representation,article,HSU2018131,
"Deep Neural Network (DNN) has recently achieved outstanding performance in a variety of computer vision tasks, including facial attribute classification. The great success of classifying facial attributes with DNN often relies on a massive amount of labelled data. However, in real-world applications, labelled data are only provided for some commonly used attributes (such as age, gender); whereas, unlabelled data are available for other attributes (such as attraction, hairline). To address the above problem, we propose a novel deep transfer neural network method based on multi-label learning for facial attribute classification, termed FMTNet, which consists of three sub-networks: the Face detection Network (FNet), the Multi-label learning Network (MNet) and the Transfer learning Network (TNet). Firstly, based on the Faster Region-based Convolutional Neural Network (Faster R-CNN), FNet is fine-tuned for face detection. Then, MNet is fine-tuned by FNet to predict multiple attributes with labelled data, where an effective loss weight scheme is developed to explicitly exploit the correlation between facial attributes based on attribute grouping. Finally, based on MNet, TNet is trained by taking advantage of unsupervised domain adaptation for unlabelled facial attribute classification. The three sub-networks are tightly coupled to perform effective facial attribute classification. A distinguishing characteristic of the proposed FMTNet method is that the three sub-networks (FNet, MNet and TNet) are constructed in a similar network structure. Extensive experimental results on challenging face datasets demonstrate the effectiveness of our proposed method compared with several state-of-the-art methods.","Transfer learning, Facial attribute classification, Multi-label learning, Deep learning, Convolutional neural networks",Ni Zhuang and Yan Yan and Si Chen and Hanzi Wang and Chunhua Shen,https://www.sciencedirect.com/science/article/pii/S0031320318301080,https://doi.org/10.1016/j.patcog.2018.03.018,0031-3203,2018,225--240,80,Pattern Recognition,Multi-label learning based deep transfer neural network for facial attribute classification,article,ZHUANG2018225,
"Owing to advances in information technology, online communications between people living in different parts of the world have considerably increased. The subsequent emergence of social networks helped this kind of communications to be further organized. One of the most important issues considered when analyzing these kinds of networks is community detection, in which a majority of studies tend to detect disjoint communities through analyzing linkages of networks. What this paper aims to achieve is to obtain overlapping communities in which the members have the same topics of interest, and where the strengths of connections between them are the consequence of their communicationsâ content analysis. Consequently, we have hereby proposed a generic framework for overlapping community detection in social networks with special focus on rating-based social networks. This framework considers the information shared by the users (ratings), as well as their topics of interest, for the sake of finding meaningful communities. This will lead us to topical communities in which members are interested in the same topics, and the strengths of their relationships are directly based on the rate of their viewpointsâ unity. Quantitative evaluations also reveal that the framework presented in this study achieves favorable results which are quite superior to the results of 3 other relevant frameworks in the literature.","Overlapping community detection, Content analysis, Topical community, Semantic network, Rating-based social networks",Ali Reihanian and Mohammad-Reza Feizi-Derakhshi and Hadi S. Aghdasi,https://www.sciencedirect.com/science/article/pii/S0031320318301481,https://doi.org/10.1016/j.patcog.2018.04.013,0031-3203,2018,370--387,81,Pattern Recognition,"Overlapping community detection in rating-based social networks through analyzing topics, ratings and links",article,REIHANIAN2018370,
"Novelty detection is an important task in a variety of applications such as object recognition, defect localization, medical diagnostics, and event detection. The objective of novelty detection is to distinguish one class, for which data are available, from all other possible classes when there is insufficient information to build an explicit model for the latter. The data from the observed class are usually represented in terms of certain features which can be modeled as random variables (RV). An important challenge for novelty detection in multivariate problems is characterizing the statistical dependencies among these RVs. Failure to consider these dependencies may lead to inaccurate predictions, usually in the form of high false positive rates. In this study, we propose conditional classifiers as a new approach for novelty detection that is capable of accounting for statistical dependencies of the relevant RVs without simplifying assumptions. To implement the proposed idea, we use Gaussian mixture models (GMM) along with forward stage-wise additive modeling and boosting methods to learn the conditional densities of RVs that represent our observed data. The resulting model, which is called a boosted conditional GMM, is then used as a basis for classification. To test the performance of the proposed method, we apply it to a realistic application problem for analyzing sensor networks and compare the results with those of alternative schemes.","Novelty detection, Mixture models, Graphical models, Conditional dependence, Conditional density, Additive modeling, Boosting, False positive",Reza Mohammadi-Ghazi and Youssef M. Marzouk and Oral BÃ¼yÃ¼kÃ¶ztÃ¼rk,https://www.sciencedirect.com/science/article/pii/S0031320318301122,https://doi.org/10.1016/j.patcog.2018.03.022,0031-3203,2018,601--614,81,Pattern Recognition,Conditional classifiers and boosted conditional Gaussian mixture model for novelty detection,article,MOHAMMADIGHAZI2018601,
"One of the fundamental assumptions of traditional manifold learning algorithms is that the embedded manifold is globally or locally isometric to Euclidean space. Under this assumption, these algorithms divided manifold into a set of overlapping local patches which are locally isometric to linear subsets of Euclidean space. Then the learnt manifold would be a flat manifold with zero Riemannian curvature. But in the general cases, manifolds may not have this property. To be more specific, the traditional manifold learning does not consider the curvature information of the embedded manifold. In order to improve the existing algorithms, we propose a curvature-aware manifold learning algorithm called CAML. Without considering the local and global assumptions, we will add the curvature information to the process of manifold learning, and try to find a way to reduce the redundant dimensions of the general manifolds which are not isometric to Euclidean space. The experiments have shown that CAML has its own advantage comparing to other traditional manifold learning algorithms in the sense of the neighborhood preserving ratios (NPR) on synthetic databases and classification accuracies on image set classification.","Manifold learning, Riemannian curvature, Second fundamental form, Hessian operator",Yangyang Li,https://www.sciencedirect.com/science/article/pii/S0031320318302176,https://doi.org/10.1016/j.patcog.2018.06.007,0031-3203,2018,273--286,83,Pattern Recognition,Curvature-aware manifold learning,article,LI2018273,
"In this paper, a novel deep convolutional neural network is proposed to learn discriminative binary hash video representations for face retrieval. The network integrates face feature extractor and hash functions into a unified optimization framework to make the two components be as compatible as possible. In order to achieve better initializations for the optimization, the low-rank discriminative binary hashing method is introduced to pre-learn the hash functions of the network during the training procedure. The input to the network is a face frame, and the output is the corresponding binary hash frame representation. Frame representations of a face video shot are fused by hard voting to generate the binary hash video representation. Each bit in the binary representation of frame/video describes the presence or absence of a face attribute, which makes it possible to retrieve faces among both the image and video domains. Extensive experiments are conducted on two challenging TV-Series datasets, and the excellent performance demonstrates the effectiveness of the proposed network.","Face video retrieval, Cross-domain face retrieval, Deep CNN, Hash learning",Zhen Dong and Chenchen Jing and Mingtao Pei and Yunde Jia,https://www.sciencedirect.com/science/article/pii/S003132031830147X,https://doi.org/10.1016/j.patcog.2018.04.014,0031-3203,2018,357--369,81,Pattern Recognition,Deep CNN based binary hash video representations for face retrieval,article,DONG2018357,
"Conventional graph based clustering methods treat all features equally even if they are redundant features or noise in the stage of graph learning, which is obviously unreasonable. In this paper, we propose a novel graph learning method named adaptive weighted nonnegative low-rank representation (AWNLRR) for data clustering. Based on the observation that noise and outliers usually cannot be represented well and suffer from larger reconstruction errors than the important features (clean features) in low-rank or sparse representation, we impose an adaptive weighted matrix on the data reconstruction errors to reinforce the role of the important features in the joint representation and thus a robust graph can be obtained. In addition, a locality constraint, i.e., distance regularization term, is introduced to capture the local structure of data and enable the obtained graph to be sparser. These appealing properties allow AWNLRR to well capture the intrinsic structure of data, and thus AWNLRR has potential to achieve a better clustering performance than other methods. Experimental results on synthetic and real databases show that the proposed method obtains the best clustering performance than some state-of-the-art methods.","Low-rank representation, Adaptive weighted matrix, Data clustering, Locality constraint",Jie Wen and Bob Zhang and Yong Xu and Jian Yang and Na Han,https://www.sciencedirect.com/science/article/pii/S0031320318301365,https://doi.org/10.1016/j.patcog.2018.04.004,0031-3203,2018,326--340,81,Pattern Recognition,Adaptive weighted nonnegative low-rank representation,article,WEN2018326,
"We propose an analysis scheme which addresses the Parzen-window and mixture model methods for estimating the probability density function of data points in feature space. Both methods construct the estimate as a sum of kernel functions (usually Gaussians). By adding an entropy-like function we prove that the probability distribution is a product of a weight function and a shape distribution. This Weight-Shape decomposition leads to new interpretations of established clustering algorithms. Furthermore, it suggests the construction of three different clustering schemes, which are based on gradient-ascent flow of replica points in feature space. Two of these are Quantum Clustering and the Mean-Shift algorithm. The third algorithm is based on maximal-entropy. In our terminology they become Maximal Shape Clustering, Maximal Probability Clustering and Maximal Weight Clustering, correspondingly. We demonstrate the different methods and compare them to each other on one artificial example and two natural data sets. We also apply the Weight-Shape decomposition to image analysis. The shape distribution acts as an edge detector. It serves to generate contours, as demonstrated on face images. Furthermore, it allows for defining a convolutional Shape Filter.","Density estimate, Quantum clustering, Mean-shift clustering, Maximum entropy, Image contour extraction",Lior Deutsch and David Horn,https://www.sciencedirect.com/science/article/pii/S0031320318301249,https://doi.org/10.1016/j.patcog.2018.03.034,0031-3203,2018,190--199,81,Pattern Recognition,The Weight-Shape decomposition of density estimates: A framework for clustering and image analysis algorithms,article,DEUTSCH2018190,
"Understanding of indoor scenes has considerable value in mission planning and monitoring in robots. This has become one of the biggest challenges in computer vision because of the diversity and changeability of 3D indoor scenes. Indoor scenes can be considered compositions of many planes in which most common external surfaces are rectangles, such as doors, windows, walls, tables. These spatial rectangles are projected into 2D projections with special geometric configurations, which may enable us to estimate their original orientation and position in 3D scenes. In this paper, the study presents a method to efficiently reconstruct 3D indoor scene without any knowledge of cameraâs internal calibration. The approach first found quadrangles composed of lines. Through the projection of spatial rectangles, our method not only can estimate room layout of scene, but also can reconstruct excellent details of scene. Due to simple geometric inferences, our method can cope with clutter without prior training, making it practical and efficient for a navigating robot. We compare the room layout estimated by our algorithm against room box ground truth, measuring the percentage of pixels that were classified correctly. Furthermore, we evaluate our ability to fit the indoor scene by comparing against the details that were reconstructed correctly in scene. The experiments showed that our method is capable of reconstructing various structures of indoor environments and that the accuracy and speed of this method meet the requirements a of indoor robot navigation.","3D reconstruction, Quadrangle, Projected rectangle, Vanishing point, Spatial rectangle",Hui Wei and Luping Wang,https://www.sciencedirect.com/science/article/pii/S0031320318301523,https://doi.org/10.1016/j.patcog.2018.04.017,0031-3203,2018,497--514,81,Pattern Recognition,Understanding of indoor scenes based on projection of spatial rectangles,article,WEI2018497,
"Level set method (LSM) is popular in image segmentation due to its intrinsic features for handling complex shapes and topological changes. Existing LSM-based segmentation models can be generally grouped into region- and edge-based models. The former often have problems to deal with images whose objects have similar color intensity to that of the background when the region descriptor is insufficient. The latter usually suffer to boundary leakage problem when the imagesâ edges are weak. To overcome these problems, we present a novel hierarchical level set evolution protocol (SDREL), wherein we propose to use both saliency map and color intensity as region external energy to motivate an initial evolution of level set function (LSF), followed by the LSF and further smoothed by an internal energy (regulation term) to recognize a more precise boundary positioning. Our results show that the newly introduced saliency map term improves extracting objects from complex background and the asynchronous evolution of a single LSF results in a better segmentation. The new hierarchical SDREL model has been evaluated extensively and the results indicate that it has the merits of flexible initialization, robust evolution, and fast convergence. SDREL is available at: www.csbio.sjtu.edu.cn/bioinf/SDREL/.","Image segmentation, Level set evolution, Saliency map, Edge, SDREL",Xu-Hao Zhi and Hong-Bin Shen,https://www.sciencedirect.com/science/article/pii/S0031320318301006,https://doi.org/10.1016/j.patcog.2018.03.010,0031-3203,2018,241--255,80,Pattern Recognition,Saliency driven region-edge-based top down level set evolution reveals the asynchronous focus in image segmentation,article,ZHI2018241,
"Understanding human behavior using computer vision techniques for recognizing body posture, gait, hand gesture, and facial expressions has recently witnessed significant research activity. Emotions/affect have a direct correlation with the mental state, as well as intention of a person, based on which his/her present and future states can be understood and predicted. As a case study in this work, we demonstrate the utility of deep learning in understanding videos of Indian classical dance (ICD) forms. ICD comprises hand gestures, body poses and facial expressions enacted by the performer along with the accompanying music and songs/Shlokas. In this work we attempt to decipher the meaning of Navarasas associated with Indian classical dance (ICD). Recognizing these emotions from images/videos of ICD is a challenge due to factors such as ambiguity in the enactment, costume, make-up, clutter, etc. Here, we propose a dataset of various emotions (Navarasas) enacted in ICD comprising RGB images along with associated depth information collected using the Microsoft Kinect sensor. We propose a deep learning framework using convolutional neural networks to understand the semantic meaning associated with videos of ICD by recognizing Navarasas enacted by the performer.","Deep learning, Convolutional neural network, Facial expression recognition, Emotion recognition",Aparna Mohanty and Rajiv R. Sahay,https://www.sciencedirect.com/science/article/pii/S003132031830030X,https://doi.org/10.1016/j.patcog.2018.01.035,0031-3203,2018,97--113,79,Pattern Recognition,Rasabodha: Understanding Indian classical dance by recognizing emotions using deep learning,article,MOHANTY201897,
"Model-based 3D gaze estimation represents a dominant technique for eye gaze estimation. It allows free head movement and gives good estimation accuracy. But it requires a personal calibration, which may significantly limit its practical utility. Various techniques have been proposed to replace intrusive and subject-unfriendly calibration methods. In this paper, we introduce a new implicit calibration method that takes advantage of four natural constraints during eye gaze tracking. The first constraint is based on two complementary gaze estimation methods. The underlying assumption is that different gaze estimation methods, though based on different principles and mechanisms, ideally predict exactly the same gaze point at the same time. The second constraint is inspired by the well-known center prior principle, it is assumed that most fixations are concentrated on the center of the screen with natural viewing scenarios. The third constraint arises from the fact that for console based eye tracking, humanâs attention/gaze are always within the screen region. The final constraint comes from eye anatomy, where the value of eye parameters must be within certain regions. The four constraints are integrated jointly and help formulate the implicit calibration as a constrained unsupervised regression problem, which can be effectively solved through the proposed iterative hard EM algorithm. Experiments on two everyday interactions Web-browsing and Video-watching demonstrate the effectiveness of the proposed implicit calibration method.","Gaze estimation, Implicit calibration, Natural constraints, Human computer interaction",Kang Wang and Qiang Ji,https://www.sciencedirect.com/science/article/pii/S0031320318300438,https://doi.org/10.1016/j.patcog.2018.01.031,0031-3203,2018,216--227,79,Pattern Recognition,3D gaze estimation without explicit personal calibration,article,WANG2018216,
"Feature selection remains a popular method for quantity reduction of attributes of high-dimensional data, to reduce computational costs in classifications. A new feature selection method based on the joint maximal information entropy between features and class (FS-JMIE) is proposed in this paper. Firstly, the joint maximal information entropy (JMIE) is defined to measure a feature subset. Next, a binary particle swarm optimization (BPSO) algorithm is introduced to search the optimal feature subset. Finally, classification is performed on UCI corpora to verify the performance of our proposed method compared to the traditional mutual information (MI) method, CHI method, as well as a binary version of particle swarm optimization-support vector machines (BPSO-SVMs) feature selection. Experiments show that FS-JMIE achieves an equal or better performance than MI, CHI, and BPSO-SVM. Further, FS-JMIE manifests relatively better robustness to the number of classes. Moreover, the method shows higher consistency and better time-efficiency than BPSO-SVM.","BPSO, Entropy, Feature selection, Maximal information coefficient",Kangfeng Zheng and Xiujuan Wang,https://www.sciencedirect.com/science/article/pii/S0031320317304946,https://doi.org/10.1016/j.patcog.2017.12.008,0031-3203,2018,20--29,77,Pattern Recognition,Feature selection method with joint maximal information entropy between features and class,article,ZHENG201820,
"Hand pose estimation is a hot topic in recent years. It has been widely used in virtual reality since it provides an interface for communication between human and cyberspace. Hand pose estimation is difficult due to some challenges. First, we need to detect human hand which is very changeable. Second, the high degree of freedom leads to difficulties in pose estimation. In this paper, we aim to build a hand pose estimation system which can correctly detect human hand and estimate its pose. We design a model called spherical part model (SPM) and train a deep convolutional neural network using this model. As a result, our network can more accurately estimate hand pose based on prior knowledge of human hand. To demonstrate it, a complete experiment is conducted on two public and one self-build datasets. The results show that our system can outperform other state of the art works.","Hand detection, Hand pose estimation, Deep learning, Convolutional neural network, Spherical part model",Tzu-Yang Chen and Pai-Wen Ting and Min-Yu Wu and Li-Chen Fu,https://www.sciencedirect.com/science/article/pii/S0031320318300839,https://doi.org/10.1016/j.patcog.2018.02.029,0031-3203,2018,1--20,80,Pattern Recognition,Learning a deep network with spherical part model for 3D hand pose estimation,article,CHEN20181,
"Gesture recognition becomes a popular analytics tool for extracting the characteristics of user movement and enables numerous practical applications in the biometrics field. Despite recent advances in this technique, complex user interaction and the limited amount of data pose serious challenges to existing methods. In this paper, we present a novel approach for hand gesture recognition based on user interaction on mobile devices. We have developed two deep models by integrating Bidirectional Long-Short Term Memory (BiLSTM) network and Bidirectional Gated Recurrent Unit (BiGRU) with Fisher criterion, termed as F-BiLSTM and F-BiGRU respectively. These two Fisher discriminative models can classify userâs gesture effectively by analyzing the corresponding acceleration and angular velocity data of hand motion. In addition, we build a large Mobile Gesture Database (MGD) containing 5547 sequences of 12 gestures. With extensive experiments, we demonstrate the superior performance of the proposed method compared to the state-of-the-art BiLSTM and BiGRU on MGD database and two other benchmark databases (i.e., BUAA mobile gesture and SmartWatch gesture). The source code and MGD database will be made publicly available at https://github.com/bczhangbczhang/Fisher-Discriminant-LSTM.","Fisher discriminant, Hand gesture recognition, Mobile devices",Ce Li and Chunyu Xie and Baochang Zhang and Chen Chen and Jungong Han,https://www.sciencedirect.com/science/article/pii/S0031320317305198,https://doi.org/10.1016/j.patcog.2017.12.023,0031-3203,2018,276--288,77,Pattern Recognition,Deep Fisher discriminant learning for mobile hand gesture recognition,article,LI2018276,
"In this paper, we propose a simple but effective method for fast image segmentation. We re-examine the locality-preserving character of spectral clustering by constructing a graph over image regions with both global and local connections. Our novel approach to build graph connections relies on two key observations: 1) local region pairs that co-occur frequently will have a high probability to reside on a common object; 2) spatially distant regions in a common object often exhibit similar visual saliency, which implies their neighborship in a manifold. We present a novel energy function to efficiently conduct graph partitioning. Based on multiple high quality partitions, we show that the generated eigenvector histogram based representation can automatically drive effective unary potentials for a hierarchical random field model to produce multi-class segmentation. Sufficient experiments, on the BSDS500 benchmark, large-scale PASCAL VOC and COCO datasets, demonstrate the competitive segmentation accuracy and significantly improved efficiency of our proposed method compared with other state of the arts.","Image segmentation, Graph partition, Manifold",Zizhao Zhang and Fuyong Xing and Hanzi Wang and Yan Yan and Ying Huang and Xiaoshuang Shi and Lin Yang,https://www.sciencedirect.com/science/article/pii/S0031320318300281,https://doi.org/10.1016/j.patcog.2018.01.037,0031-3203,2018,344--357,78,Pattern Recognition,Revisiting graph construction for fast image segmentation,article,ZHANG2018344,
"Local ternary pattern (LTP) is a simple, yet high-discriminative texture feature model. In order to further promote the performance of LTP in noisy texture classification, this paper proposes a multi-scale median LTP (MLTP) framework. It firstly designs a non-overlapped median sampling scheme to resist against noise, and then re-defines central descriptor, radial descriptor and magnitude descriptor for MLTP. Moreover, it also proposes the pattern mapping solution named rotation-invariant uniform three (riu3) so as to project original MLTP codes from high-dimensional patterns to low-dimensional ones. In our multi-scale algorithm, every image is sampled and encoded separately by three MLTP descriptors, and then original patterns are mapped by a pre-stored riu3 pattern lookup table. Finally, the frequency histogram of joint distribution on three mapped MLTP code images is computed to generate the feature vector of given sampling scale. The different vectors under different scales are concatenated together to produce the objective vectors for training classifier or texture classification. The available classifiers for our algorithm include nearest neighbor classifier (NNC), and support vector machine (SVM), etc. The experiments on five publicly-available texture databases show that the multi-scale MLTP method proposed in this paper could obtain an average accuracy of 99.48% on dataset OTC12, 96.97% on UIUC, 98.71% on KTH-TIPS2b, 98.55% on Brodatz and 97.49% on ALOT. Compared with other state-of-the-art approaches, under the low-intensity conditions of Gaussian, Salt & Pepper and random pixel corruption noise, multi-scale MLTPÂ +Â SVM could still keep higher accuracy and have better noise robustness than others.","Noisy texture classification, Median local ternary pattern, Rotation-invariant uniform-three mapping, Multi-scale joint distribution",Luping Ji and Yan Ren and Xiaorong Pu and Guisong Liu,https://www.sciencedirect.com/science/article/pii/S0031320318300682,https://doi.org/10.1016/j.patcog.2018.02.009,0031-3203,2018,387--401,79,Pattern Recognition,Median local ternary patterns optimized with rotation-invariant uniform-three mapping for noisy texture classification,article,JI2018387,
"To support large-scale visual recognition (i.e., recognizing thousands or even tens of thousands of object classes), a multi-level deep learning algorithm is developed to learn multiple deep networks and a tree classifier jointly, where a concept ontology is constructed to organize large numbers of object classes hierarchically in a coarse-to-fine fashion and determine the inter-related learning tasks automatically. Our multi-level deep learning algorithm can: (a) train multiple deep networks simultaneously to achieve more discriminative representations of both coarse-grained groups and fine-grained object classes at different levels of the concept ontology (i.e., learning multiple sets of deep features simultaneously for different tasks); (b) leverage multi-task learning to train more discriminative classifiers for the fine-grained object classes in the same group to enhance their separability significantly and enable inter-class knowledge transferring; and (c) learn multiple deep networks and the tree classifier jointly in an end-to-end fashion. Our experimental results on three image sets have demonstrated that our multi-level deep learning algorithm can achieve very competitive results on both the accuracy rates and the computational efficiency for large-scale visual recognition.","Large-scale visual recognition, Multi-level deep learning, Multiple deep networks, Concept ontology, Multi-task learning, Tree classifier",Zhenzhong Kuang and Jun Yu and Zongmin Li and Baopeng Zhang and Jianping Fan,https://www.sciencedirect.com/science/article/pii/S0031320318300323,https://doi.org/10.1016/j.patcog.2018.01.027,0031-3203,2018,198--214,78,Pattern Recognition,Integrating multi-level deep learning and concept ontology for large-scale visual recognition,article,KUANG2018198,
"The lack of resolution of imaging systems has critically adverse impacts on the recognition and performance of biometric systems, especially in the case of long range biometrics and surveillance such as face recognition at a distance, iris recognition and gait recognition. Super-resolution, as one of the core innovations in computer vision, has been an attractive but challenging solution to address this problem in both general imaging systems and biometric systems. However, a fundamental difference exists between conventional super-resolution motivations and those required for biometrics. The former aims to enhance the visual clarity of the scene while the latter, more significantly, aims to improve the recognition accuracy of classifiers by exploiting specific characteristics of the observed biometric traits. This paper comprehensively surveys the state-of-the-art super-resolution approaches proposed for four major biometric modalities: face (2D+3D), iris, fingerprint and gait. We approach the super-resolution problem in biometrics from several different perspectives, including from the spatial and frequency domains, single and multiple input images, learning-based and reconstruction-based approaches. Especially, we highlight two special categories: feature-domain super-resolution which performs super-resolution directly on the feature space to purposely improve the recognition performance, and deep-learning super-resolution which discusses the most recent advances in deep learning for the super-resolution task. Finally, we discuss the current and open research challenges and provide recommendations into the future for the improved use of super-resolution with biometrics.","Super-resolution, Biometrics, Face recognition, Iris recognition, Gait recognition, Fingerprint recognition, Non-ideal biometrics, Human identification at a distance, Deep learning",Kien Nguyen and Clinton Fookes and Sridha Sridharan and Massimo Tistarelli and Mark Nixon,https://www.sciencedirect.com/science/article/pii/S0031320318300049,https://doi.org/10.1016/j.patcog.2018.01.002,0031-3203,2018,23--42,78,Pattern Recognition,Super-resolution for biometrics: A comprehensive survey,article,NGUYEN201823,
"This paper proposes a new generalized two dimensional learning approach for particle swarm based feature selection. The core idea of the proposed approach is to include the information about the subset cardinality into the learning framework by extending the dimension of the velocity. The 2D-learning framework retains all the key features of the original PSO, despite the extra learning dimension. Most of the popular variants of PSO can easily be adapted into this 2D learning framework for feature selection problems. The efficacy of the proposed learning approach has been evaluated considering several benchmark data and two induction algorithms: NaiveâBayes and k-Nearest Neighbor. The results of the comparative investigation including the time-complexity analysis with GA, ACO and five other PSO variants illustrate that the proposed 2D learning approach gives feature subset with relatively smaller cardinality and better classification performance with shorter run times.","Classification, Dimensionality reduction, Feature selection, Particle Swarm Optimization, Machine learning",Faizal Hafiz and Akshya Swain and Nitish Patel and Chirag Naik,https://www.sciencedirect.com/science/article/pii/S0031320317304818,https://doi.org/10.1016/j.patcog.2017.11.027,0031-3203,2018,416--433,76,Pattern Recognition,A two-dimensional (2-D) learning framework for Particle Swarm based feature selection,article,HAFIZ2018416,
"Probabilistic models are one of the common approaches for classification. One way to create these models is to form a decomposable model by selecting a set of marginal distributions. Although decomposable models are attractive by having some desirable properties, they would face the over-fitting issue in model-based classification approaches. Considering this issue, we propose a new method for selecting a set of marginal distributions and creating a proper decomposable model while controlling the complexity. The obtained model will be able to capture the interdependencies among different features and can be used for classification. The proposed method is compared with three existing methods namely TAN and Averaged TAN classifiers and t-Cherry algorithm by focusing on internet traffic data. Experimental results show that our obtained model can effectively extract dependencies among features, and hence, its performance as a classifier is superior compared to other three methods.","Decomposable models, Over-fitting, TAN based on ChowâLiu algorithm, Averaged TAN, t-Cherry algorithm, Internet traffic classification",Fatemeh Ghofrani and Alireza Keshavarz-Haddad and Ali Jamshidi,https://www.sciencedirect.com/science/article/pii/S0031320317304958,https://doi.org/10.1016/j.patcog.2017.12.009,0031-3203,2018,1--11,77,Pattern Recognition,A new probabilistic classifier based on decomposable models with application to internet traffic,article,GHOFRANI20181,
"The Smile is one of the most common facial expressions, and it serves as an indicator of the positive emotion. Many feature extraction methods have been proposed for detecting a smile in an unconstrained scene. However, most of the existing feature descriptors are too large and not effective to be applied to distinguish smile and non-smile in the real world. In this paper, we proposed an ELM-based smile detection system by using a novel feature extraction method. Motivated by the observation that the mouth shape can effectively reflect a personâs smile state, a novel and snappy set of features from a few of facial landmarks around the mouth are extracted. We have tested our algorithms on the smile detection database, and the results indicate that our method is better than the state-of-the-art methods with higher accuracy and lower dimension of features.","Smile detection, Pair-wise Distance Vector, Extreme Learning Machine",Dongshun Cui and Guang-Bin Huang and Tianchi Liu,https://www.sciencedirect.com/science/article/pii/S0031320318300724,https://doi.org/10.1016/j.patcog.2018.02.019,0031-3203,2018,356--369,79,Pattern Recognition,ELM based smile detection using Distance Vector,article,CUI2018356,
"Variational methods register images by minimizing functionals that balance measures of smoothness and image similarity. However, the specificity of image-similarity measures can limit the application of variational methods. To address this issue, we propose a new energy functional that generalizes the classical variational method for nonrigid image registration. Our generalization uses Bregman divergences as the similarity measure of the registration functional. These divergences include a number of important similarity measures, such as the Squared Loss distance, the KL divergence, the Logistic Loss, the Mahalanobis distance, the ItakuraâSaito divergence, and the I-divergence. We derive the Euler Lagrange and gradient-flow equations associated with the new functional. By using Bregman divergences, our registration method can combine various types of image characterization as well as spatial and statistical information. Our experiments show that the use of Bregman divergences improves registration quality.","Image registration, Bregman divergence, EulerâLagrange equations",Daniela Portes Leal Ferreira and Eraldo Ribeiro and Celia A. {Zorzo Barcelos},https://www.sciencedirect.com/science/article/pii/S0031320317305009,https://doi.org/10.1016/j.patcog.2017.12.015,0031-3203,2018,237--247,77,Pattern Recognition,A variational approach to non-rigid image registration with Bregman divergences and multiple features,article,FERREIRA2018237,
"3D feature descriptor plays an essential role in 3D computer vision as it is a pre-requisite step for many 3D vision applications. Despite there exists many 3D feature descriptors currently, they are mostly represented in floating representation, resulting costly computation and storage. In this paper, we propose a 3D binary local feature descriptor, Binary Rotational Projection Histogram (BRoPH), aimed at compactness of representation and efficiency of computation. BRoPH is generated directly from point cloud by turning the description of 3D point cloud into a series binarization of 2D image patches. The exploited local reference frame promotes the construction efficiency meanwhile maintains repeatability and stability, the multi-view mechanism and integration of density distribution and depth information employed in BRoPH complement each other and enhance its descriptiveness, and the multi-scale extension of Center-Symmetric Local Binary Patterns (CS-LBP) provides an efficient and compact way to generate binary string. We compare BRoPH against several representative descriptors on public datasets and demonstrate that it achieves about 14 times more compact, 28 and 4 times more faster in terms of describing and matching time respectively, than the average performance of the compared floating descriptors.","Local reference frame, Binary feature descriptor, Object recognition, Multi-view&multi-scale",Yu Zou and Xueqian Wang and Tao Zhang and Bin Liang and Jingyan Song and Houde Liu,https://www.sciencedirect.com/science/article/pii/S0031320317304831,https://doi.org/10.1016/j.patcog.2017.11.029,0031-3203,2018,522--536,76,Pattern Recognition,BRoPH: An efficient and compact binary descriptor for 3D point clouds,article,ZOU2018522,
"Accurate Human Epithelial-2 (HEp-2) cell image classification plays an important role in the diagnosis of many autoimmune diseases and subsequent treatment. One of the key challenges is huge intra-class variations caused by inhomogeneous illumination. To address it, we propose a framework based on very deep supervised residual network (DSRN) to classify HEp-2 cell images. Specifically, we adopt a residual network of 50 layers (ResNet-50) that is substantially deep to extract rich and discriminative features. The deep supervision is imposed on the ResNet-based framework to further boost the classification performance by directly guiding the training of the lower and upper levels of the network. The proposed method is evaluated using two publicly available datasets (i.e., International Conference on Pattern Recognition (ICPR) 2012 and ICPR2016-Task1 cell classification contest datasets). Different from the previous deep learning models learned from scratch, a cross-modal transfer learning strategy is developed. Namely, we pretrain ICPR2012 dataset to fine-tune ICPR2016 dataset based on our DSRN model since both datasets are similar. Extensive experiments show that the proposed method delivers state-of-the-art performance and outperforms the traditional methods based on deep convolutional neural network (DCNN).","HEp-2 cell classification, Residual network, Deeply supervised ResNet, Cross-modal transfer learning",Haijun Lei and Tao Han and Feng Zhou and Zhen Yu and Jing Qin and Ahmed Elazab and Baiying Lei,https://www.sciencedirect.com/science/article/pii/S0031320318300608,https://doi.org/10.1016/j.patcog.2018.02.006,0031-3203,2018,290--302,79,Pattern Recognition,A deeply supervised residual network for HEp-2 cell classification via cross-modal transfer learning,article,LEI2018290,
"Face recognition in harsh environments is an active research topic. As one of the most important challenges, face recognition across pose has received extensive attention. LBP feature has been used widely in face recognition because of its robustness to slight illumination and pose variations. However, due to the way of pattern feature calculation, its effectiveness is limited by the big rotations. In this paper, a new LBP-like feature extraction is proposed which modifies the code rule by Huffman. Besides, a Divide-and-Rule strategy is applied to both face representation and classification, which aims to improve recognition performance across pose. Extensive experiments on CMU PIE database, FERET database and LFW database are conducted to verify the efficacy of the proposed method. The experimental results show that our method significantly outperforms other approaches.","Face recognition across pose, LBP, Huffman, Divide-and-Rule strategy",Li-Fang Zhou and Yue-Wei Du and Wei-Sheng Li and Jian-Xun Mi and Xiao Luan,https://www.sciencedirect.com/science/article/pii/S0031320318300050,https://doi.org/10.1016/j.patcog.2018.01.003,0031-3203,2018,43--55,78,Pattern Recognition,Pose-robust face recognition with Huffman-LBP enhanced by Divide-and-Rule strategy,article,ZHOU201843,
"Exemplar learning of visual similarities in an unsupervised manner is a problem of paramount importance to computer vision. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. In this paper we use weak estimates of local similarities and propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact groups. Learning visual similarities is then framed as a sequence of categorization tasks. The CNN then consolidates transitivity relations within and between groups and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.","Visual similarity learning, Deep learning, Self-supervised learning, Human pose analysis, Object retrieval",Artsiom Sanakoyeu and Miguel A. Bautista and BjÃ¶rn Ommer,https://www.sciencedirect.com/science/article/pii/S0031320318300293,https://doi.org/10.1016/j.patcog.2018.01.036,0031-3203,2018,331--343,78,Pattern Recognition,Deep unsupervised learning of visual similarities,article,SANAKOYEU2018331,
"We propose a novel initialization method designed for active contours (AC) and the level set method (LSM), based on walking particles. The algorithm defines the seeds at converging and diverging configurations of the corresponding vector field. Next, the seeds âexplodeâ, generating a set of walking particles designed to differentiate between the seeds located inside and outside the object. The exploding seeds method (ESM) has been tested against five state-of-the-art initialization methods on 180 ultrasound images from a database collected by Thammasat University Hospital of Thailand. The set of images was additionally partitioned into malignant tumors, fibroadenomas and cysts. The method has been tested for each of those cases using the ground truth hand-drawn by leading radiologists of the hospital. The competing methods were: the trial snake (TS), centers of divergence (CoD), force field segmentation (FFS), Poisson Inverse Gradient Vector Flow (PIG), and quasi-automated initialization (QAI). The numerical tests demonstrated that CoD and FFS failed on the selected test images, whereas the average accuracy of PIG and QAI were lower than that achieved by the proposed method for both AC and the LSM. The LSM combined with the ESM provides the best results.",Ultrasound image segmentation,Annupan Rodtook and Khwunta Kirimasthong and Wanrudee Lohitvisate and Stanislav S. Makhanov,https://www.sciencedirect.com/science/article/pii/S0031320318300451,https://doi.org/10.1016/j.patcog.2018.01.032,0031-3203,2018,172--182,79,Pattern Recognition,Automatic initialization of active contours and level set method in ultrasound images of breast abnormalities,article,RODTOOK2018172,
"The k-means tries to minimize the sum of the squared Euclidean distance from the mean (SSEDM) of each cluster as its objective function. Although this algorithm is effective, it is too sensitive to initial centers. So, many approaches in the literature have focused on determining suitable initial centers. However, selecting suitable initial centers is not always possible, especially when the number of clusters is increased. This paper proposes an iterative approach to improve quality of the solution produced by the k-means. This approach tries to iteratively improve the quality of solution of the k-means by removing one cluster (minus), dividing another one (plus), and applying re-clustering again, in each iteration. This method called iterative k-means minusâplus (I-k-meansâ+). The I-k-meansâ+ is speeded up using some methods to determine which cluster should be removed, which one should be divided, and how to accelerate the re-clustering process. Results of experiments show that I-k-meansâ+ can outperform k-means++, to be known one of the accurate version of the k-means, in terms of minimizing SSEDM. For some instances, the accuracy of I-k-meansâ+ is about 2 times higher than both the k-means and k-means++, while it is faster than k-means++, and has the reasonable runtime, in comparison with the k-means.","-means, Solution improving, Accurate -means, Iterative improvement",Hassan Ismkhan,https://www.sciencedirect.com/science/article/pii/S0031320318300694,https://doi.org/10.1016/j.patcog.2018.02.015,0031-3203,2018,402--413,79,Pattern Recognition,I-k-meansâ+: An iterative clustering algorithm based on an enhanced version of the k-means,article,ISMKHAN2018402,
"This paper proposes a novel descriptor called Maximal Granularity Structure Descriptor (MGSD) for feature representation and an effective metric learning method called Generalized Multi-view Discriminant Analysis based on representation consistency (GMDA-RC) for person re-identification (Re-ID). The proposed descriptor of MGSD captures rich local structural information from overlapping macro-pixels in an image, analyzes the horizontal occurrence of multi-granularity and maximizes the occurrence to extract a robust representation for viewpoint changes. As a result, the proposed descriptor of MGSD can obtain rich person appearance whilst being robust against different condition changes. Besides, considering multi-view information, we present a new GMDA-RC for different views, inspired by the observation that different views share similar data structures. The proposed metric learning method of GMDA-RC seeks multiple discriminant common spaces for multiple views by jointly learning multiple view-specific linear transforms. Finally, we evaluate the proposed method of (MGSD+GMDA-RC) on three publicly available person Re-ID datasets: VIPeR, CUHK-01 and Wide Area Re-ID dataset (WARD). For the VIPeR and CUHK-01, the experimental results show that our method significantly outperforms the state-of-the-art methods, achieving the rank-1 matching rates of 67.09%, 70.61%, and the improvements of 17.41%, 5.34%, respectively. For the WARD, we consider different pairwise camera views (camera 1â2, camera 1â3, camera 2â3) and our method can achieve the rank-1 matching rates of 64.33%, 59.42%, 70.32%, increasing of 5.68%, 11.04%, 9.06% compared with the state-of-the-art methods, respectively.","Person re-identification, Maximal granularity structure descriptor, Generalized multi-view discriminant analysis, Representation consistency",Cairong Zhao and Xuekuan Wang and Duoqian Miao and Hanli Wang and Weishi Zheng and Yong Xu and David Zhang,https://www.sciencedirect.com/science/article/pii/S003132031830044X,https://doi.org/10.1016/j.patcog.2018.01.033,0031-3203,2018,79--96,79,Pattern Recognition,Maximal granularity structure and generalized multi-view discriminant analysis for person re-identification,article,ZHAO201879,
"Nowadays, there is an increasing demand for machine learning techniques which can deal with problems where the instances are produced as a stream or in real time. In these scenarios, online learning is able to learn a model from data that comes continuously. The adaptability, efficiency and scalability of online learning techniques have been gaining interest last years with the increasing amount of data generated every day. In this paper, we propose a novel binary classification approach based on nonlinear mapping functions under an online learning framework. The non-convex optimization problem that arises is split into three different convex problems that are solved by means of Passive-Aggressive Online Learning. We evaluate both the adaptability and generalization of our model through several experiments comparing with the state of the art techniques. We improve significantly the results in several datasets widely used previously by the online learning community.","Online learning, Nonlinear functions, Passive-Aggressive, Binary classification, Nonlinear embedding",Javier Jorge and Roberto Paredes,https://www.sciencedirect.com/science/article/pii/S0031320318300335,https://doi.org/10.1016/j.patcog.2018.01.019,0031-3203,2018,162--171,79,Pattern Recognition,Passive-Aggressive online learning with nonlinear embeddings,article,JORGE2018162,
"Feature selection plays a critical role in pattern recognition. Feature selection aims to eliminate irrelevant and redundant features. A drawback of traditional feature selection methods is that they ignore the dynamic change of selected features with the class. To address this problem, we develop a novel linear feature selection method, namely, Dynamic Change of Selected Feature with the class (DCSF). In DCSF, we introduce a new term: the conditional mutual information between the selected features and the class when a candidate feature is considered. In addition, we replace the traditional feature relevancy term with a term that is based on conditional mutual information. To evaluate our method, we compare DCSF with five traditional methods and two state-of-the-art methods on 20 benchmark data sets. Experimental results show that DCSF outperforms seven other methods in terms of average classification accuracy and highest classification accuracy.","Feature selection, Information theory, Dynamic change, Classification",Wanfu Gao and Liang Hu and Ping Zhang,https://www.sciencedirect.com/science/article/pii/S0031320318300736,https://doi.org/10.1016/j.patcog.2018.02.020,0031-3203,2018,328--339,79,Pattern Recognition,Class-specific mutual information variation for feature selection,article,GAO2018328,
"Image priors based on total variation (TV) and nonlocal patch similarity have shown to be powerful techniques for the reconstruction of magnetic resonance (MR) images from undersampled k-space measurements. However, due to the uniform regularization of gradients, standard TV approaches often over-smooth edges in the image, resulting in the loss of important details. This paper proposes a novel compressed sensing method which combines both external and internal information for the high-performance reconstruction of MRI data. A probabilistic atlas is used to model the spatial distribution of gradients that correspond to various anatomical structures in the image. This atlas is then employed to control the level of gradient regularization at each image location, within a weighted TV regularization prior. The proposed method also leverages the redundancy of nonlocal similar patches through a sparse representation model. Experiments on T1-weighted images from the ABIDE dataset show the proposed method to outperform state-of-the-art approaches, for different sampling rates and noise levels.","Multi-subject MRI, Weighted TV, Sparse representation, ADMM",Mingli Zhang and Christian Desrosiers and Caiming Zhang,https://www.sciencedirect.com/science/article/pii/S003132031730479X,https://doi.org/10.1016/j.patcog.2017.11.025,0031-3203,2018,549--559,76,Pattern Recognition,Atlas-based reconstruction of high performance brain MR data,article,ZHANG2018549,
"Similarity-based approaches represent a promising direction for time series analysis. However, many such methods rely on parameter tuning, and some have shortcomings if the time series are multivariate (MTS), due to dependencies between attributes, or the time series contain missing data. In this paper, we address these challenges within the powerful context of kernel methods by proposing the robust time series cluster kernel (TCK). The approach taken leverages the missing data handling properties of Gaussian mixture models (GMM) augmented with informative prior distributions. An ensemble learning approach is exploited to ensure robustness to parameters by combining the clustering results of many GMM to form the final kernel. We evaluate the TCK on synthetic and real data and compare to other state-of-the-art techniques. The experimental results demonstrate that the TCK is robust to parameter choices, provides competitive results for MTS without missing data and outstanding results for missing data.","Multivariate time series, Similarity measures, Kernel methods, Missing data, Gaussian mixture models, Ensemble learning",Karl Ãyvind Mikalsen and Filippo Maria Bianchi and Cristina Soguero-Ruiz and Robert Jenssen,https://www.sciencedirect.com/science/article/pii/S0031320317304843,https://doi.org/10.1016/j.patcog.2017.11.030,0031-3203,2018,569--581,76,Pattern Recognition,Time series cluster kernel for learning similarities between multivariate time series with missing data,article,MIKALSEN2018569,
"The expectation & maximization (EM) for Gaussian mixtures is popular as a clustering algorithm. However, the EM algorithm is sensitive to initial values, and so Ueda and Nakano [4] proposed the deterministic annealing EM (DA-EM) algorithm to improve it. In this paper, we investigate theoretical behaviors of the EM and DA-EM algorithms. We first derive a general Jacobian matrix of the DA-EM algorithm with respect to posterior probabilities. We then propose a theoretical lower bound for initialization of the annealing parameter in the DA-EM algorithm. On the other hand, some researches mentioned that the EM algorithm exhibits a self-annealing behavior, that is, the equal posterior probability with small random perturbations can avoid the EM algorithm to output the mass center for Gaussian mixtures. However, there is no theoretical analysis on this self-annealing property. Since the DA-EM will become the EM when the annealing parameter is 1, according to the Jacobian matrix of the DA-EM, we can prove the self-annealing property of the EM algorithm for Gaussian mixtures. Based on these results, we give not only convergence behaviors of the equal posterior probabilities and initialization lower bound of the temperature parameter of the DA-EM, but also a theoretical explanation why the EM algorithm for Gaussian mixtures exhibits a self-annealing behavior.","Expectation & maximization (EM) algorithm, Deterministic annealing EM (DA-EM), GAUSSIAN mixtures, Self-annealing, Convergence, Parameter selection",Jian Yu and Chaomu Chaomurilige and Miin-Shen Yang,https://www.sciencedirect.com/science/article/pii/S0031320317305010,https://doi.org/10.1016/j.patcog.2017.12.014,0031-3203,2018,188--203,77,Pattern Recognition,On convergence and parameter selection of the EM and DA-EM algorithms for Gaussian mixtures,article,YU2018188,
"Non-parametric probability density function (pdf) estimation is a general problem encountered in many fields. A promising alternative to the dominating solutions, kernel density estimation (KDE) and Gaussian mixture modeling, is adaptive KDE where kernels are given individual bandwidths adjusted to the local data density. Traditionally the bandwidths are selected by a non-linear transformation of a pilot pdf estimate, containing parameters controlling the scaling, but identifying parameters values yielding competitive performance has turned out to be non-trivial. We present a new self-tuning (parameter free) pdf estimation method called adaptive density estimation by Bayesian averaging (ADEBA) that approximates pdf estimates in the form of weighted model averages across all possible parameter values, weighted by their Bayesian posterior calculated from the data. ADEBA is shown to be simple, robust, competitive in comparison to the current practice, and easily generalize to multivariate distributions. An implementation of the method for R is publicly available.","Adaptive density estimation, Variable bandwidth, Bayesian model averaging, Square root law, Multivariate, Univariate",Christofer L. BÃ¤cklin and Claes Andersson and Mats G. Gustafsson,https://www.sciencedirect.com/science/article/pii/S0031320318300062,https://doi.org/10.1016/j.patcog.2018.01.008,0031-3203,2018,133--143,78,Pattern Recognition,Self-tuning density estimation based on Bayesian averaging of adaptive kernel density estimations yields state-of-the-art performance,article,BACKLIN2018133,
"Handwritten word recognition and spotting of low-resource scripts are difficult as sufficient training data is not available and it is often expensive for collecting data of such scripts. This paper presents a novel cross language platform for handwritten word recognition and spotting for such low-resource scripts where training is performed with a sufficiently large dataset of an available script (considered as source script) and testing is done on other scripts (considered as target script). Training with one source script and testing with another script to have a reasonable result is not easy in handwriting domain due to the complex nature of handwriting variability among scripts. Also it is difficult in mapping between source and target characters when they appear in cursive word images. The proposed Indic cross language framework exploits a large resource of dataset for training and uses it for recognizing and spotting text of other target scripts where sufficient amount of training data is not available. Since, Indic scripts are mostly written in 3 zones, namely, upper, middle and lower, we employ zone-wise character (or component) mapping for efficient learning purpose. The performance of our cross-language framework depends on the extent of similarity between the source and target scripts. Hence, we devise an entropy based script similarity score using source to target character mapping that will provide a feasibility of cross language transcription. We have tested our approach in three Indic scripts, namely, Bangla, Devanagari and Gurumukhi, and the corresponding results are reported.","Indic script recognition, Handwritten word recognition, Word spotting, Cross language recognition, Script similarity, Hidden Markov model",Ayan Kumar Bhunia and Partha Pratim Roy and Akash Mohta and Umapada Pal,https://www.sciencedirect.com/science/article/pii/S0031320318300463,https://doi.org/10.1016/j.patcog.2018.01.034,0031-3203,2018,12--31,79,Pattern Recognition,Cross-language framework for word recognition and spotting of Indic scripts,article,BHUNIA201812,
"Continuous hand gesture recognition is an important area of HCI and challenged by various writing habits and unconstrained hand movement. In this paper, we propose a Structured Dynamic Time Warping (SDTW) approach for continuous hand trajectory recognition. We first propose an automatic continuous trajectory segmentation approach which combines templates and velocity information to spot the beginning and ending points in hand gesture trajectories. Then we assign different weights to feature sequences based on the structured information, from the positions of corner points in the arbitrary trajectories. Finally, we evaluate the SDTW on the Continuous Letter Trajectory (CLT) database. Experimental results show that the proposed approach is robust to the diversity of same handwritten letter, and significantly outperforms state-of-the-art approaches.","Continuous hand gesture recognition, Dynamic time warping, Continuous trajectory segment, Human computer interaction",Jingren Tang and Hong Cheng and Yang Zhao and Hongliang Guo,https://www.sciencedirect.com/science/article/pii/S0031320318300621,https://doi.org/10.1016/j.patcog.2018.02.011,0031-3203,2018,21--31,80,Pattern Recognition,Structured dynamic time warping for continuous hand trajectory gesture recognition,article,TANG201821,
"3D face recognition is an increasing popular modality for biometric authentication, for example in the iPhoneX. Landmarking plays a significant role in region based face recognition algorithms. The accuracy and consistency of the landmarking will directly determine the effectiveness of feature extraction and hence the overall recognition performance. While surface normals have been shown to provide high performing features for face recognition, their use in landmarking has not been widely explored. To this end, a new 3D facial landmarking algorithm based on thresholded surface normal maps is proposed, which is applicable to widely used 3D face databases. The benefits of employing surface normals are demonstrated for both facial roll and yaw rotation calibration and nasal landmarks localization. Results on the Bosphorus, FRGC and BU-3DFE databases show that the detected landmarks possess high within-class consistency and accuracy under different expressions. For several key landmarks the performance achieved surpasses that of state-of-the-art techniques and is also training free and computationally efficient. The use of surface normals therefore provides a useful representation of the 3D surface and the proposed landmarking algorithm provides an effective approach to localising the key nasal landmarks.","3D face landmarking, Surface normals",Jiangning Gao and Adrian N. Evans,https://www.sciencedirect.com/science/article/pii/S0031320318300104,https://doi.org/10.1016/j.patcog.2018.01.011,0031-3203,2018,120--132,78,Pattern Recognition,Expression robust 3D face landmarking using thresholded surface normals,article,GAO2018120,
"Dealing with large images is an on-going challenge in image segmentation, where many of the current methods run into computational and/or memory complexity issues. This work presents a novel decoupled sub-graph compression (DSC) approach for efficient and scalable image segmentation. In DSC, the image is modeled as a region graph, which is then decoupled into small sub-graphs. The sub-graphs undergo a compression process, which simplifies the graph, reducing the number of vertices and edges, while keeping the overall graph structure. Finally, the compressed sub-graphs are re-coupled and re-compressed to form a final compressed graph representing the final image segmentation. Experimental results based on a dataset of high resolution images (1000â¯Ãâ¯1500) show that the DSC method achieves better segmentation performance when compared to state-of-the-art segmentation methods (PRI=0.84 and F=0.61), while having significantly lower computational and memory complexity.","Segmentation, Graph compression, Decoupling, Scalability",R.S. Medeiros and A. Wong and J. Scharcanski,https://www.sciencedirect.com/science/article/pii/S003132031730482X,https://doi.org/10.1016/j.patcog.2017.11.028,0031-3203,2018,228--241,78,Pattern Recognition,Scalable image segmentation via decoupled sub-graph compression,article,MEDEIROS2018228,
"This paper presents an image segmentation method imitating human focusing visual attention in image interpretation using possibilistic knowledge modeling concepts. The proposed pixel level method consists on the Iterative Possibilistic Knowledge Diffusion (IPKD) on immediate neighbourhood pixels. The advantage of this mechanism is to provide iterative diffusion of per-pixel certain knowledge to surrounding pixels in order to progressively refine the segmentation process. The diffusion process is achieved using image smoothing techniques such as Nagao and Gabor filtering, mean filtering and anisotropic diffusion. Those diffusion techniques are then compared in the possibilistic knowledge representation space. The merit of a possibilistic knowledge representation, rather than a grey-level sensor based representation, is demonstrated by both experimental and synthetic data. Producing the lowest error rates, possibilistic knowledge diffusion using Nagao filter is adopted for the approach assessment. Experimental results using synthetic images as well as mammographic images from MIAS (Mammographic Image Analysis Society) data-base, are performed in order to assess the efficiency of the proposed segmentation method according to the visual criterion as well as some quantitative criteria. IPKD's performance (in terms of recognition rate, 94.37% and global predictive rate, 92.18%) is compared with three relevant reference methods: level-set, Fuzzy C-Mean and region growing methods. The IPKD approach outperforms the other three methods, respectively, at the recognition rates of 89.77%, 84.43% and 88.11% and at the global predictive rates of 87.86%, 89.72% and 84.04%. Noise-sensitivity experiments have been conducted on synthetic as well as on real images. The proposed IPKD approach outperforms the three reference methods and in addition, exhibits a desired stability behaviour.","Possibilistic knowledge representation, Knowledge diffusion modeling, Iterative segmentation, Region growing, Image segmentation, Mammographic medical images",I. {Khanfir Kallel} and S. Almouahed and B. Solaiman and Ã. BossÃ©,https://www.sciencedirect.com/science/article/pii/S0031320318300347,https://doi.org/10.1016/j.patcog.2018.01.024,0031-3203,2018,182--197,78,Pattern Recognition,An iterative possibilistic knowledge diffusion approach for blind medical image segmentation,article,KHANFIRKALLEL2018182,
"We present BAdaCost, a multi-class cost-sensitive classification algorithm. It combines a set of cost-sensitive multi-class weak learners to obtain a strong classification rule within the Boosting framework. To derive the algorithm we introduce CMEL, a Cost-sensitive Multi-class Exponential Loss that generalizes the losses optimized in various classification algorithms such as AdaBoost, SAMME, Cost-sensitive AdaBoost and PIBoost. Hence unifying them under a common theoretical framework. In the experiments performed we prove that BAdaCost achieves significant gains in performance when compared to previous multi-class cost-sensitive approaches. The advantages of the proposed algorithm in asymmetric multi-class classification are also evaluated in practical multi-view face and car detection problems.","Boosting, Multi-class classification, Cost-sensitive classification, Multi-view object detection",Antonio FernÃ¡ndez-Baldera and JosÃ© M. Buenaposada and Luis Baumela,https://www.sciencedirect.com/science/article/pii/S0031320318300748,https://doi.org/10.1016/j.patcog.2018.02.022,0031-3203,2018,467--479,79,Pattern Recognition,BAdaCost: Multi-class Boosting with Costs,article,FERNANDEZBALDERA2018467,
"In this paper, we present a new cross-modal discrete hashing (CMDH) approach to learn compact binary codes for cross-modal multimedia search. Unlike most existing cross-modal hashing methods which usually relax the optimization objective function to obtain hash codes, we develop a discrete optimization framework to jointly learn binary codes and a series of hash functions for each modality, so that the performance drop due to the inferior optimization techniques can be avoided. Specifically, we present two cross-modal hashing algorithms called CMDH-linear and CMDH-kernel under the proposed framework, which performs linear and non-linear mappings to learn binary codes, respectively. Different from existing cross-modal hashing methods which maximize the corrections of hash codes from different modalities, our CMDH learns a set of shared binary codes for samples captured from different modalities, so that the modality gap can be effectively removed in cross-modal multimedia retrieval. To further improve the flexibility of our approach for different scenarios, we extend CMDH to unsupervised CMDH (unCMDH) and discrete multi-modal hashing (MMDH), which learns hash codes for training data without label information and with multi-modal labelled data. Experimental results on three benchmark datasets clearly show that our methods achieve competitive results with the state-of-the-arts.","Multimedia retrieval, Hashing, Cross-modal, Discrete optimization",Venice Erin Liong and Jiwen Lu and Yap-Peng Tan,https://www.sciencedirect.com/science/article/pii/S0031320318300505,https://doi.org/10.1016/j.patcog.2018.02.002,0031-3203,2018,114--129,79,Pattern Recognition,Cross-Modal Discrete Hashing,article,LIONG2018114,
"Feature extraction is a key algorithm to solve the dimensionality problem. Most feature extraction algorithms use a batch mode, which requires all data available at the same time to calculate new features. Recently, with more available data and advancements of transmission technology, the need for incremental algorithms has increased. In this paper, we propose a gradient descent DBFE method (GDDBFE) that shows a substantial improvement in processing time. Based on this GDDBFE, we then propose an incremental gradient descent decision boundary feature extraction method (IGDDBFE). The proposed IGDDBFE method consists of two steps: updating the decision boundaries and adding discriminately informative features with newly added samples and then updating the feature vectors by incremental eigenvector updates. Experiments with real-world databases show that the proposed method shows improved performance compared to some existing methods.","Dimensionality reduction, Incremental learning, Neural networks, Decision boundary feature extraction",Seongyoun Woo and Chulhee Lee,https://www.sciencedirect.com/science/article/pii/S003132031730496X,https://doi.org/10.1016/j.patcog.2017.12.010,0031-3203,2018,65--74,77,Pattern Recognition,Incremental feature extraction based on decision boundaries,article,WOO201865,
"The state of classifier incongruence in decision making systems incorporating multiple classifiers is often an indicator of anomaly caused by an unexpected observation or an unusual situation. Its assessment is important as one of the key mechanisms for domain anomaly detection. In this paper, we investigate the sensitivity of Delta divergence, a novel measure of classifier incongruence, to estimation errors. Statistical properties of Delta divergence are analysed both theoretically and experimentally. The results of the analysis provide guidelines on the selection of threshold for classifier incongruence detection based on this measure.","Anomaly detection, Classifier decision incongruence, Bayesian surprise",Josef Kittler and Cemre Zor and Ioannis Kaloskampis and Yulia Hicks and Wenwu Wang,https://www.sciencedirect.com/science/article/pii/S0031320317304855,https://doi.org/10.1016/j.patcog.2017.11.031,0031-3203,2018,30--44,77,Pattern Recognition,Error sensitivity analysis of Delta divergence - a novel measure for classifier incongruence detection,article,KITTLER201830,
"Multiple-instance learning (MIL) is a variation of supervised learning, where samples are represented by labeled bags, each containing sets of instances. The individual labels of the instances within a bag are unknown, and labels are assigned based on a multi-instance assumption. One of the major complexities associated with this type of learning is the ambiguous relationship between a bagâs label and the instances it contains. This paper proposes a novel support vector machine (SVM) multiple-instance formulation and presents an algorithm with a bag-representative selector that trains the SVM based on bag-level information, named MIRSVM. The contribution is able to identify instances that highly impact classification, i.e. bag-representatives, for both positive and negative bags, while finding the optimal class separation hyperplane. Unlike other multi-instance SVM methods, this approach eliminates possible class imbalance issues by allowing both positive and negative bags to have at most one representative, which constitute as the most contributing instances to the model. The experimental study evaluates and compares the performance of this proposal against 11 state-of-the-art multi-instance methods over 15 datasets, and the results are validated through non-parametric statistical analysis. The results indicate that bag-based learners outperform the instance-based and wrapper methods, as well as MIRSVMâs overall superior performance against other multi-instance SVM models, having an average accuracy of 82.6%, which is 2.5% better than the best performing state-of-the-art MI classifier.","Machine learning, Multiple-instance learning, Support vector machines, Bag-level multi-instance classification, Bag-representative selection",Gabriella Melki and Alberto Cano and SebastiÃ¡n Ventura,https://www.sciencedirect.com/science/article/pii/S003132031830061X,https://doi.org/10.1016/j.patcog.2018.02.007,0031-3203,2018,228--241,79,Pattern Recognition,MIRSVM: Multi-instance support vector machine with bag representatives,article,MELKI2018228,
"This paper suggests a new methodology for patterning writing style evolution using dynamic similarity. We divide a text into sequential, disjoint portions (chunks) of the same size and exploit the Mean Dependence measure, aspiring to model the writing process via association between the current text chunk and its predecessors. To expose the evolution of a style, a new two-step clustering procedure is applied. In the first phase, a distance based on the Mean Dependence between each pair of chunks is evaluated. All document chunks in a pair are embedded in a high dimensional space using a Kuratowski-type embedding procedure and clustered by means of the introduced distance. In the next phase, the rows of the binary cluster classification documents matrix are clustered via the hierarchical single linkage clustering algorithm. By this way, a visualization of the inner stylistic structure of a textsâ collection, the resulting classification tree, is provided by the appropriate dendrogram. The approach applied to studying writing style evolution in the âFoundation Universeâ by Isaac Asimov, the âRamaâ series by Arthur C. Clarke, the âForsyte Sagaâ of John Galsworthy, âThe Lord of the Ringsâ by John Ronald Reuel Tolkien and a collection of books prescribed to Romain Gary demonstrates that the suggested methodology is capable of identifying style development over time. Additional numerical experiments with author determination and author verification tasks exhibit the high ability of the method to provide accurate solutions.","Patterning, Writing style, Text mining, Dynamics",Konstantin Amelin and Oleg Granichin and Natalia Kizhaeva and Zeev Volkovich,https://www.sciencedirect.com/science/article/pii/S0031320317304971,https://doi.org/10.1016/j.patcog.2017.12.011,0031-3203,2018,45--64,77,Pattern Recognition,Patterning of writing style evolution by means of dynamic similarity,article,AMELIN201845,
"Precise detection and quantification of white matter hyperintensity (WMH) is of great interest in studies of neurological and vascular disorders. In this work, we propose a novel method for automatic WMH segmentation with both supervised and semi-supervised large margin algorithms provided by the framework. The proposed algorithms optimize a kernel based max-margin objective function which aims to maximize the margin between inliers and outliers. We show that the semi-supervised learning problem can be formulated to learn a classifier and label assignment simultaneously, which can be solved efficiently by an iterative algorithm. The model is learned first via the supervised approach and then fine-tuned on a target image by using the semi-supervised algorithm. We evaluate our method on 88 brain fluid-attenuated inversion recovery (FLAIR) magnetic resonance (MR) images from subjects with vascular disease. Quantitative evaluation of the proposed approach shows that it outperforms other well known methods for WMH segmentation.","Supervised learning, Semi-supervised learning, Segmentation, White matter hyperintensity, Brain MRI",Chen Qin and Ricardo Guerrero and Christopher Bowles and Liang Chen and David Alexander Dickie and Maria del C. Valdes-Hernandez and Joanna Wardlaw and Daniel Rueckert,https://www.sciencedirect.com/science/article/pii/S0031320317305022,https://doi.org/10.1016/j.patcog.2017.12.016,0031-3203,2018,150--159,77,Pattern Recognition,A large margin algorithm for automated segmentation of white matter hyperintensity,article,QIN2018150,
"Correntropy is a local similarity measure defined in kernel space, hence can combat large outliers in robust signal processing and machine learning. So far, many robust learning algorithms have been developed under the maximum correntropy criterion (MCC), among which, a Gaussian kernel is generally used in correntropy. To further improve the learning performance, in this paper we propose the concept of mixture correntropy, which uses the mixture of two Gaussian functions as the kernel function. Some important properties of the mixture correntropy are presented. Applications of the maximum mixture correntropy criterion (MMCC) to extreme learning machine (ELM) and kernel adaptive filtering (KAF) for function approximation and data regression are also studied. Experimental results show that the learning algorithms under MMCC can perform very well and achieve better performance than the conventional MCC based algorithms as well as several other state-of-the-art algorithms.","Correntropy, Mixture correntropy, Robust learning, Extreme learning machine, Kernel adaptive filtering",Badong Chen and Xin Wang and Na Lu and Shiyuan Wang and Jiuwen Cao and Jing Qin,https://www.sciencedirect.com/science/article/pii/S0031320318300591,https://doi.org/10.1016/j.patcog.2018.02.010,0031-3203,2018,318--327,79,Pattern Recognition,Mixture correntropy for robust learning,article,CHEN2018318,
"Designed as extremely deep architectures, deep residual networks which provide a rich visual representation and offer robust convergence behaviors have recently achieved exceptional performance in numerous computer vision problems. Being directly applied to a scene labeling problem, however, they were limited to capture long-range contextual dependence, which is a critical aspect. To address this issue, we propose a novel approach, Contextual Recurrent Residual Networks (CRRN) which is able to simultaneously handle rich visual representation learning and long-range context modeling within a fully end-to-end deep network. Furthermore, our proposed end-to-end CRRN is completely trained from scratch, without using any pre-trained models in contrast to most existing methods usually fine-tuned from the state-of-the-art pre-trained models, e.g. VGG-16, ResNet, etc. The experiments are conducted on four challenging scene labeling datasets, i.e. SiftFlow, CamVid, Stanford background and SUN datasets, and compared against various state-of-the-art scene labeling methods.","Recurrent network, Residual learning, Visual representation, Context modeling, Scene labeling",T. Hoang Ngan Le and Chi Nhan Duong and Ligong Han and Khoa Luu and Kha Gia Quach and Marios Savvides,https://www.sciencedirect.com/science/article/pii/S0031320318300074,https://doi.org/10.1016/j.patcog.2018.01.005,0031-3203,2018,32--41,80,Pattern Recognition,Deep contextual recurrent residual networks for scene labeling,article,LE201832,
"The most successful video-based human action recognition methods rely on feature representations extracted using Convolutional Neural Networks (CNNs). Inspired by the two-stream network (TS-Net), we propose a multi-stream Convolutional Neural Network (CNN) architecture to recognize human actions. We additionally consider human-related regions that contain the most informative features. First, by improving foreground detection, the region of interest corresponding to the appearance and the motion of an actor can be detected robustly under realistic circumstances. Based on the entire detected human body, we construct one appearance and one motion stream. In addition, we select a secondary region that contains the major moving part of an actor based on motion saliency. By combining the traditional streams with the novel human-related streams, we introduce a human-related multi-stream CNN (HR-MSCNN) architecture that encodes appearance, motion, and the captured tubes of the human-related regions. Comparative evaluation on the JHMDB, HMDB51, UCF Sports and UCF101 datasets demonstrates that the streams contain features that complement each other. The proposed multi-stream architecture achieves state-of-the-art results on these four datasets.","Convolutional Neural Network, Action recognition, Multi-Stream, Motion salient region",Zhigang Tu and Wei Xie and Qianqing Qin and Ronald Poppe and Remco C. Veltkamp and Baoxin Li and Junsong Yuan,https://www.sciencedirect.com/science/article/pii/S0031320318300359,https://doi.org/10.1016/j.patcog.2018.01.020,0031-3203,2018,32--43,79,Pattern Recognition,Multi-stream CNN: Learning representations based on human-related regions for action recognition,article,TU201832,
"In this paper, we simulate the learning way of human to propose a self-learning framework for face clustering. Specifically, we first perform a decorrelation operation on face images through patch-based two-dimensional reconstruction, which has a similar function to the retina. Then we group the semantically similar faces by using a novel self-paced learning model, which is inspired by three major observations: (i) The learning process of human gradually proceeds from easy to complex tasks; (ii) The prior knowledge of human might change with the increase of learned experience; (iii) More prior knowledge usually leads to better prediction accuracy. Experiments on benchmark face databases demonstrate the effectiveness and efficiency of the proposed framework.","Face clustering, Patch-based two-dimensional reconstruction, Self-paced learning",Xiaoshuang Shi and Zhenhua Guo and Fuyong Xing and Jinzheng Cai and Lin Yang,https://www.sciencedirect.com/science/article/pii/S0031320318300633,https://doi.org/10.1016/j.patcog.2018.02.008,0031-3203,2018,279--289,79,Pattern Recognition,Self-learning for face clustering,article,SHI2018279,
"In this paper we further develop the recent concept of multi-component shapes, which is applicable to image processing and image analysis tasks. The domain of multi-component shapes is very diverse and includes shapes that correspond to a group of objects that act together (e.g. a fish shoal), natural components of a segmented object (e.g. cells in embryonic tissues), a set of shapes corresponding to the same object appearing at different times (e.g. human gait in an image sequence), and many more. So far, there are few methods for numerically evaluating multi-component shapes. In this paper we introduce one such method: a disconnectedness measure, that naturally corresponds to multi-component shapes, and has no analogue in single-component shape measures. The new measure depends on the number of shape components, the whole shape but also the shape of its components, on the relative size of the shapeâs components and their mutual position. All these are natural requirements for a âdisconnectednessâ multi-component shape measure. In addition, the new measure is invariant with respect to translation, rotation and scaling transformations. The measure is simple and fast to compute. The disconnectedness measure introduced here is a generic image analysis tool. It has not been developed for a specific application. As such, it can be applied to a variety of applications. Several of them are provided in the paper, as well as synthetic examples that support a better understanding of the behavior of the new measure.","Shape, Multi-component shapes, Moments, Moment invariants",JoviÅ¡a Å½uniÄ and Paul L. Rosin and Vladimir IliÄ,https://www.sciencedirect.com/science/article/pii/S0031320318300116,https://doi.org/10.1016/j.patcog.2018.01.010,0031-3203,2018,91--102,78,Pattern Recognition,Disconnectedness: A new moment invariant for multi-component shapes,article,ZUNIC201891,
"Visual attention is a kind of fundamental cognitive capability that allows human beings to focus on the region of interests (ROIs) under complex natural environments. What kind of ROIs that we pay attention to mainly depends on two distinct types of attentional mechanisms. The bottom-up mechanism can guide our detection of the salient objects and regions by externally driven factors, i.e. color and location, whilst the top-down mechanism controls our biasing attention based on prior knowledge and cognitive strategies being provided by visual cortex. However, how to practically use and fuse both attentional mechanisms for salient object detection has not been sufficiently explored. To the end, we propose in this paper an integrated framework consisting of bottom-up and top-down attention mechanisms that enable attention to be computed at the level of salient objects and/or regions. Within our framework, the model of a bottom-up mechanism is guided by the gestalt-laws of perception. We interpreted gestalt-laws of homogeneity, similarity, proximity and figure and ground in link with color, spatial contrast at the level of regions and objects to produce feature contrast map. The model of top-down mechanism aims to use a formal computational model to describe the background connectivity of the attention and produce the priority map. Integrating both mechanisms and applying to salient object detection, our results have demonstrated that the proposed method consistently outperforms a number of existing unsupervised approaches on five challenging and complicated datasets in terms of higher precision and recall rates, AP (average precision) and AUC (area under curve) values.","Background connectivity, Gestalt laws guided optimization, Image saliency detection, Feature fusion, Human vision perception",Yijun Yan and Jinchang Ren and Genyun Sun and Huimin Zhao and Junwei Han and Xuelong Li and Stephen Marshall and Jin Zhan,https://www.sciencedirect.com/science/article/pii/S0031320318300517,https://doi.org/10.1016/j.patcog.2018.02.004,0031-3203,2018,65--78,79,Pattern Recognition,Unsupervised image saliency detection with Gestalt-laws guided optimization and visual attention based refinement,article,YAN201865,
"Sketch-based Image Retrieval (SBIR) has received a lot of attentions recently. In this paper we aim to enhance SBIR with deep visual semantic descriptor and related optimization mechanisms. Our scheme significantly differs from other earlier work in: 1) A feature representation via deep visual semantic descriptor is established to bridge the gap between sketches and images, which can encode both low-level local features and high-level semantic features; 2) A clustering-based re-ranking optimization is introduced to further improve SBIR by dynamically adjusting the correlations of images in the ranking list. The main contribution of our work is that we effectively apply the deep visual semantic descriptor to enable deep sketch-image matching, which has provided a more reasonable base for us to fuse local low-level visual features with high-level semantic features by determining an optimal correlated mapping. Our experiments on a large number of public data have obtained very positive results.","Sketch-based image retrieval (SBIR), Deep learning, Deep visual semantic descriptor, Sketch-like transformation, Re-ranking optimization, Multiple feature fusion, Accelerated hierarchical K-means clustering",Fei Huang and Cheng Jin and Yuejie Zhang and Kangnian Weng and Tao Zhang and Weiguo Fan,https://www.sciencedirect.com/science/article/pii/S0031320317304867,https://doi.org/10.1016/j.patcog.2017.11.032,0031-3203,2018,537--548,76,Pattern Recognition,Sketch-based image retrieval with deep visual semantic descriptor,article,HUANG2018537,
"Because infrared small target detection plays a crucial role in infrared monitoring and early warning systems, it has been the subject of considerable research. Although many infrared small target detection approaches have been proposed, how to robustly detect small targets in poor quality infrared images remains a challenge. Since existing feature descriptors are often sensitive to the quality of infrared images, this paper advocates the use of a local steering kernel (LSK) to encode the infrared image patch because the LSK method can provide robust estimation of local intrinsic structure, even for poor quality images. Furthermore, this paper proposes a novel local adaptive contrast measure based on LSK reconstruction (LACM-LSK) for infrared small target detection. To demonstrate the effectiveness of the proposed approach, a diverse test dataset, including six infrared image sequences with different backgrounds, was collected. Extensive experiments on the test dataset confirm that the proposed infrared small target detection approach can achieve better detection performance than state-of-the-art approaches.","Infrared small target detection, Local steering kernel (LSK), Closed-form feature reconstruction",Yansheng Li and Yongjun Zhang,https://www.sciencedirect.com/science/article/pii/S0031320317304983,https://doi.org/10.1016/j.patcog.2017.12.012,0031-3203,2018,113--125,77,Pattern Recognition,Robust infrared small target detection using local steering kernel reconstruction,article,LI2018113,
"In medical diagnosis, e.g. bowel cancer detection, a large number of examples of normal cases exists with a much smaller number of positive cases. Such data imbalance usually complicates the learning process, especially for the classes with fewer representative examples, and results in miss detection. In this article, we introduce a regularized ensemble framework of deep learning to address the imbalanced, multi-class learning problems. Our method employs regularization that accommodates multi-class data sets and automatically determines the error bound. The regularization penalizes the classifier when it misclassifies examples that were correctly classified in the previous learning phase. Experiments are conducted using capsule endoscopy videos of bowel cancer symptoms and synthetic data sets with moderate to high imbalance ratios. The results demonstrate the superior performance of our method compared to several state-of-the-art algorithms for imbalanced, multi-class classification problems. More importantly, the sensitivity gain of the minority classes is accompanied by the improvement of the overall accuracy for all classes. With regularization, a diverse group of classifiers is created and the maximum accuracy improvement is at 24.7%. The reduction in computational cost is also noticeable and as the volume of training data increase, the gain of efficiency by our method becomes more significant.","Ensemble, Deep learning, Imbalanced data, Cancer detection",Xiaohui Yuan and Lijun Xie and Mohamed Abouelenien,https://www.sciencedirect.com/science/article/pii/S0031320317305034,https://doi.org/10.1016/j.patcog.2017.12.017,0031-3203,2018,160--172,77,Pattern Recognition,"A regularized ensemble framework of deep learning for cancer detection from multi-class, imbalanced training data",article,YUAN2018160,
"Topic modeling is a powerful approach for modeling data represented as high-dimensional histograms. While the high dimensionality of such input data is extremely beneficial in unsupervised applications including language modeling and text data exploration, it introduces difficulties in cases where class information is available to boost up prediction performance. Feeding such input directly to a classifier suffers from the curse of dimensionality. Performing dimensionality reduction and classification disjointly, on the other hand, cannot enjoy optimal performance due to information loss in the gap between these two steps unaware of each other. Existing supervised topic models introduced as a remedy to such scenarios have thus far incorporated only linear classifiers in order to keep inference tractable, causing a dramatical sacrifice from expressive power. In this paper, we propose the first Bayesian construction to perform topic modeling and non-linear classification jointly. We use the well-known Latent Dirichlet Allocation (LDA) for topic modeling and sparse Gaussian processes for non-linear classification. We combine these two components by a latent variable encoding the empirical topic distribution of each document in the corpus. We achieve a novel variational inference scheme by adapting ideas from the newly emerging deep Gaussian processes into the realm of topic modeling. We demonstrate that our model outperforms other existing approaches such as: (i) disjoint LDA and non-linear classification, (ii) joint LDA and linear classification, (iii) joint non-LDA linear subspace modeling and linear classification, and (iv) non-linear classification without topic modeling, in three benchmark data sets from two real-world applications: text categorization and image tagging.","Latent Dirichlet allocation, Nonparametric Bayesian inference, Gaussian processes, Variational inference, Supervised topic models",Melih Kandemir and Taygun KekeÃ§ and Reyyan Yeniterzi,https://www.sciencedirect.com/science/article/pii/S0031320317305150,https://doi.org/10.1016/j.patcog.2017.12.019,0031-3203,2018,226--236,77,Pattern Recognition,Supervising topic models with Gaussian processes,article,KANDEMIR2018226,
"Automatic activity recognition is an active research topic which aims to identify human activities automatically. A significant challenge is to recognize new activities effectively. In this paper, we propose an effective class incremental learning method, named Class Incremental Random Forests (CIRF), to enable existing activity recognition models to identify new activities. We design a separating axis theorem based splitting strategy to insert internal nodes and adopt Gini index or information gain to split leaves of the decision tree in the random forests (RF). With these two strategies, both inserting new nodes and splitting leaves are allowed in the incremental learning phase. We evaluate our method on three UCI public activity datasets and compare with other state-of-the-art methods. Experimental results show that the proposed incremental learning method converges to the performance of batch learning methods (RF and extremely randomized trees). Compared with other state-of-the-art methods, it is able to recognize new class data continuously with a better performance.","Class incremental learning, Activity recognition, Random forests",Chunyu Hu and Yiqiang Chen and Lisha Hu and Xiaohui Peng,https://www.sciencedirect.com/science/article/pii/S0031320318300360,https://doi.org/10.1016/j.patcog.2018.01.025,0031-3203,2018,277--290,78,Pattern Recognition,A novel random forests based class incremental learning method for activity recognition,article,HU2018277,
"A challenging issue in computerized detection of clustered microcalcifications (MCs) is the frequent occurrence of false positives (FPs) caused by local image patterns that resemble MCs. We develop a context-sensitive deep neural network (DNN), aimed to take into account both the local image features of an MC and its surrounding tissue background, for MC detection. The DNN classifier is trained to automatically extract the relevant image features of an MC as well as its image context. The proposed approach was evaluated on a set of 292 mammograms using free-response receiver operating characteristic (FROC) analysis on the accuracy both in detecting individual MCs and in detecting MC clusters. The results demonstrate that the proposed approach could achieve significantly higher FROC curves when compared to two MC-based detectors. It indicates that incorporating image context information in MC detection can be beneficial for reducing the FPs in detections.","Computer-aided diagnosis (CAD), Clustered microcalcifications (MCs), Deep neural network (DNN), Deep learning",Juan Wang and Yongyi Yang,https://www.sciencedirect.com/science/article/pii/S0031320318300086,https://doi.org/10.1016/j.patcog.2018.01.009,0031-3203,2018,12--22,78,Pattern Recognition,A context-sensitive deep learning approach for microcalcification detection in mammograms,article,WANG201812,
"In the context of large scale similarity search, one promising technique is to encode high dimensional data as compact binary codes to take advantage of the speed and storage efficiencies. Many existing hashing approaches achieve similarity preservation in the Hamming space by preserving similarity relationship between data points. However, most of these methods only consider the relationship between points, which can not capture the data structure comprehensively. In this paper, we propose a reconstruction-based supervised hashing (RSH) method to learn compact binary codes with holistic structure preservation. The proposed method characterizes the similarity structure by the relationship between each data point and the structure generated by the remaining points. The learning objective is set to simultaneously minimize the distance between each point and the structure with the same class label and maximize the distance between each point and the structure with different class labels. In cross-modal retrieval, we propose a reconstruction-based hashing method by distilling the correlation structure in the common latent hamming space. The correlation structure characterizes the semantic correlation by the relationship between data points and structures in the common hamming space. Minimizing the reconstruction error of each single-modal latent model makes hidden layer outputs representative for the input of each modality. Experimental results in both single-modal and cross-modal datasets demonstrate the effectiveness of our methods when compared to several recently proposed approaches.",,Xin Yuan and Zhixiang Chen and Jiwen Lu and Jianjiang Feng and Jie Zhou,https://www.sciencedirect.com/science/article/pii/S0031320318300487,https://doi.org/10.1016/j.patcog.2018.02.003,0031-3203,2018,147--161,79,Pattern Recognition,Reconstruction-based supervised hashing,article,YUAN2018147,
"The performance of conventional interactive image segmentation methods is strongly affected by seed quantity and position, and it is difficult for them to maintain global data coherence due to the bias that is caused by limited interactions. Furthermore, the pixel-level relationships in these methods are too local to capture long-range connectivity cues, which often causes them to obtain under-segmented results. To solve these problems, this paper proposes an interactive segmentation method that is based on likelihood diffusion and perceptual learning. The diffusive likelihood strategy is proposed for accurately estimating the prior label probability from limited user inputs. Superpixel-level grouping cues are utilized to enforce continuity during the segmentation process. The geometrical adjacency and long-range grouping cues are fused in the proposed framework to ensure that the segmentation results maintain proximity and continuity. The final results can be obtained by applying a joint optimization technique to solve a pair of sub-module functions. Experiments on the Berkeley segmentation data set and the Microsoft GrabCut database demonstrate that the proposed method outperforms state-of-the-art methods.","Interactive image segmentation, Likelihood diffusion, Perceptual learning, Graph cuts",Tao Wang and Zexuan Ji and Quansen Sun and Qiang Chen and Qi Ge and Jian Yang,https://www.sciencedirect.com/science/article/pii/S0031320318300761,https://doi.org/10.1016/j.patcog.2018.02.023,0031-3203,2018,440--451,79,Pattern Recognition,Diffusive likelihood for interactive image segmentation,article,WANG2018440,
"Least squares support vector machine (LS-SVM) is a popular hyperplane-based classifier and has attracted many attentions. However, it may suffer from singularity or ill-condition issue for the small sample size (SSS) problem where the sample size is much smaller than the number of features of a data set. Feature selection is an effective way to solve this problem. Motivated by this, in the paper, we propose a sparse Lq-norm least squares support vector machine (Lq-norm LS-SVM) with 0â¯<â¯qâ¯<â¯1, where feature selection and prediction are performed simultaneously. Different from traditional LS-SVM, our Lq-norm LS-SVM minimizes the Lq-norm of weight and releases the least squares problem in primal space, resulting in that feature selection can be achieved effectively and small enough number of features can be selected by adjusting the parameters. Furthermore, our Lq-norm LS-SVM can be solved by an efficient iterative algorithm, which is proved to be convergent to a global optimal solution under some assumptions on the sparsity. The effectiveness of the proposed Lq-norm LS-SVM is validated via theoretical analysis as well as some illustrative numerical experiments.","Least squares support vector machine (LS-SVM), -norm, Feature selection, Sparse approximation, Global optimality",Yuan-Hai Shao and Chun-Na Li and Ming-Zeng Liu and Zhen Wang and Nai-Yang Deng,https://www.sciencedirect.com/science/article/pii/S0031320318300244,https://doi.org/10.1016/j.patcog.2018.01.016,0031-3203,2018,167--181,78,Pattern Recognition,Sparse Lq-norm least squares support vector machine with feature selection,article,SHAO2018167,
"Breast cancer is one of the leading causes of cancer death among women worldwide. In clinical routine, automatic breast ultrasound (BUS) image segmentation is very challenging and essential for cancer diagnosis and treatment planning. Many BUS segmentation approaches have been studied in the last two decades, and have been proved to be effective on private datasets. Currently, the advancement of BUS image segmentation seems to meet its bottleneck. The improvement of the performance is increasingly challenging, and only few new approaches were published in the last several years. It is the time to look at the field by reviewing previous approaches comprehensively and to investigate the future directions. In this paper, we study the basic ideas, theories, pros and cons of the approaches, group them into categories, and extensively review each category in depth by discussing the principles, application issues, and advantages/disadvantages.","Breast ultrasound (BUS) images, Breast cancer, Segmentation, Benchmark, Early detection, Computer-aided diagnosis (CAD)",Min Xian and Yingtao Zhang and H.D. Cheng and Fei Xu and Boyu Zhang and Jianrui Ding,https://www.sciencedirect.com/science/article/pii/S0031320318300645,https://doi.org/10.1016/j.patcog.2018.02.012,0031-3203,2018,340--355,79,Pattern Recognition,Automatic breast ultrasound image segmentation: A survey,article,XIAN2018340,
"Manifold learning aims to discover the low dimensional space where the input high dimensional data are embedded by preserving the geometric structure. Unfortunately, almost all the existing manifold learning methods were proposed under single view scenario, and they cannot be straightforwardly applied to multiple feature sets. Although concatenating multiple views into a single feature provides a plausible solution, it remains a question on how to better explore the independence and interdependence of different views while conducting manifold learning. In this paper, we propose a multi-view manifold learning with locality alignment (MVML-LA) framework to learn a common yet discriminative low-dimensional latent space that contain sufficient information of original inputs. Both supervised algorithm (S-MVML-LA) and unsupervised algorithm (U-MVML-LA) are developed. Experiments on benchmark real-world datasets demonstrate the superiority of our proposed S-MVML-LA and U-MVML-LA over existing state-of-the-art methods.","Manifold learning, Multi-view learning, Locality alignment",Yue Zhao and Xinge You and Shujian Yu and Chang Xu and Wei Yuan and Xiao-Yuan Jing and Taiping Zhang and Dacheng Tao,https://www.sciencedirect.com/science/article/pii/S0031320318300128,https://doi.org/10.1016/j.patcog.2018.01.012,0031-3203,2018,154--166,78,Pattern Recognition,Multi-view manifold learning with locality alignment,article,ZHAO2018154,
"This work proposes a text independent writer identification framework for online handwritten data. We derive a strategy that encodes the sequence of feature vectors extracted at sample points of the temporal trace with descriptors obtained from a codebook. The derived descriptors take into account, the scores of each of the attributes in a feature vector, that are computed with regards of the proximity to their corresponding values in the assigned codevector of the codebook. A codebook comprises a set of codevectors that are pre-learnt by a k-means algorithm applied on feature vectors of handwritten documents pooled from several writers. In addition, for constructing the codebook, we consider features that are derived by incorporating a so called âgap parameterâ that captures characteristics of sample points in the neighborhood of the point under consideration. We formulate our strategy in a way that, for a given codebook size k, we employ the descriptors of only kâ1 codevectors to construct the final descriptor by concatenation. The usefulness of the descriptor is demonstrated by several experiments that are reported on publicly available databases.","Online writer identification, Codebook, Descriptor, Gap parameter,",Vivek Venugopal and Suresh Sundaram,https://www.sciencedirect.com/science/article/pii/S0031320318300402,https://doi.org/10.1016/j.patcog.2018.01.023,0031-3203,2018,318--330,78,Pattern Recognition,An improved online writer identification framework using codebook descriptors,article,VENUGOPAL2018318,
"Offline signature verification has been accepted as a tool for individual authentication. To address the remaining challenges and improve the discriminative power, this study proposes a new feature extraction approach based on a Fisher vector (FV) with fused KAZE features detected from both foreground and background signature images using a recent fusion strategy. Experimental results demonstrate the following: (1) KAZE features from foreground and background signature images show good performance, respectively; (2) fused KAZE features from foreground and background signature images improve performance; (3) adoption of the FV provides a more precise spatial distribution of the characteristics per writer; (4) while an FV with representation-level fusion produces a high-dimensional vector, principal component analysis for the original FV can provide a more dimensionally compact vector without significant performance loss; (5) with the popular MCYT-75 signature dataset, the proposed method yields significantly lower error rates than existing state-of-the-art offline signature verification methods.","Biometrics, Forensics, Signature verification, Fisher vector, KAZE features, Fusion strategy, Support vector machine",Manabu Okawa,https://www.sciencedirect.com/science/article/pii/S0031320318300803,https://doi.org/10.1016/j.patcog.2018.02.027,0031-3203,2018,480--489,79,Pattern Recognition,Synergy of foregroundâbackground images for feature extraction: Offline signature verification using Fisher vector with fused KAZE features,article,OKAWA2018480,
"Medical image fusion is important in image-guided medical diagnostics, treatment, and other computer vision tasks. However, most current approaches assume that the source images are noise-free, which is not usually the case in practice. The performance of traditional fusion methods decreases significantly when images are corrupted with noise. It is therefore necessary to develop a fusion method that accurately preserves detailed information even when images are corrupted. However, suppressing noise and enhancing textural details are difficult to achieve simultaneously. In this paper, we develop a novel medical image fusion, denoising, and enhancement method based on low-rank sparse component decomposition and dictionary learning. Specifically, to improve the discriminative ability of the learned dictionaries, we incorporate low-rank and sparse regularization terms into the dictionary learning model. Furthermore, in the image decomposition model, we impose a weighted nuclear norm and sparse constraint on the sparse component to remove noise and preserve textural details. Finally, the fused result is constructed by combining the fused low-rank and sparse components of the source images. Experimental results demonstrate that the proposed method consistently outperforms existing state-of-the-art methods in terms of both visual and quantitative evaluations.","Medical image fusion, Denoising, Dictionary learning, Low-rank decomposition, Sparse representation",Huafeng Li and Xiaoge He and Dapeng Tao and Yuanyan Tang and Ruxin Wang,https://www.sciencedirect.com/science/article/pii/S0031320318300529,https://doi.org/10.1016/j.patcog.2018.02.005,0031-3203,2018,130--146,79,Pattern Recognition,"Joint medical image fusion, denoising and enhancement via discriminative low-rank sparse dictionaries learning",article,LI2018130,
"Electroencephalograph (EEG), the representation of the brain's electrical activity, is a widely used measure of brain activities such as working memory during cognitive tasks. Varying in complexity of cognitive tasks, mental load results in different EEG recordings. Classification of mental load is one of core issues in studies on working memory. Various machine learning methods have been introduced into this area, achieving competitive performance. Inspired by the recent breakthrough via deep recurrent convolutional neural networks (CNNs) on classifying mental load, we propose improved CNNs methods for this task. Specifically, our frameworks contain both single-model and double-model methods. With the help of our models, spatial, spectral, and temporal information of EEG data is taken into consideration. Meanwhile, a novel fusion strategy for utilizing different networks is introduced in this work. The proposed methods have been compared with state-of-the-art ones on the same EEG database. The comparison results show that both our single-model method and double-model method can achieve comparable or even better performance than the well-performed deep recurrent CNNs. Furthermore, our proposed CNNs models contain less parameters than state-of-the-art ones, making it be more competitive in further practical application.","Deep learning, Mental load classification, CNNs, EEG",Zhicheng Jiao and Xinbo Gao and Ying Wang and Jie Li and Haojun Xu,https://www.sciencedirect.com/science/article/pii/S0031320317304879,https://doi.org/10.1016/j.patcog.2017.12.002,0031-3203,2018,582--595,76,Pattern Recognition,Deep Convolutional Neural Networks for mental load classification based on EEG data,article,JIAO2018582,
We propose several cost functions for registration of shapes encoded with Euclidean and/or non-Euclidean information (unit vectors). Our framework is assessed for estimation of both rigid and non-rigid transformations between the target and model shapes corresponding to 2D contours and 3D surfaces. The experimental results obtained confirm that using the combination of a pointâs position and unit normal vector in a cost function can enhance the registration results compared to state of the art methods.,"Shape registration, Directional information, Von Mises-Fisher,  registration",MairÃ©ad Grogan and Rozenn Dahyot,https://www.sciencedirect.com/science/article/pii/S003132031830075X,https://doi.org/10.1016/j.patcog.2018.02.021,0031-3203,2018,452--466,79,Pattern Recognition,Shape registration with directional data,article,GROGAN2018452,
"A novel framework, for real-time action detection, recognition and evaluation of motion capture data, is presented in this paper. Pose and kinematics information is used for data description. Automatic and dynamic weighting, altering joint data significance based on action involvement, and Kinetic energy-based descriptor sampling are employed for efficient action segmentation and labelling. The automatically segmented and recognized action instances are subsequently fed to the framework action evaluation component, which compares them with the corresponding reference ones, estimating their similarity. Exploiting fuzzy logic, the framework subsequently gives semantic feedback with instructions on performing the actions more accurately. Experimental results on MSR-Action3D and MSRC12 benchmarking datasets and a new, publicly available one, provide evidence that the proposed framework compares favourably to state-of-the-art methods by 0.5â6% in all three datasets, showing that the proposed method can be effectively used for unsupervised gesture/action training.","Online human action detection, Online human action recognition, Motion evaluation, Kinect, Skeleton data, Automatic joint/angle weighting, Kinetic energy",Fotini Patrona and Anargyros Chatzitofis and Dimitrios Zarpalas and Petros Daras,https://www.sciencedirect.com/science/article/pii/S0031320317304910,https://doi.org/10.1016/j.patcog.2017.12.007,0031-3203,2018,612--622,76,Pattern Recognition,"Motion analysis: Action detection, recognition and evaluation based on motion capture data",article,PATRONA2018612,
"The partially observable hidden Markov model is an extension of the hidden Markov Model in which the hidden state is conditioned on an independent Markov chain. This structure is motivated by the presence of discrete metadata, such as an event type, that may partially reveal the hidden state but itself emanates from a separate process. Such a scenario is encountered in keystroke dynamics whereby a userâs typing behavior is dependent on the text that is typed. Under the assumption that the user can be in either an active or passive state of typing, the keyboard key names are event types that partially reveal the hidden state due to the presence of relatively longer time intervals between words and sentences than between letters of a word. Using five public datasets, the proposed model is shown to consistently outperform other anomaly detectors, including the standard HMM, in biometric identification and verification tasks and is generally preferred over the HMM in a Monte Carlo goodness of fit test.","Hidden Markov model, Keystroke biometrics, Behavioral biometrics, Time intervals, Anomaly detection",John V. Monaco and Charles C. Tappert,https://www.sciencedirect.com/science/article/pii/S0031320317304752,https://doi.org/10.1016/j.patcog.2017.11.021,0031-3203,2018,449--462,76,Pattern Recognition,The partially observable hidden Markov model and its application to keystroke dynamics,article,MONACO2018449,
"Recognising human activities in sequential data from sensors is a challenging research area. A significant problem arises from the need to determine fixed sequence partitions (windows) to overcome the inability of a single sample to provide adequate information about an activity; commonly overcome by using a fixed size sliding window over consecutive samples to extract informationâeither handcrafted or learned featuresâand predicting a single label for all the samples in the window. Two key issues arise from this approach: (i) the samples in one window may not always share the same label, a problem more significant for short duration activities such as gestures. We refer to this as the multi-class windows problem. (ii) the inferencing phase is constrained by the window size selected during training while the best window size is difficult to tune in practice. We propose an efficient method for predicting the label of each sample, which we call dense labelling, in a sequence of activity data of arbitrary length based on a fully convolutional network (FCN) design. In particular, our approach overcomes the problems posed by multi-class windows and fixed size sequence partitions imposed during training. Further, our network learns both features and the classifier automatically. We conduct extensive experiments and demonstrate that our proposed approach is able to outperform the state-of-the-arts in terms of sample-based classification and activity-based label misalignment measures on three challenging datasets: Opportunity, Hand Gesture, and our new datasetâan activity dataset we release based on a wearable sensor worn by hospitalised patients.","Human activity recognition, Time series sequence classification, Fully convolutional networks",Rui Yao and Guosheng Lin and Qinfeng Shi and Damith C. Ranasinghe,https://www.sciencedirect.com/science/article/pii/S0031320317305204,https://doi.org/10.1016/j.patcog.2017.12.024,0031-3203,2018,252--266,78,Pattern Recognition,Efficient dense labelling of human activity sequences from wearables using fully convolutional networks,article,YAO2018252,
"In affective computing, stress recognition mainly focuses on the relation of stress and photoelectric information. Researchers have used artificial intelligence to determine stress and computer identification channels. However, in applications such as health and security, Emotional stress (ES) information is usually to be alongside physical stress (PS) information, making it urgent to classify ES and PS. The thermal signals of ES and PS have yet to be classified, for which, signal amplification is offered. In this study, we propose a classification algorithm based on signal amplification and correlation analysis called Eulerian magnification-canonical correlation analysis. This signal amplification algorithm expands the signals of ES and PS in different frequency domains. Sparse coding and canonical correlation analysis then fuse the original signal and its amplified features. The extracted entropy features are used to train the correlation weight between ES and PS, which formulates stress classifications. The new classification method achieves an accuracy rate of 90%. This study can lead to a practical system for the noninvasive assessment of stress states for health or security applications.","Emotional stress, Physical stress, Emotion classification",Kan Hong and Guodong Liu and Wentao Chen and Sheng Hong,https://www.sciencedirect.com/science/article/pii/S0031320317304995,https://doi.org/10.1016/j.patcog.2017.12.013,0031-3203,2018,140--149,77,Pattern Recognition,Classification of the emotional stress and physical stress using signal magnification and canonical correlation analysis,article,HONG2018140,
"A new training algorithm for neural networks in binary classification problems is presented. It is based on the minimization of an estimate of the Bayes risk by using Parzen windows applied to the final one-dimensional nonlinear transformation of the samples to estimate the probability of classification error. This leads to a very general approach to error minimization and training, where the risk that is to be minimized is defined in terms of integrated one-dimensional Parzen windows, and the gradient descent algorithm used to minimize this risk is a function of the window that is used. By relaxing the constraints that are typically applied to Parzen windows when used for probability density function estimation, for example by allowing them to be non-symmetric or possibly infinite in duration, an entirely new set of training algorithms emerge. In particular, different Parzen windows lead to different cost functions, and some interesting relationships with classical training methods are discovered. Experiments with synthetic and real benchmark datasets show that with the appropriate choice of window, fitted to the specific problem, it is possible to improve the performance of neural network classifiers over those that are trained using classical methods.","Bayes risk, Parzen windows, Binary classification",Marcelino LÃ¡zaro and Monson H. Hayes and AnÃ­bal R. Figueiras-Vidal,https://www.sciencedirect.com/science/article/pii/S0031320317305046,https://doi.org/10.1016/j.patcog.2017.12.018,0031-3203,2018,204--215,77,Pattern Recognition,Training neural network classifiers through Bayes risk minimization applying unidimensional Parzen windows,article,LAZARO2018204,
"Human pose detection has been an active research topic, and many studies have been done to address different problems in the topic. However, very few methods are proposed to detect joints in the human body. In this paper, we proposed a novel hybrid framework to detect joints automatically by using depth camera. In the proposed method, joints are categorized into two classes: implicit joints and dominant joints. Implicit joints are the joints on the torso, such as neck and shoulders. Dominant joints include elbows and knees. In the hybrid framework we proposed, a loose skeleton model is used to locate implicit joints, and data-driven method is applied to detect dominant joints. The highlight of the proposed work is that geodesic features of the human body are used to build the skeleton model and detect joints. To evaluate our work, experiments are conducted on the dataset recorded by a Microsoft Kinect and compared with state-of-art methods. The results demonstrate that the proposed work can deliver stable and accurate detection results of joints.","Human pose detection, Joint detection, Human body model, Geodesic features",Longbo Kong and Xiaohui Yuan and Amar Man Maharjan,https://www.sciencedirect.com/science/article/pii/S0031320317305162,https://doi.org/10.1016/j.patcog.2017.12.020,0031-3203,2018,216--225,77,Pattern Recognition,A hybrid framework for automatic joint detection of human poses in depth frames,article,KONG2018216,
"Most existing hashing methods usually focus on constructing hash function only, rather than learning discrete hash codes directly. Therefore the learned hash function in this way may result in the hash function which can-not achieve ideal discrete hash codes. To make the learned hash function for achieving ideal approximated discrete hash codes, in this paper, we proposed a novel supervised discrete discriminant hashing learning method, which can learn discrete hashing codes and hashing function simultaneously. To make the learned discrete hash codes to be optimal for classification, the learned hashing framework aims to learn a robust similarity metric so as to maximize the similarity of the same class discrete hash codes and minimize the similarity of the different class discrete hash codes simultaneously. The discriminant information of the training data can thus be incorporated into the learning framework. Meanwhile, the hash functions are constructed to fit the directly learned binary hash codes. Experimental results clearly demonstrate that the proposed method achieves leading performance compared with the state-of-the-art semi-supervised classification methods.","Supervised hash learning, Discrete hash learning, Discrete hash codes, Discriminant information, Robust similarity metric",Yan Cui and Jielin Jiang and Zhihui Lai and Zuojin Hu and WaiKeung Wong,https://www.sciencedirect.com/science/article/pii/S0031320318300098,https://doi.org/10.1016/j.patcog.2018.01.007,0031-3203,2018,79--90,78,Pattern Recognition,Supervised discrete discriminant hashing for image retrieval,article,CUI201879,
"Recently, multi-label learning has received much attention in the applications of image annotation and classification. However, most existing multi-label learning methods do not consider the consistency of labels, which is important in image annotation, and assume that the complete label assignment for each training image is available. In this paper, we focus on the issue of multi-label learning with missing labels, where only partial labels are available, and propose a new approach, namely SVMMN for image annotation. SVMMN integrates both example smoothness and class smoothness into the criterion function. It not only guarantees the large margin but also minimizes the number of samples that live in the large margin area. To solve SVMMN, we present an effective and efficient approximated iterative algorithm, which has good convergence. Extensive experiments on three widely used benchmark databases in image annotations illustrate that our proposed method achieves better performance than some state-of-the-art multi-label learning methods.","Multi-label learning, Missing labels, SVM, Image annotations",Yang Liu and Kaiwen Wen and Quanxue Gao and Xinbo Gao and Feiping Nie,https://www.sciencedirect.com/science/article/pii/S0031320318300372,https://doi.org/10.1016/j.patcog.2018.01.022,0031-3203,2018,307--317,78,Pattern Recognition,SVM based multi-label learning with missing labels for image annotation,article,LIU2018307,
"A method to writer verification based on handwritten stroke analysis is presented. The proposed descriptors correspond to an estimation of the pressure applied when writing using the grayscale image of the stroke. These descriptors are obtained from individual and simple graphemes, in contrast with the complexity of the handwritten stroke used in the signature processing systems. In addition, a study is presented which suggests that the combination of descriptors of simple characters improves the recognition capacity of the method. The descriptors considered correspond to different accuracy degrees of pressure distribution representation. Specifically, from the simplest representation to a more complex one, the descriptors proposed are as follows: the width of the stroke, the gray level of the grapheme skeleton, the average of the gray levels on the perpendicular line to the skeleton, and the approximation transformation coefficients of the area of the grapheme. The advantage of these descriptors is that they are invariant to scale and rotation. The descriptors performance was assessed using the original images and also reduced versions based on traditional methods such as Principal Component Analysis and Discrete Cosine Transform. For the evaluation, a one-vs-all scheme was considered which is consistent with the problem of identity verification. It was implemented with Support Vector Machine classifiers trained with K-Fold Cross Validation. The efficient search of SVM hyperparameters was performed with the heuristic optimization algorithm Simulated Annealing. The evaluation of individual simple characters gives a high average of hits and the combination of characters even improves the performance, getting closer to 100% of hits in identity verification. Qualitative and quantitative comparison with other methods and descriptors has been also carried out with satisfactory results.","Writer verification, Pseudo-dynamic features, Simple graphemes, Off-line stroke analysis",VerÃ³nica Aubin and Marco Mora and Matilde Santos-PeÃ±as,https://www.sciencedirect.com/science/article/pii/S0031320318300773,https://doi.org/10.1016/j.patcog.2018.02.024,0031-3203,2018,414--426,79,Pattern Recognition,Off-line writer verification based on simple graphemes,article,AUBIN2018414,
"Active learning aims to train a classifier as fast as possible with as few labels as possible. The core element in virtually any active learning strategy is the criterion that measures the usefulness of the unlabeled data based on which new points to be labeled are picked. We propose a novel approach which we refer to as maximizing variance for active learning or MVAL for short. MVAL measures the value of unlabeled instances by evaluating the rate of change of output variables caused by changes in the next sample to be queried and its potential labelling. In a sense, this criterion measures how unstable the classifierâs output is for the unlabeled data points under perturbations of the training data. MVAL maintains, what we refer to as, retraining information matrices to keep track of these output scores and exploits two kinds of variance to measure the informativeness and representativeness, respectively. By fusing these variances, MVAL is able to select the instances which are both informative and representative. We employ our technique both in combination with logistic regression and support vector machines and demonstrate that MVAL achieves state-of-the-art performance in experiments on a large number of standard benchmark datasets.","Active learning, Retraining information matrix, Variance maximization",Yazhou Yang and Marco Loog,https://www.sciencedirect.com/science/article/pii/S0031320318300256,https://doi.org/10.1016/j.patcog.2018.01.017,0031-3203,2018,358--370,78,Pattern Recognition,A variance maximization criterion for active learning,article,YANG2018358,
"We present a novel solution to the problem of detecting common actions in time series of motion capture data and videos. Given two action sequences, our method discovers all pairs of common subsequences, i.e. subsequences that represent the same or similar action. This is achieved in a completely unsupervised manner, i.e., without any prior knowledge of the type of actions, their number and their duration. These common subsequences (commonalities) may be located anywhere in the original sequences, may differ in duration and may be performed under different conditions e.g., by a different actor. The proposed method performs a very efficient graph-based search on the matrix of pairwise distances of frames of the two sequences. This search is supported by an objective function that captures the trade off between the similarity of the common subsequences and their lengths. The proposed method has been evaluated quantitatively on challenging datasets and in comparison to state of the art approaches. The obtained results demonstrate that the proposed method outperforms the state of the art methods both in the quality of the obtained solutions and in computational performance.","Common action detection, Video co-segmentation, Temporal action co-segmentation, Dynamic Time Warping",Costas Panagiotakis and Konstantinos Papoutsakis and Antonis Argyros,https://www.sciencedirect.com/science/article/pii/S0031320318300499,https://doi.org/10.1016/j.patcog.2018.02.001,0031-3203,2018,1--11,79,Pattern Recognition,A graph-based approach for detecting common actions in motion capture data and videos,article,PANAGIOTAKIS20181,
"Recent years have seen a growth in interest in skeleton-based human behavior recognition. Skeleton sequences can be expressed naturally as high-order tensor time series, and in this paper we report on the modeling and analysis of such time series using a linear dynamical system (LDS). Owing to their relative simplicity and efficiency, LDSs are the most common tool used in various disciplines for encoding spatiotemporal time series data. However, conventional LDSs process the latent and observed states at each frame of a video as a column vector, a representation that fails to take into account valuable structural information associated with human action. To correct this, we propose a tensor-based linear dynamical system (tLDS) for modeling tensor observations in time series and employ Tucker decomposition to estimate the parameters of the LDS model as action descriptors. In this manner, an action can be expressed as a subspace corresponding to a point on a Grassmann manifold on which classification can be performed using dictionary learning and sparse coding. Experiments using the MSR Action3D, UCF Kinect, and Northwestern-UCLA Multiview Action3D datasets demonstrate the excellent performance of our proposed method.","Skeleton joints, Action recognition, Subspace learning, Tensor learning, Grassmann manifold",Wenwen Ding and Kai Liu and Evgeny Belyaev and Fei Cheng,https://www.sciencedirect.com/science/article/pii/S0031320317304909,https://doi.org/10.1016/j.patcog.2017.12.004,0031-3203,2018,75--86,77,Pattern Recognition,Tensor-based linear dynamical systems for action recognition from 3D skeletons,article,DING201875,
"This paper presents a novel method to detect pedestrians in the far infrared (FIR) domain at night. In existing infrared data, the brightness is distorted by the contrast of the scene, causing the performance to degrade. The radiometric temperature was introduced to cope with this issue. The temperature was unaffected by the contrast and was more stable than the brightness because of the limitation to the specific thermal range with thermoregulation. The dataset was constructed across each season and four versions of thermal infrared radiometry aggregated channel feature (TIR-ACF) are presented with normalization using the maximum temperature of humans. In these experiments, the proposed method outperformed the brightness-based baseline method by a maximum 11% on the log-averaged miss rate for seasonal variations. As a result, the physical temperature enhanced the performance and helped detect pedestrians that cannot be found using the brightness with a reduction of false positives.","Pedestrian detection, Infrared, Temperature, Radiometry, Thermoregulation, Brightness, Contrast",Taehwan Kim and Sungho Kim,https://www.sciencedirect.com/science/article/pii/S0031320318300414,https://doi.org/10.1016/j.patcog.2018.01.029,0031-3203,2018,44--54,79,Pattern Recognition,Pedestrian detection at night time in FIR domain: Comprehensive study about temperature and brightness and new benchmark,article,KIM201844,
"In this paper, a novel end-to-end system for the fast reconstruction of human actor performances into 3D mesh sequences is proposed, using the input from a small set of consumer-grade RGB-Depth sensors. The proposed framework, by offline pre-reconstructing and employing a deformable actorâs 3D model to constrain the on-line reconstruction process, implicitly tracks the human motion. Handling non-rigid deformation of the 3D surface and applying appropriate texture mapping, it finally produces a dynamic sequence of temporally-coherent textured meshes, enabling realistic Free Viewpoint Video (FVV). Given the noisy input from a small set of low-cost sensors, the focus is on the fast (âquick-postâ), robust and fully-automatic performance reconstruction. Apart from integrating existing ideas into a complete end-to-end system, which is itself a challenging task, several novel technical advances contribute to the speed, robustness and fidelity of the system, including a layered approach for model-based pose tracking, the definition and use of sophisticated energy functions, parallelizable on the GPU, as well as a new texture mapping scheme. The experimental results on a large number of challenging sequences, and comparisons with model-based and model-free approaches, demonstrate the efficiency of the proposed approach.","Dynamic mesh sequences, RGB-D sensors, 3D reconstruction, Articulated motion, Surface deformation, Deformable models, Free Viewpoint",Dimitrios S. Alexiadis and Nikolaos Zioulis and Dimitrios Zarpalas and Petros Daras,https://www.sciencedirect.com/science/article/pii/S0031320318300657,https://doi.org/10.1016/j.patcog.2018.02.013,0031-3203,2018,260--278,79,Pattern Recognition,Fast deformable model-based human performance capture and FVV using consumer-grade RGB-D sensors,article,ALEXIADIS2018260,
"When dealing with semi-supervised scenarios, the Positive and Unlabeled (PU) problem is a special case in which few labeled examples from a single class of interest are received to proceed with the classification of unseen instances, according to their similarities with the known class. In the scope of time series, most of the current studies propose to address this subject using a self-training approach based on the 1-Nearest Neighbor algorithm. In order to compute the most similar instance, they compare features along the time domain using the Euclidean Distance and the Dynamic Time Warping-Delta. Despite time-domain measurements permit the analysis of local series shapes, they disconsider temporal recurrences commonly found in natural phenomena (e.g. population growth, climate studies) and are more sensitive to local noise and fluctuations, leading to poor classification performances as confirmed in this paper. This drawback motivated us to propose the use of the Maximum Diagonal Line of the Cross-Recurrence Quantification Analysis (MDL-CRQA), applied on the time series phase space, as similarity measurement. The phase space is obtained after applying Takens embedding theorem on the series, unfolding temporal relationships and dependencies among data observations. As consequence, by comparing phase spaces rather than the series themselves, we can assess how their trajectories evolve along time, including their periodicities and temporal cycles, as well as decreasing noise influences. Experimental results confirm MDL-CRQA improves classification results for PU time series when compared against the mostly used time-domain similarity measurements.","Time series, Semi-supervised classification, Positive and unlabeled, Self-training, Phase space, Cross-recurrence quantification analysis",Lucas {de Carvalho Pagliosa} and Rodrigo Fernandes {de Mello},https://www.sciencedirect.com/science/article/pii/S0031320318300815,https://doi.org/10.1016/j.patcog.2018.02.030,0031-3203,2018,53--63,80,Pattern Recognition,Semi-supervised time series classification on positive and unlabeled problems using cross-recurrence quantification analysis,article,DECARVALHOPAGLIOSA201853,
"In this work, we address human activity and hand gesture recognition problems using 3D data sequences obtained from full-body and hand skeletons, respectively. To this aim, we propose a deep learning-based approach for temporal 3D pose recognition problems based on a combination of a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) recurrent network. We also present a two-stage training strategy which firstly focuses on CNN training and, secondly, adjusts the full method (CNN+LSTM). Experimental testing demonstrated that our training method obtains better results than a single-stage training strategy. Additionally, we propose a data augmentation method that has also been validated experimentally. Finally, we perform an extensive experimental study on publicly available data benchmarks. The results obtained show how the proposed approach reaches state-of-the-art performance when compared to the methods identified in the literature. The best results were obtained for small datasets, where the proposed data augmentation strategy has greater impact.","Deep learning, Convolutional Neural Network, Recurrent neural network, Long Short-Term Memory, Human activity recognition, Hand gesture recognition, Real-time",Juan C. NÃºÃ±ez and RaÃºl Cabido and Juan J. Pantrigo and Antonio S. Montemayor and JosÃ© F. VÃ©lez,https://www.sciencedirect.com/science/article/pii/S0031320317304405,https://doi.org/10.1016/j.patcog.2017.10.033,0031-3203,2018,80--94,76,Pattern Recognition,Convolutional Neural Networks and Long Short-Term Memory for skeleton-based human activity and hand gesture recognition,article,NUNEZ201880,
"Digital watermarking is used to protect copyright information by embedding hidden data in digital media. In this study, a multiplicative watermarking scheme is proposed in the contourlet domain. Overall, selection of proper models is of great importance, as watermark detection processes can be replicated as decision rules. Accordingly, in this study, contourlet coefficients were modeled based on t-location scale distribution. Based on the KolmogorovâSmirnov test, t Location-Scale distribution showed high efficiency in modeling the coefficients. We used the likelihood ratio decision rule and t-location scale distribution to design an optimal multiplicative watermark detector. Then, we derive the receiver operating characteristics (ROC) analytically. The detector showed higher efficiency than other watermarking schemes in the literature, based on the experimental results, and its robustness against different attacks was verified.","Statistical modeling, Contourlet transform, t Location-Scale distribution, Multiplicative watermark, Likelihood ratio test",Sadegh Etemad and Maryam Amirmazlaghani,https://www.sciencedirect.com/science/article/pii/S0031320317304922,https://doi.org/10.1016/j.patcog.2017.12.006,0031-3203,2018,99--112,77,Pattern Recognition,A new multiplicative watermark detector in the contourlet domain using t Location-Scale distribution,article,ETEMAD201899,
"Adaptive graph learning methods for clustering, which adjust a data similarity matrix while taking into account its clustering capability, have drawn increasing attention in recent years due to their promising clustering performance. Existing adaptive graph learning methods are based on either original data or linearly projected data and thus rely on the assumption that either representation is a good indicator of the underlying data structure. However, this assumption is sometimes not met in high dimensional data. Studies have shown that high-dimensional data in many problems tend to lie on an embedded nonlinear manifold structure. Motivated by this observation, in this paper, we develop dual data representations, i.e., original data and a nonlinear embedding of the data obtained via an Extreme Learning Machine (ELM)-based neural network, and propose to use them as the more reliable basis for graph learning. The resulting algorithm based on ELM and Constrained Laplacian Rank (ELM-CLR) further improves the clustering capability and robustness, while retaining the advantages of adaptive graph learning, such as not requiring any post-processing to extract cluster indicators. The empirical study shows that the proposed algorithm outperforms the state-of-the-art graph-based clustering methods on a broad range of benchmark datasets.","Graph-based clustering, Constrained Laplacian rank, Extreme learning machine, Embedding, Graph Laplacian",Tianchi Liu and Chamara Kasun {Liyanaarachchi Lekamalage} and Guang-Bin Huang and Zhiping Lin,https://www.sciencedirect.com/science/article/pii/S0031320317304880,https://doi.org/10.1016/j.patcog.2017.12.001,0031-3203,2018,126--139,77,Pattern Recognition,An adaptive graph learning method based on dual data representations for clustering,article,LIU2018126,
"The labeling of Environmental Microorganisms (EM) which help decomposing pollutants, plays a fundamental role for establishing sustainable ecosystem. We propose an environmental microorganism classification engine that can automatically analyze microscopic images using Conditional Random Fields (CRF) and Deep Convolutional Neural Networks (DCNN). First, to effectively represent scarce training images, a DCNN pre-trained for image classification using a large amount of data is re-purposed to our feature extractor that distils pixel-level features in microscopic images. In addition, pixel-level classification results by such features can be refined using global features that describe the whole image in toto. Finally, our CRF model localizes and classifies EMs by considering the spatial relations among DCNN-based features, and their relations to global features. The experimental results have shown 94.2% of overall segmentation accuracy and up to 91.4% mean average precision of the results.","Environmental microorganism, Conditional random fields, Global feature extraction, Image classification, Image segmentation",Sergey Kosov and Kimiaki Shirahama and Chen Li and Marcin Grzegorzek,https://www.sciencedirect.com/science/article/pii/S0031320317305174,https://doi.org/10.1016/j.patcog.2017.12.021,0031-3203,2018,248--261,77,Pattern Recognition,Environmental microorganism classification using conditional random fields and deep convolutional neural networks,article,KOSOV2018248,
"Compared to uni-biometric systems, multi-biometric systems, which fuse multiple biometric features, can improve recognition accuracy and security. However, due to the challenging issues such as feature fusion and biometric template security, there is little research on cancelable multi-biometric systems. In this paper, we propose a fingerprint and finger-vein based cancelable multi-biometric system, which provides template protection and revocability. The proposed multi-biometric system combines the minutia-based fingerprint feature set and image-based finger-vein feature set. We develop a feature-level fusion strategy with three fusion options. Matching performance and security strength using these different fusion options are thoroughly evaluated and analyzed. Moreover, compared with the original partial discrete Fourier transform (P-DFT), security of the proposed multi-biometric system is strengthened, thanks to the enhanced partial discrete Fourier transform (EP-DFT) based non-invertible transformation.","Cancelable, Multi-biometrics, Feature level fusion, Data type incompatibility",Wencheng Yang and Song Wang and Jiankun Hu and Guanglou Zheng and Craig Valli,https://www.sciencedirect.com/science/article/pii/S0031320318300384,https://doi.org/10.1016/j.patcog.2018.01.026,0031-3203,2018,242--251,78,Pattern Recognition,A fingerprint and finger-vein based cancelable multi-biometric system,article,YANG2018242,
"We present our efforts to create a database of unconstrained Vietnamese online handwritten text sampled from pen-based devices. The database stores handwritten text for paragraphs, lines, words, and characters, with the ground truth associated with every paragraph and line. We show a detailed statistical analysis of the handwritten text in this database and describe recognition experiments using several recent methods including the Bidirectional Long Short-Term Memory (BLSTM) network. Overall, our database contains over 480,000 strokes from more than 380,000 characters, which, at present, is the largest database of Vietnamese online handwritten text. Although Vietnamese script is based on a fixed set of alphabet letters, the recognition of Vietnamese online handwritten text poses a difficult challenge because of many diacritical marks, which usually result in delayed strokes during writing. We designed and implemented an online handwriting-collection tool to gather data, as well as a line-segmentation tool and a delayed-stroke-detection tool to analyze collected handwritten text. We also conducted a statistical analysis based on the writer profiles. We applied a number of the state-of-the-art recognition methods on unconstrained Vietnamese handwriting to evaluate their performance, including the BLSTM network, which is an efficient architecture derived from the Recurrent Neural Network (RNN) and is often applied to sequence labeling problems. The BLSTM network achieved 90% character recognition accuracy, despite many long sequences with several delayed strokes. Our database is allowed open access for research to stimulate the development of handwriting research technology.","Vietnamese online handwriting, Database, Handwriting recognition, Recurrent neural network, Long short term memory",Hung Tuan Nguyen and Cuong Tuan Nguyen and Pham The Bao and Masaki Nakagawa,https://www.sciencedirect.com/science/article/pii/S0031320318300141,https://doi.org/10.1016/j.patcog.2018.01.013,0031-3203,2018,291--306,78,Pattern Recognition,A database of unconstrained Vietnamese online handwriting and recognition experiments by recurrent neural networks,article,NGUYEN2018291,
"In the problem of one-class classification, one-class classifier (OCC) tries to identify objects of a specific class, called the target class, among all objects, by learning from a training set containing only the objects of target class. However, some traditional OCCs lose their effectiveness when the distribution of target class is a multimodal distribution, and the performance is sensitive to the model parameters. To solve this problem and enhance classification performance, a novel OCC infinite Bayesian OCSVM (IB-OCSVM) is proposed in this study. In IB-OCSVM, we partition the input data into several clusters with the Dirichlet process mixture (DPM), and learn a modified OCSVM in each cluster. Specifically, the clustering procedure and the modified OCSVM are jointly learned in a unified Bayesian frame to guarantee the consistency of clustering and linear separability in each cluster. The parameters can be inferred simply and effectively via Gibbs sampling technique. Meanwhile, we modify the traditional OCSVM by replacing the kernel transformation with a new feature transformation based on the result of clustering via DPM. In the feature space of the modified OCSVM, all transformed samples preserve the same clustering characteristic as the original samples, thus guaranteeing the combination of clustering and classification in our method. Experimental results based on benchmark datasets and synthetic aperture radar real data demonstrate that the proposed method has enhanced classification performance and is more robust to the model parameters than some conventional methods.","Dirichlet process mixture, One-class classifiers, One-class support vector machine, Gibbs sampling",Wei Zhang and Lan Du and Liling Li and Xuefeng Zhang and Hongwei Liu,https://www.sciencedirect.com/science/article/pii/S0031320318300025,https://doi.org/10.1016/j.patcog.2018.01.006,0031-3203,2018,56--78,78,Pattern Recognition,Infinite Bayesian one-class support vector machine based on Dirichlet process mixture clustering,article,ZHANG201856,
"Automatic differentiation of benign and malignant mammography images is a challenging task. Recently, Convolutional Neural Networks (CNNs) have been proposed to address this task based on raw pixel input. However, these CNN-based methods are unable to exploit information from multiple sources, e.g., multi-view image and clinical data. A hybrid deep network framework is presented in this paper, aiming to efficiently integrate and exploit information from multi-view data for breast mass classification. Starting from a generic CNN for feature extraction and assuming a multi-view setup, an attention-based network is used to automatically select the informative features of breast mass. The attention mechanism attempts to make CNN focus on the semantic-related regions for a more interpretable classification result. Then, mass features from multi-view data are effectively aggregated by a Recurrent Neural Network (RNN). In contrast to previous works, the proposed framework learns the attention-driven features of CNN as well as the semantic label dependency among different views. We justify the proposed framework through extensive experiments on the BCDR data set and quantitative comparisons against other methods. We achieve a good performance in terms of ACC (0.85) and AUC (0.89).","Mass classification, Multi-view, Mammography, Convolutional Neural Networks, Recurrent Neural Networks",Hongyu Wang and Jun Feng and Zizhao Zhang and Hai Su and Lei Cui and Hua He and Li Liu,https://www.sciencedirect.com/science/article/pii/S0031320318300785,https://doi.org/10.1016/j.patcog.2018.02.026,0031-3203,2018,42--52,80,Pattern Recognition,Breast mass classification via deeply integrating the contextual information from multi-view data,article,WANG201842,
"When modeling multivariate data, one might have an extra parameter of contextual information that could be used to treat some observations as more similar to others. For example, images of faces can vary by age, and one would expect the face of a 40 year old to be more similar to the face of a 30 year old than to a baby face. We introduce a novel manifold approximation method, parameterized principal component analysis (PPCA) that models data with linear subspaces that change continuously according to the extra parameter of contextual information (e.g. age), instead of ad-hoc atlases. Special care has been taken in the loss function and the optimization method to encourage smoothly changing subspaces across the parameter values. The approach ensures that each observationâs projection will share information with observations that have similar parameter values, but not with observations that have large parameter differences. We tested PPCA on artificial data based on known, smooth functions of an added parameter, as well as on two real datasets with different types of parameters. We compared PPCA to PCA, sparse PCA and to independent principal component analysis (IPCA), an atlas based method that groups observations by their parameter values and projects each group using PCA with no sharing of information between groups. PPCA recovers the known functions with less error and projects the datasetsâ test set observations with consistently less reconstruction error than IPCA does. In some cases where the manifold is truly nonlinear, PCA outperforms all the other manifold approximation methods compared.","Manifold learning, Manifold approximation, Face modeling, Principal component analysis",Ajay Gupta and Adrian Barbu,https://www.sciencedirect.com/science/article/pii/S0031320318300268,https://doi.org/10.1016/j.patcog.2018.01.018,0031-3203,2018,215--227,78,Pattern Recognition,Parameterized principal component analysis,article,GUPTA2018215,
"Single image based rain removal is very challenging due to the lack of temporal and context information, and the existing techniques are usually unpractical in real-time applications as they are time-consuming, and make images blurred in varying degrees. To tackle this issue, this paper proposes a novel framework, based on a new observation that the background has a reasonably low correlation with rain streaks in gradient domain. The framework mainly contains three steps: 1) a rain-free direction with respect to a rain image or a block therein is proposed, describing the fact that there exists a direction along which the image is least-affected in gradient domain; 2) by combing total variation, low-rank constraint and a de-correlation term, a novel decomposition model is proposed to explicitly extract the rain and rain-free gradient components along the direction perpendicular to the just calculated rain-free direction; 3) the rain-free image is reconstructed using Poisson equation, which effectively resists the sparse noise contained in gradients. The favorable performance of the proposed framework has been confirmed by many experimental results, and especially the computational complexity is low.","Rain removal, Gradient domain, Decomposition model",Shuangli Du and Yiguang Liu and Mao Ye and Zhenyu Xu and Jie Li and Jianguo Liu,https://www.sciencedirect.com/science/article/pii/S0031320318300700,https://doi.org/10.1016/j.patcog.2018.02.016,0031-3203,2018,303--317,79,Pattern Recognition,Single image deraining via decorrelating the rain streaks and background scene in gradient domain,article,DU2018303,
"This article presents the Mean Partition Theorem of consensus clustering. We show that the Mean Partition Theorem is a fundamental result that connects to different, but not obviously related branches such as: (i) optimization, (ii) statistical consistency, (iii) optimal multiple alignment, (iv) profiles and motifs, (v) cluster stability, (vi) diversity, and (vii) Condorcetâs Jury Theorem. All proofs rest on the orbit space framework. The implications are twofold: First, the Mean Partition Theorem plays a far-reaching and central role in consensus clustering. Second, orbit spaces constitute a convenient representation for gaining insight into partition spaces.","Cluster ensembles, Consensus clustering, Mean partition, Optimal multiple alignment, Profiles, Motifs, Stability, Diversity",Brijnesh J. Jain,https://www.sciencedirect.com/science/article/pii/S0031320318300426,https://doi.org/10.1016/j.patcog.2018.01.030,0031-3203,2018,427--439,79,Pattern Recognition,The Mean Partition Theorem in consensus clustering,article,JAIN2018427,
"The use of indefinite kernels has attracted many research interests in recent years due to their flexibility. They do not possess the usual restrictions of being positive definite as in the traditional study of kernel methods. This paper introduces the indefinite unsupervised and semi-supervised learning in the framework of least squares support vector machines (LS-SVM). The analysis is provided for both unsupervised and semi-supervised models, i.e., Kernel Spectral Clustering (KSC) and Multi-Class Semi-Supervised Kernel Spectral Clustering (MSS-KSC). In indefinite KSC models one solves an eigenvalue problem whereas indefinite MSS-KSC finds the solution by solving a linear system of equations. For the proposed indefinite models, we give the feature space interpretation, which is theoretically important, especially for the scalability using NystrÃ¶m approximation. Experimental results on several real-life datasets are given to illustrate the efficiency of the proposed indefinite kernel spectral learning.","Semi-supervised learning, Scalable models, Indefinite kernels, Kernel spectral clustering, Low embedding dimension",Siamak Mehrkanoon and Xiaolin Huang and Johan A.K. Suykens,https://www.sciencedirect.com/science/article/pii/S003132031830013X,https://doi.org/10.1016/j.patcog.2018.01.014,0031-3203,2018,144--153,78,Pattern Recognition,Indefinite kernel spectral learning,article,MEHRKANOON2018144,
"Scene image or video understanding is a challenging task especially when number of video types increases drastically with high variations in background and foreground. This paper proposes a new method for categorizing scene videos into different classes, namely, Animation, Outlet, Sports, e-Learning, Medical, Weather, Defense, Economics, Animal Planet and Technology, for the performance improvement of text detection and recognition, which is an effective approach for scene image or video understanding. For this purpose, at first, we present a new combination of rough and fuzzy concept to study irregular shapes of edge components in input scene videos, which helps to classify edge components into several groups. Next, the proposed method explores gradient direction information of each pixel in each edge component group to extract stroke based features by dividing each group into several intra and inter planes. We further extract correlation and covariance features to encode semantic features located inside planes or between planes. Features of intra and inter planes of groups are then concatenated to get a feature matrix. Finally, the feature matrix is verified with temporal frames and fed to a neural network for categorization. Experimental results show that the proposed method outperforms the existing state-of-the-art methods, at the same time, the performances of text detection and recognition methods are also improved significantly due to categorization.","Rough set, Fuzzy set, Video categorization, Scene image classification, Video text detection, Video text recognition",Sangheeta Roy and Palaiahnakote Shivakumara and Namita Jain and Vijeta Khare and Anjan Dutta and Umapada Pal and Tong Lu,https://www.sciencedirect.com/science/article/pii/S0031320318300669,https://doi.org/10.1016/j.patcog.2018.02.014,0031-3203,2018,64--82,80,Pattern Recognition,Rough-fuzzy based scene categorization for text detection and recognition in video,article,ROY201864,
"Many real-world applications require multi-way feature selection rather than single-way feature selection. Multi-way feature selection is more challenging compared to single-way feature selection due to the presence of inter-correlation among the multi-way features. To address this challenge, we propose a novel non-negative matrix tri-factorization model based on co-sparsity regularization to facilitate feature co-shrinking for co-clustering. The basic idea is to learn the inter-correlation among the multi-way features while shrinking the irrelevant ones by encouraging the co-sparsity of the model parameters. The objective is to simultaneously minimize the loss function for the matrix tri-factorization, and the co-sparsity regularization imposed on the model. Furthermore, we develop an efficient and convergence-guaranteed algorithm to solve the non-smooth optimization problem, which works in an iteratively update fashion. The experimental results on various data sets demonstrate the effectiveness of the proposed approach.","Co-clustering, Non-negative matrix tri-factorization, Co-sparsity, Co-feature-selection",Qi Tan and Pei Yang and Jingrui He,https://www.sciencedirect.com/science/article/pii/S0031320317304934,https://doi.org/10.1016/j.patcog.2017.12.005,0031-3203,2018,12--19,77,Pattern Recognition,Feature co-shrinking for co-clustering,article,TAN201812,
"In this paper we present a new algorithm for parameter-free clustering by mode seeking. Mode seeking, especially in the form of the mean shift algorithm, is a widely used strategy for clustering data, but at the same time prone to poor performance if the parameters are not chosen correctly. We propose to form a clustering ensemble consisting of repeated and bootstrapped runs of the recent kNN mode seeking algorithm, an algorithm which is faster than ordinary mean shift and more suited for high dimensional data. This creates a robust mode seeking clustering algorithm with respect to the choice of parameters and high dimensional input spaces, while at the same inheriting all other strengths of mode seeking in general. We demonstrate promising results on a number of synthetic and real data sets.","Density based clustering, Consensus clustering, kNN mode seeking, Mean shift, Ensemble clustering",Jonas {Nordhaug Myhre} and Karl {Ãyvind Mikalsen} and Sigurd LÃ¸kse and Robert Jenssen,https://www.sciencedirect.com/science/article/pii/S0031320317304776,https://doi.org/10.1016/j.patcog.2017.11.023,0031-3203,2018,491--505,76,Pattern Recognition,Robust clustering using a kNN mode seeking ensemble,article,NORDHAUGMYHRE2018491,
"Many algorithms formulate graph matching as an optimization of an objective function of pairwise quantification of nodes and edges of two graphs to be matched. Pairwise measurements usually consider local attributes but disregard contextual information involved in graph structures. We address this issue by proposing contextual similarities between pairs of nodes. This is done by considering the tensor product graph (TPG) of two graphs to be matched, where each node is an ordered pair of nodes of the operand graphs. Contextual similarities between a pair of nodes are computed by accumulating weighted walks (normalized pairwise similarities) terminating at the corresponding paired node in TPG. Once the contextual similarities are obtained, we formulate subgraph matching as a node and edge selection problem in TPG. We use contextual similarities to construct an objective function and optimize it with a linear programming approach. Since random walk formulation through TPG takes into account higher order information, it is not a surprise that we obtain more reliable similarities and better discrimination among the nodes and edges. Experimental results shown on synthetic as well as real benchmarks illustrate that higher order contextual similarities increase discriminating power and allow one to find approximate solutions to the subgraph matching problem.","Subgraph matching, Product graph, Random walks, Backtrackless walks, Contextual similarities, Graphic recognition",Anjan Dutta and Josep LladÃ³s and Horst Bunke and Umapada Pal,https://www.sciencedirect.com/science/article/pii/S0031320317304892,https://doi.org/10.1016/j.patcog.2017.12.003,0031-3203,2018,596--611,76,Pattern Recognition,Product graph-based higher order contextual similarities for inexact subgraph matching,article,DUTTA2018596,
"In this paper, we propose a novel convolution neural networks (CNNs) based method for nodule type classification. Compared with classical approaches that are handling four solid nodule types, i.e., well-circumscribed, vascularized, juxta-pleuraland pleural-tail, our method could also achieve competitive classification rates on ground glass optical (GGO) nodules and non-nodules in computed tomography (CT) scans. The proposed method is based on multi-view multi-scale CNNs and comprises four main stages. First, we approximate the spherical surface centered at nodules using icosahedra and capture normalized sampling for CT values on each circular plane at a given maximum radius. Second, intensity analysis is applied based on the sampled values to achieve estimated radius for each nodule. Third, the re-sampling (which is the same as the first step but with estimated radius) is conducted, followed by a high frequency content measure analysis to decide which planes (views) are more abundant in information. Finally, with approximated radius and sorted circular planes, we build nodule captures at sorted scales and views to first pre-train a view independent CNNs model and then train a multi-view CNNs model with maximum pooling. The experimental results on both Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI)Â [1] and Early Lung Cancer Action Program(ELCAP)Â [2] have shown the promising classification performance even with complex GGO and non-nodule types.","Computed tomography, Lung nodule, CNNs",Xinglong Liu and Fei Hou and Hong Qin and Aimin Hao,https://www.sciencedirect.com/science/article/pii/S0031320317305186,https://doi.org/10.1016/j.patcog.2017.12.022,0031-3203,2018,262--275,77,Pattern Recognition,Multi-view multi-scale CNNs for lung nodule type classification from CT images,article,LIU2018262,
"In recent years, image annotation has attracted extensive attention due to the explosive growth of image data. With the capability of describing images at the semantic level, image annotation has many applications not only in image analysis and understanding but also in some relative disciplines, such as urban management and biomedical engineering. Because of the inherent weaknesses of manual image annotation, Automatic Image Annotation (AIA) has been raised since the late 1990s. In this paper, a deep review of state-of-the-art AIA methods is presented by synthesizing 138 literatures published during the past two decades. We classify AIA methods into five categories: 1) Generative model-based image annotation, 2) Nearest neighbor-based image annotation, 3) Discriminative model-based image annotation, and 4) Tag completion-based image annotation, 5) Deep Learning-based image annotation. Comparisons of the five types of AIA methods are made on the basis of the underlying idea, main contribution, model framework, computational complexity, computation time, and annotation accuracy. We also give an overview of five publicly available image datasets and four standard evaluation metrics commonly used as benchmarks for evaluating AIA methods. Then the performance of some typical or well-behaved models is assessed based on benchmark dataset and standard evaluation metrics. Finally, we share our viewpoints on the open issues and challenges in AIA as well as research trends in the future.","Automatic image annotation, Generative model, Nearest-neighbor model, Discriminative model, Tag-completion, Deep learning",Qimin Cheng and Qian Zhang and Peng Fu and Conghuan Tu and Sen Li,https://www.sciencedirect.com/science/article/pii/S0031320318300670,https://doi.org/10.1016/j.patcog.2018.02.017,0031-3203,2018,242--259,79,Pattern Recognition,A survey and analysis on automatic image annotation,article,CHENG2018242,
"Recent years have witnessed the great success of convolutional neural network (CNN) based models in the field of computer vision. CNN is able to learn hierarchically abstracted features from images in an end-to-end training manner. However, most of the existing CNN models only learn features through a feedforward structure and no feedback information from top to bottom layers is exploited to enable the networks to refine themselves. In this paper, we propose a Learning with Rethinking algorithm. By adding a feedback layer and producing the emphasis vector, the model is able to recurrently boost the performance based on previous prediction. Particularly, it can be employed to boost any pre-trained models. This algorithm is tested on four object classification benchmark datasets: CIFAR-100, CIFAR-10, MNIST-background-image and ILSVRC-2012 dataset, and the results have demonstrated the advantage of training CNN models with the proposed feedback mechanism.","Convolutional neural network, Image classification, Deep learning",Xin Li and Zequn Jie and Jiashi Feng and Changsong Liu and Shuicheng Yan,https://www.sciencedirect.com/science/article/pii/S0031320318300153,https://doi.org/10.1016/j.patcog.2018.01.015,0031-3203,2018,183--194,79,Pattern Recognition,Learning with rethinking: Recurrently improving convolutional neural networks through feedback,article,LI2018183,
"Alzheimerâs disease (AD) has been not only a substantial financial burden to the health care system but also the emotional hardship to patients and their families. Predicting cognitive performance of subjects from their magnetic resonance imaging (MRI) measures and identifying relevant imaging biomarkers are important research topics in the study of Alzheimerâs disease. Many previous works formulate the prediction task as a linear regression problem. The most critical limitation is that they assume a linear relationship between the MRI features and the cognitive outcomes. The linear models in original MRI feature spaces can be limited by their inability to exploit the nonlinear relation between the MRI features and cognitive measure prediction tasks. To better capture the complicated but more flexible relationship between the cognitive scores and the neuroimaging measures, we propose a â2,1ââ1 norm regularized multi-kernel multi-task feature learning formulation with a joint sparsity inducing regularization. The formulation facilitates the shared kernel functions, as well as the high dimensional features in the kernel induced feature spaces simultaneously, to look for the common representation that are useful for all tasks by promoting use of few kernels and few learned features in each kernel. For optimization, we develop an alternating optimization method to effectively solve the proposed mixed norm regularized formulation. We evaluate the performance of the proposed method using the Alzheimerâs Disease Neuroimaging Initiative (ADNI) datasets and demonstrate that our proposed methods achieve not only clearly improved prediction performance for cognitive measurements with single MRI modality or multi-modalities data, but also a compact set of highly suggestive biomarkers relevant to AD.","AlzheimerâS disease, Regression, Sparse learning, Multi-task learning, Kernel method",Peng Cao and Xiaoli Liu and Jinzhu Yang and Dazhe Zhao and Min Huang and Osmar Zaiane,https://www.sciencedirect.com/science/article/pii/S0031320318300396,https://doi.org/10.1016/j.patcog.2018.01.028,0031-3203,2018,195--215,79,Pattern Recognition,"â2,1ââ1 regularized nonlinear multi-task representation learning based cognitive performance prediction of Alzheimerâs disease",article,CAO2018195,
"Locality-sensitive sparse representation based classification has been shown to be effective for in-air handwritten Chinese character recognition (IAHCCR). In this paper, we present a locality-sensitive sparse representation toward optimized prototype classifier (LSROPC) for IAHCCR. In the proposed LSROPC, the learned dictionary can not only preserve local data structures, but also require the reconstruction of a pattern to get as close as possible to the prototype optimized by the minimum classification error (MCE) approach. So the LSROPC can help improve the classification accuracy effectively. The experiments are carried out on the datasets of traditional handwritten Chinese characters and in-air handwritten Chinese characters and the datasets designed for face recognition. The experimental results demonstrate the effectiveness of the proposed method.","Locality-sensitive sparse representation based classification, In-air handwritten Chinese character recognition, Minimum classification error, Handwritten Chinese character recognition",Xiwen Qu and Weiqiang Wang and Ke Lu and Jianshe Zhou,https://www.sciencedirect.com/science/article/pii/S0031320318300311,https://doi.org/10.1016/j.patcog.2018.01.021,0031-3203,2018,267--276,78,Pattern Recognition,In-air handwritten Chinese character recognition with locality-sensitive sparse representation toward optimized prototype classifier,article,QU2018267,
"Over the last decades, a great deal of research has been devoted to handwritten digit segmentation. Algorithms based on different features extracted from the background, foreground, and contour of images have been proposed, with those achieving the best results usually relying on a heavy set of heuristics and over-segmentation. Here, the challenge lies in finding a good set of heuristics to reduce the number of segmentation hypotheses. Independently of the heuristic over-segmentation strategy adopted, all algorithms used show their limitations when faced with complex cases such as overlapping digits. In this work, we postulate that handwritten digit segmentation can be successfully replaced by a set of classifiers trained to predict the size of the string and classify them without any segmentation. To support our position, we trained four Convolutional Neural Networks (CNN) on data generated synthetically and validated the proposed method on two well-known databases, namely, the Touching Pairs Dataset and NIST SD19. Our experimental results show that the CNN classifiers can handle complex cases of touching digits more efficiently than all segmentation algorithms available in the literature.",,A.G. Hochuli and L.S. Oliveira and A.S. {Britto Jr} and R. Sabourin,https://www.sciencedirect.com/science/article/pii/S0031320318300037,https://doi.org/10.1016/j.patcog.2018.01.004,0031-3203,2018,1--11,78,Pattern Recognition,Handwritten digit segmentation: Is it still necessary?,article,HOCHULI20181,
"Conventional Gaussian kernel c-means clustering algorithms are widely used in applications. However, Gaussian kernel functions have an important parameter, the width hyper-parameter, which needs to be tuned. Usually this parameter is tuned once and for all and it is the same for all variables. Thus, implicitly, all the variables are equally rescaled and therefore, they have equal importance on the clustering task. This paper presents Gaussian kernel c-means hard clustering algorithms with automated computation of the width hyper-parameters. In these kernel-based clustering algorithms, the hyper-parameters change at each iteration of the algorithm, they differ from variable to variable and can differ from cluster to cluster. Because each variable is rescaled differently according to its own hyper-parameter, these algorithms can select the important variables in the clustering process. Experiments using synthetic data sets and using UCI machine learning repository data sets corroborate the usefulness of the proposed algorithms.","Gaussian kernel clustering, Kernelization of the metric, Feature space, Width hyper-parameter",Francisco de A.T. {de Carvalho} and Eduardo C. SimÃµes and Lucas V.C. Santana and Marcelo R.P. Ferreira,https://www.sciencedirect.com/science/article/pii/S0031320318300712,https://doi.org/10.1016/j.patcog.2018.02.018,0031-3203,2018,370--386,79,Pattern Recognition,Gaussian kernel c-means hard clustering algorithms with automated computation of the width hyper-parameters,article,DECARVALHO2018370,
"Clustering is the process of finding underlying group structures in data. Although mixture model-based clustering is firmly established in the multivariate case, there is a relative paucity of work on matrix variate distributions and none for clustering with mixtures of skewed matrix variate distributions. Four finite mixtures of skewed matrix variate distributions are considered. Parameter estimation is carried out using an expectation-conditional maximization algorithm, and both simulated and real data are used for illustration.","Clustering, Matrix variate, Mixture models, Skewed distributions",Michael P.B. Gallaugher and Paul D. McNicholas,https://www.sciencedirect.com/science/article/pii/S0031320318300797,https://doi.org/10.1016/j.patcog.2018.02.025,0031-3203,2018,83--93,80,Pattern Recognition,Finite mixtures of skewed matrix variate distributions,article,GALLAUGHER201883,
"While salient object detection has been studied intensively by the computer vision and pattern recognition community, there are still great challenges in practical applications, especially when perceived objects have similar appearance such as intensity, color, and orientation, but different materials. Traditional methods do not provide good solution to this problem since they were mostly developed on color images and do not have the full capability in discriminating materials. More advanced technology and methodology are in demand to gain access to further information beyond human vision. In this paper, we extend the concept of salient object detection to material level based on hyperspectral imaging and present a material-based salient object detection method which can effectively distinguish objects with similar perceived color but different spectral responses. The proposed method first estimates the spatial distribution of different materials or endmembers using a hyperspectral unmixing approach. This step enables the calculation of a conspicuity map based on the global spatial variance of spectral responses. Then the multi-scale center-surround difference of local spectral features is calculated via spectral distance measures to generate local spectral conspicuity maps. These two types of conspicuity maps are fused for the final salient object detection. A new dataset of 45 hyperspectral images is introduced for experimental validation. The results show that our method outperforms several existing hyperspectral salient object detection approaches and the state-of-the-art methods proposed for RGB images.","Salient object detection, Hyperspectral imaging, Material composition, Hyperspectral unmixing, Spectral-spatial distribution",Jie Liang and Jun Zhou and Lei Tong and Xiao Bai and Bin Wang,https://www.sciencedirect.com/science/article/pii/S0031320317304806,https://doi.org/10.1016/j.patcog.2017.11.024,0031-3203,2018,476--490,76,Pattern Recognition,Material based salient object detection from hyperspectral images,article,LIANG2018476,
"Similarity (and distance metric) learning plays a very important role in many artificial intelligence tasks aiming at quantifying the relevance between objects. We address the challenge of learning complex relation patterns from data objects exhibiting heterogeneous properties, and develop an effective multi-view multimodal similarity learning model with much improved learning performance and model interpretability. The proposed method firstly computes multi-view convolutional features to achieve improved object representation, then analyses the similarities between objects by operating over multiple hidden relation types (modalities), and finally fine-tunes the entire model variables via back-propagating a ranking loss to the convolutional layers. We develop a topic-driven initialization scheme, so that each learned relation type can be interpreted as a representative of semantic topics of the objects. To improve model interpretability and generalization, sparsity is imposed over these hidden relations. The proposed method is evaluated by solving the image retrieval task using challenging image datasets, and is compared with seven state-of-the-art algorithms in the field. Experimental results demonstrate significant performance improvement of the proposed method over the competing ones.","Convolutional auto-encoder, Representation learning, Multi-view learning, Multimodal similarity learning",Xinjian Gao and Tingting Mu and John Y. Goulermas and Meng Wang,https://www.sciencedirect.com/science/article/pii/S0031320317300985,https://doi.org/10.1016/j.patcog.2017.02.035,0031-3203,2018,223--234,75,Pattern Recognition,Topic driven multimodal similarity learning with multi-view voted convolutional features,article,GAO2018223,Distance Metric Learning for Pattern Recognition
"It is difficult to recover the motion field from a real-world footage given a mixture of camera shake and other photometric effects. In this paper we propose a hybrid framework by interleaving a Convolutional Neural Network (CNN) and a traditional optical flow energy. We first conduct a CNN architecture using a novel learnable directional filtering layer. Such layer encodes the angle and distance similarity matrix between blur and camera motion, which is able to enhance the blur features of the camera-shake footages. The proposed CNNs are then integrated into an iterative optical flow framework, which enable the capability of modeling and solving both the blind deconvolution and the optical flow estimation problems simultaneously. Our framework is trained end-to-end on a synthetic dataset and yields competitive precision and performance against the state-of-the-art approaches.","Optical flow, Convolutional Neural Network (CNN), Video/image deblurring, Directional filtering",Wenbin Li and Da Chen and Zhihan Lv and Yan Yan and Darren Cosker,https://www.sciencedirect.com/science/article/pii/S0031320317301711,https://doi.org/10.1016/j.patcog.2017.04.020,0031-3203,2018,327--338,75,Pattern Recognition,Learn to model blurry motion via directional similarity and filtering,article,LI2018327,Distance Metric Learning for Pattern Recognition
"Domain shift is defined as the mismatch between the marginal probability distributions of a source (training set) and a target domain (test set). A successful research line has been focusing on deriving new source and target feature representations to reduce the domain shift problem. This task can be modeled as a semi-supervised domain adaptation. However, without exploiting at the same time the knowledge available on the labeled source, labeled target, and unlabeled target data, semi-supervised methods are prone to fail. Here, we present a simple and effective Semi-Supervised Transfer Subspace (SSTS) method for domain adaptation. SSTS establishes pairwise constraints between the source and labeled target data, besides it exploits the global structure of the unlabeled data to build a domain invariant subspace. After reducing the domain shift by projecting both source and target domain onto this subspace, any classifier can be trained on the source and tested on target. Results on 49 cross-domain problems confirm that SSTS is a powerful mechanism to reduce domain shift. Furthermore, SSTS yields better classification accuracy than state-of-the-art domain adaptation methods.","Cross-domain knowledge transfer, Cross-dataset classification, Dataset bias, Metric learning, Semi-supervised learning",LuÃ­s A.M. Pereira and Ricardo da Silva Torres,https://www.sciencedirect.com/science/article/pii/S0031320317301632,https://doi.org/10.1016/j.patcog.2017.04.011,0031-3203,2018,235--249,75,Pattern Recognition,Semi-supervised transfer subspace for domain adaptation,article,PEREIRA2018235,Distance Metric Learning for Pattern Recognition
"In this paper, we propose an innovative averaging of a set of time-series based on the Dynamic Time Warping (DTW). The DTW is widely used in data mining since it provides not only a similarity measure, but also a temporal alignment of time-series. However, its use is often restricted to the case of a pair of signals. In this paper, we propose to extend its application to a set of signals by providing an average time-series that opens a wide range of applications in data mining process. Starting with an existing well-established method called DBA (for DTW Barycenter Averaging), this paper points out its limitations and suggests an alternative based on a Constrained Dynamic Time Warping. Secondly, an innovative tolerance is added to take into account the admissible variability around the average signal. This new modeling of time-series is evaluated on a classification task applied on several datasets and results show that it outperforms state of the art methods.","Time-series averaging, Dynamic time warping, Local constraints, Constrained DTW barycenter averaging, Time-series classification",Marion Morel and Catherine Achard and Richard Kulpa and SÃ©verine Dubuisson,https://www.sciencedirect.com/science/article/pii/S003132031730328X,https://doi.org/10.1016/j.patcog.2017.08.015,0031-3203,2018,77--89,74,Pattern Recognition,Time-series averaging using constrained dynamic time warping with tolerance,article,MOREL201877,
"Transfer learning, or inductive transfer, refers to the transfer of knowledge from a source task to a target task. In the context of convolutional neural networks (CNNs), transfer learning can be implemented by transplanting the learned feature layers from one CNN (derived from the source task) to initialize another (for the target task). Previous research has shown that the choice of the source CNN impacts the performance of the target task. In the current literature, there is no principled way for selecting a source CNN for a given target task despite the increasing availability of pre-trained source CNNs. In this paper we investigate the possibility of automatically ranking source CNNs prior to utilizing them for a target task. In particular, we present an information theoretic framework to understand the source-target relationship and use this as a basis to derive an approach to automatically rank source CNNs in an efficient, zero-shot manner. The practical utility of the approach is thoroughly evaluated using the Places-MIT dataset, MNIST dataset and a real-world MRI database. Experimental results demonstrate the efficacy of the proposed ranking method for transfer learning.","Transfer learning, CNN selection, Deep learning",Muhammad Jamal Afridi and Arun Ross and Erik M. Shapiro,https://www.sciencedirect.com/science/article/pii/S0031320317302881,https://doi.org/10.1016/j.patcog.2017.07.019,0031-3203,2018,65--75,73,Pattern Recognition,On automated source selection for transfer learning in convolutional neural networks,article,AFRIDI201865,
"We introduce a fast Branch-and-Bound algorithm for optimal feature selection based on a U-curve assumption for the cost function. The U-curve assumption, which is based on the peaking phenomenon of the classification error, postulates that the cost over the chains of the Boolean lattice that represents the search space describes a U-shaped curve. The proposed algorithm is an improvement over the original algorithm for U-curve feature selection introduced recently. Extensive simulation experiments are carried out to assess the performance of the proposed algorithm (IUBB), comparing it to the original algorithm (UBB), as well as exhaustive search and Generalized Sequential Forward Search. The results show that the IUBB algorithm makes fewer evaluations and achieves better solutions under a fixed computational budget. We also show that the IUBB algorithm is robust with respect to violations of the U-curve assumption. We investigate the application of the IUBB algorithm in the design of imaging W-operators and in classification feature selection, using the average mean conditional entropy (MCE) as the cost function for the search.","Branch-and-bound algorithm, Feature selection, U-curve assumption, -operator design",Esmaeil Atashpaz-Gargari and Marcelo S. Reis and Ulisses M. Braga-Neto and Junior Barrera and Edward R. Dougherty,https://www.sciencedirect.com/science/article/pii/S0031320317303254,https://doi.org/10.1016/j.patcog.2017.08.013,0031-3203,2018,172--188,73,Pattern Recognition,A fast Branch-and-Bound algorithm for U-curve feature selection,article,ATASHPAZGARGARI2018172,
"A number of graph kernel-based methods have been developed with great success in many fields, but very little research has been published that is concerned with a graph kernel in Reproducing Kernel Hilbert Space (RKHS). In this paper, we firstly start with a derived expression for two forms of information entropy of an undirected graph. They are approximated von Neumann entropy and Shannon entropy, and depend on vertex degree statistics. Secondly, we show the basic solution of a generalized differential operator. This solution is a specific reproducing kernel called the H1-reproducing kernel in H1-space, and then it is proven to satisfy the condition of Mercer kernel. Thirdly, based on the two aforementioned forms of information entropy and H1-reproducing kernel, we define two reproducing graph kernels: one is approximated von Neumann entropy reproducing graph kernel (AVNERGK), the other is Shannon entropy reproducing graph kernel (SERGK). And then we prove that they satisfy the condition of Mercer kernel. Finally, to obtain better classification results, we further propose a hybrid reproducing graph kernel (HRGK) based on the two reproducing graph kernels. We use the HRGK as a means to establish the similarity between a pair of graphs. Experimental results reveal that our method gives better classification performance on graphs extracted from several graph datasets.","Differential operator, Reproducing kernel, Information entropy, Graph kernel, Graph classification",Lixiang Xu and Xiaoyi Jiang and Lu Bai and Jin Xiao and Bin Luo,https://www.sciencedirect.com/science/article/pii/S0031320317302960,https://doi.org/10.1016/j.patcog.2017.07.025,0031-3203,2018,89--98,73,Pattern Recognition,A hybrid reproducing graph kernel based on information entropy,article,XU201889,
"Indexing methods have been widely used for fast data retrieval on large scale datasets. When the data are represented by high dimensional vectors, hashing is often used as an efficient solution for approximate similarity search. When a retrieval task does not involve supervised training data, most hashing methods aim at preserving data similarity defined by a distance metric on the feature vectors. Hash codes generated by these approaches normally maintain the Hamming distance of the data in accordance with the similarity function, but ignore the local details of the distribution of data. This objective is not suitable for k-nearest neighbor search since the similarity to the nearest neighbors can vary significantly for different data samples. In this paper, we present a novel adaptive similarity measure which is consistent with k-nearest neighbor search, and prove that it leads to a valid kernel if the original similarity function is a kernel function. Next we propose a method which calculates hash codes using the kernel function. With a low-rank approximation, our hashing framework is more effective than existing methods that preserve similarity over an arbitrary kernel. The proposed similarity function, hashing framework, and their combination demonstrate significant improvement when compared with several alternative state-of-the-art methods.","Hashing, K-NN, Kernel, Binary indexing, Normalized Euclidean distance",Xiao Bai and Cheng Yan and Haichuan Yang and Lu Bai and Jun Zhou and Edwin Robert Hancock,https://www.sciencedirect.com/science/article/pii/S0031320317301310,https://doi.org/10.1016/j.patcog.2017.03.020,0031-3203,2018,136--148,75,Pattern Recognition,Adaptive hash retrieval with kernel based similarity,article,BAI2018136,Distance Metric Learning for Pattern Recognition
"Multidimensional Scaling (MDS) has been exploited to visualise the hidden structures among a set of entities in a reduced dimensional metric space. Here, we are interested in cases whenever the initial dissimilarity matrix is contaminated by outliers. It is well-known that the state-of-the-art algorithms for solving the MDS problem generate erroneous embeddings due to the distortion introduced by such outliers. To remedy this vulnerability, a unified framework for the solution of MDS problem is proposed, which resorts to half-quadratic optimization and employs potential functions of M-estimators in combination with â2,1 norm regularization. Two novel algorithms are derived. Their performance is assessed for various M-estimators against state-of-the-art MDS algorithms on four benchmark data sets. The numerical tests demonstrate that the proposed algorithms perform better than the competing alternatives.","Multidimensional scaling, Robustness, -estimators, Half-quadratic optimization, â norm regularization",Fotios Mandanas and Constantine Kotropoulos,https://www.sciencedirect.com/science/article/pii/S0031320317303333,https://doi.org/10.1016/j.patcog.2017.08.023,0031-3203,2018,235--246,73,Pattern Recognition,"M-estimators for robust multidimensional scaling employing â2,1 norm regularization",article,MANDANAS2018235,
"In real applications, data is usually collected from heterogeneous sources and represented with multiple modalities. To facilitate the analysis of such complex tasks, it is important to learn an effective similarity across different modalities. Existing similarity learning methods usually requires a large number of labeled training examples, leading to high labeling costs. In this paper, we propose a novel approach COSLAQ for active cross modal similarity learning, which actively queries the most important supervised information based on the disagreement among different intra-modal and inter-modal similarities. Furthermore, the closeness to decision boundary of similarity is utilized to avoid querying outliers and noises. Experiments on benchmark datasets demonstrate that the proposed method can reduce the labeling cost effectively.","Active learning, Cross modal similarity learning, Metric learning",Nengneng Gao and Sheng-Jun Huang and Yifan Yan and Songcan Chen,https://www.sciencedirect.com/science/article/pii/S0031320317301991,https://doi.org/10.1016/j.patcog.2017.05.011,0031-3203,2018,214--222,75,Pattern Recognition,Cross modal similarity learning with active queries,article,GAO2018214,Distance Metric Learning for Pattern Recognition
"Feature curves are largely adopted to highlight shape features, such as sharp lines, or to divide surfaces into meaningful segments, like convex or concave regions. Extracting these curves is not sufficient to convey prominent and meaningful information about a shape. We have first to separate the curves belonging to features from those caused by noise and then to select the lines, which describe non-trivial portions of a surface. The automatic detection of such features is crucial for the identification and/or annotation of relevant parts of a given shape. To do this, the Hough transform (HT) is a feature extraction technique widely used in image analysis, computer vision and digital image processing, while, for 3D shapes, the extraction of salient feature curves is still an open problem. Thanks to algebraic geometry concepts, the HT technique has been recently extended to include a vast class of algebraic curves, thus proving to be a competitive tool for yielding an explicit representation of the diverse feature lines equations. In the paper, for the first time we apply this novel extension of the HT technique to the realm of 3D shapes in order to identify and localize semantic features like patterns, decorations or anatomical details on 3D objects (both complete and fragments), even in the case of features partially damaged or incomplete. The method recognizes various features, possibly compound, and it selects the most suitable feature profiles among families of algebraic curves.","Feature curve recognition, Hough transform, Curve identification on surfaces, Robust line detection",Maria-Laura Torrente and Silvia Biasotti and Bianca Falcidieno,https://www.sciencedirect.com/science/article/pii/S0031320317303096,https://doi.org/10.1016/j.patcog.2017.08.008,0031-3203,2018,111--130,73,Pattern Recognition,Recognition of feature curves on 3D shapes using an algebraic approach to Hough transforms,article,TORRENTE2018111,
"Multivariate Time Series (MTS) modeling has received significant attention in the last decade because of the complex nature of the data. Efficient representations are required to deal with the high dimensionality due to the increase in the number of variables and duration of the time series in different applications. For example, model-based approaches such as Hidden Markov Models (HMM) or autoregressive (AR) models focus on finding a model to represent the series with the model parameters to handle this problem. Both HMM and AR models are known to be very successful in the representation of the time series however most of the HMM approaches assume independence and traditional AR models consider linear dependence between the variables of MTS. As most of the real systems exhibit nonlinear relations, traditional approaches fail to represent the time series. To handle these problems, we propose an autoregressive tree-based ensemble approach that can model the nonlinear behavior embedded in the time series with the help of tree-based learning. Multivariate autoregressive forest, namely mv-ARF, is a nonparametric vector autoregression approach which provides an easy and efficient representation that scales well with large datasets. An error-based representation based on the learned models is the basis of the proposed approach. This is very similar to time series kernels used for multivariate time series classification problems. We test mv-ARF on MTS classification problems and show that mv-ARF provides fast and competitive results on benchmark datasets from several domains. Furthermore, mv-ARF provides a research direction for vector autoregressive models that breaks from the linear dependency models to potentially foster other promising nonlinear approaches.","Multivariate time series, Vector autoregression, Time series representation, Ensemble learning, Classification",Kerem Sinan Tuncel and Mustafa Gokce Baydogan,https://www.sciencedirect.com/science/article/pii/S0031320317303291,https://doi.org/10.1016/j.patcog.2017.08.016,0031-3203,2018,202--215,73,Pattern Recognition,Autoregressive forests for multivariate time series modeling,article,TUNCEL2018202,
"Most existing approaches address multi-view subspace clustering problem by constructing the affinity matrix on each view separately and afterwards propose how to extend spectral clustering algorithm to handle multi-view data. This paper presents an approach to multi-view subspace clustering that learns a joint subspace representation by constructing affinity matrix shared among all views. Relying on the importance of both low-rank and sparsity constraints in the construction of the affinity matrix, we introduce the objective that balances between the agreement across different views, while at the same time encourages sparsity and low-rankness of the solution. Related low-rank and sparsity constrained optimization problem is for each view solved using the alternating direction method of multipliers. Furthermore, we extend our approach to cluster data drawn from nonlinear subspaces by solving the corresponding problem in a reproducing kernel Hilbert space. The proposed algorithm outperforms state-of-the-art multi-view subspace clustering algorithms on one synthetic and four real-world datasets.","Subspace clustering, Multi-view data, Low-rank, Sparsity, Alternating direction method of multipliers, Reproducing kernel Hilbert space",Maria BrbiÄ and Ivica Kopriva,https://www.sciencedirect.com/science/article/pii/S0031320317303370,https://doi.org/10.1016/j.patcog.2017.08.024,0031-3203,2018,247--258,73,Pattern Recognition,Multi-view low-rank sparse subspace clustering,article,BRBIC2018247,
"Feature transformation is of great importance to strengthen the descriptive power of feature representation for many classification and recognition tasks. In this paper, we propose a novel cross-view semantic projection learning method for extracting latent semantics from the hand-crafted features. Specifically, the shared latent basis matrix, the view-specific semantic projection functions and the optimal associations of different views are jointly learned in a unified matrix factorization framework, to get a common semantic space where images of the same person can be well characterized. We further present a generalization of the approach to multiple views. Extensive experiments on a series of challenging datasets highlight the superiorities of the proposed algorithm and demonstrate the effectiveness of the generalized version in multi-view person re-identification applications.","Person re-identification, Feature transformation, Semantic representation learning, Semantic projection learning",Ju Dai and Ying Zhang and Huchuan Lu and Hongyu Wang,https://www.sciencedirect.com/science/article/pii/S0031320317301723,https://doi.org/10.1016/j.patcog.2017.04.022,0031-3203,2018,63--76,75,Pattern Recognition,Cross-view semantic projection learning for person re-identification,article,DAI201863,Distance Metric Learning for Pattern Recognition
"Learning an appropriate distance metric is a crucial problem in pattern recognition. To confront with the scalability issue of massive data, hamming distance on binary codes is advocated since it permits exact sub-linear kNN search and meanwhile shares the advantage of efficient storage. In this paper, we study hamming metric learning in the context of multimodal data for cross-view similarity search. We present a new method called Parametric Local Multiview Hamming metric (PLMH), which learns multiview metric based on a set of local hash functions to locally adapt to the data structure of each modality. To balance locality and computational efficiency, the hash projection matrix of each instance is parameterized, with guaranteed approximation error bound, as a linear combination of basis hash projections associated with a small set of anchor points. The weak-supervisory information (side information) provided by pairwise and triplet constraints are incorporated in a coherent way to achieve semantically effective hash codes. A local optimal conjugate gradient algorithm with orthogonal rotations is designed to learn the hash functions for each bit, and the overall hash codes are learned in a sequential manner to progressively minimize the bias. Experimental evaluations on cross-media retrieval tasks demonstrate that PLMH performs competitively against the state-of-the-art methods.","Metric learning, Hamming distance, Hash function learning",Deming Zhai and Xianming Liu and Hong Chang and Yi Zhen and Xilin Chen and Maozu Guo and Wen Gao,https://www.sciencedirect.com/science/article/pii/S0031320317302418,https://doi.org/10.1016/j.patcog.2017.06.018,0031-3203,2018,250--262,75,Pattern Recognition,Parametric local multiview hamming distance metric learning,article,ZHAI2018250,Distance Metric Learning for Pattern Recognition
In this article we present two different approaches for automatic remote sensing image interpretation which are based on a multi-paradigm collaborative framework which uses classification in order to guide the segmentation process. The first approach applies sequentially many one-vs-all class extractors in a manner inspired by cascading techniques in machine learning. The second approach applies many collaborating one-vs-all class extractors in parallel. We show that the collaboration of the segmentation and classification paradigms result in a remarkable reduction of segmentation errors but also in better object classification in comparison to a hybrid pixel-object approach as well as a deep learning approach.,"Segmentation, Classification, Collaborative approaches, Remote sensing",AndrÃ©s Troya-Galvis and Pierre GanÃ§arski and Laure Berti-Ãquille,https://www.sciencedirect.com/science/article/pii/S0031320317303424,https://doi.org/10.1016/j.patcog.2017.08.030,0031-3203,2018,259--274,73,Pattern Recognition,Remote sensing image analysis by aggregation of segmentation-classification collaborative agents,article,TROYAGALVIS2018259,
"Accurate classification of different tumors in mammography plays a critical role in the early diagnosis of breast cancer. However, owing to variations in appearance, it is a challenging task to distinguish malignant instances from benign ones. For this purpose, we train a deep convolutional neural networks (CNNs) to obtain more discriminative description of breast tissues. Benefiting from the discriminative representation, metric learning layers are proposed to further improve performance of the deep structure. The best-performing model restricts the depth of backpropagation of joint training in only the metric learning layers. Relation between metric learning layers and tradition CNNs structures seems like parasitism relationship between species, where one species, the parasite, benefits at the expense of the other. Therefore, the proposed method is named as parasitic metric learning net. To confirm veracity of our method, classification experiments on breast mass images of two widely used databases are performed. Comparing performance of the proposed method with traditional ones, competitive results are achieved. Meanwhile, the parameter updating strategy for our parasitic metric net may inspire a way of improving performance of a pre-trained CNNs model on particular medical image processing or other computer vision tasks.","Deep learning, Metric learning, Breast mass classification, Mammography, Convolutional neural network",Zhicheng Jiao and Xinbo Gao and Ying Wang and Jie Li,https://www.sciencedirect.com/science/article/pii/S0031320317302698,https://doi.org/10.1016/j.patcog.2017.07.008,0031-3203,2018,292--301,75,Pattern Recognition,A parasitic metric learning net for breast mass classification based on mammography,article,JIAO2018292,Distance Metric Learning for Pattern Recognition
"Nowadays, due to the exponential growth of user generated images and videos, there is an increasing interest in learning-based hashing methods. In computer vision, the hash functions are learned in such a way that the hash codes can preserve essential properties of the original space (or label information). Then the Hamming distance of the hash codes can approximate the data similarity. On the other hand, vector quantization methods quantize the data into different clusters based on the criteria of minimal quantization error, and then perform the search using look-up tables. While hashing methods using Hamming distance can achieve faster search speed, their accuracy is often outperformed by quantization methods with the same code length, due to the low quantization error and more flexible distance lookups. To improve the effectiveness of the hashing methods, in this work, we propose Quantization-based Hashing (QBH), a general framework which incorporates the advantages of quantization error reduction methods into conventional property preserving hashing methods. The learned hash codes simultaneously preserve the properties in the original space and reduce the quantization error, and thus can achieve better performance. Furthermore, the hash functions and a quantizer can be jointly learned and iteratively updated in a unified framework, which can be readily used to generate hash codes or quantize new data points. Importantly, QBH is a generic framework that can be integrated to different property preserving hashing methods and quantization strategies, and we apply QBH to both unsupervised and supervised hashing models as showcases in this paper. Experimental results on three large-scale unlabeled datasets (i.e., SIFT1M, GIST1M, and SIFT1B), three labeled datastes (i.e., ESPGAME, IAPRTC and MIRFLICKR) and one video dataset (UQ_VIDEO) demonstrate the superior performance of our QBH over existing unsupervised and supervised hashing methods.","Hashing, Pseudo labels, Multimedia retrieval",Jingkuan Song and Lianli Gao and Li Liu and Xiaofeng Zhu and Nicu Sebe,https://www.sciencedirect.com/science/article/pii/S0031320317301322,https://doi.org/10.1016/j.patcog.2017.03.021,0031-3203,2018,175--187,75,Pattern Recognition,Quantization-based hashing: a general framework for scalable image and video retrieval,article,SONG2018175,Distance Metric Learning for Pattern Recognition
"In this paper, a locality constraint distance metric learning is proposed for traffic congestion detection. First of all, an accurate and unified definition of congestion is proposed and the congestion level analysis is treated as a regression problem in the paper. Based on that definition, a dataset consists of 20 different scenes is constructed for the first time since the existing dataset is not diverse for real applications. To characterize the congestion level in different scenes, the low-level texture feature and kernel regression is utilized to detect traffic congestion level. To reduce the influence among different scenes, a Locality Constraint Distance Metric Learning (LCML) which ensured the local smoothness and preserved the correlations between samples is proposed. The extensive experiments confirm the effectiveness of the proposed method.","Distance metric learning, Locality constraint, Kernel regression, Traffic congestion analysis",Qi Wang and Jia Wan and Yuan Yuan,https://www.sciencedirect.com/science/article/pii/S0031320317301401,https://doi.org/10.1016/j.patcog.2017.03.030,0031-3203,2018,272--281,75,Pattern Recognition,Locality constraint distance metric learning for traffic congestion detection,article,WANG2018272,Distance Metric Learning for Pattern Recognition
"3D neuron tips could be very good candidates of seeding points for neuron tracing applications. Previously, a ray-shooting model was proposed to detect the neuron tips by analyzing the intensity distribution of the neighborhood around the tip candidates. However, the length of the shooting rays and the number of how many z-slices should be taken into account in this model are fixed empirical numbers, so it cannot handle challenging dataset where the diameter of the neuron varies much. In this paper, we propose an adaptive ray-shooting model by changing the length of the shooting rays and the number of adjacent slices according to the local diameter of the neuron obtained by the Multistencils Fast Marching (MSFM) method. Compared with the previous work, the experiments results show that the proposed method could improve the detection accuracy rate by about 10% in challenging datasets.","Neuron tips, Neuron tracing, Adaptive ray-shooting model, Multistencils Fast Marching method",Min Liu and Rong Gong and Weixun Chen and Hanchuan Peng,https://www.sciencedirect.com/science/article/pii/S0031320317300511,https://doi.org/10.1016/j.patcog.2017.02.010,0031-3203,2018,263--271,75,Pattern Recognition,3D neuron tip detection in volumetric microscopy images using an adaptive ray-shooting model,article,LIU2018263,Distance Metric Learning for Pattern Recognition
"Statistics show that worldwide motor vehicle collisions lead to significant deaths and disabilities as well as substantial financial costs to both society and the individuals involved. Unintended lane departure is a leading cause of road fatalities by the collision. To reduce the number of traffic accidents and to improve driverâs safety lane departure warning (LDW), the system has emerged as a promising tool. Vision-based lane detection and departure warning system has been investigated over two decades. During this period, many different problems related to lane detection and departure warning have been addressed. This paper provides an overview of current LDW system, describing in particular pre-processing, lane models, lane de Ntection techniques and departure warning system.","Edge detection, Feature extraction, Lane departure measurement, Lane detection, Lane modeling",Sandipann P. Narote and Pradnya N. Bhujbal and Abbhilasha S. Narote and Dhiraj M. Dhane,https://www.sciencedirect.com/science/article/pii/S0031320317303266,https://doi.org/10.1016/j.patcog.2017.08.014,0031-3203,2018,216--234,73,Pattern Recognition,A review of recent advances in lane detection and departure warning system,article,NAROTE2018216,
"In this paper, we investigate the problem of video-based kinship verification via human face analysis. While several attempts have been made on facial kinship verification from still images, to our knowledge, the problem of video-based kinship verification has not been formally addressed in the literature. In this paper, we make the two contributions to video-based kinship verification. On one hand, we present a new video face dataset called Kinship Face Videos in the Wild (KFVW) which were captured in wild conditions for the video-based kinship verification study, as well as the standard benchmark. On the other hand, we employ our benchmark to evaluate and compare the performance of several state-of-the-art metric learning based kinship verification methods. Experimental results are presented to demonstrate the efficacy of our proposed dataset and the effectiveness of existing metric learning methods for video-based kinship verification. Lastly, we also evaluate human ability on kinship verification from facial videos and experimental results show that metric learning based computational methods are not as good as that of human observers.","Kinship verification, Metric learning, Face recognition, Video-based",Haibin Yan and Junlin Hu,https://www.sciencedirect.com/science/article/pii/S0031320317301000,https://doi.org/10.1016/j.patcog.2017.03.001,0031-3203,2018,15--24,75,Pattern Recognition,Video-based kinship verification using distance metric learning,article,YAN201815,Distance Metric Learning for Pattern Recognition
"Inherent to state-of-the-art dimension reduction algorithms is the assumption that global distances between observations are Euclidean, despite the potential for altogether non-Euclidean data manifolds. We demonstrate that a non-Euclidean manifold chart can be approximated by implementing a universal approximator over a dictionary of dissimilarity measures, building on recent developments in the field. This approach is transferable across domains such that observations can be vectors, distributions, graphs and time series for instance. Our novel dissimilarity learning method is illustrated with four standard visualisation datasets showing the benefits over the linear dissimilarity learning approach.","Dissimilarity, Visualisation, Multidimensional scaling, RBF network",Iain Rice,https://www.sciencedirect.com/science/article/pii/S0031320317302893,https://doi.org/10.1016/j.patcog.2017.07.016,0031-3203,2018,76--88,73,Pattern Recognition,Improved data visualisation through nonlinear dissimilarity modelling,article,RICE201876,
"Of late, neural networks and Multiple Instance Learning (MIL) are both attractive topics in the research areas related to Artificial Intelligence. Deep neural networks have achieved great successes in supervised learning problems, and MIL as a typical weakly-supervised learning method is effective for many applications in computer vision, biometrics, natural language processing, and so on. In this article, we revisit Multiple Instance Neural Networks (MINNs) that the neural networks aim at solving the MIL problems. The MINNs perform MIL in an end-to-end manner, which take bags with a various number of instances as input and directly output the labels of bags. All of the parameters in a MINN can be optimized via back-propagation. Besides revisiting the old MINNs, we propose a new type of MINN to learn bag representations, which is different from the existing MINNs that focus on estimating instance label. In addition, recent tricks developed in deep learning have been studied in MINNs; we find deep supervision is effective for learning better bag representations. In the experiments, the proposed MINNs achieve state-of-the-art or competitive performance on several MIL benchmarks. Moreover, it is extremely fast for both testing and training, for example, it takes only 0.0003 s to predict a bag and a few seconds to train on MIL datasets on a moderate CPU.","Multiple instance learning, Neural networks, Deep learning, End-to-end learning",Xinggang Wang and Yongluan Yan and Peng Tang and Xiang Bai and Wenyu Liu,https://www.sciencedirect.com/science/article/pii/S0031320317303382,https://doi.org/10.1016/j.patcog.2017.08.026,0031-3203,2018,15--24,74,Pattern Recognition,Revisiting multiple instance neural networks,article,WANG201815,
"Facial expression recognition in video has been an important and relatively new topic in human face analysis and attracted growing interests in recent years. Unlike conventional image-based facial expression recognition methods which recognize facial expression category from still images, facial expression recognition in video is more challenging because there are usually larger intra-class variations among facial frames within a video. This paper presents a collaborative discriminative multi-metric learning (CDMML) for facial expression recognition in video. We first compute multiple feature descriptors for each face video to describe facial appearance and motion information from different aspects. Then, we learn multiple distance metrics with these extracted multiple features collaboratively to exploit complementary and discriminative information for recognition. Experimental results on the Acted Facial Expression in Wild (AFEW) 4.0 and the extended CohnâKanada (CK+) datasets are presented to demonstrate the effectiveness of our proposed method.","Facial expression recognition, Multi-metric learning, Collaborative learning, Video-based, Multi-view learning",Haibin Yan,https://www.sciencedirect.com/science/article/pii/S0031320317300948,https://doi.org/10.1016/j.patcog.2017.02.031,0031-3203,2018,33--40,75,Pattern Recognition,Collaborative discriminative multi-metric learning for facial expression recognition in video,article,YAN201833,Distance Metric Learning for Pattern Recognition
"Symmetric Positive semi-Definite (SPD) matrices, as a kind of effective feature descriptors, have been widely used in pattern recognition and computer vision tasks. Affine-invariant metric (AIM) is a popular way to measure the distance between SPD matrices, but it imposes a high computational burden in practice. Compared with AIM, the Log-Euclidean metric embeds the SPD manifold via the matrix logarithm into a Euclidean space in which only classical Euclidean computation is involved. The advantage of using this metric for the non-linear SPD matrices representation of data has been recognized in some domains such as compressed sensing, however one pays little attention to this metric in data clustering. In this paper, we propose a novel Low Rank Representation (LRR) model on SPD matrices space with Log-Euclidean metric (LogELRR), which enables us to handle non-linear data through a linear manipulation manner. To further explore the intrinsic geometry distance between SPD matrices, we embed the SPD matrices into Reproductive Kernel Hilbert Space (RKHS) to form a family of kernels on SPD matrices based on the Log-Euclidean metric and construct a novel kernelized LogELRR method. The clustering results on a wide range of datasets, including object images, facial images, 3D objects, texture images and medical images, show that our proposed methods overcome other conventional clustering methods.","Symmetrical positive definite matrices, Log-Euclidean metric, Low Rank Representation, Subspace clustering",Boyue Wang and Yongli Hu and Junbin Gao and Muhammad Ali and David Tien and Yanfeng Sun and Baocai Yin,https://www.sciencedirect.com/science/article/pii/S0031320317302704,https://doi.org/10.1016/j.patcog.2017.07.009,0031-3203,2018,623--634,76,Pattern Recognition,Low Rank Representation on SPD matrices with Log-Euclidean metric,article,WANG2018623,
"Performing effective image retrieval tasks, capable of exploiting the underlying structure of datasets still constitutes a challenge research scenario. This paper proposes a novel manifold learning approach that exploits the intrinsic dataset geometry for improving the effectiveness of image retrieval tasks. The underlying dataset manifold is modeled and analyzed in terms of a Reciprocal kNN Graph and its Connected Components. The method computes the new retrieval results on an unsupervised way, without the need of any user intervention. A large experimental evaluation was conducted, considering different image retrieval tasks, various datasets and features. The proposed method yields better effectiveness results than various methods recently proposed, achieving effectiveness gains up to +40.75%.","Content-based image retrieval, Unsupervised manifold learning, Reciprocal kNN graph, Connected components",Daniel Carlos GuimarÃ£es Pedronette and Filipe Marcel Fernandes GonÃ§alves and Ivan Rizzo Guilherme,https://www.sciencedirect.com/science/article/pii/S0031320317301978,https://doi.org/10.1016/j.patcog.2017.05.009,0031-3203,2018,161--174,75,Pattern Recognition,Unsupervised manifold learning through reciprocal kNN graph and Connected Components for image retrieval tasks,article,PEDRONETTE2018161,Distance Metric Learning for Pattern Recognition
"Traditional depth estimation from stereo images is usually formulated as a patch-matching problem, which requires post-processing stages to impose smoothness and handle depth discontinuities and occlusions. While recent deep network approaches directly learn a regressor for the entire disparity map, they still suffer from large errors near the depth discontinuities. In this paper, we propose a novel method to refine the disparity maps generated by deep regression networks. Instead of relying on ad hoc post-processing, we learn a unified deep network model that predicts a confidence map and the disparity gradients from the learned feature representation in regression networks. We integrate the initial disparity estimation, the confidence map and the disparity gradients into a continuous Markov Random Field (MRF) for depth refinement, which is capable of representing rich surface structures. Our disparity MRF model can be solved via efficient global optimization in a closed form. We evaluate our approach on both synthetic and real-world datasets, and the results show it achieves the state-of-art performance and produces more structure-preserving disparity maps with smaller errors in the neighborhood of depth boundaries.","Stereo matching, Confidence measure, Convolutional neural network",Feiyang Cheng and Xuming He and Hong Zhang,https://www.sciencedirect.com/science/article/pii/S0031320317302984,https://doi.org/10.1016/j.patcog.2017.07.027,0031-3203,2018,122--133,74,Pattern Recognition,Learning to refine depth for robust stereo estimation,article,CHENG2018122,
"Image segmentation is an important step for large-scale image analysis and object recognition. Variational-based segmentation methods are widely studied due to their good performance, but they still suffer from incapability to deal with images bearing weak contrast, overlapped noise and cluttered texture. To tackle this problem, we propose a new statistical information analysis based multi-scale and global optimization method for image segmentation. This multi-scale processing which is consistent with humanâs cognition mechanism enables us identify target at coarse scale. The high-level prior is obtained by the multiple Gaussian kernel gray equalization and used as shape constraint in following fine-scale. An efficient energy functional is proposed with convexity and improved TV regularization in order to segment inhomogeneous target from noisy background. A convex relaxation function is explicitly represented by cubic B-Spline basis for fast convergence and intrinsic smooth segmentation. Finally, the energy functional is minimized by standard methods of Split Bregman, Gradient Descent Flow and the corresponding EulerâLagrange Equation. Experimental results on synthetic and real world images validate the robustness and high accuracy boundaries detection for low contrast, noisy and texture images.","Multi-scale image segmentation, Prior shape, Global optimization, B-Spline, Split Bregman, TV regularization",Shenhai Zheng and Bin Fang and Laquan Li and Mingqi Gao and Rui Chen and Kaiyi Peng,https://www.sciencedirect.com/science/article/pii/S003132031730314X,https://doi.org/10.1016/j.patcog.2017.08.011,0031-3203,2018,144--157,73,Pattern Recognition,B-Spline based globally optimal segmentation combining low-level and high-level information,article,ZHENG2018144,
"In this paper, we focus on user attribute analysis by recasting such a problem as a multi-task learning issue, where each attribute is considered as an independent task. In comparison with traditional data analysis, the missing labels problem broadly presents for smart sensor data due to some objective / subjective factors, where the label incompleteness increases the difficulty significantly. Therefore, we design a semi-supervised multi-task learning model (S2MTL) to handle the missing labels issue. For modeling, we integrate the matrix factorization to learn the mapping feature dictionary and attribute space information simultaneously, and adopt the pairwise affinity similarity to incorporate the unlabeled data information, where the low rank property and model efficiency can be well controlled. For model optimization, we convert our model as two individual convex subproblems with one non-smooth, and implement an alternating direction method to generate an efficient optimal solution. State-of-the-art models have validated the effectiveness and efficiency of our proposed model via extensive experiments and comparisons, on two public datasets and our new smart building dataset.","User attribute, Smart sensor, Multi-task learning, Semi-supervised learning, Missing labels, Low rank",Yang Cong and Gan Sun and Ji Liu and Haibin Yu and Jiebo Luo,https://www.sciencedirect.com/science/article/pii/S0031320317302820,https://doi.org/10.1016/j.patcog.2017.07.012,0031-3203,2018,33--46,73,Pattern Recognition,User attribute discovery with missing labels,article,CONG201833,
"This paper presents a multi-modal feature fusion based framework to improve the geographic image annotation. To achieve effective representations of geographic images, the method leverages a low-to-high learning flow for both the deep and shallow modality features. It first extracts low-level features for each input image pixel, such as shallow modality features (SIFT, Color, and LBP) and deep modality features (CNNs). It then constructs mid-level features for each superpixel from low-level features. Finally it harvests high-level features from mid-level features by using deep belief networks (DBN). It uses a restricted Boltzmann machine (RBM) to mine deep correlations between high-level features from both shallow and deep modalities to achieve a final representation for geographic images. Comprehensive experiments show that this feature fusion based method achieves much better performances compared to traditional methods.","Convolutional neural networks (CNNs), Deep learning, Geographic image annotation, Multi-modal feature fusion",Ke Li and Changqing Zou and Shuhui Bu and Yun Liang and Jian Zhang and Minglun Gong,https://www.sciencedirect.com/science/article/pii/S0031320317302583,https://doi.org/10.1016/j.patcog.2017.06.036,0031-3203,2018,1--14,73,Pattern Recognition,Multi-modal feature fusion for geographic image annotation,article,LI20181,
"Bioidentification is one of the most popular methods of user identification, one of the reasons is the fact that the âaccess tokensâ are part of userâs body and cannot be simply lost or forgotten. Recently, the popularity of biometric methods increases even more due to the improved accuracy of measuring devices and lower error rates offered by the algorithms. Despite the technological progress, prices of the top tier equipment remain on a constant, high level. In this paper, we propose a hand geometry user identification algorithm that utilizes data acquired by a standard office scanner, and that has reasonable execution times of both data collection and the identification process. In order to achieve an algorithm that is as accurate as current state of the art algorithms (or even outperforms them) and utilizes only devices that are commonly available in most offices we had to include several non-standard hand geometric features, e.g. the crookedness of the fingers. Our algorithm was tested on 60 volunteer adults, with age ranging from early 20s to late 50s. The most important results are False Acceptance Rate (FAR) equal to 0.0% and False Rejection Rate (FRR) at the level of 1.19%, with Equal Error Rate (EER) 0.59% during authorization (referred to as verification mode since algorithm verifies the claimed identity), when using a template derived from three images. In identification mode we got results FAR=0.0%, FRR=1.19% and ERR=0.59%. We achieved Identification Rate (IR) equal to 100.0% when taking only subjects from the database in identification mode. The subjects were not limited regarding the position of their hand on the scanner, nor were hand injuries and jewelry a disqualifying factor. Moreover, we describe the performance and time needed in a real-life experiments, showing that the algorithm may be used by people without technical background at low cost and is adequate for systems that require medium to high level of security.","Bioidentification, Pattern recognition, Palm scan, Basic devices",Marek Klonowski and Marcin Plata and Piotr Syga,https://www.sciencedirect.com/science/article/pii/S0031320317303278,https://doi.org/10.1016/j.patcog.2017.08.017,0031-3203,2018,189--201,73,Pattern Recognition,User authorization based on hand geometry without special equipment,article,KLONOWSKI2018189,
"The binarization of degraded document images is a challenging problem in terms of document analysis. Binarization is a classification process in which intra-image pixels are assigned to either of the two following classes: foreground text and background. Most of the algorithms are constructed on low-level features in an unsupervised manner, and the consequent disenabling of full utilization of input-domain knowledge considerably limits distinguishing of background noises from the foreground. In this paper, a novel supervised-binarization method is proposed, in which a hierarchical deep supervised network (DSN) architecture is learned for the prediction of the text pixels at different feature levels. With higher-level features, the network can differentiate text pixels from background noises, whereby severe degradations that occur in document images can be managed. Alternatively, foreground maps that are predicted at lower-level features present a higher visual quality at the boundary area. Compared with those of traditional algorithms, binary images generated by our architecture have cleaner background and better-preserved strokes. The proposed approach achieves state-of-the-art results over widely used DIBCO datasets, revealing the robustness of the presented method.","Document image binarization, Convolutional neural network, Document analysis",Quang Nhat Vo and Soo Hyung Kim and Hyung Jeong Yang and Gueesang Lee,https://www.sciencedirect.com/science/article/pii/S0031320317303394,https://doi.org/10.1016/j.patcog.2017.08.025,0031-3203,2018,568--586,74,Pattern Recognition,Binarization of degraded document images based on hierarchical deep supervised network,article,VO2018568,
"Squared planar markers are a popular tool for fast, accurate and robust camera localization, but its use is frequently limited to a single marker, or at most, to a small set of them for which their relative pose is known beforehand. Mapping and localization from a large set of planar markers is yet a scarcely treated problem in favour of keypoint-based approaches. However, while keypoint detectors are not robust to rapid motion, large changes in viewpoint, or significant changes in appearance, fiducial markers can be robustly detected under a wider range of conditions. This paper proposes a novel method to simultaneously solve the problems of mapping and localization from a set of squared planar markers. First, a quiver of pairwise relative marker poses is created, from which an initial pose graph is obtained. The pose graph may contain small pairwise pose errors, that when propagated, leads to large errors. Thus, we distribute the rotational and translational error along the basis cycles of the graph so as to obtain a corrected pose graph. Finally, we perform a global pose optimization by minimizing the reprojection errors of the planar markers in all observed frames. The experiments conducted show that our method performs better than Structure from Motion and visual SLAM techniques.","Fiducial markers, Marker mapping, SLAM, SfM",Rafael MuÃ±oz-Salinas and Manuel J. MarÃ­n-Jimenez and Enrique Yeguas-Bolivar and R. Medina-Carnicer,https://www.sciencedirect.com/science/article/pii/S0031320317303151,https://doi.org/10.1016/j.patcog.2017.08.010,0031-3203,2018,158--171,73,Pattern Recognition,Mapping and localization from planar markers,article,MUNOZSALINAS2018158,
"In this paper, we propose a new nonlinear dictionary learning (NDL) method and apply it to image classification. While a variety of dictionary learning algorithms have been proposed in recent years, most of them learn only a linear dictionary for feature learning and encoding, which cannot exploit the nonlinear relationship of image samples for feature extraction. Even though kernel-based dictionary learning methods can address this limitation, they still suffer from the scalability problem. Unlike existing dictionary learning methods, our NDL employs a feed-forward neural network to seek hierarchical feature projection matrices and dictionary simultaneously, so that the nonlinear structure of samples can be well exploited for feature learning and encoding. To better exploit the discriminative information, we extend the NDL into supervised NDL (SNDL) by learning a class-specific dictionary with the labels of training samples. Experimental results on four image datasets show the effectiveness of the proposed methods.","Nonlinear dictionary learning, Sparse coding, Neural network, Image classification",Junlin Hu and Yap-Peng Tan,https://www.sciencedirect.com/science/article/pii/S003132031730050X,https://doi.org/10.1016/j.patcog.2017.02.009,0031-3203,2018,282--291,75,Pattern Recognition,Nonlinear dictionary learning with application to image classification,article,HU2018282,Distance Metric Learning for Pattern Recognition
"In the past decade, several local region-based level set models have been proposed to segment images with intensity inhomogeneity. In general, these models are designed based on one assumption, i.e. intensity inhomogeneity is slowly varying in image domain. However, this assumption is not valid for those images with serious intensity inhomogeneity, which inevitably leads to poor segmentation performance of existing models. In this paper, we propose a novel level set method named as polynomial piecewise constant approximation (PPCA) to well segment images with serious intensity inhomogeneity. The basic idea of the PPCA method is to transform the original image to piecewise constant image so as to make piecewise constant segmentation criterion become applicable. Specially, we firstly define an initial objective function with some constraint conditions to transform the original image. Then, in order to obtain desirable piecewise constant image and highlight the anti-noise performance, the PPCA method is used to realize the dual constraint relaxation for objective function. The dual constraint relaxation reflects in two parts: on one hand, the objective function based on local region is exploited to replace the pointwise approximation method; on the other hand, considering the variance of local intensity distribution and the reliability of polynomial approximation, we utilize a Gaussian pyramid convolution strategy to devise polynomial fitting. The PPCA method relaxes the constraint condition of objective function so that the piecewise constant image is easily approximated. According to piecewise constant segmentation criterion, we obtain the partial differential equation based on polynomial piecewise constant approximation. Finally, we utilize level set method to construct the energy functional. The visual and quantitative experimental results demonstrate that the proposed PPCA method can yield better results than existing classical local models for segmenting images with serious intensity inhomogeneity.","Level set, Intensity inhomogeneity, Dual constraint relaxation, Image segmentation",Hai Min and Wei Jia and Xiao-Feng Wang and Yang Zhao and Yue-Tong Luo,https://www.sciencedirect.com/science/article/pii/S0031320317302637,https://doi.org/10.1016/j.patcog.2017.07.002,0031-3203,2018,15--32,73,Pattern Recognition,A polynomial piecewise constant approximation method based on dual constraint relaxation for segmenting images with intensity inhomogeneity,article,MIN201815,
"Over the past few years, linear representation models have seen a lot of successful applications such as face recognition in computer vision. In the context of face recognition, occlusion is a key factor that often curbs the performance of practical face recognition systems. In this paper, we propose to alleviate such negative influence of the occlusion noises by explicitly encoding the spatial continuity prior of the occlusion. Given the fact that many real-world occlusions such as sunglasses and scarves are contiguous, taking such prior into account can help build a more accurate model and achieve higher recognition rates. Besides, a general framework has also been proposed in which many off-the-shelf linear representation models can be nicely incorporated. And the minimization objectives of all these models can be solved via the same Half-Quadratic optimization procedure. Therefore the robustness of these models to occlusions can be comprehensively evaluated on a fair platform. Extensive experiments on the AR and Extended Yale B face databases corroborate that the proposed algorithms can improve the model robustness to contiguous occlusions.","Linear representation, Half-Quadratic optimization, Occlusion prior",Dong Wang and Ran He and Liang Wang and Tieniu Tan,https://www.sciencedirect.com/science/article/pii/S0031320317303369,https://doi.org/10.1016/j.patcog.2017.08.027,0031-3203,2018,560--568,76,Pattern Recognition,Robust linear representation via exploiting structure prior,article,WANG2018560,
"Person re-identification problem is targeting to match people in the views of non-overlapped camera networks. It is an important task in the fields of computer vision and video surveillance. It shows great value in applications like surveillance and action recognition. Existing metric learning based methods measure the similarity of sample pairs by learning a metric space in which the positive pairs are closer than negative pairs. However, the appearance features undergo with drastic variation. Person re-identification is a typical small sample problem. It is hard to learn a robust projection of metric subspace that takes all the situations into consideration. The learned metric subspace is usually over-fitting to training dataset due to the strict metric learning constraint. And the hard pairs in training dataset will weaken the discrimination of matching pairsâ similarity. To address these problems, a feedback mechanism based iterative metric learning method is proposed. The proposed method introduces a mean distance of multi-metric subspace to deal with the over-fitting problem. The joint discriminant optimal model on feedback top ranks matching pairs will enhance the discrimination of matching pairsâ similarity. It is a robust and discriminative distance metric which measures the matching pairs similarity with distances of multiple metric projections learned by a set of training datasets. Aiming to learn the multi-metric subspace, the proposed method gives a feedback mechanism based approach which back propagates the top ranks identification results as pseudo training datasets. The effectiveness of proposed mean distance of multi-metric projection is analyzed and proved theoretically. And extensive experiments on three challenging datasets, VIPeR, GRID and CUHK01 are conducted. The results show that the proposed method achieves the best performance and improves the state-of-the-art rank-1 identification rates by 18.48%, 2.00% and 5.41% on three datasets respectively.","Person re-identification, Metric learning, Feedback, Iteration",Yutao Ren and Xuelong Li and Xiaoqiang Lu,https://www.sciencedirect.com/science/article/pii/S0031320317301620,https://doi.org/10.1016/j.patcog.2017.04.012,0031-3203,2018,99--111,75,Pattern Recognition,Feedback mechanism based iterative metric learning for person re-identification,article,REN201899,Distance Metric Learning for Pattern Recognition
"As each energy source in a multi-energy-source vehicle has its own high-efficient range, the fuel-saving performance of the vehicle is sensitive to driving conditions. In order to improve the vehicle fuel economy, the relationship of fuel consumption and driving condition type is needed to be established clearly as the basis of adaptive energy allocation strategy. This work proposes a two-steps methodology to establish such relationship through clustering micro-trips (which are segmented from driving data and used to indicate driving conditions) with a similar fuel consumption level together. The first step is to construct a fuel consumption-related feature vector by implementing correlation analysis for feature selection and applying principal component analysis for feature dimension reduction. The second step is to cluster micro-trips based on their density centers. This methodology is validated based on the driving data in Beijing, and three types of micro-trips are obtained from these driving data, and their average fuel consumptions are 8.5068, 12.7663 and 17.0686â¯ L/100â¯km, respectively.","Driving conditions, Clustering, Principal component analysis, Driving features, Dynamic programming",Haiming Xie and Guangyu Tian and Hongxu Chen and Jing Wang and Yong Huang,https://www.sciencedirect.com/science/article/pii/S0031320317303126,https://doi.org/10.1016/j.patcog.2017.08.006,0031-3203,2018,131--143,73,Pattern Recognition,A distribution density-based methodology for driving data cluster analysis: A case study for an extended-range electric city bus,article,XIE2018131,
"In this paper, we propose a collaborative multiview hashing (CMH) approach to incorporate multiview representations into the binary code learning for scalable visual retrieval. Unlike most existing multiview hashing methods which learn linear projections to preserve the fused similarity relationship across different views in unsupervised manner, we employ the nonlinear hashing functions as the projection in each view and exploit the diverse information of multiview representations by utilizing the collaboration between view representations and the correlation between the view representations and the semantic labels. Specifically, the binary codes in each view are constrained to be predictive to each other to exploit the collaboration between the descriptors in different views that describe the same sample. Furthermore, the binary codes in all views are enforced to preserve the semantic relationship between data samples. The hashing functions are implemented in the form of multi-layer neural network with nonlinear transformations at each layer and trained with both the view collaboration and semantic preserving constraints on the outputs. Experimental results on two datasets validate the superiority of the proposed approach in comparison with several state-of-the-art hashing methods.","Multiview hashing, View collaboration, Nonlinear hashing, Binary code",Zhixiang Chen and Jie Zhou,https://www.sciencedirect.com/science/article/pii/S0031320317300894,https://doi.org/10.1016/j.patcog.2017.02.026,0031-3203,2018,149--160,75,Pattern Recognition,Collaborative multiview hashing,article,CHEN2018149,Distance Metric Learning for Pattern Recognition
"In word spotting literature, many approaches have considered word images as temporal signals that could be matched by classical Dynamic Time Warping algorithm. Consequently, DTW has been widely used as a on the shelf tool. However there exists many other improved versions of DTW, along with other robust sequence matching techniques. Very few of them have been studied extensively in the context of word spotting whereas it has been well explored in other application domains such as speech processing, data mining etc. The motivation of this paper is to investigate such area in order to extract significant and useful information for users of such techniques. More precisely, this paper has presented a comparative study of classical Dynamic Time Warping (DTW) technique and many of its improved modifications, as well as other sequence matching techniques in the context of word spotting, considering both theoretical properties as well as experimental ones. The experimental study is performed on historical documents, both handwritten and printed, at word or line segmentation level and with a limited or extended set of queries. The comparative analysis is showing that classical DTW remains a good choice when there is no segmentation problems for word extraction. Its constrained version (e.g. Itakura Parallelogram) seems better on handwritten data, as well as Hilbert transform also shows promising performances on handwritten and printed datasets. In case of printed data and low level features (pixelâs column based), the aggregation of features (e.g. Piecewise-DTW) seems also very important. Finally, when there are important word segmentation errors or when we are considering line segmentation level, Continuous Dynamic Programming (CDP) seems to be the best choice.","Word spotting, Degraded historical documents, Hand-written documents, George Washington dataset, Bentham dataset, Japanese handwriting recognition",Tanmoy Mondal and Nicolas Ragot and Jean-yves Ramel and Umapada Pal,https://www.sciencedirect.com/science/article/pii/S0031320317302832,https://doi.org/10.1016/j.patcog.2017.07.011,0031-3203,2018,47--64,73,Pattern Recognition,Comparative study of conventional time series matching techniques for word spotting,article,MONDAL201847,
"In this paper, aiming at improving the generalization capability, we propose a cross-dataset person re-identification framework via integrating patch-based metric learning and local salience learning. Firstly, Convolution Neural Network(CNN) features are extracted to represent patches of a person. Secondly, only two positive patch-pairs are chosen and input into a Large Margin Nearest Neighbour(LMNN) network to learn two patch-based metric matrices for feature projection respectively. Thirdly, according to projected new features, a local salience learning algorithm based on Kmeans clustering is proposed to train the weights of patches. Finally, the similarity of image-pair is computed by a weighted summing of all patches. The experimental results indicate that the proposed method outperforms existing conventional approaches based on hand-crafted features and achieves a comparable performance with most recent CNN-based methods, which demonstrates our methodâs effectiveness and practicality. It does not need a large-scale labeled training dataset, and has a high matching rate with a low computation complexity.","Person re-identification, CNN feature, Patch-based metric learning, Local salience learning, Cross-dataset",Zhicheng Zhao and Binlin Zhao and Fei Su,https://www.sciencedirect.com/science/article/pii/S0031320317301346,https://doi.org/10.1016/j.patcog.2017.03.023,0031-3203,2018,90--98,75,Pattern Recognition,Person re-identification via integrating patch-based metric learning and local salience learning,article,ZHAO201890,Distance Metric Learning for Pattern Recognition
"Recent years have witnessed the promising efficacy and efficiency of hashing (also known as binary code learning) for retrieving nearest neighbor in large-scale data collections. Particularly, with supervision knowledge (e.g., semantic labels), we may further gain considerable performance boost. Nevertheless, most existing supervised hashing schemes suffer from the following limitations: (1) severe quantization error caused by continuous relaxation of binary codes; (2) disturbance of unreliable codes in subsequent hash function learning; and (3) erroneous guidance derived from imprecise and incomplete semantic labels. In this work, we propose a novel supervised hashing approach, termed as Robust Discrete Code Modeling (RDCM), which directly learns high-quality discrete binary codes and hash functions by effectively suppressing the influence of unreliable binary codes and potentially noisily-labeled samples. RDCM employs â2, p norm, which is capable of inducing sample-wise sparsity, to jointly perform code selection and noisy sample identification. Moreover, we preserve the discrete constraint in RDCM to eliminate the quantization error. An efficient algorithm is developed to solve the discrete optimization problem. Extensive experiments conducted on various real-life datasets show the superiority of the proposed RDCM approach as compared to several state-of-the-art hashing methods.","Supervised hashing, Robust modeling, Discrete optimization.",Yadan Luo and Yang Yang and Fumin Shen and Zi Huang and Pan Zhou and Heng Tao Shen,https://www.sciencedirect.com/science/article/pii/S0031320317300973,https://doi.org/10.1016/j.patcog.2017.02.034,0031-3203,2018,128--135,75,Pattern Recognition,Robust discrete code modeling for supervised hashing,article,LUO2018128,Distance Metric Learning for Pattern Recognition
"We study the problem of learning a similarity function from a set of binary labeled data pairs. A common approach is to learn a similarly function which is a bilinear form associated to the pair of data points. We argue that this class may be too restrictive when handling heterogeneous datasets. To overcome this limitation local metric learning techniques have been advocated in the literature. However, they are subject to certain constraints preventing their usage in many applications. For example, they require knowledge of the class label of the training points. In this paper, we present a local metric learning method, which overcomes these limitations. The method first initializes a Gaussian mixture model on the training data. Then it estimates a set of local metrics and simultaneously refines the mixtureâs parameters. Finally, a similarity function is obtained by aggregating the local metrics. We also introduce a novel regularization term, which works well in a transfer learning setting. Our experiments show that the proposed method achieves state-of-the-art results on several real datasets.","Similarity function learning, Local metric learning, Nearest neighbors classification, Face verification",Julien BohnÃ© and Yiming Ying and StÃ©phane Gentric and Massimiliano Pontil,https://www.sciencedirect.com/science/article/pii/S0031320317301462,https://doi.org/10.1016/j.patcog.2017.04.002,0031-3203,2018,315--326,75,Pattern Recognition,Learning local metrics from pairwise similarity data,article,BOHNE2018315,Distance Metric Learning for Pattern Recognition
"Handcrafted ordinal measures (OM) have been widely used in many computer vision problems. This paper presents a structured OM (SOM) method in a data driven way. SOM simultaneously learns ordinal filters and structured ordinal features. It leads to a structural distance metric for video-based face recognition. The SOM problem is posed as a non-convex integer program problem that includes two parts. The first part learns stable ordinal filters to project video data into a large-margin ordinal space. The second seeks self-correcting and discrete codes by balancing the projected data and a rank-one ordinal matrix in a structured low-rank way. Weakly-supervised and supervised structures are considered for the ordinal matrix. In addition, as a complement to hierarchical structures, deep feature representations are integrated into our method to enhance coding stability. An alternating minimization method is employed to handle the discrete and low-rank constraints, yielding high-quality codes that capture prior structures well. Experimental results on three commonly used face video databases show that our SOM method with a simple voting classifier can achieve state-of-the-art recognition rates using fewer features and samples.","Ordinal measure, Metric learning, Local feature",Ran He and Tieniu Tan and Larry Davis and Zhenan Sun,https://www.sciencedirect.com/science/article/pii/S0031320317300493,https://doi.org/10.1016/j.patcog.2017.02.005,0031-3203,2018,4--14,75,Pattern Recognition,Learning structured ordinal measures for video based face recognition,article,HE20184,Distance Metric Learning for Pattern Recognition
"In recent years, multi-view graph partitioning has attracted more and more attention, but most efforts have been made to develop graph partitioning approaches directly in the original topological structure. In many real-world applications, graph may contain noisy links and the distance metric may not be so discriminative for revealing cluster structure. This paper addresses the problem of discriminative metric learning for multi-view graph partitioning. In particular, we propose a novel method called Multi-view DML (abbr. of Multi-view Discriminative Metric Learning) to transform the metric space in the original graph into a more discriminative metric space, in which better graph partitioning results will be obtained. We envision the multi-view graph as an adaptive dynamic system, where both the intra-view connections and the inter-view couplings are interplayed to gradually update the relation metric among nodes. On the one hand, the inter-view coupling will be influenced by the intra-view connection between two nodes. On the other hand, the inter-view coupling will also affect the intra-view connection. Such interplay eventually makes the whole graph reach a steady state which has a stronger cluster structure than the original graph. Extensive experiments are conducted on both synthetic and real-world graphs to confirm that the proposed method is able to learn more discriminative metric.","Discriminative metric learning, Multi-view, Graph partitioning, Permanence, Modularity",Juan-Hui Li and Chang-Dong Wang and Pei-Zhen Li and Jian-Huang Lai,https://www.sciencedirect.com/science/article/pii/S0031320317302352,https://doi.org/10.1016/j.patcog.2017.06.012,0031-3203,2018,199--213,75,Pattern Recognition,Discriminative metric learning for multi-view graph partitioning,article,LI2018199,Distance Metric Learning for Pattern Recognition
"Person re-identification (re-id) aims to match pedestrians observed by disjoint camera views. It attracts increasing attention in computer vision due to its importance to surveillance systems. To combat the major challenge of cross-view visual variations, deep embedding approaches are proposed by learning a compact feature space from images such that the Euclidean distances correspond to their cross-view similarity metric. However, the global Euclidean distance cannot faithfully characterize the ideal similarity in a complex visual feature space because features of pedestrian images exhibit unknown distributions due to large variations in poses, illumination and occlusion. Moreover, intra-personal training samples within a local range which are robust to guide deep embedding against uncontrolled variations cannot be captured by a global Euclidean distance. In this paper, we study the problem of person re-id by proposing a novel sampling to mine suitable positives (i.e., intra-class) within a local range to improve the deep embedding in the context of large intra-class variations. Our method is capable of learning a deep similarity metric adaptive to local sample structure by minimizing each sampleâs local distances while propagating through the relationship between samples to attain the whole intra-class minimization. To this end, a novel objective function is proposed to jointly optimize similarity metric learning, local positive mining and robust deep feature embedding. This attains local discriminations by selecting local-ranged positive samples, and the learned features are robust to dramatic intra-class variations. Experiments on benchmarks show state-of-the-art results achieved by our method.","Deep feature embedding, Person re-identification, Local positive mining",Lin Wu and Yang Wang and Junbin Gao and Xue Li,https://www.sciencedirect.com/science/article/pii/S0031320317303400,https://doi.org/10.1016/j.patcog.2017.08.029,0031-3203,2018,275--288,73,Pattern Recognition,Deep adaptive feature embedding with local sample distributions for person re-identification,article,WU2018275,
"Abnormal event detection is now a widely concerned research topic, especially for crowded scenes. In recent years, many dictionary learning algorithms have been developed to learn normal event regularities, and have presented promising performance for abnormal event detection. However, they seldom consider the structural information, which plays important roles in many computer vision tasks, such as image denoising and segmentation. In this paper, structural information is explored within a sparse representation framework. On the one hand, we introduce a new concept named reference event, which indicates the potential event patterns in normal video events. Compared with abnormal events, normal ones are more likely to approximate these reference events. On the other hand, a smoothness regularization is constructed to describe the relationships among video events. The relationships consist of both similarities in the feature space and relative positions in the video sequences. In this case, video events related to each other are more likely to possess similar representations. The structured dictionary and sparse representation coefficients are optimized through an iterative updating strategy. In the testing phase, abnormal events are identified as samples which cannot be well represented using the learned dictionary. Extensive experiments and comparisons with state-of-the-art algorithms have been conducted to prove the effectiveness of the proposed algorithm.","Video surveillance, Abnormal event detection, Dictionary learning, Sparse representation, Reference event",Yuan Yuan and Yachuang Feng and Xiaoqiang Lu,https://www.sciencedirect.com/science/article/pii/S0031320317303047,https://doi.org/10.1016/j.patcog.2017.08.001,0031-3203,2018,99--110,73,Pattern Recognition,Structured dictionary learning for abnormal event detection in crowded scenes,article,YUAN201899,
"Crowded scene analysis is a popular research topic due to its great application potentials, such as intelligent video surveillance and crowd density estimation. In this paper, we propose a novel approach to detecting crowd groups and learning semantic regions with a unified hierarchical clustering framework. According to the Gestalt laws of grouping, we propose three priors to define a unified similarity metric to measure the similarities of pairs of original tracklets and pairs of representative tracklets from different crowd groups, so that the short-term crowd groups and the long-term semantic paths commonly composed of several short-term crowd groups can be detected by a bottom-up hierarchical clustering algorithm simultaneously. In order to verify our method at the longer time duration video sequences in the crowded scene, we construct a new crowd database (CASIA crowd database 1) with various crowd densities in real scenes. Extensive experiments on our CASIA crowd database, Collective Motion Database and CUHK database are performed, and the results demonstrate that our approach is effective and reliable for crowd detection and semantic scene understanding in various crowd densities, especially for the crowd analysis in long temporal video clips.","Similarity measurement, Group detection, Semantic regions, Hierarchical clustering",Weiqi Zhao and Zhang Zhang and Kaiqi Huang,https://www.sciencedirect.com/science/article/pii/S0031320317302431,https://doi.org/10.1016/j.patcog.2017.06.020,0031-3203,2018,112--127,75,Pattern Recognition,Gestalt laws based tracklets analysis for human crowd understanding,article,ZHAO2018112,Distance Metric Learning for Pattern Recognition
"Hashing has been widely utilized for Approximate Nearest Neighbor (ANN) search due to its fast retrieval speed and low storage cost. In this work, we propose a novel supervised hashing method for scalable face image retrieval, i.e., Deep Hashing based on Classification and Quantization errors (DHCQ), by simultaneously learning feature representations of images, hash codes and classifiers. The supervised information and the deep architecture are collaboratively explored. Specifically, a deep convolutional network is introduced to learn discriminative feature representations, which are directly used to generate hash codes and predict labels of images. The quantization errors and the prediction errors jointly guide the learning of the deep network. They are highly interrelated and promoted each other. It is worth noting that the proposed method is a general hashing method and can be applied to the general image retrieval task. Extensive experiments on two face image datasets and one general image dataset demonstrate the effectiveness of the proposed method compared with several state-of-the-art hashing methods.","Image retrieval, Supervised hashing, Binary codes, Deep learning",Jinhui Tang and Zechao Li and Xiang Zhu,https://www.sciencedirect.com/science/article/pii/S0031320317301383,https://doi.org/10.1016/j.patcog.2017.03.028,0031-3203,2018,25--32,75,Pattern Recognition,Supervised deep hashing for scalable face image retrieval,article,TANG201825,Distance Metric Learning for Pattern Recognition
"To achieve a low computational cost when performing online metric learning for large-scale data, we present a one-pass closed-form solution namely OPML in this paper. Typically, the proposed OPML first adopts a one-pass triplet construction strategy, which aims to use only a very small number of triplets to approximate the representation ability of whole original triplets obtained by batch-manner methods. Then, OPML employs a closed-form solution to update the metric for new coming samples, which leads to a low space (i.e., O(d)) and time (i.e., O(d2)) complexity, where d is the feature dimensionality. In addition, an extension of OPML (namely COPML) is further proposed to enhance the robustness when in real case the first several samples come from the same class (i.e., cold start problem). In the experiments, we have systematically evaluated our methods (OPML and COPML) on three typical tasks, including UCI data classification, face verification, and abnormal event detection in videos, which aims to fully evaluate the proposed methods on different sample number, different feature dimensionalities and different feature extraction ways (i.e., hand-crafted and deeply-learned). The results show that OPML and COPML can obtain the promising performance with a very low computational cost. Also, the effectiveness of COPML under the cold start setting is experimentally verified.","One-pass, Online metric learning, Triplet construction, Face verification, Abnormal event detection",Wenbin Li and Yang Gao and Lei Wang and Luping Zhou and Jing Huo and Yinghuan Shi,https://www.sciencedirect.com/science/article/pii/S0031320317301140,https://doi.org/10.1016/j.patcog.2017.03.016,0031-3203,2018,302--314,75,Pattern Recognition,OPML: A one-pass closed-form solution for online metric learning,article,LI2018302,Distance Metric Learning for Pattern Recognition
"With the fast development of information acquisition, there is a rapid growth of multi-modality data, e.g., text, audio, image and video, in health care, multimedia retrieval and many other applications. Confronted with the challenges of clustering, classification or regression with multi-modality information, it is essential to effectively measure the distance or similarity between objects described with heterogeneous features. Metric learning, aimed at finding a task-oriented distance function, is a hot topic in machine learning. However, most existing algorithms lack efficiency for high-dimensional multi-modality tasks. In this work, we develop an effective and efficient metric learning algorithm for multi-modality data, i.e., Efficient Multi-modal Geometric Mean Metric Learning (EMGMML). The proposed algorithm learns a distinctive distance metric for each view by minimizing the distance between similar pairs while maximizing the distance between dissimilar pairs. To avoid overfitting, the optimization objective is regularized by symmetrized LogDet divergence. EMGMML is very efficient in that there is a closed-form solution for each distance metric. Experimental results show that the proposed algorithm outperforms the state-of-the-art metric learning methods in terms of both accuracy and efficiency.","Metric learning, Multi-modality, Efficiency, Geometric mean",Jianqing Liang and Qinghua Hu and Pengfei Zhu and Wenwu Wang,https://www.sciencedirect.com/science/article/pii/S003132031730095X,https://doi.org/10.1016/j.patcog.2017.02.032,0031-3203,2018,188--198,75,Pattern Recognition,Efficient multi-modal geometric mean metric learning,article,LIANG2018188,Distance Metric Learning for Pattern Recognition
"Small target detection is one of the key techniques in infrared search and tracking applications. When small targets are very dim and of low signal-to-noise ratio, they are very similar to background noise, which usually causes high false alarm rates for conventional methods. To address this problem, we novelly treat the small-dim targets as a special sparse noise component of the complex background noise and adopt Mixture of Gaussians (MoG) with Markov random field (MRF) to model this problem. Firstly, the spatio-temporal patch image is constructed using several consecutive frames to utilize the temporal information of the image sequence. Then, the MRF guided MoG noise model under the Bayesian framework is proposed to model the small target detection problem. After that, by variational Bayesian, the small target component can be effectively separated from complex background noise. Finally, a simple adaptive segmentation method is used to extract small targets. Several series of experiments are done to evaluate the proposed method and the results show that the proposed method is robust for real infrared images with complex background.","Infrared image, Small target detection, Mixture of Gaussians, Markov random field, Variational Bayesian",Chenqiang Gao and Lan Wang and Yongxing Xiao and Qian Zhao and Deyu Meng,https://www.sciencedirect.com/science/article/pii/S0031320317304703,https://doi.org/10.1016/j.patcog.2017.11.016,0031-3203,2018,463--475,76,Pattern Recognition,Infrared small-dim target detection based on Markov random field guided noise modeling,article,GAO2018463,
"In this paper, we mainly propose a semi-supervised local multi-manifold Isomap learning framework by linear embedding, termed SSMM-Isomap, that can apply the labeled and unlabeled training samples to perform the joint learning of neighborhood preserving local nonlinear manifold features and a linear feature extractor. The formulation of SSMM-Isomap aims at minimizing pairwise distances of intra-class points in the same manifold and maximizing the distances over different manifolds. To enhance the performance of nonlinear manifold feature learning, we also incorporate the neighborhood reconstruction error to preserve local topology structures between both labeled and unlabeled samples. To enable our SSMM-Isomap to extract local manifold features from the outside new data, we also add a feature approximation error that correlates manifold features with embedded features by the jointly learnt feature extractor. Thus, the learnt linear extractor can extract the local manifold features from the new data efficiently by direct embedding. To optimize the proposed objective function, two effective schemes are presented, i.e., Scaling by MAjorizing a Complicated Function and Eigen-decomposition. Notice that the comparison of the proposed two solvers is also described. We mainly evaluate SSMM-Isomap for manifold feature learning, data clustering and classification. Extensive simulation results verify the effectiveness of our SSMM-Isomap algorithm, compared with other related feature learning techniques.","Semi-supervised manifold feature extraction, Local multi-manifold Isomap, Linear embedding, Classification",Yan Zhang and Zhao Zhang and Jie Qin and Li Zhang and Bing Li and Fanzhang Li,https://www.sciencedirect.com/science/article/pii/S0031320317303977,https://doi.org/10.1016/j.patcog.2017.09.043,0031-3203,2018,662--678,76,Pattern Recognition,Semi-supervised local multi-manifold Isomap by linear embedding for feature extraction,article,ZHANG2018662,
"This paper proposes a novel method to build a rotation invariant local descriptor by mixed intensity feature histogram. Most existing local descriptors based on intensity ordinal information typically encode only one local feature for each sampling point in the image patch. To address this problem, we proposed a method to encode more than one different local features for each pixel in the image patch and construct a 2D mixed intensity feature histogram, from which our proposed MIFH descriptor is then constructed. Since the rotation invariant coordinate system is adopted, the MIFH descriptor does not need to estimate the reference orientation. In order to evaluate the performance and the robustness of the MIFH with other well-known local descriptors (e.g., SIFT, GLOH, DAISY, HRI-CSLTP, LIOP, MROGH), image matching experiments were carried out on standard Oxford dataset, additional image pairs with complex illumination changes and image sequences with different noises. To further investigate the discriminative ability of the MIFH descriptor, a simple object recognition experiment was conducted on three public datasets. The experimental results demonstrate that our descriptor MIFH exhibits better performance and robustness than other evaluated local descriptors.","Rotation invariant, Reference orientation, Mixed intensity feature histogram, SIFT, Image matching",Yi Yang and Fajie Duan and Ling Ma,https://www.sciencedirect.com/science/article/pii/S0031320317304429,https://doi.org/10.1016/j.patcog.2017.10.035,0031-3203,2018,162--174,76,Pattern Recognition,A rotationally invariant descriptor based on mixed intensity feature histograms,article,YANG2018162,
"Conventional matrix completion methods are generally linear because they assume that the given data are from linear transformations of lower-dimensional latent subspace and the matrix is of low-rank. Therefore, these methods are not effective in recovering incomplete matrices when the data are from non-linear transformations of lower-dimensional latent subspace. Matrices consisting of such nonlinear data are always of high-rank or even full-rank. In this paper, a novel method, called non-linear matrix completion (NLMC), is proposed to recover missing entries of data matrices with non-linear structures. NLMC minimizes the rank (approximated by Schatten p-norm) of a matrix in the feature space given by a non-linear mapping of the data (input) space, where kernel trick is used to avoid carrying out the unknown non-linear mapping explicitly. The proposed NLMC is compared with existing methods on a toy example of matrix completion and real problems including image inpainting and single-/multi-label classification. The experimental results verify the effectiveness and superiority of the proposed method. In addition, the idea of NLMC can be extended to a non-linear rank-minimization framework applicable to other problems such as non-linear denoising.","Matrix completion, Low-rank, Kernel, Schatten -norm, Image inpainting, Single-/multi-label classification, Non-linear denoising",Jicong Fan and Tommy W.S. Chow,https://www.sciencedirect.com/science/article/pii/S0031320317304028,https://doi.org/10.1016/j.patcog.2017.10.014,0031-3203,2018,378--394,77,Pattern Recognition,Non-linear matrix completion,article,FAN2018378,
"In this paper, we model the salient object detection problem under a probabilistic framework encoding the boundary connectivity saliency cue and smoothness constraints into an optimization problem. We show that this problem has a closed form global optimum solution, which estimates the salient object. We further show that along with the probabilistic framework, the proposed method also enjoys a wide range of interpretations, i.e. graph cut, diffusion maps and one-class classification. With an analysis according to these interpretations, we also find that our proposed method provides approximations to the global optimum to another criterion that integrates local/global contrast and large area saliency cues. The proposed unsupervised approach achieves mostly leading performance compared to the state-of-the-art unsupervised algorithms over a large set of salient object detection datasets including around 17k images for several evaluation metrics. Furthermore, the computational complexity of the proposed method is favorable/comparable to many state-of-the-art unsupervised techniques.","Saliency, Salient object detection, Spectral graph cut, Diffusion maps, Probabilistic model, One-class classification",Caglar Aytekin and Alexandros Iosifidis and Moncef Gabbouj,https://www.sciencedirect.com/science/article/pii/S0031320317303734,https://doi.org/10.1016/j.patcog.2017.09.023,0031-3203,2018,359--372,74,Pattern Recognition,Probabilistic saliency estimation,article,AYTEKIN2018359,
"This paper addresses the problems of both general and also fine-grained human action recognition in video sequences. Compared with general human actions, fine-grained action information is more difficult to detect and occupies relatively small-scale image regions. Our work seeks to improve fine-grained action discrimination, while also retaining the ability to perform general action recognition. Our method first estimates human pose and human parts positions in video sequences by extending our recent work on human pose tracking, and crops different scaled patches to obtain richer action information in a variety of different scales of appearance and motion cues. We then utilize a Convolutional Neural Network (CNN) to process each such image patch. Instead of using the output one dimension feature from the full-connection layer, we utilize the outputs of the pooling layer of CNN structure, which contains more spatial information. Then the high dimension of the pooling features is reduced by encoding, to generate the final human action descriptors for classification. Our method reduces feature dimension while also effectively combining appearance and motion information in a unified framework. We have carried out empirical experiments using two publicly available human action datasets, comparing the human action recognition result of our algorithm against six recent state-of-the-art methods from the literature. The results suggest comparatively strong performance of our method.","Human pose, Action recognition, Video understanding",Miao Ma and Naresh Marturi and Yibin Li and Ales Leonardis and Rustam Stolkin,https://www.sciencedirect.com/science/article/pii/S0031320317304788,https://doi.org/10.1016/j.patcog.2017.11.026,0031-3203,2018,506--521,76,Pattern Recognition,Region-sequence based six-stream CNN features for general and fine-grained human action recognition in videos,article,MA2018506,
"On-line handwritten signatures are collected as real-time dynamical signals which are written on collective devices by users. Since individuals have different writing habits, consistent and discriminative features should be selected to distinguish genuine signatures from forged signatures. In this paper, two methods, which are based on full factorial experiment design and optimal orthogonal experiment design, are proposed for selecting discriminative features among candidates. To improve the robustness, consistency of feature is analyzed at first, and more consistent features are selected as candidates for discriminative feature selection. To reduce the influences of fluctuations caused by internal and external writing environments changes before verification, signatures are effectively aligned to their reference templates based on Gaussian mixture model. A modified dynamic time warping with signature curve constraint is presented for verification to improve the efficiency. Comprehensive experiments are implemented based on the data of the open access databases MCYT and SVC2004 Task2. Experimental results verify the effectiveness and robustness of our proposed methods.","On-line signature verification, Discriminative feature selection, Factorial experiment design, Orthogonal experiment design, Signature alignment, Signature curve constraint",Xinghua Xia and Xiaoyu Song and Fangun Luan and Jungang Zheng and Zhili Chen and Xiaofu Ma,https://www.sciencedirect.com/science/article/pii/S0031320317303850,https://doi.org/10.1016/j.patcog.2017.09.033,0031-3203,2018,422--433,74,Pattern Recognition,Discriminative feature selection for on-line signature verification,article,XIA2018422,
"The nearest neighbor method together with the dynamic time warping (DTW) distance is one of the most popular approaches in time series classification. This method suffers from high storage and computation requirements for large training sets. As a solution to both drawbacks, this article extends learning vector quantization (LVQ) from Euclidean spaces to DTW spaces. The proposed generic LVQ scheme uses asymmetric weighted averaging as update rule. We theoretically justify the asymmetric LVQ scheme via subgradient techniques and by the margin-growth principle. In addition, we show that the decision boundary of two prototypes from different classes is piecewise quadratic. Empirical results exhibited superior performance of asymmetric generalized LVQ (GLVQ) over other state-of-the-art prototype generation methods for nearest neighbor classification.","Learning vector quantization, Time series, Dynamic time warping",Brijnesh J. Jain and David Schultz,https://www.sciencedirect.com/science/article/pii/S0031320317304387,https://doi.org/10.1016/j.patcog.2017.10.029,0031-3203,2018,349--366,76,Pattern Recognition,Asymmetric learning vector quantization for efficient nearest neighbor classification in dynamic time warping spaces,article,JAIN2018349,
"Person re-identification (re-id), aiming to search a specific person among a non-overlapping camera network, has attracted plenty of interest in recent years. This task is highly challenging, especially when there exists only single image per person in the database. In this paper, we present an algorithm for learning a Mahalanobis distance for person re-identification. Our method has two distinctive features: (1) to obtain the best separability of the training data, we first minimize the intra-class distances to the most extent by forcing intra-class distances to be zero, and (2) to promote the generalization ability of the learned metric, we then maximize the minimum margin between different classes. Inspired by the simple geometric intuition that a regular simplex maximizes its minimum side length, provided the sum of all side length is fixed, our method, called EquiDistance constrained Metric Learning (EquiDML), applies least-square regression technique to map images of the same person to the same vertex of a regular simplex, and images of different persons to different vertices of a regular simplex. Consequently, under the learned metric, images of the same class are collapsed to a single point, while images of different classes are transformed to be equidistant. This simple motivation is further formulated as a convex optimization problem, solved by the projected gradient descent method and proved to be very effective in person re-identification task. Although it is fairly simple, our method outperforms the state-of-the-art methods on CUHK01, CUHK03, Market1501 and DukeMTMC-reID datasets, and achieves very competitive performance on the widely used VIPeR dataset.","Person re-identification, Metric learning, Equidistance embedding",Jin Wang and Zheng Wang and Chao Liang and Changxin Gao and Nong Sang,https://www.sciencedirect.com/science/article/pii/S0031320317303576,https://doi.org/10.1016/j.patcog.2017.09.014,0031-3203,2018,38--51,74,Pattern Recognition,Equidistance constrained metric learning for person re-identification,article,WANG201838,
"We develop a novel method for the recognition of curvilinear profiles in digital images. The proposed method, semi-automatic for both closed and open planar profiles, essentially consists of a preprocessing step exploiting an edge detection algorithm, and a main step involving the Hough transform technique. In the preprocessing step, a Canny edge detection algorithm is applied in order to obtain a reduced point set describing the profile curve to be reconstructed. Also, to identify in the profile possible sharp points like cusps, we additionally use an algorithm to find the approximated tangent vector of every edge point. In the subsequent main step, we then use a piecewisely defined Hough transform to locally recognize from the point set a low-degree piecewise polynomial curve. The final outcome of the algorithm is thus a spline curve approximating the underlined profile image. The output curve consists of polynomial pieces connected G1 continuously, except in correspondence of the identified cusps, where the order of continuity is only C0, as expected. To illustrate effectiveness and efficiency of the new profile detection technique we present several numerical results dealing with detection of open and closed profiles in images of different type, i.e., medical and photographic images.","Hough transform, Profile recognition, Spline fitting, -continuity, Cusps",Costanza Conti and Lucia Romani and Daniela Schenone,https://www.sciencedirect.com/science/article/pii/S0031320317303692,https://doi.org/10.1016/j.patcog.2017.09.017,0031-3203,2018,64--76,74,Pattern Recognition,Semi-automatic spline fitting of planar curvilinear profiles in digital images using the Hough transform,article,CONTI201864,
"Multilabel segmentation is an important research branch in image segmentation field. In our previous work, Multiphase Multiple Piecewise Constant and Geodesic Active Contour (MMPC-GAC) model was proposed, which can effectively describe multiple objects and background with intensity inhomogeneity. It can be approximately solved with Multiple Layer Graph (MLG) methods. To make the optimization more efficient and limit the approximate error, four-color labeling theorem was further introduced which can limit the MLG within three layers (representing four phases). However, the adopted random four-color labeling method usually provides chaotic color maps with obvious inhomogeneity for those semantic consistent regions. For this case, a new and alternative method named heuristic four-color labeling is proposed in this paper, which aims to generate more reasonable color maps with a global view of the whole image. And compared with the random four-color labeling strategy, the whole iterative algorithm based on our method usually produces better segmentations with faster convergence, particularly for images with clutters and complicated structures. This strategy is a good substitute for random coloring method when the latter produces unsatisfactory messy segmentation. Experiments conducted on public dataset demonstrate the effectiveness of the proposed method.","Image segmentation, Mean shift, Four color theorem, Affinity propagation clustering",Kunqian Li and Wenbing Tao and Xiaobai Liu and Liman Liu,https://www.sciencedirect.com/science/article/pii/S0031320317304260,https://doi.org/10.1016/j.patcog.2017.10.023,0031-3203,2018,69--79,76,Pattern Recognition,Iterative image segmentation with feature driven heuristic four-color labeling,article,LI201869,
"This paper presents an effective approach for the local threshold binarization of degraded document images. We utilize the structural symmetric pixels (SSPs) to calculate the local threshold in neighborhood and the voting result of multiple thresholds will determine whether one pixel belongs to the foreground or not. The SSPs are defined as the pixels around strokes whose gradient magnitudes are large enough and orientations are symmetric opposite. The compensated gradient map is used to extract the SSP so as to weaken the influence of document degradations. To extract SSP candidates with large magnitudes and distinguish the faint characters and bleed-through background, we propose an adaptive global threshold selection algorithm. To further extract pixels with opposite orientations, an iterative stroke width estimation algorithm is applied to ensure the proper size of neighborhood used in orientation judgement. At last, we present a multiple threshold vote based framework to deal with some inaccurate detections of SSP. The experimental results on seven public document image binarization datasets show that our method is accurate and robust compared with many traditional and state-of-the-art document binarization approaches based on multiple evaluation measures.","Document image binarization, Structural symmetry of strokes, Local threshold, Stroke width estimation",Fuxi Jia and Cunzhao Shi and Kun He and Chunheng Wang and Baihua Xiao,https://www.sciencedirect.com/science/article/pii/S0031320317303849,https://doi.org/10.1016/j.patcog.2017.09.032,0031-3203,2018,225--240,74,Pattern Recognition,Degraded document image binarization using structural symmetry of strokes,article,JIA2018225,
"Multi-task learning (MTL) has been proved to improve performance of individual tasks by learning multiple related tasks together. Recently Nonparametric Bayesian Gaussian Process (GP) models have also been adapted to MTL and exhibit enough flexibility due to its non-parametric nature, thus can exempt from the assumption about the probability distributions of variables. To date, there have had two approaches proposed to implement GP-based MTL, i.e., cross-covariance-based and joint feature learning methods. Although successfully applied in scenarios such as face verification and collaborative filtering, these methods have their own drawbacks, for example, the cross-covariance-based method suffers from poor scalability because of the large covariance matrix involved; while the joint feature learning method can just implicitly incorporate relation between tasks, thus leading to a failure in explicitly exploiting the prior knowledge like correlation between tasks, which is crucial for further promoting MTLs. To address both issues, in this paper, we establish a two layer unified framework called Hierarchical Gaussian Process Multi-task Learning (HGPMT) method to jointly learn the latent shared features among tasks and a multi-task model. Furthermore, since the HGPMT does not need to involve the cross-covariance, its computational complexity is much lower. Finally, experimental results on both toy multi-task regression dataset and real datasets demonstrate its superiority in performance of multi-task learning to recently proposed approaches.","GP-LVM, Multi-task learning, Feature learning, Hierarchical model",Ping Li and Songcan Chen,https://www.sciencedirect.com/science/article/pii/S0031320317303746,https://doi.org/10.1016/j.patcog.2017.09.021,0031-3203,2018,134--144,74,Pattern Recognition,Hierarchical Gaussian Processes model for multi-task learning,article,LI2018134,
"Selective image segmentation is an important topic in medical imaging and real applications. In this paper, we propose a weighted variational selective image segmentation model which contains two steps. The first stage is to obtain a smooth approximation related to Mumford-Shah model to the target region in the input image. Using weighted function, the approximation provides a larger value for the target region and smaller values for other regions. In the second stage, we make use of this approximation and perform a thresholding procedure to obtain the object of interest. The approximation can be obtained by the alternating direction method of multipliers and the convergence analysis of the method can be established. Experimental results for medical image selective segmentation are given to demonstrate the usefulness of the proposed method. We also do some comparisons and show that the performance of the proposed method is more competitive than other testing methods.","Selective segmentation, Mumford-Shah model, Thresholding, Medical images, Iterative algorithm",Chunxiao Liu and Michael Kwok-Po Ng and Tieyong Zeng,https://www.sciencedirect.com/science/article/pii/S0031320317304715,https://doi.org/10.1016/j.patcog.2017.11.019,0031-3203,2018,367--379,76,Pattern Recognition,Weighted variational model for selective image segmentation with application to medical images,article,LIU2018367,
"Kernel sparse representation for classification (KSRC) has attracted much attention in pattern recognition community in recent years. Although it has been widely used in many applications such as face recognition, KSRC still has some open problems needed to be addressed. One is that if the training set is of a small scale, KSRC may potentially suffer from lack of training samples when a nonlinear mapping is used to transform the original input data into a high dimensional feature space, which is often accomplished using a kernel-based method. In order to address this problem, this work proposes a scheme that automatically yields a number of new training samples, termed virtual dictionary, from the original training set. We then use the yielded virtual dictionary and the original training set to build the KSRC model. To improve the computational efficiency of KSRC, we exploit the coordinate descend algorithm to solve the KSRC model. Our approach is referred to as kernel coordinate descent based on virtual dictionary (KCDVD). KCDVD is easy to implement and is computationally efficient. Experiments on many face databases show that the proposed algorithm is effective at remedying the problem with small training samples.","Kernel sparse representation for classification (KSRC), Virtual dictionary, Coordinate descend, Face recognition",Zizhu Fan and Da Zhang and Xin Wang and Qi Zhu and Yuanfang Wang,https://www.sciencedirect.com/science/article/pii/S0031320317303989,https://doi.org/10.1016/j.patcog.2017.10.001,0031-3203,2018,1--13,76,Pattern Recognition,Virtual dictionary based kernel sparse representation for face recognition,article,FAN20181,
"We consider the statistical problem of learning a common source of variability in data which are synchronously captured by multiple sensors, and demonstrate that Siamese neural networks can be naturally applied to this problem. This approach is useful in particular in exploratory, data-driven applications, where neither a model nor label information is available. In recent years, many researchers have successfully applied Siamese neural networks to obtain an embedding of data which corresponds to a âsemantic similarityâ. We present an interpretation of this âsemantic similarityâ as learning of equivalence classes. We demonstrate the ability of Siamese networks to learn common variability in a range of experiments on synthetic and real-world data, and demonstrate the potential of Siamese networks to provide new leads for data-driven research through unsupervised learning in cancer data.","Similarity learning, Representation learning, Unsupervised learning, Siamese networks, Common variable",Uri Shaham and Roy R. Lederman,https://www.sciencedirect.com/science/article/pii/S0031320317303588,https://doi.org/10.1016/j.patcog.2017.09.015,0031-3203,2018,52--63,74,Pattern Recognition,Learning by coincidence: Siamese networks and common variable learning,article,SHAHAM201852,
"A novel deep learning method for face sketch synthesis is proposed in this work. It builds a lightweight neural network which contains two convolutional layers, a pooling layer and a multilayer perceptron convolutional layer to learn a mapping from face photos to sketches. Unlike conventional example-based methods which need to solve complex optimization problems, the proposed method only computes convolution and pooling operations, hence significantly improves the synthesis efficiency. Besides, due to the global feature extraction of the convolutional layer, it achieves more continuous and faithful facial contours. Experiments on three benchmark datasets demonstrate that compared with several state-of-the-arts, the proposed method achieves highly competitive numerical results and is more robust to illumination and expression variations.","Face sketch synthesis, Convolutional neural networks, Sparse coding",Licheng Jiao and Sibo Zhang and Lingling Li and Fang Liu and Wenping Ma,https://www.sciencedirect.com/science/article/pii/S0031320317304314,https://doi.org/10.1016/j.patcog.2017.10.025,0031-3203,2018,125--136,76,Pattern Recognition,A modified convolutional neural network for face sketch synthesis,article,JIAO2018125,
"A nonparametric real-time and high-quality moving object detection strategy in a GPU is proposed. To improve the quality of the results in sequences where the moving objects and the background have similar appearance, not only the background but also the foreground is modelled. Both models are constructed from spatio-temporal reference data to reduce false detections due to small displacements of the background, and to take into consideration the natural displacements of the foreground. To avoid using kernels with too large spatial widths, the spatial positions of the foreground reference data are updated at each new frame using a particle filter that is able to deal with an unknown and variable amount of regions. Additionally, an automatic selection of regions of interest is carried out, which allows reducing drastically the computational cost of both foreground and background models. The proposed strategy has been validated using three databases containing many challenges for motion detection and the results have been compared to those of other state-of-the-art approaches.","Foreground segmentation, Background subtraction, Nonparametric modelling, Parallel processing, Real-time GPU",Daniel BerjÃ³n and Carlos Cuevas and Francisco MorÃ¡n and Narciso GarcÃ­a,https://www.sciencedirect.com/science/article/pii/S0031320317303503,https://doi.org/10.1016/j.patcog.2017.09.009,0031-3203,2018,156--170,74,Pattern Recognition,Real-time nonparametric background subtraction with tracking-based foreground update,article,BERJON2018156,
"Discovering and tracking topics in a text stream has attracted the interests of many researchers. A limitation of most existing methods is that they organize topics in flat structures. Topic hierarchy could reveal the potential relations between topics, which can help to find high quality topics when analyzing the text stream. In this paper, a hierarchical online non-negative matrix factorization method (HONMF) is proposed to generate topic hierarchies from text streams. The proposed method can dynamically adjust the topic hierarchy to adapt to the emerging, evolving, and fading processes of the topics. In the experiment, HONMF is evaluated under a variety of metrics. Compared with the baseline methods, our method can achieve better performance with competitive time efficiency.","Topic modeling, Hierarchical matrix factorization, Online learning",Ding Tu and Ling Chen and Mingqi Lv and Hongyu Shi and Gencai Chen,https://www.sciencedirect.com/science/article/pii/S0031320317304557,https://doi.org/10.1016/j.patcog.2017.11.002,0031-3203,2018,203--214,76,Pattern Recognition,Hierarchical online NMF for detecting and tracking topic hierarchies in a text stream,article,TU2018203,
"Exact fingertip positions are of particular importance to the fingertip-based humanâcomputer interaction. We build a multi-objective optimization model for the problem of fingertip localization, and present a method to solve the above model based on evolutionary algorithms. When building the model, we take the positions of a series of pixels as the decision variable, the shape of the hand-edge curve corresponding to each of the pixels as one objective function, and the distance between each of the pixels and the gravity center of the palm as the other objective function. In addition, based on the correlation among the positions of pixels of the fingertip regions, we present a multi-objective estimation of distribution algorithm to solve the model so as to obtain the best pixel set, thus gaining the fingertip positions. The experimental results demonstrate the effectiveness of the proposed model and algorithm.","Fingertip localization, Multi-objective optimization, Estimation of distribution algorithm, Probability distribution model",Dunwei Gong and Ke Liu,https://www.sciencedirect.com/science/article/pii/S003132031730345X,https://doi.org/10.1016/j.patcog.2017.09.001,0031-3203,2018,385--405,74,Pattern Recognition,A multi-objective optimization model and its evolution-based solutions for the fingertip localization problem,article,GONG2018385,
"Multi-label classification associates an unseen instance with multiple relevant labels. In recent years, a variety of methods have been proposed to handle the multi-label problems. Classifier chains is one of the most popular multi-label methods because of its efficiency and simplicity. In this paper, we consider to optimize classifier chains from the viewpoint of conditional likelihood maximization. In the proposed unified framework, classifier chains can be optimized in either or both of two aspects: label correlation modeling and multi-label feature selection. In this paper we show that previous classifier chains algorithms are specified in the unified framework. In addition, previous information theoretic multi-label feature selection algorithms are specified with different assumptions on the feature and label spaces. Based on these analyses, we propose a novel multi-label method, k-dependence classifier chains with label-specific features, and demonstrate the effectiveness of the method.","Multi-label classification, Classifier chains, Conditional likelihood maximization, -dependence Bayesian network, Multi-label feature selection",Lu Sun and Mineichi Kudo,https://www.sciencedirect.com/science/article/pii/S0031320317303862,https://doi.org/10.1016/j.patcog.2017.09.034,0031-3203,2018,503--517,74,Pattern Recognition,Optimization of classifier chains via conditional likelihood maximization,article,SUN2018503,
"In this study, we address the problem of similar local motions that create confusion within different group activities. To reduce the influences of motions, we propose a discriminative group context feature (DGCF) that considers prominent sub-events. Moreover, we adopt a gated recurrent unit (GRU) model that can learn temporal changes in a sequence. In real-world scenarios, people perform activities with different temporal lengths. The GRU model handles an arbitrary length of data for training with nonlinear hidden units in the network. However, when we use a deep neural network model, data scarcity causes overfitting problems. Data augmentation methods for images are ineffective for trajectory data augmentation. Thus, we also propose a method for trajectory augmentation. We evaluate the effectiveness of the proposed method on three datasets. In our experiments on each dataset, we show that the proposed method outperforms the competing state-of-the-art methods for group activity recognition.","Group activity recognition, Sequence modeling, Recurrent neural network, Gated recurrent unit, Data augmentation, Video surveillance",Pil-Soo Kim and Dong-Gyu Lee and Seong-Whan Lee,https://www.sciencedirect.com/science/article/pii/S0031320317304430,https://doi.org/10.1016/j.patcog.2017.10.037,0031-3203,2018,149--161,76,Pattern Recognition,Discriminative context learning with gated recurrent unit for group activity recognition,article,KIM2018149,
"An exemplar is an observation that represents a group of similar observations. Exemplars from data are examined to divide entire heterogeneous data into several homogeneous subgroups, wherein each subgroup is represented by an exemplar. With its inherent sparsity, an exemplar-based learning model provides a parsimonious model to represent or cluster large-scale data. A novel exemplar learning method using one-class Gaussian process (GP) regression is proposed in this study. The proposed method constructs data distribution support from one-class GP regression using automatic relevance determination prior and heterogeneous GP noise. Exemplars that correspond to the basis vectors of the constructed support function are then automatically located during the training process. The proposed method is applied to various data sets to examine its operability, characteristics of data representation, and cluster analysis. The exemplars of some real data generated by the proposed method are also reported.","Representative exemplars, One class Gaussian process regression, Support-based clustering, Automatic relevance determination, Kernel methods",Youngdoo Son and Sujee Lee and Saerom Park and Jaewook Lee,https://www.sciencedirect.com/science/article/pii/S0031320317303461,https://doi.org/10.1016/j.patcog.2017.09.002,0031-3203,2018,185--197,74,Pattern Recognition,Learning representative exemplars using one-class Gaussian process regression,article,SON2018185,
"We propose a novel local color texture descriptor called local binary pattern for color images (LBPC). The proposed descriptor uses a plane to threshold color pixels in the neighborhood of a local window into two categories. To boost the discriminative power of the proposed LBPC operator, local binary patterns of the hue component in the HSI color space, called the local binary pattern of the hue (LBPH) is derived. Further, LBPC, LBPH are fused to derive LBPC+LBPH which when combined with the color histogram (CH) of the hue component results in an effective image retrieval method LBPC+LBPH+CH. The uniform patterns of the two proposed descriptors ULBPC and ULBPH are combined to yield another low dimension local color descriptor ULBPC+ULBPH+CH which provides a good tradeoff between retrieval accuracy and speed. Detailed experiments conducted on Wang, Holidays, Corel-5K and Corel-10K datasets demonstrate that the proposed low dimension descriptors LBPC+LBPH+CH, and ULBPC+ULBPH+CH outperform the state-of-the-art color texture descriptors in terms of retrieval accuracy and speed.","Local binary pattern (LBP), Local binary pattern for color images (LBPC), Local binary pattern of hue component (LBPH), Local color texture, Image retrieval",Chandan Singh and Ekta Walia and Kanwal Preet Kaur,https://www.sciencedirect.com/science/article/pii/S0031320317304272,https://doi.org/10.1016/j.patcog.2017.10.021,0031-3203,2018,50--68,76,Pattern Recognition,Color texture description with novel local binary patterns for effective image retrieval,article,SINGH201850,
"Ensemble methods have shown to be more effective than monolithic classifiers, in particular when diversity holds among their components. How to enforce diversity in classifier ensembles has received much attention from machine learning researchers, yielding a variety of different techniques and algorithms. In this paper, a novel algorithm for ensemble classifiers is proposed, in which ensemble components are trained with focus on different regions of the sample space. In so doing, diversity is mainly a consequence of the intention to limit the scope of base classifiers. The algorithm proposed in this paper shares roots with several ensemble paradigms, in particular with random forests, as it generates forests of decision trees as well. As decision trees are trained with focus on specific subsets of the sample space, the resulting ensemble is in fact a forest of âlocalâ trees. Comparative experimental results highlight that, on average, these ensembles perform better than other relevant kinds of ensemble classifiers, including random forests.","Classifier ensembles, Random forests, Mixture of experts",Giuliano Armano and Emanuele Tamponi,https://www.sciencedirect.com/science/article/pii/S0031320317304727,https://doi.org/10.1016/j.patcog.2017.11.017,0031-3203,2018,380--390,76,Pattern Recognition,Building forests of local trees,article,ARMANO2018380,
"This paper presents a novel hyperspectral image (HSI) classification method to effectively exploit the 3D spectral-spatial information via superpixel-based 3D deep neural networks (3D DNNs). Superpixel can represent the structure of HSI with adaptive sizes and shapes, and therefore, it is incorporated into 3D DNNs to improve the classification performance, especially for noisy classification and boundary misclassification. First, a spatial feature image via superpixel is constructed to increase the spectral-spatial similarity and diversity. Second, a 3D superpixel-based sample filling method is designed to solve the misclassification problem of boundaries. Third, a 3D recurrent convolutional networks (3D RCNNs) are designed to further exploit spatial continuity and suppress noisy prediction. Experimental results on real HSI datasets demonstrate the superiority of the proposed method over several well-known methods in both visual appearance and classification accuracy.","Hyperspectral image classification, Superpixel, 3D deep neural networks",Cheng Shi and Chi-Man Pun,https://www.sciencedirect.com/science/article/pii/S0031320317303515,https://doi.org/10.1016/j.patcog.2017.09.007,0031-3203,2018,600--616,74,Pattern Recognition,Superpixel-based 3D deep neural networks for hyperspectral image classification,article,SHI2018600,
"Multilayer network is a structure commonly used to describe and model the complex interaction between sets of entities/nodes. A three-layer example is the author-paper-word structure in which authors are linked by co-author relation, papers are linked by citation relation, and words are linked by semantic relation. Network embedding, which aims to project the nodes in the network into a relatively low-dimensional space for latent factor analysis, has recently emerged as an effective method for a variety of network-based tasks, such as collaborative filtering and link prediction. However, existing studies of network embedding both focus on the single-layer network and overlook the structural properties of the network, e.g., the degree distribution and communities, which are significant for node characterization, such as the preferences of users in a social network. In this paper, we propose four multilayer network embedding algorithms based on Nonnegative Matrix Factorization (NMF) with consideration given to four structural properties: whole network (NNMF), community (CNMF), degree distribution (DNMF), and max spanning tree (TNMF). Experiments on synthetic data show that the proposed algorithms are able to preserve the desired structural properties as designed. Experiments on real-world data show that multilayer network embedding improves the accuracy of document clustering and recommendation, and the four embedding algorithms corresponding to the four structural properties demonstrate the differences in performance on these two tasks. These results can be directly used in document clustering and recommendation systems.","Multilayer network, Network embedding, Nonnegative matrix factorization",Jie Lu and Junyu Xuan and Guangquan Zhang and Xiangfeng Luo,https://www.sciencedirect.com/science/article/pii/S0031320317304569,https://doi.org/10.1016/j.patcog.2017.11.004,0031-3203,2018,228--241,76,Pattern Recognition,Structural property-aware multilayer network embedding for latent factor analysis,article,LU2018228,
"Motivated by researching new image texture modeling that improves state-of-the-art LBP variants and non-LBP descriptors, this paper proposes a novel approach for constructing local image descriptors, which are suitable for histogram based image representation. Instead of heuristic code constructions, the proposed approach is based on local concave-and-convex characteristics, which have high ability to extract discriminative and stable texture representation. Different from the majority of descriptors that only encode relationships between the pixels in doublets around central pixel (within 3Â â¯Ãâ¯Â 3 neighborhood), the proposed approach encodes relationships between the pixels in triplets by including the central pixel in the modeling. We build two distinct descriptors by dividing local features into two distinct groups, i.e., local concave and convex microstructure patterns (LCvMSP and LCxMSP), according to relationships between the pixels inside the triplets, formed along closed path around the central pixel of a 3â¯Ãâ¯3-grayscale image patch. To make the descriptors more insensitive to noise and invariant to monotonic gray scale transformation, two supplementary triplets are added in the modeling. These triplets are formed using the central pixel and four virtual pixels set to the median of the grey-scale values of the 3â¯Ãâ¯3 neighbourhood and the whole image and the average local and global gray levels respectively. The histograms obtained from the single scale descriptors LCvMSP and LCxMSP are concatenated together to build multi-scale histogram feature vector referred to as local concave-and-convex micro-structure pattern (LCCMSP), that is expected to better represent salient local texture structure. We evaluated the effectiveness of the proposed methods on thirteen challenging representative widely-used texture datasets, and found that the proposed LCvMSP, LCxMSP and LCCMSP operators achieve performances that are competitive or better than a large number of recent most promising state-of- the-art LBP variants and non-LBP descriptors. Statistical comparison based on Wilcoxon signed rank test demonstrated that the proposed methods are the top three over all the tested datasets.","LBP, Local concave-and-convex characteristics, LCvMSP, LCxMSP, LCCMSP, Feature extraction, Texture classification",Y. {El merabet} and Y. Ruichek,https://www.sciencedirect.com/science/article/pii/S0031320317304600,https://doi.org/10.1016/j.patcog.2017.11.005,0031-3203,2018,303--322,76,Pattern Recognition,Local Concave-and-Convex Micro-Structure Patterns for texture classification,article,ELMERABET2018303,
"In this paper, we construct a least squares version of the recently proposed twin bounded support vector machine (TBSVM) for binary classification. As a valid classification tool, TBSVM attempts to seek two non-parallel planes that can be produced by solving a pair of quadratic programming problems (QPPs), but this is time-consuming. Here, we solve two systems of linear equations rather than two QPPs to avoid this deficiency. Furthermore, the distance in least squares TBSVM (LSTBSVM) is measured by L2-norm, but L1-norm distance is usually regarded as an alternative to L2-norm to improve model robustness in the presence of outliers. Inspired by the advantages of least squares twin support vector machine (LSTWSVM), TBSVM and L1-norm distance, we propose a LSTBSVM based on L1-norm distance metric for binary classification, termed as L1-LSTBSVM, which is specially designed for suppressing the negative effect of outliers and improving computational efficiency in large datasets. Then, we design a powerful iterative algorithm to solve the L1-norm optimal problems, and it is easy to implement and its convergence to an optimum solution is theoretically ensured. Finally, the feasibility and effectiveness of L1-LSTBSVM are validated by extensive experimental results on both UCI datasets and artificial datasets.","L1-LSTBSVM, TBSVM, L1-norm distance, Outliers",He Yan and Qiaolin Ye and Tianâan Zhang and Dong-Jun Yu and Xia Yuan and Yiqing Xu and Liyong Fu,https://www.sciencedirect.com/science/article/pii/S0031320317303874,https://doi.org/10.1016/j.patcog.2017.09.035,0031-3203,2018,434--447,74,Pattern Recognition,Least squares twin bounded support vector machines based on L1-norm distance metric for classification,article,YAN2018434,
"A two-way subspace weighting partitional co-clustering method TWCC is proposed. In this method, two types of subspace weights are introduced to simultaneously weight the data in two ways, i.e., columns on row clusters and rows on column clusters. An objective function that uses the two types of weights in the distance function to determine the co-clusters of data is defined, and an iterative TWCC co-clustering algorithm to optimize the objective function is proposed, in which the two types of subspace weights are automatically computed. A series of experiments on both synthetic and real-life data were conducted to investigate the properties of TWCC, compare the two-way clustering results of TWCC with those of eight co-clustering algorithms, and compare one-way clustering results of TWCC with those of six clustering algorithms. The results have shown that TWCC is robust and effective for large high-dimensional data.","Data mining, Co-clustering, Subspace clustering, Variable weighting",Xiaojun Chen and Min Yang and Joshua {Zhexue Huang} and Zhong Ming,https://www.sciencedirect.com/science/article/pii/S0031320317304326,https://doi.org/10.1016/j.patcog.2017.10.026,0031-3203,2018,404--415,76,Pattern Recognition,TWCC: Automated Two-way Subspace Weighting Partitional Co-Clustering,article,CHEN2018404,
"Person re-identificationÂ (Re-ID) usually suffers from noisy samples with background clutter and mutual occlusion, which makes it extremely difficult to distinguish different individuals across the disjoint camera views. In this paper, we propose a novel deep self-paced learningÂ (DSPL) algorithm to alleviate this problem, in which we apply a self-paced constraint and symmetric regularization to help the relative distance metric training the deep neural network, so as to learn the stable and discriminative features for person Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive the adaptive weights to samples based on both the training loss and model age. As a result, the high-confidence fidelity samples will be emphasized and the low-confidence noisy samples will be suppressed at early stage of the whole training process. Such a learning regime is naturally implemented under a self-paced learningÂ (SPL) framework, in which samples weights are adaptively updated based on both model age and sample loss using an alternative optimization method. Secondly, we introduce a symmetric regularizer term to revise the asymmetric gradient back-propagation derived by the relative distance metric, so as to simultaneously minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Finally, we build a part-based deep neural network, in which the features of different body parts are first discriminately learned in the lower convolutional layers and then fused in the higher fully connected layers. Experiments on several benchmark datasets have demonstrated the superior performance of our method as compared with the state-of-the-art approaches.","Person re-identification, Convolutional neural network, Self-paced learning, Metric learning",Sanping Zhou and Jinjun Wang and Deyu Meng and Xiaomeng Xin and Yubing Li and Yihong Gong and Nanning Zheng,https://www.sciencedirect.com/science/article/pii/S003132031730403X,https://doi.org/10.1016/j.patcog.2017.10.005,0031-3203,2018,739--751,76,Pattern Recognition,Deep self-paced learning for person re-identification,article,ZHOU2018739,
"This paper describes a framework for Dynamic Classifier Selection (DCS) whose novelty resides in its use of features that address the difficulty posed by the classification problem in terms of orienting both pool generation and classifier selection. The classification difficulty is described by meta-features estimated from problem data using complexity measures. Firstly, these features are used to drive the classifier pool generation expecting a better coverage of the problem space, and then, a dynamic classifier selection based on similar features estimates the ability of the classifiers to deal with the test instance. The rationale here is to dynamically select a classifier trained on a subproblem (training subset) having a similar level of difficulty as that observed in the neighborhood of the test instance defined in a validation set. A robust experimental protocol based on 30 datasets, and considering 20 replications, has confirmed that a better understanding of the classification problem difficulty may positively impact the performance of a DCS. For the pool generation method, it was observed that in 126 of 180 experiments (70.0%) adopting the proposed pool generator allowed an improvement of the accuracy of the evaluated DCS methods. In addition, the main results from the proposed framework, in which pool generation and classifier selection are both based on problem difficulty features, are very promising. In 165 of 180 experiments (91.6%), it was also observed that the proposed DCS framework based on the problem difficulty achieved a better classification accuracy when compared to 6 well known DCS methods in the literature.","Multiple classifier systems, Classifier pool generation, Dynamic classifiers selection, Classification problem difficulty",AndrÃ© L. Brun and Alceu S. Britto and Luiz S. Oliveira and Fabricio Enembreck and Robert Sabourin,https://www.sciencedirect.com/science/article/pii/S0031320317304442,https://doi.org/10.1016/j.patcog.2017.10.038,0031-3203,2018,175--190,76,Pattern Recognition,A framework for dynamic classifier selection oriented by the classification problem difficulty,article,BRUN2018175,
"Fine-grained image recognition is a challenging computer vision problem, due to the small inter-class variations caused by highly similar subordinate categories, and the large intra-class variations in poses, scales and rotations. In this paper, we prove that selecting useful deep descriptors contributes well to fine-grained image recognition. Specifically, a novel Mask-CNN model without the fully connected layers is proposed. Based on the part annotations, the proposed model consists of a fully convolutional network to both locate the discriminative parts (e.g., head and torso), and more importantly generate weighted object/part masks for selecting useful and meaningful convolutional descriptors. After that, a three-stream Mask-CNN model is built for aggregating the selected object- and part-level descriptors simultaneously. Thanks to discarding the parameter redundant fully connected layers, our Mask-CNN has a small feature dimensionality and efficient inference speed by comparing with other fine-grained approaches. Furthermore, we obtain a new state-of-the-art accuracy on two challenging fine-grained bird species categorization datasets, which validates the effectiveness of both the descriptor selection scheme and the proposed Mask-CNN model.","Fine-grained image recognition, Deep descriptor selection, Part localization",Xiu-Shen Wei and Chen-Wei Xie and Jianxin Wu and Chunhua Shen,https://www.sciencedirect.com/science/article/pii/S0031320317303990,https://doi.org/10.1016/j.patcog.2017.10.002,0031-3203,2018,704--714,76,Pattern Recognition,Mask-CNN: Localizing parts and selecting descriptors for fine-grained bird species categorization,article,WEI2018704,
"Utilizing unlabeled data to train deep neural networks (DNNs) is a crucial but challenging task. In this paper, we propose an end-to-end approach to tackle this problem with consistent inference of latent representations. Specifically, each unlabeled data point is considered as a seed to generate a set of latent labeled data points by adding various random disturbances or transformations. Under the expectation maximization framework, DNNs can be trained in an unsupervised way by minimizing the distances between the data points with the same latent representations. Furthermore, several variants of our approach can be derived by applying regularized and sparse constraints during optimization. Theoretically, the convergence of the proposed method and its variants are fully analyzed. Experimental results show that the proposed approach can significantly improve the performance on various tasks, including image classification and clustering. Such results also indicate that our method can guide DNNs to learn more invariant feature representations in comparison with traditional unsupervised methods.","Deep unsupervised learning, Consistent inference of latent representations",Jianlong Chang and Lingfeng Wang and Gaofeng Meng and Shiming Xiang and Chunhong Pan,https://www.sciencedirect.com/science/article/pii/S0031320317304284,https://doi.org/10.1016/j.patcog.2017.10.022,0031-3203,2018,438--453,77,Pattern Recognition,Deep unsupervised learning with consistent inference of latent representations,article,CHANG2018438,
"Dimensionality reduction has been proven to be efficient in preparing high dimensional data for various tasks in machine learning. As supervised dimensionality reduction methods such as Fisher discriminant analysis (FDA) and local Fisher discriminant analysis (LFDA) tend to suffer from overfitting when only a small number of labeled samples are available, the abundant unlabeled samples could be helpful in finding a better embedding space. However, applying discriminant analysis on unlabeled data is challenging since we do not have labels for unlabeled data. In this paper, we propose a semi-supervised Semi-Supervised Local Fisher Discriminant Analysis (SSLFDA) using pseudo labels, aiming to perform discriminant analysis on both labeled and unlabeled samples. SSLFDA makes use of pseudo labels, learned from the Dirichlet process mixture model (DPMM) based clustering algorithm, to enable local Fisher discriminant analysis on unlabeled data. In addition, a kernel extension of SSLFDA is derived for non-linear dimensionality reduction. We present experimental results with real hyperspectral data to show that our method provides better classification performance compared to other existing dimensionality reduction methods.","Dimensionality reduction, Semi-supervised learning, Dirichlet process mixture model, Hyperspectral data classification",Hao Wu and Saurabh Prasad,https://www.sciencedirect.com/science/article/pii/S0031320317303473,https://doi.org/10.1016/j.patcog.2017.09.003,0031-3203,2018,212--224,74,Pattern Recognition,Semi-supervised dimensionality reduction of hyperspectral imagery using pseudo-labels,article,WU2018212,
"Despite the great advances in face-related works in recent years, face recognition across age remains a challenging problem. The traditional approaches to this problem usually include two basic steps: feature extraction and the application of a distance metric, sometimes common space projection is also involved. On the one hand, handling these steps separately ignores the interactions of these components, and on the other hand, the fixed-distance threshold of measurement affects the modelâs robustness. In this paper, we present a novel distance metric optimization driven learning approach that integrates these traditional steps via a deep convolutional neural network, which learns feature representations and the decision function in an end-to-end way. Given the labelled training images, we first generate a large number of pairs with a certain proportion of matched and unmatched pairs. For matched pairs, we try to select as many different age instances as possible for each person to learn the identification information that is not affected by age. Then, taking these pairs as input, we aim to enlarge the differences between the unmatched pairs while reducing the variations between the matched pairs, and we update the model parameters by using the mini-batch stochastic gradient descent (SGD) algorithm. Specifically, the distance matrix is used as the top fully connected layer, and the bottom layers representing the image features are integrated with it seamlessly. Thus, the image features and the distance metric can be optimized simultaneously by backward propagation. In particular, we introduce several training strategies to reduce the computational cost and overcome insufficient memory capacity. We evaluate our method on three tasks: age-invariant face identification on the MORPH database, age-invariant face retrieval on the CACD database and age-invariant face verification on CACD-VS database. The experimental results demonstrate the effectiveness of our approach.","Age invariant, Face recognition, Distance metric, Deep CNN, Joint learning",Ya Li and Guangrun Wang and Lin Nie and Qing Wang and Wenwei Tan,https://www.sciencedirect.com/science/article/pii/S0031320317304041,https://doi.org/10.1016/j.patcog.2017.10.015,0031-3203,2018,51--62,75,Pattern Recognition,Distance metric optimization driven convolutional neural network for age invariant face recognition,article,LI201851,Distance Metric Learning for Pattern Recognition
"The recent increasing availability of fine-grained electrical consumption data allows the exploitation of Pattern Recognition techniques to characterize and analyse the behaviour of energy customers. The Pattern Recognition analysis is typically performed at group level, i.e. with the aim of discovering, via clustering techniques, groups of users with a coherent behaviour â this being useful, for example, for targeted pricing or collective energy purchasing. In this paper we took a step forward along this direction, investigating the possibility of discriminating the behaviours of single users â i.e., in a biometrics sense. This aspect has not been properly addressed and would pave the way to crucial operations, such as the derivation of alternative advertising schemes based on behavioural targeting. To investigate the uniqueness of the load profiles (i.e. the daily consumption of electrical energy), in our study we used the raw data (the original energy consumption time series) as well as different types of features such as frequency coefficients and normalized load shape indexes, together with various classification schemes. Results obtained on two real world datasets suggest that the load profile does contain significant distinctive information about the single user.","Energy market, Load profile, Biometrics, Classification, Pre-processing",M. Bicego and A. Farinelli and E. Grosso and D. Paolini and S.D. Ramchurn,https://www.sciencedirect.com/science/article/pii/S0031320317303904,https://doi.org/10.1016/j.patcog.2017.09.039,0031-3203,2018,317--325,74,Pattern Recognition,On the distinctiveness of the electricity load profile,article,BICEGO2018317,
"3D Morphable Face Models (3DMM) have been used in pattern recognition for some time now. They have been applied as a basis for 3D face recognition, as well as in an assistive role for 2D face recognition to perform geometric and photometric normalisation of the input image, or in 2D face recognition system training. The statistical distribution underlying 3DMM is Gaussian. However, the single-Gaussian model seems at odds with reality when we consider different cohorts of data, e.g.Â Black and Chinese faces. Their means are clearly different. This paper introduces the Gaussian Mixture 3DMM (GM-3DMM) which models the global population as a mixture of Gaussian subpopulations, each with its own mean. The proposed GM-3DMM extends the traditional 3DMM naturally, by adopting a shared covariance structure to mitigate small sample estimation problems associated with data in high dimensional spaces. We construct a GM-3DMM, the training of which involves a multiple cohort dataset, SURREY-JNU, comprising 942 3D face scans of people with mixed backgrounds. Experiments in fitting the GM-3DMM to 2D face images to facilitate their geometric and photometric normalisation for pose and illumination invariant face recognition demonstrate the merits of the proposed mixture of Gaussians 3D face model.","Gaussian-mixture model, 3D morphable model, 3D face reconstruction, Face model fitting, Face recognition",Paul Koppen and Zhen-Hua Feng and Josef Kittler and Muhammad Awais and William Christmas and Xiao-Jun Wu and He-Feng Yin,https://www.sciencedirect.com/science/article/pii/S0031320317303527,https://doi.org/10.1016/j.patcog.2017.09.006,0031-3203,2018,617--628,74,Pattern Recognition,Gaussian mixture 3D morphable face model,article,KOPPEN2018617,
"Biometric key generation based on facial features is far more complicated than face recognition, the problem being that there is no relevant information saved in databases for matching facial features in the key generation system, contrary to the face recognition system. In this paper, first, we develop an efficient unified framework for generating stable, robust and secure cryptography keys based on facial features, without the need to save information related to facial features in the database. Second, the facial features are extracted using a proposed equalized local binary pattern which shows promising results when simulated on standard face databases. Third, to cater for variations and provide flexibility in error tolerance, we propose a quantization scheme which not only cater for the variations, it also aids in providing security and reducing the size of the features. Fourth, a secure key generation mechanism is developed based on the facial features in which keys can be periodically updated. Fifth, the robustness and security of the generated keys are evaluated on a set of standard statistical tests comprising three requirements: randomness, weak biometric privacy and strong biometric privacy. Lastly, comparing our approach with several state-of-the-art methods, it exhibited superior performance.","Equalized local binary pattern, Facial features, Biometric key generation, Quantization, Recognition rate, Statistical analysis and robustness",Amir Anees and Yi-Ping Phoebe Chen,https://www.sciencedirect.com/science/article/pii/S0031320317304739,https://doi.org/10.1016/j.patcog.2017.11.018,0031-3203,2018,289--305,77,Pattern Recognition,Discriminative binary feature learning and quantization in biometric key generation,article,ANEES2018289,
"Sensor pattern noise (SPN) is an inherent fingerprint of imaging devices, which provides an effective way for source camera identification (SCI). Although SPNs extracted from large image blocks usually yield high identification accuracy, their high dimensionality would incur a high computational cost in the matching stage, consequently hindering many applications that require efficient camera matchings. In this work, we employ and evaluate the concept of principal component analysis (PCA) de-noising in SCI tasks. Based on this concept, we present a framework that formulates a compact SPN representation. To enhance the de-noising effect, we introduce a training set construction procedure that minimizes the impact of various interfering artifacts, which is especially useful in some challenging cases, e.g., when only textured reference images are available. To further boost the SCI performance, a novel approach based on linear discriminant analysis (LDA) is adopted to extract more discriminant SPN features. To evaluate our methods, extensive experiments are conducted on the Dresden image database. The results indicate that the proposed framework can serve as an effective post-processing procedure, which not only boosts the performance, but also greatly reduces the computational cost in the matching phase.","Image forensics, Source camera identification (SCI), Sensor pattern noise (SPN), PCA de-noising",Ruizhe Li and Chang-Tsun Li and Yu Guan,https://www.sciencedirect.com/science/article/pii/S0031320317303801,https://doi.org/10.1016/j.patcog.2017.09.027,0031-3203,2018,556--567,74,Pattern Recognition,Inference of a compact representation of sensor fingerprint for source camera identification,article,LI2018556,
"Portfolio optimization (PO) has been catching more and more attention in the artificial intelligence and the machine learning communities. In this paper, we propose a novel Trend Representation based Log-density Regularization (TRLR) system for portfolio optimization. Its novelty falls into two aspects. First, it introduces a log-density regularization to the increasing factor of portfolio, which is seldom addressed by previous PO systems. It reflects a relationship between the portfolio and the price relative at an equilibrium point. Second, TRLR exploits a novel trend representation by taking the time variable as regressor in a weighted ridge regression, hence TRLR captures price trend patterns effectively. Extensive experiments conducted on 5 benchmark datasets from real-world financial markets demonstrate that TRLR achieves significantly better performance than other state-of-the-art strategies and runs fast, which shows its effectiveness and efficiency for large-scale applications.","Trend representation, Log-density regularization, Ridge regression, Portfolio optimization",Pei-Yi Yang and Zhao-Rong Lai and Xiaotian Wu and Liangda Fang,https://www.sciencedirect.com/science/article/pii/S0031320317304338,https://doi.org/10.1016/j.patcog.2017.10.024,0031-3203,2018,14--24,76,Pattern Recognition,Trend representation based log-density regularization system for portfolio optimization,article,YANG201814,
"Recently, deep learning has achieved great success in visual tracking. The goal of this paper is to review the state-of-the-art tracking methods based on deep learning. First, we introduce the background of deep visual tracking, including the fundamental concepts of visual tracking and related deep learning algorithms. Second, we categorize the existing deep-learning-based trackers into three classes according to network structure, network function and network training. For each categorize, we explain its analysis of the network perspective and analyze papers in different categories. Then, we conduct extensive experiments to compare the representative methods on the popular OTB-100, TC-128 and VOT2015 benchmarks. Based on our observations, we conclude that: (1) The usage of the convolutional neural network (CNN) model could significantly improve the tracking performance. (2) The trackers using the convolutional neural network (CNN) model to distinguish the tracked object from its surrounding background could get more accurate results, while using the CNN model for template matching is usually faster. (3) The trackers with deep features perform much better than those with low-level hand-crafted features. (4) Deep features from different convolutional layers have different characteristics and the effective combination of them usually results in a more robust tracker. (5) The deep visual trackers using end-to-end networks usually perform better than the trackers merely using feature extraction networks. (6) For visual tracking, the most suitable network training method is to per-train networks with video information and online fine-tune them with subsequent observations. Finally, we summarize our manuscript and highlight our insights, and point out the further trends for deep visual tracking.","Visual tracking, Deep learning, CNN, RNN, Pre-training, Online learning",Peixia Li and Dong Wang and Lijun Wang and Huchuan Lu,https://www.sciencedirect.com/science/article/pii/S0031320317304612,https://doi.org/10.1016/j.patcog.2017.11.007,0031-3203,2018,323--338,76,Pattern Recognition,Deep visual tracking: Review and experimental comparison,article,LI2018323,
"The consistently increasing of the feature dimension brings about great time complexity and storage burden for multi-label learning. Numerous multi-label feature selection techniques are developed to alleviate the effect of high-dimensionality. The existing multi-label feature selection algorithms assume that the labels of the training data are complete. However, this assumption does not always hold true for labeling data is costly and there is ambiguity among classes. Hence, in real-world applications, the data available usually have an incomplete set of labels. In this paper, we present a novel multi-label feature selection model under the circumstance of missing labels. With the proposed algorithm, the most discriminative features are selected and missing labels are recovered simultaneously. To remove the irrelevant and noisy features, the effective l2, p-norm (0â¯<â¯pâ¯â¤â¯1) regularization item is imposed on the feature selection matrix. To solve the optimization problem, we developed an iterative reweighted least squares (IRLS) algorithm with guaranteed convergence. Experimental results on benchmark datasets show that the proposed method outperforms the state-of-the-art multi-label feature selection algorithms.","Feature selection, Multi-label learning, Missing labels",Pengfei Zhu and Qian Xu and Qinghua Hu and Changqing Zhang and Hong Zhao,https://www.sciencedirect.com/science/article/pii/S0031320317303886,https://doi.org/10.1016/j.patcog.2017.09.036,0031-3203,2018,488--502,74,Pattern Recognition,Multi-label feature selection with missing labels,article,ZHU2018488,
"Here, we present IDNet, a user authentication framework from smartphone-acquired motion signals. Its goal is to recognize a target user from their way of walking, using the accelerometer and gyroscope (inertial) signals provided by a commercial smartphone worn in the front pocket of the userâs trousers. IDNet features several innovations including: (i) a robust and smartphone-orientation-independent walking cycle extraction block, (ii) a novel feature extractor based on convolutional neural networks, (iii) a one-class support vector machine to classify walking cycles, and the coherent integration of these into (iv) a multi-stage authentication technique. IDNet is the first system that exploits a deep learning approach as universal feature extractors for gait recognition, and that combines classification results from subsequent walking cycles into a multi-stage decision making framework. Experimental results show the superiority of our approach against state-of-the-art techniques, leading to misclassification rates (either false negatives or positives) smaller than 0.15% with fewer than five walking cycles. Design choices are discussed and motivated throughout, assessing their impact on the user authentication performance.","Biometric gait analysis, Target recognition, Classification methods, Convolutional neural networks, Support vector machines, Inertial sensors, Feature extraction, Signal processing, Accelerometer, Gyroscope",Matteo Gadaleta and Michele Rossi,https://www.sciencedirect.com/science/article/pii/S0031320317303485,https://doi.org/10.1016/j.patcog.2017.09.005,0031-3203,2018,25--37,74,Pattern Recognition,IDNet: Smartphone-based gait recognition with convolutional neural networks,article,GADALETA201825,
"Face recognition across age progression remains one of the area's most challenging tasks, as the aging process affects both the shape and texture of a face. One possible solution is to apply a probabilistic model to represent a face simultaneously with its identity variable, which is stable through time, and its aging variable, which changes with time. However, as the aging process varies for different people, a person may look younger or older than another person, even though their ages are the same. Consequently, using the ârealâ age labels given by existing face datasets for age-invariant face recognition will inevitably introduce ambiguity to learning algorithms. In this paper, an identity-inference model, based on age-subspace learning from appearance-age labels, is proposed. We first model human identity and aging variables simultaneously using Probabilistic Linear Discriminant Analysis (PLDA). Then, the aging subspace is learnt independently with the appearance-age labels, and the identity subspace is then determined iteratively with the Expectation-Maximization (EM) algorithm. We found that the learned aging subspace is insensitive to the training face images used, and is independent of the identity model. Consequently, the recognition of aging faces becomes simpler as identity inference no longer needs to consider age labels. Furthermore, in our algorithm, different identity features learnt from the identity model are further combined using Canonical Correlation Analysis (CCA), where their correlations are maximized for face recognition. A thorough experimental analysis of face recognition is performed on three public domain face-aging datasets: FGNET, MORPH, and CACD. Experiment results show that the proposed framework can achieve a comparable, or even better, performance against other state-of-the-art methods, especially when the age range is large.","Age-invariant, Canonical correlation analysis, Face recognition, Face verification, Identity inference, Probabilistic LDA",Huiling Zhou and Kin-Man Lam,https://www.sciencedirect.com/science/article/pii/S0031320317304454,https://doi.org/10.1016/j.patcog.2017.10.036,0031-3203,2018,191--202,76,Pattern Recognition,Age-invariant face recognition based on identity inference from appearance age,article,ZHOU2018191,
"To produce a complete 3D reconstruction of a large-scale architectural scene, both ground and aerial images are usually captured. A common approach is to first reconstruct the models from different image sources separately, and align them afterwards. Using this pipeline, this work proposes an accurate and efficient approach for ground-to-aerial model alignment in a coarse-to-fine manner. First, both the ground model and aerial model are transformed into the geo-referenced coordinate system using GPS meta-information for coarse alignment. Then, the coarsely aligned models are refined by a similarity transformation that is estimated based on 3D point correspondences between them, and the 3D point correspondences are determined in a 2D-image-matching manner by considering the rich textural and contextual information in the 2D images. Due to the dramatic differences in viewpoint and scale between ground and aerial images, which make matching them directly nearly impossible, we perform an intermediate view-synthesis step to mitigate the matching difficulty. To this end, the following three key issues are addressed: (a) selecting a suitable subset of aerial images to cover the ground model properly; (b) synthesizing images from the ground model under the viewpoints of the selected aerial images; and finally, (c) obtaining the 2D point matches between the synthesized images and the selected aerial images. The experimental results show that the proposed model alignment approach is quite effective and outperforms several state-of-the-art techniques in terms of both accuracy and efficiency.","Image based modeling, Ground-to-aerial model alignment, Ground-to-aerial image matching",Xiang Gao and Lihua Hu and Hainan Cui and Shuhan Shen and Zhanyi Hu,https://www.sciencedirect.com/science/article/pii/S0031320317304570,https://doi.org/10.1016/j.patcog.2017.11.003,0031-3203,2018,288--302,76,Pattern Recognition,Accurate and efficient ground-to-aerial model alignment,article,GAO2018288,
"Recent years have witnessed the quick progress of the hyperspectral images (HSI) classification. Most of existing studies either heavily rely on the expensive label information using the supervised learning or can hardly exploit the discriminative information borrowed from related domains. To address this issues, in this paper we show a novel framework addressing HSI classification based on the domain adaptation (DA) with active learning (AL). The main idea of our method is to retrain the multi-kernel classifier by utilizing the available labeled samples from source domain, and adding minimum number of the most informative samples with active queries in the target domain. The proposed method adaptively combines multiple kernels, forming a DA classifier that minimizes the bias between the source and target domains. Further equipped with the nested actively updating process, it sequentially expands the training set and gradually converges to a satisfying level of classification performance. We study this active adaptation framework with the Margin Sampling (MS) strategy in the HSI classification task. Our experimental results on two popular HSI datasets demonstrate its effectiveness.","Active learning, Multi-kernel, Domain adaptation, Hyperspectral image classification, Remote sensing",Cheng Deng and Xianglong Liu and Chao Li and Dacheng Tao,https://www.sciencedirect.com/science/article/pii/S0031320317304053,https://doi.org/10.1016/j.patcog.2017.10.007,0031-3203,2018,306--315,77,Pattern Recognition,Active multi-kernel domain adaptation for hyperspectral image classification,article,DENG2018306,
"We survey unsupervised machine learning algorithms in the context of outlier detection. This task challenges state-of-the-art methods from a variety of research fields to applications including fraud detection, intrusion detection, medical diagnoses and data cleaning. The selected methods are benchmarked on publicly available datasets and novel industrial datasets. Each method is then submitted to extensive scalability, memory consumption and robustness tests in order to build a full overview of the algorithmsâ characteristics.","Outlier detection, Fraud detection, Novelty detection, Variational inference, Isolation forest",RÃ©mi Domingues and Maurizio Filippone and Pietro Michiardi and Jihane Zouaoui,https://www.sciencedirect.com/science/article/pii/S0031320317303916,https://doi.org/10.1016/j.patcog.2017.09.037,0031-3203,2018,406--421,74,Pattern Recognition,A comparative evaluation of outlier detection algorithms: Experiments and analyses,article,DOMINGUES2018406,
"In this paper, we present a feature description method called semantic descriptor with objectness (SDO) for scene recognition. Most existing scene representation methods exploit the characteristics of constituent objects in scenes with inter-class independence, which ignore the negative effects caused by the common objects among different scenes. The generic characteristics of the common objects cause some generality among different scenes, which weakens the discriminative characteristics among scenes. To address this problem, we exploit the correlations of object configurations among different scenes by the co-occurrence pattern of all objects across scenes to choose representative and discriminative objects which enhances the inter-class discriminability. Specifically, we capture the statistic information of objects appearing in each scene to compute the distribution of each object across scenes, which obtains the co-occurrence pattern of objects. Moreover, we represent the image descriptors with the occurrence probabilities of discriminative objects in image patches to eliminate the negative effects of common objects. To make image descriptors more discriminative, we discard the patches with non-discriminative objects to enhance the intra-class generalized characteristics. Experimental results on three widely used scene recognition datasets show that our method outperforms the state-of-the-art methods.","Scene recognition, Deep learning, Co-occurrence pattern",Xiaojuan Cheng and Jiwen Lu and Jianjiang Feng and Bo Yuan and Jie Zhou,https://www.sciencedirect.com/science/article/pii/S003132031730376X,https://doi.org/10.1016/j.patcog.2017.09.025,0031-3203,2018,474--487,74,Pattern Recognition,Scene recognition with objectness,article,CHENG2018474,
"This paper presents a new facial landmark detection method for images and videos under uncontrolled conditions, based on a proposed Face Alignment Recurrent Network (FARN). The network works in recurrent fashion and is end-to-end trained to help avoid over-strong early stage regressors and over-weak later stage regressors as in many existing works. Long Short Term Memory (LSTM) model is employed in our network to make full use of the spatial and temporal middle stage information in a natural way, where by spatial we mean that for each image (frame), the predicted landmark position in the current stage will be used to guide the estimation for the next stage, and by temporal we mean that the predicted landmark position in the current frame will be used to guide the estimation for the next frame, and thus providing an unified framework for facial landmark detection in both images and videos. We conduct experiments on public image datasets (COFW, Helen, 300-W) as well as on video datasets (300-VW), and results show clear improvement over most of the current state-of-the-art approaches. In addition, it works in 18Â ms per image (frame).11Our source code and models will be released soon.","Face alignment, Recurrent network",Qiqi Hou and Jinjun Wang and Ruibin Bai and Sanping Zhou and Yihong Gong,https://www.sciencedirect.com/science/article/pii/S0031320317303813,https://doi.org/10.1016/j.patcog.2017.09.028,0031-3203,2018,448--458,74,Pattern Recognition,Face alignment recurrent network,article,HOU2018448,
"Epipolar plane images (EPIs) contain special linear structures that reflect the disparity of a 3D point and are widely used in light field depth estimation. However, previous EPI-based approaches only utilize horizontal and vertical EPIs to estimate local disparities and ignore diagonal directions. In order to make full use of the regular grid light field images, we develop a strategy to extract epipolar plane images in all available directions. Based on the multi-orientation EPIs, a specific EPI in which the point is not occluded is found and used to calculate robust depth estimation. We also design a novel framework to estimate the depth information which combines the local depth with edge orientation. The multi-orientation EPIs and optimal orientation selection are proved to be effective in detecting and excluding occlusions. Experimental results show that the proposed method outperforms state-of-the-art depth estimation methods, especially near occlusion boundaries.","Light field, Depth estimation, Multi-orientation EPIs, Occlusion analysis",Hao Sheng and Pan Zhao and Shuo Zhang and Jun Zhang and Da Yang,https://www.sciencedirect.com/science/article/pii/S0031320317303539,https://doi.org/10.1016/j.patcog.2017.09.010,0031-3203,2018,587--599,74,Pattern Recognition,Occlusion-aware depth estimation for light field using multi-orientation EPIs,article,SHENG2018587,
"Exemplar-based face sketch synthesis plays an important role in both digital entertainment and law enforcement. It generally consists of two parts: neighbor selection and recognition weight representation. In this paper, we proposed a simple but effective method which employs offline random sampling instead of K-NN used in state-of-the-art methods. The proposed random sampling strategy reduces the time consuming for synthesis and has stronger scalability than state-of-the-art methods. In addition, we introduced locality constraint to model the distinct correlations between the test patch and random sampled patches. Extensive experiments on public face sketch databases demonstrate the superiority of the proposed method in comparison to state-of-the-art methods, in terms of both synthesis quality and time consumption. The proposed method could be extended to other heterogeneous face image transformation problems such as face hallucination. We release the source codes of our proposed methods and the evaluation metrics for future study online: http://www.ihitworld.com/RSLCR.html.","Face sketch synthesis, Locality constraint, Neighbor selection, Random sampling, Weight computation",Nannan Wang and Xinbo Gao and Jie Li,https://www.sciencedirect.com/science/article/pii/S0031320317304624,https://doi.org/10.1016/j.patcog.2017.11.008,0031-3203,2018,215--227,76,Pattern Recognition,Random sampling for fast face sketch synthesis,article,WANG2018215,
"While standing as one of the most widely considered and successful supervised classification algorithms, the k-nearest Neighbor (kNN) classifier generally depicts a poor efficiency due to being an instance-based method. In this sense, Approximated Similarity Search (ASS) stands as a possible alternative to improve those efficiency issues at the expense of typically lowering the performance of the classifier. In this paper we take as initial point an ASS strategy based on clustering. We then improve its performance by solving issues related to instances located close to the cluster boundaries by enlarging their size and considering the use of Deep Neural Networks for learning a suitable representation for the classification task at issue. Results using a collection of eight different datasets show that the combined use of these two strategies entails a significant improvement in the accuracy performance, with a considerable reduction in the number of distances needed to classify a sample in comparison to the basic kNN rule.","Efficient NN classification, Clustering, Deep neural networks",Antonio-Javier Gallego and Jorge Calvo-Zaragoza and Jose J. Valero-Mas and Juan R. Rico-Juan,https://www.sciencedirect.com/science/article/pii/S0031320317303898,https://doi.org/10.1016/j.patcog.2017.09.038,0031-3203,2018,531--543,74,Pattern Recognition,Clustering-based k-nearest neighbor classification for large-scale data with neural codes representation,article,GALLEGO2018531,
"Mobile biometrics technologies are nowadays the new frontier for secure use of data and services, and are considered particularly important due to the massive use of handheld devices in the entire world. Among the biometric traits with potential to be used in mobile settings, the iris/ocular region is a natural candidate, even considering that further advances in the technology are required to meet the operational requirements of such ambitious environments. Aiming at promoting these advances, we organized the Mobile Iris Challenge Evaluation (MICHE)-I contest. This paper presents a comparison of the performance of the participant methods by various Figures of Merit (FoMs). A particular attention is devoted to the identification of the image covariates that are likely to cause a decrease in the performance levels of the compared algorithms. Among these factors, interoperability among different devices plays an important role. The methods (or parts of them) implemented by the analyzed approaches are classified into segmentation (S), which was the main target of MICHE-I, and recognition (R). The paper reports both the results observed for either S or R, and also for different recombinations (S+R) of such methods. Last but not least, we also present the results obtained by multi-classifier strategies.","Mobile Iris Recognition, Evaluation, Biometric algorithm fusion",Maria {De Marsico} and Michele Nappi and Fabio Narducci and Hugo ProenÃ§a,https://www.sciencedirect.com/science/article/pii/S0031320317303412,https://doi.org/10.1016/j.patcog.2017.08.028,0031-3203,2018,286--304,74,Pattern Recognition,Insights into the results of MICHE I - Mobile Iris CHallenge Evaluation,article,DEMARSICO2018286,
"One of the major challenges in person Re-Identification (ReID) is the inconsistent visual appearance of a person. Current works on visual feature and distance metric learning have achieved significant achievements, but still suffer from the limited robustness to pose variations, viewpoint changes, etc., and the high computational complexity. This makes person ReID among multiple cameras still challenging. This work is motivated to learn mid-level human attributes which are robust to visual appearance variations and could be used as efficient features for person matching. We propose a weakly supervised multi-type attribute learning framework which considers the contextual cues among attributes and progressively boosts the accuracy of attributes only using a limited number of labeled data. Specifically, this framework involves a three-stage training. A deep Convolutional Neural Network (dCNN) is first trained on an independent dataset labeled with attributes. Then it is fine-tuned on another dataset only labeled with person IDs using our defined triplet loss. Finally, the updated dCNN predicts attribute labels for the target dataset, which is combined with the independent dataset for the final round of fine-tuning. The predicted attributes, namely deep attributes exhibit promising generalization ability across different datasets. By directly using the deep attributes with simple Cosine distance, we have obtained competitive accuracy on four person ReID datasets. Experiments also show that a simple distance metric learning modular further boosts our method, making it outperform many recent works.","Deep attributes, Person re-identification",Chi Su and Shiliang Zhang and Junliang Xing and Wen Gao and Qi Tian,https://www.sciencedirect.com/science/article/pii/S0031320317302686,https://doi.org/10.1016/j.patcog.2017.07.005,0031-3203,2018,77--89,75,Pattern Recognition,Multi-type attributes driven multi-camera person re-identification,article,SU201877,Distance Metric Learning for Pattern Recognition
"Vector of Locally Aggregated Descriptor (VLAD) is a very popular feature coding method in image classification and image retrieval. Recently, the original VLAD method is extended to an end-to-end model called NetVLAD. The NetVLAD layer is readily embedded into a deep neural network and can be trained by the back-propagation algorithm. Although the NetVLAD model has achieved noticeable classification results in many image databases, the discrimination embedded in the NetVLAD method is not fully exploited. In this paper, in order to design a more discriminative feature coding network, a novel localized and second-order VLAD Network (LSO-VLADNet) is proposed. First, we design a localized and second-order VLAD coding method. Second, the back propagation functions of all newly designed layers are obtained. Third, the new feature coding method is extended to an end-to-end feature coding network which can be jointly trained with a deep convolutional neural network for visual recognition. Some experiments show that the newly designed network has the significant improvements over the original NetVLAD. Some experimental comparisons of the proposed model and other state-of-art methods will also be given to validate the effectiveness of the proposed model.","Deep neural network, Feature coding network, Localized and second order VLAD, End to end, Image recognition",Boheng Chen and Jie Li and Gang Wei and Biyun Ma,https://www.sciencedirect.com/science/article/pii/S0031320317304466,https://doi.org/10.1016/j.patcog.2017.10.039,0031-3203,2018,339--348,76,Pattern Recognition,A novel localized and second order feature coding network for image recognition,article,CHEN2018339,
"Most of the existing clustering methods have difficulty in processing complex nonlinear data sets. To remedy this deficiency, in this paper, a novel data model termed Hybrid K-Nearest-Neighbor (HKNN) graph, which combines the advantages of mutual k-nearest-neighbor graph and k-nearest-neighbor graph, is proposed to represent the nonlinear data sets. Moreover, a Clustering method based on the HKNN graph (CHKNN) is proposed. The CHKNN first generates several tight and small subclusters, then merges these subclusters by exploiting the connectivity among them. In order to select the optimal parameters for CHKNN, we further propose an internal validity index termed K-Nearest-Neighbor Index (KNNI), which can also be used to evaluate the validity of nonlinear clustering results by varying a control parameter. Experimental results on synthetic and real-world data sets, as well as that on the video clustering, have demonstrated the significant improvement on performance over existing nonlinear clustering methods and internal validity indices.","Graph clustering, Hybrid k-nearest-neighbor graph, Internal validity index, Nonlinear data set, Video clustering",Yikun Qin and Zhu Liang Yu and Chang-Dong Wang and Zhenghui Gu and Yuanqing Li,https://www.sciencedirect.com/science/article/pii/S0031320317303497,https://doi.org/10.1016/j.patcog.2017.09.008,0031-3203,2018,1--14,74,Pattern Recognition,A Novel clustering method based on hybrid K-nearest-neighbor graph,article,QIN20181,
"Person re-identification aims to match images of the same person across disjoint camera views, which is a challenging problem in video surveillance. The major challenge of this task lies in how to preserve the similarity of the same person against large variations caused by complex backgrounds, mutual occlusions and different illuminations, while discriminating the different individuals. In this paper, we present a novel deep ranking model with feature learning and fusion by learning a large adaptive margin between the intra-class distance and inter-class distance to solve the person re-identification problem. Specifically, we organize the training images into a batch of pairwise samples. Treating these pairwise samples as inputs, we build a novel part-based deep convolutional neural networkÂ (CNN) to learn the layered feature representations by preserving a large adaptive margin. As a result, the final learned model can effectively find out the matched target to the anchor image among a number of candidates in the gallery image set by learning discriminative and stable feature representations. Overcoming the weaknesses of conventional fixed-margin loss functions, our adaptive margin loss function is more appropriate for the dynamic feature space. On four benchmark datasets, PRID2011, Market1501, CUHK01 and 3DPeS, we extensively conduct comparative evaluations to demonstrate the advantages of the proposed method over the state-of-the-art approaches in person re-identification.","Person re-identification, Deep ranking model, Metric learning",Jiayun Wang and Sanping Zhou and Jinjun Wang and Qiqi Hou,https://www.sciencedirect.com/science/article/pii/S0031320317303771,https://doi.org/10.1016/j.patcog.2017.09.024,0031-3203,2018,241--252,74,Pattern Recognition,Deep ranking model by large adaptive margin learning for person re-identification,article,WANG2018241,
"Subspace methods are popular for image set classification due to the excellent representation ability of subspaces. Generalized difference subspace and orthogonal subspace are two currently effective projection strategies for extracting discriminative subspaces. However, both of these methods discard part of the common subspace to form the constraint subspace, which may cause a loss of discriminative information. In this work, we combine the difference subspace and orthogonal subspace to form a full rank constraint subspace. Moreover, we generalize this approach to a common framework using eigenspectrum regularization models (ERMs). The full rank constraint subspace that is regularized by different ERMs is called the regularized constraint subspace (RCS). Furthermore, we propose a new ERM using the concept of difference subspace, namely, the difference subspace regularization model (DSRM). The DSRM and two other current ERMs are incorporated in our RCS-based framework. The results from extensive experiments have demonstrated the effectiveness of our proposed approaches.","Subspace method, Constraint subspace, Difference subspace, Orthogonal subspace, Eigenspectrum regularization model",Hengliang Tan and Ying Gao and Zhengming Ma,https://www.sciencedirect.com/science/article/pii/S0031320317304740,https://doi.org/10.1016/j.patcog.2017.11.020,0031-3203,2018,434--448,76,Pattern Recognition,Regularized constraint subspace based method for image set classification,article,TAN2018434,
"In this paper we present architectures based on deep neural nets for expression recognition in videos, which are invariant to local scaling. We amalgamate autoencoder and predictor architectures using an adaptive weighting scheme coping with a reduced size labeled dataset, while enriching our models from enormous unlabeled sets. We further improve robustness to lighting conditions by introducing a new adaptive filter based on temporal local scale normalization. We provide superior results over known methods, including recent reported approaches based on neural nets.","Deep learning, Expression recognition, Video classification, Neural nets, Machine learning",Otkrist Gupta and Dan Raviv and Ramesh Raskar,https://www.sciencedirect.com/science/article/pii/S0031320317304223,https://doi.org/10.1016/j.patcog.2017.10.017,0031-3203,2018,25--35,76,Pattern Recognition,Illumination invariants in deep video expression recognition,article,GUPTA201825,
"This paper proposes a no-reference (NR)/referenceless quality evaluation method for stereoscopic three-dimensional (S3D) images based on deep nonnegativity constrained sparse autoencoder (DNCSAE). To address the quality issue of stereopairs whose perceived quality is not only determined by the individual left and right image qualities but also their interactions, a three-column DNCSAE framework is customized with individual DNCSAE module coping with the left image, the right image, and the cyclopean image, respectively. In the proposed framework, each individual DNCSAE module shares the same network architecture consisting of multiple stacked NCSAE layers and one Softmax regression layer at the end. The contribution of our model is that hierarchical feature evolution and nonlinear feature mapping are jointly optimized in a unified and perceptual-aware deep network (DNCSAE), which well resembles several important visual properties, i.e., hierarchy, sparsity, and non-negativity. To be more specific, for each DNCSAE, by taking a set of handcrafted natural scene statistic (NSS) features as inputs in the visible layer, the features in hidden layers are successively evolved to deeper levels producing increasingly discriminative quality-aware features (QAFs). Then, QAFs in the last NCSAE layer are summarized to their corresponding quality score by Softmax regression. Finally, three individual yet complementary quality scores estimated by each DNCSAE model are combined based on a Bayesian framework to obtain an overall 3D quality score. Experiments on three benchmark databases demonstrate the superiority of our method in terms of both prediction accuracy and generalization capability.","Image quality assessment, No-reference/referenceless, Stereoscopic 3D image, Deep learning, Nonnegativity constrained, Sparse autoencoder",Qiuping Jiang and Feng Shao and Weisi Lin and Gangyi Jiang,https://www.sciencedirect.com/science/article/pii/S0031320317304582,https://doi.org/10.1016/j.patcog.2017.11.001,0031-3203,2018,242--255,76,Pattern Recognition,Learning a referenceless stereopair quality engine with deep nonnegativity constrained sparse autoencoder,article,JIANG2018242,
"Multiple instance learning (MIL) is a form of weakly supervised learning where training instances are arranged in sets, called bags, and a label is provided for the entire bag. This formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data. Consequently, it has been used in diverse application fields such as computer vision and document classification. However, learning from bags raises important challenges that are unique to MIL. This paper provides a comprehensive survey of the characteristics which define and differentiate the types of MIL problems. Until now, these problem characteristics have not been formally identified and described. As a result, the variations in performance of MIL algorithms from one data set to another are difficult to explain. In this paper, MIL problem characteristics are grouped into four broad categories: the composition of the bags, the types of data distribution, the ambiguity of instance labels, and the task to be performed. Methods specialized to address each category are reviewed. Then, the extent to which these characteristics manifest themselves in key MIL application areas are described. Finally, experiments are conducted to compare the performance of 16 state-of-the-art MIL methods on selected problem characteristics. This paper provides insight on how the problem characteristics affect MIL algorithms, recommendations for future benchmarking and promising avenues for research. Code is available on-line at https://github.com/macarbonneau/MILSurvey.","Multiple instance learning, Weakly supervised learning, Classification, Multi-instance learning, Computer vision, Computer aided diagnosis, Document classification, Drug activity prediction",Marc-AndrÃ© Carbonneau and Veronika Cheplygina and Eric Granger and Ghyslain Gagnon,https://www.sciencedirect.com/science/article/pii/S0031320317304065,https://doi.org/10.1016/j.patcog.2017.10.009,0031-3203,2018,329--353,77,Pattern Recognition,Multiple instance learning: A survey of problem characteristics and applications,article,CARBONNEAU2018329,
"In this work, we present a novel background subtraction from video sequences algorithm that uses a deep Convolutional Neural Network (CNN) to perform the segmentation. With this approach, feature engineering and parameter tuning become unnecessary since the network parameters can be learned from data by training a single CNN that can handle various video scenes. Additionally, we propose a new approach to estimate background model from video sequences. For the training of the CNN, we employed randomly 5% video frames and their ground truth segmentations taken from the Change Detection challenge 2014 (CDnet 2014). We also utilized spatial-median filtering as the post-processing of the network outputs. Our method is evaluated with different data-sets, and it (so-called DeepBS) outperforms the existing algorithms with respect to the average ranking over different evaluation metrics announced in CDnet 2014. Furthermore, due to the network architecture, our CNN is capable of real time processing.","Background subtraction, Video segmentation, CNN, Deep learning",Mohammadreza Babaee and Duc Tung Dinh and Gerhard Rigoll,https://www.sciencedirect.com/science/article/pii/S0031320317303928,https://doi.org/10.1016/j.patcog.2017.09.040,0031-3203,2018,635--649,76,Pattern Recognition,A deep convolutional neural network for video sequence background subtraction,article,BABAEE2018635,
"In this paper, we propose a robust method, called Gaussian Field Consensus (GFC), for outlier rejection from given putative point set matching correspondences. Finding correct correspondences (inliers) is a key component in many computer vision and pattern recognition tasks, and the goal of outlier (mismatch) rejection is to fit the transformation function that maps one feature point set to another. Our GFC starts by inputting a putative correspondence set which is contaminated by many outliers, and the main task of our GFC is to identify the underlying true correspondences from outliers. Then we formulate this challenging problem by Gaussian Field nonparametric matching model which bases on the exponential distance loss and kernel method in a reproducing kernel Hilbert space. Next, We introduce a local linear constraint based on the regularization theory to preserve the topological structure of the feature points. Moreover, the sparse approximation is used to reduce the search space, in this way, we can handle a large number of points easily. Finally, we test the GFC method on several real image datasets in the presence of outliers, where the experimental results show that our proposed method outperforms current state-of-the-art methods in most test scenarios.","Gaussian field, Outlier rejection, Mismatch removal, Point matching, Sparse approximation",Gang Wang and Yufei Chen and Xiangwei Zheng,https://www.sciencedirect.com/science/article/pii/S0031320317303825,https://doi.org/10.1016/j.patcog.2017.09.029,0031-3203,2018,305--316,74,Pattern Recognition,Gaussian field consensus: A robust nonparametric matching method for outlier rejection,article,WANG2018305,
"In a non-stationary environment, newly received data may have different knowledge patterns from the data used to train learning models. As time passes, a learning modelâs performance may become increasingly unreliable. This problem is known as concept drift and is a common issue in real-world domains. Concept drift detection has attracted increasing attention in recent years. However, very few existing methods pay attention to small regional drifts, and their accuracy may vary due to differing statistical significance tests. This paper presents a novel concept drift detection method, based on regional-density estimation, named nearest neighbor-based density variation identification (NN-DVI). It consists of three components. The first is a k-nearest neighbor-based space-partitioning schema (NNPS), which transforms unmeasurable discrete data instances into a set of shared subspaces for density estimation. The second is a distance function that accumulates the density discrepancies in these subspaces and quantifies the overall differences. The third component is a tailored statistical significance test by which the confidence interval of a concept drift can be accurately determined. The distance applied in NN-DVI is sensitive to regional drift and has been proven to follow a normal distribution. As a result, the NN-DVIâs accuracy and false-alarm rate are statistically guaranteed. Additionally, several benchmarks have been used to evaluate the method, including both synthetic and real-world datasets. The overall results show that NN-DVI has better performance in terms of addressing problems related to concept drift-detection.","Concept drift, Dataset shift, Changing environments, Covariate shift, Empirical density estimation",Anjin Liu and Jie Lu and Feng Liu and Guangquan Zhang,https://www.sciencedirect.com/science/article/pii/S0031320317304636,https://doi.org/10.1016/j.patcog.2017.11.009,0031-3203,2018,256--272,76,Pattern Recognition,Accumulating regional density dissimilarity for concept drift detection in data streams,article,LIU2018256,
"Minimum Barrier Distance (MBD) is one recently proposed saliency measure which provides more robust result than the geodesic distance. Due to accurate pixel-wise MBD computation needs high time expenditure, approximate implementation with raster scan and minimum spanning tree (MST) are proposed recently. Inspired by the natural phenomena of the water flow, we propose one efficient approximate method â water flow driven MBD. Seed pixels (such as image boundary in salient object detection) are assumed as source of water, the water can flow from source pixels to other pixels with different priority which determined by MBD cost, lower cost means flow earlier. MBD of each pixel can be computed during processing of water flow. Our MBD computation shows higher performance in terms both speed and accuracy. Proposed MBD computation only needs visit image once, while raster based MBD needs scan image three times, MST based MBD needs traversal on image twice and additionally needs time to construct a tree. Compared with two previous MBD approximation algorithm, our computation speed increased by 2.4 times and 5 times, while approximation error declined by 50% and 80%. Based on our fast MBD computation, a fast salient object detection method is also proposed. The accuracy of proposed method outperforms other MBD based methods, and shows better performance than the other state-of-the-art methods. The proposed method achieves 180Â fps speed performance on four public datasets. In state-of-the-art methods, the highest speed performance is about 52Â fps, our method shows 3.5 times speed improvement.","Salient object detection, Minimum barrier distance, Water flow, Geodesic distance, Saliency detection",Xiaoming Huang and Yujin Zhang,https://www.sciencedirect.com/science/article/pii/S003132031730434X,https://doi.org/10.1016/j.patcog.2017.10.027,0031-3203,2018,95--107,76,Pattern Recognition,Water flow driven salient object detection at 180Â fps,article,HUANG201895,
"Identification and bi-manual handling of deformable objects, like textiles, is one of the most challenging tasks in the field of industrial and service robotics. Their unpredictable shape and pose makes it very difficult to identify the type of garment and locate the most relevant parts that can be used for grasping. In this paper, we propose an algorithm that first, identifies the type of garment and second, performs a search of the two grasping points that allow a robot to bring the garment to a known pose. We show that using an active search strategy it is possible to grasp a garment directly from predefined grasping points, as opposed to the usual approach based on multiple re-graspings of the lowest hanging parts. Our approach uses a hierarchy of three Convolutional Neural Networks (CNNs) with different levels of specialization, trained both with synthetic and real images. The results obtained in the three steps (recognition, first grasping point, second grasping point) are promising. Experiments with real robots show that most of the errors are due to unsuccessful grasps and not to the localization of the grasping points, thus a more robust grasping strategy is required.","Garment classification, Garment grasping, Deep learning, Depth images",Enric Corona and Guillem AlenyÃ  and Antonio Gabas and Carme Torras,https://www.sciencedirect.com/science/article/pii/S0031320317303941,https://doi.org/10.1016/j.patcog.2017.09.042,0031-3203,2018,629--641,74,Pattern Recognition,Active garment recognition and target grasping point detection using deep learning,article,CORONA2018629,
"Chinese font recognition (CFR) has gained significant attention in recent years. However, due to the sparsity of labeled font samples and the structural complexity of Chinese characters, CFR is still a challenging task. In this paper, a DropRegion method is proposed to generatea large number of stochastic variant font samples whose local regions are selectively disrupted and an inception font network (IFN) with two additional convolutional neural network (CNN) structure elements, i.e., a cascaded cross-channel parametric pooling (CCCP) and global average pooling, is designed. Because the distribution of strokes in a font image is non-stationary, an elastic meshing technique that adaptively constructs a set of local regions with equalized information is developed. Thus, DropRegion is seamlessly embedded in the IFN, which enables end-to-end training; the proposed DropRegion-IFN can be used for high performance CFR. Experimental results have confirmed the effectiveness of our new approach for CFR.","Chinese font recognition, DropRegion, Elastic meshing technique, Inception font network (IFN), Text block-based font recognition",Shuangping Huang and Zhuoyao Zhong and Lianwen Jin and Shuye Zhang and Haobin Wang,https://www.sciencedirect.com/science/article/pii/S0031320317304235,https://doi.org/10.1016/j.patcog.2017.10.018,0031-3203,2018,395--411,77,Pattern Recognition,DropRegion training of inception font network for high-performance Chinese font recognition,article,HUANG2018395,
"Visual object tracking is a fundamental function of computer vision that has been the object of numerous studies. The diversity of the proposed approaches leads to the idea of trying to fuse them and take advantage of their individual strengths while controlling the noise they may introduce in some circumstances. The work presented here describes a generic framework for combining and/or selecting on-line the different components of the processing chain of a set of trackers, and examines the impact of various fusion strategies. The results are assessed from a repertoire of 9 state-of-the-art trackers evaluated over 46 fusion strategies on the VOT 2013, VOT 2015 and OTB-100 datasets. A complementarity measure able to predict the overall performance of a given set of trackers is also proposed.","Visual object tracking, Fusion, On-line behavior analysis",Isabelle Leang and StÃ©phane Herbin and BenoÃ®t Girard and Jacques Droulez,https://www.sciencedirect.com/science/article/pii/S0031320317303783,https://doi.org/10.1016/j.patcog.2017.09.026,0031-3203,2018,459--473,74,Pattern Recognition,On-line fusion of trackers for single-object tracking,article,LEANG2018459,
"The increasing advancement of mobile technology explosively popularizes the mobile devices (e.g. iPhone, iPad). A large number of mobile devices provide great convenience and cost effectiveness for the speaker recognition based applications. However, the compromise of speech template stored in mobile devices highly likely lead to the severe security and privacy breaches while the existing proposals for speech template protection do not completely guarantee the required properties such as unlinkability and non-invertibility. In this paper, we propose a cancellable transform, namely Random Binary Orthogonal Matrices Projection (RBOMP) hashing, to protect a well-known speech representation (i.e. i-vector). RBOMP hashing is inspired from Winner-Takes-All hash and further strengthened by the integration of the prime factorization (PF) function. Briefly, RBOMP hashing projects the i-vector using random binary orthogonal matrices and records the discrete value. Due to the strong non-linearity of RBOMP, the resultant hashed code withstands the template invertibility attack. Further, the experimental results suggest that the speech template generated using RBOMP hashing can still be verified with reasonable accuracy. Besides that, rigorous analysis shows that the proposed cancellable technique for speech resists several major attacks while the other criteria of biometric template protection can be justified simultaneously.","Cancellable biometrics, Speaker recognition, RBOMP hashing, PF function, Security & privacy",Kong-Yik Chee and Zhe Jin and Danwei Cai and Ming Li and Wun-She Yap and Yen-Lung Lai and Bok-Min Goi,https://www.sciencedirect.com/science/article/pii/S0031320317304478,https://doi.org/10.1016/j.patcog.2017.10.041,0031-3203,2018,273--287,76,Pattern Recognition,Cancellable speech template via random binary orthogonal matrices projection hashing,article,CHEE2018273,
"Vector field images are a type of new multidimensional data that appear in many engineering areas. Although the vector fields can be visualized as images, they differ from graylevel and color images in several aspects. To analyze them, special methods and algorithms must be originally developed or substantially adapted from the traditional image processing area. In this paper, we propose a method for the description and matching of vector field patterns under an unknown rotation of the field. Rotation of a vector field is so-called total rotation, where the action is applied not only on the spatial coordinates but also on the field values. Invariants of vector fields with respect to total rotation constructed from orthogonal GaussianâHermite moments and Zernike moments are introduced. Their numerical stability is shown to be better than that of the invariants published so far. We demonstrate their usefulness in a real world template matching application of rotated vector fields.","Vector field, Total rotation, Invariants, GaussianâHermite moments, Zernike moments, Numerical stability",Bo Yang and Jitka KostkovÃ¡ and Jan Flusser and TomÃ¡Å¡ Suk and Roxana Bujack,https://www.sciencedirect.com/science/article/pii/S0031320317303540,https://doi.org/10.1016/j.patcog.2017.09.004,0031-3203,2018,110--121,74,Pattern Recognition,Rotation invariants of vector fields from orthogonal moments,article,YANG2018110,
"In this paper, we leverage the manifold structure of visual data in order to improve performance in general optimization problems subject to linear constraints. As the main theoretical result, we show that manifold constraints can be transferred from the data to the optimized variables if these are linearly correlated. We also show that the resulting optimization problem can be solved with an efficient alternating direction method of multipliers that can consistently integrate the manifold constraints during the optimization process. This leads to a simplification of the approach, which instead of directly optimizing on the manifold, we can iteratively recast the problem as the projection over the manifold via an embedding method. The proposed method is extremely versatile since it can be applied to different problems including Kernel Ridge Regression (KRR) and sparse coding which have numerous applications in machine learning and computer vision. In particular, we apply the methods to different problems such as tracking, object recognition and categorization showing a consistent increase of performance with respect to the state of the art.","Manifold, Transfer learning, Alternating direction method of multipliers, Object tracking, Augmented lagrange multiplier, Image tracking, Image recognition, Object categorization",Baochang Zhang and Alessandro Perina and Ce Li and Qixiang Ye and Vittorio Murino and Alessio {Del Bue},https://www.sciencedirect.com/science/article/pii/S0031320317304594,https://doi.org/10.1016/j.patcog.2017.11.006,0031-3203,2018,87--98,77,Pattern Recognition,Manifold constraint transfer for visual structure-driven optimization,article,ZHANG201887,
"In this paper, a couple of new algorithms for independent component analysis (ICA) are proposed. In the proposed methods, the independent sources are assumed to follow a predefined distribution of the form f(s)=Î±exp(âÎ²|s|p) and a maximum likelihood estimation is used to separate the sources. In the first method, a gradient ascent method is used for the maximum likelihood estimation, while in the second, a non-iterative algorithm is proposed based on the relaxation of the problem. The maximization of the log-likelihood of the estimated source XTw given the parameter p and the data X is shown to be equivalent to the minimization of lp-norm of the projected data XTw. This formulation of ICA has a very close relationship with the Lp-PCA where the maximization of the same objective function is solved. The proposed algorithm solves an approximation of the lp-norm minimization problem for both super-(pâ¯<â¯2) and sub-Gaussian (pâ¯>â¯2) cases and shows superior performance in separating independent sources than the state of the art algorithms for ICA computation.","ICA, PCA, -Norm, Maximum likelihood estimation, Super-Gaussian, Sub-Gaussian",Sungheon Park and Nojun Kwak,https://www.sciencedirect.com/science/article/pii/S0031320317304077,https://doi.org/10.1016/j.patcog.2017.10.006,0031-3203,2018,752--760,76,Pattern Recognition,Independent component analysis by lp-norm optimization,article,PARK2018752,
"Natural Language Processing plays a key role in man-machine interactions, allowing computers to understand and analyze human language. One of its more challenging sub-domains is word sense disambiguation, the task of automatically identifying the intended sense (or concept) of an ambiguous word based on the context in which the word is used. This requires proper feature extraction to capture specific data properties and a dedicated machine learning solution to allow for the accurate labeling of the appropriate sense. However, the pattern classification problem posed here is highly challenging, as we must deal with high-dimensional and multi-class imbalanced data that additionally may be corrupted with class label noise. To address these issues, we propose a local ensemble learning solution. It uses a one-class decomposition of the multi-class problem, assigning an ensemble of one-class classifiers to each of the distributions. The classifiers are trained on the basis of low-dimensional subsets of features and a kernel feature space transformation to obtain a more compact representation. Instance weighting is used to filter out potentially noisy instances and reduce overlapping among classes. Finally, a two-level classifier fusion technique is used to reconstruct the original multi-class problem. Our results show that the proposed learning approach displays robustness to both multi-class skewed distributions and class label noise, making it a useful tool for the considered task.","Machine learning, Natural language processing, Imbalanced classification, Multi-class imbalance, Ensemble learning, One-class classification, Class label noise, Word sense disambiguation",Bartosz Krawczyk and Bridget T. McInnes,https://www.sciencedirect.com/science/article/pii/S0031320317304351,https://doi.org/10.1016/j.patcog.2017.10.028,0031-3203,2018,103--119,78,Pattern Recognition,Local ensemble learning from imbalanced and noisy data for word sense disambiguation,article,KRAWCZYK2018103,
"Historical images are essential documents of the recent past. Nevertheless, time and bad preservation corrupt their physical supports. Digitization can be the solution to extend their âlivesâ, and digital techniques can be used to recover lost information. This task is often difficult and time-consuming, if commercial restoration tools are used for the purpose. A new solution is proposed to help non-expert users in restoring their damaged photos. First, we defined a dual taxonomy for the defects in printed and digitized photos. We represented our restoration domain with an ontology and we created some rules to suggest actions to perform in case of some specific events. Classes and properties of the ontology are included into a knowledge base, that grows dynamically with its use. A prototypal tool and a web application version have been implemented as an interface to the database, and to support non-expert users in the restoration process.","Image restoration, Historical photos, Digitization, Ontology, Knowledge base",E. Ardizzone and H. Dindo and G. Mazzola,https://www.sciencedirect.com/science/article/pii/S0031320317303837,https://doi.org/10.1016/j.patcog.2017.09.031,0031-3203,2018,326--339,74,Pattern Recognition,A knowledge based architecture for the virtual restoration of ancient photos,article,ARDIZZONE2018326,
"Short text clustering is an increasingly important methodology but faces the challenges of sparsity and high-dimensionality of text data. Previous concept decomposition methods have obtained concept vectors via the centroids of clusters using k-means-type clustering algorithms on normal, full texts. In this study, we propose a new concept decomposition method that creates concept vectors by identifying semantic word communities from a weighted word co-occurrence network extracted from a short text corpus or a subset thereof. The cluster memberships of short texts are then estimated by mapping the original short texts to the learned semantic concept vectors. The proposed method is not only robust to the sparsity of short text corpora but also overcomes the curse of dimensionality, scaling to a large number of short text inputs due to the concept vectors being obtained from term-term instead of document-term space. Experimental tests have shown that the proposed method outperforms state-of-the-art algorithms.","Short text clustering, Concept decomposition, Spherical -means, Semantic word community, Community detection",Caiyan Jia and Matthew B. Carson and Xiaoyang Wang and Jian Yu,https://www.sciencedirect.com/science/article/pii/S0031320317303953,https://doi.org/10.1016/j.patcog.2017.09.045,0031-3203,2018,691--703,76,Pattern Recognition,Concept decompositions for short text clustering by identifying word communities,article,JIA2018691,
"Dynamic Time Warping (DTW) is an algorithm to align temporal sequences with possible local non-linear distortions, and has been widely applied to audio, video and graphics data alignments. DTW is essentially a point-to-point matching method under some boundary and temporal consistency constraints. Although DTW obtains a global optimal solution, it does not necessarily achieve locally sensible matchings. Concretely, two temporal points with entirely dissimilar local structures may be matched by DTW. To address this problem, we propose an improved alignment algorithm, named shape Dynamic Time Warping (shapeDTW), which enhances DTW by taking point-wise local structural information into consideration. shapeDTW is inherently a DTW algorithm, but additionally attempts to pair locally similar structures and to avoid matching points with distinct neighborhood structures. We apply shapeDTW to align audio signal pairs having ground-truth alignments, as well as artificially simulated pairs of aligned sequences, and obtain quantitatively much lower alignment errors than DTW and its two variants. When shapeDTW is used as a distance measure in a nearest neighbor classifier (NN-shapeDTW) to classify time series, it beats DTW on 64 out of 84 UCR time series datasets, with significantly improved classification accuracies. By using a properly designed local structure descriptor, shapeDTW improves accuracies by more than 10% on 18 datasets. To the best of our knowledge, shapeDTW is the first distance measure under the nearest neighbor classifier scheme to significantly outperform DTW, which had been widely recognized as the best distance measure to date. Our code is publicly accessible at: https://github.com/jiapingz/shapeDTW.","Dynamic Time Warping, Sequence alignment, Time series classification",Jiaping Zhao and Laurent Itti,https://www.sciencedirect.com/science/article/pii/S0031320317303710,https://doi.org/10.1016/j.patcog.2017.09.020,0031-3203,2018,171--184,74,Pattern Recognition,shapeDTW: Shape Dynamic Time Warping,article,ZHAO2018171,
"In this paper, a new random subspace based ensemble sparse representation (RS_ESR) algorithm is proposed, where the random subspace is introduced into sparse representation model. For high-dimensional data, the random subspace method can not only reduce dimension of data but also make full use of effective information of data. It is not like traditional dimensionality reduction methods that may lose some information of original data. Additionally, a joint sparse representation model is emloyed to obtain the sparse representation of a sample set in the low dimensional random subspace. Then the sparse representations in multiple random subspaces are integrated as an ensemble sparse representation. Moreover, the obtained RS_ESR is applied in classical clustering and semi-supervised classification. The experimental results on different real-world data sets show the superiority of RS_ESR over traditional methods.","Random subspace, Sparse representation, Clustering, Semi-supervised classification",Jing Gu and Licheng Jiao and Fang Liu and Shuyuan Yang and Rongfang Wang and Puhua Chen and Yuanhao Cui and Junhu Xie and Yake Zhang,https://www.sciencedirect.com/science/article/pii/S0031320317303679,https://doi.org/10.1016/j.patcog.2017.09.016,0031-3203,2018,544--555,74,Pattern Recognition,Random subspace based ensemble sparse representation,article,GU2018544,
"Level set methods often suffer from boundary leakage and inadequate segmentation when used to segment images with inhomogeneous intensities. To handle this issue, a novel region-based level set method was developed, in which two different local fitted images are used to construct a hybrid region intensity fitting energy functional. This novel method enables simultaneous segmentation of the regions of interest and estimation of the bias fields from inhomogeneous images. Our experiments on both synthetic images and a publicly available dataset demonstrate the feasibility and reliability of the proposed method.","Level set, Image segmentation, Local fitted images, Intensity inhomogeneity, Bias field",Lei Wang and Jianbing Zhu and Mao Sheng and Adriena Cribb and Shaocheng Zhu and Jiantao Pu,https://www.sciencedirect.com/science/article/pii/S0031320317303436,https://doi.org/10.1016/j.patcog.2017.08.031,0031-3203,2018,145--155,74,Pattern Recognition,Simultaneous segmentation and bias field estimation using local fitted images,article,WANG2018145,
"Matching pedestrians across disjoint camera views, known as person re-identification (re-id), is a challenging problem that is of importance to visual recognition and surveillance. Most existing methods exploit local regions with spatial manipulation to perform matching in local correspondences. However, they essentially extract fixed representations from pre-divided regions for each image and then perform matching based on these extracted representations. For models in this pipeline, local finer patterns that are crucial to distinguish positive pairs from negative ones cannot be captured, and thus making them underperformed. In this paper, we propose a novel deep multiplicative integration gating function, which answers the question of what-and-where to match for effective person re-id. To address what to match, our deep network emphasizes common local patterns by learning joint representations in a multiplicative way. The network comprises two Convolutional Neural Networks (CNNs) to extract convolutional activations, and generates relevant descriptors for pedestrian matching. This leads to flexible representations for pair-wise images. To address where to match, we combat the spatial misalignment by performing spatially recurrent pooling via a four-directional recurrent neural network to impose spatial dependency over all positions with respect to the entire image. The proposed network is designed to be end-to-end trainable to characterize local pairwise feature interactions in a spatially aligned manner. To demonstrate the superiority of our method, extensive experiments are conducted over three benchmark data sets: VIPeR, CUHK03 and Market-1501.","Multiplicative integration gating, Convolutional neural networks, Recurrent neural networks, Person re-identification",Lin Wu and Yang Wang and Xue Li and Junbin Gao,https://www.sciencedirect.com/science/article/pii/S0031320317304004,https://doi.org/10.1016/j.patcog.2017.10.004,0031-3203,2018,727--738,76,Pattern Recognition,What-and-where to match: Deep spatially multiplicative integration networks for person re-identification,article,WU2018727,
"This paper introduces a database of Persian handwritten bank checks. The database includes legal amounts, courtesy amounts, dates, receiver names, signatures and account numbers. In addition to checks, the database also comprises handwritten forms with words used in legal amounts, digits employed in the courtesy amounts and signatures of contributors. The database can be employed for evaluation of segmentation and recognition of different fields in checks as well as for verification of signatures. Data is collected and organized in two series. The first series, comprising 500 hand filled Persian checks and 500 forms contributed by 500 different individuals, supports evaluation of segmentation and recognition tasks. The second series contributed by 100 writers supports evaluation of signature verification systems and comprises 200 genuine checks, 100 forms and 200 forged checks. To provide an insight to other researchers, experiments have been carried out on recognition of dates and courtesy amounts with recognition rates of 76% and 73%, respectively. Likewise, experiments on signature verification with skilled forgeries realized an average error rate of 12.25%. The database, named Persian Handwriting Bank Checks (PHBC) Database, is freely available to the research community upon request to the authors. It is expected that the developed database will not only allow researchers working on automatic processing of Persian checks to evaluate their proposed techniques but will also serve to meaningfully compare different recognition and verification techniques.","Document analysis, Persian handwritten checks, Automatic check processing, Handwritten database, Optical Character Recognition (OCR), Signature verification",Younes Akbari and Mohammad J. Jalili and Javad Sadri and Kazem Nouri and Imran Siddiqi and Chawki Djeddi,https://www.sciencedirect.com/science/article/pii/S0031320317303552,https://doi.org/10.1016/j.patcog.2017.09.011,0031-3203,2018,253--265,74,Pattern Recognition,A novel database for automatic processing of Persian handwritten bank checks,article,AKBARI2018253,
"To improve the performance of ensemble techniques for temporal data clustering, we propose a novel bi-weighted ensemble in this paper to solve the initialization and automated model selection problems encountered by all HMM-based clustering techniques and their applications. Our proposed ensemble features in a bi-weighting scheme in the process of examining each partition and optimizing consensus function on these input partitions in accordance with their level of importance. Within our proposed scheme, the multiple partitions, generated by HMM-based K-models under different initializations, are optimally re-consolidated into a representation of bi-weighted hypergraph, and the final consensus partition is generated and optimized via the agglomerative clustering algorithm in association with a dendrogram-based similarity partitioning (DSPA). In comparison with the existing state of the arts, our proposed approach not only achieves the advantage that the number of clusters can be automatically determined, but also the superior clustering performances on a range of temporal datasets, including synthetic dataset, time series benchmark, and real-world motion trajectory datasets.","Data clustering, Ensemble learning, Hidden Markov Model, Model selection",Yun Yang and Jianmin Jiang,https://www.sciencedirect.com/science/article/pii/S0031320317304764,https://doi.org/10.1016/j.patcog.2017.11.022,0031-3203,2018,391--403,76,Pattern Recognition,Bi-weighted ensemble via HMM-based approaches for temporal data clustering,article,YANG2018391,
"In this paper, we are interested in making decisions by combining classifiers providing uncertain outputs, in the form of sets of probability distributions. More precisely, each classifier provides lower and upper bounds on the conditional probabilities of the associated classes. The classifiers are combined by computing the set of unconditional probability distributions compatible with these bounds, by solving linear optimization problems. When the classifier outputs are inconsistent, we propose a correcting step that restores this consistency. The experiments show the interest of our approach for solving multi-class classification problems, particularly when information is scarce (i.e., a limited number of classifiers is available). In this case, modeling the lack of information associated with classifier outputs gives good results even when they are poorly regularized or overfit the data.","Classifier combination, Reasoning under uncertainty, Cautious predictions",Benjamin Quost and SÃ©bastien Destercke,https://www.sciencedirect.com/science/article/pii/S0031320317304247,https://doi.org/10.1016/j.patcog.2017.10.019,0031-3203,2018,412--425,77,Pattern Recognition,Classification by pairwise coupling of imprecise probabilities,article,QUOST2018412,
"In this article, we propose a multiscale method of embedding a graph into a vector space using diffusion wavelets. At each scale, we extract a detail subspace and a corresponding lower-scale approximation subspace to represent the graph. Representative features are then extracted at each scale to provide a scale-space description of the graph. The lower-scale is constructed using a super-node merging strategy based on nearest neighbor or maximum participation and the new adjacency matrix is generated using vertex identification. This approach allows the comparison of graphs where the important structural differences may be present at varying scales. Additionally, this method can improve the differentiating power of the embedded vectors and this property reduces the possibility of cospectrality typical in spectral methods, substantially. The experimental results show that augmenting the features of abstract levels to the graph features increases the graph classification accuracies in different datasets.","Spectral graph embedding, Diffusion wavelet, Multi-resolution analysis, Graph summarization, Scale space",Hoda Bahonar and Abdolreza Mirzaei and Richard C. Wilson,https://www.sciencedirect.com/science/article/pii/S0031320317303795,https://doi.org/10.1016/j.patcog.2017.09.030,0031-3203,2018,518--530,74,Pattern Recognition,Diffusion wavelet embedding: A multi-resolution approach for graph embedding in vector space,article,BAHONAR2018518,
"The classification of dynamical data streams is among the most complex problems encountered in classification. This is, firstly, because the distribution of the data streams is non-stationary, and it changes without any prior âwarningâ. Secondly, the manner in which it changes is also unknown. Thirdly, and more interestingly, the model operates with the assumption that the correct classes of previously-classified patterns become available at a juncture after their appearance. This paper pioneers the use of unreported novel schemes that can classify such dynamical data streams by invoking the recently-introduced âAnti-Bayesianâ (AB) techniques. Contrary to the Bayesian paradigm, that compare the testing sample with the distributionâs central points, AB techniques are based on the information in the distant-from-the-mean samples. Most Bayesian approaches can be naturally extended to dynamical systems by dynamically tracking the mean of each class using, for example, the exponential moving average based estimator, or a sliding window estimator. The AB schemes introduced by Oommen etÂ al.., on the other hand, work with a radically different approach and with the non-central quantiles of the distributions. Surprisingly and counter-intuitively, the reported AB methods work equally or close-to-equally well to an optimal supervised Bayesian scheme on a host of accepted Pattern Recognition problems. This thus begs its natural extension to the unexplored arena of classification for dynamical data streams. Naturally, for such an AB classification approach, we need to track the non-stationarity of the quantiles of the classes. To achieve this, in this paper, we develop an AB approach for the online classification of data streams by applying the efficient and robust quantile estimators developed by Yazidi and Hammer [12,37]. Apart from the methodology itself, in this paper, we compare the Bayesian and AB approaches using both real-life and synthetic data. The results demonstrate the intriguing and counter-intuitive results that the AB approach, sometimes, actually outperforms the Bayesian approach for this application both with respect to the peak performance obtained, and the robustness of the choice of the respective tuning parameters. Furthermore, the AB approach is much more robust against outliers, which is an inherent property of quantile estimators [12,37], which is a property that the Bayesian approach cannot match, since it rather tracks the mean.","Anti-Bayesian classification, Data streams, Classification with delay, Incremental quantile estimation",Hugo Lewi Hammer and Anis Yazidi and B. John Oommen,https://www.sciencedirect.com/science/article/pii/S0031320317304363,https://doi.org/10.1016/j.patcog.2017.10.031,0031-3203,2018,108--124,76,Pattern Recognition,On the classification of dynamical data streams using novel âAnti-Bayesianâ techniques,article,HAMMER2018108,
"Classification of hyperspectral images (HSI) has been a challenging problem under active investigation for years especially due to the extremely high data dimensionality and limited number of samples available for training. It is found that hyperspectral image classification can be generally improved only if the feature extraction technique and the classifier are both addressed. In this paper, a novel classification framework for hyperspectral images based on the joint bilateral filter and sparse representation classification (SRC) is proposed. By employing the first principal component as the guidance image for the joint bilateral filter, spatial features can be extracted with minimum edge blurring thus improving the quality of the band-to-band images. For this reason, the performance of the joint bilateral filter has shown better than that of the conventional bilateral filter in this work. In addition, the spectral similarity-based joint SRC (SS-JSRC) is proposed to overcome the weakness of the traditional JSRC method. By combining the joint bilateral filtering and SS-JSRC together, the superiority of the proposed classification framework is demonstrated with respect to several state-of-the-art spectral-spatial classification approaches commonly employed in the HSI community, with better classification accuracy and Kappa coefficient achieved.","Hyperspectral imaging, Joint bilateral filtering, Sparse representation, Feature extraction, Data classification",Tong Qiao and Zhijing Yang and Jinchang Ren and Peter Yuen and Huimin Zhao and Genyun Sun and Stephen Marshall and Jon Atli Benediktsson,https://www.sciencedirect.com/science/article/pii/S0031320317304089,https://doi.org/10.1016/j.patcog.2017.10.008,0031-3203,2018,316--328,77,Pattern Recognition,Joint bilateral filtering and spectral similarity-based sparse representation: A generic framework for effective feature extraction and data classification in hyperspectral imaging,article,QIAO2018316,
"In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.","Convolutional neural network, Deep learning",Jiuxiang Gu and Zhenhua Wang and Jason Kuen and Lianyang Ma and Amir Shahroudy and Bing Shuai and Ting Liu and Xingxing Wang and Gang Wang and Jianfei Cai and Tsuhan Chen,https://www.sciencedirect.com/science/article/pii/S0031320317304120,https://doi.org/10.1016/j.patcog.2017.10.013,0031-3203,2018,354--377,77,Pattern Recognition,Recent advances in convolutional neural networks,article,GU2018354,
"Recovering facial albedo from low quality face images is a challenging task which arises when face recognition is attempted in the wild. Low quality of facial images is usually caused by extrinsic factors such as low resolution and noises, and intrinsic ones such as expressions. Existing research recovers facial albedo by dealing with the extrinsic and intrinsic factors separately. However, it is more natural and potentially more useful to approach albedo recovery by removing the two effects simultaneously. In this paper, we present a novel framework which can recover facial albedo by jointly solving these for both the extrinsic and intrinsic sources of uncertainty. This framework models albedo recovery problem by a joint optimization process which alternatively (1) removes intra-personal variations and (2) performs super resolution. To deal with the intrinsic sources of albedo variability, we use a linear model. To handle extrinsic problems associated with low quality imaging, we use a sparse coding method which is applied to super resolution. The proposed method can also significantly improve the performance of face recognition and clustering in case of very low resolution and in the presence of various facial variations. Extensive experiments and comparisons are conducted on the AR and FERET face databases. Experimental results show the effectiveness of the proposed method.","Facial albedo estimation, Low quality facial image, Sparse coding, ADMM",Xu Chen and Zhihong Zhang and Beizhan Wang and Guosheng Hu and Edwin R. Hancock,https://www.sciencedirect.com/science/article/pii/S0031320317303709,https://doi.org/10.1016/j.patcog.2017.09.019,0031-3203,2018,373--384,74,Pattern Recognition,Recovering variations in facial albedo from low resolution images,article,CHEN2018373,
"Learning a scene-specific distance metric from labeled data is critical for person re-identification. Most of the earlier works in this area aim to seek a linear transformation of the feature space such that relevant dimensions are emphasized while irrelevant ones are discarded in a global sense. However, when training data exhibit multi-modality transitions, the globally learned metric would deviate from the correct metrics learned from each modality. In this study, we propose a multi-modality mining approach for metric learning (M3L) to automatically discover multiple modalities of illumination changes by exploring the shift-invariant property in log-chromaticity space, and then learn a sub-metric for each modality to maximally reduce the bias derived from metric learning model with global sense. The experiments on the challenging VIPeR dataset and the fusion dataset VIPeR&PRID 450S have validated the effectiveness of the proposed method with an average improvement of 2â7% over original baseline methods.","Person re-identification, Multi-modality mining, Diagonal model, Metric learning",Xiaokai Liu and Xiaorui Ma and Jie Wang and Hongyu Wang,https://www.sciencedirect.com/science/article/pii/S003132031730393X,https://doi.org/10.1016/j.patcog.2017.09.041,0031-3203,2018,650--661,76,Pattern Recognition,M3L: Multi-modality mining for metric learning in person re-Identification,article,LIU2018650,
"In this paper, we propose a novel skeletal representation that models the human body as the articulated interconnections of multiple rigid bodies. An articulated human action can be viewed as the combination of multiple rigid body motion trajectories. The Dual Square-Root Function (DSRF) descriptor is firstly introduced by calculating the gradient-based invariants for representing 6-D rigid body motion trajectories, which can offer substantial advantages over raw data. To effectively incorporate the DSRF descriptors in the skeletal representation, a skeleton is decomposed into five parts and the most informative part method is raised for offering compact representation. Besides, two rigid body configurations are investigated for representing the movement of each part. In the recognition stage, we first follow the simple template matching strategy with the nearest neighbor classifier. A robust distance measure between two skeletal representations is designed. In addition, the DSRF-based skeletal representation can be encoded as a sparse histogram by the bag-of-words approach. Then, a support vector machine classifier with chi-square kernel is trained for multiclass recognition tasks. Experimental results on three benchmark datasets demonstrate that our proposed approach outperforms existing skeleton representations in terms of recognition accuracy.","Articulated human action recognition, Rigid body motion trajectory, Dual square root function descriptor, Most informative part, Virtual rigid body",Yao Guo and Youfu Li and Zhanpeng Shao,https://www.sciencedirect.com/science/article/pii/S0031320317304417,https://doi.org/10.1016/j.patcog.2017.10.034,0031-3203,2018,137--148,76,Pattern Recognition,DSRF: A flexible trajectory descriptor for articulated human action recognition,article,GUO2018137,
"The modified quadratic discriminant function (MQDF) is an effective classifier for handwritten Chinese character recognition (HCCR). However, it suffers from high memory requirement for the storage of its parameters, which makes it impractical to be embedded in memory limited hand-held devices. In this paper, we explore the applicability of sparse coding to build compact MQDF classifiers. To be specific, we use sparse coding to compact the parameters of MQDF. Two methods of sparse coding, viz., the maximum likelihood-based method and the K-SVD method, are adopted to build two compact MQDF classifiers, namely, MQDF-ML classifier and MQDF-KSVD classifier. Furthermore, we learn multiple dictionaries rather than single dictionary for sparse coding, because the multiple dictionary learning is capable of not only greatly reducing the computational complexity, but also alleviating the degradation of recognition accuracy, compared to the single dictionary learning. Experiments and comparison with the existing method have demonstrated the effectiveness of our proposed method for the issue of unconstrained handwritten Chinese character recognition.","Sparse coding, Compact MQDF classifier, Multiple dictionary learning, Handwritten Chinese character recognition",Xiaohua Wei and Shujing Lu and Yue Lu,https://www.sciencedirect.com/science/article/pii/S0031320317303965,https://doi.org/10.1016/j.patcog.2017.09.044,0031-3203,2018,679--690,76,Pattern Recognition,Compact MQDF classifiers using sparse coding for handwritten Chinese character recognition,article,WEI2018679,
"Pose and illumination variations are very challenging for face recognition with a single sample per person (SSPP). In this paper, we address this issue by a Pose-Aware Metric Learning (PAML) approach. Our primary idea is âfrom one to manyâ: Synthesizing many images of sufficient pose and illumination variability from the single training image, based on which metric learning approach is applied to reduce these âsynthesizedâ variations at each quantified pose. For this purpose, given a single frontal training image, a multi-depth generic elastic model and an extended generic elastic model are developed to synthesize facial images of the target pose with varying 3D shape (depth) and illumination variations respectively. To reduce these âsynthesizedâ variability, Pose-Aware Metric spaces are separately learnt by linear regression analysis at each quantized pose, and pose-invariant recognition is performed in the corresponding metric space. By preserving the detailed texture and reducing the shape variability, the PAML method achieves an 100% accuracy on the Multi-PIE database under the test setting across poses, which is significantly better than the traditional methods that use a large generic image ensemble to learn the cross-pose transformations. On the more challenging setting across both poses and illuminations, PAML outperforms the recent deep learning approaches by over 10% accuracy.","Face recognition, Single sample per person, Metric learning, 3D generic elastic model, Face re-rendering, 3D face construction",Weihong Deng and Jiani Hu and Zhongjun Wu and Jun Guo,https://www.sciencedirect.com/science/article/pii/S0031320317304259,https://doi.org/10.1016/j.patcog.2017.10.020,0031-3203,2018,426--437,77,Pattern Recognition,From one to many: Pose-Aware Metric Learning for single-sample face recognition,article,DENG2018426,
"We are surrounded by plenty of information about our environment. From these multiple sources, numerous data could be extracted: set of images, 3D model, coloured points cloud... When classical localization devices failed (e.g. GPS sensor in cluttered environments), aforementioned data could be used within a localization framework. This is called Visual Based Localization (VBL). Due to numerous data types that can be collected from a scene, VBL encompasses a large amount of different methods. This paper presents a survey about recent methods that localize a visual acquisition system according to a known environment. We start by categorizing VBL methods into two distinct families: indirect and direct localization systems. As the localization environment is almost always dynamic, we pay special attention to methods designed to handle appearances changes occurring in a scene. Thereafter, we highlight methods exploiting heterogeneous types of data. Finally, we conclude the paper with a discussion on promising trends that could permit to a localization system to reach high precision pose estimation within an area as large as possible.","Image-based localization, Visual geo-localization, Camera relocalisation, Pose estimation",Nathan Piasco and DÃ©sirÃ© SidibÃ© and CÃ©dric Demonceaux and ValÃ©rie Gouet-Brunet,https://www.sciencedirect.com/science/article/pii/S0031320317303448,https://doi.org/10.1016/j.patcog.2017.09.013,0031-3203,2018,90--109,74,Pattern Recognition,A survey on Visual-Based Localization: On the benefit of heterogeneous data,article,PIASCO201890,
"While face recognition has been intensively studied in the literature, there are only a few attempts on visual recognition of multiple faces simultaneously in videos, which has potential applications in practical video surveillance. In this paper, we address the problem of visual recognition and tracking of multiple faces in real-world videos involving large pose variation and occlusion. Instead of recognizing individual face independently, we introduce the constraints of inter-frame temporal smoothness and within-frame identity exclusivity on multiple faces in videos, and model the tasks of multiple face recognition (MFR) and multiple face tracking (MFT) jointly in an alternative optimization framework. We show this joint formulation for two different tasks leads to significantly improved MFR accuracy. Specifically, as appearance matching for face instances over consecutive frames plays a critical role in MFT, we propose an identity-specific metric learning method with a part-based object representation to learn a localized transformation for each face subject in an online manner, under which face instances of the same subject over consecutive frames are pulled as close as possible, while those of different subjects are pushed far away. Empirically, we evaluate our method on several MFR sequences against baselines, and the results demonstrate that our method can achieve improved accuracy performance in various challenging recognition scenarios.","Multiple face recognition, Multiple face tracking, Integer programming, Localized metric learning",Xiuzhuang Zhou and Kai Jin and Qian Chen and Min Xu and Yuanyuan Shang,https://www.sciencedirect.com/science/article/pii/S0031320317303722,https://doi.org/10.1016/j.patcog.2017.09.022,0031-3203,2018,41--50,75,Pattern Recognition,Multiple face tracking and recognition with identity-specific localized metric learning,article,ZHOU201841,Distance Metric Learning for Pattern Recognition
"Focus stacking is a computational technique to extend the Depth of Field (DOF) through combining multiple images taken at various focus distances. However, existing focus stacking methods could not cope with false edges produced by propagation of blur kernels, and are affected by colored texture in the stack. In this work, we propose a novel all-in-focus method based on directional-max-gradient flow (DMGF) and labeled iterative depth propagation. Firstly, we present a novel directional-max-gradient flow to describe gradient propagation along different directions in the stack to remove false edges and preserve accurate depth values of both strong and weak edges(also called source points). Then the source points are further labeled as in-plane edges and off-plane edges by unsupervised classification technique. Finally in our proposed labeled iterative Laplacian optimization, these edges are utilized to remove artifacts produced by colored texture in the stack and refine the all-in-focus image. Extensive experiments on both synthesized data and real data show that our method has achieved superior performance to state-of-the-art methods.","Focus stacking, Directional-max-gradient flow, Blur kernel, Depthmap, Laplacian optimization",Guijin Wang and Wentao Li and Xuanwu Yin and Huazhong Yang,https://www.sciencedirect.com/science/article/pii/S003132031730448X,https://doi.org/10.1016/j.patcog.2017.10.040,0031-3203,2018,173--187,77,Pattern Recognition,All-in-focus with directional-max-gradient flow and labeled iterative depth propagation,article,WANG2018173,
"Modern technologies have been producing data with complex intrinsic structures, which can be naturally represented as two-dimensional matrices, such as gray digital images, and electroencephalographyÂ (EEG) signals. When processing these data for classification, traditional classifiers, such as support vector machineÂ (SVM) and logistic regression, have to reshape each input matrix into a feature vector, resulting in the loss of structural information. In contrast, modern classification methods such as support matrix machine capture these structures by regularizing the regression matrix to be low-rank. These methods assume that all entities within each input matrix can serve as the explanatory features for its label. However, in real-world applications, many features are redundant and useless for certain classification tasks, thus it is important to perform feature selection to filter out redundant features for more interpretable modeling. In this paper, we tackle this issue, and propose a novel classification technique called Sparse Support Matrix MachineÂ (SSMM), which is favored for taking both the intrinsic structure of each input matrix and feature selection into consideration simultaneously. The proposed SSMM is defined as a hinge loss for model fitting, with a new regularization on the regression matrix. Specifically, the new regularization term is a linear combination of nuclear norm and â1 norm, to consider the low-rank property and sparse property respectively. The resulting optimization problem is convex, and motivates us to propose a novel and efficient generalized forward-backward algorithm for solving it. To evaluate the effectiveness of our method, we conduct comparative studies on the applications of both image and EEG data classification problems. Our approach achieves state-of-the-art performance consistently. It shows the promise of our SSMM method on real-world applications.","Classification, Support vector machine, Matrix analysis, Sparse, Low rank",Qingqing Zheng and Fengyuan Zhu and Jing Qin and Badong Chen and Pheng-Ann Heng,https://www.sciencedirect.com/science/article/pii/S0031320317304016,https://doi.org/10.1016/j.patcog.2017.10.003,0031-3203,2018,715--726,76,Pattern Recognition,Sparse Support Matrix Machine,article,ZHENG2018715,
"With flexible data description ability, one-class Support Vector Machine (OCSVM) is one of the most popular and widely-used methods for one-class classification (OCC). Nevertheless, the performance of OCSVM strongly relies on its hyperparameter selection, which is still a challenging open problem due to the absence of outlier data. This paper proposes a fully automatic OCSVM hyperparameter selection method, which requires no tuning of additional hyperparameter, based on a novel self-adaptive âdata shiftingâ mechanism: Firstly, by efficient edge pattern detection (EPD) and ânegativelyâ shifting edge patterns along the negative direction of estimated data density gradient, a constrained number of high-quality pseudo outliers are self-adaptively generated at more desirable locations, which readily avoids two major difficulties in previous outlier generation methods. Secondly, to avoid time-consuming cross-validation and enhance robustness to noise in the given training data, a pseudo target set is generated for model validation by âpositivelyâ shifting each given target datum along the positive direction of data density gradient. Experiments on synthetic and benchmark datasets demonstrate the effectiveness of the proposed method.","One-class SVM, Hyperparameter selection, Data shifting",Siqi Wang and Qiang Liu and En Zhu and Fatih Porikli and Jianping Yin,https://www.sciencedirect.com/science/article/pii/S0031320317303564,https://doi.org/10.1016/j.patcog.2017.09.012,0031-3203,2018,198--211,74,Pattern Recognition,Hyperparameter selection of one-class support vector machine by self-adaptive data shifting,article,WANG2018198,
"Time series averaging in dynamic time warping (DTW) spaces has been successfully applied to improve pattern recognition systems. This article proposes and analyzes subgradient methods for the problem of finding a sample mean in DTW spaces. The class of subgradient methods generalizes existing sample mean algorithms such as DTW Barycenter Averaging (DBA). We show that DBA is a majorize-minimize algorithm that converges to necessary conditions of optimality after finitely many iterations. Empirical results show that for increasing sample sizes the proposed stochastic subgradient (SSG) algorithm is more stable and finds better solutions in shorter time than the DBA algorithm on average. Therefore, SSG is useful in online settings and for non-small sample sizes. The theoretical and empirical results open new paths for devising sample mean algorithms: nonsmooth optimization methods and modified variants of pairwise averaging methods.","Dynamic time warping, Time series averaging, Sample mean, FrÃ©chet function, Subgradient methods",David Schultz and Brijnesh Jain,https://www.sciencedirect.com/science/article/pii/S0031320317303163,https://doi.org/10.1016/j.patcog.2017.08.012,0031-3203,2018,340--358,74,Pattern Recognition,Nonsmooth analysis and subgradient methods for averaging in dynamic time warping spaces,article,SCHULTZ2018340,
"This paper introduces the Bag of Graphs (BoG), a Bag-of-Words model that encodes in graphs the local structures of a digital object. We present a formal definition, introducing concepts and rules that make this model flexible and adaptable for different applications. We define two BoG-based methods â Bag of Singleton Graphs (BoSG) and Bag of Visual Graphs (BoVG), which create vector representations for graphs and images, respectively. We evaluate the Bag of Singleton Graphs (BoSG) for graph classification on four datasets of the IAM repository, obtaining significant results in accuracy and execution time. The method Bag of Visual Graphs (BoVG) is evaluated for image classification on Caltech and ALOI datasets, and for remote sensing image classification on images of Monte Santo and Campinas datasets. This framework opens possibilities for retrieval, classification, and clustering tasks on large datasets that use graph-based representations impractical before due to the complexity of inexact graph matching.","Bag models, Graph matching, Graph-based retrieval, Graph-based classification",Fernanda B. Silva and Rafael de O. Werneck and Siome Goldenstein and Salvatore Tabbone and Ricardo da S. Torres,https://www.sciencedirect.com/science/article/pii/S0031320317303680,https://doi.org/10.1016/j.patcog.2017.09.018,0031-3203,2018,266--285,74,Pattern Recognition,Graph-based bag-of-words for classification,article,SILVA2018266,
"Dynamic Textures (DTs) are sequences of images of moving scenes that exhibit certain stationarity properties in time such as smoke, vegetation and fire. The analysis of DT is important for recognition, segmentation, synthesis or retrieval for a range of applications including surveillance, medical imaging and remote sensing. Convolutional Neural Networks (CNNs) have recently proven to be well suited for texture analysis with a design similar to filter banks. We develop a new DT analysis method based on a CNN method applied on three orthogonal planes. We train CNNs on spatial frames and temporal slices extracted from the DT sequences and combine their outputs to obtain a competitive DT classifier trained end-to-end. Our results on a wide range of commonly used DT classification benchmark datasets prove the robustness of our approach. Significant improvement of the state of the art is shown on the larger datasets.","Dynamic texture, Image recognition, Convolutional neural network, Filter banks, Spatiotemporal analysis",Vincent Andrearczyk and Paul F. Whelan,https://www.sciencedirect.com/science/article/pii/S0031320317304375,https://doi.org/10.1016/j.patcog.2017.10.030,0031-3203,2018,36--49,76,Pattern Recognition,Convolutional neural network on three orthogonal planes for dynamic texture classification,article,ANDREARCZYK201836,
