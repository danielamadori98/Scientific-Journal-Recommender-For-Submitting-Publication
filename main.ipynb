{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMdvYrtPfZqF"
   },
   "source": [
    "# **Scientific journal recommender for submitting a publication**\n",
    "\n",
    "Parser:  \n",
    "https://it.wikipedia.org/wiki/BibTeX  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NAniabkkfix9",
    "ExecuteTime": {
     "end_time": "2024-01-07T15:59:55.230909200Z",
     "start_time": "2024-01-07T15:59:55.216912800Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "folder = \"datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "For each class (journal) there is a file in BibTeX format containing the articles published in that journal. Each file was cleaned and formatted with the following online tool [BibTeX Tidy](https://flamingtempura.github.io/bibtex-tidy/index.html).\n",
    "\n",
    "Each article is represented by a record with the following fields:\n",
    "* **abstract**: Abstract of the article.\n",
    "* **author**: Author of the article.\n",
    "* **ENTRYTYPE**: Type of entry (article, book, inproceedings, etc.).\n",
    "* **doi**: Digital Object Identifier of the article.\n",
    "* **ID**: Unique identifier of the article.\n",
    "* **issn**: International Standard Serial Number of the journal in which the article was published.\n",
    "* **journal**: Journal in which the article was published.\n",
    "* **keywords**: Keywords of the article.\n",
    "* **note**: Additional information about the article.\n",
    "* **pages**: Pages of the article.\n",
    "* **title**: Title of the article.\n",
    "* **url**: URL of the article.\n",
    "* **volume**: Volume of the journal in which the article was published.\n",
    "* **year**: Year of publication of the article.\n",
    "\n",
    "The goal is to create a model that is able to predict the **journal** in which it will be published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T02:22:34.485651200Z",
     "start_time": "2024-01-07T02:22:34.351082700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "4-CVi1n56Ygf",
    "outputId": "cbf9ad07-1618-4f09-8aeb-9d6988a54697"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bibtexparser\n",
    "import pandas as pd\n",
    "\n",
    "# 1, 3, 4, 5\n",
    "def read_bib_to_dataframe(file_path):\n",
    "    #with open(file_path, 'r', encoding='utf-8') as bibtex_file:\n",
    "    with open(file_path, 'r', encoding='latin-1') as bibtex_file:\n",
    "        return bibtexparser.load(bibtex_file)\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".bib\"):\n",
    "        filename_path = os.path.join(folder, filename)\n",
    "        bib_data = read_bib_to_dataframe(filename_path)\n",
    "        if bib_data.entries:\n",
    "            df = pd.DataFrame(bib_data.entries)\n",
    "            df.to_csv(os.path.splitext(filename_path)[0] + '.csv', index=False)\n",
    "        else:\n",
    "            print(\"Error: \", filename, \" is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-06T19:02:46.048240400Z",
     "start_time": "2024-01-06T19:01:17.886723400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6768\\3584873582.py:8: DtypeWarning: Columns (2,3,4,5,7,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs.append(pd.read_csv(os.path.join(folder, filename)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>doi</th>\n",
       "      <th>issn</th>\n",
       "      <th>year</th>\n",
       "      <th>pages</th>\n",
       "      <th>volume</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>ID</th>\n",
       "      <th>note</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>journal_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This study proposed to investigate the thermal...</td>\n",
       "      <td>Virtual reality headsets, Thermal comfort, Mic...</td>\n",
       "      <td>Zihao Wang and Renke He and Ke Chen</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>https://doi.org/10.1016/j.apergo.2020.103066</td>\n",
       "      <td>0003-6870</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>103066</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "      <td>Thermal comfort and virtual reality headsets</td>\n",
       "      <td>article</td>\n",
       "      <td>WANG2020103066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A one-pedal system for operating an electric v...</td>\n",
       "      <td>One-pedal operation, Electric vehicle, Emotion...</td>\n",
       "      <td>Fumie Sugimoto and Motohiro Kimura and Yuji Ta...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>https://doi.org/10.1016/j.apergo.2020.103179</td>\n",
       "      <td>0003-6870</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>103179</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "      <td>Effects of one-pedal automobile operation on t...</td>\n",
       "      <td>article</td>\n",
       "      <td>SUGIMOTO2020103179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surgery has changed significantly in recent ye...</td>\n",
       "      <td>Human reliability analysis (HRA), HEART, Dynam...</td>\n",
       "      <td>Rossella Onofrio and Paolo Trucco</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>https://doi.org/10.1016/j.apergo.2020.103150</td>\n",
       "      <td>0003-6870</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>103150</td>\n",
       "      <td>88.0</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "      <td>A methodology for Dynamic Human Reliability An...</td>\n",
       "      <td>article</td>\n",
       "      <td>ONOFRIO2020103150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Truck platoon driving is a current branch of a...</td>\n",
       "      <td>Truck platoon driving, Technology acceptance, ...</td>\n",
       "      <td>Sarah-Maria Castritius and Heiko Hecht and Joh...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>https://doi.org/10.1016/j.apergo.2019.103042</td>\n",
       "      <td>0003-6870</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>103042</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "      <td>Acceptance of truck platooning by professional...</td>\n",
       "      <td>article</td>\n",
       "      <td>CASTRITIUS2020103042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This paper presents a new handle for instrumen...</td>\n",
       "      <td>Laparoscopic surgery, Handle design, Biomechanics</td>\n",
       "      <td>Ramon Sancibrian and Carlos Redondo-Figuero an...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>https://doi.org/10.1016/j.apergo.2020.103210</td>\n",
       "      <td>0003-6870</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>103210</td>\n",
       "      <td>89.0</td>\n",
       "      <td>Applied Ergonomics</td>\n",
       "      <td>Ergonomic evaluation and performance of a new ...</td>\n",
       "      <td>article</td>\n",
       "      <td>SANCIBRIAN2020103210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  This study proposed to investigate the thermal...   \n",
       "1  A one-pedal system for operating an electric v...   \n",
       "2  Surgery has changed significantly in recent ye...   \n",
       "3  Truck platoon driving is a current branch of a...   \n",
       "4  This paper presents a new handle for instrumen...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  Virtual reality headsets, Thermal comfort, Mic...   \n",
       "1  One-pedal operation, Electric vehicle, Emotion...   \n",
       "2  Human reliability analysis (HRA), HEART, Dynam...   \n",
       "3  Truck platoon driving, Technology acceptance, ...   \n",
       "4  Laparoscopic surgery, Handle design, Biomechanics   \n",
       "\n",
       "                                              author  \\\n",
       "0                Zihao Wang and Renke He and Ke Chen   \n",
       "1  Fumie Sugimoto and Motohiro Kimura and Yuji Ta...   \n",
       "2                  Rossella Onofrio and Paolo Trucco   \n",
       "3  Sarah-Maria Castritius and Heiko Hecht and Joh...   \n",
       "4  Ramon Sancibrian and Carlos Redondo-Figuero an...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.sciencedirect.com/science/article/...   \n",
       "1  https://www.sciencedirect.com/science/article/...   \n",
       "2  https://www.sciencedirect.com/science/article/...   \n",
       "3  https://www.sciencedirect.com/science/article/...   \n",
       "4  https://www.sciencedirect.com/science/article/...   \n",
       "\n",
       "                                            doi       issn    year   pages  \\\n",
       "0  https://doi.org/10.1016/j.apergo.2020.103066  0003-6870  2020.0  103066   \n",
       "1  https://doi.org/10.1016/j.apergo.2020.103179  0003-6870  2020.0  103179   \n",
       "2  https://doi.org/10.1016/j.apergo.2020.103150  0003-6870  2020.0  103150   \n",
       "3  https://doi.org/10.1016/j.apergo.2019.103042  0003-6870  2020.0  103042   \n",
       "4  https://doi.org/10.1016/j.apergo.2020.103210  0003-6870  2020.0  103210   \n",
       "\n",
       "   volume             journal  \\\n",
       "0    85.0  Applied Ergonomics   \n",
       "1    88.0  Applied Ergonomics   \n",
       "2    88.0  Applied Ergonomics   \n",
       "3    85.0  Applied Ergonomics   \n",
       "4    89.0  Applied Ergonomics   \n",
       "\n",
       "                                               title ENTRYTYPE  \\\n",
       "0       Thermal comfort and virtual reality headsets   article   \n",
       "1  Effects of one-pedal automobile operation on t...   article   \n",
       "2  A methodology for Dynamic Human Reliability An...   article   \n",
       "3  Acceptance of truck platooning by professional...   article   \n",
       "4  Ergonomic evaluation and performance of a new ...   article   \n",
       "\n",
       "                     ID note combined_text  journal_num  \n",
       "0        WANG2020103066  NaN           NaN          NaN  \n",
       "1    SUGIMOTO2020103179  NaN           NaN          NaN  \n",
       "2     ONOFRIO2020103150  NaN           NaN          NaN  \n",
       "3  CASTRITIUS2020103042  NaN           NaN          NaN  \n",
       "4  SANCIBRIAN2020103210  NaN           NaN          NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "dfs = []\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        dfs.append(pd.read_csv(os.path.join(folder, filename)))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.to_csv(folder + 'all.csv', index=False)\n",
    "\n",
    "for tmp_df in dfs:\n",
    "    tmp_df = None\n",
    "dfs = None\n",
    "\n",
    "gc.collect()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "The following features are selected:\n",
    "* **abstract**: Abstract of the article.\n",
    "* **keywords**: Keywords of the article.\n",
    "* **title**: Title of the article.\n",
    "\n",
    "The target feature is:\n",
    "* **journal**: Journal in which the article was published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "id": "I7xjj5RY6V_p",
    "outputId": "fb5f6523-278a-43aa-d340-602cc38d6adf",
    "ExecuteTime": {
     "end_time": "2024-01-07T16:00:56.407862300Z",
     "start_time": "2024-01-07T15:59:56.044486500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_224\\1325182269.py:3: DtypeWarning: Columns (0,1,2,3,4,5,7,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(folder + 'all.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(661201, 16)\n",
      "\n",
      " (661201, 4) \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 661201 entries, 0 to 661200\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   abstract  444022 non-null  object\n",
      " 1   keywords  443826 non-null  object\n",
      " 2   title     444311 non-null  object\n",
      " 3   journal   444311 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 20.2+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 443820 entries, 0 to 444310\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   abstract  443820 non-null  object\n",
      " 1   keywords  443820 non-null  object\n",
      " 2   title     443820 non-null  object\n",
      " 3   journal   443820 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 16.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Removing unnecessary columns\n",
    "import pandas as pd\n",
    "df = pd.read_csv(folder + 'all.csv')\n",
    "\n",
    "feature_names = ['abstract', 'keywords', 'title']\n",
    "target_name = 'journal'\n",
    "\n",
    "print(df.shape)\n",
    "df = df[feature_names + [target_name]]\n",
    "print('\\n',df.shape,'\\n')\n",
    "df.info()\n",
    "\n",
    "df = df.dropna()\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "df.to_csv(folder + 'selected.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "aG1BuPv2oREW",
    "outputId": "9de22848-a359-42d9-836c-9c32d7fbcc97",
    "ExecuteTime": {
     "end_time": "2024-01-07T17:44:30.056597100Z",
     "start_time": "2024-01-07T16:02:41.111504100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 35\u001B[0m\n\u001B[0;32m     33\u001B[0m df\u001B[38;5;241m.\u001B[39mto_csv(folder \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mselected_cleaned.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     34\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 35\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "# Cleaning data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "language = 'english'\n",
    "# Convert to lowercase\n",
    "df[feature_names] = df[feature_names].applymap(lambda x: str(x).lower())\n",
    "# Remove stopwords\n",
    "stopwords_list = stopwords.words(language)\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(lambda words: ' '.join([w for w in words.split() if w not in stopwords_list])))\n",
    "# Remove punctuation\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.str.translate(str.maketrans('', '', string.punctuation)))\n",
    "# Stemming\n",
    "stemmer = nltk.stem.SnowballStemmer(language=language)\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(lambda words: ' '.join([stemmer.stem(w) for w in words.split()])))\n",
    "# Tokenize\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(nltk.word_tokenize))\n",
    "\n",
    "#add journal_num column with numeric integer to process\n",
    "labels = df[target_name].unique()\n",
    "labels_value = list(labels)\n",
    "# Ordina la lista in ordine alfabetico e assegna i rank\n",
    "ranks_ordinati = [rank for rank, _ in sorted(enumerate(labels_value, start=0), key=lambda x: x[1])] # Estrai solo i rank ordinati\n",
    "target_name_num = target_name + '_num'\n",
    "df[target_name_num] = df[target_name].map(dict(zip(labels_value, ranks_ordinati)))\n",
    "\n",
    "df.head()\n",
    "df.to_csv(folder + 'selected_cleaned.csv', index=False)\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0MYiOGlOFYA",
    "outputId": "91079fdd-6a05-44f3-aeff-5e5876be6e08",
    "ExecuteTime": {
     "end_time": "2024-01-07T20:10:59.127133700Z",
     "start_time": "2024-01-07T20:09:33.361215400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "folder = \"datasets/\"\n",
    "feature_name = 'combined_text'\n",
    "target_name = 'journal_num'\n",
    "gc.collect()\n",
    "df = pd.read_csv(folder + 'selected_cleaned.csv')\n",
    "\n",
    "#df[feature_names] = df[feature_names].apply(lambda x: x.apply(lambda words: ' '.join(words)))\n",
    "df[feature_name] = df[feature_names[0]]\n",
    "for i in range(1, len(feature_names)):\n",
    "    df[feature_name] = df[feature_name] + df[feature_names[i]]\n",
    "\n",
    "#Selecting only feature_name and target_name_num\n",
    "df = df[[feature_name, target_name_num]]\n",
    "\n",
    "df.to_csv(folder + 'selected_cleaned_' + feature_name + '.csv', index=False)\n",
    "df_train, df_test = train_test_split(df, train_size=0.8)\n",
    "df = None\n",
    "\n",
    "#Clean memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction**\n",
    "\n",
    "* **Objective** = Convert unstructured text into a numerical structure.\n",
    "* **Result** = Feature vector valid for use by modeling algorithms (classification, clustering, etc.).\n",
    "* The most commonly used approach is the concept of **\"Bag of Words\"** or **BoW**.\n",
    "  – The sequence in which words appear and their positions in the document are not taken into account.\n",
    "\n",
    "Create a **Bag of Words** using the **CountVectorizer** class from **scikit-learn**.\n",
    "\n",
    "**Training** and **Transformation**: Apply `fit_transform()` to the collection of documents to train the vectorizer and transform the text into a term-document matrix. The result `X` is a sparse matrix in CSR (Compressed Sparse Row) format.\n",
    "\n",
    "**Term-Document Matrix**: The term-document matrix can be visualized using `X.toarray()`, which converts it into a two-dimensional NumPy array based on word **frequency**.\n",
    "\n",
    "**Vocabulary**: Obtain the vocabulary (the sorted list of words) using `vectorizer.get_feature_names_out()`.\n",
    "\n",
    "[CountVectorizer Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXJsRwKzTRSl",
    "outputId": "db4cdc2a-b806-4370-85a3-5f465c0f18da",
    "ExecuteTime": {
     "end_time": "2024-01-07T20:17:16.903199200Z",
     "start_time": "2024-01-07T20:11:16.303169500Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.65 GiB for an array with shape (355056, 1000) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m bow_Tfidf \u001B[38;5;241m=\u001B[39m TfidfVectorizer(max_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n\u001B[0;32m      6\u001B[0m X_train_Count \u001B[38;5;241m=\u001B[39m bow_Count\u001B[38;5;241m.\u001B[39mfit_transform(df_train[feature_name])\u001B[38;5;241m.\u001B[39mtoarray()\n\u001B[1;32m----> 7\u001B[0m X_train_Tfidf \u001B[38;5;241m=\u001B[39m bow_Count\u001B[38;5;241m.\u001B[39mfit_transform(df_train[feature_name])\u001B[38;5;241m.\u001B[39mtoarray()\n\u001B[0;32m      8\u001B[0m y_train \u001B[38;5;241m=\u001B[39m df_train[target_name_num]\n\u001B[0;32m     10\u001B[0m X_test_Count \u001B[38;5;241m=\u001B[39m bow_Count\u001B[38;5;241m.\u001B[39mtransform(df_test[feature_name])\u001B[38;5;241m.\u001B[39mtoarray()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001B[0m, in \u001B[0;36m_cs_matrix.toarray\u001B[1;34m(self, order, out)\u001B[0m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m order \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1049\u001B[0m     order \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_swap(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcf\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m-> 1050\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_toarray_args(order, out)\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (out\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mc_contiguous \u001B[38;5;129;01mor\u001B[39;00m out\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mf_contiguous):\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOutput array must be C or F contiguous\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001B[0m, in \u001B[0;36m_spbase._process_toarray_args\u001B[1;34m(self, order, out)\u001B[0m\n\u001B[0;32m   1265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[0;32m   1266\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1267\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype, order\u001B[38;5;241m=\u001B[39morder)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 2.65 GiB for an array with shape (355056, 1000) and data type int64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "bow_Count = CountVectorizer(max_features=1000)\n",
    "bow_Tfidf = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "X_train_Count = bow_Count.fit_transform(df_train[feature_name]).toarray()\n",
    "X_train_Tfidf = bow_Count.fit_transform(df_train[feature_name]).toarray()\n",
    "y_train = df_train[target_name_num]\n",
    "\n",
    "X_test_Count = bow_Count.transform(df_test[feature_name]).toarray()\n",
    "X_test_Tfidf = bow_Count.transform(df_test[feature_name]).toarray()\n",
    "y_test = df_test[target_name_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjbf5z8aiEUt"
   },
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "URkwEQaGQJCO",
    "outputId": "8b14e75e-c974-4b74-86a2-f019e61c324c",
    "ExecuteTime": {
     "start_time": "2024-01-07T20:17:16.879413600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def plot_class_distribution(y):\n",
    "    plt.hist(y)\n",
    "    plt.title(\"Number of instances per class\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Number of instances\")\n",
    "    plt.xticks(ranks_ordinati, labels_value)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def performance(y_test, y_pred):\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='.5%', cmap='Blues', yticklabels=labels, xticklabels=labels, cbar=False)\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(y_train)\n",
    "\n",
    "# BoW ConuntVectorizer\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_Count, y_train)\n",
    "y_pred = cls.predict(X_test_Count)\n",
    "\n",
    "performance(y_test, y_pred)\n",
    "\n",
    "# BoW TfidfVectorizer\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_Tfidf, y_train)\n",
    "y_pred = cls.predict(X_test_Tfidf)\n",
    "\n",
    "performance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "id": "7Ccq6Hb2oTEl",
    "outputId": "8b5e1d41-3f88-4502-a862-bbad3961cfc7",
    "ExecuteTime": {
     "start_time": "2024-01-07T20:17:16.886309600Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import gc\n",
    "\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# BoW ConuntVectorizer\n",
    "X_train_under_sampled, y_train_under_sampled = sampler.fit_resample(X_train_Count, y_train)\n",
    "plot_class_distribution(y_train_under_sampled)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_Count, y_train)\n",
    "y_pred = cls.predict(X_test_Count)\n",
    "\n",
    "performance(y_test, y_pred)\n",
    "\n",
    "# BoW TfidfVectorizer\n",
    "X_train_under_sampled, y_train_under_sampled = sampler.fit_resample(X_train_Tfidf, y_train)\n",
    "plot_class_distribution(y_train_under_sampled)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_Tfidf, y_train)\n",
    "y_pred = cls.predict(X_test_Tfidf)\n",
    "\n",
    "performance(y_test, y_pred)\n",
    "\n",
    "#Clean memory\n",
    "X_train_under_sampled = None\n",
    "y_train_under_sampled = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "jj9_bUbx2fAx",
    "outputId": "e152f131-6ca8-43d3-dcb5-2031a46e516f",
    "ExecuteTime": {
     "start_time": "2024-01-07T20:17:16.891358Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import gc\n",
    "\n",
    "sampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# BoW ConuntVectorizer\n",
    "X_train_over_sampled, y_train_over_sampled = sampler.fit_resample(X_train_Count, y_train)\n",
    "plot_class_distribution(y_train_over_sampled)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_over_sampled, y_train_over_sampled)\n",
    "y_pred = cls.predict(X_test_Count)\n",
    "\n",
    "performance(y_test, y_pred)\n",
    "\n",
    "# BoW TfidfVectorizer\n",
    "X_train_over_sampled, y_train_over_sampled = sampler.fit_resample(X_train_Tfidf, y_train)\n",
    "plot_class_distribution(y_train_over_sampled)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_over_sampled, y_train_over_sampled)\n",
    "y_pred = cls.predict(X_test_Tfidf)\n",
    "\n",
    "performance(y_test, y_pred)\n",
    "\n",
    "X_train_over_sampled = None\n",
    "y_train_over_sampled = None\n",
    "sampler = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xk2zCYd5cmK4"
   },
   "source": [
    "\n",
    "## Connectionist techniques\n",
    "\n",
    "In this case, after pre-processing, a neural network based on an LSTM unit is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bjKZ7Y-g9tdH",
    "ExecuteTime": {
     "end_time": "2024-01-07T22:54:53.086332100Z",
     "start_time": "2024-01-07T22:54:23.791416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "Num. tarjetas:  0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "device = tf.config.list_physical_devices('GPU')\n",
    "print('Num. tarjetas: ', len(device))\n",
    "gc.collect()\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hA8DkOATcsTG",
    "outputId": "af35cf0b-0c14-4c6c-dbd9-5bcec5cd4578",
    "ExecuteTime": {
     "end_time": "2024-01-07T22:57:17.838247500Z",
     "start_time": "2024-01-07T22:57:00.049649900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(355056,)\n",
      "(88764,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(folder + 'selected_cleaned_' + feature_name + '.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[feature_name], df[target_name_num], test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "A5p_CIfi9MVx",
    "ExecuteTime": {
     "end_time": "2024-01-08T00:36:11.747853800Z",
     "start_time": "2024-01-07T22:59:30.380707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           50000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               60400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110804 (432.83 KB)\n",
      "Trainable params: 110804 (432.83 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "11096/11096 [==============================] - 1127s 100ms/step - loss: 0.2386 - accuracy: 0.9255 - val_loss: 0.1216 - val_accuracy: 0.9697\n",
      "Epoch 2/5\n",
      "11096/11096 [==============================] - 1085s 98ms/step - loss: 0.1181 - accuracy: 0.9702 - val_loss: 0.1027 - val_accuracy: 0.9738\n",
      "Epoch 3/5\n",
      "11096/11096 [==============================] - 1094s 99ms/step - loss: 0.0979 - accuracy: 0.9755 - val_loss: 0.0879 - val_accuracy: 0.9783\n",
      "Epoch 4/5\n",
      "11096/11096 [==============================] - 1068s 96ms/step - loss: 0.0878 - accuracy: 0.9779 - val_loss: 0.0819 - val_accuracy: 0.9797\n",
      "Epoch 5/5\n",
      "11096/11096 [==============================] - 1055s 95ms/step - loss: 0.0810 - accuracy: 0.9795 - val_loss: 0.0766 - val_accuracy: 0.9808\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x21cd01185d0>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_words = 1000\n",
    "sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words = num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_pad = sequence.pad_sequences(X_train_seq, maxlen=sequence_length)\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_pad= sequence.pad_sequences(X_test_seq, maxlen=sequence_length)\n",
    "\n",
    "\n",
    "num_classes = len(labels)\n",
    "# Convert your integer labels to one-hot encoded format\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with input_dim=num_words, output_dim=embedding_dim, input_length=sequence_length\n",
    "embedding_dim = 50  # You can adjust this value based on your specific task\n",
    "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=sequence_length))\n",
    "\n",
    "# Add an LSTM layer with a certain number of units (you can experiment with different values)\n",
    "lstm_units = 100\n",
    "model.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model.fit(X_train_pad, y_train_one_hot, batch_size=32, epochs=5, validation_data=(X_test_pad, y_test_one_hot), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-08T00:40:23.451917Z",
     "start_time": "2024-01-08T00:39:27.519475100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2774/2774 [==============================] - 51s 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.42      0.59      2455\n",
      "           1       0.99      0.91      0.95      3328\n",
      "           2       0.95      1.00      0.97     27428\n",
      "           3       1.00      1.00      1.00     55553\n",
      "\n",
      "    accuracy                           0.98     88764\n",
      "   macro avg       0.98      0.83      0.88     88764\n",
      "weighted avg       0.98      0.98      0.98     88764\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set using accuracy, f1-score, precision and recall\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(X_test_pad)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-06T20:16:46.919983200Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-06T20:16:46.919983200Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
